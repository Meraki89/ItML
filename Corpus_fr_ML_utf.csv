informatique;"Un algorithme est une suite finie et non ambiguë d'instructions et d’opérations permettant de résoudre une classe de problèmes.Le mot algorithme vient d'Al-Khwârizmî (en arabe : الخوارزمي), nom d'un mathématicien persan du IXe siècle.Le domaine qui étudie les algorithmes est appelé l'algorithmique. On retrouve aujourd'hui des algorithmes dans de nombreuses applications telles que le fonctionnement des ordinateurs, la cryptographie, le routage d'informations, la planification et l'utilisation optimale des ressources, le traitement d'images, le traitement de textes, la bio-informatique, etc.Un algorithme est une méthode générale pour résoudre un type de problèmes. Il est dit correct lorsque, pour chaque instance du problème, il se termine en produisant la bonne sortie, c'est-à-dire qu'il résout le problème posé.L'efficacité d'un algorithme est mesurée notamment par :sa durée de calcul ;sa consommation de mémoire vive (en partant du principe que chaque instruction a un temps d'exécution constant) ;la précision des résultats obtenus (par exemple avec l'utilisation de méthodes probabilistes) ;sa scalabilité (son aptitude à être efficacement parallélisé) ;etc.Les ordinateurs sur lesquels s'exécutent ces algorithmes ne sont pas infiniment rapides, car le temps de machine reste une ressource limitée, malgré une augmentation constante des performances des ordinateurs. Un algorithme sera donc dit performant s'il utilise avec parcimonie les ressources dont il dispose, c'est-à-dire le temps CPU, la mémoire vive et (objet de recherches récentes) la consommation électrique. L’analyse de la complexité algorithmique permet de prédire l'évolution en temps calcul nécessaire pour amener un algorithme à son terme, en fonction de la quantité de données à traiter.Donald Knuth (1938-) liste, comme prérequis d'un algorithme, cinq propriétés :finitude : « un algorithme doit toujours se terminer après un nombre fini d’étapes » ;définition précise : « chaque étape d'un algorithme doit être définie précisément, les actions à transposer doivent être spécifiées rigoureusement et sans ambiguïté pour chaque cas » ;entrées : « quantités qui lui sont données avant qu'un algorithme ne commence. Ces entrées sont prises dans un ensemble d'objets spécifié » ;sorties : « quantités ayant une relation spécifiée avec les entrées » ;rendement : « toutes les opérations que l'algorithme doit accomplir doivent être suffisamment basiques pour pouvoir être en principe réalisées dans une durée finie par un homme utilisant un papier et un crayon ».George Boolos (1940-1996), philosophe et mathématicien, propose la définition suivante :« Des instructions explicites pour déterminer le nième membre d'un ensemble, pour n un entier arbitrairement grand. De telles instructions sont données de façon bien explicite, sous une forme qui puisse être utilisée par une machine à calculer ou par un humain qui est capable de transposer des opérations très élémentaires en symboles. »Gérard Berry (1948-), chercheur en science informatique, en donne la définition grand public suivante :« Un algorithme, c’est tout simplement une façon de décrire dans ses moindres détails comment procéder pour faire quelque chose. Il se trouve que beaucoup d’actions mécaniques, toutes probablement, se prêtent bien à une telle décortication. Le but est d’évacuer la pensée du calcul, afin de le rendre exécutable par une machine numérique (ordinateur…). On ne travaille donc qu’avec un reflet numérique du système réel avec qui l’algorithme interagit. »Les algorithmes sont des objets historiquement dédiés à la résolution de problèmes arithmétiques, comme la multiplication de deux nombres. Ils ont été formalisés bien plus tard avec l'avènement de la logique mathématique et l'émergence des machines qui permettaient de les mettre en œuvre, à savoir les ordinateurs.La plupart des algorithmes ne sont pas numériques.On peut distinguer :des algorithmes généralistes qui s'appliquent à toute donnée numérique ou non numérique : par exemple les algorithmes liés au chiffrement, ou qui permettent de les mémoriser ou de les transmettre ;des algorithmes dédiés à un type de données particulier (par exemple ceux liés au traitement d'images).Voir aussi : Liste de sujets généraux sur les algorithmes (en)L'algorithmique intervient de plus en plus dans la vie quotidienne.Une recette de cuisine peut être réduite à un algorithme si on peut réduire sa spécification aux éléments constitutifs :des entrées (les ingrédients, le matériel utilisé) ;des instructions élémentaires simples (frire, flamber, rissoler, braiser, blanchir, etc.) dont les exécutions dans un ordre précis amènent au résultat voulu ;un résultat : le plat préparé.Cependant, les recettes de cuisine ne sont en général pas présentées rigoureusement sous forme non ambiguë : il est d'usage d'y employer des termes vagues laissant une liberté d'appréciation à l'exécutant alors qu'un algorithme non probabiliste stricto sensu doit être précis et sans ambiguïté.Le tissage, surtout tel qu'il a été automatisé par le métier Jacquard, est une activité que l'on peut dire algorithmique.Un casse-tête, comme le cube Rubik, peut être résolu de façon systématique par un algorithme qui mécanise sa résolution.En sport, l'exécution de séquences répondant à des finalités d'attaque, de défense, de progression, correspond à des algorithmes (dans un sens assez lâche du terme). Voir en particulier l'article tactique (football).En soins infirmiers, le jugement clinique est assimilable à un algorithme. Le jugement clinique désigne l'ensemble des procédés cognitifs et métacognitifs qui aboutissent au diagnostic infirmier. Il met en jeu des processus de pensée et de prise de décision dans le but d’améliorer l’état de santé et le bien-être des personnes que les soignants accompagnent.Un code juridique, qui décrit un ensemble de procédures applicables à un ensemble de cas, est un algorithme.Les progrès de ce qu'on appelle l'intelligence artificielle s'appuient sur un algorithmique de plus en plus complexe qui devient l'un des rouages cachés du Web 2.0 et des grands réseaux sociaux.À partir des années 2000, ce qui est appelé « algorithmique » est un ensemble de « boîtes noires » (autrement dit de processus informatiques dont on ne sait pas ce qu'il y a à l'intérieur) qui exploitent et influencent les comportements inconscients des consommateurs, et des électeurs.Au milieu des années 2010 la plate-forme logicielle Ripon permet secrètement l'élection de Donald Trump. Elle le fait grâce à une intelligence artificielle s'appuyant sur des logiciels issus de la guerre psychologique telle que développée en Afghanistan, et désormais nourrie du big data disponible sur l'Internet, et en particulier de données personnelles piratées dans plusieurs dizaines de millions de comptes Facebook. Ce piratage a été réalisé par Cambridge analytica au Royaume-Uni (devenu Emerdata en aout 2017) sur la plate-forme Facebook insuffisamment protégée. Les données ont été analysées et utilisées par sa société-sœur canadienne, Aggregate IQ, sous le contrôle du groupe SCL (leur société-mère) via Ripon. Cette plateforme Ripon ayant été conçue pour produire des profils psychographiques et des processus d'utilisation dans des campagnes électorales microciblées. Ces campagnes visaient à influer sur les émotions des électeurs, pour modifier leurs intentions de vote, ou les inciter à rester ou devenir abstentionnistes,,.Ces processus plus ou moins frauduleux (la législation de protection des individus sur l'Internet étant encore émergente) seront découvertes tardivement, dans le cadre du scandale Facebook-Cambridge Analytica/Aggregate IQ, après que ces outils aient conduits à l'élection de D. Trump, puis au Brexit, et qu'ils aient influencé au moins une vingtaine d'élections ou de référendums dans le monde. Dans les années 2010, les lanceurs d'alertes comme le canadien Christopher Wylie, Carole Cadwalladr, Shahmir Sanni, Brittany Kaiser, David Caroll, des journalistes comme Carole Cadwalladr et des ONG telles que AlgorithmWatch alertent sur les dérives éthiques qu'ils constatent dans l'usage malhonnête des algorithmes.Dans la vie quotidienne, un glissement de sens s'est opéré, ces dernières années, dans le concept d'« algorithme » qui devient à la fois plus réducteur, puisque ce sont pour l'essentiel des algorithmes de gestion du big data, et d'autre part plus universel en ce sens qu'il intervient dans tous les domaines du comportement quotidien. La famille des algorithmes dont il est question effectue des calculs à partir de grandes masses de données (les big data). Ils réalisent des classements, sélectionnent des informations et en déduisent un profil, en général de consommation, qui est ensuite utilisé ou exploité commercialement. Les implications sont nombreuses et touchent les domaines les plus variés. Mais les libertés individuelles et collectives pourraient être finalement mises en péril, comme le montre la mathématicienne américaine Cathy O'Neil dans le livre Weapons of Math Destruction, publié en 2016 et sorti en français en 2018 sous le titre Algorithmes : la bombe à retardement (aux éditions Les Arènes).« Aujourd’hui, les modèles mathématiques et les algorithmes prennent des décisions majeures, servent à classer et catégoriser les personnes et les institutions, influent en profondeur sur le fonctionnement des États sans le moindre contrôle extérieur. Et avec des effets de bords incontrôlables. […] Il s’agit d’un pouvoir utilisé contre les gens. Et pourquoi ça marche ? Parce que les gens ne connaissent pas les maths, parce qu’ils sont intimidés. C’est cette notion de pouvoir et de politique qui m’a fait réaliser que j’avais déjà vu ça quelque part. La seule différence entre les modèles de risque en finances et ce modèle de plus-value en science des données, c’est que, dans le premier cas, en 2008, tout le monde a vu la catastrophe liée à la crise financière. Mais, dans le cas des profs, personne ne voit l’échec. Ça se passe à un niveau individuel. Des gens se font virer en silence, ils se font humilier, ils ont honte d’eux. »Dans cet ouvrage, l'auteure alerte le lecteur sur les décisions majeures que nous déléguons aujourd'hui aux algorithmes dans des domaines aussi variés que l'éducation, la santé, l'emploi et la justice, sous prétexte qu'ils sont neutres et objectifs, alors que, dans les faits, ils donnent lieu à « des choix éminemment subjectifs, des opinions, voire des préjugés insérés dans des équations mathématiques ».L'opacité des algorithmes est l'une des raisons principales de ces critiques. Une meilleure information sur leur mode de fonctionnement spécifique permettrait de rendre plus clair le « contrat social passé entre les internautes et les calculateurs ». La description pour chaque algorithme de son propre principe de classement de l'information aide l'utilisateur à mieux comprendre les choix proposés par l'algorithme et les résultats obtenus.Les philosophes Wendell Wallach et Colin Allen ont soulevé des questions liées à l'implantation par les programmeurs de règles morales dans les algorithmes d'intelligence artificielle : « Aujourd'hui, les systèmes [automatiques] s'approchent d'un niveau de complexité qui, selon nous, exige qu'ils prennent eux-mêmes des décisions morales […]. Cela va élargir le cercle des agents moraux au-delà des humains à des systèmes artificiellement intelligents, que nous appellerons des agents moraux artificiels ». Dans son livre Faire la morale aux robots : une introduction à l'éthique des algorithmes, Martin Gibert met en évidence le rôle de la programmation dans l'éthique des robots, en traitant plus précisément des enjeux moraux liés à la construction des algorithmes. Il définit un algorithme comme « rien de plus qu'une suite d'instructions – ou de règles – pour parvenir à un objectif donné ». L'éthique des algorithmes poserait donc une question : « Quelles règles implanter dans les robots, et comment le faire ? ». Gibert souligne notamment l'ambiguïté de ces agents moraux artificiels :« Les agents moraux artificiels (AMA) ne sont pas cependant des agents moraux au sens fort du terme. Contrairement aux humains, ils ne semblent pas imputables [sic] de leurs actes. Ils n'ont toutefois pas besoin de l'être pour prendre des décisions moralement significatives et soulever tout un tas de questions en éthique des algorithmes. »Analyse de la complexité des algorithmesAlgorithmiqueCorrection d'un algorithmeBiais algorithmiqueRégulation des algorithmesRessource relative à la santé : (en) Medical Subject Headings Qu’est-ce qu'un algorithme ? par Philippe Flajolet et Étienne Parizot sur la revue en ligne IntersticesDéfinition du terme « algorithme » par des savants Portail de l'informatique théorique"
informatique;"L'apprentissage automatique, (en anglais : machine learning, litt. « apprentissage machine, »), apprentissage artificiel ou apprentissage statistique est un  champ d'étude de l'intelligence artificielle qui se fonde sur des approches mathématiques et statistiques pour donner aux ordinateurs la capacité d'« apprendre » à partir de données, c'est-à-dire d'améliorer leurs performances à résoudre des tâches sans être explicitement programmés pour chacune. Plus largement, il concerne la conception, l'analyse, l'optimisation, le développement et l'implémentation de telles méthodes. On parle d'apprentissage statistique car l'apprentissage consiste à créer un modèle dont l'erreur statistique moyenne est la plus faible possible.L'apprentissage automatique comporte généralement deux phases. La première consiste à estimer un modèle à partir de données, appelées observations, qui sont disponibles et en nombre fini, lors de la phase de conception du système. L'estimation du modèle consiste à résoudre une tâche pratique, telle que traduire un discours, estimer une densité de probabilité, reconnaître la présence d'un chat dans une photographie ou participer à la conduite d'un véhicule autonome. Cette phase dite « d'apprentissage » ou « d'entraînement » est généralement réalisée préalablement à l'utilisation pratique du modèle. La seconde phase correspond à la mise en production : le modèle étant déterminé, de nouvelles données peuvent alors être soumises afin d'obtenir le résultat correspondant à la tâche souhaitée. En pratique, certains systèmes peuvent poursuivre leur apprentissage une fois en production, pour peu qu'ils aient un moyen d'obtenir un retour sur la qualité des résultats produits.Selon les informations disponibles durant la phase d'apprentissage, l'apprentissage est qualifié de différentes manières. Si les données sont étiquetées (c'est-à-dire que la réponse à la tâche est connue pour ces données), il s'agit d'un apprentissage supervisé. On parle de classification ou de classement si les étiquettes sont discrètes, ou de régression si elles sont continues. Si le modèle est appris de manière incrémentale en fonction d'une récompense reçue par le programme pour chacune des actions entreprises, on parle d'apprentissage par renforcement. Dans le cas le plus général, sans étiquette, on cherche à déterminer la structure sous-jacente des données (qui peuvent être une densité de probabilité) et il s'agit alors d'apprentissage non supervisé. L'apprentissage automatique peut être appliqué à différents types de données, tels des graphes, des arbres, des courbes, ou plus simplement des vecteurs de caractéristiques, qui peuvent être des variables qualitatives ou quantitatives continues ou discrètes.Depuis l'antiquité, le sujet des machines pensantes préoccupe les esprits. Ce concept est la base de pensées pour ce qui deviendra ensuite l'intelligence artificielle, ainsi qu'une de ses sous-branches : l'apprentissage automatique.La concrétisation de cette idée est principalement due à Alan Turing (mathématicien et cryptologue britannique) et à son concept de la « machine universelle » en 1936, qui est à la base des ordinateurs d'aujourd'hui. Il continuera à poser les bases de l'apprentissage automatique, avec son article sur « L'ordinateur et l'intelligence » en 1950, dans lequel il développe, entre autres, le test de Turing.En 1943, le neurophysiologiste Warren McCulloch et le mathématicien Walter Pitts publient un article décrivant le fonctionnement de neurones en les représentant à l'aide de circuits électriques. Cette représentation sera la base théorique des réseaux neuronaux.Arthur Samuel, informaticien américain pionnier dans le secteur de l'intelligence artificielle, est le premier à faire usage de l'expression machine learning (en français, « apprentissage automatique ») en 1959 à la suite de la création de son programme pour IBM en 1952. Le programme jouait au Jeu de Dames et s'améliorait en jouant. À terme, il parvint à battre le 4e meilleur joueur des États-Unis,.Une avancée majeure dans le secteur de l'intelligence machine est le succès de l'ordinateur développé par IBM, Deep Blue, qui est le premier à vaincre le champion mondial d'échecs Garry Kasparov en 1997. Le projet Deep Blue en inspirera nombre d'autres dans le cadre de l'intelligence artificielle, particulièrement un autre grand défi : IBM Watson, l'ordinateur dont le but est de gagner au jeu Jeopardy!. Ce but est atteint en 2011, quand Watson gagne à Jeopardy! en répondant aux questions par traitement de langage naturel.Durant les années suivantes, les applications de l'apprentissage automatique médiatisées se succèdent bien plus rapidement qu'auparavant.En 2012, un réseau neuronal développé par Google parvient à reconnaître des visages humains ainsi que des chats dans des vidéos YouTube,.En 2014, 64 ans après la prédiction d'Alan Turing, le dialogueur Eugene Goostman est le premier à réussir le test de Turing en parvenant à convaincre 33 % des juges humains au bout de cinq minutes de conversation qu'il est non pas un ordinateur, mais un garçon ukrainien de 13 ans.En 2015, une nouvelle étape importante est atteinte lorsque l'ordinateur « AlphaGo » de Google gagne contre un des meilleurs joueurs au jeu de Go, jeu de plateau considéré comme le plus dur du monde.En 2016, un système d'intelligence artificielle à base d'apprentissage automatique nommé LipNet parvient à lire sur les lèvres avec un grand taux de succès,.L'apprentissage automatique (AA) permet à un système piloté ou assisté par ordinateur comme un programme, une IA ou un robot, d'adapter ses réponses ou comportements aux situations rencontrées, en se fondant sur l'analyse de données empiriques passées issues de bases de données, de capteurs, ou du web.L'AA permet de surmonter la difficulté qui réside dans le fait que l'ensemble de tous les comportements possibles compte tenu de toutes les entrées possibles devient rapidement trop complexe à décrire et programmer de manière classique (on parle d'explosion combinatoire). On confie donc à des programmes d'AA le soin d'ajuster un modèle pour simplifier cette complexité et de l'utiliser de manière opérationnelle. Idéalement, l'apprentissage visera à être non supervisé, c'est-à-dire que les réponses aux données d’entraînement ne sont pas fournies au modèle.Ces programmes, selon leur degré de perfectionnement, intègrent éventuellement des capacités de traitement probabiliste des données, d'analyse de données issues de capteurs, de reconnaissance (reconnaissance vocale, de forme, d'écriture…), de fouille de données, d'informatique théorique…L'apprentissage automatique est utilisé dans un large spectre d'applications pour doter des ordinateurs ou des machines de capacité d'analyser des données d'entrée comme : perception de leur environnement (vision, Reconnaissance de formes tels des visages, schémas, segmentation d'image, langages naturels, caractères dactylographiés ou manuscrits ; moteurs de recherche, analyse et indexation d'images et de vidéo, en particulier pour la recherche d'image par le contenu ; aide aux diagnostics, médical notamment, bio-informatique, chémoinformatique ; interfaces cerveau-machine ; détection de fraudes à la carte de crédit, cybersécurité, analyse financière, dont analyse du marché boursier ; classification des séquences d'ADN ; jeu ; génie logiciel ; adaptation de sites Web ; robotique (locomotion de robots, etc.) ; analyse prédictive dans de nombreux domaines (financière, médicale, juridique, judiciaire), diminution des temps de calcul pour les simulations informatiques en physique (calcul de structures, de mécanique des fluides, de neutronique, d'astrophysique, de biologie moléculaire, etc.),, optimisation de design dans l'industrie,,, etc.Exemples :un système d'apprentissage automatique peut permettre à un robot ayant la capacité de bouger ses membres, mais ne sachant initialement rien de la coordination des mouvements permettant la marche, d'apprendre à marcher. Le robot commencera par effectuer des mouvements aléatoires, puis, en sélectionnant et privilégiant les mouvements lui permettant d'avancer, mettra peu à peu en place une marche de plus en plus efficace[réf. nécessaire] ;la reconnaissance de caractères manuscrits est une tâche complexe car deux caractères similaires ne sont jamais exactement identiques. Il existe des systèmes d'apprentissage automatique qui apprennent à reconnaître des caractères en observant des « exemples », c'est-à-dire des caractères connus. Un des premiers système de ce type est celui de reconnaissance des codes postaux US manuscrits issu des travaux de recherche de Yann Le Cun, un des pionniers du domaine ,, et ceux utilisés pour la  reconnaissance d'écriture ou OCR.Les algorithmes d'apprentissage peuvent se catégoriser selon le mode d'apprentissage qu'ils emploient.Si les classes sont prédéterminées et les exemples connus, le système apprend à classer selon un modèle de classification ou de classement ; on parle alors d'apprentissage supervisé (ou d'analyse discriminante). Un expert (ou oracle) doit préalablement étiqueter des exemples. Le processus se passe en deux phases. Lors de la première phase (hors ligne, dite d'apprentissage), il s'agit de déterminer un modèle à partir des données étiquetées. La seconde phase (en ligne, dite de test) consiste à prédire l'étiquette d'une nouvelle donnée, connaissant le modèle préalablement appris. Parfois il est préférable d'associer une donnée non pas à une classe unique, mais une probabilité d'appartenance à chacune des classes prédéterminées ; on parle alors d'apprentissage supervisé probabiliste.Fondamentalement, le machine learning supervisé revient à apprendre à une machine à construire une fonction f telle que Y = f(X), Y étant un (ou plusieurs) résultat(s) d'intérêt calculé en fonction de données d'entrées X effectivement à la disposition de l'utilisateur. Y peut être une grandeur continue (une température par exemple), et on parle alors de régression, ou discrète (une classe, chien ou chat par exemple), et on parle alors de classification.Des cas d'usage typiques d'apprentissage automatique peuvent être d'estimer la météo du lendemain en fonction de celle du jour et des jours précédents, de prédire le vote d'un électeur en fonction de certaines données économiques et sociales, d'estimer la résistance d'un nouveau matériau en fonction de sa composition, de déterminer la présence ou non d'un objet dans une image. L'analyse discriminante linéaire ou les SVM en sont d'autres exemples typiques. Autre exemple, en fonction de points communs détectés avec les symptômes d'autres patients connus (les exemples), le système peut catégoriser de nouveaux patients, au vu de leurs analyses médicales, en risque estimé de développer telle ou telle maladie.Quand le système ou l'opérateur ne dispose que d'exemples, mais non d'étiquette, et que le nombre de classes et leur nature n'ont pas été prédéterminées, on parle d'apprentissage non supervisé ou clustering en anglais. Aucun expert n'est requis. L'algorithme doit découvrir par lui-même la structure plus ou moins cachée des données. Le partitionnement de données, data clustering en anglais, est un algorithme d'apprentissage non supervisé.Le système doit ici — dans l'espace de description (l'ensemble des données) — cibler les données selon leurs attributs disponibles, pour les classer en groupes homogènes d'exemples. La similarité est généralement calculée selon une fonction de distance entre paires d'exemples. C'est ensuite à l'opérateur d'associer ou déduire du sens pour chaque groupe et pour les motifs (patterns en anglais) d'apparition de groupes, ou de groupes de groupes, dans leur « espace ». Divers outils mathématiques et logiciels peuvent l'aider. On parle aussi d'analyse des données en régression (ajustement d'un modèle par une procédure de type moindres carrés ou autre optimisation d'une fonction de coût). Si l'approche est probabiliste (c'est-à-dire que chaque exemple, au lieu d'être classé dans une seule classe, est caractérisé par un jeu de probabilités d'appartenance à chacune des classes), on parle alors de « soft clustering » (par opposition au « hard clustering »).Cette méthode est souvent source de sérendipité. ex. : Pour un épidémiologiste qui voudrait dans un ensemble assez large de victimes de cancer du foie tenter de faire émerger des hypothèses explicatives, l'ordinateur pourrait différencier différents groupes, que l'épidémiologiste chercherait ensuite à associer à divers facteurs explicatifs, origines géographique, génétique, habitudes ou pratiques de consommation, expositions à divers agents potentiellement ou effectivement toxiques (métaux lourds, toxines telle que l'aflatoxine, etc.).Contrairement à l’apprentissage supervisé où l’apprentissage automatique consiste à trouver une fonction f telle que Y = f(X), où Y est un résultat connu et objectif (par exemple Y = « présence d’une tumeur » ou « absence de tumeur » en fonction de X = image radiographique), dans l’apprentissage non supervisé, on ne dispose pas de valeurs de Y, uniquement de valeurs de X (dans l’exemple précédent, on disposerait uniquement des images radiographiques sans connaissance de la présence ou non d’une tumeur. L'apprentissage non supervisé pourrait découvrir deux ""clusters"" ou groupes correspondant à ""présence"" ou ""absence"" de tumeur, mais les chances de réussite sont moindres que dans le cas supervisé où la machine est orientée sur ce qu'elle doit trouver).L’apprentissage non supervisé est généralement moins performant que l’apprentissage supervisé, il évolue dans une zone « grise » où il n’y a généralement pas de « bonne » ou de « mauvaise » réponse mais simplement des similarités mathématiques discernables ou non. L’apprentissage non supervisé présente cependant l’intérêt de pouvoir travailler sur une base de données de X sans qu’il soit nécessaire d’avoir des valeurs de Y correspondantes, or les Y sont généralement compliqués et/ou coûteux à obtenir, alors que les seuls X sont généralement plus simples et moins coûteux à obtenir (dans l’exemple des images radiographiques, il est relativement aisé d’obtenir de telles images, alors qu’obtenir les images avec le label « présence de tumeur » ou « absence de tumeur » nécessite l’intervention longue et coûteuse d’un spécialiste en imagerie médicale).L’apprentissage non supervisé permet potentiellement de détecter des anomalies dans une base de données, comme des valeurs singulières ou aberrantes pouvant provenir d’une erreur de saisie ou d’une singularité très particulière. Il peut donc s’agir d’un outil intéressant pour vérifier ou nettoyer une base de données.Effectué de manière probabiliste ou non, il vise à faire apparaître la distribution sous-jacente des exemples dans leur espace de description. Il est mis en œuvre quand des données (ou « étiquettes ») manquent… Le modèle doit utiliser des exemples non étiquetés pouvant néanmoins renseigner. ex. : En médecine, il peut constituer une aide au diagnostic ou au choix des moyens les moins onéreux de tests de diagnostic.Probabiliste ou non, quand l'étiquetage des données est partiel. C'est le cas quand un modèle énonce qu'une donnée n'appartient pas à une classe A, mais peut-être à une classe B ou C (A, B et C étant trois maladies par exemple évoquées dans le cadre d'un diagnostic différentiel).L’apprentissage auto-supervisé consiste à construire un problème d’apprentissage supervisé à partir d’un problème non supervisé à l’origine.Pour rappel, l’apprentissage supervisé consiste à construire une fonction Y = f(X) et nécessite donc une base de données où l’on possède des Y en fonction des X (par exemple, en fonction du texte X correspondant à la critique d’un film, retrouver la valeur du Y correspondant à la note attribuée au film), alors que dans l’apprentissage non supervisé, on dispose uniquement des valeurs de X et pas de valeurs de Y (on disposerait par exemple ici uniquement du texte X correspondant à la critique du film, et pas de la note Y attribuée au film).L’apprentissage auto-supervisé consiste donc à créer des Y à partir des X pour passer à un apprentissage supervisé, en ""masquant"" des X pour en faire des Y. Dans le cas d'une image, l'apprentissage auto-supervisé peut consister à reconstruire la partie manquante d'une image qui aurait été tronquée. Dans le cas du langage, lorsqu’on dispose d’un ensemble de phrases qui correspondent aux X sans cible Y particulière, l’apprentissage auto-supervisé consiste à supprimer certains X (certains mots) pour en faire des Y. L’apprentissage auto-supervisé revient alors pour la machine à essayer de reconstruire un mot ou un ensemble de mots manquants en fonction des mots précédents et/ou suivants, en une forme d’auto-complétion. Cette approche permet potentiellement à une machine de « comprendre » le langage humain, son sens sémantique et symbolique. Les modèles IA de langage comme BERT ou GPT-3 sont conçus selon ce principe. Dans le cas d’un film, l’apprentissage auto-supervisé consisterait à essayer de prédire les images suivantes en fonction des images précédentes, et donc à tenter de prédire « l’avenir » sur la base de la logique probable du monde réel.Certains chercheurs, comme Yann Le Cun, pensent que si l’IA générale est possible, c’est probablement par une approche de type auto-supervisé qu’elle pourrait être conçue, par exemple en étant immergée dans le monde réel pour essayer à chaque instant de prédire les images et les sons les plus probables à venir, en comprenant qu’un ballon en train de rebondir et de rouler va encore continuer à rebondir et à rouler, mais de moins en moins haut et de moins en moins vite jusqu’à s’arrêter, et qu'un obstacle est de nature à arrêter le ballon ou à modifier sa trajectoire, ou à essayer de prédire les prochains mots qu’une personne est susceptible de prononcer ou le prochain geste qu’elle pourrait accomplir. L’apprentissage auto-supervisé dans le monde réel serait une façon d’apprendre à une machine le sens commun, le bon sens, la réalité du monde physique qui l’entoure, et permettrait potentiellement d’atteindre une certaine forme de conscience. Il ne s’agit évidemment que d’une hypothèse de travail, la nature exacte de la conscience, son fonctionnement et sa définition même restant un domaine actif de recherche.L'algorithme apprend un comportement étant donné une observation. L'action de l'algorithme sur l'environnement produit une valeur de retour qui guide l'algorithme d'apprentissage.Par exemple, l'algorithme de Q-learning est un exemple classique.L'apprentissage par renforcement peut aussi être vu comme une forme d'apprentissage auto-supervisé. Dans un problème d'apprentissage par renforcement, il n'y a en effet à l'origine pas de données de sorties Y, ni même de données d'entrée X, pour construire une fonction Y = f(X). Il y a simplement un ""écosystème"" avec des règles qui doivent être respectées, et un ""objectif"" à atteindre. Par exemple, pour le football, il y a des règles du jeu à respecter et des buts à marquer. Dans l'apprentissage par renforcement, le modèle crée lui-même sa base de donnes en ""jouant"" (d'où le concept d'auto-supervisé) : il teste des combinaisons de données d'entrée X et il en découle un résultat Y qui est évalué, s'il est conforme aux règles du jeu et atteint son objectif, le modèle est récompensé et sa stratégie est ainsi validée, sinon le modèle est pénalisé. Par exemple pour le football, dans une situation du type ""ballon possédé, joueur adverse en face, but à 20 mètres"", une stratégie peut être de ""tirer"" ou de ""dribbler"", et en fonction du résultat (""but marqué"", ""but raté"", ""balle toujours possédée, joueur adverse franchi""), le modèle apprend de manière incrémentale comment se comporter au mieux en fonction des différentes situations rencontrées.L’apprentissage par transfert peut être vu comme la capacité d’un système à reconnaître et à appliquer des connaissances et des compétences, apprises à partir de tâches antérieures, sur de nouvelles tâches ou domaines partageant des similitudes. Il s'agit d'identifier les similitudes entre la ou les tâche(s) cible(s) et la ou les tâche(s) source(s), puis de transférer la connaissance de la ou des tâche(s) source(s) vers la ou les tâche(s) cible(s),.Une application classique de l’apprentissage par transfert est l’analyse d’images. Pour une problématique de classification, l’apprentissage par transfert consiste à repartir d’un modèle existant plutôt que de repartir de zéro. Si par exemple on dispose déjà d’un modèle capable de repérer un chat parmi tout autre objet du quotidien, et que l’on souhaite classifier les chats par races, il est possible que réentraîner partiellement le modèle existant permette d’obtenir de meilleures performances et à moindre coût qu’en repartant de zéro,. Un modèle souvent utilisé pour réaliser un apprentissage par transfert de ce type est VGG-16, un réseau de neurones conçu par l'Université d'Oxford, entraîné sur ~14 millions d'images, capable de classer avec ~93% de précision mille objets du quotidien.Les algorithmes se classent en quatre familles ou types principaux :régressionclassificationpartitionnement de donnéesréduction de dimensions.Plus précisément :la régression linéaire ;la régression logistique ;les machines à vecteur de support ;les réseaux de neurones, dont les méthodes d'apprentissage profond (deep learning en anglais) pour un apprentissage supervisé ou non-supervisé ;la méthode des k plus proches voisins pour un apprentissage supervisé ;les arbres de décision, méthodes à l'origine des Random Forest, par extension également du boosting (notamment XGBoost) ;les méthodes statistiques comme le modèle de mixture gaussienne ;l'analyse discriminante linéaire ;les algorithmes génétiques et la programmation génétique ;le boosting ;le bagging.Ces méthodes sont souvent combinées pour obtenir diverses variantes d'apprentissage. Le choix d'un algorithme dépend fortement de la tâche à résoudre (classification, estimation de valeurs…), du volume et de la nature des données. Ces modèles reposent souvent sur des modèles statistiques.La qualité de l'apprentissage et de l'analyse dépendent du besoin en amont et a priori de la compétence de l'opérateur pour préparer l'analyse. Elle dépend aussi de la complexité du modèle (spécifique ou généraliste), de son adéquation et de son adaptation au sujet à traiter. In fine, la qualité du travail dépendra aussi du mode (de mise en évidence visuelle) des résultats pour l'utilisateur final (un résultat pertinent pourrait être caché dans un schéma trop complexe, ou mal mis en évidence par une représentation graphique inappropriée).Avant cela, la qualité du travail dépendra de facteurs initiaux contraignants, liées à la base de données :nombre d'exemples (moins il y en a, plus l'analyse est difficile, mais plus il y en a, plus le besoin de mémoire informatique est élevé et plus longue est l'analyse) ;nombre et qualité des attributs décrivant ces exemples. La distance entre deux « exemples » numériques (prix, taille, poids, intensité lumineuse, intensité de bruit, etc.) est facile à établir, celle entre deux attributs catégoriels (couleur, beauté, utilité…) est plus délicate ;pourcentage de données renseignées et manquantes ;bruit : le nombre et la « localisation » des valeurs douteuses (erreurs potentielles, valeurs aberrantes…) ou naturellement non-conformes au pattern de distribution générale des « exemples » sur leur espace de distribution impacteront sur la qualité de l'analyse.L'apprentissage automatique ne se résume pas à un ensemble d'algorithmes, mais suit une succession d'étapes,.Définir le problème à résoudre.Acquérir des données : l'algorithme se nourrissant des données en entrée, c'est une étape importante. Il en va de la réussite du projet, de récolter des données pertinentes et en quantité et qualité suffisantes, et en évitant tout biais dans leur représentativité.Analyser et explorer les données. L'exploration des données peut révéler des données d'entrée ou de sortie déséquilibrées pouvant nécessiter un rééquilibrage, le machine learning non supervisé peut révéler des clusters qu'il pourrait être utile de traiter séparément ou encore détecter des anomalies qu'il pourrait être utile de supprimer.Préparer et nettoyer les données : les données recueillies doivent être retouchées avant utilisation. En effet, certains attributs sont inutiles, d’autre doivent être modifiés afin d’être compris par l’algorithme (les variables qualitatives doivent être encodées-binarisées), et certains éléments sont inutilisables car leurs données sont incomplètes (les valeurs manquantes doivent être gérées, par exemple par simple suppression des exemples comportant des variables manquantes, ou par remplissage par la médiane, voire par apprentissage automatique). Plusieurs techniques telles que la visualisation de données, la transformation de données (en) ou encore la normalisation (variables projetées entre 0 et 1) ou la standardisation (variables centrées - réduites) sont employées afin d'homogénéiser les variables entre elles, notamment pour aider la phase de descente de gradient nécessaire à l'apprentissage.Ingénierie ou extraction de caractéristiques : les attributs peuvent être combinés entre eux pour en créer de nouveaux plus pertinents et efficaces pour l’entraînement du modèle. Ainsi, en physique, de la construction de nombres adimensionnels adaptés au problème, de solutions analytiques approchées, de statistiques pertinentes, de corrélations empiriques ou l'extraction de spectres par transformée de Fourier ,. Il s'agit d'ajouter l'expertise humaine au préalable de l'apprentissage machine pour favoriser celui-ci.Choisir ou construire un modèle d’apprentissage : un large choix d'algorithmes existe, et il faut en choisir un adapté au problème et aux données. La métrique optimisée doit être choisie judicieusement (erreur absolue moyenne, erreur relative moyenne, précision, rappel, etc.)Entraîner, évaluer et optimiser : l'algorithme d'apprentissage automatique est entraîné et validé sur un premier jeu de données pour optimiser ses hyperparamètres.Test : puis il est évalué sur un deuxième ensemble de données de test afin de vérifier qu'il est efficace avec un jeu de donnée indépendant des données d’entraînement, et pour vérifier qu'il ne fasse pas de surapprentissage.Déployer : le modèle est alors déployé en production pour faire des prédictions, et potentiellement utiliser les nouvelles données en entrée pour se ré-entraîner et être amélioré.Expliquer : déterminer quelles sont les variables importantes et comment elles impactent les prédictions du modèle en général et au cas par casLa plupart de ces étapes se retrouvent dans les méthodes et processus de projet KDD, CRISP-DM et SEMMA, qui concernent les projets d'exploration de données.Toutes ces étapes sont complexes et requièrent du temps et de l'expertise, mais il existe des outils permettant de les automatiser au maximum pour ""démocratiser"" l'accès à l'apprentissage automatique. Ces approches sont dites ""Auto ML"" (pour machine learning automatique) ou ""No Code"" (pour illustrer que ces approches ne nécessitent pas ou très peu de programmation informatique), elles permettent d'automatiser la construction de modèles d'apprentissage automatique pour limiter au maximum le besoin d'intervention humaine. Parmi ces outils, commerciaux ou non, on peut citer Caret, PyCaret, pSeven, Jarvis, Knime, MLBox ou DataRobot.La voiture autonome paraît en 2016 réalisable grâce à l’apprentissage automatique et les énormes quantités de données générées par la flotte automobile, de plus en plus connectée. Contrairement aux algorithmes classiques (qui suivent un ensemble de règles prédéterminées), l’apprentissage automatique apprend ses propres règles.Les principaux innovateurs dans le domaine insistent sur le fait que le progrès provient de l’automatisation des processus. Ceci présente le défaut que le processus d’apprentissage automatique devient privatisé et obscur. Privatisé, car les algorithmes d’AA constituent des gigantesques opportunités économiques, et obscurs car leur compréhension passe derrière leur optimisation. Cette évolution peut potentiellement nuire à la confiance du public envers l’apprentissage automatique, mais surtout au potentiel à long terme de techniques très prometteuses.La voiture autonome présente un cadre test pour confronter l’apprentissage automatique à la société. En effet, ce n’est pas seulement l’algorithme qui se forme à la circulation routière et ses règles, mais aussi l’inverse. Le principe de responsabilité est remis en cause par l’apprentissage automatique, car l’algorithme n’est plus écrit mais apprend et développe une sorte d’intuition numérique. Les créateurs d’algorithmes ne sont plus en mesure de comprendre les « décisions » prises par leurs algorithmes, ceci par construction mathématique même de l’algorithme d’apprentissage automatique.Dans le cas de l’AA et les voitures autonomes, la question de la responsabilité en cas d’accident se pose. La société doit apporter une réponse à cette question, avec différentes approches possibles. Aux États-Unis, il existe la tendance à juger une technologie par la qualité du résultat qu’elle produit, alors qu’en Europe le principe de précaution est appliqué, et on y a plus tendance à juger une nouvelle technologie par rapport aux précédentes, en évaluant les différences par rapport à ce qui est déjà connu. Des processus d’évaluation de risques sont en cours en Europe et aux États-Unis.La question de responsabilité est d’autant plus compliquée que la priorité chez les concepteurs réside en la conception d’un algorithme optimal, et non pas de le comprendre. L’interprétabilité des algorithmes est nécessaire pour en comprendre les décisions, notamment lorsque ces décisions ont un impact profond sur la vie des individus. Cette notion d’interprétabilité, c’est-à-dire de la capacité de comprendre pourquoi et comment un algorithme agit, est aussi sujette à interprétation.La question de l’accessibilité des données est sujette à controverse : dans le cas des voitures autonomes, certains défendent l’accès public aux données, ce qui permettrait un meilleur apprentissage aux algorithmes et ne concentrerait pas cet « or numérique » dans les mains d’une poignée d’individus, de plus d’autres militent pour la privatisation des données au nom du libre marché, sans négliger le fait que des bonnes données constituent un avantage compétitif et donc économique,.La question des choix moraux liés aux décisions laissées aux algorithmes d'AA et aux voitures autonomes en cas de situations dangereuses ou mortelles se pose aussi. Par exemple en cas de défaillance des freins du véhicule, et d'accident inévitable, quelles vies sont à sauver en priorité: celle des passagers ou bien celle des piétons traversant la rue ?Dans les années 2000-2010, l'apprentissage automatique est encore une technologie émergente, mais polyvalente, qui est par nature théoriquement capable d'accélérer le rythme de l'automatisation et de l'autoaprentissage lui-même. Combiné à l'apparition de nouveaux moyens de produire, stocker et faire circuler l'énergie, ainsi qu'à l'informatique ubiquiste, il pourrait bouleverser les technologies et la société comme l'ont fait la machine à vapeur et l'électricité, puis le pétrole et l'informatique lors des révolutions industrielles précédentes.L'apprentissage automatique pourrait générer des innovations et des capacités inattendues, mais avec un risque selon certains observateurs de perte de maîtrise de la part des humains sur de nombreuses tâches qu'ils ne pourront plus comprendre et qui seront faites en routine par des entités informatiques et robotisées. Ceci laisse envisager des impacts spécifiques complexes et encore impossibles à évaluer sur l'emploi, le travail et plus largement l'économie et les inégalités. Selon le journal Science fin 2017 : « Les effets sur l'emploi sont plus complexes que la simple question du remplacement et des substitutions soulignées par certains. Bien que les effets économiques du BA soient relativement limités aujourd'hui et que nous ne soyons pas confrontés à une « fin du travail » imminente comme cela est parfois proclamé, les implications pour l'économie et la main-d'œuvre sont profondes ».Il est tentant de s'inspirer des êtres vivants sans les copier naïvement pour concevoir des machines capables d'apprendre. Les notions de percept et de concept comme phénomènes neuronaux physiques ont d'ailleurs été popularisés dans le monde francophone par Jean-Pierre Changeux. L'apprentissage automatique reste avant tout un sous-domaine de l'informatique, mais il est étroitement lié opérationn"
informatique;Dans le domaine informatique et de l'intelligence artificielle, l'apprentissage non supervisé désigne la situation d'apprentissage automatique où les données ne sont pas étiquetées. Il s'agit donc de découvrir les structures sous-jacentes à ces données non étiquetées. Puisque les données ne sont pas étiquetées, il est impossible à l'algorithme de calculer de façon certaine un score de réussite.L'absence d'étiquetage ou d'annotation caractérise les tâches d'apprentissage non supervisé et les distingue donc des tâches d'apprentissage supervisé.L'introduction dans un système d'une approche d'apprentissage non supervisé est un moyen d'expérimenter l'intelligence artificielle. En général, des systèmes d'apprentissage non supervisé permettent d'exécuter des tâches plus complexes que les systèmes d'apprentissage supervisé, mais ils peuvent aussi être plus imprévisibles. Même si un système d'IA d'apprentissage non supervisé parvient tout seul, par exemple, à faire le tri entre des chats et des chiens, il peut aussi ajouter des catégories inattendues et non désirées, et classer des races inhabituelles, introduisant plus de bruit que d'ordre.L'apprentissage non supervisé consiste à apprendre sans superviseur. Il s’agit d’extraire des classes ou groupes d’individus présentant des caractéristiques communes. La qualité d'une méthode de classification est mesurée par sa capacité à découvrir certains ou tous les motifs cachés.On distingue l'apprentissage supervisé et non supervisé. Dans le premier apprentissage, il s’agit d’apprendre à classer un nouvel individu parmi un ensemble de classes prédéfinies: on connaît les classes a priori. Tandis que dans l'apprentissage non supervisé, le nombre et la définition des classes ne sont pas donnés a priori.Différence entre les deux types d'apprentissage. Apprentissage supervisé On dispose d'éléments déjà classésExemple : articles en rubrique cuisine, sport, culture...On veut classer un nouvel élémentExemple: lui attribuer un nom parmi cuisine, sport, culture... Apprentissage non supervisé On dispose d'éléments non classésExemple : une fleurOn veut les regrouper en classesExemple: si deux fleurs ont la même forme, elles sont en rapport avec une même plante correspondante.Il existe deux principales méthodes d'apprentissage non supervisées :Les méthodes par partitionnement telles que les algorithmes des k-moyennes ou k-médoïdes.Les méthodes de regroupement hiérarchique.Les techniques d'apprentissage non supervisé peuvent être utilisées pour résoudre, entre autres, les problèmes suivants :le partitionnement de données (par exemple avec l'algorithme des k-moyennes, le regroupement hiérarchique),l'estimation de densité de distribution (distribution de mélange, estimation par noyau),la réduction de dimension (analyse en composantes principales, carte auto-adaptative)L'apprentissage non supervisé peut aussi être utilisé en conjonction avec une inférence bayésienne pour produire des probabilités conditionnelles pour chaque variable aléatoire étant donné les autres.K-means clustering (K-moyenne)Dimensionality Reduction (Réduction de la dimensionnalité)Principal Component Analysis (Analyse en composantes principales)Singular Value Decomposition (Décomposition en valeurs singulières)Independent Component Analysis (Analyse en composantes indépendantes)Distribution models (Modèles de distribution)Hierarchical clustering (Classification hiérarchique)Le regroupement ou Clustering est la technique la plus utilisée pour résoudre les problèmes d'apprentissage non supervisé. La mise en cluster consiste à séparer ou à diviser un ensemble de données en un certain nombre de groupes, de sorte que les ensembles de données appartenant aux mêmes groupes se ressemblent davantage que ceux d’autres groupes. En termes simples, l’objectif est de séparer les groupes ayant des traits similaires et de les assigner en grappes.Voyons cela avec un exemple. Supposons que vous soyez le chef d’un magasin de location et que vous souhaitiez comprendre les préférences de vos clients pour développer votre activité. Vous pouvez regrouper tous vos clients en 10 groupes en fonction de leurs habitudes d’achat et utiliser une stratégie distincte pour les clients de chacun de ces 10 groupes. Et c’est ce que nous appelons le Clustering.Le clustering consiste à grouper des points de données en fonction de leurs similitudes, tandis que l’association consiste à découvrir des relations entre les attributs de ces points de données:Les techniques de clustering cherchent à décomposer un ensemble d'individus en plusieurs sous ensembles les plus homogènes possiblesOn ne connaît pas la classe des exemples (nombre, forme, taille)Les méthodes sont très nombreuses, typologies généralement employées pour les distinguer  Méthodes de partitionnement / Méthodes hiérarchiquesAvec recouvrement / sans recouvrementAutre : incrémental / non incrémentalD'éventuelles informations sur les classes ou d'autres informations sur les données n'ont pas d'influence sur la formation des clusters, seulement sur leur interprétation.L'un des algorithmes le plus connu et utilisé en clustering est la K-moyenne.Cet algorithme va mettre dans des “zones” (Cluster), les données qui se ressemblent. Les données se trouvant dans le même cluster sont similaires.L’approche de K-Means consiste à affecter aléatoirement des centres de clusters (appelés centroids), et ensuite assigner chaque point de nos données au centroid qui lui est le plus proche. Cela s’effectue jusqu’à assigner toutes les données à un cluster. Portail de l’informatique   Portail des probabilités et de la statistique   Portail des données
informatique;L'apprentissage supervisé (supervised learning en anglais) est une tâche d'apprentissage automatique consistant à apprendre une fonction de prédiction à partir d'exemples annotés, au contraire de l'apprentissage non supervisé. On distingue les problèmes de régression des problèmes de classement. Ainsi, on considère que les problèmes de prédiction d'une variable quantitative sont des problèmes de régression tandis que les problèmes de prédiction d'une variable qualitative sont des problèmes de classification.Les exemples annotés constituent une base d'apprentissage, et la fonction de prédiction apprise peut aussi être appelée « hypothèse » ou « modèle ». On suppose cette base d'apprentissage représentative d'une population d'échantillons plus large et le but des méthodes d'apprentissage supervisé est de bien généraliser, c'est-à-dire d'apprendre une fonction qui fasse des prédictions correctes sur des données non présentes dans l'ensemble d'apprentissage.Soit                     (        Ω        ,                              A                          ,                  P                )              {\displaystyle (\Omega ,{\mathcal {A}},\mathbb {P} )}  , un espace probabilisé.Soit                     (                              X                          ,                                            F                                            X                          )        ,        (                              Y                          ,                                            F                                            Y                          )              {\displaystyle ({\mathcal {X}},{\mathcal {F}}_{X}),({\mathcal {Y}},{\mathcal {F}}_{Y})}   deux espaces mesurables. On peut définir une base de données d'apprentissage (ou ensemble d'apprentissage) comme un ensemble de couples entrée-sortie                     (                  x                      n                          ,                  y                      n                                    )                      1            ≤            n            ≤            N                                {\displaystyle (x_{n},y_{n})_{1\leq n\leq N}}   où chaque                               x                      n                          ∈                              X                                {\displaystyle x_{n}\in {\mathcal {X}}}   et                               y                      n                          ∈                              Y                                {\displaystyle y_{n}\in {\mathcal {Y}}}   sont des réalisations respectives des variables aléatoires                               X                      n                                {\displaystyle X_{n}}   et                               Y                      n                                {\displaystyle Y_{n}}  . Les couples de la suite                     (        (                  X                      n                          ,                  Y                      n                          )                  )                      n            ≤            N                                {\displaystyle ((X_{n},Y_{n}))_{n\leq N}}   sont indépendants et identiquement distribués suivant la loi d'un couple                     (        X        ,        Y        )              {\displaystyle (X,Y)}   à valeurs dans                     (                              X                          ×                              Y                          ,                                            F                                            X                          ⊗                                            F                                            Y                          )              {\displaystyle ({\mathcal {X}}\times {\mathcal {Y}},{\mathcal {F}}_{X}\otimes {\mathcal {F}}_{Y})}  . On rappelle que cette loi est caractérisée par une mesure de probabilité                                           P                                (            X            ,            Y            )                                {\displaystyle \mathbb {P} _{(X,Y)}}   définie pour tout évènement                     A        ∈                                            F                                            X                          ⊗                                            F                                            Y                                {\displaystyle A\in {\mathcal {F}}_{X}\otimes {\mathcal {F}}_{Y}}   par                                           P                                (            X            ,            Y            )                          (        A        )        =                  P                [        (        X        ,        Y                  )                      −            1                          (        A        )        ]              {\displaystyle \mathbb {P} _{(X,Y)}(A)=\mathbb {P} [(X,Y)^{-1}(A)]}  Par exemple                               X                      n                                {\displaystyle X_{n}}   suit une loi uniforme et                               Y                      n                          =        f        (                  X                      n                          )        +                  ϵ                      n                                {\displaystyle Y_{n}=f(X_{n})+\epsilon _{n}}   où                               ϵ                      n                                {\displaystyle \epsilon _{n}}   est un bruit centré. Dans ce cas, la méthode d'apprentissage supervisé utilise cette base d'apprentissage pour déterminer une estimation de f notée g et appelée indistinctement fonction de prédiction, hypothèse ou modèle qui à une nouvelle entrée x associe une sortie g(x). Le but d'un algorithme d'apprentissage supervisé est donc de généraliser pour des entrées inconnues ce qu'il a pu « apprendre » grâce aux données déjà annotées par des experts, ceci de façon « raisonnable ». On dit que la fonction de prédiction apprise doit avoir de bonnes garanties en généralisation.Plus généralement, l'objectif de l'apprentissage supervisé est d'apprendre une fonction                     f              {\displaystyle f}   qui « minimise l'écart entre les variables aléatoires                     f        (        X        )              {\displaystyle f(X)}   et                     Y              {\displaystyle Y}   ». Pour définir cet écart, nous introduisons une fonction de perte                     L        :                              Y                          ×                              Y                          →                              R                                +                                {\displaystyle L:{\mathcal {Y}}\times {\mathcal {Y}}\rightarrow \mathbb {R} _{+}}   qui quantifie la distance entre une prédiction du modèle                     f        (        x        )              {\displaystyle f(x)}   et une sortie attendue                     y              {\displaystyle y}  . À partir de cette fonction, nous pouvons définir le risque statistique d'une modèle                     f              {\displaystyle f}  . Il est noté                     R              {\displaystyle R}   et est défini par :En pratique, on n'a jamais accès directement à                                           P                                (            X            ,            Y            )                                {\displaystyle \mathbb {P} _{(X,Y)}}  , en revanche il est possible de l'estimer à partir du jeu de données en utilisant la mesure empirique                                           P                                (            X            ,            Y            )                                N                                {\displaystyle \mathbb {P} _{(X,Y)}^{N}}   définie pour tout                     A        ∈                                            F                                            X                          ⊗                                            F                                            Y                                {\displaystyle A\in {\mathcal {F}}_{X}\otimes {\mathcal {F}}_{Y}}   par                                           P                                (            X            ,            Y            )                                N                          (        A        )        =                                            1              N                                                ∑                      n            =            1                                N                                    δ                      (                          X                              n                                      ,                          Y                              n                                      )                          (        A        )              {\displaystyle \mathbb {P} _{(X,Y)}^{N}(A)={\dfrac {1}{N}}\sum _{n=1}^{N}\delta _{(X_{n},Y_{n})}(A)}  .Dès lors, un algorithme d'apprentissage supervisé mettra en œuvre des algorithmes d'optimisation afin de trouver une fonction                     f              {\displaystyle f}   qui minimise le risque empirique                               R                      N                          (        f        )        =                                            1              N                                                ∑                      n            =            1                                N                          L        (                  Y                      n                          ,        f        (                  X                      n                          )        )              {\displaystyle R_{N}(f)={\dfrac {1}{N}}\sum _{n=1}^{N}L(Y_{n},f(X_{n}))}  . Il faut noter que                               R                      N                                {\displaystyle R_{N}}   n'est rien d'autre que la moyenne des écart (au sens de                     L              {\displaystyle L}  ) entre les prédictions du modèle et les sorties attendues.On distingue trois types de problèmes solubles avec une méthode d'apprentissage automatique supervisée :                                          Y                          ⊂                  R                      {\displaystyle {\mathcal {Y}}\subset \mathbb {R} }   : lorsque la sortie que l'on cherche à estimer est une valeur dans un ensemble continu de réels, on parle d'un problème de régression. La fonction de prédiction est alors appelée un régresseur.                                          Y                          =        {        1        ,        …        ,        I        }              {\displaystyle {\mathcal {Y}}=\{1,\ldots ,I\}}   : lorsque l'ensemble des valeurs de sortie est fini, on parle d'un problème de classification, qui revient à attribuer une étiquette à chaque entrée. La fonction de prédiction est alors appelée un classifieur.Lorsque                                           Y                                {\displaystyle {\mathcal {Y}}}   est un ensemble de données structurées, on parle d'un problème de prédiction structurée, qui revient à attribuer une sortie complexe à chaque entrée. Par exemple, en bio-informatique le problème de prédiction de réseaux d’interactions entre gènes peut être considéré comme un problème de prédiction structurée dans laquelle l'ensemble possible des sorties structurées est l'ensemble de tous les graphes modélisant les interactions possibles.Une bonne estimation de                     f              {\displaystyle f}   vérifierait                     f        (        X        )        =                  E                (        Y                  |                X        )              {\displaystyle f(X)=\mathbb {E} (Y|X)}  . On estimerait donc                     Y              {\displaystyle Y}   par son espérance conditionnelle par rapport à                     X              {\displaystyle X}  . Le théorème suivant montre l'intérêt d'utiliser la fonction de perte quadratique dans le cas d'une régression.BoostingMachine à vecteurs de supportMélanges de loisRéseau de neurones artificielsMéthode des k plus proches voisinsArbre de décisionClassification naïve bayésienneInférence grammaticaleEspace de versionsVision par ordinateurReconnaissance de formesReconnaissance de l'écriture manuscriteReconnaissance vocaleTraitement automatique de la langueBio-informatiqueReconnaissance optique de caractèresVincent Barra, Antoine Cornuéjols, Laurent Miclet, Apprentissage Artificiel : Concepts et algorithmes, Eyrolles, 2021 (ISBN 978-2-416-001-04-8) [détail des éditions](en) Tom M. Mitchell, Machine Learning, 1997 [détail des éditions](en) Christopher M. Bishop, Pattern Recognition And Machine Learning, Springer, 2006 (ISBN 0-387-31073-8) [détail des éditions] Portail des probabilités et de la statistique   Portail de l’informatique   Portail des données
informatique;"L'attaque par force brute est une méthode utilisée en cryptanalyse pour trouver un mot de passe ou une clé. Il s'agit de tester, une à une, toutes les combinaisons possibles. Cette méthode est en général considérée comme la plus simple concevable. Elle permet de casser tout mot de passe  en un temps fini indépendamment de la protection utilisée, mais le temps augmente avec la longueur du mot de passe. En théorie la complexité d'une attaque par force brute est une fonction exponentielle de la longueur du mot de passe, la rendant en principe impossible pour des mots de passe de longueur moyenne. En pratique des optimisations heuristiques peuvent donner des résultats dans des délais beaucoup plus courts.Cette méthode est souvent combinée avec l'attaque par dictionnaire et par table arc-en-ciel pour trouver le secret plus rapidement.Si le mot de passe contient N caractères, indépendants (la présence d'un caractère ne va pas influencer un autre) et uniformément distribués (aucun caractère n'est privilégié), le nombre maximum d'essais nécessaires se monte alors à : 26N si le mot de passe ne contient que des lettres de l'alphabet totalement en minuscules ou en majuscules ;36N si le mot de passe mélange des chiffres et des lettres de l'alphabet totalement en minuscules ou en majuscules ;62N si le mot de passe mélange les majuscules et les minuscules ainsi que les chiffres.Il suffit en fait d'élever la taille de « l'alphabet » utilisé à la puissance N. Il s'agit ici d'une borne supérieure et en moyenne, il faut deux fois moins d'essais pour trouver le mot de passe (si celui-ci est aléatoire). En réalité, bien peu de mots de passe sont totalement aléatoires et le nombre d'essais est bien inférieur aux limites données ci-dessus (grâce à la possibilité d'une attaque par dictionnaire). Le tableau ci-dessous donne le nombre maximum d'essais nécessaires pour trouver des mots de passe de longueurs variables.Un ordinateur personnel est capable de tester plusieurs centaines de milliers voire quelques millions de mots de passe par seconde. Cela dépend de l'algorithme utilisé pour la protection mais on voit qu'un mot de passe de seulement 6 caractères, eux-mêmes provenant d'un ensemble de 62 symboles (minuscules ou majuscules accompagnés de chiffres), ne tiendrait pas très longtemps face à une telle attaque. Dans le cas des clés utilisées pour le chiffrement, la longueur est souvent donnée en bits. Dans ce cas, le nombre de possibilités (si la clé est aléatoire) à explorer est de l'ordre de 2N où N est la longueur de la clé en bits. Une clé de 128 bits représente déjà une limite impossible à atteindre avec la technologie actuelle et l'attaquant doit envisager d'autres solutions cryptanalytiques si celles-ci existent. Il faut cependant prendre en compte que la puissance du matériel informatique évolue sans-cesse (voir Loi de Moore) et un message indéchiffrable à un moment donné peut l'être par le même type d'attaque une dizaine d'années plus tard.Le principe général de l'attaque par force brute reste toujours de tester l'ensemble des mots de passe possibles, cependant l'ordre de test peut être optimisé afin d'obtenir de meilleurs rendements qu'une attaque par ordre alphabétique.ordre aléatoire : certains systèmes de mots de passe sont capables de reconnaître les tentatives d'attaque par force brute suivant les algorithmes les plus courants et de les bloquer, l'introduction d'éléments aléatoires peut masquer l'attaque.Plutôt que d'utiliser des chaînes de caractère aléatoires comme mot de passe, les utilisateurs ont tendance à utiliser des mots courant plus faciles à retenir. Or, s'il existe un nombre important de combinaisons aléatoires pour une chaîne de longueur donnée, le nombre de mots présents dans un ou plusieurs langages est beaucoup plus faible (à titre d'exemple l'Académie française estime que les dictionnaires encyclopédiques comptent environ 200 000 mots). Connaissant ce phénomène culturel, il peut être judicieux de tester ces mots courants et leurs dérivés (y compris argot, dialectes, mots avec fautes d'orthographe courante…) en priorité.De manière générale, pour développer le principe de l'attaque par dictionnaire, l'attaquant peut tirer parti du fait qu'en l'état actuel des connaissances, il n'existe pas de générateur  aléatoire parfait et que de ce fait le générateur étant toujours pseudo-aléatoire (qu'il soit un processus informatique ou la saisie par une personne) il est toujours possible en observant de grands échantillons de mots de passe produits par un générateur donné d'identifier des tendances permettant un résultat généralement meilleur qu'une recherche alphabétique ou aléatoire.Outre ces attaques théoriques, il existe des attaques tirant parti de l'implémentation des systèmes pour augmenter le rendement de l'attaque par force brute, notamment la transmission des mots de passe sous la forme de hash autorise l'usage de tables arc en ciel, du fait des collisions liées aux fonctions de hachage il est possible de casser une protection sans connaitre le mot de passe réel.Outre ces améliorations algorithmiques, l'attaque peut également être accélérée en augmentant la puissance de calcul matérielle consacrée à celle-ci par exemple en utilisant des superordinateurs ou de l'informatique distribuée (parfois sous la forme d'un botnet).La première défense consiste à renforcer le mot de passe en évitant les écueils qu'exploitent les attaques par force brute optimisée.Renforcer la force brute du mot de passe consiste à :allonger le mot de passe ou la clé si cela est possible ;utiliser la plus grande gamme de symboles possibles (minuscules, majuscules, ponctuations, chiffres) ; l'introduction de caractères nationaux (Â, ÿ…) rend plus difficile le travail des pirates (mais parfois aussi l'entrée de son mot de passe quand on se trouve à l'étranger).Éviter toutes les formes ou habitudes (patterns) identifiées ou identifiables par les attaquantséviter l'emploi de mot du langage commun pour empêcher les attaques par dictionnaireéviter les répétitions de formes de mot de passe (par exemple les mots de passe constitués de caractères en majuscules, caractères en minuscule puis terminés par des symboles sont une famille identifiée et testée en priorité par les logiciels d'attaque par force brute)en poussant le raisonnement précédent jusqu'au bout il apparaît que la seule méthode de choix de mot de passe qui échappe à toute optimisation est la génération aléatoire (ou en pratique une génération pseudo-aléatoire de qualité suffisante).Les mots de passes sont censés rendre l'attaque virtuellement impossible par des temps de cassage extrêmement longs, or les temps de cassage sont souvent une fonction linéaire de la capacité de la ou des machine(s) attaquante. Certains attaquants (services secrets, laboratoires…) peuvent disposer de machines très puissantes et les capacités des machines disponibles au grand public sont en constante progression que ce soit par leur puissance brute ou par l'introduction de nouveaux paradigmes (calcul parallèle par exemple). Il en résulte qu'une protection qui paraissait suffisante à un instant donné peut se trouver dépassée par les capacités disponibles à l'attaquant. La principale méthode pour neutraliser la puissance de calcul d'un attaquant consiste à limiter les tentatives possibles dans le temps. La méthode la plus restrictive et la plus sûre (qu'on retrouve sur les cartes bancaires en France) consiste à n'autoriser qu'un nombre limité d'erreurs avant verrouillage du système. Des méthodes moins contraignantes peuvent être de limiter le nombre de tentatives par unité de temps. Ces méthodes présentent cependant des contraintes d'exploitation et peuvent être détournées par un attaquant pour créer des attaques par déni de service. Deux brevets principaux existent à ce sujet :Un des Laboratoires Bell consistant à doubler le temps d'attente après chaque essai infructueux, pour le faire redescendre ensuite en vol plané après un certain temps sans attaques ;Un de la compagnie IBM consistant à répondre « Mot de passe invalide » après N essais infructueux en un temps T, y compris si le mot de passe est valide : le pirate a alors toutes les chances de rayer de façon erronée le mot de passe valide en le considérant invalide. De plus, cette méthode empêche toute attaque visant à un déni de service pour l'utilisateur.Une variante de l'attaque de la limitation temporelle du nombre de tentatives consiste à augmenter les ressources nécessaires pour réaliser chaque tentative.Une première méthode consiste à utiliser une fonction de hachage cryptographique de complexité relativement élevée. Ainsi le coût de chaque tentative se trouve augmenté.Comparé à la simple temporisation, l'intérêt de l'augmentation du coût du hachage est qu'il ne peut être contourné (l'opération de hachage est strictement nécessaire pour effectuer une tentative). Les inconvénients est que le temps de l'opération baisse avec la puissance de la machine attaquante, là où la temporisation reste constante et peut être choisie de façon arbitraire, et que l'augmentation de coût s'applique également aux opérations légitimes.Un autre exemple de système de limitation des tentatives de connexion est l'utilisation de CAPTCHA. Ces dispositifs peuvent poser des difficultés significatives pour une machine tout en restant acceptables pour un utilisateur humain.Mais la protection contre les attaques par force brute est souvent apportée par des solutions pragmatiques, en adéquation avec les besoins propres à l’utilisateur, comme la restriction d’accès à une, plusieurs, ou à une plage entière d’adresses IP, ce qui correspond à la grande majorité des cas et se présente comme une alternative fiable à la limitation de durée de validité des mots de passe.Une solution peut consister à limiter la durée de validité des mots de passe à une durée inférieure à celle estimée pour leur cassage en les renouvelant à intervalles réguliers. Ceci peut passer soit par une politique de sécurité informatique appliquée avec rigueur pour des périodes de renouvellement jusqu'à quelques jours ou par des dispositifs physiques token pour des fréquences de renouvellement très élevées. Les systèmes de mots de passe comme celui d'Unix utilisent une version modifiée du chiffrement DES. Chaque mot de passe est accompagné d'une composante aléatoire appelée sel dont le but est de modifier la structure interne de DES et éviter ainsi une recherche exhaustive en utilisant du matériel spécialement conçu pour DES. Ce système peut cependant créer une faille de sécurité en facilitant les attaques par déni de service: le temps d'attente peut être utilisé pour gêner la connexion d'utilisateurs légitimes.En théorie et avec suffisamment de temps, l'attaquant peut toujours trouver le mot de passe, mais lorsque ce temps dépasse la décennie, il ne pourra pas en escompter un grand profit, et le mot de passe aura de toute façon changé. Il change même à chaque fois si l'on emploie le principe du masque jetable. Le problème est tout autre si l'attaquant récupère directement le fichier des hashs des mots de passe ; plus rien ne l'empêche alors de tester chez lui des mots de passe à la vitesse de son(es) ordinateur(s). C'est pourquoi dans tous les UNIX modernes ces hashs sont généralement situés dans le fichier /etc/shadow, lisible uniquement par l'utilisateur root. Une compromission de cet utilisateur permet par conséquent de récupérer ce fichier, et ainsi de lancer une attaque par force brute sur son contenu.Cassage de mot de passeAttaque par dictionnaireAuthentification forteRobustesse d'un mot de passe(fr) Programme qui génère des mots de passe, en évalue et en améliore la robustesse(fr) Calculer une bonne taille de clef (www.keylength.com) Portail de la cryptologie   Portail de la sécurité informatique"
informatique;"C++ est un langage de programmation compilé permettant la programmation sous de multiples paradigmes, dont la programmation procédurale, la programmation orientée objet et la programmation générique. Ses bonnes performances, et sa compatibilité avec le C en font un des langages de programmation les plus utilisés dans les applications où la performance est critique.Créé initialement par Bjarne Stroustrup dans les années 1980, le langage C++ est aujourd'hui normalisé par l'ISO. Sa première normalisation date de 1998 (ISO/CEI 14882:1998), ensuite amendée par l'erratum technique de 2003 (ISO/CEI 14882:2003). Une importante mise à jour a été ratifiée et publiée par l'ISO en septembre 2011 sous le nom de ISO/IEC 14882:2011, ou C++11. Depuis, des mises à jour sont publiées régulièrement : en 2014 (ISO/CEI 14882:2014, ou C++14), en 2017 (ISO/CEI 14882:2017, ou C++17) puis en 2020 (ISO/IEC 14882:2020, ou C++20).En langage C, ++ est l'opérateur d'incrémentation, c'est-à-dire l'augmentation de la valeur d'une variable de 1. C'est pourquoi C++ porte ce nom : cela signifie que C++ est un niveau au-dessus de C.Bjarne Stroustrup commence le développement de C with Classes (C avec classes) en 1979. Il travaille alors dans les laboratoires Bell où il est notamment collègue de l'inventeur du C Dennis Ritchie. L'idée de créer un nouveau langage venait de l'expérience en programmation de Stroustrup pour sa thèse de doctorat. Il s'agissait en l'occurrence d'améliorer le langage C. Stroustrup trouvait que Simula avait des fonctionnalités très utiles pour le développement de gros programmes mais qu'il était trop lent pour être utilisé en pratique (cela était dû à un problème d'implémentation du compilateur Simula), tandis que BCPL était rapide mais de trop bas niveau et non adapté au développement de gros logiciels. Quand Stroustrup commença à travailler aux laboratoires Bell, on lui demanda d'analyser le noyau UNIX en vue de faire du calcul distribué. Se rappelant sa thèse, Stroustrup commença à améliorer le langage C avec des fonctionnalités similaires à celle de Simula. C fut choisi parce qu'il est rapide, portable et d'usage général. En outre, il était une bonne base pour le principe original et fondateur de C++ : « vous ne payez pas pour ce que vous n'utilisez pas ». Dès le départ, le langage ajoutait à C la notion de classe (avec encapsulation des données), de classe dérivée, de vérification des types renforcés (typage fort), d'« inlining », et d'argument par défaut.Alors que Stroustrup développait C with classes, il écrivit CFront, un compilateur qui générait du code source C à partir de code source C with classes. La première commercialisation se fit en octobre 1985. En 1983 le nom « C++ » est inventé, et en 1984 le nom du langage passa de C with classes à celui de « C++ ». Parmi les nouvelles fonctionnalités qui furent ajoutées au langage, il y avait les fonctions virtuelles, la surcharge des opérateurs et des fonctions, les références, les constantes, le contrôle du typage amélioré et les commentaires en fin de ligne. En 1985 fut publiée la première édition de The C++ Programming Language, apportant ainsi une référence importante au langage qui n'avait pas encore de standard officiel. En 1989, c'est la sortie de la version 2.0 de C++. Parmi les nouvelles fonctionnalités, il y avait l'héritage multiple, les classes abstraites, les fonctions membres statiques, les fonctions membres constantes, et les membres protégés. En 1990, The Annotated C++ Reference Manual (« ARM ») fut publié apportant les bases du futur standard. Les ajouts de fonctionnalités tardifs qu'il comportait couvraient les templates, les exceptions, les espaces de noms, les nouvelles conversions et le type booléen.Pendant l'évolution du langage C++, la bibliothèque standard évoluait de concert. Le premier ajout à la bibliothèque standard du C++ concernait les flux d'entrées/sorties qui apportaient les fonctionnalités nécessaires au remplacement des fonctions C traditionnelles telles que printf et scanf. Ensuite, parmi les ajouts les plus importants, il y avait la Standard Template Library. Après des années de travail, un comité réunissant l'ANSI et l'ISO standardisa C++ en 1998 (ISO/CEI 14882:1998), l'année où le comité de standardisation se réunissait à Sophia Antipolis dans le sud de la France. Pendant quelques années après la sortie officielle du standard, le comité traita des problèmes remontés par les utilisateurs, et publia en 2003 une version corrigée du standard C++.Personne ne possède le langage C++. Il est libre de droits ; cependant, le document de standardisation n'est quant à lui pas disponible gratuitement.On peut considérer que C++ « est du C » avec un ajout de fonctionnalités. Cependant, plusieurs programmes syntaxiquement corrects en C ne le sont pas en C++, à commencer bien sûr par ceux qui font usage d'identificateurs correspondant à des mots-clefs en C++.Parmi les fonctionnalités ajoutées figurent :le typage des « prototypes » de fonctions (repris dans ANSI C89) ;La surcharge des fonctions ;les déclarations reconnues comme instructions (repris dans C99) ;les opérateurs new et delete pour la gestion d'allocation mémoire ;le type de données bool (booléen) ;les références & ;les variables et les fonctions membres const (repris partiellement par C à la fin des années 1980) ;les fonctions inline (repris dans C99) ;les paramètres par défaut dans les fonctions ;les référentiels lexicaux (espaces de noms) et l'opérateur de résolution de portée :: ;les classes, ainsi que tout ce qui y est lié : l'héritage, les fonctions membres, les fonctions membres virtuelles, les constructeurs et le destructeur ;la surcharge des opérateurs ;les templates ;la gestion d'exceptions ;l'identification de type pendant l'exécution (RTTI : run-time type information) ;le commentaire sur une ligne introduit par // (existant dans BCPL, repris dans C99) ;les références de rvalue && (C++11) ;la déduction de type à la compilation via auto (C++11) ;les expressions constantes constexpr (C++11);les fonctions lambda (C++11, étendu dans tous les standards publiés depuis) ;les boucles for basées sur une plage (C++11, étendu en C++20) ;les modules via import, export et module (C++20) ;les contraintes et concepts via concept et requires (C++20) ;les fonctions immédiates consteval (C++20) ;les coroutines (C++20) ;La compilation d'un programme en C++ effectue également un contrôle plus minutieux du typage.La bibliothèque standard du C++ englobe la Standard Template Library (STL) qui met à la disposition du programmeur des outils puissants comme des collections (conteneurs) et des itérateurs.À l'origine, la STL était une bibliothèque développée par Alexander Stepanov qui travaillait pour Hewlett-Packard. Dans la norme, celle-ci n'est pas appelée STL, car elle est considérée comme faisant partie de la bibliothèque standard de C++. Toutefois, beaucoup de personnes l'appellent encore de cette manière pour distinguer d'une part, les fonctions d'entrées/sorties comprises dans cette bibliothèque et, d'autre part, celles fournies par la bibliothèque C.Comme en C, l'utilisation d'une bibliothèque peut se faire par l'intermédiaire de la directive #include (suivie du nom du fichier d'en-tête), et certaines d'entre elles (cmath, thread, etc.) nécessitent d'être liées explicitement. Depuis C++20 le mot clé import peut servir à des fins similaires.Le langage C++ utilise les concepts de la programmation orientée objet et permet entre autres :la création de classes ;l'encapsulation ;des relations entre les classes :la composition de classes (composition dans un diagramme de classes),l'association de classes (en) (association dans un diagramme de classes),l'agrégation de classes (agrégation dans un diagramme de classes),la dépendance (dépendance dans un diagramme de classes),l'héritage simple et multiple (héritage dans un diagramme de classes) ;le polymorphisme ;l'abstraction ;la généricité ;la méta-programmation.L'encapsulation permet de faire abstraction du fonctionnement interne (c'est-à-dire la mise en œuvre) d'une classe et ainsi de ne se préoccuper que des services rendus par celle-ci. C++ met en œuvre l'encapsulation en permettant de déclarer les membres d'une classe avec le mot réservé public, private ou protected. Ainsi, lorsqu'un membre est déclaré :public, il sera accessible depuis n'importe quelle fonction ;private, il sera uniquement accessible d'une part, depuis les fonctions qui sont membres de la classe et, d'autre part, depuis les fonctions autorisées explicitement par la classe (par l'intermédiaire du mot réservé friend) ;protected, il aura les mêmes restrictions que s'il était déclaré private, mais il sera en revanche accessible par les classes filles.C++ n'impose pas l'encapsulation des membres dans leurs classes. On pourrait donc déclarer tous les membres publics, mais en perdant une partie des bénéfices apportés par la programmation orientée objet. Il est de bon usage de déclarer toutes les données privées, ou au moins protégées, et de rendre publiques les fonctions membres agissant sur ces données. Ceci permet de cacher les détails de la mise en œuvre de la classe.Voici l'exemple de Hello world donné dans The C++ Programming Language, Third Edition de Bjarne Stroustrup :Dans l'exemple ci-dessus, le code source std::cout << ""Hello, new world!\n"" envoie la chaîne de caractères ""Hello, new world!\n"" à l'objet global cout, défini dans l'espace de noms standard std, grâce à l'opérateur << de cout.En C++, le mot clef namespace permet de définir et de nommer des espaces de noms (namespaces), notion déjà présente en langage C ; en effet, le corps d'une routine, d'une structure de contrôle de flux d'exécution, d'une structure de données ou d'une section de code (délimitée par les accolades { et }) constitue un espace de noms. En C++, le corps d'une classe, à l'instar du corps d'une structure de données, constitue aussi un espace de noms.Dans différents espaces de noms, on peut ainsi définir des entités (routines, variables, etc.) ayant le même identificateur. L'ambiguïté est résolue en utilisant le nom de l'espace de nom devant l'opérateur de portée (::) pour indiquer l'espace de noms dans lequel on veut accéder. Notez que l'espace de noms global du programme n'a pas de nom. Pour accéder à une entité globale, cachée par une entité locale par exemple, on utilise l'opérateur de portée précédé d'aucun nom.Il est possible de spécifier un espace de noms précis à utiliser afin d'éviter d'avoir à recourir à l'opérateur de résolution de portée. Pour cela, le mot-clé using est utilisé avec cette syntaxe :Ainsi, pour utiliser la variable cout définie dans le namespace standard sans utiliser l'opérateur de résolution de portée, il est possible d'écrire using namespace std; ou using std::cout;. Cela est valable pour tous les espaces de noms. Cette instruction se place en général avant le début du code source proprement dit :Il est aussi possible, et conseillé, d'importer un symbole particulier, ou de placer cette instruction dans une fonction afin de limiter la portée :Le mot-clé using peut aussi être utilisé dans les classes. Si une classe B hérite d'une classe A, elle peut grâce à ce mot-clé passer des membres protected de A en public dans B, ou encore démasquer une fonction membre de A qui le serait par une fonction membre de B de même nom :Le programme ci-dessus affiche :Il est aussi possible de définir un nouveau nom pour un namespace :Il est d'usage de séparer prototype (déclaration) et implémentation (définition) de classe dans deux fichiers : la déclaration se fait dans un fichier d'en-tête (dont l'extension varie selon les préférences des développeurs : sans extension dans le standard, .h comme en C, .hh ou .hpp ou .hxx pour différencier le code source C++ du C) alors que la définition se fait dans un fichier source (d'extension également variable : .c comme en C, .cc ou .cpp ou .cxx pour différencier C++ du C).Exemple de la déclaration d'une classe comportant des attributs privés et des fonctions membres publiques :Le nom d'une fonction membre déclarée par une classe doit nécessairement être précédé du nom de la classe suivi de l'opérateur de résolution de portée ::.Exemple de définition des fonctions membres d'une classe (celle déclarée précédemment) :Les Modèles (ou templates) permettent d'écrire des variables, des fonctions et des classes en paramétrant le type de certains de leurs constituants (type des paramètres ou type de retour pour une fonction, type des éléments pour une classe collection par exemple). Les modèles permettent d'écrire du code générique, c'est-à-dire qui peut servir pour une famille de fonctions ou de classes qui ne diffèrent que par le type de leurs constituants.Les paramètres peuvent être de différentes sortes :types simples, tels que les classes ou les types élémentaires (int, double, etc.) ;tableaux de taille constante, dont la taille, déduite par le compilateur, peut être utilisée dans l'instanciation du modèle ;constantes scalaires, c'est-à-dire de type entier (int, char, bool), mais pas flottant (float, double) car leur représentation binaire ne fait pas partie de la norme du langage (jusqu'en C++20 où ils sont autorisés) ;templates, dont la définition doit être passée en paramètre, ce qui permet notamment de s'appuyer sur la définition abstraite, par exemple, d'une collection ;pointeurs ou références, à condition que leur valeur soit définie à l'édition de liens ;fonction membre d'une classe, dont la signature et la classe doivent être aussi passées en paramètres ;attribut d'une classe, dont le type et la classe doivent être aussi passés en paramètres.En programmation, il faut parfois écrire de nombreuses versions d'une même fonction ou classe suivant les types de données manipulées. Par exemple, un tableau de int ou un tableau de double sont très semblables, et les fonctions de tri ou de recherche dans ces tableaux sont identiques, la seule différence étant le type des données manipulées. En résumé, l'utilisation des templates permet de « paramétrer » le type des données manipulées.Les avantages des modèles sont :des écritures uniques pour les fonctions et les classes ;moins d'erreurs dues à la réécriture ;Dans la bibliothèque standard C++, on trouve de nombreux templates. On citera à titre d'exemple, les entrées/sorties, les chaînes de caractères ou les conteneurs. Les classes string, istream, ostream et iostream sont toutes des instanciations de type char.Les fonctions de recherche et de tri sont aussi des templates écrits et utilisables avec de nombreux types.Dans la ligne float f = max<float>(1, 2.2f);, on doit explicitement donner le type float pour le type paramétré T car le compilateur ne déduit pas le type de T lorsqu'on passe en même temps un int (1) et un float (2.2f).Un template donné peut avoir plusieurs instanciations possibles selon les types donnés en paramètres. Si un seul paramètre est spécialisé, on parle de spécialisation partielle. Ceci permet par exemple :de choisir un type de calcul selon qu'un type est un entier, un flottant, une chaîne de caractères, etc. Spécialisons l'exemple précédent pour le cas des pointeurs de chaînes de caractères :d'effectuer au moment de la compilation des calculs arithmétiques, si et seulement si tous les arguments sont connus à ce moment. Un exemple classique est le calcul de la fonction factorielle :À partir de C++14 pour arriver aux mêmes fins nous pourrions aussi utiliser les variables templates :Ainsi nous pouvons écrire factorielle<8>; à la place de Factorielle<8>::value;.Le mécanisme décrit par l'abréviation SFINAE (Substitution Failure Is Not an Error) permet de surcharger un template par plusieurs classes (ou fonctions), même si certaines spécialisations, par exemple, ne peuvent pas être utilisées pour tous les paramètres de templates. Le nom décrit précisément le fonctionnement du mécanisme, littéralement l’acronyme de « Un échec de substitution n'est pas une erreur », le compilateur, lors de la substitution, ignore alors les instanciations inapplicables, au lieu d'émettre une erreur de compilation. Par exemple :Ici f est définie deux fois, le type de retour est conditionné par le type donné en paramètre, il est du type du retour de f.foo() dans le premier cas et de celui de f.bar() dans le deuxième cas. Ainsi, si on appelle f avec un objet de la classe A, seule la première fonction fonctionne puisque la classe A n'a pas de fonction membre bar() et donc la substitution est possible avec cette première version mais pas pour la deuxième. Ainsi, f(a) appelle la première version de f, f(b) appelle la deuxième avec le même raisonnement, mais cette fois pour la fonction membre bar().Si lors d'un développement à venir, un développeur venait à écrire une nouvelle classe ayant une fonction membre publique foo ou bien (ou exclusif) bar, il pourrait également utiliser f avec.Le polymorphisme d'inclusion est mis en œuvre à l'aide du mécanisme des fonctions membres virtuelles en C++. Une fonction membre est rendue virtuelle par le placement du mot-clé virtual devant la déclaration de la fonction membre dans la classe. Lorsqu'une fonction membre virtuelle est appelée, l'implémentation de la fonction membre exécutée est choisie en fonction du type réel de l'objet. L'appel n'est donc résolu qu'à l'exécution, le type de l'objet ne pouvant pas a priori être connu à la compilation.Le mot-clé virtual indique au compilateur que la fonction membre déclarée virtuelle est susceptible d'être redéfinie dans une classe dérivée. Il suffit alors de dériver une classe et de définir une nouvelle fonction membre de même signature (même nom, paramètres compatibles — voir la notion de covariance). Ainsi l'appel de cette fonction membre sur un objet accédé en tant qu'objet de la classe de base mais appartenant en réalité à la classe dérivée donnera lieu à l'appel de la fonction membre définie dans la classe dérivée.En particulier, il est obligatoire d'utiliser le mot-clé virtual devant la déclaration du destructeur de la classe de base lorsque le programme souhaite pouvoir détruire un objet via un pointeur d'instance de la classe de base au lieu d'un pointeur d'instance de la classe dérivée.Ce type de polymorphisme (le polymorphisme d'inclusion) est dit dynamique. Le mécanisme de la surcharge de fonction qui est un polymorphisme ad hoc est de type statique. Dans les deux cas il faut appliquer une logique (par exemple : le nombre et le type des paramètres) pour résoudre l'appel. Dans le cas de la surcharge de fonction, la logique est entièrement calculée à la compilation. Ce calcul permet des optimisations rendant le polymorphisme statique plus rapide que sa version dynamique. La liaison dynamique de fonctions membres issues du mécanisme des fonctions membres virtuelles induit souvent une table cachée de résolution des appels, la table virtuelle. Cette table virtuelle augmente le temps nécessaire à l'appel de fonction membre à l'exécution par l'ajout d'une indirection supplémentaire.Le choix entre liaison dynamique et surcharge (polymorphisme dynamique et statique) est typiquement un problème de calculabilité des appels, ayant souvent pour conséquence finale un choix entre expressivité et performance.Malgré ce dynamisme, il est à noter que le compilateur est capable de « dévirtualiser » les appels de fonctions membres qui peuvent être résolus au moment de la compilation. Dans gcc par exemple, l'option -fdevirtualize lors de la compilation permet cette optimisation, s'il est possible de faire une telle résolution.Un programme C++ peut être produit avec des outils qui automatisent le processus de construction. Les plus utilisés sont :make ;Ant (génération portable en XML) ;SCons (génération portable en Python) ;CMake (génération de Makefile portable) ;Bazel.Anjuta DevStudio ;C++ Builder ;CLion (en) ;Code::Blocks (open-source) ;Dev-C++ et son extension RAD WxDev-C++ ;Eclipse avec le plugin CDT (open-source) ;Emacs (libre) ;KDevelop ;NetBeans (open-source) ;QtCreator (open-source) ;Sun Studio ;Vim ;Microsoft Visual C++ (a été intégré au framework Visual Studio) ;Xcode.GCC pour GNU Compiler Collection (libre, multilangage et multiplateforme : UNIX, Windows, DOS, etc.) ;Clang ;Microsoft Visual C++ (Windows) ;Borland C++ Builder (Windows) ;Intel C++ Compiler (Windows, Linux, MacOS) ;Open64 (en) compilateur opensource d'AMD (Linux) ;Digital Mars C/C++ compiler (Windows) ;Open Watcom ;Boost ;Qt ;Gtkmm ;wxWidgets ;SFML ;OpenCV ;SDLmm, surcouche C++ à la SDL ;LLVM.etc. Ouvrages en langue anglaise [Deitel et Deitel 2011] (en) P. Deitel et H. Deitel, C++ : How to Program, 20 Hall, 2011, 8e éd., 1104 p. (ISBN 978-0-13-266236-9).[Dawson 2010] (en) M. Dawson, Beginning C++ Through Game Programming, Course Technology PTR, 2010, 3e éd., 432 p. (ISBN 978-1-4354-5742-3).[Gregoire, Solter et Kleper 2011] (en) Marc Gregoire, Nicolas A. Solter et Scott J. Kleper, Professional C++, John Wiley, octobre 2011, 1104 p. (ISBN 978-0-470-93244-5, présentation en ligne).[Josuttis 2011] (en) Nicolaï Josuttis, The C++ Standard Library, A Tutorial and Reference, Addison-Wesley, 2011, 2e éd., 1099 p. (ISBN 978-0-321-62321-8, présentation en ligne).[Koenig et Moo 2000] (en) A. Koenig et B. Moo, Accelerated C++ : Practical Programming by Example, Addison-Wesley, 2000, 1re éd., 352 p. (ISBN 978-0-201-70353-5).[Lippman, Lajoie et Moo 2012] (en) Stanley B. Lippman, Josée Lajoie et Barbara E. Moo, C++ Primer : 5th Edition, août 2012, 5e éd., 1399 p. (ISBN 978-0-321-71411-4).[Lischner 2003] (en) R. Lischner, C++ in a nutshell, O'Reilly Media, 2003, 1re éd., 704 p. (ISBN 978-0-596-00298-5).[Meyers 2005] (en) S. Meyers, Effective C++ : 55 Specific Ways to Improve Your Programs and Designs, Addison-Wesley Professional, 2005, 3e éd., 320 p. (ISBN 978-0-321-33487-9, présentation en ligne).[Oualline 2003] (en) S. Oualline, Practical C++ programming, O'Reilly Media, 2003, 2e éd., 600 p. (ISBN 978-0-596-00419-4, présentation en ligne).[Lafore 2001] (en) R. Lafore, Object-oriented programming in C++, Sams, 2001, 4e éd., 1040 p. (ISBN 978-0-672-32308-9).[Prata 2011] (en) S. Prata, C++ Primer Plus (Developer's Library), Addison-Wesley Professional, 2011, 6e éd., 1200 p. (ISBN 978-0-321-77640-2, présentation en ligne).[Stroustrup 2009] (en) Bjarne Stroustrup, Programming : Principles and Practice using C++, Addison-Wesley, 2009, 1236 p. (ISBN 978-0-321-54372-1).[Stroustrup 2013] (en) Bjarne Stroustrup, The C++ Programming Language : 4th Edition, Addison-Wesley Professional, 2013, 4e éd., 1368 p. (ISBN 978-0-321-56384-2).[Stroustrup 1994] (en) Bjarne Stroustrup, The Design and Evolution of C++, Addison-Wesley professional, 1994, 1re éd., 480 p. (ISBN 978-0-201-54330-8).[Sutter 1999] (en) H. Sutter, Exceptional C++ : 47 Engineering Puzzles, Programming Problems, and Solutions, Addison-Wesley Professional, 1999, 240 p. (ISBN 978-0-201-61562-3, présentation en ligne).[Vandevoorde et Josuttis 2002] (en) David Vandevoorde et Nicolaï Josuttis, C++ Templates : the Complete Guide, Addison-Weslay, 2002, 528 p. (ISBN 978-0-201-73484-3).[Vandevoorde 1998] (en) David Vandevoorde, C++ Solutions : Companion to the C++ Programming Language, Addison-Wesley, 1998, 3e éd., 292 p. (ISBN 978-0-201-30965-2). Ouvrages en langue française [Benharrats et Vittupier 2021] Mehdi Benharrat et Benoît Vittupier, Le guide du C++ moderne : de débutant à développeur, D-Booker, 2021, 1re éd., 708 p. (ISBN 978-2-8227-0881-4, présentation en ligne).[Chappelier et Seydoux 2005] J-C. Chappelier et F. Seydoux, C++ par la pratique : Recueil d'exercices corrigés et aide-mémoire, PPUR, 2005, 2e éd., 412 p. (ISBN 978-2-88074-732-9, présentation en ligne).[Deitel et Deitel 2004] P. Deitel et H. Deitel, Comment programmer en C++, Reynald Goulet, 2004, 1178 p. (ISBN 978-2-89377-290-5).[Delannoy 2001] Claude Delannoy, Programmer en langage C++, Paris, Eyrolles, 2011, 8e éd., 822 p. (ISBN 978-2-212-12976-2, présentation en ligne).[Delannoy 2007] Claude Delannoy, Exercices en langage C++, Paris, Eyrolles, 2007, 3e éd., 336 p. (ISBN 978-2-212-12201-5, présentation en ligne).[Géron et Tawbi 2003] Aurélien Géron et Fatmé Tawbi (préf. Gilles Clavel), Pour mieux développer avec C++ : Design patterns, STL, RTTI et smart pointers, Paris, Dunod, 2003, 188 p. (ISBN 978-2-10-007348-1).[Guidet 2008] Alexandre Guidet, Programmation objet en langage C++, Paris, Ellipses, coll. « Cours et exercices. », 2008, 364 p. (ISBN 978-2-7298-3693-1, OCLC 221607125, BNF 41206426).[Hubbard 2002] J. R. Hubbard (trad. Virginie Maréchal), C++ [« Schaum's easy outline of programming with C++ »], Paris, EdiScience, coll. « Mini Schaum's », 2002, 192 p. (ISBN 978-2-10-006510-3).[Liberty et Jones 2005] Jesse Liberty et Bradley Jones (trad. Nathalie Le Guillou de Penanros), Le langage C++ [« Teach yourself C++ in 21 days »], Paris, CampusPress, 2005, 859 p. (ISBN 978-2-7440-1928-9).[Stephens, Diggins, Turkanis et al. 2006] D. Ryan Stephens, Christopher Diggins, Jonathan Turkanis et J. Cogswell (trad. Yves Baily & Dalil Djidel), C++ en action [« C++ Cookbook - Solutions and Examples for C++ Programmers »], Paris, O'Reilly, 2006, 555 p. (ISBN 978-2-84177-407-4, OCLC 717532188, BNF 40170870).[Stroustrup 2012] Bjarne Stroustrup (trad. Marie-Cécile Baland, Emmanuelle Burr, Christine Eberhardt), Programmation : principes et pratique avec C++ : Avec plus de 1000 exercices. [« Programming : principles and practice using C++ »], Paris, Pearson education, 2012, 944 p. (ISBN 978-2-7440-7718-0).[Stroustrup 2003] Bjarne Stroustrup (trad. Christine Eberhardt), Le langage C++ [« The C++ programming language »], Paris, Pearson education, 2003, 1098 p. (ISBN 978-2-7440-7003-7 et 2-744-07003-3).[Sutter et Alexandrescu 2005] Herb Sutter et Andrei Alexandrescu, Standards de programmation C [« C++ Coding Standards: 101 Rules, Guidelines, and Best Practices »], Paris, Pearson Education France, coll. « C++ », 2005, 243 p. (ISBN 978-2-7440-7144-7 et 2-744-07144-7).Langage C(en) Le Comité du Standard C++(fr) Documentation du C++ (wiki sous double licence CC-BY-SA et GFDL)(en) Documentation du C++ (le même wiki mais en anglais et plus complet)(en) Documentation du C++ (contenu non libre, édité par The C++ ressources network) Portail de la programmation informatique   Portail de l’informatique"
informatique;"En informatique, le code source est un texte qui présente les instructions composant un programme sous une forme lisible, telles qu'elles ont été écrites dans un langage de programmation. Le code source se matérialise généralement sous la forme d'un ensemble de fichiers texte.Le code source est souvent traduit — par un assembleur ou un compilateur — en code binaire composé d'instructions exécutables par le processeur. Il peut sinon être directement interprété à l'exécution du programme. Dans ce deuxième cas, il est parfois traduit au préalable en un code intermédiaire dont l'interprétation est plus rapide.L'expression est une traduction de l'anglais source code. Les expressions omettant le terme de code sont communes : les sources, le source.Dans les tout premiers temps de l'informatique, les programmes étaient entrés dans la mémoire de l'ordinateur par l'intermédiaire des interrupteurs du pupitre de commande, sous forme du codage binaire des instructions machines. Ce qui ne convenait qu'à de tout petits programmes. Ils ont ensuite été chargés depuis des bandes perforées, puis des cartes perforées.Très rapidement, les programmes ont été rédigés dans un langage symbolique, langage d'assemblage ou langage évolué comme Fortran, Cobol, puis traduit automatiquement par un programme (assembleur, compilateur).Avec l'apparition des disques magnétiques et des consoles interactives, des éditeurs de lignes puis des éditeurs de textes ont été utilisés pour taper et modifier le code source.Les possibilités limitées des ordinateurs de l'époque nécessitaient souvent l'impression du code source sur papier continu (en) avec des bandes Carol.Aujourd'hui, il existe des environnements de développement, dits Environnement de développement intégré (IDE, Integrated Development Environment), qui intègrent notamment les tâches d'édition et de compilation.Un logiciel est une suite d'instructions données à une machine. Un processeur ne peut exécuter que des instructions représentées sous une forme binaire particulière. Sauf mécanismes expérimentaux, il n'est pas possible pour un être humain de saisir directement un code binaire dans la représentation qu'en attend le processeur : un être humain ne peut pas écrire directement les champs de bits aux adresses attendues. Il est obligé de passer par un code distinct appelé code source, et qui est par la suite traduit dans la représentation binaire attendue par la machine puis chargé et exécuté par la cible.Toutefois, l'écriture d'un code sous forme binaire, même dans un fichier séparé, pose de nombreux problèmes de compréhension aux êtres humains. C'est une représentation uniquement constituée d'une suite ininterrompue de 0 et de 1 qui est difficile à lire, à écrire et à maintenir sans assistance technique. La diversité des microprocesseurs et des composants présents dans un ordinateur ou automate, implique qu'un code binaire généré pour un système ne puisse pas être a priori le même que sur une machine distincte. Aussi, il existe autant de codes binaires que de configurations et une complexité accrue excluant que l'être humain puisse concevoir simplement un code binaire de grande ampleur.Pour éviter ces écueils, et puisqu'une traduction est toujours nécessaire, l'être humain écrit un code textuel afin qu'il soit plus lisible, plus compréhensible et plus simple à maintenir : c'est le code source écrit dans un langage de programmation. Il est, dans la plupart des cas, plus lisible, plus simple à écrire et indépendant du système cible. Un programme tiers (compilateur, interpréteur ou machine virtuelle) se charge de la traduction du code source en code binaire exécutable par la cible.Le code généré par l'être humain est appelé code source ; la façon dont est rédigé ce code source est appelée langage de programmation ; le traducteur de ce code dans sa représentation binaire est appelé compilateur, interpréteur ou machine virtuelle selon les modalités de la traduction.Dans la plupart des langages, on peut distinguer différents éléments dans un code source :les éléments décrivant l’algorithme et les données (le code source proprement dit) :des symboles identifiant des variables, des mots clefs dénotant des instructions, des représentations de données ;des constantes littérales.les commentaires, qui documentent le code source le plus souvent en langage naturel, destinés aux relecteurs du code source. Ils ne sont pas nécessaires à la production du code exécutable mais peuvent être utilisés par le compilateur pour, par exemple, produire automatiquement de la documentation.Un code est plus facile à lire et à écrire avec un éditeur fournissant une coloration syntaxique permettant de distinguer les différents éléments du code source. Les commentaires peuvent par exemple être mis en vert.Exemple de code en Ruby :Autre exemple de code en Ruby :Autre exemple de code en Ruby :L'analogie du code source et de la recette de cuisine est souvent employée dans une volonté de vulgarisation. Une recette est une liste organisée d'ingrédients dont les quantités et les fonctions sont définies. Le but est d'obtenir le résultat voulu par le cuisinier, selon une technique et un enchaînement d'opérations déterminés.Ainsi le code source peut être apparenté à une recette de cuisine.Ainsi, une personne dégustant un plat est en mesure de deviner les ingrédients qui le composent et d'imaginer comment le réaliser. Néanmoins, pour un plat très raffiné et subtil (comme pourrait l'être un programme), il est fort probable qu'elle ignore le mode opératoire du cuisinier. Pour le connaître, une recette détaillée serait nécessaire (pour un programme, la recette peut compter plusieurs millions de lignes de code). La solution alternative à cela serait d'acheter des plats préparés, c'est un peu ce que l'on fait lorsqu'on achète des logiciels.Le code source peut être public ou privé (voir logiciel libre et logiciel propriétaire). Toutefois, le code binaire n'étant qu'une traduction du code source, il est toujours possible d'étudier un logiciel à partir de son code binaire. La légalité des techniques utilisées à ces fins dépend du pays et de l'époque. Elle peut notamment être mise en œuvre pour percer les secrets d'une machine comme l'ES3B. Portail de la programmation informatique"
informatique;"En informatique, un compilateur  est un programme qui transforme un code source en un code objet. Généralement, le code source est écrit dans un langage de programmation (le langage source), il est de haut niveau d'abstraction, et facilement compréhensible par l'humain. Le code objet est généralement écrit en langage de plus bas niveau (appelé langage cible), par exemple un langage d'assemblage ou langage machine, afin de créer un programme exécutable par une machine.Un compilateur effectue les opérations suivantes : analyse lexicale, pré-traitement (préprocesseur), analyse syntaxique (parsing), analyse sémantique, et génération de code optimisé. La compilation est souvent suivie d'une étape d’édition des liens, pour générer un fichier exécutable. Quand le programme compilé (code objet) est exécuté sur un ordinateur dont le processeur ou le système d'exploitation est différent de celui du compilateur, on parle de compilation croisée.On distingue deux options de compilation : Ahead-of-time (AOT), où il faut compiler le programme avant de lancer l'application : c'est la situation traditionnelle.Compilation à la volée (just-in-time, en abrégé JIT) : cette faculté est apparue dans les années 1980 (par exemple avec Tcl/Tk).La chaine de compilation                 Les logiciels des premiers ordinateurs étaient écrits en langage assembleur. Les langages de programmation de plus haut niveau (dans les couches d'abstraction) n'ont été inventés que lorsque les avantages apportés par la possibilité de réutiliser le logiciel sur différents types de processeurs sont devenus plus importants que le coût de l'écriture d'un compilateur. La capacité de mémoire très limitée des premiers ordinateurs a également posé plusieurs problèmes techniques dans le développement des compilateurs.Vers la fin des années 1950, des langages de programmation indépendants des machines font pour la première fois leur apparition. Par la suite, plusieurs compilateurs expérimentaux sont développés. Le premier compilateur, A-0 System (pour le langage A-0) est écrit par Grace Hopper, en 1952. L'équipe FORTRAN dirigée par John Backus d'IBM est considérée comme ayant développé le premier compilateur complet, durant la période 1954-1957, et il s'agit du premier compilateur optimiseur, l'objectif de l'équipe étant de générer un code en langage machine quasiment aussi rapide que celui qu'aurait généré un programmeur. COBOL, développé en 1959 et reprenant largement des idées de Grace Hopper, est le premier langage à être compilé sur plusieurs architectures.Dans plusieurs domaines d'application[Lesquels ?], l'idée d'utiliser un langage de plus haut niveau d'abstraction s'est rapidement répandue. Avec l'augmentation des fonctionnalités supportées par les langages de programmation plus récents et la complexité croissante de l'architecture des ordinateurs, les compilateurs se sont de plus en plus complexifiés.En 1962, le premier compilateur « auto-hébergé » - capable de compiler en code objet, son propre code source  exprimé en langage de haut niveau - est créé, pour le Lisp, par Tim Hart et Mike Levin au Massachusetts Institute of Technology (MIT). À partir des années 1970, il est devenu très courant de développer un compilateur dans le langage qu'il doit compiler, faisant du  Pascal et du C des langages de développement très populaires.On peut aussi utiliser un langage ou un environnement spécialisé dans le développement de compilateurs : on parle lors d'outils de méta-compilation, et on utilise par exemple un compilateur de compilateur. Cette méthode est particulièrement utile pour réaliser le premier compilateur d'un nouveau langage ; l'utilisation d'un langage adapté et rigoureux[Par exemple ?] facilite ensuite mise au point et évolution.La tâche principale d'un compilateur est de produire un code objet correct qui s'exécutera sur un ordinateur. La plupart des compilateurs permettent d'optimiser le code, c'est-à-dire qu'ils vont chercher à améliorer la vitesse d'exécution, ou réduire l'occupation mémoire du programme.En général, le langage source est « de plus haut niveau » que le langage cible, c'est-à-dire qu'il présente un niveau d'abstraction supérieur. De plus, le code source du programme est généralement réparti dans plusieurs fichiers.Un compilateur fonctionne par analyse-synthèse : au lieu de remplacer chaque construction du langage source par une suite équivalente de constructions du langage cible, il commence par analyser le texte source pour en construire une représentation intermédiaire qu'il traduit à son tour en langage cible.On sépare le compilateur en au moins deux parties : une partie avant (ou frontale), parfois appelée « souche », qui lit le texte source et produit la représentation intermédiaire ; et une partie arrière (ou finale), qui parcourt cette représentation pour produire le texte cible. Dans un compilateur idéal, la partie avant est indépendante du langage cible, tandis que la partie arrière est indépendante du langage source.Certains compilateurs effectuent des traitements substantiels sur la partie intermédiaire, devenant une partie centrale à part entière, indépendante à la fois du langage source et de la machine cible. On peut ainsi écrire des compilateurs pour toute une gamme de langages et d'architectures en partageant la partie centrale, à laquelle on attache une partie avant par langage et une partie arrière par architecture.Les étapes de la compilation incluent :le prétraitement, nécessaire pour certains langages comme C, qui prend en charge la substitution de macro et de la compilation conditionnelle.Généralement, la phase de prétraitement se produit avant l'analyse syntaxique ou sémantique ; par exemple dans le cas de C, le préprocesseur manipule les symboles lexicaux plutôt que des formes syntaxiques.l'analyse lexicale, qui découpe le code source en petits morceaux appelés jetons (tokens).Chaque jeton est une unité atomique unique de la langue (unités lexicales ou lexèmes), par exemple un mot-clé, un identifiant ou un symbole. La syntaxe de jeton est généralement un langage régulier, donc reconnaissable par un automate à états finis.Cette phase est aussi appelée à balayage ou lexing ; le logiciel qui effectue une analyse lexicale est appelé un analyseur lexical ou un scanner. Un analyseur lexical pour un langage régulier peut être généré par un programme informatique, à partir d'une description du langage par des expressions régulières. Deux générateurs classiques sont lex et flex.l'analyse syntaxique implique l'analyse de la séquence jeton pour identifier la structure syntaxique du programme.Cette phase s'appuie généralement sur la construction d'un arbre d'analyse ; on remplace la séquence linéaire des jetons par une structure en arbre construite selon la grammaire formelle qui définit la syntaxe du langage. Par exemple, une condition est toujours suivie d'un test logique (égalité, comparaison…). L'arbre d'analyse est souvent modifié et amélioré au fur et à mesure de la compilation. Yacc et GNU Bison sont les analyseurs syntaxiques les plus utilisés.l'analyse sémantique est la phase durant laquelle le compilateur ajoute des informations sémantiques à l'arbre d'analyse et construit la table des symboles.Cette phase vérifie le type (vérification des erreurs de type), ou l'objet de liaison (associant variables et références de fonction avec leurs définitions), ou une tâche définie (toutes les variables locales doivent être initialisées avant utilisation), peut émettre des avertissements, ou rejeter des programmes incorrects.L'analyse sémantique nécessite habituellement un arbre d'analyse complet, ce qui signifie que cette phase fait suite à la phase d'analyse syntaxique, et précède logiquement la phase de génération de code ; mais il est possible de replier ces phases en une seule passe.la transformation du code source en code intermédiaire ;l'application de techniques d'optimisation sur le code intermédiaire : c'est-à-dire rendre le programme « meilleur » selon  son usage (voir infra) ;la génération de code avec l'allocation de registres et la traduction du code intermédiaire en code objet, avec éventuellement l'insertion de données de débogage et d'analyse de l'exécution ;et finalement l'édition des liens.L'analyse lexicale, syntaxique et sémantique, le passage par un langage intermédiaire et l'optimisation forment la partie frontale.La génération de code et l'édition de liens constituent la partie finale.Ces différentes étapes font que les compilateurs sont toujours l'objet de recherches.L'implémentation (réalisation concrète) d'un langage de programmation peut être interprétée ou compilée. Cette réalisation est un compilateur ou un interpréteur, et un langage de programmation peut avoir une implémentation compilée, et une autre interprétée.On parle de compilation si la traduction est faite avant l'exécution (le principe d'une boucle est alors traduit une fois), et d'interprétation si la traduction est finie pas à pas, durant l'exécution (les éléments d'une boucle sont alors examinés à chaque usage).L'interprétation est utile pour la mise au point ou si les moyens sont limités. La compilation est préférable en exploitation.Les premiers compilateurs ont été écrits directement en langage assembleur, un langage symbolique élémentaire correspondant aux instructions du processeur cible et quelques structures de contrôle légèrement plus évoluées. Ce langage symbolique doit être assemblé (et non compilé) et lié pour obtenir une version exécutable. En raison de sa simplicité, un programme simple suffit à le convertir en instructions machines.Les compilateurs actuels sont généralement écrits dans le langage qu'ils doivent compiler ; par exemple un compilateur C est écrit en C, SmallTalk en SmallTalk, Lisp en Lisp, etc. Dans la réalisation d'un compilateur, une étape décisive est franchie lorsque le compilateur pour le langage X est suffisamment complet pour se compiler lui-même : il ne dépend alors plus d'un autre langage (même de l'assembleur) pour être produit.Il est complexe de détecter un bogue de compilateur. Par exemple, si un compilateur C comporte un bogue, les programmeurs en langage C auront naturellement tendance à mettre en cause leur propre code source, non pas le compilateur.Pire, si ce compilateur buggé (version V1) compile un compilateur (version V2) non buggé, l'exécutable compilé (par V1) du compilateur V2 pourrait être buggé. Pourtant son code source est bon. Le bootstrap oblige donc les programmeurs de compilateurs à contourner les bugs des compilateurs existants.La classification des compilateurs par nombre de passes a pour origine le manque de ressources matérielles des ordinateurs.La compilation est un processus coûteux et les premiers ordinateurs n'avaient pas assez de mémoire pour contenir un programme devant faire ce travail. Les compilateurs ont donc été divisés en sous programmes qui font chacun une lecture de la source pour accomplir les différentes phases d’analyse lexicale, d'analyse syntaxique et d'analyse sémantique.L'aptitude à combiner le tout en un seul passage a été considérée comme un avantage, car elle simplifie l'écriture du compilateur, qui s'exécute généralement plus rapidement qu’un compilateur multi passe.Ainsi, dus aux ressources limitées des premiers systèmes, de nombreux langages ont été spécifiquement conçus afin qu'ils puissent être compilés en un seul passage (par exemple, le langage Pascal). Structure non linéaire du programme Dans certains cas, telle ou telle fonctionnalité du langage requiert que son compilateur effectue plus d'une passe. Par exemple, considérons une déclaration figurant à la ligne 20 de la source qui affecte la traduction d'une déclaration figurant à la ligne 10. Dans ce cas, la première passe doit recueillir des renseignements sur les déclarations, tandis que la traduction proprement dite ne s’effectue que lors d'un passage ultérieur. Optimisations Le fractionnement d'un compilateur en petits programmes est une technique utilisée par les chercheurs intéressés à produire des compilateurs performants. En effet, l'inconvénient de la compilation en une seule passe est qu'elle ne permet pas l'exécution de la plupart des optimisations sophistiquées nécessaires à la génération de code de haute qualité. Il devient alors difficile de dénombrer exactement le nombre de passes qu’un compilateur optimisant effectue. Fractionnement de la démonstration de correction Démontrer la correction d'une série de petits programmes nécessite souvent moins d'effort que de démontrer la correction d'un plus grand programme unique équivalent.Un compilateur de compilateur est un programme qui peut générer une, voire toutes les parties d'un compilateur.On peut par exemple compiler les bases d'un langage, puis, utiliser les bases du langage pour compiler le reste.Selon l'usage et la machine qui va exécuter un programme, on peut vouloir optimiser la vitesse d'exécution, l'occupation mémoire, la consommation d'énergie, la portabilité sur d'autres architectures, ou le temps de compilation.Il existe des compilateurs qui sont vérifiés mathématiquement. Ces compilateurs garantissent que les propriétés de sécurité prouvées sur le code source sont également valables pour le code compilé exécutable,. Ce type de compilateurs est notamment utilisé pour le développement d'algorithmes de contrôle de vol et de navigation dans l'aviation ou dans le domaine de l'énergie nucléaire.La compilation croisée fait référence aux chaînes de compilation capables de traduire un code source en code objet dont l'architecture processeur diffère de celle où la compilation est effectuée. Ces chaînes sont principalement utilisées en informatique industrielle et dans les systèmes embarqués.Certains compilateurs traduisent un langage source en langage machine virtuel (dit langage intermédiaire), c'est-à-dire en un code (généralement binaire) exécuté par une machine virtuelle : un programme émulant les principales fonctionnalités d'un ordinateur. De tels langages sont dits semi-compilés. Le portage d'un programme ne requiert ainsi que le portage de la machine virtuelle, qui sera de fait soit un interprète, soit un second traducteur (pour les compilateurs multi-cibles). Ainsi, des compilateurs traduisent Pascal en P-Code, Modula 2 en M-Code, Simula en S-code, ou plus récemment du code Java en bytecode Java (code objet).     Quand la compilation repose sur un byte code, on parle de compilation à la volée. On utilise alors des machines virtuelles comme la machine virtuelle Java avec laquelle on peut notamment compiler du Scala. Il est possible dans certains langages d'utiliser une bibliothèque permettant la compilation à la volée de code entré par l'utilisateur, par exemple en C avec libtcc.D’autres compilateurs traduisent un code d’un langage de programmation vers un autre. On les appelle des transcompilateurs, ou bien encore par anglicisme, des transpileurs ou transpilateurs. Par exemple, le logiciel LaTeX permet, à partir d’un code source en LaTeX, d’obtenir un fichier au format PDF (avec par exemple la commande pdflatex sous Ubuntu) ou HTML. Autre exemple, LLVM est une bibliothèque aidant à réaliser des compilateurs, également utilisée par AMD pour développer « HIP », un transcompilateur de code CUDA (langage spécifique à NVIDIA et très utilisé) afin de l’exécuter sur les processeurs graphiques d’AMD.          Certains compilateurs traduisent, de façon incrémentale ou interactive, le programme source (entré par l’utilisateur) en code machine. On peut citer comme exemple certaines implantations de Common Lisp (comme SBCL (en)).Alfred Aho, Monica Lam, Ravi Sethi et Jeffrey Ullman (trad. de l'anglais par Philippe Deschamp, Bernard Lorho, Benoît Sagot et François Thomasset), Compilateurs : Principes, techniques et outils [« Compilers: Principles, Techniques, and Tools »], France, Pearson Education, novembre 2007, 2e éd. (1re éd. 1977), 901 p. (ISBN 978-2-7440-7037-2, présentation en ligne)InterprèteLow Level Virtual MachineCompilation à la voléeCompilation incrémentaleCompilation anticipéeDécompilateur, programme qui traduit un langage de bas niveau vers un langage de plus haut niveauGCC est une suite de compilation particulièrement connue, beaucoup utilisée pour les langages C et C++, mais également Java ou encore Ada.Clang est un front-end pour les langages de la famille du C, utilisant le back-end LLVMJavac, le compilateur Java le plus répanduGHC, un compilateur pour HaskellDe nombreux autres[Lesquels ?], pour les mêmes langages et pour d'autres[Lesquels ?](en) Liste de compilateurs gratuits et/ou libresCours plutôt complet et contenant des exemples en C/ASM. Portail de la programmation informatique   Portail de l’informatique"
informatique;"La cryptographie est une des disciplines de la cryptologie s'attachant à protéger des messages (assurant confidentialité, authenticité et intégrité) en s'aidant souvent de secrets ou clés. Elle se distingue de la stéganographie qui fait passer inaperçu un message dans un autre message alors que la cryptographie rend un message supposément inintelligible à autre que qui-de-droit.Elle est utilisée depuis l'Antiquité, mais certaines de ses méthodes les plus modernes, comme la cryptographie asymétrique, datent de la fin du XXe siècle.Le mot cryptographie vient des mots en grec ancien kruptos (κρυπτός) « caché » et graphein (γράφειν) « écrire ». Beaucoup des termes de la cryptographie utilisent la racine « crypt- », ou des dérivés du terme « chiffre » :chiffrement : transformation à l'aide d'une clé d'un message en clair (dit texte clair) en un message incompréhensible (dit texte chiffré) pour celui qui ne dispose pas de la clé de déchiffrement (en anglais encryption key ou private key pour la cryptographie asymétrique) ;chiffre : un ensemble de règles permettant d'écrire et de lire dans un langage secret ;cryptogramme : message chiffré ;cryptosystème : algorithme de chiffrement ;décrypter : retrouver le message clair correspondant à un message chiffré sans posséder la clé de déchiffrement (terme que ne possèdent pas les anglophones, qui eux « cassent » des codes secrets) ;cryptographie : étymologiquement « écriture secrète », devenue par extension l'étude de cet art (donc aujourd'hui la science visant à créer des cryptogrammes, c'est-à-dire à chiffrer) ;cryptanalyse : science analysant les cryptogrammes en vue de les décrypter ;cryptologie : science regroupant la cryptographie et la cryptanalyse ;cryptolecte : jargon réservé à un groupe restreint de personnes désirant dissimuler leur communication.Plus récemment sont apparus les termes « crypter » (pour chiffrer) et « cryptage » pour chiffrement. Ceux-ci sont acceptés par l'Office québécois de la langue française dans son grand dictionnaire terminologique, qui note que « La tendance actuelle favorise les termes construits avec crypt-. ». Le Grand Robert mentionne également « cryptage », et date son apparition de 1980. Cependant le Dictionnaire de l'Académie française n'intègre ni « crypter » ni « cryptage » dans sa dernière édition (entamée en 1992). Ces termes sont d'ailleurs considérés comme incorrects par exemple par l'ANSSI, qui met en avant le sens particulier du mot « décrypter » (retrouver le message clair à partir du message chiffré sans connaître la clef) en regard du couple chiffrer/déchiffrer.La cryptographie est utilisée depuis l'antiquité, et l'une des utilisations les plus célèbres pour cette époque est le chiffre de César, nommé en référence à Jules César qui l'utilisait pour ses communications secrètes. Mais la cryptographie est bien antérieure à cela : le plus ancien document chiffré est une recette secrète de poterie datant du XVIe siècle av. J.-C., notée sur une tablette d'argile qui a été découverte dans l'actuel Irak.L'historien en cryptographie David Kahn considère l'humaniste Leon Battista Alberti comme le « père de la cryptographie occidentale », grâce à trois avancées significatives : « la plus ancienne théorie occidentale de cryptanalyse, l'invention de la substitution polyalphabétique, et l'invention du code de chiffrement ».Bien qu'éminemment stratégique, la cryptographie est restée pendant très longtemps un art, pour ne devenir une science qu'au XXIe siècle. Avec l'apparition de l'informatique, son utilisation se popularise et se vulgarise, quitte à se banaliser et à être utilisée à l'insu de l’utilisateur[réf. nécessaire].Enfin, la Cryptographie post-quantique est une sous-discipline de la cryptographie qui cherche à proposer des algorithmes résistant au calculateur quantique.Les domaines d'utilisations de la cryptographie sont vastes et vont du domaine militaire, au commercial, en passant par la protection de la vie privée.Les techniques de cryptographie sont parfois utilisées pour protéger notre vie privée. Ce droit est en effet plus facilement bafoué dans la sphère numérique. Ainsi les limites de la cryptographie quant à sa capacité à préserver la vie privée soulève des questionnements. Deux exemples qui illustrent bien ce sujet sont à trouver dans le domaine de la santé et celui de la blockchain.La santé est un domaine sensible quant à la protection des données : le secret médical est remis en question avec l’informatisation de la médecine.La cryptographie permet en théorie de protéger les données médicales pour qu’elles ne soient pas accessible à n’importe qui, mais elle n’est pas suffisante.Car tant que le droit n’est pas suffisamment large[pas clair], il existe des failles qui permettent à certains acteurs d’utiliser des données personnelles dès l'accord de l'usager donné, or cet accord est exigé pour l'accès au service, faisant ainsi perdre à l'utilisateur la possibilité de contrôle de ses  accès à nos données personnelles.De plus l’inviolabilité des données médicales est remise en question par les développements qui permettent le déchiffrement de ces données, en effet selon Bourcier et Filippi, l’« anonymat ne semble plus garanti de façon absolue en l’état actuel des techniques de cryptographie ». Avec cette double constatation ils proposent de protéger nos données médicales avec une réforme juridique qui permettrait de faire rentrer les données personnelles médicales non pas dans le droit à la vie privée qui est un droit personnel, mais dans un droit collectif qui permettrait de protéger plus efficacement des données telles que les données génétiques qui concernent plusieurs individus. La création d’un droit collectif pour la santé permettrait ainsi de compenser les limites de la cryptographie qui n’est pas en mesure d’assurer à elle seule la protection de ce type de données.La blockchain est elle aussi l’une des applications de la cryptographie en lien avec la protection de la vie privée. C’est un système décentralisé qui se base entre autres sur des techniques de cryptographie destinées à assurer la fiabilité des échanges tout en garantissant en principe la vie privée. Qui dit système décentralisé implique qu’il n’y a pas de tierce personne par laquelle passe les informations. Ainsi seuls les individus concernés ont accès aux données vu que les données sont chiffrées, d’où un respect important de la vie privée. En pratique cela dit, ce système présente des limites : « la décentralisation est acquise au prix de la transparence ». En effet un tel système ne protège pas les informations concernant la transaction : destinataire, date, et autres métadonnées qui sont nécessaires pour s’assurer de la légitimité. Ainsi une protection complète de la vie privée en blockchain nécessite que ces métadonnées soient elles aussi protégées, puisque celles-ci sont transparentes et donc visibles par tout le monde. Cette protection supplémentaire est rendue possible par de nouvelles techniques d'anonymisation des signatures telles que la signature aveugle, qui sont réputées de garantir la légitimité des transactions sans les rendre publiques. Mais ce processus n’est pas encore applicable partout et n’est qu’à l’état embryonnaire pour certaines techniques. Malgré tout avec le temps de plus en plus de systèmes permettront de résoudre cette limitation.[Quand ?]Le cadre législatif de la cryptographie est variable et sujet aux évolutions.D’une part, il est sujet aux évolutions des technologies, de leur efficacité et de leur accessibilité. En effet la démocratisation d’Internet et des ordinateurs personnels fondent un nouveau cadre dans les années 80-90, comme nous le verrons avec l’exemple de la loi française.D’autre part, ces lois évoluent selon le contexte politique. En effet, à la suite des attentats du 11 septembre 2001, les gouvernements occidentaux opèrent une reprise du contrôle des données circulant sur Internet et de toutes les données potentiellement cachées par la cryptographie.Cela se fait de plusieurs façons : d’une part, par la mise en place de lois obligeant les fournisseurs de systèmes de communication, cryptés ou non, à fournir à certaines entités étatiques des moyens d’accéder à toutes ces données. Par exemple en France, alors qu’en 1999, la loi garantit la protection des communications privées par voie électronique, celle-ci subit l’amendement à la Loi no 91-646 du 10 juillet 1991 relative au secret des correspondances émises par la voie des communications électroniques. Cet amendement formalise précisément le moyen législatif d’accéder à des données encryptées décrit précédemment.D’autre part, certains services gouvernementaux développent des systèmes d’inspection de réseaux afin de tirer des informations malgré le chiffrement des données. On peut notamment citer le programme de surveillance électronique Carnivore aux États-Unis.Toutefois, la réglementation sur les systèmes de cryptographie ne laisse que peu de place à un contrôle par des entités telles que des gouvernements. En effet, les logiciels et algorithmes les plus performants et répandus sont issus de la connaissance et des logiciels libres comme PGP ou OpenSSH. Ceux-ci offrent une implémentation fonctionnelle des algorithmes de chiffrement modernes pour assurer le chiffrement de courriels, de fichiers, de disques durs ou encore la communication dite sécurisée entre plusieurs ordinateurs. Ces logiciels étant sous licence libre, leur code source est accessible, reproductible et modifiable. Cela implique qu’il est techniquement très difficile de les rendre exclusifs à une entité — étatique par exemple — et d’en avoir le contrôle. Le chiffrement devient alors utilisable par nombre de personnes, permettant de contrevenir à une loi.Bien que la cryptographie puisse paraître être une opportunité pour la démocratie au premier abord, la réalité n’est pas forcément si unilatérale. Il est clair que l’utilisation de cette technologie permet de protéger la liberté d’expression. Toutefois, cela ne suffit pas à dire que la cryptographie est bénéfique à la démocratie, puisque l'enjeu démocratique dépasse la simple liberté l’expression. En particulier, la démocratie suppose un système de lois et de mécanismes de sanctions qui mène la liberté d’expression vers une activité politique constructive.Avec l’apparition de la cryptographie électronique et dans un monde toujours plus numérisé, la politique doit aussi s’adapter. Winkel observe trois politiques différentes pour les gouvernements: la stratégie libérale, la stratégie de prohibition et la stratégie du tiers de confiance. Stratégie de prohibition La stratégie de prohibition consiste à restreindre l’utilisation de la cryptographie en imposant des contrôles d’import-export, des restrictions d’utilisation ou encore d’autres mesures pour permettre à l’État et ses institutions de mettre en œuvre dans le monde virtuel la politique (principes et lois) du « vrai » monde. Cette stratégie est généralement appliquée dans des pays à régime politique autoritaire, par exemple en Chine avec le Grand Firewall ou en Corée du Nord. Stratégie du tiers de confiance La stratégie du tiers de confiance a pour but de garder la balance qu’il existe dans le « vrai » monde entre d’un côté la législation et les potentielles sanctions de l’État et de l’autre la protection de secrets économiques ou de la sphère privée, dans le monde virtuel. La mise en place d’un tel système est toutefois plus technique.Le principe consiste en un dépôt des copies des clés d’encryption des utilisateurs dans les mains d’un tiers de confiance. Celui-ci pourrait ensuite répondre à une demande d'une autorité légale compétente et lui transmettre une clef - par exemple à des fins d’audit - à condition que cette demande ait suivi une procédure bien définie. Cette solution, bien que paraissant optimale du point de vue de la théorie démocratique, présente déjà un certain nombre de difficultés techniques comme la mise en place et l'entretien de l’infrastructure requise. De plus, il est utopique d’imaginer que la mise en place de cadres légaux plus sévères découragera les criminels et organisations anticonstitutionnelles d’arrêter leurs activités. Cela s’applique à la stratégie du tiers de confiance et à celle de prohibition. Stratégie libérale La stratégie libérale répandue dans le monde laisse un accès ""total"" aux technologies de cryptographie, pour sécuriser la vie privée des citoyens, défendre la liberté d’expression dans l’ère numérique, laisser les entreprises garder leurs secrets et laisser les entreprises exporter des solutions informatiques sécurisées sur les marchés internationaux.Cependant, les criminels et opposants de la Constitution[Laquelle ?] peuvent utiliser cette technologie à des fins illicites — ou anticonstitutionnelles —[Laquelle ?] comme  armes, drogue ou pédopornographie sur le Dark Web. Autres formes de législation Les États-Unis et la France interdisent l'exportation de certaines formes de cryptographie, voir Lois sur les chiffrement sur wikipedia anglophone.Les premiers algorithmes utilisés pour le chiffrement d'une information étaient assez rudimentaires dans leur ensemble. Ils consistaient notamment au remplacement de caractères par d'autres. La confidentialité de l'algorithme de chiffrement était donc la pierre angulaire de ce système pour éviter un décryptage rapide.Exemples d'algorithmes de chiffrement faibles :ROT13 (rotation de 13 caractères, sans clé) ;Chiffre de César (décalage de trois lettres dans l'alphabet sur la gauche) ;Chiffre de Vigenère (introduit la notion de clé).Les algorithmes de chiffrement symétrique se fondent sur une même clé pour chiffrer et déchiffrer un message. L'un des problèmes de cette technique est que la clé, qui doit rester totalement confidentielle, doit être transmise au correspondant de façon sûre. La mise en œuvre peut s'avérer difficile, surtout avec un grand nombre de correspondants car il faut autant de clés que de correspondants.Quelques algorithmes de chiffrement symétrique très utilisés :Chiffre de Vernam (le seul offrant une sécurité théorique absolue, à condition que la clé ait au moins la même longueur que le message à chiffrer, qu'elle ne soit utilisée qu'une seule fois et qu'elle soit totalement aléatoire)DES3DESAESRC4RC5MISTY1et d'autres (voir la liste plus exhaustive d'algorithmes de cryptographie symétrique).Pour résoudre le problème de l'échange de clés, la cryptographie asymétrique a été mise au point dans les années 1970. Elle se base sur le principe de deux clés :une publique, permettant le chiffrement ;une privée, permettant le déchiffrement.Comme son nom l'indique, la clé publique est mise à la disposition de quiconque désire chiffrer un message. Ce dernier ne pourra être déchiffré qu'avec la clé privée, qui doit rester confidentielle.Quelques algorithmes de cryptographie asymétrique très utilisés :RSA (chiffrement et signature) ;DSA (signature) ;Protocole d'échange de clés Diffie-Hellman (échange de clé) ;et d'autres ; voir cette liste plus complète d'algorithmes de cryptographie asymétrique.Le principal inconvénient de RSA et des autres algorithmes à clés publiques est leur grande lenteur par rapport aux algorithmes à clés secrètes. RSA est par exemple 1000 fois plus lent que DES. En pratique, dans le cadre de la confidentialité, on s'en sert pour chiffrer un nombre aléatoire qui sert ensuite de clé secrète pour un algorithme de chiffrement symétrique. C'est le principe qu'utilisent des logiciels comme PGP par exemple.La cryptographie asymétrique est également utilisée pour assurer l'authenticité d'un message. L'empreinte du message est chiffrée à l'aide de la clé privée et est jointe au message. Les destinataires déchiffrent ensuite le cryptogramme à l'aide de la clé publique et retrouvent normalement l'empreinte. Cela leur assure que l'émetteur est bien l'auteur du message. On parle alors de signature ou encore de scellement.La plupart des algorithmes de cryptographie asymétrique sont vulnérables à des attaques utilisant un calculateur quantique, à cause de l'algorithme de Shor. La branche de la cryptographie visant à garantir la sécurité en présence d'un tel adversaire est la cryptographie post-quantique.Une fonction de hachage est une fonction qui convertit un grand ensemble en un plus petit ensemble, l'empreinte. Il est impossible de la déchiffrer pour revenir à l'ensemble d'origine, ce n'est donc pas une technique de chiffrement.Quelques fonctions de hachage très utilisées :MD5 ;SHA-1 ;SHA-256 ;et d'autres ; voir cette liste plus complète d'algorithmes de hachage.L'empreinte d'un message ne dépasse généralement pas 256 bits (maximum 512 bits pour SHA-512) et permet de vérifier son intégrité.Projet NESSIEAdvanced Encryption Standard processLes cryptologues sont des experts en cryptologie : ils conçoivent, analysent et cassent les algorithmes (voir cette liste de cryptologues).Le mouvement Cypherpunk, qui regroupe des partisans d'une idéologie dite « cyber libertarienne », est un mouvement créé en 1991 œuvrant pour défendre les droits civils numériques des citoyens, à travers la cryptographie.Essentiellement composé de hackers, de juristes et de militants de la liberté sur le web ayant pour objectif commun une plus grande liberté de circulation de l'information, ce groupe s'oppose à toute intrusion et tentative de contrôle du monde numérique par des grandes puissances, en particulier les États.Les crypto-anarchistes considèrent la confidentialité des données privées comme un droit inhérent. En s'inspirant du système politique libéral américain, ils défendent le monde numérique en tant qu'espace à la fois culturel, économique et politique à l'intérieur d'un réseau ouvert et décentralisé, où chaque utilisateur aurait sa place et pourrait jouir de tous ses droits et libertés individuelles.Les crypto-anarchistes cherchent à démontrer que les libertés numériques ne sont pas des droits à part, contraints d’exister seulement dans le domaine technique qu’est internet mais que maintenant le numérique est un élément important et omniprésent dans la vie quotidienne, et ainsi, il est primordial dans la définition des libertés fondamentales des citoyens. Les droits et libertés numériques ne doivent pas être considérées comme moins importante que celles qui régissent le monde matériel.La création des crypto-monnaies en mai 1992[réf. souhaitée], remplit un des objectifs du mouvement en offrant une monnaie digitale intraçable en ligne mais permet également l'expansion de marchés illégaux sur le web.L’apparition de nouvelles techniques (logiciels de surveillance de masse comme Carnivore, PRISM, XKeyscore...) a en fait mené à plus de surveillance, moins de vie privée, et un plus grand contrôle de la part des États qui se sont approprié ces nouvelles technologies.Crypto-anarchistes (pour l’anonymisation des communications) et États (pour le contrôle des communications) s’opposent le long de ces arguments.Un axiome central du mouvement Cypherpunk est que, pour rééquilibrer les forces entre l’État et les individus, il faut la protection des communications privées ainsi que la transparence des informations d’intérêt public, comme l’énonce la devise : « Une vie privée pour les faibles et une transparence pour les puissants ».Dans ce sens, Julian Assange (un des plus importants membres du mouvement Cypherpunk) a créé WikiLeaks, un site qui publie aux yeux de tous, des documents et des secrets d’État initialement non connus du grand public.Les événements du 11 septembre 2001 ont été des arguments de poids pour les États, qui avancent qu'une régulation et un contrôle du monde d'internet sont nécessaires afin de préserver nos libertés.L'apparition de lanceurs d'alerte comme Edward Snowden en 2013 est un événement important en faveur du mouvement crypto-anarchiste qui s'oppose au contrôle de l’État dans le monde numérique.D'autres groupes/mouvements importants sont créés pour défendre les libertés d’internet, partageant des objectifs avec le mouvement Cypherpunk :Les Anonymous qui défendent la liberté d'expression sur internet et en dehors.L'Electronic Frontier Foundation (EFF) qui défend la confidentialité des données numériques.Le Parti Pirate qui défend l’idée des partages des données et se bat pour les libertés fondamentales sur Internet (partage d’informations, de savoirs culturels et scientifiques qui sont parfois bannis d’internet).David Kahn (trad. de l'anglais par Pierre Baud, Joseph Jedrusek), La guerre des codes secrets [« The Codebreakers »], Paris, InterEditions, 1980, 405 p. (ISBN 2-7296-0066-3).Simon Singh (trad. Catherine Coqueret), Histoire des codes secrets [« The Code Book »], Librairie générale française (LFG), coll. « Le Livre de Poche », 3 septembre 2001, 504 p., Poche (ISBN 2-253-15097-5, ISSN 0248-3653, OCLC 47927316).Jacques Stern, La Science du secret, Paris, Odile Jacob, coll. « Sciences », 5 janvier 1998, 203 p. (ISBN 2-7381-0533-5, OCLC 38587884, lire en ligne)Gilles Zémor, Cours de cryptographie, Paris, Cassini, 15 décembre 2000, 227 p. (ISBN 2-84225-020-6, OCLC 45915497).« L'art du secret », Pour la science, dossier hors-série, juillet-octobre 2002.Douglas Stinson (trad. de l'anglais par Serge Vaudenay, Gildas Avoine, Pascal Junod), Cryptographie : Théorie et pratique [« Cryptography : Theory and Practice »], Paris, Vuibert, coll. « Vuibert informatique », 28 février 2003, 337 p., Broché (ISBN 2-7117-4800-6, ISSN 1632-4676, OCLC 53918605)(en) Handbook of Applied Cryptography, A.J. Menezes, éd. P.C. van Oorschot et S.A. Vanstone - CRC Press, 1996. Disponible en ligne : [1]Site thématique de la sécurité des systèmes d'information : site officiel de l'Agence nationale de la sécurité des systèmes d'information sur la question de la sécurité informatique. Présentation de la cryptographie, des signatures numériques, de la législation française sur le sujet, etc.Bruce Schneier (trad. de l'anglais par Laurent Viennot), Cryptographie appliquée [« Applied cryptography »], Paris, Vuibert, coll. « Vuibert informatique », 15 janvier 2001, 846 p., Broché (ISBN 2-7117-8676-5, ISSN 1632-4676, OCLC 46592374).Niels Ferguson, Bruce Schneier (trad. de l'anglais par Henri-Georges Wauquier, Raymond Debonne), Cryptographie : en pratique [« Practical cryptography »], Paris, Vuibert, coll. « En pratique / Sécurité de l'information et des systèmes », 18 mars 2004, 338 p., Broché (ISBN 2-7117-4820-0, ISSN 1632-4676, OCLC 68910552).Pierre Barthélemy, Robert Rolland et Pascal Véron (préf. Jacques Stern), Cryptographie : principes et mises en œuvre, Paris, Hermes Science Publications : Lavoisier, coll. « Collection Informatique », 22 juillet 2005, 414 p., Broché (ISBN 2-7462-1150-5, ISSN 1242-7691, OCLC 85891916).Auguste Kerckhoffs, La Cryptographie militaire, L. Baudoin, 1883.Marcel Givierge, Cours de cryptographie, Berger-Levrault, 1925.Jean-Guillaume Dumas, Pascal Lafourcade, Patrick Redon, Architectures de sécurité pour internet - 2e éd. Protocoles, standards et déploiement , Dunod 2020.Jean-Guillaume Dumas, Jean-Louis Roch, Sébastien Varrette, Eric Tannier,Théorie des codes - 3e éd. : Compression, cryptage, correction, Dunod 2018.Jean-Guillaume Dumas, Pascal Lafourcade, Etienne Roudeix, Ariane Tichit, Sébastien Varrette, Les NFT en 40 questions: Comprendre les jetons Non Fungible, Dunod 2022.Jean-Guillaume Dumas, Pascal Lafourcade, Ariane Tichit, Sébastien Varrette, Les blockchains en 50 questions - 2éd.: Comprendre le fonctionnement de cette technologie, Dunod 2022.Pascal Lafourcade, Malika More, 25 énigmes ludiques pour s'initier à la cryptographie, Dunod 2021.Henry Mamy, « La cryptographie », dans Science et Guerre, vol. 16, Bernard Tignole éditeur, 1888 (lire en ligne), disponible sur GallicaLa Cryptogr@phie expliquée!, démonstrations avec des applets Java.ACrypTA, cours, exercices, textes, liens concernant la cryptographie.Ars cryptographica , vulgarisation très complète.Cryptographie, ressources, algorithmes, des ressources sur les algorithmes cryptographiques de dernière génération et sur la cryptographie classique.Cryptographie, du chiffre et des lettres, exposé de François Cayre sur le site Interstices.(en) Handbook of Applied Cryptography, une référence de plus de 800 pages dont l'édition de 1996 peut être téléchargée gratuitement Portail de la cryptologie   Portail de la sécurité de l’information   Portail de la sécurité informatique"
informatique;"La cryptologie, étymologiquement la « science du secret » est considérée comme une science que depuis le XXe siècle. Elle englobe la cryptographie — l'écriture secrète – et la cryptanalyse – l'analyse de cette dernière.Le terme « crypto » provient du latin et du grec et signifie ce qui est dissimulé ou caché.À la fois art ancien et science nouvelle, la cryptologie est utilisée durant l'Antiquité par les Spartiates (la scytale) et elle devient thème de recherche scientifique académique universitaire, depuis les années 1970. Cette discipline est liée à beaucoup d'autres, notamment l'arithmétique modulaire, l'algèbre, la théorie de la complexité, la théorie de l'information ou encore les codes correcteurs d'erreurs.Les premières méthodes de chiffrement remontent à l’Antiquité et se sont améliorées, avec la fabrication de différentes machines de chiffrement, pour obtenir un rôle majeur lors de la Première Guerre mondiale et de la Seconde Guerre mondiale.La cryptographie se scinde en deux parties nettement différenciées :d'une part la cryptographie à clef secrète, encore appelée symétrique ou bien classique ;d'autre part la cryptographie à clef publique, dite également asymétrique ou moderne.La première est la plus ancienne, on peut la faire remonter à l'Égypte de l'an 2000 av. J.-C. en passant par Jules César ; la seconde remonte à l'article de W. Diffie et M. Hellman, New directions in cryptography daté de 1976.Toutes deux visent à assurer la confidentialité de l'information, mais la cryptographie à clef secrète nécessite au préalable la mise en commun entre les destinataires d'une certaine information : la clef (symétrique), nécessaire au chiffrement ainsi qu'au déchiffrement des messages. Dans le cadre de la cryptographie à clef publique, ce n'est plus nécessaire. En effet, les clefs sont alors différentes, ne peuvent se déduire l'une de l'autre, et servent à faire des opérations opposées, d'où l'asymétrie entre les opérations de chiffrement et de déchiffrement.Bien que beaucoup plus récente et malgré d'énormes avantages – signature numérique, échange de clefs... – la cryptographie à clef publique ne remplace pas totalement celle à clef secrète, qui pour des raisons de vitesse de chiffrement et parfois de simplicité reste présente. À ce titre, signalons la date du dernier standard américain en la matière, l'AES : décembre 2001, ce qui prouve la vitalité encore actuelle de la cryptographie symétrique.Dans le bestiaire des algorithmes de chiffrement, on peut citer :pour les systèmes symétriques, le DES, l'AES, Blowfish, IDEA, etc.pour les systèmes asymétriques, le RSA, DSA-DH, ElGamal, les courbes elliptiques, etc.Le pendant de cette confidentialité se trouve dans la cryptanalyse. Évidemment, depuis l'existence de ces codes secrets, on a cherché à les casser, à comprendre les messages chiffrés bien que l'on n'en soit pas le destinataire légitime, autrement dit décrypter. Si la cryptanalyse du système de César est aisée (un indice : les propriétés statistiques de la langue, en français, le e est la lettre la plus fréquente), des systèmes beaucoup plus résistants ont vu le jour. Certains ont résisté longtemps, celui de Vigenère (Le traité des secrètes manières d'écrire 1586) par exemple, n'ayant été cassé par Charles Babbage qu'au milieu du XIXe siècle. D'autres, bien que n'ayant pas de faille exploitable, ne sont plus utilisés car ils sont à la portée des puissances de calcul modernes. C'est le cas du DES avec sa clef de 56 bits jugée trop courte car elle peut être trouvée par recherche exhaustive (force brute).Dans un bestiaire de la cryptanalyse, il faudrait presque passer chaque système en revue — non seulement chaque système, mais aussi chaque mise en œuvre : à quoi sert la meilleure porte blindée si le mur qui la soutient est en contreplaqué ? Cela dit, si l'on veut vraiment citer quelques techniques, on a :la cryptanalyse différentielle, Biham et Shamir (le S de RSA), 1991, systèmes symétriques ;la cryptanalyse linéaire, Matsui, 1994, systèmes symétriques ;la factorisation, seul vrai moyen de déchiffrer RSA à l'heure actuelle ;la force brute, c'est-à-dire l'essai systématique de toutes les clefs possibles ;et d'autres encore.La confidentialité n'est que l'une des facettes de la cryptologie. Elle permet également :l'authentification ou l'authentification forte d'un message : l'assurance qu'un individu est bien l'auteur du message chiffré ;la non-répudiation est le fait de s'assurer qu'un contrat ne peut être remis en cause par l'une des parties ;l'intégrité : on peut vérifier que le message n'a pas été manipulé sans autorisation ou par erreur ;la preuve à divulgation nulle de connaissance — par exemple d'identité —, on peut prouver que l'on connaît un secret sans le révéler ;et autres, dont l'anonymat et la mise en gage.Pour l'essentiel, c'est la cryptographie à clef publique qui fournit les bases nécessaires à ces aspects de la cryptologie.La cryptologie a très longtemps été considérée comme une arme de guerre. Au IVe siècle av. J.-C., Énée le Tacticien, un général grec, y consacre un chapitre dans Commentaires sur la défense des places fortes. On peut aussi citer le siège de la Rochelle, où Antoine Rossignol (1600 - 1682) décrypte les messages que les huguenots assiégés tentent de faire sortir. Richelieu y apprend ainsi que les huguenots sont affamés et attendent la flotte anglaise. Celle-ci trouvera à son arrivée la flotte française, prête au combat, ainsi qu'une digue bloquant l'accès au port.Autre exemple, la Première Guerre mondiale, où le Room 40 — service du chiffre britannique — s'illustre tout particulièrement en décryptant un télégramme envoyé en janvier 1917 de Berlin à l'ambassadeur allemand à Washington, qui devait le retransmettre au Mexique. Ils apprennent ainsi que l'Allemagne va se lancer dans une guerre sous-marine totale et demande une alliance militaire, devant permettre au Mexique de récupérer le Nouveau-Mexique, le Texas et l'Arizona. Les Britanniques pouvaient transmettre directement ces renseignements aux États-Unis, mais ils auraient ainsi révélé aux Allemands l'interception et la mise à jour de leur code. Ils préfèrent donc envoyer un espion récupérer le message destiné aux Mexicains, faisant ainsi croire à une fuite côté Mexique. Le télégramme en clair se retrouve publié dans les journaux américains le 1er mars 1917. À la suite de cela, le président Wilson n'a pas de mal à obtenir l'accord du congrès, les États-Unis entrent en guerre.Ces exemples illustrent bien pourquoi les gouvernements sont prudents quant à l'utilisation de moyen cryptographique. Philip Zimmermann en a fait l'expérience lorsqu'il a mis à disposition son logiciel de messagerie sécurisée, Pretty Good Privacy (PGP), en 1991. Violant les restrictions à l'exportation pour les produits cryptographiques, PGP a été très mal accueilli par le gouvernement américain qui a ouvert une enquête en 1993 — abandonnée en 1996, peu avant que le gouvernement Clinton ne libéralise grandement, à l'aube de l'ère du commerce électronique, l'usage de la cryptographie.En France, depuis la loi pour la confiance dans l'économie numérique (LCEN), l'usage de la cryptologie est libre. Néanmoins, l'article 132-79 du code pénal prévoit que lorsqu'un moyen de cryptologie a été utilisé pour préparer ou commettre un crime ou un délit, ou pour en faciliter la préparation ou la commission, le maximum de la peine privative de liberté encourue est relevé.Les dispositions pénales ne sont toutefois pas applicables à l'auteur ou au complice de l'infraction qui, à la demande des autorités judiciaires ou administratives, leur a remis la version en clair des messages chiffrés ainsi que les conventions secrètes nécessaires au déchiffrement.Des logiciels de chiffrement avec une fonction de déni plausible permettent d'échapper à l'aggravation des peines (ex : FreeOTFE et TrueCrypt). Ouvrages historiques Cours de cryptographie du général Marcel Givierge, 1925Éléments de cryptographie du commandant Roger Baudouin, 1939 Ouvrages contemporains Simon Singh (trad. Catherine Coqueret), Histoire des codes secrets [« The Code Book »], Librairie générale française (LFG), coll. « Le Livre de Poche », 3 septembre 2001, 504 p., Poche (ISBN 2-253-15097-5, ISSN 0248-3653, OCLC 47927316)Gilles Zémor, Cours de cryptographie, Paris, Cassini, 15 décembre 2000, 227 p. (ISBN 2-84225-020-6, OCLC 45915497)L'Art du secret, Pour la science, dossier hors-série, juillet-octobre 2002.La Guerre des codes secrets, de D. Kahn, Interéditions, 1980 (trad. de The Codebreakers)(en) Handbook of Applied Cryptography, de A. J. Menezes, P. C. van Oorschot et S. A. Vanstone, CRC Press, 1996, en ligneLe Décryptement de A. Muller, PUF, 1983 (cryptanalyse des systèmes « traditionnels »)Les Écritures secrètes de A. Muller, PUF, 1971 (présentation des systèmes « traditionnels »)Bruce Schneier (trad. de l'anglais par Laurent Viennot), Cryptographie appliquée [« Applied cryptography »], Paris, Vuibert, coll. « Vuibert informatique », 15 janvier 2001, 846 p., Broché (ISBN 2-7117-8676-5, ISSN 1632-4676, OCLC 46592374)Douglas Stinson (trad. de l'anglais par Serge Vaudenay, Gildas Avoine, Pascal Junod), Cryptographie : Théorie et pratique [« Cryptography : Theory and Practice »], Paris, Vuibert, coll. « Vuibert informatique », 28 février 2003, 337 p., Broché (ISBN 2-7117-4800-6, ISSN 1632-4676, OCLC 53918605)Jacques Stern, La science du secret, Paris, Odile Jacob, coll. « Sciences », 5 janvier 1998, 203 p. (ISBN 2-7381-0533-5, OCLC 38587884, lire en ligne)Cryptologie, une histoire des écritures secrètes des origines à nos jours de Gilbert Karpman, éditions Charles Lavauzelle 2006Codage, cryptologie et applications de Bruno Martin, éditions PPUR 2004Théorie des codes (Compression, chiffrement, correction) de J.-G. Dumas, J.-L. Roch, E. Tannier et S. Varrette, éditions Dunod 2007Pierre Barthélemy, Robert Rolland et Pascal Véron (préf. Jacques Stern), Cryptographie : principes et mises en œuvre, Paris, Hermes Science Publications : Lavoisier, coll. « Collection Informatique », 22 juillet 2005, 414 p., Broché (ISBN 2-7462-1150-5, ISSN 1242-7691, OCLC 85891916)Ars Cryptographica Étude des messages secrets de l'Antiquité à nos jours, et cours de cryptologieCours et logiciels en téléchargementFAQ sur la longueur des clefsAlgorithmes sans limitation de longueur de clefFAQ en français du forum Usenet sci.cryptForum Usenet francophone consacré à la cryptologieExemple de nombreux messages chiffrés réels présents sur des groupes de discussions Usenet(fr) ACrypTA, cours, exercices, textes, liens concernant la cryptologie Portail de la cryptologie   Portail de la sécurité de l’information"
informatique;"Un cybercrime est une « infraction pénale susceptible de se commettre sur ou au moyen d’un système informatique généralement connecté à un réseau ».Il s’agit donc d’une nouvelle forme de criminalité et de délinquance qui se distingue des formes traditionnelles en ce qu’elle se situe dans un espace virtuel, le « cyberespace ». Depuis quelques années la démocratisation de l’accès à l’informatique et la globalisation des réseaux ont été des facteurs de développement du cybercrime.La cybercriminalité regroupe trois types d’infractions :les infractions spécifiques aux technologies de l’information et de la communication : parmi ces infractions, on recense les atteintes aux systèmes de traitement automatisé de données, les traitements non autorisés de données personnelles (comme la cession illicite des informations personnelles), les infractions aux cartes bancaires, les chiffrements non autorisés ou non déclarés ou encore les interceptions ;les infractions liées aux technologies de l’information et de la communication : cette catégorie regroupe la pédopornographie, l’incitation au terrorisme et à la haine raciale sur internet, les atteintes aux personnes privées et non aux personnages publics, les atteintes aux biens ;les infractions facilitées par les technologies de l’information et de la communication, que sont les escroqueries en ligne (cyberarnaques), le blanchiment d'argent, la contrefaçon ou toute autre violation de propriété intellectuelle.En France la cybercriminalité est prise juridiquement en compte depuis la loi informatique et libertés (loi relative à l'informatique, aux fichiers et aux libertés du 6 janvier 1978).La loi Godfrain du 5 février 1988 relative à la fraude informatique a introduit les articles 323-1 et suivants dans le Code pénal, concernant notamment la suppression ou modification de données (art 323-1 al 1), ou encore la tentative d’infraction sur un STAD (323-7).La loi du 15 novembre 2001 relative à la sécurité quotidienneLa loi du 18 mars 2003 pour la sécurité intérieureLa loi du 9 mars 2004 portant adaptation de la justice aux évolutions de la criminalitéLa loi pour la confiance dans l'économie numérique du 21 juin 2004, qui a modifié les articles 323-1 et suivant du Code pénal. Cette loi a, en outre, modifié l’article 94 du Code de procédure pénale relatif à l’inclusion des données informatiques dans la liste des pièces susceptibles d'être saisies lors des perquisitions réalisées en flagrant délit ou au cours d'une instruction (ces perquisitions sont aussi régies par les art. 56 et 97 du Code de procédure pénale).La loi du 9 juillet 2004 relative aux communications électroniques et aux services de communication audiovisuelle.La Loi du 23 janvier 2006 relative à la lutte contre le terrorisme et comportant diverses dispositions relatives à la sécurité et aux contrôles frontaliers.La loi du 5 mars 2007 relative à la prévention de la délinquancePar ailleurs de nombreux textes réglementaires ont été adoptés. On peut citer pour exemple le décret du 24 mars 2006 sur la conservation des données de trafic prévu par la loi relative à la sécurité quotidienne.La lutte contre la cybercriminalité est en pleine évolution et elle fait l’objet de nombreuses réflexions en France. Par exemple le plan de lutte contre la cybercriminalité qui a été présenté en février 2008 contient des mesures visant à moderniser les méthodes d’investigation. Par ailleurs, la même année, au mois d’octobre a été présenté le plan du numérique 2012 qui contient des propositions relatives à la lutte contre le cybercrime.Malgré cette évolution permanente le dispositif législatif français en matière de cybercriminalité est « éparpillé » dans divers textes. Il est donc peu aisé, autant pour les professionnels que pour les profanes, de connaître avec précision ce qui est aujourd’hui reconnu comme un acte cybercriminel par le droit français. Myriam Quéméner et Joël Ferry, dans Cybercriminalité Défi Mondial (2e édition) décrivent le dispositif législatif et réglementaire français comme un « ‘maquis’ quelque peu ésotérique ».Le 23 novembre 2001 les pays membres du Conseil de l'Europe ainsi que les États-Unis, le Canada, le Japon et l'Afrique du Sud, ont adopté la convention sur la cybercriminalité, aboutissement d'un long processus de négociations (vingt-sept versions antérieures et quatre années de négociations officielles). Il s'agit d'une convention pénale à vocation internationale destinée à lutter contre le cybercrime. En 2007, seuls quatorze États avaient ratifié la convention sur les quarante-sept signataires.Par ailleurs en 2003, a été ouvert à la signature le protocole additionnel à la convention sur la cybercriminalité, qui visait à élargir le champ d'application de la convention aux infractions de propagande raciste ou xénophobe commis via les réseaux internet. Ce protocole, non ratifié par les États-Unis, prévoit par ailleurs des mesures facilitant l'extradition et l'entraide judiciaire.La France a ratifié ces deux textes par la loi no 2005-493 du 19 mai 2005 autorisant l'approbation de la Convention du Conseil de l'Europe sur la cybercriminalité et du protocole additionnel à cette Convention.La convention sur la cybercriminalité de 2001 poursuit trois objectifs déterminés :L'harmonisation des législations des États signataires ;La modernisation de ces législations, notamment en matière procédurale ;L'amélioration de la coopération internationale en matière d'extradition et d'entraide répressive.Le premier axe est l'harmonisation des législations nationales en ce qui concerne la définition des infractions répertoriées par la Convention. Il s'agit donc d'incriminer quatre séries d'infractions qui sont :Les infractions informatiques : falsification et fraude informatique ;Les infractions de contenu : la pornographie enfantine. Le protocole additionnel inclut la propagation via Internet d'idées racistes et xénophobes ;Les infractions liées aux atteintes à la propriété intellectuelle et aux droits connexes : le partage non autorisé via Internet des œuvres protégées ;Les infractions contre la confidentialité, l'intégrité et la disponibilité des données et systèmes : accès illégal, interception illégale, atteinte à l'intégrité des données ou des systèmes.Ensuite, le deuxième axe, d'ordre procédural, définit les moyens d'enquêtes et de poursuites pénales les mieux adaptés à la mondialisation du réseau internet. La Convention prévoit des règles pour garantir les droits des individus, mais aussi pour faciliter la conduite d'enquête. En ce sens, on peut citer, entre autres, les règles régissant la conservation des données stockées, la conservation et la divulgation rapide des données relatives au trafic, la perquisition des systèmes informatiques, la saisie de données informatiques, la collecte en temps réel des données relatives au trafic et l'interception de données relatives au contenu.Enfin, le troisième axe concerne la mise en place d'un système rapide et efficace de coopération internationale. À côté des formes traditionnelles de coopération pénale internationale, prévues notamment par les Conventions européennes d'extradition et d'entraide judiciaire, la Convention sur la cybercriminalité prévoit des formes d'entraide correspondant aux pouvoirs définis préalablement par la Convention. Ces conditions sont exigées afin que les autorités judiciaires et les services de police d'un État membre puissent agir pour le compte d'un autre État dans la recherche de preuves électroniques, sans toutefois mener d'enquêtes ni de perquisitions transfrontalières. En outre, toute donnée obtenue devrait être rapidement communiqué à l'État intéressé.Sans doute, ce texte international — constitue un complément indispensable aux lois nationales pour contenir le phénomène de cette nouvelle criminalité « caméléon » dont on ne connaît pas encore - du moins avec certitude — toutes « les couleurs » et les menaces.Par ailleurs, le 17 janvier 2005 le Conseil de l'Union européenne a adopté la décision cadre 2005/222/JAI du Conseil « relative aux attaques visant les systèmes d'information », qui va permettre une harmonisation des règles pénales concernant les principales activités criminelles visant les systèmes d'information, l'atteinte à l'intégrité d'un système et l'atteinte à l'intégrité des données.Après les attaques de 2017 d’ampleur internationale (NotPetya et WannaCry notamment) le cyber risque est le second risque le plus craint par les entreprises du monde entier,.60 % des cyberattaques dans le monde en 2011 proviennent des États-Unis.En réponse à la cyber présumée espionnant sur des opposants aux meilleurs intérêts de l'Iran par le gouvernement iranien en 2010 et 2011, Les États-Unis ont aidé les Émirats arabes unis à la fin de 2011 avec la création de l'autorité nationale de la sécurité électronique (NESA) qui est l'équivalente des ÉAU à la NSA américaine. Project Raven Project Raven était une initiative confidentielle visant à aider les ÉAU visionnez d'autres gouvernements, militants et activistes des droits de l'homme. Son équipe comprenait d'anciens agents de renseignement américains, qui ont appliqué leur formation au piratage téléphonique et ordinateurs appartenant aux victimes du Projet Raven. L'opération était basée dans un manoir converti à Abou Dabi surnommé «la villa».CyberPoint fourni Projet Raven avec des entrepreneurs formés aux États-Unis d'environ 2014 à 2016. La réputation de Cyberpoint en tant que société de cybersécurité défensive a été terni en 2016 après que les nouvelles ont éclaté que la société avait travaillé avec l'équipe de piratage des groupes de logiciels espions italiens.Le 24 octobre 2016, un article de The Intercept a révélé la surveillance aux ÉAU. Le chef des finances de DarkMatter[Quoi ?], Samer Khalife, a transféré des citoyens américains de DarkMatter vers une nouvelle société appelée Systèmes de connexion et des équipes de Tiger ont été formées par DarkMatter pour contrer les allégations de l'article de The Intercept.Le FBI étudie les DarkMatter pour des crimes tels que l'espionnage numérique, la complicité de la mort de Jamal Khashoggi et la détention de dissidents d'outre-mer. Le FBI a également enquêté sur d'anciens employés américains de DarkMatter possiblement impliqués dans des cybercrimes.Le 14 septembre 2021, trois anciens officiers de renseignements américains, Marc Baiier, Ryan Adams et Daniel Gericke, recrutés par les ÉAU pour mener des cyberopérations sophistiquées admis au piratage des infractions et à la violation des règles d'exportation américaines interdisant le transfert de technologie militaire à d'autres gouvernements. En outre, ils sont convenus de remettre plus de 1,7 million de dollars et leurs autorisations de sécurité américaine en échange d'une restriction de leur travail futur et de « coopérer pleinement» avec des enquêteurs »[pas clair],,. Cas d'espionnage saoudien Ahmad Abouammo, un citoyen américain et libanais et ancien employé de Twitter, et Ali Alzabarah, autre ancien employé de Twitter, ont été approchés par Riyad en 2014 et 2015 pour transférer des informations personnelles sur les utilisateurs. En novembre 2019, Abouammo a vendu des informations personnelles sur des utilisateurs anonymes à Riyad. En retour, il a reçu dix milles dollars et une montre de luxe. En novembre 2019, il a été arrêté à Seattle. Après un procès de deux semaines devant le tribunal fédéral de San Francisco, il a été reconnu coupable de blanchiment d'argent, de complot en vue de commettre une fraude télégraphique, de falsification de dossiers et d'être agent pour l'Arabie saoudite, et a été condamné à 10 à 20 ans de prison. Le verdict est intervenu après que les défenseurs des droits de l'homme ont critiqué Joe Biden et Emmanuel Macron pour leur approche diplomatique du prince héritier Mohammed ben Salmane[C'est-à-dire ?], qui a été exclu de la scène internationale à la suite du meurtre du journaliste saoudien Jamal Khashoggi en Turquie en 2018. En outre, le prince héritier et son gouvernement sont fréquemment accusés, par les ONG d'espionnage[Quoi ?], d'enlèvement et de torture de dissidents ; Riyad réfute vigoureusement ces affirmations.La cybercriminalité est reconnue par beaucoup d'experts comme étant la nouvelle forme de criminalité du XXIe siècle. Dès lors, pour la contrôler, la France a mis en place de nombreux organes de lutte. Voici quelques exemples de cyber-investigation.Dès 1998, a été créé, au sein de la gendarmerie, le département de lutte contre la cybercriminalité au sein du service technique de recherches judiciaires et de documentation (STRJD, devenu SCRC). Celle cellule a évolué et est devenue la Division de lutte contre la cybercriminalité (DLCC) composée du Département coordination et appuis numériques (DCAN), du Département investigations sur Internet (D2I), du Département prévention et suivi des phénomènes sur Internet (DPSPI) et du Département répression des atteintes aux mineurs sur Internet (DRAMI) qui intègre le Centre national d'analyse des images de pédopornographie (CNAIP). Devenue un centre de lutte contre les criminalités numériques (C3N) en 2015, cette unité rejoint en 2021 le nouveau commandement de la gendarmerie dans le cyberespace (ComCyberGend).Le 15 mai 2000 a été créé l'Office central de lutte contre la criminalité liée aux technologies de l'information et de la communication (OCLCTIC), au sein de la direction centrale de la police judiciaire au Ministère de l'Intérieur. Elle regroupe notamment en son sein la plate-forme de signalement des contenus illicites sur internet. Cette même année, en complément de l'action de l'OCLCTIC, a été mise en place, la direction de la Surveillance du territoire (DST), qui est compétente pour diligenter des enquêtes judiciaires relatives à des actes de piratage sur les systèmes informatiques des établissements à régime restrictif ou des données classifiées de défense.Par ailleurs, en 2006 a été créé l'OCRVP, office central pour la répression des violences aux personnes, dont la mission est la coordination, sur le plan national, de la lutte contre les infractions violentes à l'encontre des personnes, notamment concernant la pédopornographie sur internet.Enfin, la police nationale dispose de services spéciaux comme le SITT service de l'informatique et des traces technologiques. Les directions inter régionales et régionales de police judiciaire disposent d'ICC (Investigateurs en CyberCriminalité) anciennement dénommés ESCI (Enquêteurs Spécialisés en Criminalité Informatique). Il existe, en outre, différentes brigades spécialisées, telle la Brigade d'enquêtes sur les fraudes aux technologies de l'information (BEFTI).Le 30 juin 2014, le magistrat Marc Robert remet son rapport à Bernard Cazeneuve, Axelle Lemaire, Arnaud Montebourg et Christiane Taubira, pour mettre en place des mesures juridiques et techniques visant à freiner les risques liés à la cybersécurité et améliorer la protection des internautes. Marc Robert prône la création d'un Centre d'Alerte, l'ouverture d'un 17 de l'internet, la mise en place d'une Délégation interministérielle à la lutte contre la cybercriminalité placée sous la responsabilité directe du Premier ministre, etc..Les États ont rapidement compris que pour être plus efficace la lutte contre la cybercriminalité devait être européenne. Des compétences dans ce domaine ont alors été rapidement confiées à INTERPOL dont le rôle est la facilitation d’échange de renseignements afin de lutter efficacement contre toute forme de criminalité et notamment la criminalité informatique.Europol est aussi compétent en ce qui concerne la facilitation d’échanges de renseignements entre polices nationales notamment en matière de cybercriminalité. L'Union européenne (UE) a établi un Centre européen de lutte contre la cybercriminalité au sein d'Europol : EC3 (European Cybercrime Centre). L'EC3 est compétent pour soutenir les enquêtes des services spécialisés des États membres de l'UE dans des domaines tels que toutes fraudes en ligne en particulier la fraude à la carte de crédit, l'exploitation sexuelle des enfants en ligne (pédopornographie sur internet), les cyberattaques contre les systèmes d'infrastructures critiques de l'UE. L'EC3 apporte également un soutien en termes d'analyse criminelle stratégique aux États-membres notamment en produisant des analyses de la menace thématiques sur les dernières tendances en matière de cybercrime.EUROJUST, organe de l’Union européenne, a pour compétence l’amélioration de l’efficacité des autorités compétentes des états membres dans la lutte contre la criminalité organisée transfrontalière, donc notamment la cybercriminalité transnationale.Par ailleurs, a été créée en 2004 l’ENISA, agence européenne chargée de la sécurité des réseaux et de l’information, qui a diverses missions, dont notamment le recueil et l’analyse des données relatives aux incidents liés à la sécurité, ou encore le suivi de l’élaboration des normes pour les produits et services en matière de sécurité de réseaux et de l’information, mais aussi la promotion d’activités d’évaluation et de gestion des risques.Enfin, il existe le programme européen Safer internet plus qui lutte contre les contenus illicites, le traitement des contenus non désirés et préjudiciables, et qui fait la promotion d’un environnement plus sûr.Malheureusement la lutte contre la cybercriminalité n’est pas aisée. Il existe plusieurs obstacles juridiques et non juridiques à cette lutte. En premier lieu, le caractère vaste des réseaux informatiques, mais aussi la rapidité de commission des infractions, la difficulté de rassembler des preuves, et enfin des méthodes d’investigation et de contrôle qui peuvent se révéler attentatoires aux droits fondamentaux, en particulier au droit à l’anonymat et à la liberté d’expression.Au niveau juridique, ce qui pose aujourd’hui beaucoup de difficultés c’est le fait qu’un même comportement en France et à l’étranger n’est pas pareillement considéré. Il peut constituer une infraction dans un pays et pas dans l’autre. On peut citer pour exemple, la « promotion du cannabis », ou encore la « provocation pour surprendre les pédophiles ». Cela renvoie à un autre problème celui de la loi applicable. En effet, la cybercriminalité « bouleverse le principe classique de la territorialité de la loi pénale ». La loi française sera applicable dès lors qu’un élément constitutif de l’infraction a eu lieu en France (TGI de Paris 17e chambre, 26 février 2002). Ainsi, par exemple, la simple réception par l’utilisateur est un élément constitutif de l’infraction. Mais s’il n’y a pas d’élément constitutif de l’infraction en France, la loi française ne sera pas applicable.Il faut alors lutter chaque jour contre les paradis juridiques « cyber paradis », pour une meilleure efficacité du droit relatif à la cyber criminalité.Pour Jean-Loup Richet (Research Fellow à l'ESSEC ISIS), une autre difficulté dans la lutte contre la cybercriminalité est la rapide diffusion de nouvelles techniques de hacking, la réduction des coûts de l'activité criminelle et enfin la réduction des connaissances requises pour devenir un cybercriminel. En effet, les barrières à l'entrée n'ont jamais été aussi réduites : les services offerts par les plateformes de cloud computing peuvent être détournés pour lancer des campagnes de spam à moindre coûts, cracker un mot de passe voire augmenter la puissance d'un botnet. Selon Jean-Loup Richet, plus besoin d'être un expert en informatique pour devenir un cybercriminel : les communautés de hackers black hat commercialisent des logiciels permettant à leurs utilisateurs de mener des cyber attaques sans aucune compétence technique (Crimeware-as-a-service). Les communautés en ligne de cybercriminels contribuent au développement du cybercrime, fournissant des astuces, techniques, outils clefs en main et proposant même dans certains cas du tutorat de débutants désireux de devenir des cybercriminels.Selon la Revue française de criminologie et de droit pénal, la difficulté de la lutte contre la cybercriminalité réside également dans l’ambiguïté du cadre de régulation. Si le but d'une agression informatique est le système informatique de l'adversaire alors ce système peut-être assimilé à l'adversaire lui-même. La question est donc de savoir s'il faut établir un encadrement légal entre les machines et leurs propriétaires pour identifier ces actes criminels.Le coût de la cybercriminalité étant difficile à évaluer, des chiffres divers sont donnés.Selon deux études menées par le FBI et IBM en 2006, la cybercriminalité coûterait 67 milliards de dollars par an, rien qu'aux États-Unis.Selon le chef d'Interpol Khoo Boon Hui, 80 % de la cybercriminalité est liée en 2012 à des bandes organisées transfrontalières et représente un coût financier (750 milliards d'euros par an en Europe) plus important que les coûts combinés des trafics de cocaïne, marijuana et héroïne. Selon le rapport du Center for Strategic and International Studies (CSIS) de l'éditeur en sécurité McAfee, les activités cybercriminelles coûteraient entre 375 et 575 milliards de dollars par an.Le cybercrime et le piratage ont lourdement pesés sur les ventes du jeu The Witness, de Jonathan Blow, à sa sortie, au point que son créateur avoua que cela risquerait fortement de le limiter pour la création d'un nouveau jeu par la suite .« Chaque année, plus de 26 millions de Français sont victimes de cybercrimes, dont 9,17 millions subissent une perte financière nette. Au-delà des particuliers, les entreprises françaises sont de plus en plus ciblées, pour un dommage de 8,7 millions d’euros » en 2019.Roman d'anticipationLouis Charbonneau, Le Grand Ordinateur (Intruder) 1982 ;Élise Fontenaille, Unica, Paris, éditions Stock, 2006 — polar d'anticipation autour de la cyber-pédophilie, prix du Lundi ou grand prix de la Science-Fiction Française 2007, prix Rosny aîné 2008.Le hacking ou la cybercriminalité sont les sujets, principaux ou pas, de nombreux films, comme :1995 : Traque sur Internet (The Net), d'Irwin Winkler ;2000 : Cybertraque2007 : Die Hard 4 : Retour en enfer (Live Free or Die Hard) de Len Wiseman ;2015 : Hacker (Blackhat) de Michael Mann.Mr. RobotEye CandyPierre Penalba et Abigaelle Penalba, Cyber crimes. Un flic 2.0 raconte, Albin Michel, 2020, 288 p. (lire en ligne)Myriam Quéméner et Jean-Paul Pinte, Cybersécurité des acteurs économiques : Risques, réponses stratégiques et juridiques, Hermes Science Publications, coll. « Cyberconflits et cybercriminalité », 13 décembre 2012, 274 p. (ISBN 978-2-7462-3915-9)Éric Freyssinet, La cybercriminalité en mouvement, Cachan, Hermes Science Publications, coll. « Management et informatique », 27 septembre 2012, 240 p. (ISBN 978-2-7462-3288-4)Myriam Quéméner et Christian Aghroum, Etablissements financiers & cyberfraudes, Paris, La Revue Banque, 6 juin 2011, 127 p. (ISBN 978-2-86325-563-6)Myriam Quéméner et Yves Charpenel, Cybercriminalité : droit pénal appliqué, Paris, Economica, 13 septembre 2010, 272 p. (ISBN 978-2-7178-5902-7)Mohamed Chawki, Combattre la cybercriminalité, Perpignan, Editions de Saint-Amans, 15 mai 2009, 458 p. (ISBN 978-2-35941-002-0)Myriam Quéméner et Joël Ferry, Cybercriminalité : Défi mondial et réponses - 2e édition, Perpignan, Economica, 9 mars 2009, 308 p. (ISBN 978-2-7178-5700-9)Myriam Quéméner, Cybermenaces, Entreprises et Internautes, Paris, Economica, 1er novembre 2008, 274 p. (ISBN 978-2-7178-5642-2)Jean-Loup Richet, From Young Hackers to Crackers. International Journal of Technology and Human Interaction (IJTHI), 2013, 9(3), 53-62.Les infractions commises sur Internet, Abbas JABER, Thèse, Université de Bourgogne, France, novembre 2007.Le business de la cybercriminalité, Rodolphe Monnet et Franck Franchin, Hermès - Lavoisier, avril 2005.Le droit penal à l’épreuve de la cybercriminalité, Mohamed Chawki, Thèse, Université Lyon III, France, septembre 2006.Convention sur la cybercriminalitéCyberattaqueCyberguerreCybersécuritéCybercriminalité au CanadaForum international de la cybersécurité (FIC)Internet Crime Complaint CenterKevin MitnickIngénierie socialeSécurité des systèmes d'informationInternet Crimes Against Children Task Force (en)PédopornographieDépartement de la Justice des États-UnisOffice of Juvenile Justice and Delinquency Prevention (en)Force opérationnelleUn blog consacré à la cybercriminalitéL'actualité de la piraterie dans tous les domainesEssai sur la notion de cybercriminalitéLe vol d'informationsLes enjeux des fichiers cookiesL'anonymat dans le cyberespaceLa Fraude 419Le vol d'identité via le cyberespace Portail du droit   Portail de la sécurité informatique"
informatique;"La sécurité des systèmes d’information (SSI) ou plus simplement sécurité informatique, est l’ensemble des moyens techniques, organisationnels, juridiques et humains nécessaires à la mise en place de moyens visant à empêcher l'utilisation non autorisée, le mauvais usage, la modification ou le détournement du système d'information. Assurer la sécurité du système d'information est une activité du management du système d'information.Aujourd’hui, la sécurité est un enjeu majeur pour les entreprises ainsi que pour l’ensemble des acteurs qui l’entourent. Elle n'est plus confinée uniquement au rôle de l’informaticien. Sa finalité sur le long terme est de maintenir la confiance des utilisateurs et des clients. La finalité sur le moyen terme est la cohérence de l’ensemble du système d’information. Sur le court terme, l’objectif est que chacun ait accès aux informations dont il a besoin. La norme traitant des systèmes de management de la sécurité de l'information (SMSI) est l’ISO/CEI 27001 qui insiste sur Confidentiality – Integrity – Availability, c'est-à-dire en français disponibilité, intégrité et confidentialité.Les responsables de systèmes d'information se préoccupent depuis longtemps de sécuriser les données. Le cas le plus répandu, et sans aucun doute précurseur en matière de sécurité de l'information, reste la sécurisation de l'information stratégique et militaire. Le Department of Defense (DoD) des États-Unis est à l'origine du TCSEC, ouvrage de référence en la matière. De même, le principe de sécurité multi-niveau trouve ses origines dans les recherches de résolution des problèmes de sécurité de l'information militaire. La défense en profondeur, tout droit sorti d'une pratique militaire ancienne, et toujours d'actualité aujourd'hui. Cette pratique consiste à sécuriser chaque sous-ensemble d'un système.Les conséquences d'une mauvaise sécurisation peuvent concerner les organisations, mais aussi la vie privée d'une ou plusieurs personnes, notamment par la diffusion d'informations confidentielles comme leurs coordonnées bancaires, leurs situations patrimoniales, leurs codes confidentiels, etc. De manière générale, la préservation des données relatives aux personnes fait l'objet d'obligations légales régies par la Loi Informatique et Libertés.Aujourd'hui, il est généralement admis que la sécurité ne peut être garantie à 100 % et requiert donc le plus souvent la mobilisation d'une panoplie de mesures pour réduire les chances de pénétration des systèmes d'information.« Le système d'information représente un patrimoine essentiel de l'organisation, qu'il convient de protéger. La sécurité informatique consiste à garantir que les ressources matérielles ou logicielles d'une organisation sont uniquement utilisées dans le cadre prévu ».La sécurité des systèmes d'information vise les objectifs suivants (C.A.I.D.) :Confidentialité : seules les personnes autorisées peuvent avoir accès aux informations qui leur sont destinées (notions de droits ou permissions). Tout accès indésirable doit être empêché.Authenticité : les utilisateurs doivent prouver leur identité par l'usage de code d'accès. Il ne faut pas mélanger identification et authentification : dans le premier cas, l'utilisateur n'est reconnu que par son identifiant public, tandis que dans le deuxième cas, il doit fournir un mot de passe ou un élément que lui-seul connaît (secret). Mettre en correspondance un identifiant public avec un secret est le mécanisme permettant de garantir l'authenticité de l'identifiant. Cela permet de gérer les droits d'accès aux ressources concernées et maintenir la confiance dans les relations d'échange.Intégrité : les données doivent être celles que l'on attend, et ne doivent pas être altérées de façon fortuite, illicite ou malveillante. En clair, les éléments considérés doivent être exacts et complets. Cet objectif utilise généralement des méthodes de calcul de checksum ou de hachage.Disponibilité : l'accès aux ressources du système d'information doit être permanent et sans faille durant les plages d'utilisation prévues. Les services et ressources sont accessibles rapidement et régulièrement.D'autres aspects peuvent aussi être considérés comme des objectifs de la sécurité des systèmes d'information, tels que :La traçabilité (ou « preuve ») : garantie que les accès et tentatives d'accès aux éléments considérés sont tracés et que ces traces sont conservées et exploitables.La non-répudiation et l'imputation : aucun utilisateur ne doit pouvoir contester les opérations qu'il a réalisées dans le cadre de ses actions autorisées et aucun tiers ne doit pouvoir s'attribuer les actions d'un autre utilisateur.Une fois les objectifs de la sécurisation déterminés, les risques pesant sur chacun de ces éléments peuvent être estimés en fonction des menaces. Le niveau global de sécurité des systèmes d'information est défini par le niveau de sécurité du maillon le plus faible. Les précautions et contre-mesures doivent être envisagées en fonction des vulnérabilités propres au contexte auquel le système d'information est censé apporter service et appui.Il faut pour cela estimer :La gravité des conséquences au cas où les risques se réaliseraient ;La vraisemblance des risques (ou leur potentialité, ou encore leur probabilité d'occurrence).Pour sécuriser les systèmes d'information, la démarche adopte une spirale évolutive régulière : la fin d'un cycle entraîne le début d'un nouveau, comme dans la roue de Deming. En sécurité cela consiste à :évaluer les risques et leur criticitéquels risques et quelles menaces, sur quelles données et quelles activités, avec quelles conséquences ?  On parle de « cartographie des risques ». De la qualité de cette cartographie dépend la qualité de la sécurité qui va être mise en œuvre.rechercher et sélectionner les paradesque va-t-on sécuriser, quand et comment ?  Étape difficile des choix de sécurité : dans un contexte de ressources limitées (en temps, en compétences et en argent), seules certaines solutions pourront être mises en œuvre.mettre en œuvre les protections et vérifier leur efficacitéC'est l'aboutissement de la phase d'analyse et là que commence la protection du système d'information. Une faiblesse fréquente de cette phase est d'omettre de vérifier que les protections sont bien efficaces (tests de fonctionnement en mode dégradé, tests de reprise de données, tests d'attaque malveillante, etc.). Étape 1 : Périmètre et Politique Il est important de prendre en compte les actifs ayant de la valeur en définissant un périmètre du système de management du système d’information. Il peut être orienté sur l’ensemble de l’entreprise, sur un site précis, sur un service en fonction de la stratégie de l’entreprise. Le capital intellectuel des entreprises intègre des informations sensibles, ce patrimoine informationnel doit être protégé. L’entreprise doit donc mettre en place une politique de sécurité des systèmes d’information, de sécurité des données, et des mécanismes d’identification. De plus, il faut définir une politique du SMSI, qui est l’engagement de l’entreprise sur un certain nombre de points en matière de sécurité. Ces deux points forment la pierre angulaire du SMSI, dans le but d’établir la norme ISO/CEI 27001 et ainsi d’apporter la confiance aux parties prenantes. Étape 2 : Évaluation des risques Tenter de sécuriser un système d'information revient à essayer de se protéger contre les menaces intentionnelles et d'une manière plus générale contre tous les risques pouvant avoir une influence sur la sécurité de celui-ci ou des informations qu'il traite. Méthode d'analyse des risques Différentes méthodes d'analyse des risques sur le système d'information existent. Voici les méthodes d’appréciation des risques les plus courantes :En France, la première méthode développée a été Marion. Aujourd’hui, elle a été remplacée, même si certaines entreprises ont conservé ce modèle initial, par la méthode Méhari (Méthode harmonisée d'analyse des risques) développée par le CLUSIF, et par la méthode EBIOS (Expression des besoins et identification des objectifs de sécurité) développée par l'Agence nationale de la sécurité des systèmes d'information (ANSSI).En Angleterre, Cramm est une méthode d'analyse des risques développée par l'organisation du gouvernement britannique ACTC (Agence centrale de communication et des télécommunications). C’est la méthode d'analyse des risques préférée par le gouvernement britannique, mais elle est également utilisée par beaucoup d’autre pays.Les États-Unis utilisent OCTAVE (Operationally Critical Threat, Asset, and Vulnerability Evaluation), développée par l'Université de Carnegie Mellon.À l'international, on utilise ISO/CEI 27005, qui est une norme internationale répondant point par point aux exigences de la certification ISO/CEI 27001. C’est la norme la plus récente, de plus elle est facilement applicable car pragmatique.Même si le but de ces méthodes est identique, les termes et les expressions utilisés peuvent varier. Celles utilisées ci-dessus sont globalement inspirés de la méthode Feros.Paradoxalement, dans les entreprises, la définition d'indicateurs « sécurité du SI » mesurables, pertinents et permettant de définir ensuite des objectifs dans le temps raisonnables à atteindre, s'avère délicate. Pour mesurer la performance on peut désigner comme indicateurs les états d'installation d'outils ou de procédures, mais les indicateurs de résultats sont plus complexes à définir et à apprécier, par exemple ceux concernant les « alertes virales »[évasif]. Identifier les actifs Cela consiste à faire une liste de tous les éléments importants en matière d’information au sein du périmètre SMSI. Il existe différent types d'actifs :matérielphysiquelogicielhumaindocumentsimmatérielsPour l’identification des actifs, trois problèmes se posent :Le niveau de granularité : plus la notion de granularité est élevée plus la notion d’actif est large.Lister le plus important : le but n’est pas de faire une liste exhaustive d’actifs mais de recenser les plus importants d’entre eux.Ne pas confondre les actifs avec les actifs d’information : un actif est un élément qui possède de la valeur pour l’entreprise, alors qu'un actif d’information est ce qui possède de l’importance en matière d’information.Il est aujourd'hui indispensable de disposer de plans de sécurisation de l'activité pour en assurer la continuité et la reprise si un sinistre survient (Plan de reprise d'activité). Ces plans tentent de minimiser les pertes de données et d’accroître la réactivité en cas de sinistre majeur. Un plan de continuité d'activité efficace est quasi-transparent pour les utilisateurs, et garantit l'intégrité des données sans aucune perte d'information. Identifier les personnes responsables C’est la personne responsable d’un bien qui en répond. Il s’agit en général de celle qui connaît le mieux la valeur et les conséquences de disponibilité, d’intégrité et de confidentialité de l’actif. Dans une entreprise c’est généralement le responsable de la sécurité des systèmes d’information qui connaît le mieux les actifs de l’information. Identifier les vulnérabilités Chaque actif recensé présente des vulnérabilités, c’est une propriété intrinsèque du bien qui l’expose à des menaces. Identifier et modéliser les menaces Les vulnérabilités précédemment identifiées exposent les biens à des menaces. La norme ISO/CEI 27001 impose l’identification des menaces pour tous les biens recensés.Les principales menaces auxquelles un système d’information peut être confronté sont :un utilisateur du système : l'énorme majorité des problèmes liés à la sécurité d'un système d'information a pour origine un utilisateur, généralement insouciant. Il n'a pas le désir de porter atteinte à l'intégrité du système sur lequel il travaille, mais son comportement favorise le danger ;une personne malveillante : une personne parvient à s'introduire sur le système, légitimement ou non, et à accéder ensuite à des données ou à des programmes auxquels elle n'est pas censée avoir accès. Le cas fréquent est de passer par des logiciels utilisés au sein du système, mais mal sécurisés. Le Shoulder surfing est également une faille.un programme malveillant : un logiciel destiné à nuire ou à abuser des ressources du système est installé (par mégarde ou par malveillance) sur le système, ouvrant la porte à des intrusions ou modifiant les données ; des données confidentielles peuvent être collectées à l'insu de l'utilisateur et être réutilisées à des fins malveillantes ;un sinistre (vol, incendie, dégât des eaux) : une mauvaise manipulation ou une malveillance entraînant une perte de matériel et/ou de données. Identifier les conséquences La norme ISO 27001 oblige l’évaluation de conséquences ; tel que : la perte de confidentialité, de disponibilité ou d’intégrité. Cela revient à donner une note en trois dimensions (confidentialité ; disponibilité et intégrité), selon des critères définis, pour chaque actif. Identifier les dommages Quatre types de dommages peuvent affecter le système d'information d'une organisation:Les dommages financiers :Sous forme de dommages directs, c'est l'action de reconstituer des bases de données qui ont disparu, de reconfigurer un parc de postes informatiques ou de réécrire une application,Sous la forme de dommages indirects, c'est le dédommagement des victimes d'un piratage, le vol d'un secret de fabrication ou la perte de marchés commerciaux ;La perte de l'image de marque :Perte directe par la publicité négative faite autour d'une sécurité insuffisante tel que l'hameçonnage,Perte indirecte par la baisse de confiance du public dans une société, par exemple, les techniques répandues de défacement ;Les dommages réglementaires :L'indisponibilité d'un systèmes d'informations peut mettre en défaut l'entité devant ses obligations légales et juridiques ;Les dommages écologiques et/ou Sanitaires :La défaillance d'un système peut provoquer des catastrophes écologiques (ex. : AZF, marées noires, etc.),La défaillance d'un système peut provoquer des dégâts sanitaires (ex. : les centrales nucléaires, etc.). Évaluer la vraisemblance Il s’agit de remettre le bien d’information dans son contexte environnemental et donc de prendre en compte les mesures qui sont déjà mises en place (ex. : si un fichier client est déjà chiffré, alors la vraisemblance de voir sa confidentialité compromise est limitée). Il est possible d’évaluer la notion de vraisemblance par une note sur une échelle de 1 à 5. Estimer les niveaux de risque L’attribution d’une note finale reflétera le niveau de risque réel tout en tenant compte des éléments ci-dessus. La norme ISO 27001 n’impose aucune formule c’est donc à l’implémenteur de la choisir. Il peut s’agir d’une note allant de 0 à 100 ou d’un code couleur. Étape 3 : Traiter le risque et identifier le risque résiduel L’entreprise peut traiter les risques identifiés de 4 façons :Accepter le risquesolution ponctuelle lorsque la survenance du risque entraîne des répercussions acceptables pour l’entreprise.Éviter le risquesolution lorsque les conséquences d’une attaque sont jugées trop périlleuses pour l’entreprise.Transférer le risquesolution quand l’entreprise ne peut pas faire face au risque par ses propres moyens (souscription d’une assurance ou contrat de sous-traitance).Réduire le risquesolution pour rendre le risque acceptable.Enfin, il ne faut pas oublier de prendre en compte les « Risques résiduels » qui persistent après la mise en place de l’ensemble des mesures de sécurité. Il faut prendre des mesures complémentaires de protection pour rendre ces risques acceptables. Étape 4 : Sélectionner les mesures à mettre en place (Annexe A à ISO/CEI 27001) L'implémentation de la norme ISO2/CEI 27001 se déroule généralement en cinq phases complémentaires:Phase 1 Réunion de lancement : cette réunion sert à cadrer la prestation et à présenter la démarche des consultants.Phase 2 Entretiens : rencontres avec les différents responsables des services clé de l'entreprise dans le but de faire le point sur leur niveau de conformité avec la norme ISO/CEI 27001.Phase 3 Prise de connaissance de la documentation : documents de la politique générale (politique de sécurité, charte utilisateurs, etc.), documents de politique spécifiques (mots de passe, accès distant et procédure).Phase 4 Rédaction du rapport : rapport tenant compte de tous les éléments obtenus lors des phases précédentes.Phase 5 Présentation des résultats : réunion au cours de laquelle les points suivants sont traités ; rappel synthétique des points clé, présentation du plan de mise en conformité ISO/CEI 27001 et discussion. Plan de traitement des risques L’étape de planification identifie les mesures à prendre dans l’organisation, mais ne permet pas de les mettre en place concrètement. Il faut les organiser, sélectionner les moyens nécessaires et définir les responsabilités en établissant un plan de traitement des risques. Cette étape relève de la gestion de projet. Déployer les mesures de sécurité De nombreux moyens techniques peuvent être mis en œuvre pour assurer une sécurité du système d'information. Il convient de choisir les moyens nécessaires, suffisants, et justes. Voici une liste non exhaustive de moyens techniques pouvant répondre à certains besoins en matière de sécurité du système d'information :Contrôle des accès au système d'information ;Surveillance du réseau : sniffer, système de détection d'intrusion ;Sécurité applicative : séparation des privilèges, audit de code, rétro-ingénierie ;Emploi de technologies ad hoc : pare-feu, UTM, anti-logiciels malveillants (antivirus, anti-spam, anti-logiciel espion) ;Cryptographie : authentification forte, infrastructure à clés publiques, chiffrement.Plan de continuité d'activité : sauvegarde et restauration de données, Plan de Reprise d'activité. Générer des indicateurs Une des nouveautés de la norme ISO/CEI 27001 est d’exiger une vérification régulière de la sécurité. Le responsable doit choisir des indicateurs qui permettent de mesurer sa fiabilité. Ils peuvent être de deux sortes :indicateurs de performance : ils mesurent l’efficacité des mesures ;indicateurs de conformité : ils mesurent l’adéquation des mesures aux normes. Former et sensibiliser le personnel L’information du personnel est primordiale dans la réussite d’un projet de sécurisation du SI, pour qu’il en comprenne l’utilité et sache l’appliquer. Une bonne pratique est donc de sensibiliser l’ensemble du personnel aux enjeux de la sécurité informatique pour leur organisation, de manière généraliste. Cette explication doit rappeler les engagements de l’organisation, et donner des exemples très pratiques et des procédures internes pour éviter les incidents les plus habituels. Les employés directement concernés par la sécurité informatique doivent être formés pour qu’ils sachent utiliser correctement les outils.Une formation incluant l’inoculation psychologique contre les techniques d’ingénierie sociale, permet aux gens de résister aux tentations de s’écarter des procédures et des principes de sécurité. Gérer le SMSI au quotidien La norme ISO/CEI 27001 n’impose pas seulement de mettre en place un système de sécurité, mais aussi de prouver son efficacité. Les entreprises doivent donc gérer correctement leurs ressources et développer la traçabilité. Détection et réaction rapide des incidents Cette phase repose sur la théorie de time-based security. Le principe est de prendre en compte le délai nécessaire pour qu’une attaque contre la sécurité réussisse. Pendant ce laps de temps, l’entreprise doit être capable de détecter la menace et de la contrer, avec une marge de sécurité supplémentaire.Il doit y avoir des moyens de contrôle pour surveiller l’efficacité du SMSI ainsi que sa conformité.Il existe des outils pour vérifier cela comme :Les audits internes : Audit planifié longtemps en avance et faisant appel à des auditeurs.Le contrôle interne : Contrôle en permanence au sein de l’organisation, pour vérifier que chacun applique les procédures au quotidien.Les réexamens : Prendre du recul pour mettre en adéquation le SMSI et son environnement.On peut s’aider de :COBIT : permet l’analyse des risques et le contrôle des investissementsITIL : l’objectif est de favoriser l’efficacité des affaires dans l’utilisation du SI dans le but de satisfaire les demandes d’organisation pour réduire les coûts tout en maintenant ou améliorant les services informatiquesISO/CEI 27007 : lignes directrices pour aider les auditeurs internes ou externes à contrôler si le SMSI est correctement développé.Après la mise en lumière de dysfonctionnements grâce à la phase Check, il est important de les analyser et de mettre en place des :Actions correctives : Il faut agir sur le dysfonctionnement et en supprimer les effets.Actions préventives : On agit avant que le dysfonctionnement ne se produise.Actions d'amélioration : On améliore les performances d’un processus.Daniel Guinier, Sécurité et qualité des systèmes d'information : Approche systémique, Masson, 1992, 298 p. (ISBN 978-2-225-82686-3)Laurent Bloch et Christophe Wolfhugel, Sécurité informatique : Principes et méthode, Paris, Eyrolles, 2011, 325 p. (ISBN 978-2-212-13233-5, lire en ligne)Fernandez-Toro, Management de la sécurité de l'information. Implémentation ISO 27001 et audit de certification, Eyrolles, 2012Bernard Foray, La fonction RSSI (Responsable Sécurité Système d'Information) : Guide des pratiques et retours d'expérience - 2e édition, Dunod, 2011Laurent Bloch et Christophe Wolfhugel, Sécurité informatique : Principes et méthodes à l'usage des DSI, RSSI et administrateurs, Eyrolles, 2009, 292 p. (ISBN 978-2-212-12525-2, lire en ligne) Portail du management   Portail de la sécurité de l’information   Portail de la sécurité informatique"
informatique;""
informatique;""
informatique;""
informatique;"Le langage machine, ou code machine, est la suite de bits qui est interprétée par le processeur d'un ordinateur exécutant un programme informatique. C'est le langage natif d'un processeur, c'est-à-dire le seul qu'il puisse traiter. Il est composé d'instructions et de données à traiter codées en binaire.Chaque processeur possède son propre langage machine, dont un code machine qui ne peut s'exécuter que sur la machine pour laquelle il a été préparé. Si un processeur A est capable d'exécuter toutes les instructions du processeur B, on dit que A est compatible avec B. L'inverse n'est pas forcément vrai : A peut avoir des instructions supplémentaires que B ne connaît pas.Le code machine est aujourd'hui généré automatiquement, généralement par le compilateur d'un langage de programmation ou par l'intermédiaire d'un bytecode.Les « mots » d'un langage machine sont appelés instructions. Chacune d'elles déclenche une commande de la part du processeur (par exemple : chercher une valeur dans la mémoire pour charger un registre, additionner deux registres, etc.).Un processeur à architecture RISC ne reconnaît que peu d'instructions différentes, alors qu'un processeur à architecture CISC en possède un large éventail. Néanmoins certains processeurs CISC récents transforment en interne les instructions complexes en une suite d'instructions simples, qui sont alors exécutées.Un programme n'est qu'une longue séquence d'instructions qui sont exécutées par le processeur. Elles sont exécutées séquentiellement sauf quand une instruction de saut transfère l'exécution à une autre instruction que celle qui suit. Il existe également des sauts conditionnels qui sont soit exécutés (l'exécution continue à une autre adresse), soit ignorés (l'exécution continue à l'instruction suivante) selon certaines conditions.Chaque instruction commence par un nombre appelé opcode (ou code opération) qui détermine la nature de l'instruction.Par exemple, pour les ordinateurs d'architecture x86, l'opcode 0x6A (en binaire 01101010) correspond à l'instruction push (ajouter une valeur en haut de la pile).Par conséquent, l'instruction 0x6A 0x14 (01101010 00010100) correspond à push 0x14 (ajouter la valeur hexadécimale 0x14 , ou 20 en décimal, en haut de la pile).Certains processeurs codent toutes leurs instructions avec le même nombre de bits (par exemple : ARM, MIPS, PowerPC), tandis que chez d'autres la longueur de l'instruction dépend de l'opcode (exemple : x86). L'organisation des combinaisons de bits dépend largement du processeur. Le plus commun est la division en champs. Un ou plusieurs champs spécifient l'opération exacte (par exemple une addition). Les autres champs indiquent le type des opérandes, leur localisation, ou une valeur littérale (les opérandes contenus dans une instruction sont appelés immédiat). Avantages et inconvénients Lorsque toutes les instructions ont la même taille elles sont également alignées en mémoire. Par exemple si toutes les instructions sont alignées sur 32 bits (4 octets), alors les deux bits de poids faibles de l'adresse mémoire de n'importe quelle instruction sont à zéro. Cela permet notamment une implémentation plus aisée du cache des prédictions de branchement bimodales.En revanche le code machine prend moins de place en mémoire s'il ne possède pas de taille minimum, étant donné qu'on élimine les champs non utilisés.Alors que le langage machine était le seul disponible à l'aube des ordinateurs, il est aujourd'hui très long et fastidieux de développer en binaire : il faut passer par au moins un langage intermédiaire.De très nombreux langages de programmation sont transformés en langage machine lors de la compilation. Tous les programmes exécutables contiennent au moins une petite partie en langage machine.Le langage le plus facile à convertir en code machine est l'assembleur car il possède quasiment les mêmes instructions. L'assembleur (ou langage assembleur) diffère d'une machine à une autre, bien que les instructions soient au bout du compte très semblables. Les langages de plus haut niveau sont convertis en assembleur pendant la compilation. Les langages utilisant une machine virtuelle passent par un bytecode qui est converti à la volée par la machine virtuelle.Comme exemple spécifique, regardons l'architecture MIPS. Ses instructions ont toujours une longueur de 32 bits. Le type général de l'instruction est donné par les 6 bits de poids les plus forts (dans une représentation sur 32 bits, les 6 de gauche), qu'on appelle le champ op.Les instructions de type-J et de type-I sont pleinement spécifiées par le champ op. Les instructions de type-R ont un champ supplémentaire, fonct, pour déterminer la nature exacte de l'opération. Les champs de ces 3 types d'instructions sont :   6      5     5     5     5      6 bits[  op  |  rs |  rt |  rd |shamt| fonct]  type-R[  op  |  rs |  rt | adresse/immédiat ]  type-I[  op  |        adresse cible         ]  type-Jrs, rt, et rd indiquent des opérandes de type registre ; shamt indique un décalage (shift amount) ; et le champ adresse ou immédiat contient un opérande sous forme de valeur.Par exemple, ajouter les registres 1 et 2 et placer le résultat dans le registre 6 est codé :[  op  |  rs |  rt |  rd |shamt| fonct]    0     1     2     6     0     32     décimal 000000 00001 00010 00110 00000 100000   binaireCharger une valeur depuis la cellule mémoire 68 cellules après celle pointée par le registre 3 dans le registre 8 :[  op  |  rs |  rt | adresse/immédiat ]   35     3     8           68           décimal 100011 00011 01000  0000000001000100    binaireSauter à l'adresse 1025 (la prochaine instruction à exécuter se trouve à l'adresse 1025) :[  op  |        adresse cible         ]    2                 1025               décimal 000010   00000000000000010000000001     binaireLes processeurs de l'architecture ARM sont un cas particulier dans la mesure où toutes les instructions sont conditionnelles. Elles sont toutes d'une longueur de 32 bits, et leurs quatre premiers bits indiquent dans quelles conditions l'instruction doit être exécutée.Langage assembleur dit de deuxième générationLangage de haut niveau dit de troisième générationLangage de quatrième générationProcesseur, la composante qui exécute le langage machinePipeline (architecture des processeurs) Portail de la programmation informatique"
informatique;Une page web dynamique est une page web générée à la demande, par opposition à une page web statique. Le contenu d'une page web dynamique peut donc varier en fonction d'informations (heure, nom de l'utilisateur, formulaire rempli par l'utilisateur, etc.) qui ne sont connues qu'au moment de sa consultation. À l'inverse, le contenu d'une page web statique est a priori identique à chaque consultation.Lors de la consultation d'une page web statique, un serveur HTTP renvoie le contenu du fichier où la page est enregistrée.Lors de la consultation d'une page web dynamique, un serveur HTTP transmet la requête au logiciel correspondant à la requête, et le logiciel se charge de générer et envoyer le contenu de la page. La programmation web est le domaine de l'ingénierie informatique consacré au développement de tels logiciels. Les logiciels générant des pages web dynamiques sont fréquemment écrits avec les langages PHP, JavaServer Pages (JSP) ou Active Server Pages (ASP).Un site web dynamique peut ainsi fournir des informations aux utilisateurs en fonction de leur navigation sur celui-ci. Deux utilisateurs peuvent accéder simultanément à la même page web sans pour autant avoir le même contenu affiché à l'écran.Avec un site web dynamique, des modifications effectuées, par exemple via un système de soumission de commentaire ou bien une interface privée de gestion du site, pourront être directement visibles sur le site.Un site web dynamique peut permettre la mise en œuvre de différentes fonctionnalités, par exemples :- un système de gestion des accès, granulaire ou non, à certaines parties d'un site (administration du site, comptes utilisateurs, etc),- un système de soumission de commentaires publiques.En 2004, Le Journal du Net consacrait un article comparant les avantages des technologies statiques et dynamiques.Web profondProgrammation webFeuilles de style dynamiques en cascade Portail de l’informatique   Portail d’Internet
informatique;"PHP: Hypertext Preprocessor, plus connu sous son sigle PHP (sigle auto-référentiel), est un langage de programmation libre, principalement utilisé pour produire des pages Web dynamiques via un serveur HTTP, mais pouvant également fonctionner comme n'importe quel langage interprété de façon locale. PHP est un langage impératif orienté objet.PHP a permis de créer un grand nombre de sites web célèbres, comme Facebook et Wikipédia. Il est considéré comme une des bases de la création de sites web dits dynamiques mais également des applications web.PHP est un langage de script utilisé le plus souvent côté serveur : dans cette architecture, le serveur interprète le code PHP des pages web demandées et génère du code (HTML, XHTML, CSS par exemple) et des données (JPEG, GIF, PNG par exemple) pouvant être interprétés et rendus par un navigateur web. PHP peut également générer d'autres formats comme le WML, le SVG et le PDF.Il a été conçu pour permettre la création d'applications dynamiques, le plus souvent développées pour le Web. PHP est le plus souvent couplé à un serveur Apache bien qu'il puisse être installé sur la plupart des serveurs HTTP tels que IIS ou nginx. Ce couplage permet de récupérer des informations issues d'une base de données, d'un système de fichiers (contenu de fichiers et de l'arborescence) ou plus simplement des données envoyées par le navigateur afin d'être interprétées ou stockées pour une utilisation ultérieure.C'est un langage peu typé et souple et donc facile à apprendre par un débutant mais, de ce fait, des failles de sécurité peuvent rapidement apparaître dans les applications. Pragmatique, PHP ne s'encombre pas de théorie et a tendance à choisir le chemin le plus direct. Néanmoins, le nom des fonctions (ainsi que le passage des arguments) ne respecte pas toujours une logique uniforme, ce qui peut être préjudiciable à l'apprentissage.Son utilisation commence avec le traitement des formulaires puis par l'accès aux bases de données. L'accès aux bases de données est aisé une fois l'installation des modules correspondants effectuée sur le serveur. La force la plus évidente de ce langage est qu'il a permis au fil du temps la résolution aisée de problèmes autrefois compliqués et est devenu par conséquent un composant incontournable des offres d'hébergements.Il est multi-plateforme : autant sur Linux qu'avec Windows il permet aisément de reconduire le même code sur un environnement à peu près semblable (quoiqu'il faille prendre en compte les règles d'arborescences de répertoires, qui peuvent changer).Libre, gratuit, simple d'utilisation et d'installation, ce langage nécessite comme tout langage de programmation une bonne compréhension des principales fonctions usuelles ainsi qu'une connaissance aiguë des problèmes de sécurité liés à ce langage.La version 5.3 a introduit de nombreuses fonctions nouvelles : les espaces de noms (Namespace) — un élément fondamental de l'élaboration d'extensions, de bibliothèques et de frameworks structurés, les fonctions anonymes, les fermetures, etc.En 2018, près de 80 % des sites web utilisent le langage PHP sous ses différentes versions.Le langage PHP fait l'objet, depuis plusieurs années maintenant, de rassemblements nationaux organisés par l'AFUP (l'Association Française des Utilisateurs de PHP), où experts de la programmation et du milieu se retrouvent pour échanger autour du PHP et de ses développeurs. L'association organise ainsi deux évènements majeurs : le « Forum PHP », habituellement en fin d'année, et les « AFUP Day », qui ont lieu au cours du premier semestre, simultanément dans plusieurs villes.Le langage PHP a été créé en 1994 par Rasmus Lerdorf pour son site web. C'était à l'origine une bibliothèque logicielle en C dont il se servait pour conserver une trace des visiteurs qui venaient consulter son CV. Au fur et à mesure qu'il ajoutait de nouvelles fonctionnalités, Rasmus a transformé la bibliothèque en une implémentation capable de communiquer avec des bases de données et de créer des applications dynamiques et simples pour le Web. Rasmus a alors décidé, en 1995, de publier son code, pour que tout le monde puisse l'utiliser et en profiter. PHP s'appelait alors PHP/FI (pour Personal Home Page Tools/Form Interpreter). En 1997, deux étudiants, Andi Gutmans et Zeev Suraski, ont redéveloppé le cœur de PHP/FI. Ce travail a abouti un an plus tard à la version 3 de PHP, devenu alors PHP: Hypertext Preprocessor. Peu de temps après, Andi Gutmans et Zeev Suraski ont commencé la réécriture du moteur interne de PHP. C’est ce nouveau moteur, appelé Zend Engine — le mot Zend est la contraction de Zeev et Andi — qui a servi de base à la version 4 de PHP.En 2002, PHP est utilisé par plus de 8 millions de sites Web à travers le monde, en 2007 par plus de 20 millions et en 2013 par plus de 244 millions.De plus, PHP est devenu le langage de programmation web côté serveur le plus utilisé depuis plusieurs années :Enfin en 2010, PHP est le langage dont les logiciels open source sont les plus utilisés dans les entreprises, avec 57 % de taux de pénétration.Depuis juin 2011 et le nouveau processus de livraison de PHP, le cycle de livraison de PHP se résume à une mise à jour annuelle comportant des changements fonctionnels importants.La durée de vie d'une branche est de 3 ans, laissant trois branches stables et maintenues (cela signifie que lorsqu'une nouvelle version de PHP 5.x sort, la version 5.x-3 n'est plus supportée). Version 8.1 La version 8.1, sortie le 25 novembre 2021, introduit de nouvelles fonctionnalités comme : les énumérations ;les fibers ;la propriété Readonly. Version 8 Sortie le 26 novembre 2020, cette version majeure se démarque principalement par la fonctionnalité de « compilation à la volée » (Just-in-time compilation) qui permet un gain de vitesse d'exécution de plus de 45 % pour certaines applications Web. D'autres nouveautés sont également introduites comme : les weakmaps ;la Stringable Interface ;l'expression throw. Version 7.4 La version 7.4 est sortie le 20 février 2020. Elle vise à être maintenue jusqu'en novembre 2022.La version 7.4 se démarque de ses précédentes versions par :les propriétés typées 2.0 ;le pré-chargement ;l'opérateur d'affectation de coalescence nulle  ;improve openssl_random_pseudo_bytes ;les références faibles ;FFI (Foreign Function Interface) ;l'extension de hachage omniprésente ;le registre de hachage de mot de passe ;le fractionnement des chaînes multi-octets ;la réflexion sur les références ;le retrait de ext/wddx ;un nouveau mécanisme de sérialisation d'objets personnalisés. Version 7.3 Le 6 décembre 2018, la sortie de la version 7.3 mettait l'accent sur :l'évolution de la syntaxe Heredoc et Nowdoc ;la prise en charge de l'affectation de référence et de la déconstruction de tableau avec `list()` ;la prise en charge de PCRE2 ;l'introduction de la fonction High Resolution Time `hrtime()` function. Version 7.2 Le 30 novembre 2017, la version de PHP 7.2, qui utilise Zend Engine 2, a introduit une modélisation objet plus performante, une gestion des erreurs fondée sur le modèle des exceptions, ainsi que des fonctionnalités de gestion pour les entreprises. PHP 5 apporte beaucoup de nouveautés, telles que le support de SQLite ainsi que des moyens de manipuler des fichiers et des structures XML basés sur libxml2 :une API simple nommée SimpleXML ;une API Document Object Model assez complète ;une interface XPath utilisant les objets DOM et SimpleXML ;l'intégration de libxslt pour les transformations XSLT via l'extension XSL ;une bien meilleure gestion des objets par rapport à PHP 4, avec des possibilités qui tendent à se rapprocher de celles de Java. Version 7 (PHP7) Au vu des orientations différentes prises par le langage de celles prévues par PHP 6, une partie des développeurs propose de nommer la version succédant à PHP 5 « PHP 7 » au lieu de « PHP 6 ». Un vote parmi les développeurs valide cette proposition par 58 voix contre 24.PHP 7.0.0 est sorti en décembre 2015.La nouvelle version propose une optimisation du code et, d'après la société Zend, offre des performances dépassant celles de machines virtuelles comme HHVM,. Les benchmarks externes montrent des performances similaires pour HHVM et PHP 7, avec un léger avantage d'HHVM dans la plupart des scénarios. PHP 6 et Unicode En 2005, le projet de faire de PHP un langage fonctionnant d'origine en Unicode a été lancé par Andrei Zmievski, ceci en s'appuyant sur la bibliothèque International Components for Unicode (ICU) et en utilisant UTF-16 pour représenter les chaînes de caractères dans le moteur.Étant donné que cela représentait un changement majeur tant dans le fonctionnement du langage que dans le code PHP créé par ses utilisateurs, il fut décidé d'intégrer cela dans une nouvelle version 6.0 avec d'autres fonctionnalités importantes alors en développement. Toutefois, le manque de développeurs experts en Unicode ainsi que les problèmes de performance résultant de la conversion des chaînes de et vers UTF-16 (rarement utilisé dans un contexte web), ont conduit au report récurrent de la livraison de cette version. Par conséquent, une version 5.3 fut créée en 2009 intégrant de nombreuses fonctionnalités non liées à Unicode qui était initialement prévues pour la version 6.0, notamment le support des espaces de nommage (namespaces) et des fonctions anonymes. En mars 2010, le projet 6.0 intégrant unicode fut abandonné et la version 5.4 fut préparée afin d'intégrer la plupart des fonctionnalités non liées à l'unicode encore dans la branche 6.0, telles que les traits ou l'extension des fermetures au modèle objet.Le projet est depuis passé à un cycle de livraison prévisible (annuel) contenant des avancées significatives mais contenues tout en préservant au maximum la rétro-compatibilité avec le code PHP existant (5.4 en 2012, 5.5 en 2013, 5.6 prévue pour l'été 2014). Depuis janvier 2014, l'idée d'une nouvelle version majeure introduisant Unicode mais se basant sur UTF-8 (largement devenu depuis le standard du Web pour l'Unicode) et permettant certains changements pouvant casser la rétro-compatibilité avec du code PHP ancien est de nouveau discutée et les RFC sont maintenant triées selon leur implémentation en 5.x (évolutions ne causant pas ou marginalement de cassure de la rétro-compatibilité) ou dans la future version majeure (évolutions majeures du moteur et évolutions impliquant une non-compatibilité ascendante). À noter Il est à noter qu'historiquement, PHP disposait d'une configuration par défaut privilégiant la souplesse à la sécurité (par exemple register globals, qui a été activé par défaut jusqu'à PHP 4.2). Cette souplesse a permis à de nombreux développeurs d'apprendre PHP mais le revers de la médaille a été que de nombreuses applications PHP étaient mal sécurisées. Le sujet a bien été pris en main par le PHPGroup qui a mis en place des configurations par défaut mettant l'accent sur la sécurité. Il en résultait une réputation de langage peu sécurisé, réputation d'insécurité qui n'a plus de raison d'être[réf. nécessaire]. Détail de l'historique complet des versions PHP appartient à la grande famille des descendants du C, dont la syntaxe est très proche. En particulier, sa syntaxe et sa construction ressemblent à celles des langages Java et Perl, à ceci près que du code PHP peut facilement être mélangé avec du code HTML au sein d'un fichier PHP.Dans une utilisation destinée à l'internet, l'exécution du code PHP se déroule ainsi : lorsqu'un visiteur demande à consulter une page de site web, son navigateur envoie une requête au serveur HTTP correspondant. Si la page est identifiée comme un script PHP (généralement grâce à l'extension .php), le serveur appelle l'interprète PHP qui va traiter et générer le code final de la page (constitué généralement d'HTML ou de XHTML, mais aussi souvent de feuilles de style en cascade et de JS). Ce contenu est renvoyé au serveur HTTP, qui l'envoie finalement au client.Ce schéma explique ce fonctionnement :Une étape supplémentaire est souvent ajoutée : celle du dialogue entre PHP et la base de données. Classiquement, PHP ouvre une connexion au serveur de SGBD voulu, lui transmet des requêtes et en récupère le résultat, avant de fermer la connexion.L'utilisation de PHP en tant que générateur de pages Web dynamiques est la plus répandue, mais il peut aussi être utilisé comme langage de programmation ou de script en ligne de commande sans utiliser de serveur HTTP ni de navigateur. Il permet alors d'utiliser de nombreuses fonctions du langage C et plusieurs autres sans nécessiter de compilation à chaque changement du code source.Pour réaliser en Linux/UNIX un script PHP exécutable en ligne de commande, il suffit comme en Perl ou en Bash d'insérer dans le code en première ligne le shebang : #! /usr/bin/php. Sous un éditeur de développement comme SciTE, même en Windows, une première ligne <?php suffit, si le fichier possède un type .php.Il existe aussi une extension appelée PHP-GTK permettant de créer des applications clientes graphiques sur un ordinateur disposant de la bibliothèque graphique GTK+, ou encore son alternative WinBinder.PHP possède un grand nombre de fonctions permettant des opérations sur le système de fichiers, exécuter des commandes dans le terminal, la gestion des bases de données, des fonctions de tri et hachage, le traitement de chaînes de caractères, la génération et la modification d'images, des algorithmes de compression...Le moteur de Wikipédia, MediaWiki, est écrit en PHP et interagit avec une base MySQL ou PostgreSQLQuelques exemples du traditionnel Hello world :echo étant une structure du langage, il est possible – et même recommandé – de ne pas mettre de parenthèses.Il est aussi possible d'utiliser la version raccourcie :Résultat affiché :Le code PHP doit être inséré entre les balises <?php et ?> (la balise de fermeture est facultative en fin de fichier).Il y existe d'autres notations pour les balises :<?= et ?> (notation courte avec affichage) ;<? et ?> (notation courte sans affichage non disponible en PHP 8) ;<% et %> (notation ASP) ;<script language=""php""> et </script> (notation script).Les notations autres que la standard (<?php et ?>) et la notation courte avec affichage (<?= et ?>) sont déconseillées, car elles peuvent être désactivées dans la configuration du serveur (php.ini ou .htaccess) : la portabilité du code est ainsi réduite.Depuis PHP 7, les notations ASP et script ont été supprimées. La notation courte sans affichage reste déconseillée.Les instructions sont séparées par des ; (il n'est pas obligatoire après la dernière instruction) et les sauts de ligne ne modifient pas le fonctionnement du programme. Il serait donc possible d'écrire :Pour des raisons de lisibilité, il est néanmoins recommandé d'écrire une seule instruction par ligne. Il est aussi préférable d'écrire le dernier ;.Le code PHP est composé par des appels à des fonctions, dans le but d'attribuer des valeurs à des variables, le tout encadré dans des conditions, des boucles. Exemple :Une condition est appliquée quand l'expression entre parenthèses est évaluée à true, et elle ne l'est pas dans le cas de false. Sous forme numérique, 0 représente le false, et 1 (et tous les autres nombres) représentent le true.Le code précédent pourrait aussi être écrit de cette manière :Ici on teste l'égalité entre $lang et 'fr', mais pas directement dans le if : le test retourne un boolean (c'est-à-dire soit true, soit false) qui est stocké dans la variable $is_lang_fr. On entre ensuite cette variable dans le if et celui-ci, selon la valeur de la variable, effectuera ou non le traitement.Les blocs if, elseif et else sont généralement délimités par les caractères { et }, qui peuvent être omis, comme dans les codes précédents, lorsque ces blocs ne contiennent qu'une instruction.Il est également possible d'écrire else if en deux mots, comme en C/C++.On peut générer du code HTML avec le script PHP, par exemple :Il est également possible d'utiliser une syntaxe alternative pour la structure if/else :Une autre approche consiste à concaténer l'intégralité du code HTML dans une variable et de réaliser un echo de la variable en fin de fichier :Dans le cas où l'utilisateur aura préféré l'utilisation de la commande echo à la concaténation, il lui sera possible de capturer le flux en utilisant les fonctions ob_start() et ob_get_clean() :PHP, tout comme JavaScript, permet aussi de construire un modèle objet de document (DOM), ce qui permet de créer ou modifier un document (X)HTML sans écrire de HTML, comme le montre l'exemple suivant :Qui crée le code HTML suivant :Cette méthode est cependant peu utilisée pour générer un document complet, on l'utilise généralement pour générer un fichier XML.La commande phpinfo() est aussi utilisée pour générer un code HTML décrivant les paramètres du serveur ; elle est aussi très utilisée pour tester la bonne exécution du moteur d’exécution PHP.Comme en C++ et en Java, PHP permet de programmer en orienté objet, en créant des classes contenant des attributs et des méthodes, qui peuvent être instanciées ou utilisées en statique.Toutefois, PHP est un langage à héritage simple, c'est-à-dire qu'une classe ne peut hériter que d'au plus une seule autre classe (sinon il faut utiliser un trait pour simuler l'héritage multiple par composition). Cependant les interfaces peuvent en étendre plusieurs autres.Voici un exemple de création d'une classe :Comme de nombreux projets Open Source, PHP possède une mascotte. Il s'agit de l'éléPHPant, dessiné en 1998 par El Roubio.El Roubio s'est inspiré de la ressemblance des lettres PHP avec un éléphant et du fait que deux des lettres du langage soient déjà présentes dans ce mot, ce qui a permis de créer le néologisme éléPHPant. Toutes les œuvres d'El Roubio sont distribuées sous licence GNU GPL. Une peluche de l'ÉléPHPant bleu existe. D'autres versions ont vu le jour ces dernières années (rose, jaune, rouge, violet et orange) sous l'impulsion de sociétés (PHP Architect ou Zend Technologies) ou de groupes utilisateurs comme PHP Women ou PHP Amsterdam. Le site afieldguidetoelephpant.net recense tous les éléphpants existants.Wiki (MediaWiki, DokuWiki...)forum (phpBB, Vanilla, IPB, punBB...)FacebookSystèmes de gestion de blog (Dotclear)Systèmes de gestion de contenu (appelés aussi CMS) (WordPress, SPIP, ExpressionEngine, Drupal, Xoops, Joomla, K-Box...)Administration de bases de données (phpMyAdmin, phpPgAdmin, Adminer...)Frameworks (Laravel, Symfony, Zend Framework, CodeIgniter, CakePHP, etc.)Logiciel ECMLogiciel BPM, CRM et ou ERP (Dolibarr...)E-commerce (PrestaShop, WooCommerce, Magento, osCommerce, Sylius, etc.)Partis politiques (Parti chrétien-démocrate (France), etc.)Universités et formations supérieures alliant art et sciences (Ingénieur IMAC, UPEM, etc.)Un serveur Web en architecture trois tiers est composé d'un système d'exploitation, un serveur HTTP, un langage serveur et enfin un système de gestion de base de données (SGBD), cela constituant une plate-forme.Dans le cas de PHP comme langage serveur, les combinaisons les plus courantes sont celles d'une plateforme LAMP (pour Linux Apache MySQL PHP) et WAMP (Windows Apache MySQL PHP). Une plate-forme WAMP s'installe généralement par le biais d'un seul logiciel qui intègre Apache, MySQL et PHP, par exemple EasyPHP, VertrigoServ, WampServer ou UwAmp. Il existe le même type de logiciels pour les plates-formes MAMP (Mac OS Apache MySQL PHP), à l'exemple du logiciel MAMP.Il existe d'autres variantes, par exemple les plates-formes LAPP (le M de MySQL est remplacé par le P de PostgreSQL) ou encore le logiciel XAMPP (Apache MySQL Perl PHP ; le X indique que le logiciel est multiplate-forme), un kit de développement multiplate-forme.On peut décliner une grande variété d'acronymes sous cette forme. Des confusions peuvent parfois exister entre la plate-forme en elle-même et le logiciel permettant de l'installer, si elles ont le même nom. Il faut également remarquer que la grande majorité des logiciels « tout en un » sont destinés au développement d'applications Web en local, et non à être installés sur des serveurs Web. Une exception à cette règle est peut-être Zend Server, le serveur distribué par Zend Technologies, qui est prévu pour fonctionner aussi bien en environnement de développement que de production.PHP est à la base un langage interprété, ce qui est au détriment de la vitesse d'exécution du code. Sa forte popularité associée à son utilisation sur des sites Web à très fort trafic (Yahoo, Facebook) ont amené un certain nombre de personnes à chercher à améliorer ses performances pour pouvoir servir un plus grand nombre d'utilisateurs de ces sites Web sans nécessiter l'achat de nouveaux serveurs.La réécriture du cœur de PHP, qui a abouti au Zend Engine pour PHP 4 puis au Zend Engine 2 pour PHP 5, est une optimisation. Le Zend Engine compile en interne le code PHP en bytecode exécuté par une machine virtuelle. Les projets open source APC et eAccelerator fonctionnent en mettant le bytecode produit par Zend Engine en cache afin d'éviter à PHP de charger et d'analyser les scripts à chaque requête. À partir de la version 5.5 de PHP, le langage dispose d'un cache d'opcode natif (appelé OpCache) rendant obsolète le module APC.Il existe également des projets pour compiler du code PHP :Roadsend et phc compilent du PHP en C ;Quercus compile du PHP en bytecode Java exécutable sur une machine virtuelle Java ;Phalanger compile du PHP en Common Intermediate Language exécutable sur le Common Language Runtime du framework .NET ;HipHop for PHP transforme du PHP en C++ qui est ensuite compilé en code natif. Ce projet open source a été démarré par Facebook.(en) Luke Welling et Laura Thomson, PHP and MySQL Web development, Sams Publishing, 2008, 4e éd. (ISBN 978-0-672-32916-6 et 0-672-32916-6, OCLC 854795897)Damien Seguy et Philippe Gamache, Sécurité PHP 5 et MySQL, 3e édition, Eyrolles, 1er décembre 2011, 277 p. (ISBN 978-2-212-13339-4 et 2-212-13339-1, lire en ligne)Jean Engels PHP 5 Cours et Exercices, 3e édition, Eyrolles 2013, 631 pages  (ISBN 978-2-212-13725-5)Paamayim Nekudotayim : nom de l'opérateur :: en PHPListe de frameworks PHP : liste des cadres de développement (Frameworks) en PHPSuhosin: module de durcissement de PHP5(en) Site officiel Portail des logiciels libres   Portail de la programmation informatique"
informatique;"La programmation déclarative est un paradigme de programmation qui consiste à créer des applications sur la base de composants logiciels indépendants du contexte et ne comportant aucun état interne. Autrement dit, l'appel d'un de ces composants avec les mêmes arguments produit exactement le même résultat, quel que soit le moment et le contexte de l'appel.En programmation déclarative, on décrit le quoi, c'est-à-dire le problème. Par exemple, les pages HTML sont déclaratives car elles décrivent ce que contient une page (texte, titres, paragraphes, etc.) et non comment les afficher (positionnement, couleurs, polices de caractères…). Alors qu'en programmation impérative (par exemple, avec le C ou Java), on décrit le comment, c'est-à-dire la structure de contrôle correspondant à la solution.C'est une forme de programmation sans effets de bord, ayant généralement une correspondance avec la logique mathématique.Il existe plusieurs formes de programmation déclarative :la programmation descriptive, à l'expressivité réduite, qui permet de décrire des structures de données, comme HTML ou LaTeX ;la programmation fonctionnelle, qui perçoit les applications comme un ensemble de fonctions mathématiques, comme Lisp, Caml, Haskell et Oz ;la programmation logique, pour laquelle les composants d'une application sont des relations logiques, comme Prolog et Mercury ;la programmation par contraintes.Peter Van Roy, Seif Haridi. Concepts, Techniques, and Models of Computer Programming. MIT Press, 2004. Portail de la programmation informatique"
informatique;"En informatique, la programmation impérative est un paradigme de programmation qui décrit les opérations en séquences d'instructions exécutées par l'ordinateur pour modifier l'état du programme. Ce type de programmation est le plus répandu parmi l'ensemble des langages de programmation existants, et se différencie de la programmation déclarative (dont la programmation logique ou encore la programmation fonctionnelle sont des sous-ensembles).La quasi-totalité des processeurs qui équipent les ordinateurs sont de nature impérative : ils sont faits pour exécuter une suite d'instructions élémentaires, codées sous forme d'opcodes (pour operation codes). L'ensemble des opcodes forme le langage machine spécifique à l'architecture du processeur. L'état du programme à un instant donné est défini par le contenu de la mémoire centrale à cet instant.Les langages de plus haut niveau utilisent des variables et des opérations plus complexes, mais suivent le même paradigme. Les recettes de cuisine et les vérifications de processus industriel sont deux exemples de concepts familiers qui s'apparentent à de la programmation impérative ; de ce point de vue, chaque étape est une instruction, et le monde physique constitue l'état modifiable. Puisque les idées de base de la programmation impérative sont à la fois conceptuellement familières et directement intégrées dans l'architecture des microprocesseurs, la grande majorité des langages de programmation est impérative.La plupart des langages de haut niveau comporte cinq types d'instructions principales :la séquence d'instructionsl'assignation ou affectationl'instruction conditionnellela boucleles branchementsUne séquence d'instructions, (ou bloc d'instruction) désigne le fait de faire exécuter par la machine une instruction, puis une autre, etc., en séquence. Par exemple                                           ouvrirConnexion                          ;                              envoyerMessage                          ;                              fermerConnexion                          ;              {\displaystyle {\mbox{ouvrirConnexion}};{\mbox{envoyerMessage}};{\mbox{fermerConnexion}};}   est une séquence d'instructions. Cette construction se distingue du fait d'exécuter en parallèle des instructions.Les instructions d'assignation, en général, effectuent une opération sur l'information en mémoire et y enregistrent le résultat pour un usage ultérieur. Les langages de haut niveau permettent de plus l'évaluation d'expressions complexes qui peuvent consister en une combinaison d'opérations arithmétiques et d'évaluations de fonctions et l'assignation du résultat en mémoire. Par exemple:                     x        ←        2        +        3        ;              {\displaystyle x\leftarrow 2+3;}   assigne la valeur                     2        +        3              {\displaystyle 2+3}  , donc 5, à la variable de nom                     x              {\displaystyle x}  .Les instructions conditionnelles permettent à un bloc d'instructions de n'être exécuté que si une condition prédéterminée est réalisée. Dans le cas contraire, les instructions sont ignorées et la séquence d'exécution continue à partir de l'instruction qui suit immédiatement la fin du bloc. Par exemple                     s        i                                      connexionOuverte                                  a        l        o        r        s                                      envoyerMessage                          ;              {\displaystyle si\;{\mbox{connexionOuverte}}\;alors\;{\mbox{envoyerMessage}};}   n'enverra le message que si la connexion est ouverte.Les instructions de bouclage servent à répéter une suite d'instructions un nombre prédéfini de fois (voir Boucle_for), ou jusqu'à ce qu'une certaine condition soit réalisée. Par exemple                     t        a        n        t        q        u        e                                      connexionNonOuverte                                  a        l        o        r        s                                      attendreUnPeu                          ;              {\displaystyle tantque\;{\mbox{connexionNonOuverte}}\;alors\;{\mbox{attendreUnPeu}};}   bouclera jusqu'à ce que la connexion soit ouverte.Il se trouve que ces quatre constructions permettent de faire tous les programmes informatiques possibles, elles permettent de faire un système Turing-complet.Les branchements sans condition permettent à la séquence d'exécution d'être transférée à un autre endroit du programme. Cela inclut le saut, appelé « goto » (go to, /ɡəʊ tuː/, « aller à ») dans de nombreux langages, et les sous-programmes, ou appels de procédures. Les instructions de bouclage peuvent être vues comme la combinaison d'un branchement conditionnel et d'un saut. Les appels à une fonction ou une procédure (donc un Sous-programme) correspondent à un saut, complété du  passage de paramètres, avec un saut en retour.Les langages impératifs les plus anciens sont les langages machine des premiers ordinateurs. Dans ces langages, le jeu d'instructions est minimal, ce qui rend la mise en œuvre matérielle plus simple — on maîtrise directement ce qui se passe en mémoire —, mais gêne la création de programmes complexes.Le premier compilateur – un programme destiné à vérifier un programme au préalable et à le traduire en langage machine – dénommé A-0, fut écrit en 1951 par Grace Murray Hopper.Fortran, développé par John Backus (prix Turing 1977) chez IBM à partir de 1954, fut le premier langage de programmation capable de réduire les obstacles présentés par le langage machine dans la création de programmes complexes. Fortran était un langage compilé, qui autorisait entre autres l'utilisation de variables nommées, d'expressions complexes, et de sous-programmes. Premier langage normalisé au milieu des années 60, il continue d'évoluer et est toujours utilisé dans le milieu scientifique pour la qualité de ses bibliothèques numériques et sa grande rapidité, ce qui en fait le langage informatique ayant eu la plus grande longévité. Les normes Fortran apparues depuis le début du XXIe siècle sont Fortran 2003, Fortran 2008 et Fortran 2018.Les deux décennies suivantes virent l'apparition de plusieurs autres langages de haut niveau importants. ALGOL, développé en 1958 par un consortium américano-européen pour concurrencer FORTRAN, qui était un langage propriétaire, fut l'ancêtre de nombreux langages de programmation d'aujourd'hui.COBOL (1960) est un langage pour la programmation des applications de gestion développé avec plusieurs objectifs : d'une part avoir un langage standardisé, avec des sources portables sur des matériels différents, d'autre part avoir des sources lisibles et vérifiables par des non-spécialistes de l'informatique. Dans cet objectif, il a été défini avec une syntaxe proche de l'anglais. Le langage a ensuite évolué pour intégrer la programmation structurée (COBOL 85), et la programmation orientée objet (2000). Le parc énorme d'applications COBOL existantes dans les grandes entreprises assure sa longévité.Le langage BASIC (1963) a été conçu comme une version simplifiée de FORTRAN à but éducatif, destinée aux débutants et interactive. Sa simplicité et le fait que BASIC soit interprété facilitaient grandement la mise au point des programmes, ce qui lui conféra rapidement une grande popularité, malgré la pauvreté de ses constructions. Malheureusement, cette pauvreté même devait mener à une quantité de programmes non structurés et donc difficilement maintenables. Après un article de Edsger Dijkstra dénonçant les ravages de BASIC, la réputation de BASIC comme langage pour l'enseignement de la programmation déclina, au profit de Pascal.Dans les années 1970, le Pascal fut développé par Niklaus Wirth, dans le but d'enseigner la programmation structurée et modulaire. Pascal dérivait d'une proposition faite par N. Wirth (et refusée) pour l'évolution du langage ALGOL. Il combine les constructions de base de la programmation structurée (boucles tant-que, répéter-jusqu'à et boucle avec compteur), la possibilité de définir ses propres types de donnée, dans un ensemble élégant (servi par un grand nombre de types prédéfinis : ensemble, énumérations, intervalle), qui lui assura un succès durable comme langage d'initiation (en remplacement de BASIC). Par la suite, Niklaus Wirth fut à l'origine de Modula-2, Modula-3, et d'Oberon, les successeurs de Pascal.À la même époque, Dennis Ritchie créa le langage C aux laboratoires Bell, pour le développement du système Unix. La puissance du C, permettant grâce aux pointeurs de travailler à un niveau proche de la machine, ainsi qu'un accès complet aux primitives du système, lui assura un succès qui ne s'est jamais démenti depuis.Une des raisons du succès du langage C par rapport aux autres langages procéduraux de la même génération vient de son mode de distribution : les universités américaines pouvaient acheter une licence au prix de 300 dollars pour toute l'université et tous ses étudiants[réf. nécessaire].En 1974, le Département de la Défense des États-Unis cherchait un langage dont le cahier des charges mettait l'accent sur la sûreté d'exécution, pour tous ses besoins futurs. Le choix se porta sur Ada, langage créé par Jean Ichbiah chez CII-Honeywell Bull, dont la spécification ne fut complétée qu'en 1983. Le langage a connu plusieurs révisions, la dernière en date remontant à 2012.Dans les années 1980, devant les problèmes que posaient la complexité grandissante des programmes, il y eut un rapide gain d'intérêt pour la programmation orientée objet. Smalltalk-80, conçu à l'origine par Alan Kay en 1969, fut présenté en 1980 par le Palo Alto Research Center de la compagnie Xerox (États-Unis).À partir des concepts objet, Bjarne Stroustrup, chercheur aux Bell Labs, conçut en 1985 une extension orientée objet de C nommée C++. Parallèlement, une extension à C moins ambitieuse, mais inspirée de Smalltalk avait vu le jour, Objective C. Le succès d'Objective C, notamment utilisé pour le développement sur les stations NeXT et Mac OS X, est resté faible par rapport à C++.Dans les décennies 1980 et 1990, de nouveaux langages impératifs interprétés ou semi-interprétés doivent leur succès au développement de scripts pour des pages web dynamiques et les applications client-serveur. On peut citer dans ces catégories Perl (Larry Wall, 1987), Tcl (John Ousterhout, 1988), Python (Guido van Rossum, 1990), PHP (Rasmus Lerdorf, 1994), Java (Sun Microsystems, 1995), JavaScript (Brendan Eich, Netscape Navigator, 1995).Les langages de programmation impératifs doivent être distingués d'autres types de langages, les langages fonctionnels et les langages de programmation logique. Les langages fonctionnels, tels que Haskell ou ML, ne sont pas des suites d'instructions et ne s'appuient pas sur l'idée d'état global, mais au contraire tendent à s'extraire de ce modèle pour se placer à un niveau plus conceptuel (qui a ses fondations dans le lambda-calcul). Les langages de programmation logiques, tels que Prolog, se concentrent sur ce qui doit être calculé, et non comment le calcul doit être effectué.Un synopsis de l'histoire des langages de programmationUn cours en ligne de l'Université Paris XIII Portail de la programmation informatique"
informatique;"La programmation orientée objet (POO), ou programmation par objet, est un paradigme de programmation informatique. Elle consiste en la définition et l'interaction de briques logicielles appelées objets ; un objet représente un concept, une idée ou toute entité du monde physique, comme une voiture, une personne ou encore une page d'un livre. Il possède une structure interne et un comportement, et il sait interagir avec ses pairs. Il s'agit donc de représenter ces objets et leurs relations ; l'interaction entre les objets via leurs relations permet de concevoir et réaliser les fonctionnalités attendues, de mieux résoudre le ou les problèmes. Dès lors, l'étape de modélisation revêt une importance majeure et nécessaire pour la POO. C'est elle qui permet de transcrire les éléments du réel sous forme virtuelle.La programmation par objet consiste à utiliser des techniques de programmation pour mettre en œuvre une conception basée sur les objets. Celle-ci peut être élaborée en utilisant des méthodologies de développement logiciel objet, dont la plus connue est le processus unifié (« Unified Software Development Process » en anglais), et exprimée à l'aide de langages de modélisation tels que le Unified Modeling Language (UML).La programmation orientée objet est facilitée par un ensemble de technologies dédiées :les langages de programmation (chronologiquement : Simula, LOGO, Smalltalk, Ada, C++, Objective C, Eiffel, Python, PHP, Java, Ruby, AS3, C#, VB.NET, Fortran 2003, Vala, Haxe, Swift) ;les outils de modélisation qui permettent de concevoir sous forme de schémas semi-formels la structure d'un programme (Objecteering, UMLDraw, Rhapsody, DBDesigner…) ;les bus distribués (DCOM, CORBA, RMI, Pyro…) ;les ateliers de génie logiciel ou AGL (Visual Studio pour des langages Dotnet, NetBeans ou Eclipse pour le langage Java).Il existe actuellement deux grandes catégories de langages à objets : les langages à classes, que ceux-ci soient sous forme fonctionnelle (Common Lisp Object System), impérative (C++, Java) ou les deux (Python, OCaml) ;les langages à prototypes (JavaScript, Lua).En implantant les Record Class de Hoare, le langage Simula 67 pose les constructions qui seront celles des langages orientés objet à classes : classe, polymorphisme, héritage, etc. Mais c'est réellement par et avec Smalltalk 71 puis Smalltalk 80, inspiré en grande partie par Simula 67 et Lisp, que les principes de la programmation par objets, résultat des travaux d'Alan Kay, sont véhiculés : objet, encapsulation, messages, typage et polymorphisme (via la sous-classification) ; les autres principes, comme l'héritage, sont soit dérivés de ceux-ci ou une implantation. Dans Smalltalk, tout est objet, même les classes. Il est aussi plus qu'un langage à objets, c'est un environnement graphique interactif complet.À partir des années 1980, commence l'effervescence des langages à objets : C++ (1983), Objective-C (1984), Eiffel (1986), Common Lisp Object System (1988), etc. Les années 1990 voient l'âge d'or de l'extension de la programmation par objets dans les différents secteurs du développement logiciel.Depuis, la programmation par objets n'a cessé d'évoluer aussi bien dans son aspect théorique que pratique et différents métiers et discours mercatiques à son sujet ont vu le jour :l'analyse objet (AOO ou OOA en anglais) ;la conception objet (COO ou OOD en anglais) ;les bases de données objet (SGBDOO) ;les langages objets avec les langages à prototypes ;ou encore la méthodologie avec MDA (Model Driven Architecture).Aujourd'hui, la programmation par objets est vue davantage comme un paradigme, le paradigme objet, que comme une simple technique de programmation. C'est pourquoi, lorsque l'on parle de nos jours de programmation par objets, on désigne avant tout la partie codage d'un modèle à objets obtenu par AOO et COO.La programmation orientée objet a été introduite par Alan Kay avec Smalltalk. Toutefois, ses principes n'ont été formalisés que pendant les années 1980 et, surtout, 1990. Par exemple le typage de second ordre, qui qualifie le typage de la programmation orientée objet (appelé aussi duck typing), n'a été formulé qu'en 1995 par Cook.Concrètement, un objet est une structure de données qui répond à un ensemble de messages. Cette structure de données définit son état tandis que l'ensemble des messages qu'il comprend décrit son comportement :les données, ou champs, qui décrivent sa structure interne sont appelées ses attributs ;l'ensemble des messages forme ce que l'on appelle l'interface de l'objet ; c'est seulement au travers de celle-ci que les objets interagissent entre eux. La réponse à la réception d'un message par un objet est appelée une méthode (méthode de mise en œuvre du message) ; elle décrit quelle réponse doit être donnée au message.Certains attributs et/ou méthodes (ou plus exactement leur représentation informatique) sont cachés : c'est le principe d'encapsulation. Ainsi, le programme peut modifier la structure interne des objets ou leurs méthodes associées sans avoir d'impact sur les utilisateurs de l'objet.Un exemple avec un objet représentant un nombre complexe : celui-ci peut être représenté sous différentes formes (cartésienne (réel, imaginaire), trigonométrique, exponentielle (module, angle)). Cette représentation reste cachée et est interne à l'objet. L'objet propose des messages permettant de lire une représentation différente du nombre complexe. En utilisant les seuls messages que comprend notre nombre complexe, les objets appelants sont assurés de ne pas être affectés lors d'un changement de sa structure interne. Cette dernière n'est accessible que par les méthodes des messages.Dans la programmation par objets, chaque objet est typé. Le type définit la syntaxe (« Comment l'appeler ? ») et la sémantique des messages (« Que fait-il ? ») auxquels peut répondre un objet. Il correspond donc, à peu de chose près, à l'interface de l'objet. Toutefois, la plupart des langages objets ne proposent que la définition syntaxique d'un type (C++, Java, C#…) et rares sont ceux qui fournissent aussi la possibilité de définir formellement sa sémantique (comme dans le langage Eiffel avec sa conception par contrats).Un objet peut appartenir à plus d'un type : c'est le polymorphisme ; cela permet d'utiliser des objets de types différents là où est attendu un objet d'un certain type. Une façon de réaliser le polymorphisme est le sous-typage (appelé aussi héritage de type) : on raffine un type-parent en un autre type (le sous-type) par des restrictions sur les valeurs possibles des attributs. Ainsi, les objets de ce sous-type sont conformes au type parent. De ceci découle le principe de substitution de Liskov. Toutefois, le sous-typage est limité et ne permet pas de résoudre le problème des types récursifs (un message qui prend comme paramètre un objet du type de l'appelant). Pour résoudre ce problème, Cook définit en 1995 la sous-classification et le typage du second ordre qui régit la programmation orientée objet : le type est membre d'une famille polymorphique à point fixe de types (appelée classe). Les traits sont une façon de représenter explicitement les classes de types. (La représentation peut aussi être implicite comme avec Smalltalk, Ruby, etc.).On distingue dans les langages objets deux mécanismes du typage :le typage dynamique : le type des objets est déterminé à l'exécution lors de la création desdits objets (Smalltalk, Common Lisp, Python, PHP…) ;le typage statique : le type des objets est vérifié à la compilation et est soit explicitement indiqué par le développeur lors de leur déclaration (C++, Java, C#, Pascal…), soit déterminé par le compilateur à partir du contexte (Scala, OCaml…).De même, deux mécanismes de sous-typage existent : l'héritage simple (Smalltalk, Java, C#) et l'héritage multiple (C++, Python, Common Lisp, Eiffel, WLangage).Le polymorphisme ne doit pas être confondu avec le sous-typage ou avec l'attachement dynamique (dynamic binding en anglais).La programmation objet permet à un objet de raffiner la mise en œuvre d'un message défini pour des objets d'un type parent, autrement dit de redéfinir la méthode associée au message : c'est le principe de redéfinition des messages (ou overriding en anglais).Or, dans une définition stricte du typage (typage du premier ordre), l'opération résultant d'un appel de message doit être la même quel que soit le type exact de l'objet référé. Ceci signifie donc que, dans le cas où l'objet référé est de type exact un sous-type du type considéré dans l'appel, seule la méthode du type père est exécutée :Soit un type Reel contenant une méthode * faisant la multiplication de deux nombres réels, soient Entier un sous-type de Reel, i un Entier et r un Reel, alors l'instruction i * r va exécuter la méthode * de Reel. On pourrait appeler celle de Entier grâce à une redéfinition.Pour réaliser alors la redéfinition, deux solutions existent :le typage du premier ordre associé à l'attachement dynamique (c'est le cas de C++, Java, C#…). Cette solution induit une faiblesse dans le typage et peut conduire à des erreurs. Les relations entre type sont définies par le sous-typage (théorie de Liskov) ;le typage du second ordre (duquel découlent naturellement le polymorphisme et l'appel de la bonne méthode en fonction du type exact de l'objet). Ceci est possible avec Smalltalk et Eiffel. Les relations entre types sont définies par la sous-classification (théorie F-Bound de Cook).La structure interne des objets et les messages auxquels ils répondent sont définis par des modules logiciels. Ces mêmes modules créent les objets via des opérations dédiées. Deux représentations existent de ces modules : la classe et le prototype.La classe est une structure informatique particulière dans le langage objet. Elle décrit la structure interne des données et elle définit les méthodes qui s'appliqueront aux objets de même famille (même classe) ou type. Elle propose des méthodes de création des objets dont la représentation sera donc celle donnée par la classe génératrice. Les objets sont dits alors instances de la classe. C'est pourquoi les attributs d'un objet sont aussi appelés variables d'instance et les messages opérations d'instance ou encore méthodes d'instance. L'interface de la classe (l'ensemble des opérations visibles) forme les types des objets. Selon le langage de programmation, une classe est soit considérée comme une structure particulière du langage, soit elle-même comme un objet (objet non-terminal). Dans le premier cas, la classe est définie dans le runtime ; dans l'autre, la classe a besoin elle aussi d'être créée et définie par une classe : ce sont les méta-classes. L'introspection des objets (ou « méta-programmation ») est définie dans ces méta-classes.La classe peut être décrite par des attributs et des messages. Ces derniers sont alors appelés, par opposition aux attributs et messages d'un objet, variables de classe et opérations de classe ou méthodes de classe. Parmi les langages à classes on retrouve Smalltalk, C++, C#, Java, etc.Le prototype est un objet à part entière qui sert de prototype de définition de la structure interne et des messages. Les autres objets de mêmes types sont créés par clonage. Dans le prototype, il n'y a plus de distinction entre attributs et messages : ce sont tous des slots. Un slot est un label de l'objet, privé ou public, auquel est attachée une définition (ce peut être une valeur ou une opération). Cet attachement peut être modifié à l'exécution. Chaque ajout d'un slot influence l'objet et l'ensemble de ses clones. Chaque modification d'un slot est locale à l'objet concerné et n'affecte pas ses clones.Le concept de trait permet de modifier un slot sur un ensemble de clones. Un trait est un ensemble d'opérations de même catégorie (clonage, persistance, etc.) transverse aux objets. Il peut être représenté soit comme une structure particulière du langage, comme un slot dédié ou encore comme un prototype. L'association d'un objet à un trait fait que l'objet et ses clones sont capables de répondre à toutes les opérations du trait. Un objet est toujours associé à au moins un trait, et les traits sont les parents des objets (selon une relation d'héritage). Un trait est donc un mixin doté d'une parenté. Parmi les langages à prototype on trouve Javascript, Self, Io, Slater, Lisaac, etc.Différents langages utilisent la programmation orientée objet, par exemple PHP, Python, etc.En PHP la programmation orientée objet est souvent utilisée pour mettre en place une architecture MVC (Modèle Vue Contrôleur), où les modèles représentent des objets.La modélisation objet consiste à créer un modèle du système informatique à réaliser. Ce modèle représente aussi bien des objets du monde réel que des concepts abstraits propres au métier ou au domaine dans lequel le système sera utilisé.La modélisation objet commence par la qualification de ces objets sous forme de types ou de classes sous l'angle de la compréhension des besoins et indépendamment de la manière dont ces classes seront mises en œuvre. C'est ce que l'on appelle l'analyse orientée objet ou OOA (acronyme de « Object-Oriented Analysis »). Ces éléments sont alors enrichis et adaptés pour représenter les éléments de la solution technique nécessaires à la réalisation du système informatique. C'est ce que l'on appelle la conception orientée objet ou OOD (acronyme de « Object-Oriented Design »). À un modèle d'analyse peuvent correspondre plusieurs modèles de conception. L'analyse et la conception étant fortement interdépendants, on parle également d'analyse et de conception orientée objet (OOAD). Une fois un modèle de conception établi, il est possible aux développeurs de lui donner corps dans un langage de programmation. C'est ce que l'on appelle la programmation orientée objet ou OOP (en anglais « Object-Oriented Programming »). Pour écrire ces différents modèles, plusieurs langages et méthodes ont été mis au point. Ces langages sont pour la plupart graphiques. Les trois principaux à s'imposer sont OMT de James Rumbaugh, la méthode Booch de Grady Booch et OOSE de Ivar Jacobson. Toutefois, ces méthodes ont des sémantiques différentes et ont chacune des particularités qui les rendent particulièrement aptes à certains types de problèmes. OMT offre ainsi une modélisation de la structure de classes très élaborée. Booch a des facilités pour la représentation des interactions entre les objects. OOSE innove avec les cas d'utilisation pour représenter le système dans son environnement. La méthode OMT prévaut sur l'ensemble des autres méthodes au cours de la première partie de la décennie 1990.À partir de 1994, Booch et Jacobson, rapidement rejoints par Rumbaugh, décident d'unifier leurs approches au sein d'une nouvelle méthode qui soit suffisamment générique pour pouvoir s'appliquer à la plupart des contextes applicatifs. Ils commencent par définir le langage de modélisation UML (Unified Modeling Language) appelé à devenir un standard de l'industrie. Le processus de normalisation est confié à l'Object Management Group (OMG), un organisme destiné à standardiser des technologies orientées objet comme CORBA (acronyme de « Common Object Request Broker Architecture »), un intergiciel (« middleware » en anglais) objet réparti. Rumbaugh, Booch et Jacobson s'affairent également à mettre au point une méthode permettant d'une manière systématique et répétable d'analyser les exigences et de concevoir et mettre en œuvre une solution logicielle à l'aide de modèles UML. Cette méthode générique de développement orienté objet devient le processus unifié (également connu sous l'appellation anglo-saxonne de « Unified Software Development Process »). Elle est itérative et incrémentale, centrée sur l'architecture et guidée par les cas d'utilisation et la réduction des risques. Le processus unifié est de plus adaptable par les équipes de développement pour prendre en compte au mieux les particularités du contexte.Néanmoins pour un certain nombre de concepteurs objet, dont Bertrand Meyer, l'inventeur du langage orienté objet Eiffel, guider une modélisation objet par des cas d'utilisations est une erreur de méthode qui n'a rien d'objet et qui est plus proche d'une méthode fonctionnelle. Pour eux, les cas d'utilisations sont relégués à des utilisations plutôt annexes comme la validation d'un modèle par exemple[réf. nécessaire].(en) Brad J. Cox et Andrew J. Novobilski, Object-Oriented Programming : An Evolutionary Approach, Addison-Wesley, 1986 (ISBN 0-201-54834-8).Grady Booch, James Rumbaugh et Ivar Jacobson, Le guide de l'utilisateur UML, EYROLLES, 2000 (ISBN 2-212-09103-6).Erich Gamma, Richard Helm, Ralph Johnson et John Vlissides (trad. de l'anglais par Jean-Marie Lasvergères), Design Patterns : Catalogue des modèles de conception réutilisables, Vuibert, 1999 (ISBN 2-7117-8644-7).Bertrand Meyer (2000). Conception et programmation orientées objet,  (ISBN 2-212-09111-7).De Hugues Bersini (2007). L'Orienté Objet,  (ISBN 978-2-212-12084-4).Francisco Bonito (2000). La programmation : l'orienté objet.Introduction à la POO Apprendre simplement la Programmation Orientée ObjetDes paradigmes « classiques » à l'orienté objetAnalyse et conception orientée objet avec UML et RUP, un survol rapide(en) The Theory of Classification de Anthony J.H. Simons sur le JOT (Journal of Object Technology) Portail de la programmation informatique"
informatique;"En informatique, la programmation procédurale est un paradigme  qui se fonde sur le concept d'appel procédural. Une procédure, aussi appelée routine, sous-routine ou fonction (à ne pas confondre avec les fonctions de la programmation fonctionnelle reposant sur des fonctions mathématiques), contient simplement une série d'étapes à réaliser. N'importe quelle procédure peut être appelée à n'importe quelle étape de l'exécution du programme, y compris à l'intérieur d'autres procédures, voire dans la procédure elle-même (récursivité).La programmation procédurale est un meilleur choix qu'une simple programmation séquentielle. Les avantages sont en effet les suivants :la possibilité de réutiliser le même code à différents emplacements dans le programme sans avoir à le dupliquer (principe « DRY »), ce qui a pour effet la réduction de la taille du code source et un gain en localité des modifications, donc une amélioration de la maintenabilité (compréhension plus rapide, réduction du risque de régression) ;une façon plus simple de suivre l'exécution du programme : la programmation procédurale permet de se passer d'instructions telles que goto, évitant ainsi bien souvent de se retrouver avec un programme compliqué qui part dans toutes les directions (appelé souvent « programmation spaghetti[réf. nécessaire] ») ; cependant, la programmation procédurale permet les « effets de bord », c'est-à-dire la possibilité pour une procédure qui prend des arguments de modifier des variables extérieures à la procédure auxquelles elle a accès (variables de contexte plus global que la procédure).La modularité est une caractéristique souhaitable pour un programme ou une application informatique, et consiste enle découpage du programme ou de l'application en unités sans effet de bord entre elles, c'est-à-dire dont le fonctionnement et le résultat renvoyé au module appelant ne dépend que des paramètres explicitement passés en argument (unités fonctionnelles). Un module est un ensemble de structure de données et de procédures, dont l'effet de bord est confiné à cet ensemble de données.De ce fait, un module offre un service. Un module peut avoir un contexte d'exécution différent de celui du code appelant : on parle alors de RPC (Remote Procedure Call) si ce contexte est un autre ordinateur, ou de communication inter-processus (légers ou système) s'il s'agit du même ordinateur. Lorsque la transmission des données ne se fait pas en mémoire mais par fichiers, les modules qui communiquent peuvent être compilés séparément et un script doit assurer l'enchainement des appels.On constate qu'il n'est pas contre-indiqué pour une procédure d'accéder en lecture et en écriture à des variables de contexte plus global (celui d'un module) : cela permet une réduction essentielle du nombre d'arguments passés, mais au détriment de la réutilisation telle quelle dans d'autres contextes d'une procédure. C'est le module en entier qui est réutilisable.Du fait de leur comportement sans effet de bord, chaque module peut être développé par une personne ou un groupe de personnes distinct de ceux qui développent d'autres modules. Les bibliothèques sont des modules. À noter que pour qu'une procédure puisse être considérée comme se comportant comme une « fonction pure » mathématique, il faut que la valeur de son résultat renvoyé au programme appelant prenne toujours la même valeur pour chaque valeur des arguments. Il faut donc qu'elle ne dépende pas d'une variable globale statique éventuelle du module, statique au sens qu'elle garde sa valeur après la fin de l'invocation du module (par une de ses procédures).La programmation objet et générique permet une mutualisation et une unicité de l'information et des traitements/procédures/méthodes (en théorie). En identifiant les variables globales à un module à une structure au sens C ou Pascal et à un type utilisateur, ces modules deviennent par définition des « classes » dont l'instanciation correspond à l'instanciation d'un type composé (une structure C ou Pascal).De plus, par le jeu du polymorphisme et de la généricité, les méthodes d'une classe, qui correspondent exactement aux procédures du module correspondant, en confiant l'effet de bord aux attributs de cette classe, peuvent accepter des arguments dont le type est variable (d'une manière contrôlée par le graphe d'héritage). De ce fait, la programmation objet va plus loin dans la factorisation des traitements que la programmation procédurale (en prolongeant celle-ci), et permet de répondre bien mieux à des besoins où des traitements similaires sont attendus dans des endroits différents d'une solution. Dans une programmation objet aboutie, les « procédures d'aiguillage » (routage de traitements en fonction du type d'une variable passée en argument) sont reléguées au compilateur par utilisation de la liaison dynamique. Le code source s'en trouve aussi réduit.Le plus vieil exemple de ce type de langage est l'ALGOL. D'autres exemples sont Fortran, PL/I, Modula-2 et Ada (dans sa première version). Portail de la programmation informatique"
informatique;La théorie algorithmique de l'information, initiée par Kolmogorov, Solomonov et Chaitin dans les années 1960, vise à quantifier et qualifier le contenu en information d'un ensemble de données, en utilisant la théorie de la calculabilité et la notion de machine universelle de Turing.Cette théorie permet également de formaliser la notion de complexité d'un objet, dans la mesure où l'on considère qu'un objet (au sens large) est d'autant plus complexe qu'il faut beaucoup d'informations pour le décrire, ou — à l'inverse — qu'un objet contient d'autant plus d'informations que sa description est longue. La théorie algorithmique de l'information est fondée sur cette équivalence : la description d'un objet est formalisée par un algorithme (autrement dit une machine de Turing), et sa complexité (autrement dit  son contenu en information) est  formalisé par certaines caractéristiques de l'algorithme : sa longueur ou son temps de calcul.  Ces fondements  sont différents de ceux de la théorie de l'information de Shannon : cette dernière n'utilise pas la notion de calculabilité et n'a de sens que par rapport à un ensemble statistique de données. Cependant, les deux théories sont compatibles et des liens formels entre elles peuvent être établis.Tandis que la théorie de l'information de Shannon a eu de nombreuses applications en informatique, télécommunications, traitement de signal et neurosciences computationnelles, la théorie algorithmique de l'information a été utilisée avec succès dans les domaines de la biologie, de la physique et même de la philosophie.L'idée principale de la théorie algorithmique de l'information est qu'une chose est d'autant plus complexe, ou contient d'autant plus d'information, qu'elle est difficile à expliquer, c'est-à-dire fondamentalement longue à expliquer. Voici par exemple trois descriptions d'objets :D1 : « un mur tout blanc de 1 m sur 1 m. »D2 : « un mur tout blanc de 1m sur 1m, avec une rayure rouge horizontale de 2 cm de large en bas, une autre 8 cm au-dessus, encore une autre 8 cm au-dessus, encore une autre 8 cm au-dessus, encore une autre 8 cm au-dessus, encore une autre 8 cm au-dessus, encore une autre 8 cm au-dessus, encore une autre 8 cm au-dessus, encore une autre 8 cm au-dessus, encore une autre 8 cm au-dessus et une dernière encore 8 cm au-dessus. »D2' : « un mur tout blanc de 1 m sur 1 m, avec des rayures rouges horizontales de 2 cm de large, de bas en haut tous les 8 cm. »En termes de longueur de description, D1 est plus courte que D2' qui est plus courte que D2. Que D1 soit la plus courte description semble normal, et est lié au fait que l'objet décrit est « plus simple ». Mais en ce qui concerne D2 et D2', les objets décrits sont identiques bien que D2' soit plus courte que D2. Ainsi la longueur brute d'une description n'est pas une mesure parfaitement adaptée.L'idée « algorithmique » est alors de considérer, comme complexité de l'objet décrit, sa plus courte description possible. Idée « algorithmique » dans le sens où la description n'est pas forcément extensive, mais peut — comme D2' dans l'exemple ci-dessus — décrire un procédé d'obtention de l'objet (ici : tracer des bandes horizontales à intervalles réguliers).Ainsi, un objet sera d'autant plus compliqué qu'on ne peut le décrire plus brièvement qu'une liste exhaustive de ses propriétés… Ce dernier cas constitue le cas limite d'une complexité maximale.Jean-Paul Delahaye Information, complexité et hasard [détail des éditions]Li, M., and Vitanyi, P. An Introduction to Kolmogorov Complexity and its Applications, Springer-Verlag, New York, 1997Analyse de la complexité des algorithmesThéorie de la complexitéThéorie de la simplicité Portail de l'informatique théorique
informatique;"Un serveur web est soit un logiciel de service de ressources web (serveur HTTP), soit un serveur informatique (ordinateur) qui répond à des requêtes du World Wide Web sur un réseau public (Internet) ou privé (intranet),,, en utilisant principalement le protocole HTTP.Un serveur informatique peut être utilisé à la fois pour servir des ressources du Web et pour faire fonctionner en parallèle d'autres services liés, comme l'envoi d'e-mails, l'émission de flux en streaming, le stockage de données dans des bases de données, le transfert de fichiers par FTP.Les serveurs web publics sont reliés à Internet et hébergent des ressources (pages web, images, vidéos, etc.) du Web. Ces ressources peuvent être statiques (servies telle quelles) ou dynamiques (construites à la demande par le serveur).Certains serveurs sont seulement accessibles sur des réseaux privés (intranets) et hébergent des sites utilisateurs, des documents, ou des logiciels, internes à une entreprise, une administration, etc.Techniquement il serait possible qu'un même ordinateur remplisse ces deux fonctions, mais c'est rarement le cas pour des raisons de sécurité[réf. nécessaire]. La fonction principale d'un serveur Web est de stocker et délivrer des pages web qui sont généralement rendues en HTML. Le protocole de communication Hypertext Transfer Protocol (HTTP) permet le dialogue via le réseau avec le logiciel client, généralement un navigateur web.Les deux termes sont utilisés pour le logiciel car le protocole HTTP a été développé pour le Web, et les pages Web sont en pratique toujours servies avec ce protocole. Cependant d'autres ressources du Web comme les fichiers à télécharger ou les flux audio ou vidéo sont parfois servis avec d'autres protocoles, telle que, par exemple, le protocole de transport Temps Réel (Real-time Transport Protocol), ainsi que son pendant sécurisé, le protocole de transport sécurisé Temps Réel (Secure Real-time Transport Protocol).CERN httpd est le premier serveur HTTP, inventé en même temps que le World Wide Web, en 1990 au CERN de Genève. Il est rapidement devenu obsolète en raison de l'évolution exponentielle des fonctionnalités du protocole.Quelques serveurs HTTP :Apache HTTP Server de la Apache Software Foundation, successeur du NCSA HTTPd ;Apache Tomcat de la Apache Software Foundation, évolution de Apache pour J2EE ;BusyBox httpd, utilisé dans le domaine de l'informatique embarquée, et notamment avec OpenWRT ;Google Web Server de Google ;Internet Information Services (IIS) de Microsoft ;lighttpd de Jan Kneschke ;Monkey web server de Eduardo Silva Pereira, dédié au noyau Linux, permettant d'utiliser pleinement ses fonctionnalités ;nginx d'Igor Sysoev ;Hiawatha de Hugo LeisinkNodeJS sous MIT Licence conçu par Ryan Lienhart Dahl en lignes de programmation en JavaScript ;Sun Java System Web Server de Sun Microsystems (anciennement iPlanet de Netscape, puis Sun ONE de Sun Microsystems) ;Tengine, fork de nginx, de Taobao (9e rang mondial Alexa en juillet 2014) ;Zeus Web Server de Zeus Technology ;Gunicorn est un serveur web HTTP WSGI écrit en Python pour Unix ;Zazouminiwebserver, serveur extrêmement léger (approx. 500 kilooctets), sous environnement Microsoft Windows.Abyss Web Server, un serveur gratuit, multi-plateforme (Linux, Windows, MacOS, BSD), permettant un paramétrage très facile via une interface graphique multilingue.Le serveur HTTP le plus utilisé est Apache HTTP Server qui sert environ 55 % des sites web en janvier 2013 selon Netcraft.Le serveur HTTP le plus utilisé dans les 1 000 sites les plus actifs est en revanche Nginx avec 38,2 % de parts de marché en 2016  selon w3techs et 53,9 % en avril 2017Historiquement, d'autres serveurs HTTP importants furent CERN httpd, développé par les inventeurs du Web, abandonné le 15 juillet 1996 et NCSA HTTPd, développé au NCSA en même temps que NCSA Mosaic, abandonné mi-1994, ainsi que WebObjects.Il existe aussi des serveurs HTTP qui sont des serveurs d'applications capables de faire serveur HTTP, comme Caudium et GlassFish. À l'inverse, on peut trouver des serveurs HTTP spécialisés dans un service distinct comme : HTTP File Server qui est uniquement destiné au partage de fichiersLe logiciel serveur HTTP ou daemon HTTP est le logiciel prenant en charge les requêtes client-serveur du protocole HTTP développé pour le World Wide Web. Ces logiciels intègrent généralement des modules permettant d'exécuter un langage serveur comme PHP pour générer des pages web dynamiques. Les plus connus sont Apache, Nginx, IIS, et Lighttpd.Le plus souvent, un serveur Web exécute continuellement d'autres logiciels qui fonctionnent en collaboration avec le logiciel de serveur HTTP. Selon les besoins, certains services gourmands en ressources, comme le serveur de base de données, peuvent être situés sur la même machine ou un serveur spécialisé.Certaines combinaisons de logiciels de base sont connues sous différents acronymes, notamment celle d'Apache (serveur HTTP) logiciel installé et exécuté sur le serveur web en parallèle de MySQL (serveur de base de données) et le script d'interprétation et d'exécution de PHP (voire PHP-FPM).Voir en PDF l'introduction « Qu'entend-t-on par serveur HTTP et serveur Web ? » d'Anthony Garcia (2008) - IBISC[source insuffisante] :LAMP pour « Linux, Apache, MySQL, PHP » ;WAMP pour « Windows, Apache, MySQL, PHP » ;MAMP pour « Macintosh, Apache, MySQL, PHP ».Il existe aussi la distribution de Microsoft nommée IIS pour « Internet Information Services » qui comprend plusieurs services : HTTP, FTP, SMTP et NNTP.L’équilibrage de charge des serveurs web, ou répartition de charge des serveurs Web, regroupe l’ensemble des mécanismes utilisés pour distribuer les requêtes sur de multiples serveurs Web. Cette pratique est devenue indispensable depuis l’explosion du trafic du Web qui a pour conséquence un accroissement important de la charge demandé au serveur. Cela a entraîné une évolution des architectures, destinée à apporter plus de scalabilité, de disponibilité et de performances. Portail des réseaux informatiques   Portail d’Internet   Portail de l’informatique"
informatique;"Un superordinateur ou supercalculateur est un ordinateur conçu pour atteindre les plus hautes performances possibles avec les techniques connues lors de sa conception, en particulier en ce qui concerne la vitesse de calcul. Pour des raisons de performance, c'est presque toujours un ordinateur central, dont les tâches sont fournies en traitement par lots.La science des superordinateurs est appelée « calcul haute performance » (en anglais : high-performance computing ou HPC). Cette discipline se divise en deux : la partie matérielle (conception électronique de l'outil de calcul) et la partie logicielle (adaptation logicielle du calcul à l'outil). Ces deux parties font appel à des champs de connaissances différents.Les premiers superordinateurs (ou supercalculateurs) apparaissent dans les années 1960.En 1961, IBM développe l'IBM Stretch ou IBM 7030, dont une unité est exploitée en France en 1963.À cette époque, et jusque dans les années 1970, le plus important constructeur mondial de superordinateurs est la société Control Data Corporation (CDC), avec son concepteur Seymour Cray. Par la suite, Cray Research, fondée par Seymour Cray après son départ de CDC, prend l’avantage sur ses autres concurrents, jusqu’aux alentours de l'année 1990. Dans les années 1980, à l’image de ce qui s’était produit sur le marché des micro-ordinateurs des années 1970, de nombreuses petites sociétés se lancèrent sur ce marché, mais la plupart disparaissent dans le « crash » du marché des superordinateurs, au milieu des années 1990.Ce que désigne le terme superordinateur varie avec le temps, car les ordinateurs les plus puissants du monde à un moment donné tendent à être égalés, puis dépassés, par des machines d’utilisation courante plusieurs années après. Les premiers superordinateurs CDC étaient de simples ordinateurs mono-processeurs (mais possédant parfois jusqu’à dix processeurs périphériques pour les entrées-sorties) environ dix fois plus rapides que la concurrence. Dans les années 1970, la plupart des superordinateurs adoptent un processeur vectoriel, qui effectue le décodage d’une instruction une seule fois pour l’appliquer à toute une série d’opérandes.C’est seulement vers la fin des années 1980 que la technique des systèmes massivement parallèles est adoptée, avec l’utilisation dans un même superordinateur de milliers de processeurs. De nos jours, certains de ces superordinateurs parallèles utilisent des microprocesseurs de type « RISC », conçus pour des ordinateurs de série, comme les PowerPC ou les PA-RISC. D’autres supercalculateurs utilisent des processeurs de moindre coût, de type « CISC », microprogrammés en RISC dans la puce électronique (AMD ou Intel) : le rendement en est un peu moins élevé, mais le canal d’accès à la mémoire — souvent un goulet d’étranglement — est bien moins sollicité.Au XXIe siècle, les superordinateurs sont le plus souvent conçus comme des modèles uniques par des constructeurs informatiques « traditionnels » comme International Business Machines (IBM), Hewlett-Packard (HP), ou Bull, qu’ils aient derrière eux une longue tradition en la matière (IBM) ou qu’ils aient racheté dans les années 1990 des entreprises spécialisées, alors en difficulté, pour acquérir de l’expérience dans ce domaine.Les superordinateurs sont utilisés pour toutes les tâches qui nécessitent une très forte puissance de calcul, comme les prévisions météorologiques, l’étude du climat (à ce sujet, voir les programmes financés par le G8-HORCs), la modélisation d'objets chimiques (calcul de structures et de propriétés, modélisation moléculaire, etc.), les simulations physiques (simulations aérodynamiques, calculs de résistance des matériaux, simulation d'explosion d'arme nucléaire, étude de la fusion nucléaire, etc.), la cryptanalyse ou les simulations en finance et en assurance (calcul stochastique).Les institutions de recherche civiles et militaires comptent parmi les plus gros utilisateurs de superordinateurs.En France, on trouve ces machines dans les centres nationaux de calculs universitaires, tels que l'Institut du développement et des ressources en informatique scientifique (IDRIS), le Centre informatique national de l'enseignement supérieur (CINES), mais aussi au Commissariat à l'énergie atomique et aux énergies alternatives (CEA) ou dans certaines grandes entreprises, comme Total, EDF ou encore Météo-France.Les superordinateurs tirent leur supériorité sur les ordinateurs conventionnels à la fois grâce à :leur architecture, en « pipeline » (exécution d’une instruction identique sur une longue série de données) ou parallèle (nombre très élevé de processeurs fonctionnant chacun sur une partie du calcul), qui leur permet d’exécuter plusieurs tâches simultanément ;des composants électroniques rapides (structure de type serveurs lame utilisant des processeurs multi-cœur ou des cartes graphiques dédiées au calcul scientifique de dernière génération, de la mémoire vive et des équipements de stockage de masse — disque dur — reliés à la fibre optique en grande quantité, etc.) associés à un système d'exploitation dédié (comme Linux, majoritairement utilisé actuellement).Ils sont presque toujours conçus spécifiquement pour un certain type de tâches (le plus souvent des calculs numériques scientifiques : calcul matriciel ou vectoriel) et ne cherchent pas de performance particulière dans d'autres domaines.L’architecture mémorielle des supercalculateurs est étudiée pour fournir en continu les données à chaque processeur afin d’exploiter au maximum sa puissance de calcul. Les performances supérieures de la mémoire (meilleurs composants et meilleure architecture) expliquent pour une large part l’avantage des superordinateurs sur les ordinateurs classiques.Leur système d’entrée/sortie (bus) est conçu pour fournir une large bande passante, la latence étant moins importante puisque ce type d’ordinateur n’est pas conçu pour traiter des transactions.Comme pour tout système parallèle, la loi d’Amdahl s’applique, les concepteurs de superordinateurs consacrant une partie de leurs efforts à éliminer les parties non parallélisables du logiciel et à développer des améliorations matérielles pour supprimer les goulots d'étranglement restants.D'une part, les superordinateurs ont souvent besoin de plusieurs mégawatts de puissance électrique. Cette alimentation doit aussi être de qualité. En conséquence, ils produisent une grande quantité de chaleur et doivent donc être refroidis pour fonctionner normalement. Le refroidissement (par exemple à air) de ces ordinateurs pose souvent un problème important de climatisation.D'autre part, les données ne peuvent circuler plus vite que la vitesse de la lumière entre deux parties d'un ordinateur. Lorsque la taille d’un superordinateur dépasse plusieurs mètres, le temps de latence entre certains composants se compte en dizaines de nanosecondes. Les éléments sont donc disposés pour limiter la longueur des câbles qui relient les composants. Sur le Cray-1 ou le Cray-II, par exemple, ils étaient disposés en cercle.De nos jours, ces ordinateurs sont capables de traiter et de communiquer de très importants volumes de données en très peu de temps. La conception doit assurer que ces données puissent être lues, transférées et stockées rapidement. Dans le cas contraire, la puissance de calcul des processeurs serait sous-exploitée (goulot d’étranglement).           En 1993, l'Institut de Physique du Globe de Paris (IPGP) opère un ordinateur CM-5/128 qui utilise des processeurs SuperSPARC, il est classé 25e au TOP500. Trois ans plus tard, en 1996, l'Institut du développement et des ressources en informatique scientifique (IDRIS) parvient à atteindre la 12e place mondiale avec le T3E construit par Cray.À la mi-2002, le plus puissant des supercalculateurs français se classe 4e au TOP500, c'est le TERA basé sur des processeurs Alpha à 1 GHz (AlphaServer SC45) et développé par Hewlett-Packard ; il appartenait au Commissariat à l'énergie atomique (CEA). En janvier 2006, le TERA-10 de Bull lui succède, il génère une puissance de calcul de 60 téraFLOPS et se placera au 5e rang mondial du TOP500.En juin 2008, l'IDRIS et son Blue Gene/P Solution d'IBM affiche, selon le test LINPACK, une puissance de 120 téraflops et remporte la 10e place.En novembre 2009, la première machine française a pour nom Jade. De type « SGI Altix (en) » elle est basée au Centre informatique national de l'enseignement supérieur (CINES) de Montpellier. Ce supercalculateur se classe au 28e rang mondial avec 128 téraflops au test LINPACK. Peu après, la configuration de la machine Jade est complétée pour atteindre une performance de 237 téraflops. La machine passe en juin 2010 au 18e rang du TOP500. C’est alors le troisième système informatique européen et le premier français, il est destiné à la recherche publique.En novembre 2010, le record français est détenu par le TERA-100 de Bull. Installé au CEA à Bruyères-le-Châtel pour les besoins de la simulation militaire nucléaire française, avec une performance de 1 050 téraflops, cette machine se hisse au 6e rang mondial et gagne le 1er rang européen. Elle est constituée de 17 296 processeurs Intel Xeon 7500 dotés chacun de huit cœurs et connectés par un réseau de type InfiniBand.En mars 2012, Curie, un système conçu par Bull pour le GENCI, installé sur le site du Très Grand Centre de Calcul (TGCC) à Bruyères-le-Châtel, dispose d'une puissance de 1,359 pétaflops. Il sera le supercalculateur le plus puissant de France en prenant la 9e place du classement mondial. Il est conçu pour délivrer 2 pétaflops.En janvier 2013, les systèmes Ada et Turing construits par IBM sont installés à l'IDRIS d'Orsay. La somme de leur puissance dépasse le pétaflops. Ces deux machines sont à la disposition des chercheurs. En mars 2013, le supercalculateur Pangea détenu par la société Total est inauguré, il devient le système le plus performant jamais installé en France. Sa puissance de calcul s'élève à 2,3 pétaflops. Équivalant à 27 000 ordinateurs de bureau réunis, il obtient la 11e place mondiale.En janvier 2015, le système Occigen, conçu par Bull, Atos technologies, pour le GENCI est installé sur le site du CINES ; il est doté d'une puissance de 2,1 pétaflops. Il se situait en 26e position au classement mondial du TOP500 de novembre 2014.En mars 2016, Total annonce avoir triplé la capacité de calcul de son supercalculateur Pangea, passant à une puissance de calculs de 6,7 pétaflops en pics de performance et de 5,28 pétaflops en puissance utilisable. Cela lui permet de retrouver le 11e rang au TOP500 et le place ainsi en tête du secteur industriel mondial.En juin 2022, le GENCI met en service Adastra, un superordinateur fourni par HPE-Cray hébergé au CINES. Ses 46,10 pétaflops lui permettent de gagner le 10e rang mondial en termes de performances de calcul.L'essor des supercalculateurs a vu Linux devenir le système d'exploitation équipant la majorité des 500 supercalculateurs les plus puissants de la planète,, Unix perdant progressivement du terrain face à Linux, mais occupant pendant un temps une place de choix sur le marché des supercalculateurs (5 %).[réf. souhaitée]Windows ne fut exécuté que par deux des 500 supercalculateurs les plus puissants de la planète, soit 0,4 %, tandis que BSD n'était présent que sur une seule machine du top 500, soit 0,2 %. Enfin, les autres configurations (« Mixed », soit un ensemble de plusieurs types de systèmes d'exploitation) représentaient 4,6 %.[réf. souhaitée]En novembre 2017, Linux équipe la totalité des 500 superordinateurs les plus puissants au monde.Georges Karadimas (Snecma), « Les superordinateurs dans le secteur aérospatial français », dans Nouvelle revue Aéronautique et Astronautique, no 2, juin 1994  (ISSN 1247-5793).Site HPC du commissariat à l'énergie atomique (CEA)Site officiel du centre informatique national de l'enseignement supérieur (CINES)Site officiel de l'institut du développement et des ressources en informatique scientifique (IDRIS) Portail de l’informatique"
informatique;"Un système de traitement de l'information est un système constitué d'un ensemble de composants (mécaniques, électroniques, chimiques, photoniques ou biologiques) permettant de traiter automatiquement des informations. Il est régi par la théorie de l'information et c'est l'élément central de tout appareil informatique.Les premiers systèmes permettant de traiter automatiquement des informations étaient des appareils mécaniques tel que la pascaline ou encore la machine d'Anticythère qui représentent les tout premiers calculateurs spécifiques à l'inverse de l'ordinateur qui est programmable et universel.Aujourd'hui[Quand ?] il existe une panoplie d'appareils en électronique numérique permettant de traiter automatiquement des informations : commutateur réseau, ordinateur, console de jeu, calculatrice, certaines cartes à puce, distributeur automatique de billets, enregistreur vidéo personnel, GPS, téléphone portable, etc.Avant d'être un appareil fonctionnel, le système de traitement de l'information est d'abord un objet d'étude conceptuelle qui propose une approche systémique à l'informaticien. Il permet d'élargir les champs de recherche en informatique et permet de catégoriser les systèmes de traitement par rapport à leurs capacités plutôt qu'à leurs fonctionnements.Un système de traitement de l'information se caractérise par un minimum de quatre unités :l'unité d'entrée (anglais : input), recueille l'information ;l'unité de stockage (anglais : storage) conserve toute ou une partie de l'information ;l'unité de traitement (anglais : processing), transforme l'état de l'information ;l'unité de sortie (anglais : output) présente le résultat de la modification.L'évolution de l'état d'une information ne peut être observée qu'en accord avec la notion de temps. L'horloge est le système de traitement de l'information le plus élémentaire. Tous les systèmes de traitement de l'information sont jusqu'à présent mus par un système de balancement ou de mouvement oscillatoire dont la source d'entraînement est naturelle (ex. : l'actionnement d'une manivelle) ou artificielle (rotation d'un moteur, alimentation électrique d'un quartz, etc.).L'information est représenté par un  signal de type analogique ou numérique et qui peut être véhiculé sous forme matériel (engrenage, molécule, etc.) ou énergétique (particule élémentaire).La Pascaline est le premier calculateur mécanique. Il a été construit par Blaise Pascal en 1642.La première conceptualisation d'un système de traitement de l'information programmable et universel est appelé machine de Turing. C'est la thèse de Turing qui est aujourd'hui considérée comme l'article fondateur de l'ordinateur.Le premier calculateur électronique à utiliser le système binaire est l'EDVAC. Construit en 1945, il occupait une salle de 45 m2, et pesait près de 8 tonnes.C'est l'invention du transistor en 1947 et celle du circuit intégré en 1958 qui ont permis la miniaturisation électronique des systèmes de traitement de l'information.La première console de jeu, l'Odyssey a été construite en 1973. C'était un système de traitement de l'information qui n'était pas Turing-complet.Le premier type d'informations que les systèmes étaient capables de manipuler étaient des nombres, puis des textes, jusqu'à l'arrivée dans les années 1980 des systèmes multimédia, c'est-à-dire capables de manipuler divers types d'informations: images, sons, vidéos…Exemples d'informations :nombres : prix, poids, volume, température, vitesse, pression…textes : courrier, publications, articles de presse…images : plans, dessins, graphiques, diagrammes, cartes géographiques, photos, images 3D…sons : paroles, chants, bruitages ou musique ,vidéo : prises de vue, clips ou films ;les instructions d'un programme informatique sont aussi des informations.Dans le système de traitement de l'information, les informations circulent sous forme de suite de bits (chiffres en base 2) et le octet groupés dans des fichiers ou des enregistrements (voir : électronique numérique).Un format désigne la manière dont les bits sont disposés à l'intérieur du fichier ou de l'enregistrement pour stocker l'information.Un protocole est un ensemble de règles normalisées qui, lorsqu'elles sont appliquées de manière commune par deux appareils, leur permettent de s'échanger des informations.Les règles établies par un protocole concernent autant le point de vue électronique (tensions, fréquences) que le point de vue informationnel (choix des informations, format) ainsi que le déroulement des opérations de communication (qui initie la communication, comment réagit le correspondant, combien de temps dure la communication…).Les protocoles sont utilisés en informatique et en télécommunication (téléphonie, télévision).Un système de traitement de l'information est composé de quatre unités :l'unité d'entrée (anglais : input), qui permet de faire entrer les informations dans le système ;l'unité de stockage (anglais : storage) qui permet de conserver les informations ;l'unité de traitement (anglais : processor) qui comme son nom l'indique va traiter les informations ;l'unité de sortie' (anglais : output) qui permet de faire sortir les résultats des traitements.Les informations peuvent être introduites par une personne à l'aide d'un clavier et d'une souris, enregistrées à l'aide de différents appareils — microphone, caméra, scanner, appareil photo, ou apportées par des dispositifs de télécommunication (voir: réseau informatique).Les premiers appareils permettant d'introduire des informations étaient des lecteurs de cartes perforées. Un dispositif semblable à celui des pianos mécaniques. Cette technologie datant du XVIIIe siècle a été utilisée en informatique jusque dans les années 1980.La première étape des traitements consiste en la réceptions des informations en provenance des différents appareils.La numérisation est le procédé qui consiste à transformer une information provenant du monde réel en une suite de chiffres qui seront utilisés dans le système de traitement de l'information.Les informations stockées peuvent être des informations qui viennent d'être entrées dans la machine, ou résultats d'un traitement.Les informations sont conservées dans des dispositifs de stockage tels que disque durs, DVD, CD-ROM ou mémoire flash.La possibilité de stocker des informations existe depuis les années 1960, auparavant les informations étaient traitées à mesure qu'elles étaient entrées.Le système de traitement des informations effectue les traitements en suivant scrupuleusement les  instructions d'un programme informatique.Les traitements peuvent consister à :à partir de certaines informations, d'obtenir d'autres informations, par exemple par calcul ;transformer les informations ;stocker les informations dans le système d'informations, en vue d'effectuer des traitements plus tard ;extraire des informations préalablement stockées.Exemples de traitements :tri, classement, recherche ;calculs de comptabilité, statistiques, analyses de physique ou d'économie ;correction orthographique ;reconnaissance de texte ;reconnaissance vocale ;traitement d'images tels que fausse couleur, négatif.Un composant électronique qui effectue des traitements s'appelle un processeur.La sortie est l'étape finale des traitements qui consiste à faire sortir les résultats du système d'informations.Selon leur nature, les informations des résultats peuvent être restituées sur un écran, du papier par une imprimante ou un traceur, des enceintes audio ou tout autre appareil. Les informations peuvent aussi être transportées vers d'autres systèmes par des moyens de télécommunication (voir : réseau informatique).Boîte noireInformatiqueOrdinateurSystème binaireMario Borillo, Informatique pour les sciences de l'homme : limites de la formalisation du raisonnement, Éditions Mardaga, 1984  (ISBN 2-8700-9202-4) Portail de l’informatique"
informatique;"La théorie de l'information, sans précision, est le nom usuel désignant la théorie de l'information de Shannon, qui est une théorie probabiliste permettant de quantifier le contenu moyen en information d'un ensemble de messages, dont le codage informatique satisfait une distribution statistique précise. Ce domaine trouve son origine scientifique avec Claude Shannon qui en est le père fondateur avec son article A Mathematical Theory of Communication publié en 1948.Parmi les branches importantes de la théorie de l'information de Shannon, on peut citer :le codage de l'information ;la mesure quantitative de redondance d'un texte ;la compression de données ;la cryptographie.Dans un sens plus général, une théorie de l'information est une théorie visant à quantifier et qualifier la notion de contenu en information présent dans un ensemble de données. À ce titre, il existe une autre théorie de l'information : la théorie algorithmique de l'information, créée par Kolmogorov, Solomonoff et Chaitin au début des années 1960.L'information est un concept physique nouveau qui a surgi dans un champ technologique. Le concept théorique d'information a été introduit à partir de recherches théoriques sur les systèmes de télécommunication. L'origine de ces recherches remonte aux études entreprises dès la fin du XIXe siècle, en physique et en mathématique par Boltzmann et Markov sur la notion de probabilité d'un événement et les possibilités de mesure de cette probabilité. Plus récemment, avant la Seconde Guerre mondiale, les contributions les plus importantes sont dues à la collaboration des mathématiciens et des ingénieurs des télécommunications, qui ont été amenés à envisager les propriétés théoriques de tout système de signaux utilisé par les êtres, vivants ou techniques, à des fins de communication.À la suite des travaux de Hartley (1928), Shannon (1948) détermine l'information comme grandeur mesurable, sinon observable — car nul n'a jamais vu l'information — et celle-ci devient la poutre maîtresse de la théorie de la communication qu'il élabore avec Warren Weaver.Cette théorie est née de préoccupations techniques pratiques. La société Bell cherche à transmettre les messages de la façon à la fois la plus économique et la plus fiable. Aussi le cadre originel de la théorie est celui d'un système de communications où un émetteur transmet un message à un récepteur à travers un canal matériel/énergétique donné. Émetteur et récepteur ont par hypothèse un répertoire commun, un code qui contient les catégories de signaux utilisables. Ainsi le message codé est transmis, de l'émetteur au récepteur à travers le canal, sous forme de signes ou signaux portés par de la matière/énergie.Ainsi, le concept d'information a été l'objet d'une théorie que la postérité a choisi d'appeler « théorie de l'information » alors qu'il s'agissait, à proprement parler, d'une théorie mathématique de la communication de l'information ; or cette expression est exactement celle de Shannon et Weaver. Cette source de confusion est régulièrement rappelée dans la littérature. On dit, en pareil cas, que l'expression abrégée a été retenue par l'usage ; l'emploi du sigle TMCI clarifierait pourtant bien la situation.Cette théorie mathématique appliquée aux techniques de la télécommunication a été élaborée plus spécialement par Claude Shannon, ingénieur à la Compagnie des Téléphones Bell et reste jusqu'à nos jours la base du concept dit scientifique d'information. Cependant, cette théorie ne pourrait s'appuyer ni sur la forme matérielle/énergétique, ni sur le contenu cognitif des messages émis : leur contenu sémantique est laissé de côté, de même que leur contenant physique, pour ne s'intéresser qu'aux aspects mathématiques et communicationnels.Dans sa conception originale, la théorie de l'information de Shannon s'est limitée à analyser les moyens à mettre en œuvre dans les techniques de télécommunication pour transmettre l'information le plus rapidement possible et avec le maximum de sécurité. Elle s'est donc efforcée de développer des méthodes susceptibles de minimiser la probabilité d'erreur dans la reconnaissance du message. Une notion fondamentale sera nécessaire pour développer ces méthodes : la mesure de l'information, au sens mathématique du terme.Pour Shannon, l'information présente un caractère essentiellement aléatoire. Un événement aléatoire est par définition incertain. Cette incertitude est prise comme mesure de l'information. Une information sera donc uniquement définie par sa probabilité (I = – log p). Donc l'information est la mesure de l'incertitude calculée à partir de la probabilité de l'événement. Shannon a donc confondu la notion d'information et de mesure d'incertitude. Il faut remarquer que dans cette définition l'information est bien synonyme de mesure d'incertitude. Dans cet ordre d'idée, plus une information est incertaine, plus elle est intéressante, et un événement certain ne contient aucune information. En théorie de l'information de Shannon, il s'agit donc de raisonner en probabilité et non en logique pure.L'information de Shannon se mesure en unités binaires dites bits. Le bit peut être défini comme un événement qui dénoue l'incertitude d'un récepteur placé devant une alternative dont les deux issues sont pour lui équiprobables. Plus les éventualités que peut envisager ce récepteur sont nombreuses, plus le message comporte d'événements informatifs, plus s'accroît la quantité de bits transmis. Il est clair que nul récepteur ne mesure en bits l'information obtenue dans un message. C'est seulement le constructeur d'un canal de télécommunication qui a besoin de la théorie, et mesure l'information en bits pour rendre la transmission de message la plus économique et la plus fiable.La notion d'information d'après Shannon est nécessairement associée à la notion de « redondance » et à celle de « bruit ». Par exemple, en linguistique l'information n'est ni dans le mot, ni dans la syllabe, ni dans la lettre. Il y a des lettres voire des syllabes qui sont inutiles à la transmission de l'information que contient le mot : il y a dans une phrase, des mots inutiles à la transmission de l'information. La théorie de Shannon appelle redondance tout ce qui dans le message apparaît comme en surplus. Aussi est-il économique de ne pas transmettre la redondance.L'information chemine à travers un canal matériel/énergétique : fil téléphonique, onde radio, etc. Or, dans son cheminement, l'information rencontre du bruit. Le bruit est constitué par les perturbations aléatoires de toutes sortes qui surgissent dans le canal de transmission et tendent à brouiller le message. Le problème de la dégradation de l'information par le bruit est donc un problème inhérent à sa communication. Ici, l'idée de redondance présente une face nouvelle ; alors qu'elle apparaît comme un surplus inutile sous l'angle économique, elle devient, sous l'angle de la fiabilité de la transmission un fortifiant contre le bruit, un préventif contre les risques d'ambiguïté et d'erreur à la réception.Très vite de multiples applications de la théorie de l'information de Shannon sont apparues dans le domaine des sciences humaines : les modèles mathématiques élaborés ont permis de préciser certains concepts utilisés couramment dans les analyses linguistiques structurales, en même temps qu'ils faisaient apparaître les limites inhérentes à ce type d'analyse et provoquaient des recherches nouvelles (en traduction automatique et en psycho-linguistique). Tandis que se développait un champ scientifique nouveau : la cybernétique.Cependant, une caractéristique majeure de la théorie de Shannon est de donner à la notion d'information (telle que définie par cette théorie) un statut physique à part entière. Effectivement, l'information acquiert les caractères fondamentaux de toute réalité physique organisée : abandonnée à elle-même, elle ne peut évoluer que dans le sens de sa désorganisation, c'est-à-dire l'accroissement d'entropie ; de fait, l'information subit, dans ses transformations (codage, transmission, décodage, etc.), l'effet irréversible et croissant de la dégradation. Par conséquent Shannon définit comme entropie d'information la mesure H (H = – K log p). De façon étonnante, l'équation par laquelle Shannon définit l'entropie de l'information coïncide, à un facteur multiplicatif près, avec l'équation de Boltzmann-Gibbs définissant l'entropie S en thermodynamique (S = – K log p). Cet épisode important a été abondamment commenté.Certains, comme Couffignal, ont soutenu que la coïncidence est sans signification : l'application de la fonction de Shannon à la thermodynamique et à l'information serait un hasard de rencontre de l'application d'une même formule mathématique, sans plus. Certes, il peut y avoir rencontre de deux équations de probabilité provenant d'univers différents.À l'inverse, Brillouin avait prétendu établir une relation logique entre le H de Shannon et le S de Boltzmann, ce que retiennent la plupart des chercheurs qui appliquent la théorie aux disciplines non mathématiques, la biologie en particulier. Selon ce point de vue, il est possible d'inscrire l'information telle que définie par Shannon dans la physique. En effet, il existe une dualité dans le concept d'information reliant l'information à la matière/énergie véhiculant cette information. L'information telle que définie par Shannon s'enracine ainsi dans la physique d'une part, dans les mathématiques d'autre part, mais sans qu'on puisse la réduire aux maîtres-concepts de la physique classique : masse et énergie. Comme le dit Wiener : « l'information n'est ni la masse, ni l'énergie, l'information est l'information », ce qui laisse la porte ouverte à des conceptions diverses, à commencer par celle d'un troisième constituant de l'univers, après la matière et l'énergie précisément !La théorie mathématique de l'Information résulte initialement des travaux de Ronald Aylmer Fisher. Celui-ci, statisticien, définit formellement l'information comme égale à la valeur moyenne du carré de la dérivée partielle (δ) du logarithme naturel de la loi de probabilité étudiée.                                          I                          (        θ        )        =                  E                          {                                                                                    [                                                                                    ∂                                                  ∂                          θ                                                                                      ln                    ⁡                    L                    (                    X                    ;                    θ                    )                                    ]                                                  2                                            |                        θ                    }                      {\displaystyle {\mathcal {I}}(\theta )=\mathrm {E} \left\{\left.\left[{\frac {\partial }{\partial \theta }}\ln L(X;\theta )\right]^{2}\right|\theta \right\}}  À partir de l'inégalité de Cramer, on déduit que la valeur d'une telle information est proportionnelle à la faible variabilité des conclusions résultantes. En termes simples, moins une observation est probable plus elle est porteuse d'information. Par exemple, lorsque le journaliste commence le journal télévisé par la phrase « Bonsoir », ce mot, qui présente une forte probabilité, n'apporte que peu d'information. En revanche, si la première phrase est, par exemple « La France a peur  », sa faible probabilité fera que l'auditeur apprendra qu'il s'est passé quelque chose, et, partant, sera plus à l'écoute.D'autres modèles mathématiques ont complété et étendu de façon formelle la définition de l'information.Claude Shannon et Warren Weaver renforcent le paradigme. Ils sont ingénieurs en télécommunication et se préoccupent de mesurer l'information pour en déduire les fondamentaux de la Communication (et non une théorie de l'information). Dans Théorie Mathématique de la Communication en 1948, ils modélisent l'information pour étudier les lois correspondantes : bruit, entropie et chaos, par analogie générale aux lois d'énergétique et de thermodynamique. Leurs travaux complétant ceux d'Alan Turing, de Norbert Wiener et de John von Neumann (pour ne citer que les principaux) constituent le socle initial de la théorie du signal  et des « sciences de l'information ».Pour une source X comportant n symboles, un symbole xi ayant une probabilité pi = P(X = xi) d'apparaître, l'entropie H de la source X est définie comme :                    H        (        X        )        =        −                  ∑                      i                                n                                    p                      i                                    log                      2                          ⁡        (                  p                      i                          )              {\displaystyle H(X)=-\sum _{i}^{n}p_{i}\log _{2}(p_{i})}  Au départ, c'était le logarithme naturel, à base                     10              {\displaystyle 10}  , qui était utilisé. Mais la base                     2              {\displaystyle 2}   est justifiée par un étalonnage. On considère l'expérience probabiliste la plus élémentaire : le tirage aléatoire à deux issues équiprobables, pile ou face, chacune de probabilité                     1                  /                2              {\displaystyle 1/2}  . En imposant que la quantité d'information fournie par l'issue d'un tirage aléatoire à pile ou face soit de                     1              {\displaystyle 1}  , c'est-à-dire que l'on doive avoir                     −                  log                      a                          ⁡        (        1                  /                2        )        =        1              {\displaystyle -\log _{a}(1/2)=1}  , on trouve que                     a        =        2              {\displaystyle a=2}  . La valeur de                     1              {\displaystyle 1}   avec cette base                     2              {\displaystyle 2}   du logarithme définit l'unité de mesure de l'information, le shannon (avec une minuscule), couramment appelé le bit (voir article shannon (unité)).Les considérations d'entropie maximale (MAXENT) permettront à l'inférence bayésienne de définir de façon rationnelle ses distributions a priori.L'informatique constituera une déclinaison technique automatisant les traitements (dont la transmission et le transport) d'information. L'appellation « Technologies de l'Information et de la Communication » recouvre les différents aspects (systèmes de traitements, réseaux, etc.) de l'informatique au sens large.Les sciences de l'information dégagent du sens depuis des données en s'appuyant sur les notions de corrélation, d'entropie et d'apprentissage (voir Fouille de données). Les technologies de l'information, quant à elles, s'occupent de la façon de concevoir, implémenter et déployer des solutions pour répondre à des besoins identifiés.Adrian Mc Donough dans Information economics définit l'information comme la rencontre d'une donnée et d'un problème. La connaissance est une information potentielle. Le rendement informationnel d'un système de traitement de l'information est le quotient entre le nombre de bits du réservoir de données et celui de l'information extraite. Les données sont l'aspect coût du système, l'information, l'aspect valeur. Il en résulte que lorsqu'un informaticien calcule la productivité de son système par le rapport entre la quantité de données produites et le coût financier, il commet une erreur, car les deux termes de l'équation négligent la quantité d'information réellement produite. Cette remarque prend tout son sens à la lumière du grand principe de Russell Ackoff qui postule qu'au-delà d'une certaine masse de données, la quantité d'information baisse et qu'à la limite elle devient nulle. Ceci correspond à l'adage « trop d'information détruit l'information ». Ce constat est aggravé lorsque le récepteur du système est un processeur humain, et pis encore, le conscient d'un agent humain. En effet, l'information est tributaire de la sélection opérée par l'attention, et par l'intervention de données affectives, émotionnelles, et structurelles absentes de l'ordinateur. L'information se transforme alors en sens, puis en motivation. Une information qui ne produit aucun sens est nulle et non avenue pour le récepteur humain, même si elle est acceptable pour un robot. Une information chargée de sens mais non irriguée par une énergie psychologique (drive, cathexis, libido, ep, etc.) est morte. On constate donc que dans la chaîne qui mène de la donnée à l'action (données → information → connaissance → sens → motivation), seules les deux premières transformations sont prises en compte par la théorie de l'information classique et par la sémiologie. Kevin Bronstein remarque que l'automate ne définit l'information que par deux valeurs : le nombre de bits, la structure et l'organisation des sèmes, alors que le psychisme fait intervenir des facteurs dynamiques tels que passion, motivation, désir, répulsion, etc. qui donnent vie à l'information psychologique.Une information désigne, parmi un ensemble d'événements, un ou plusieurs événements possibles.En théorie, l'information diminue l'incertitude. En théorie de la décision, on considère même qu'il ne faut appeler « information » que ce qui est « susceptible d'avoir un effet sur nos décisions ».En pratique, l'excès d'information, tel qu'il se présente dans les systèmes de messagerie électronique, peut aboutir à une saturation, et empêcher la prise de décision.Soit une source pouvant produire des tensions entières de 1 à 10 volts et un récepteur qui va mesurer cette tension. Avant l'envoi du courant électrique par la source, le récepteur n'a aucune idée de la tension qui sera délivrée par la source. En revanche, une fois le courant émis et reçu, l'incertitude sur le courant émis diminue. La théorie de l'information considère que le récepteur possède une incertitude de 10 états.Une bibliothèque possède un grand nombre d'ouvrages, des revues, des livres et des dictionnaires. Nous cherchons un cours complet sur la théorie de l'information. Tout d'abord, il est logique que nous ne trouverons pas ce dossier dans des ouvrages d'arts ou de littérature ; nous venons donc d'obtenir une information qui diminuera notre temps de recherche. Nous avions précisé que nous voulions aussi un cours complet, nous ne le trouverons donc ni dans une revue, ni dans un dictionnaire. Nous avons obtenu une information supplémentaire (nous cherchons un livre), qui réduira encore le temps de notre recherche.Il faut moins d'octets pour écrire « chien » que « mammifère ». Pourtant l'indication « Médor est un chien » contient bien plus d'information que l'indication « Médor est un mammifère » : le contenu d'information sémantique d'un message dépend du contexte. En fait, c'est le couple message + contexte qui constitue le véritable porteur d'information, et jamais le message seul (voir paradoxe du compresseur).Le mot même de « message » n'a d'ailleurs de sens qui si on postule un émetteur (conscient ou non, par exemple un phénomène créant des ondes gravitationnelles) et un récepteur soit réel (LIGO) soit hypothétique (par exemple message d'un naufragé glissé dans une bouteille), en plus des informations de contexte : langue, dictionnaire, grammaire.Considérons N boîtes numérotées de 1 à N. Un individu A a caché au hasard un objet dans une de ces boîtes. Un individu B doit trouver le numéro de la boîte où est caché l'objet. Pour cela, il a le droit de poser des questions à l'individu A auxquelles celui-ci doit répondre sans mentir par OUI ou NON. Mais chaque question posée représente un coût à payer par l'individu B (par exemple un euro). Un individu C sait dans quelle boîte est caché l'objet. Il a la possibilité de vendre cette information à l'individu B. B n'acceptera ce marché que si le prix de C est inférieur ou égal au coût moyen que B devrait dépenser pour trouver la boîte en posant des questions à A. L'information détenue par C a donc un certain prix. Ce prix représente la quantité d'information représentée par la connaissance de la bonne boîte : c'est le nombre moyen de questions à poser pour identifier cette boîte. Nous la noterons I.ExempleSi N = 1, I = 0.Il n'y a qu'une seule boîte. Aucune question n'est nécessaire.Si N = 2, I = 1.On demande si la bonne boîte est la boîte no 1. La réponse OUI ou NON détermine alors sans ambiguïté quelle est la boîte cherchée.Si N = 4, I = 2.On demande si la boîte porte le no 1 ou 2. La réponse permet alors d'éliminer deux des boîtes et il suffit d'une dernière question pour trouver quelle est la bonne boîte parmi les deux restantes.Si N = 2k, I = k.On écrit les numéros des boîtes en base 2. Les numéros ont au plus k chiffres binaires, et pour chacun des rangs de ces chiffres, on demande si la boîte cherchée possède le chiffre 0 ou le chiffre 1. En k questions, on a déterminé tous les chiffres binaires de la bonne boîte. Cela revient également à poser k questions, chaque question ayant pour but de diviser successivement le nombre de boîtes considérées par 2 (méthode de dichotomie).On est donc amené à poser I = log2(N), mais cette configuration ne se produit que dans le cas de N événements équiprobables.Supposons maintenant que les boîtes soient colorées, et qu'il y ait n boîtes rouges. Supposons également que C sache que la boîte où est caché l'objet est rouge. Quel est le prix de cette information ? Sans cette information, le prix à payer est log2(N). Muni de cette information, le prix à payer n'est plus que log2(n). Le prix de l'information « la boîte cherchée est rouge » est donc log2(N) – log2(n) = log2(N/n).On définit ainsi la quantité d'information comme une fonction croissante de N⁄n avec :N le nombre d'évènements possibles ;n le nombre d'éléments du sous-ensemble délimité par l'information.Afin de mesurer cette quantité d'information, on pose :                    I        =                  log                      2                          ⁡                  (                                    N              n                                )                      {\displaystyle I=\log _{2}\left({\frac {N}{n}}\right)}  I est exprimé en bit (ou « logon », unité introduite par Shannon[réf. nécessaire], de laquelle, dans les faits, bit est devenu un synonyme), ou bien en « nat » si on utilise le logarithme naturel à la place du logarithme de base 2.Cette définition se justifie, car l'on veut les propriétés suivantes :l'information est comprise entre 0 et ∞ ;un évènement avec peu de probabilité représente beaucoup d'information (exemple : « Il neige en janvier » contient beaucoup moins d'information que « Il neige en août » pour peu que l'on soit dans l'hémisphère nord) ;l'information doit être additive.Remarque : lorsqu'on dispose de plusieurs informations, la quantité d'information globale n'est pas la somme des quantités d'information. Ceci est dû à la présence du logarithme. Voir aussi : information mutuelle, information commune à deux messages, qui, dans l'idée, explique cette « sous-additivité » de l'information.Supposons maintenant que les boîtes soient de diverses couleurs : n1 boîtes de couleur C1, n2 boîtes de couleur C2…, nk boîtes de couleurs Ck, avec n1 + n2 + … + nk = N. La personne C sait de quelle couleur est la boîte recherchée. Quel est le prix de cette information ? L'information « la boîte est de couleur C1 » vaut log N/n1, et cette éventualité a une probabilité n1/N. L'information « la boîte est de couleur C2 » vaut log N/n2, et cette éventualité a une probabilité n2/N… Le prix moyen de l'information est donc n1/N log N/n1 + n2/N log N/n2 + … + nk/N log N/nk. Plus généralement, si on considère k évènements disjoints de probabilités respectives p1, p2…, pk avec p1 + p2 + … + pk = 1, alors la quantité d'information correspondant à cette distribution de probabilité est p1 log 1/p1 + … + pk log 1/pk. Cette quantité s'appelle entropie de la distribution de probabilité.L'entropie permet donc de mesurer la quantité d'information moyenne d'un ensemble d'évènements (en particulier de messages) et de mesurer son incertitude. On la note H :                    H                  (          I          )                =        −                  ∑                      i            ∈            I                                    p                      i                                    log                      2                                            p                      i                                {\displaystyle H\left(I\right)=-\sum _{i\in I}p_{i}\log _{2}\;p_{i}}  avec                               p                      i                          =                                            n                              i                                      N                                {\displaystyle p_{i}={\frac {n_{i}}{N}}}   la probabilité associée à l'apparition de l'évènement i.On considère une suite de symboles. Chaque symbole peut prendre deux valeurs s1 et s2 avec des probabilités respectivement p1 = 0,8 et p2 = 0,2. La quantité d'information contenue dans un symbole est :                              p                      1                          ×                  log                      2                          ⁡                              1                          p                              1                                                    +                  p                      2                          ×                  log                      2                          ⁡                              1                          p                              2                                                    ≈        0        ,        7219              {\displaystyle p_{1}\times \log _{2}{\frac {1}{p_{1}}}+p_{2}\times \log _{2}{\frac {1}{p_{2}}}\approx 0,7219}  Si chaque symbole est indépendant du suivant, alors un message de N symboles contient en moyenne une quantité d'information égale à 0,72N. Si le symbole s1 est codé 0 et le symbole s2 est codé 1, alors le message a une longueur de N, ce qui est une perte par rapport à la quantité d'information qu'il porte. Les théorèmes de Shannon énoncent qu'il est impossible de trouver un code dont la longueur moyenne soit inférieure à 0,72N, mais qu'il est possible de coder le message de façon que le message codé ait en moyenne une longueur aussi proche que l'on veut de 0,72N lorsque N augmente.Par exemple, on regroupe les symboles trois par trois et on les code comme suit :Le message s1s1s1s1s1s2s2s2s1 sera codé 010011110.La longueur moyenne du code d'un message de N symboles est :                                          N            3                          (        0        ,        512        +        3        ×        0        ,        128        ×        3        +        3        ×        0        ,        032        ×        5        +        0        ,        008        ×        5        )        =        0        ,        728        N              {\displaystyle {N \over 3}(0,512+3\times 0,128\times 3+3\times 0,032\times 5+0,008\times 5)=0,728N}  L'une des caractéristiques fondamentales de cette théorie est l'exclusion de la sémantique. La théorie de l'information est indifférente à la signification des messages. Le sens d'un message peut pourtant être considéré comme essentiel dans la caractérisation de l'information. Mais le point de vue de la théorie de l'information se limite à celui d'un messager dont la fonction est de transférer un objet.La théorie de l'information de Shannon est toujours relative à un ensemble de données, une famille de chaînes de caractères, caractérisée par une loi de distribution bien précise. Elle donne donc un contenu en information en moyenne, ce qui en fait une théorie probabiliste, particulièrement bien adaptée au contexte de la transmission de donnée, et dans ce cadre cette théorie a produit des résultats importants. En revanche, elle n'est pas en mesure de quantifier le contenu en information d'une chaine prise isolément, un brin d'ADN par exemple, alors que la théorie algorithmique de l'information en est capable jusqu'à un certain point. Mais cette dernière théorie possède également ses propres limitations. C'est pourquoi il ne faut pas considérer que la notion d'information est entièrement cernée par la théorie de l'information de Shannon, ou la théorie algorithmique de l'information, mais que cette notion a besoin d'une variété de modélisations formelles pour s'exprimer.L'information de Fisher semble ainsi parfois avantageusement remplacer l'information de Shannon dans la mesure où elle est une quantification locale et non globale de l'information contenue dans une distribution. Cela dit, les deux notions sont liées et peuvent dans diverses applications mener aux mêmes résultats.Léon Brillouin Science et théorie de l'information, J. Gabay, 2000  (ISBN 2876470365)Léon Brillouin Science and information theory (typographie plus lisible, mais version en anglais)(en) [PDF] C. E. Shannon « A Mathematical Theory of Communication », sur L’Institut d’électronique et d’informatique Gaspard-Monge (Reprinted with corrections from The Bell System Technical Journal, Vol. 27, p. 379–423, 623–656, July, October, 1948.)Claude Shannon et Warren Weaver, Théorie mathématique de la communication, Paris, Cassini, coll. « le sel et le fer », 2018 (ISBN 978-2-84225-222-9), compte rendu par Olivier Rioul, « Une théorie mathématique de la communication », Bibnum. Textes fondateurs de la science analysés par les scientifiques d'aujourd'hui,‎ 2018 (lire en ligne)Thomas M. Cover, Joy A. Thomas, Elements of Information Theory, Wiley-Interscience, 2006 (ISBN 978-0-471-24195-9) [détail des éditions](en) David MacKay, Information Theory, Inference, and Learning Algorithms, Cambridge University Press, 2003 (ISBN 0-521-64298-1) [détail des éditions]Théorie algorithmique de l'informationAutorégulationCodage de l'informationCompression de donnéesLoi de Zipf et sa généralisation par MandelbrotProjet:Sciences de l'information et des bibliothèquesSciences de l'information et de la communicationSciences de l'information et des bibliothèquesTechnologies de l'information et de la communicationTraitement de l'informationTransport de l'information« Un cours de théorie de l’information par Louis Wehenkel », sur Université de LiègeVidéo sur l'entropie dans la théorie de l'information Portail de l’informatique   Portail des mathématiques   Portail de l'informatique théorique   Portail des télécommunications"
informatique;"La théorie de la complexité est le domaine des mathématiques, et plus précisément de l'informatique théorique, qui étudie formellement le temps de calcul, l'espace mémoire (et plus marginalement la taille d'un circuit, le nombre de processeurs, l'énergie consommée …) requis par un algorithme pour résoudre un problème algorithmique. Il s'agit donc d'étudier la difficulté intrinsèque des problèmes, de les organiser par classes de complexité et d'étudier les relations entre les classes de complexité.Considérons l'exemple du problème du voyageur de commerce. La donnée du problème est un ensemble de villes et de distances séparant ces villes. L'objectif du problème est de trouver un plus court circuit qui passe une et une seule fois par toutes les villes. Il existe un problème de décision associé : étant donné un ensemble de villes, les distances entre villes et un entier k, déterminer s'il existe un circuit qui passe une et une seule fois par toutes les villes de longueur inférieure à k. Ainsi, on distingue deux types de problèmes.Les problèmes de décision posent une question dont la réponse est oui ou non ; dans le cas du problème du voyageur de commerce : existe-il oui ou non un circuit de longueur inférieure à k ?Les problèmes de recherche d'une solution comportent une question ou plutôt une injonction de la forme « renvoyer un élément tel que… » dont la réponse consiste à fournir un tel élément; dans le cas du problème du voyageur de commerce, exhiber un circuit de longueur minimale. Il s'agit donc d'un problème fonctionnel, et il sera donc catégorisé dans une classe fonctionnelle, par exemple FP si la solution est calculée en temps polynomial.La théorie de la complexité étudie principalement (mais pas uniquement) les problèmes de décision.Un exemple de problème de décision est :« Étant donné un entier n, est-il premier ? » (PRIME)Dans le cadre de la théorie de la complexité, la donnée d'un problème s'appelle une instance. Une instance du problème du voyageur de commerce est la donnée de villes et de distances séparant les villes. Comme on mesure la complexité en fonction de la taille d'une instance, la représentation (le codage) d'une instance joue un rôle important. Par exemple, un entier comme 13 peut être représenté en unaire (IIIIIIIIIIIII) ou en binaire (1101). En unaire, la taille de l'instance 13 est 13 et en binaire, la taille de l'instance est 4 (car il y a quatre chiffres dans 1101).Un problème de recherche peut parfois être transformé en un problème de décision équivalent. Par exemple, le problème du voyageur de commerce qui cherche, dans un graphe dont les arêtes sont étiquetées par des coûts, à trouver un cycle de coût minimum, passant une fois par chaque sommet, peut s'énoncer en un problème de décision comme suit : Existe-t-il un cycle hamiltonien (passant une fois par chaque sommet) de coût inférieur à k (donné dans l'instance) ? L'équivalence de ces deux problèmes doit être démontrée, elle montre que la démonstration d'existence repose sur un argument constructif, c'est-à-dire, par exemple, dans le cas du voyageur de commerce, fournissant effectivement un cycle. Ceci n'est pas vrai en général : par exemple le problème PRIME ci-dessus est soluble en temps polynomial, mais aucune solution (algorithme) polynomiale n'est connue pour le problème de recherche associé : la factorisation, fournir les facteurs d'un nombre naturel.Le problème de rechercher un cycle de coût minimum est équivalent au problème du voyageur de commerce, au sens où si l'on sait résoudre efficacement l'un, on sait aussi résoudre efficacement l'autre. Dans la suite de cet article, nous ne parlerons que de problèmes de décision, mais il existe une branche de la complexité dédiée aux problèmes fonctionnels.Si toute entrée est acceptée, autrement dit si le problème n'a pas de précondition, un problème de décision peut être vu comme l'ensemble des instances positives, c'est-à-dire les instances pour lesquelles la réponse est ""oui"". Par exemple, pour le problème de primalité, le problème de décision est l'ensemble des nombres premiers. Étant donné un codage des instances par des mots, on peut voir un problème de décision comme un langage formel : l'ensemble des mots qui représentent les instances positives.Considérons à présent un algorithme qui résout le problème du voyageur de commerce. L'algorithme exécute des étapes élémentaires de calcul et le nombre d'étapes dépend de la taille de l'entrée. Pour le problème du voyageur de commerce, la taille de l'entrée est la quantité de mémoire nécessaire pour représenter les villes et les distances qui les séparent. Pour mesurer le temps d'exécution d'un algorithme, on définit la complexité en temps qui représente le nombre d'étapes qui sont nécessaires pour résoudre le problème pour une entrée de taille donnée.Bien sûr, il existe de nombreux algorithmes qui résolvent le même problème. La théorie de la complexité s'attache à connaître la difficulté (ou la complexité) intrinsèque d'un problème algorithmique, c'est-à-dire celle de l'algorithme le plus efficace pour ce problème. On classifie les problèmes (et non pas les algorithmes) en termes de classes de complexité.Dans chaque catégorie de problèmes ci-dessus, on dit qu'un problème a une réponse algorithmique si sa réponse peut être fournie par un algorithme. Un problème de décision — donc un problème dont la réponse est soit « oui » soit « non » — est décidable si sa réponse peut être fournie par un algorithme. De la même façon, on dit qu'un problème fonctionnel est calculable si l'élément solution peut être calculé par un algorithme. Il existe aussi des classes de complexité pour les problèmes non-décidables, comme celles de la hiérarchie arithmétique.Pour les problèmes décidables, on cherche à évaluer les ressources – temps et espace mémoire – mobilisées pour obtenir algorithmiquement la réponse.La théorie de la complexité vise à savoir si la réponse à un problème peut être donnée très efficacement, efficacement ou au contraire être inatteignable en pratique, avec des niveaux intermédiaires de difficulté entre les deux extrêmes ; pour cela, elle se fonde sur une estimation — théorique — des temps de calcul et des besoins en mémoire informatique. Dans le but de mieux comprendre comment les problèmes se placent les uns par rapport aux autres, la théorie de la complexité établit des hiérarchies de difficulté entre les problèmes algorithmiques, dont les niveaux sont appelés des « classes de complexité ». Ces hiérarchies comportent des ramifications, suivant que l'on considère des calculs déterministes — l'état suivant du calcul est « déterminé » par l'état courant — ou non déterministes.L'analyse de la complexité est étroitement associée à un modèle de calcul. L'un des modèles de calcul les plus utilisés, car il permet de mesurer le temps de calcul et la mémoire utilisée, est celui de la machine de Turing proposé par Alan Turing en 1936. Un calcul est constitué d'étapes élémentaires. À chaque étape, la machine exécute une action élémentaire (changer d'état interne et déplacer la tête de lecture) en fonction de sa configuration courante (état interne et du symbole lu par la tête de lecture).D'autres modèles de calcul qui permettent d'étudier la complexité existent :machines de Turing non-déterministes (il peut y avoir plusieurs choix possibles d'actions à effectuer dans une configuration donnée) ;machines de Turing alternantes ;machines de Turing probabilistes ;la machine RAM (Random Access Machine) ;les circuits booléens et les programmes straight-line ;les fonctions récursives, dues à Kleene ;le lambda-calcul ;les automates cellulaires ;la logique linéaire ;les pebble games.Les ressources les plus classiques sont le temps et l'espace utilisés.Généralement, on mesure la quantité de ressources (temps, espace, etc.) requis en fonction de la taille de l'entrée (instance). La façon dont cette taille est mesurée joue un rôle crucial dans l'évaluation de la complexité de l'algorithme. Par exemple, pour le problème de tester si un nombre entier naturel est premier, une instance est un nombre entier naturel. La taille d'un nombre entier naturel se mesure généralement par le nombre de chiffres (par exemple le nombre de bits si le nombre est représenté en binaire). Ainsi, le nombre 1 024 peut être représenté avec seulement onze chiffres binaires et quatre chiffres décimaux et donc sa taille est 11 ou 4 ; la taille d'un entier p vaut alors O(log(p)). La théorie de l'information montre qu'on ne peut diminuer la taille davantage. On mesure la quantité de ressources en fonction de cette taille, qui sera notée n. L'évaluation des ressources requises permet de répartir les problèmes dans des classes de complexité.Pour les machines déterministes, on définit la classe TIME(t(n)) des problèmes qui peuvent être résolus en temps t(n), c'est-à-dire pour lesquels il existe au moins un algorithme sur une machine déterministe résolvant le problème en temps t(n). Le temps est le nombre de transitions sur machine de Turing ou le nombre d’opérations sur machine RAM. En fait, ce temps n'est pas une fonction précise, mais un ordre de grandeur. On parle aussi d'évaluation asymptotique. En particulier les constantes multiplicatives sont systématiquement ignorées grâce au théorème de speedup linéaire. Ainsi, pour un temps qui s'évalue par un polynôme, ce qui compte est le degré du polynôme. Si ce degré est 2, on dira que l'ordre de grandeur est en O(n²), que la complexité est quadratique et que le problème appartient à la classe TIME(n²).D'autre mesures existent :nombre de communications (voir la théorie de la complexité de la communication) ;nombre de portes dans un circuit booléen ;profondeur d'un circuit booléen ;nombre d'alternations (voir machines alternantes).Une classe de complexité regroupe les problèmes de même complexité, souvent à une réduction polynomiale près. Les classes usuelles sont définies en utilisant les machines de Turing comme modèles de calcul et les ressources sont le temps et l'espace. Le tableau suivant donne quelques exemples de classes de complexité :On a P ⊆ NP car tout algorithme déterministe est un cas particulier d'algorithme non déterministe. En revanche, la réciproque : NP ⊆ P, qui est la véritable difficulté de l'égalité P = NP, est un problème ouvert fondamental de l'informatique théorique. Il a été posé en 1970 indépendamment par Stephen Cook et Leonid Levin ; il fait partie des listes, établies en 2000, des problèmes du prix du millénaire et des problèmes de Smale.La plupart des spécialistes conjecturent que les problèmes NP-complets ne sont pas résolubles en temps polynomial (donc, que P ≠ NP). Cela ne signifie pas pour autant que toute tentative de résoudre un problème NP-complet est vaine (voir la section « Résolution » de l'article sur la NP-complétude). Il existe de nombreuses approches (qui se sont finalement révélées irrémédiablement erronées) attaquant le problème P ≠ NP ; le spécialiste de la théorie de la complexité Gerhard Woeginger maintient une liste de ces erreurs.La revendication de Vinay Deolalikar (6 août 2010), travaillant aux HP Labs (en), d'une démonstration de P ≠ NP, a été la première à faire l'objet d'une attention relativement importante de nombreux mathématiciens et informaticiens de renom, que ce soit sous la forme d'échanges dans des blogs,,, de journaux en ligne ou sous la forme plus construite d'un projet d'étude collaborative en ligne (du type projet Polymath, tel que promu par les médaillés Fields Terence Tao et Tim Gowers). Cette étude collaborative donne la liste des points où l'approche de Vinay Deolalikar achoppe actuellement.À l'instar du problème P = NP, on ne sait pas par exemple si :L = NL ?L = P ?NP = co-NP ?P = Pspace ?NP = Pspace ?Exptime = NExptime ?La complexité des opérations à réaliser a des conséquences sur leur déroulement concret, notamment la consommation d'énergie nécessaire à leur réalisation. Celle-ci peut varier considérablement suivant la performance des processus utilisés pour effectuer les calculs. En 1961, Rolf Landauer, de la société IBM, a proposé un modèle permettant d'estimer le coût en énergie minimal théorique en donnant une estimation du coût énergétique minimal de changement d'état d'un bit informatique. En date de 2013, cette relation entre complexité et énergie, appelée principe de Landauer, est confirmée par les études expérimentales.Complexité, article général sur la complexitéComplexité de KolmogorovComplexité descriptiveHiérarchie arithmétiqueHiérarchie de GrzegorczykExplosion combinatoireMachine de Turing non déterministeRéduction polynomialeComplexité des preuves, le domaine qui étudie la longueur des preuves des énoncés mathématiques selon les systèmes de preuvesSylvain Perifel, Complexité algorithmique, Ellipses, 2014, 432 p. (ISBN 9782729886929, lire en ligne)Olivier Carton, Langages formels, calculabilité et complexité, 2008 [détail de l’édition] (lire en ligne)(en) Sanjeev Arora et Boaz Barak, Computational Complexity : A Modern Approach, Cambridge University Press, 2009 (ISBN 0-521-42426-7)(en) Christos Papadimitriou, Computational Complexity, Addison-Wesley, 1993 (ISBN 978-0-201-53082-7)(en) Michael R. Garey et David S. Johnson, Computers and Intractability : A guide to the theory of NP-completeness, W.H. Freeman & Company, 1979  (ISBN 0-7167-1045-5)Richard Lassaigne et Michel de Rougemont, Logique et Complexité, Hermes, 1996  (ISBN 2-86601-496-0)Nicolas Hermann et Pierre Lescanne, Est-ce que P = NP ? Les Dossiers de La Recherche, 20:64–68, août-octobre 2005Ressource relative à la recherche : (en) Stanford Encyclopedia of Philosophy (en) Complexity Zoo consulté le 21 avril 2014(en) The Status of the P versus NP Problem : cet article, publié dans les Comm. of the ACM, a été cité par l'article New York Times ci-dessous.(en) « Prizes Aside, the P-NP Puzzler Has Consequences », The New York Times,  7 octobre 2009 Portail de l’informatique   Portail de l'informatique théorique   Portail des mathématiques"
informatique;"Le Langage de Modélisation Unifié, de l'anglais Unified Modeling Language (UML), est un langage de modélisation graphique à base de pictogrammes conçu comme une méthode normalisée de visualisation  dans les domaines du développement logiciel et en conception orientée objet.L'UML est une synthèse de langages de modélisation objet antérieurs : Booch, OMT, OOSE. Principalement issu des travaux de Grady Booch, James Rumbaugh et Ivar Jacobson, UML est à présent un standard adopté par l'Object Management Group (OMG). UML 1.0 a été normalisé en janvier 1997; UML 2.0 a été adopté par l'OMG en juillet 2005. La dernière version de la spécification validée par l'OMG est UML 2.5.1 (2017).UML est destiné à faciliter la conception des documents nécessaires au développement d'un logiciel orienté objet, comme standard de modélisation de l'architecture logicielle. Les différents éléments représentables sont :Activité d'un objet/logicielActeursProcessusSchéma de base de donnéesComposants logicielsRéutilisation de composants.Il est également possible de générer automatiquement tout ou partie du code, par exemple en langage Java, à partir des documents réalisés.UML est un langage de modélisation. La version actuelle, UML 2.5, propose 14 types de diagrammes dont sept structurels et sept comportementaux. À titre de comparaison, UML 1.3 comportait 25 types de diagrammes.UML n'étant pas une méthode, l'utilisation des diagrammes est laissée à l'appréciation de chacun. Le diagramme de classes est généralement considéré comme l'élément central d'UML. Des méthodes, telles que le processus unifié proposé par les créateurs originels de UML, utilisent plus systématiquement l'ensemble des diagrammes et axent l'analyse sur les cas d'utilisation (« use case ») pour développer par itérations successives un modèle d'analyse, un modèle de conception, et d'autres modèles. D'autres approches se contentent de modéliser seulement partiellement un système, par exemple certaines parties critiques qui sont difficiles à déduire du code.UML se décompose en plusieurs parties :Les vues : ce sont les observables du système. Elles décrivent le système d'un point de vue donné, qui peut être organisationnel, dynamique, temporel, architectural, géographique, logique, etc. En combinant toutes ces vues, il est possible de définir (ou retrouver) le système complet.Les diagrammes : ce sont des ensembles d'éléments graphiques. Ils décrivent le contenu des vues, qui sont des notions abstraites. Ils peuvent faire partie de plusieurs vues.Les modèles d'élément : ce sont les éléments graphiques des diagrammes.Une façon de mettre en œuvre UML est de considérer différentes vues qui peuvent se superposer pour collaborer à la définition du système :Vue des cas d'utilisation (use-case view) : c'est la description du modèle vu par les acteurs du système. Elle correspond aux besoins attendus par chaque acteur (c'est le quoi et le qui).Vue logique (logical view): c'est la définition du système vu de l'intérieur. Elle explique comment peuvent être satisfaits les besoins des acteurs (c'est le comment).Vue d'implémentation (implementation view) : cette vue définit les dépendances entre les modules.Vue des processus  (process view) : c'est la vue temporelle et technique, qui met en œuvre les notions de tâches concurrentes, stimuli, contrôle, synchronisation…Vue de déploiement (deployment view) : cette vue décrit la position géographique et l'architecture physique de chaque élément du système (c'est le où).Le pourquoi n'est pas défini dans UML.En UML 2.5, les diagrammes sont représentés sous deux types de vue : d'un point de vue statique ou structurelle du domaine avec les diagramme de structure (Structure Diagrams).D'un point de vue dynamique avec les diagrammes de comportement (Behavior Diagrams) et les diagrammes d’interactions (Interaction Diagrams).Les diagrammes sont dépendants hiérarchiquement et se complètent, de façon à permettre la modélisation d'un projet tout au long de son cycle de vie. Il en existe quatorze depuis UML 2.3. Diagrammes de structure ou diagrammes statiques Les diagrammes de structure (structure diagrams) ou diagrammes statiques (static diagrams) rassemblent :Diagramme de classes (class diagram) : représentation des classes intervenant dans le système.Diagramme d'objets (object diagram) : représentation des instances de classes (objets) utilisées dans le système.Diagramme de composants (component diagram) : représentation des composants du système d'un point de vue physique, tels qu'ils sont mis en œuvre (fichiers, bibliothèques, bases de données…)Diagramme de déploiement (deployment diagram) : représentation des éléments matériels (ordinateurs, périphériques, réseaux, systèmes de stockage…) et la manière dont les composants du système sont répartis sur ces éléments matériels et interagissent entre eux.Diagramme des paquets (package diagram) : représentation des dépendances entre les paquets (un paquet étant un conteneur logique permettant de regrouper et d'organiser les éléments dans le modèle UML), c'est-à-dire entre les ensembles de définitions.Diagramme de structure composite (composite structure diagram) : représentation sous forme de boîte blanche des relations entre composants d'une classe (depuis UML 2.x).Diagramme de profils (profile diagram) : spécialisation et personnalisation pour un domaine particulier d'un meta-modèle de référence d'UML (depuis UML 2.2). Diagrammes de comportement Les diagrammes de comportement (behavior diagrams) rassemblent :Diagramme des cas d'utilisation (use-case diagram) : représentation des possibilités d'interaction entre le système et les acteurs (intervenants extérieurs au système), c'est-à-dire de toutes les fonctionnalités que doit fournir le système.Diagramme états-transitions (state machine diagram) : représentation sous forme de machine à états finis du comportement du système ou de ses composants.Diagramme d'activité (activity diagram) : représentation sous forme de flux ou d'enchaînement d'activités du comportement du système ou de ses composants. Diagrammes d'interaction ou diagrammes dynamiques Les diagrammes d'interaction (interaction diagrams) ou diagrammes dynamiques (dynamic diagrams) rassemblent :Diagramme de séquence (sequence diagram) : représentation de façon séquentielle du déroulement des traitements et des interactions entre les éléments du système et/ou de ses acteurs.Diagramme de communication (communication diagram) : représentation de façon simplifiée d'un diagramme de séquence se concentrant sur les échanges de messages entre les objets (depuis UML 2.x).Diagramme global d'interaction (interaction overview diagram) : représentation des enchaînements possibles entre les scénarios préalablement identifiés sous forme de diagrammes de séquences (variante du diagramme d'activité) (depuis UML 2.x).Diagramme de temps (timing diagram) : représentation des variations d'une donnée au cours du temps (depuis UML 2.3).Un stéréotype est une marque de généralisation notée par des guillemets, montrant que l'objet est une variété d'un modèle.Un classeur est une annotation qui permet de regrouper des unités ayant le même comportement ou structure. Un classeur se représente par un rectangle conteneur, en traits pleins.Un paquet regroupe des diagrammes ou des unités.Chaque classe ou objet se définit précisément avec le signe « :: ». Ainsi l'identification d'une classe X en dehors de son paquet ou de son classeur sera définie par « Paquet A::Classeur B::Classe X ». Modèles d'éléments de type commun Symbolique des modèles d'éléments :                  Fourche (fork).             État initial (initial state).État final (final state).Interface (interface).O←--- sens du flux de l'interface.O)----- est un raccourci pour la superposition de ---→O et O←---. Modèles d'éléments de type relation Généralisation (generalisation).Association (association).    Réalisation.Utilisation. Autres modèles d'éléments Les stéréotypes peuvent dépendre du langage utilisé.Les archétypes.Les profils.UML n'est pas une norme en droit mais un simple standard « industriel » (ou norme de fait), parce que promu par l'OMG (novembre 1997) au même titre que CORBA et en raison de son succès. Depuis juillet 2005, la première version 2.x de UML est validée par l'OMG.Par ailleurs, depuis 2003, l'OMG a mis en place un programme de certification à la pratique et la connaissance d'UML OCUP qui recouvre trois niveaux de maîtrise.S'il existe de nombreux logiciels de modélisation UML, aucun ne respecte entièrement chacune des versions de UML, particulièrement UML 2, et beaucoup introduisent des notations non conformes. En revanche, de nombreux logiciels comportent des modules de génération de code, particulièrement à partir du diagramme de classes, qui est celui qui se prête le mieux à une telle automatisation.Grady Booch, James Rumbaugh, Ivar Jacobson, Le guide de l'utilisateur UML, 2000 (ISBN 2-212-09103-6)Laurent Audibert, UML 2, De l'apprentissage à la pratique (cours et exercices), Ellipses, 2009 (ISBN 978-2729852696)Franck Barbier, UML 2 et MDE, Ingénierie des modèles avec études de cas, 2009 (ISBN 978-2-10-049526-9)Craig Larman, UML 2 et les design patterns, Analyse et conception orientées objet et développement itératif (3e édition), Pearson Education, 2005  (ISBN 2-7440-7090-4)Martin Fowler et al., UML 2.0, Initiation aux aspects essentiels de la notation, 2004 (ISBN 2-7440-1713-2)Pascal Roques, UML 2, Modéliser une application Web, Eyrolles, 2007 (ISBN 2-212-12136-9)Pascal Roques, UML 2 par la pratique, Études de cas et exercices corrigés, Eyrolles, 2006 (ISBN 2-212-12014-1)Jim Conallen, Concevoir des applications web avec UML, Eyrolles, 2000, 288 p. (ISBN 978-2-212-09172-4)Unified ProcessIngénierie dirigée par les modèlesModel Driven ArchitectureATLAS Transformation LanguageObject Constraint LanguageTransformation de modèlesModeling and Analysis of Real Time and Embedded systems(en) UML.org(en) Dernière version de la spécification UML(en) OMG (Object Management Group)(en) Profil UML standardisé par l'ITU-T basé sur le Specification and Description Language Portail de l’informatique   Portail de la programmation informatique"
informatique;"Un informaticien ou une informaticienne est une personne qui exerce un métier dans l'étude, la conception, la production, la gestion ou la maintenance des systèmes de traitement de l'information.La définition générale désigne le technicien ou l'ingénieur spécialiste d'un système informatique. Comme pour le mathématicien, l'informaticien est plus strictement un scientifique diplômé d'un doctorat en informatique. Ses recherches se basent notamment sur la théorie de l'information.Le métier d'informaticien apparaît à la fin du XIXe siècle avec l’émergence de la mécanographie. Celle-ci consiste alors à traiter l'information à l'aide de systèmes électromécaniques. Les mécanographes sont ainsi employés à s'occuper de ces systèmes et ce n'est qu'au milieu du XXe siècle, avec l'arrivée du terme allemand « Informatik » créé par l'ingénieur Karl Steinbuch et repris en France par Philippe Dreyfus, que le terme d'informaticien voit le jour.Dans les années 1960 à 1980, on nomme ainsi informaticien toute personne exerçant un métier en rapport avec l'informatique. Le métier est alors peu valorisé, il est largement exercé par les femmes ingénieures. Depuis le milieu des années 1980 et l'arrivée de micro-ordinateurs dans les foyers, ce domaine est largement investi par les hommes, dans les pays occidentaux. Dans les écoles d'informatique, les femmes forment 15 % des effectifs en 2018. La variété et le peu de rapport des métiers du génie informatique fait tomber le terme informaticien en désuétude en France au profit de noms plus spécifiques : agent d'exploitation, administrateur système, responsable de sécurité, administrateur de sécurité, administrateur réseau, analyste, programmeur, architecte informatique, etc. Ces métiers peuvent concerner le domaine matériel et/ou le domaine logiciel. « Informaticien » est donc un terme générique désignant des métiers très éloignés les uns des autres et propre à une époque où on ne les distinguait pas toujours.Depuis le début du XXIe siècle, le terme est différencié de l’opérateur, qui se sert de l'informatique comme d'un outil destiné à son propre métier. La profession d’informaticien regroupe ainsi tous les corps de métier qui visent à concevoir, à coordonner ou à mettre en œuvre le développement ou le déploiement d'une solution informatisée qui est mise à la disposition des opérateurs appelés alors utilisateurs.Le travail d'un informaticien logiciel (programmeur) est d'utiliser ses connaissances en langages informatiques (Assembleur, Fortran, Pascal, Cobol, RPG, Python, C, C++, Java, C#, Visual Basic, Visual Basic for Applications, HTML, MAD...) afin de concevoir et de superviser le développement d'applications informatiques ou de logiciels.Ce travail se traduit concrètement par différentes activités, souvent liées à l'âge et à l'expérience de l'informaticien logiciel :le débutant, sorti de l'école ou de la faculté, fera généralement du développement ; il code les tâches décrites d'un programme selon les spécifications qui lui sont fournies. Il effectue aussi de la maintenance sur des programmes existants, ainsi que des évolutions (ajouts de fonctionnalités aux programmes, etc.) ;après quelques années, l'informaticien logiciel dispose d'une meilleure connaissance technique et métier. En commençant à encadrer de plus jeunes développeurs, l'une des évolutions possibles est de devenir chef de projet. La connaissance grandissante des techniques et du métier permet à l'informaticien logiciel de conseiller les utilisateurs ou clients afin de les aider à cerner leurs besoins, évoquer des fonctionnalités oubliées, etc. Il propose également des solutions techniques sur lesquelles il tranche avec le client : interface locale sur chacun des postes, interface web partagée, base de données… Il peut ensuite devenir architecte du système d'information ou responsable du système d'information, à la tête de plusieurs projets d'envergure, comme un progiciel de gestion intégré.Formations les plus courantes : école d'ingénieurs en informatique ;diplôme universitaire en informatique ;diplôme universitaire en génie logiciel ;brevet de technicien supérieur en informatique.L'évolution du métier d'informaticien logiciel est soumise à certaines turbulences depuis la fin du XXe siècle. Les fonctions d'un programmeur débutant ne comportent pas toujours de responsabilités de conception et impliquent rarement un rôle de direction de projet. Mais en même temps, les règles d'avancement, les grilles de rémunération en vigueur et les critères de reconnaissance sociale limitent considérablement les possibilités de carrière pour les programmeurs, provoquant un phénomène de « fuite des cerveaux » vers d'autres métiers. Les programmeurs de haut niveau sont donc extrêmement rares; on les trouve principalement dans les cellules de recherche et développement des constructeurs informatiques et éditeurs de logiciels (certains ont d'ailleurs une notoriété internationale). Il est à noter aussi que, dans les autres entreprises, la dévalorisation de la fonction se manifeste notamment par la quasi-disparition du mot programmeur dans les intitulés de fonction des informaticiens qui exercent ce métier (nommés de préférence ingénieurs d'étude, ingénieur de développement, etc.).Les métiers de l'informatique regroupent : Métiers techniques (informaticien) les administrateurs de bases de données ou DBA : chargés du bon fonctionnement d'une base de données et/ou d'un système de gestion de base de données  ;les administrateurs réseau : chargés de gérer les comptes et les machines d'un réseau ;les administrateurs système : chargés de la maintenance applicative des serveurs ;les analystes-programmeurs : chargés de spécifier techniquement les concepts définis par le Concepteur (ou analyste) en composants informatiques ;les architectes de systèmes d’information : chargés de définir la cartographie de systèmes informatiques (logicielle et matérielle) ;les chercheurs : chargés de formaliser les problèmes à résoudre, de développer des algorithmes permettant de les résoudre, de définir de nouvelles structures de données, de nouveaux concepts, de nouveaux langages de programmation ou de nouveaux systèmes informatiques  ;les développeurs : chargés de la programmation au sein du projet ;les pen-testeurs : chargés, avec l'accord de leurs propriétaires, d'attaquer les systèmes informatiques en vue d'évaluer l'efficacité des solutions de sécurité mises en place et d'en proposer des améliorations (voir test d'intrusion) ;les techniciens de maintenance : chargés de l'assistance technique, de la disponibilité des postes de travail, des sauvegardes de données, du déploiement des ordinateurs, etc. Ils doivent veiller au bon fonctionnement du parc informatique et faire de la maintenance ""préventive"" ;les techniciens en télécommunications ; Métiers non-techniques (employés du secteur informatique) les chefs de projets : chargés de la rédaction des cahiers de charges des applications manuelles ou innovantes et à élaborer les résultats informatiques exigés par les demandeurs. Une autre de leurs tâches consiste en la planification des projets, techniquement et en termes de ressources humaines  ;les concepteurs (ou analystes) : chargés d'identifier les besoins des utilisateurs et de les spécifier. Leur rôle consiste en particulier à expliquer les concepts à des experts non informaticiens ;les consultants : chargés par essence d'analyser un environnement, un besoin ou un problème informatiques sur les plans fonctionnel et technique et de proposer un ou plusieurs scénarios d'évolution ou de résolution adéquats. Leur champ d'activité est, en pratique, beaucoup plus variable et vaste : il s'étend du conseil à la gestion de projet, en passant par l'action commerciale ;les directeurs des systèmes d'information chargés d'encadrer l'ensemble de l'activité informatique d'une structure ;les ergonomes informatiques : chargés notamment d'améliorer la convivialité et l'efficacité des interfaces homme-machine ;les rédacteurs techniques : chargés de produire la documentation destinée aux métiers ;les responsables de la sécurité des systèmes d'information : chargés de la sécurité des systèmes d'information ;les techniciens helpdesk : chargés d'assister les usagers par téléphone ;les testeurs ou qualifieurs : chargés de tester le logiciel ou les chaines de programmes produit par les programmeurs, par exemple durant la période de VABF ;les urbanistes : chargés de redéfinir les projets sur le plan fonctionnel ;les Community manager ou les Gestionnaires de communautés, qui animent et fédèrent des communautés sur Internet pour le compte d'une société, d'une marque, d’une célébrité ou d’une institution.les webmasters : chargés de la ligne éditoriale et du contenu des sites Internet ; De nombreux métiers sont apparentés à l'informatique. Parmi ceux-ci, certains peuvent néanmoins être exercés par des autodidactes en informatique, ou par des personnes ne disposant pas spécialement de notions techniques :vendeur de produits informatiques : chargé de conseiller les acheteurs sur les produits informatiques (exemple : vendeur en micro-informatique) ;téléassistant : préposé à l'assistance aux utilisateurs ;commercial ;webmestre gérant le contenu : chargés du suivi éditorial d'un site web, sans responsabilité de maintenance technique ;Rédacteur de documentation : chargé de produire les documents écrits destinés aux utilisateurs ;infographiste : concepteur des outils d'infographie, puis graphiste utilisant l'ordinateur comme outil de création graphique chargés de produire :des icônes, dessins ou diagrammes dans le respect de la charte graphique d'un logiciel,un travail graphique ou artistique qui n'est pas propre au domaine informatique (exemples : mise en couleur, dessin assisté par ordinateur…) ;ingénieur du son : souvent appelé à utiliser l'informatique, en particulier dans les studios d'enregistrements pour l'enregistrement sonore, l'édition, le mixage audio et le mastering.Filières courtes : France : BTS ou BUT (anciennement DUT), licence. Belgique : bachelier en informatique et systèmes, bachelier en informatique de gestion. Suisse : école technique (équivalent au BTS français), Certificat fédéral de capacité d'informaticien, puis brevet fédéral et/ou diplôme fédéral. Québec : diplôme d'études collégiales (DÉC) en techniques de l'informatique (équivalent au BTS français).Filières longues : Tous pays : doctorat. France : master en informatique ou école d'ingénieurs. Belgique : Master en sciences informatiques, master en sciences de l'ingénieur industriel en informatique, ingénieur civil en informatique ou ingénieur civil en informatique et gestion. Suisse : Master en sciences informatiques, EPF (master), HES (master). Québec : baccalauréat en informatique (3 ans d'études), baccalauréat en génie informatique (4 ans d'études universitaires).Entreprises de formation privées, organismes de formation d'état.Isidore de Séville, saint patron des informaticiens.HackerListe d'informaticiens et précurseurs de l'informatiqueSSII - Société de services en ingénierie informatique.MUNCI - Association professionnelle en France fédérant les membres des professions des technologies de l'information.Humour :Dilbert - BD : L'informaticien dans l'entreprise.Commitstrip - BD : Le blog qui raconte la vie des codeurs.The IT Crowd - Série télévisée britannique, suivant la vie d'un service informatique. Portail de l’informatique   Portail du travail et des métiers"
économie;"Un agent économique est, en économie, une personne physique ou morale prenant des décisions qui participent à l'activité économique. Il est l'actant économique principal des modèles économiques. Le périmètre pertinent de définition de l'agent économique dépend des conceptions de l'économie : les courants de pensée économiques les définissent de manière différentes, ainsi que la comptabilité nationale. La question de la définition de l'agent économique est au centre des controverses économiques du XVIIIe siècle. Au sein de l'école physiocrate, François Quesnay crée un système de pensée où il définit l'économie nationale comme peuplée de trois agents économiques : les fermiers, les propriétaires fonciers et les artisans. Il qualifie cette dernière de « classe stérile ». Cette conception est plus tard critiquée par Adam Smith, qui considère les travailleurs et les commerçants comme les agents économiques majeurs.Le marxisme se fonde lui aussi à partir d'une remise en question de la définition des agents économiques. Karl Marx propose une analyse de l'économie avec une bipartition sociale, entre les capitalistes d'un côté, et les travailleurs de l'autre, représentatifs de la bourgeoisie et du prolétariat. Néanmoins, au sein de la classe des capitalistes, Marx distingue deux catégories d'agents économiques : ceux qui produisent des biens de production et ceux qui produisent des biens de consommation. Pour l'école du circuit, les agents économiques doivent être définis de manière proche de la définition de la comptabilité nationale, c'est-à-dire en étant regroupés en pôles fonctionnels. Cette école considère ainsi que les grands agents économiques sont les institutions financières (et notamment les banques), les entreprises, les ménages et les administrations publiques. Chez les keynésiens, chaque agent économique est classé par référence à sa fonction principale dans l'économie. Un entrepreneur appartement à l'agent "" entreprises "" en raison de sa fonction de base bien qu'il peut effectuer des opérations de consommation courante, à titre secondaire, qui concerne l'agent "" ménages "". Une banque effectue, à titre principal des opérations financières est classée donc dans l'agent "" banques "" bien qu'elle peut effectuer des opérations, à titre secondaire, relevant des agents "" ménages "" ou "" entreprises "". On peut faire un raisonnement analogue pour démontrer l'appartenance des individus à l'agent "" ménages "".La macroéconomie considère que l'agent économique pertinent peut être, ou bien un agent représentatif (le ménage moyen ou médian), ou bien une agrégation d'agents économiques, ou bien, dans le cadre d'une étude sectorielle, un groupe homogène. Dans tous les cas, les agents économiques sont agents car ils agissent dans le cadre d'échanges économiques avec d'autres agents,.L'objectif de l'étude menée par l'économiste oriente son choix dans la définition des agents économiques qu'il souhaite étudier.La microéconomie s'intéresse aux décisions prises par les agents économiques. Chaque agent possède des caractéristiques particulières qui permettent aux économistes de prévoir ses décisions. Plutôt qu'être un simple représentant de sa classe ou de son groupe d'appartenance, l'agent microéconomique arbitre entre les choix possibles, pour maximiser son utilité. L'hypothèse qui sous-tend cette conception est celle de la rationalité des agents, qui sont censés effectuer des choix optimaux en s'appuyant sur un calcul coût-avantage.Les néoclassiques considèrent qu'il faut s'intéresser à deux types d'agents économiques : le consommateur et le producteur. Le consommateur offre son travail en échange d'un salaire, qu'il va consommer sur le marché des biens et services. Le producteur achète la force de travail et les capitaux nécessaire à la production, et l'écoule ensuite sur le marché des biens et services. Comme le note la Direction générale du Trésor dans une note longue de 2021, « les modèles traitent les ménages et les entreprises de manière quasi-symétrique malgré leur profonde différence de nature, en les rassemblant dans le concept d'agents économiques ».Les organes de comptabilité nationale regroupent les agents économiques selon leurs fonctions, à l'instar de l'école du circuit. Les catégories les plus simples sont les ménages, les entreprises et le gouvernement. La fonction principale des entreprises et du gouvernement est de produire des biens et services, celle des ménages est de consommer.En France, l'INSEE catégorise les agents économiques en six catégories, aussi appelées unités institutionnelles. Chaque catégorie inclut des sous-divisions.Les modèles économiques théoriques sont le plus souvent basés sur des hypothèses comportementales homogènes, afin de décrire les ensembles économiques de la manière la plus simple. Ainsi les ménages consomment, les entreprises produisent.La théorie économique gagnant en complexité, les économistes affinent leur segmentation des agents économiques pour les différencier selon certains critères comme le revenu, le patrimoine ou l'âge.Agent (fonction publique)Unité institutionnelleÉconomie des institutions[PDF] http://www.newschooljournal.com/files/NSER01/82-94.pdf Duncan Foley, The strange history of the economic agent, 2002Les agents économiques et leurs opérations Portail de l’économie"
économie;"L'allocation des ressources est un concept économique qui concerne l'utilisation des ressources rares et notamment les facteurs de production (travail, capital, matières premières) pour satisfaire à court et long terme les besoins de consommation de la population. Cette allocation sert également à financer des services non marchands comme la justice, la police, certaines infrastructures communes (voirie…) indispensables au fonctionnement de la société.Dans le domaine de l'écologie et de l'évolution, l'allocation des ressources (correspondant à l'allocation de nutriments en nutrition microbienne, fongique, végétale et animale) destinées au développement (caractérisé par le taux de croissance, les patrons d'allocation de la biomasse produite aux différentes parties de l'organisme, la durée de développement et survie…), à la maintenance des fonctions somatiques (nutrition, respiration, régulation, défense, relation…) et reproductives (allocation des ressources à la reproduction (en) caractérisée par la fécondation et la fertilité), reflète l'existence de compromis évolutifs entre différents traits biologiques.L'économie elle-même, en tant que domaine du savoir, a pour rôle d'étudier la façon dont sont allouées (et créées) les ressources rares. Cette allocation demande, dès que l'activité économique atteint une certaine taille et complexité, de définir un mode d'arbitrage autre que la guerre ou la rapine, et donc des institutions sociales adaptées.Cet arbitrage se fait de façon plus ou moins libre Par le biais des prix de marché, le fonctionnement de ces marchés étant eux-mêmes formalisés par des règles de droit.Ou par les administrations d'État, à l'aide de règles ou de lois.Dans un sens plus étroit, l'allocation des ressources peut concerner L'arbitrage entre les divers facteurs de production,Voire les choix et dosages à faire à l'intérieur d'un type de facteur (par exemple allocation des capitaux entre divers investissements : voir ci-dessous ""l'allocation d'actifs"")L'allocation des ressources économiques et financières se double de l'allocation des risques propres à ces domainesDans le cadre de la transition écologique, l'économie circulaire promeut - dans toute la mesure des possibilités techniques et financières disponibles - le recyclage des ressources et en particulier des ressources rares, qui devraient en outre être économisées ou remplacées par des alternatives quand cela est possible ; ce qui pourrait être grandement facilité par une généralisation du principe des écotaxe et de l'écoconceptionL'allocation d'actifs est une technique de gestion de patrimoine ou de portefeuille consistant à revoir périodiquement la composition du portefeuille par ""classes d'actifs"" (autrement dit par types de placement : obligations, actions, produits dérivés…)CorruptionRichesseEvolution de l'allocation à la reproductionOptimisation (mathématiques) Portail de l’économie   Portail de l’écologie   Portail origine et évolution du vivant"
économie;Un bien de consommation est un produit fabriqué destiné au consommateur final. En économie, on le distingue d’un bien de production. Un service ne peut pas être considéré comme un bien.  Biens et services qui se consomment en une seule fois (pain, électricité du logement…)Biens semi-durables : ils durent quelque temps mais s'usent assez facilementBiens durables au plein sens du terme que l'on peut utiliser durant de nombreuses années (réfrigérateur, automobile…)Équipements (machines, moyens de transport…)Produits semi-finisMatières premièresÉnergie (électricité, pétrole…)Services rendus par une entreprise à une autre entrepriseBiens et services marchandsBien intermédiaireConsommation« Industrie des biens de consommation / Biens de consommation », sur le site de l'Insee Portail de l’économie
économie;"Les biens et services marchands sont tous les « produits », l'ensemble des biens matériels et immatériels, appelés « produits » en comptabilité nationale, qui sont destinés à être commercialisés sur un marché.Les biens marchands sont des produits matériels pouvant être vendus et achetés.Les services marchands sont des services payants par ex : coiffeur pour particuliers … services rendus par des entreprises contre le versement d'une rémunération monétaire qui leur permet de dégager un profit. On les oppose aux services non marchands qui sont rendus par les administrations publiques et les associations (au sens large d'institutions sans but lucratif au service des ménages, ou ISBLSM) : souvent payants, ces services ne sont pas considérés comme marchands car ils ne sont pas à l'origine d'un profit de la part de l'unité productive.Un bien matériel est tangible lorsqu'on peut le toucher, le voir.Un service est une ""aide"" donnée à une autre personne en échange de quelque chose. Un bien matériel est un objet que l'on peut acheter. Service exemple : le coiffeur (quelqu'un qui vous rend service : il vous coupe les cheveux (un coiffeur).Les services ne sont pas stockables et sont immatériels, contrairement aux biens matériels. On dit donc que les services sont intangibles.Cependant, fonder la distinction entre bien et service sur la matérialité du bien et l'immatérialité du service peut prêter à confusion, en effet pour fournir un service le producteur a recours souvent à des biens (consommations intermédiaires nécessaires à la production du service). Ainsi, qui irait acheter un service de restauration si on lui servait une assiette vide? Pour éviter toute confusion il convient donc de s'en référer à un critère de distinction plus sûr, les services ne sont pas stockables et donc leur effet sur le consommateur ne peuvent être différés dans le temps. Il ne peut ainsi pas y avoir de décalage dans le temps entre production et consommation. La réparation du plombier et les effets sur les canalisations du client sont synchronisés. Le cours du prof (en tant que service) nécessite la présence de l'élève (l'enregistrement ou la copie d'un cours imprimé crée donc un bien). De même, au restaurant, la fourniture du repas nécessite la présence du gourmand. Au contraire, un sandwich à emporter est un bien et non un service puisqu'un décalage temporel entre production du bien et consommation de celui-ci est possible.Autres exemples : l’hôpital (soins médicaux, opérations…), service de transport… Les trois principaux biens Les biens de consommation finale (répondent aux besoins des particuliers)Les biens intermédiaires (matière première transformée par l'entreprise industrielle)Les biens d'équipement ou de production (biens nécessaires à la production : énergie…) Les trois principaux services Les services aux particuliers (restaurants, administration, coiffeur, école, hôtels, musées, expositions…)La promotion commerciale / La gestion immobilière (agences…)Les services aux entreprises (recherche et développement…)Un service marchand est généralement produit par les entreprises, il est payant. Le producteur bénéficie d'une richesse.Par exemple : un concert, un opéra, un voyage en train…Un service non marchand est gratuit ou quasi gratuit, il est généralement produit par les administrations publiques et les associations : leur objectif est non lucratif, c'est-à-dire que leur objectif principal est de venir en aide aux citoyens et non réaliser des profits. Elles peuvent toutefois produire des biens marchands (T-shirt avec leur logo, badges…). La richesse supplémentaire sera utilisée pour améliorer cette association.Les services gratuits n'ont pas de prix de vente, alors que les services quasi-gratuits ont un prix non significatif, c'est-à-dire nettement inférieur au coût de la production.Exemples de services gratuits : commissariat, l'école publique…Exemples de services quasi-gratuits : l'école privée, l'inscription dans une bibliothèque publique, les transports publics…Le marché des biens et services est le marché sur lequel s'échangent les biens et services marchands, c’est-à-dire qu'il représente l'ensemble de l'offre (quantité des biens et services proposés, théoriquement le prix total de ce qui est à vendre) et de la demande (quantité réclamée par des acheteurs potentiels) de biens et services à un moment donné. Ce marché se subdivise bien sûr en marchés de différents types de produits (comme le marché de l'acier, ou le marché de l'automobile d'occasion).Bien de consommationComptabilité nationaleSecteur marchandServices non marchandsBien immatérielDéfinition de l'InseeProduction des services marchands Portail de l’économie"
économie;"Pour un agent économique (à savoir un individu, un ménage, une association, une entreprise, un État...) ou une entité (à savoir un équipement, un service, un établissement, un projet, une mission, une fonction...) le budget est un document récapitulatif des recettes et des dépenses prévisionnelles déterminées et chiffrées pour un exercice comptable à venir (généralement l'année).La démarche budgétaire peut concerner le domaine de la gestion privée ou publique. Le présent article ne traite que des concepts et des pratiques ayant trait à la gestion privée (voir par ailleurs les articles détaillés : le Budget de l'État ou le Budget de l'État français ou le Budget de l'Union européenne).Le mot provient du gaulois « bouge » puis de l'ancien français « bougette », qui désignait une petite bourse accrochée à la ceinture de l'habit d'une personne, contenant de la menue monnaie lui permettant de faire face aux dépenses prévisibles de la journée. Le mot est exporté au Moyen-Âge en Angleterre, où il désigne ensuite le sac dans lequel le chancelier de l'Échiquier présente au Parlement les comptes et autres pièces justificatives, et il prend progressivement le sens actuel de « budget ». Le mot avait disparu de la langue française depuis plusieurs siècles quand il y revient au XVIIIe siècle par l'intermédiaire de l'anglais, avec son nouveau sens.Le budget n'est pas un document normalisé et ne fait pas partie des documents prévus par la méthodologie de la comptabilité.Ce qui n'empêche pas l'outil « budget » d'être largement utilisé par les gestionnaires : la méthode et la procédure budgétaire se révèlent en effet très utiles dans le cadre d'une entreprise ou d'une organisation pour servir de point d'appui aux tâches de prévision, de pilotage ou de contrôle.Le budget d'exploitation traite les recettes-dépenses qui concernent l'exploitation.Le budget d'investissement traite les recettes-dépenses qui concernent l'investissement.Un budget peut être :indicatif (on fait une estimation, mais la réalisation pourra se révéler différente),impératif (les dépenses doivent être inférieures ou égales aux dépenses prévues, les recettes doivent être supérieures ou égales aux prévisions).Un budget comporte des données chiffrées qui peuvent être exprimées de façon plus ou moins poussée :Chiffrage exprimé uniquement en volume ou uniquement en valeur monétaire ;Double chiffrage comprenant à la fois un volet exprimé en volume, doublé d'un volet exprimé en valeur monétaire.La notion de budget est normalement inséparable d'une période de temps bien définie : soit généralement un exercice fiscal de douze mois (budget dit annuel).De manière à assurer une vision opérationnelle plus concrète pour les opérateurs impliqués dans la mise en œuvre, le budget est souvent assorti d'un calendrier de réalisation (parfois appelé « tableau de marche » ou « tableau de progress control »). Le budget est alors dit « phasé » : en fonction du degré de finesse visé, le chiffrage des recettes-dépenses est ventilé et détaillé par phase de réalisation (période mensuelle, trimestrielle ou semestrielle).Préparer un budget, c'est décaler le travail de la machine vers l'homme comme le préconisait l'économiste Zraffa, il faut constater, planifier et élaborer.Au moment de son établissement, un budget n'a de valeur que si les prévisions affichées sont conformes à la réalité qu'elles sont censées décrire. Les principes comptables généraux doivent être respectés :Principe d'unicitéRien ne doit être oublié : le budget doit récapituler à lui seul l'ensemble des dépenses et recettes concernées. Rien ne doit être ajouté : le budget n'a pas à supporter des dépenses qui ne concernent pas son objet ou se voir crédité de recettes auxquelles il ne peut prétendre.Principes de sincérité et d'intégritéLes évaluations données sont censées être les plus réalistes possible : aucune dépense ne doit être minorée, aucun revenu ne doit être majoré…Principe de prudenceL'erreur, l'incertitude, l'irréalisme et la force majeure font peser un risque sur tout exercice de prévision. Le chiffrage tant des dépenses que des recettes doit les prendre en compte de manière raisonnable.En début d'exercice sont fixées les Estimations Originales (EO). Celles-ci peuvent être établies de manière concertée ou non avec les services chargés de l'exécution. En cas de procédure concertée, la première version des EO est éditée par la direction en mai-juin précédant l'exercice annuel suivant. Les services disposent donc d'une période de deux mois (juillet-août) pour établir leurs propres prévisions dans le cadre général fourni par les EO. Toutes les prévisions des services remontent et sont consolidées en septembre par la direction. La consolidation étant rarement parfaitement en ligne avec les intentions de la direction, une procédure de négociation s'établit en octobre entre la direction et les services concernés. La consolidation définitive pouvant intervenir courant octobre, sinon début novembre.Les estimations originales (EO), une fois validées comme cadre de référence s'imposant à tous les membres de l'organisation, doivent être diffusées en temps utile et expliquées à tous les exécutants concernés et ce, suffisamment avant le démarrage de l'exercice concerné. Ceci implique que cette diffusion intervienne fin novembre - début décembre, de manière à être répercutée sur l'ensemble des parties prenantes présentes à tous les niveaux et dans toutes les unités de l'organisation.C'est l'objet du reporting (en particulier dans les grands groupes où sont mis en œuvre de nombreux budgets). Ceci afin d'avoir en temps presque réel une vision exacte de la réalisation du budget. Tout reporting suppose un suivi chronologique (généralement trimestriel) assez étoffé. Dans la pratique la plus répandue, les estimations originales sont mises à jour de manière « roulante » au terme de chaque trimestre. PERPremière Estimation Révisée, éditée à la fin du premier trimestre d'exécution. Les données réelles du premier trimestre -désormais connues remplacent les données prévisionnelles correspondantes : données TRIM 1 réel + données TRIM 2, 3 et 4 prévisionnel.SERSeconde Estimation Révisée, éditée à la fin du second trimestre d'exécution : données TRIM 1 et 2 réels + données TRIM 3 et 4 prévisionnels TERTroisième Estimation Révisée, éditée à la fin du troisième trimestre d'exécution : données TRIM 1,2 et 3 réels + données TRIM 4 prévisionnelet ainsi de suite, jusqu'à QER, la Quatrième Estimation Révisée, éditée en fin de période budgétaire, ne comprend que des données réelles de l'exercice.L'intérêt de cette actualisation progressive est de pouvoir comparer en base annuelle (sur 12 mois) les estimations initiales avec la part de réalisé tel que connu au fur et à mesure de l'avancement de l'exercice.L'analyse des écarts (écart-volumes ou écarts-prix) fournit des indications précieuses sur l'existence de décalages plus ou moins importants entre le prévu et le réalisé. Ces écarts sont mesurés et localisés. Le pilote du budget est en mesure de décider selon l'ampleur des éléments constatés et la meilleure prévision qu'il peut faire pour la suite des événements :soit des ajustements marginaux avec rappel à l'ordre des exécutants ;soit des corrections plus significatives lorsque les écarts constatés font apparaître que la réalisation d'un objectif est devenu hors de portée. Le pilote peut choisir de réviser ses prévisions. On parle alors d'un budget révisé. Il convient cependant de manier ces révisions avec précaution car un usage trop fréquent des révisions discrédite la notion de Budget en tant qu'indicateur d'objectifs anticipés et partagés.Dans la période suivant immédiatement la clôture d'un exercice budgétaire, un bilan-évaluation complet doit être tiré pour dégager et capitaliser toutes les leçons utiles pour les exercices ultérieurs.Certaines organisations — à des fins de « motivation » — pratiquent un double budget. Le premier (réservé à la direction) est le « vrai » budget. Le second (plus largement diffusé auprès des collaborateurs, notamment commerciaux) comporte des objectifs plus ambitieux que ceux contenus dans la version direction. Un tel usage de la démarche budgétaire relève d'une démarche manipulatoire et peut se révéler à terme contre-productif : les objectifs irréalistes finissent par démobiliser.Le suivi de budget implique une logistique particulière : la « comptabilité budgétaire » afin de s'assurer que les recettes et/ou dépenses prévues s'effectuent :dans les volumes et valorisations prévues ;selon le calendrier prévu (dans la mesure où le budget a été « phasé »).Ce suivi comptable ouvre la possibilité du « contrôle budgétaire » via l'analyse des écarts :Évaluation des « écarts-volume » ou des « écarts-prix » éventuellement constatés entre prévisions et réalisations ;Opportunité d'effectuer en temps utile des mesures correctrices.Les services budgétaires ont une importance très variable selon les organisations: Bien utilisé, le contrôle budgétaire fournit une aide très appréciable pour le pilotage de toute gestion. C'est la porte d'entrée au domaine plus large du contrôle de gestion particulièrement nécessaire dans les organisations importantes ou complexes.Le contrôle budgétaire peut perdre de son impact lorsqu'il est aux mains d'un scribe docile compilant des chiffres inutilisés ou inutilisables, d'une Cassandre pointant uniquement les risques et dérapages, ou d'un censeur exerçant un droit de veto tatillon ou systématiquement méfiant envers la moindre dépense.BBZ, Budget Base ZéroLes démarches de rationalisation budgétaire pratiquées dans la gestion privée sont à rapprocher de celles de la gestion publique. Des emprunts méthodologiques sont constatés. Voir en particulier :RCB, Rationalisation des choix budgétaires (en anglais, PPBS Planning and Programming Budget Setting)RGPP, Révision générale des politiques publiquesComme le disait Henri Fayol, gérer c’est administrer qui peut signifier prévoir et « prévoir, c’est déjà supputer l’avenir et le préparer : prévoir c’est agir ». Le budget est un outil essentiel de gestion prévisionnelle qui permet de repérer à l’avance les difficultés de choisir les programmes d’activité à partir de l’analyse de l’environnement pour assurer à l’entreprise la rentabilité souhaitée.Le système budgétaire est fondé sur l’idée d’une mise à la disposition des opérationnels d’un certain nombre d’outils utiles au processus de management. Il s’agit :de la définition d’objectifs clairs, précis et réalistes ;de la préparation de plans d’actions économes ;d'un suivi rapide centré sur l’objectif grâce à la communication régulière des résultats à la suite d'un rapprochement entre ce qui fait et ce qui devrait être fait.de moyens ;de la possibilité de contrôle a priori.Le budget est un outil qui permet à l’entreprise d’atteindre ses objectifs notamment en facilitant le couplage de celle-ci avec l’environnement et l’intégration des différentes fonctions.Mais pour que le budget soit efficace, il faut que les préalables ci-après soient remplis :une structure organisation adaptée ou procéder à la structuration de l’organisation ou découpage en centres de responsabilité ;la formulation d’une stratégie ;la prévision des objectifs ;le système de suivi des réalisations au jour le jour ;une possibilité de contrôle a posteriori.C’est ce dernier aspect qui fait du budget un outil d’évaluation. La constatation des écarts constituent ainsi l’évaluation qui est faite action par action. Alors que la nomenclature budgétaire est tout autre.Ressource relative à la santé : (en) Medical Subject Headings  Portail du management   Portail de la finance   Portail de l’économie"
économie;""
économie;"La comptabilité nationale est une représentation schématique et quantifiée de l'activité économique d'un pays. Elle consiste en une mesure des flux monétaires représentatifs de l'économie d'un pays pendant une période donnée, en principe une année, et les regroupe dans des totaux nommés agrégats, dans un but analytique direct. La comptabilité nationale prend en compte de nombreux indicateurs macroéconomiques, dont le plus important est le PIB (produit intérieur brut), qui correspond à la somme des valeurs ajoutées — auxquels il faut ajouter les impôts nets des subventions sur les produits — des biens et services produits dans un pays donné au cours d'une année. La comptabilité nationale prend en compte de nombreuses informations, contenues dans les documents comptables des entreprises d'une part, mais aussi dans les rapports des institutions administratives. La comptabilité nationale classe ainsi les différents agents économiques en catégories, les secteurs institutionnels, afin de recenser au mieux les différentes informations relatives à l'économie.Les premiers systèmes de comptabilité nationale datent de la Seconde Guerre mondiale, tout d'abord avec l'économiste britannique Keynes qui développe dès 1941 des instruments de mesure de l'économie, puis avec Jan Tinbergen et Wassily Leontief, considérés comme les véritables inventeurs de la comptabilité nationale. La comptabilité nationale s'est ensuite développée dans la plupart des pays développés. Ainsi, dans le cadre du système monétaire européen (SME), les systèmes de comptes nationaux ont été harmonisés autour de normes communes, et les États européens utilisent le même plan comptable : le SEC (système européen de comptabilité).La comptabilité nationale est née de la volonté des États d'intervenir dans une régulation conjoncturelle de l'économie. Selon un article du Figaro en 2009, « l'invention de la comptabilité nationale a été une réponse à la Grande Dépression des années 1930. On ne disposait à l'époque d'aucune statistique générale, en dehors des cours boursiers ou des données de production établies plus ou moins bien par les professions. Dès 1932, avant même l'élection de Roosevelt et le New Deal, le Congrès américain avait demandé à l'économiste Simon Kuznets (couronné par le Prix Nobel en 1971) d'estimer le recul de l'activité globale. Il s'est alors avéré qu'elle avait chuté de 40 % entre 1929 et 1932. »Le premier vrai système de comptabilité nationale fut créé par John Maynard Keynes (qui dirigeait alors la délégation britannique chargée de rédiger les accords de Bretton Woods) en 1941 à la suite de la demande du parlement de Grande-Bretagne. Les collaborateurs de Keynes élaborèrent une série de tableaux illustrant les ressources produites et leur utilisation sous forme de consommation, dépenses publiques, subventions et investissements. En outre, les travaux menés par l'américain Wassily Leontief (« Prix Nobel » d'économie en 1973) et le néerlandais Jan Tinbergen, « Prix Nobel » d'économie en 1969 ont permis de développer des analyses plus proches de celles que nous connaissons aujourd'hui.Les travaux de Richard Stone et de Simon Kuznets sont à l'origine de ce que l'on a baptisé un « modèle normalisé de la comptabilité nationale ».En ce qui concerne les tableaux de synthèse, en particulier le tableau entrées-sorties (TES) le précurseur fut l'économiste d'origine russe naturalisé américain Wassily Leontief.En France, François Quesnay, chef de file de l'école physiocratique, apparaît comme le premier à avoir élaboré un modèle dynamique, en 1758, pour représenter, à une échelle macroéconomique, la comptabilité nationale dans son ensemble. Au xixe siècle, plusieurs économistes ou hommes politiques s'efforcent de quantifier l'activité économique : Lesur dresse un bilan économique de la France en 1817 et y évalue la somme des revenus à cinq milliards ; en 1819, Jean-Antoine Chaptal estime la valeur de la production agricole et manufacturière en s’appuyant sur les données statistiques des préfectures et du cadastre. Des économistes comme François Perroux (également auteur de la théorie des « pôles de croissance ») ont les premiers établi des modèles modernes de comptabilité nationale sous le régime de Vichy et à la Libération. Selon une étude sur le sujet, « ces pionniers aux vues anticipatrices élaborent des outils statistiques et amorcent la réflexion sur la comptabilité nationale, à partir de la fin des années trente, puis pendant l'occupation. Ces économistes non traditionnels (Jean Fourastié) et ces statisticiens de l'Insee (André Vincent, Jacques Dumontier) se joignent ensuite à l'équipe de Jean Monnet à partir de 1945. »La comptabilité nationale a deux vocations principales : modéliser et étudier l'activité économique d'un pays donné pendant une durée précise d'une part, et prévoir l'évolution d'une conjoncture d'autre part. Elle peut ainsi être un outil de prévision pour aider un gouvernement à trouver des solutions ou à relancer la consommation par exemple. Les comptes nationaux sont publiés par trimestre ou par année.La comptabilité nationale est ex-post, elle s'effectue une fois l'année écoulée. Elle se mesure à prix constants, c'est-à-dire qu'elle ne tient pas compte de l'inflation.L'information la plus connue utilisée par la comptabilité nationale est le PIB (Produit intérieur brut). Le PIB est un indicateur macroéconomique nommé agrégat, c’est-à-dire une grandeur globale qui mesure l'activité économique. Il est possible de proposer trois approches du PIB, cependant, on le considère la plupart du temps comme la somme des valeurs ajoutées produites par l'ensemble des unités résidentes, c’est-à-dire les agents économiques effectivement présents sur le territoire pendant au moins 183 jours sur une année.Le PIB a ainsi une triple optique basée sur les grands principes de la comptabilité nationale :la production : PIB = somme des VAB + IP - SUBV. L'approche par la production, met ainsi en relation la somme des valeurs ajoutées brutes, l'impôt sur la production ainsi que les différentes subventions ;la formation de revenu : PIB = RS + EBE + RMB - SUBV + IP, avec RS la rémunération des salariés, EBE l'Excédent brut d'exploitation, RMB les revenus mixtes bruts, SUBV les subventions et IP les impôts sur la production (liés à la production et aux importations) ;la demande : PIB = CF + FBCF + (X-M), avec CF la consommation finale, FBCF la formation brute de capital fixe (l'investissement), X les exportations et M les importations.Le PIB (Produit intérieur brut) ne doit pas être confondu avec le PNB (produit national brut) qui est la somme des revenus primaires reçus effectivement par les agents économiques d'une même nationalité, qu'ils soient situés sur le territoire ou non. On a ainsi la relation PNB = PIB + revenus des facteurs en provenance de l'extérieur - revenus des facteurs versés à l'extérieur.Les différents agents économiques sont regroupés dans différentes branches baptisées unités institutionnelles. Elles constituent les unités de base de la comptabilité nationale.Une unité institutionnelle est un centre de décision autonome pouvant être une personne (ou plusieurs) physique, les économistes disent alors qu'il s'agit d'un ménage, ou une personne morale, c'est-à-dire une entreprise, une administration publique ou une association. Elles sont susceptibles de posséder elles-mêmes des actifs, de souscrire des engagements, de s'engager dans des activités économiques et de réaliser des opérations avec d'autres unités.Ces unités institutionnelles doivent exercer des opérations économiques pendant un an au moins sur le territoire national pour être comptabilisées dans les secteurs institutionnels. Ce territoire est, si on prend l'exemple de la France, la métropole et les départements d'outre-mer, les enclaves territoriales françaises hors du territoire, l'espace aérien, les eaux territoriales et les espaces qui regroupent des ressources appartenant à la France. En revanche, les enclaves étrangères, à l'image de consulats et ambassades présents sur le sol français, ne sont pas considérées comme des unités résidentes.Les unités institutionnelles ayant la même activité principale et la même source principale de revenu sont regroupées en cinq secteurs institutionnels.On distingue cinq secteurs institutionnels résidents :les ménages ;les sociétés non financières (SNF) ;les sociétés financières (SF) ;les administrations publiques (APU) ;les institutions sans but lucratif au service des ménages (ISBLSM).L'ensemble des unités non-résidentes, dans la mesure où elles entretiennent des relations économiques avec des unités résidentes, sont regroupées dans une catégorie appelée reste du monde, parfois baptisée catégorie « plus-un ».La fonction principale des ménages est la consommation à partir de ressources principales obtenues de deux manières :d'une part par la rémunération des facteurs de production, à savoir le travail, la terre, le capital ;d'autre part, par les transferts effectués par d'autres secteurs institutionnels à destination des ménages.Au sein des ménages, on peut distinguer :le ménage « ordinaire » ou « pur », à savoir un ensemble de personnes vivant dans un logement ;le ménage « collectif » qui est constitué par les populations des maisons de retraite, des foyers de travailleurs, etc.On retrouve également dans ce secteur les entreprises individuelles qui sont des unités économiques dont la fonction principale est la production de biens et services pour leur usage final propre. On retrouve ainsi dans cette catégorie les agriculteurs, les artisans, les professions libérales, les petits commerçants, etc.Les sociétés non financières (SNF) regroupent l'ensemble des sociétés et quasi-sociétés dont la fonction principale est de produire des biens et services marchands, c'est-à-dire dont le prix de vente couvre au moins 50 % du coût de production.Les ressources des sociétés et quasi-sociétés non financières sont le résultat de la production et des éventuelles subventions versées par les administrations publiques (collectivités locales).La CN classe actuellement les SNF en trois catégories, selon le contrôle :Les SNF sous contrôle public, c'est-à-dire sous le contrôle de l'État : la SNCF, la RATP… ;Les SNF sous contrôle privé national : Bouygues, Total… ;Les SNF sous contrôle privé étranger : Google France, Toyota France…Les sociétés financières (ou SF) sont constituées par l'ensemble des sociétés et quasi-sociétés dont la principale fonction est d'offrir des services d’intermédiation financière et/ou d'exercer des activités financières auxiliaires. Leurs ressources sont des fonds provenant des engagements financiers.Cinq sous-secteurs institutionnels constituent le secteur institutionnel des sociétés financières :Les banques centrales ;Les autres institutions financières monétaires (la compatibilité nationale y exclut par convention les sociétés d'assurance et les fonds de pension) ;Les intermédiaires financiers ;Les auxiliaires financiers ;Les sociétés d’assurance et les fonds de pension.Les administrations publiques sont regroupées sous le sigle APU. La fonction principale de ces unités institutionnelles est de produire des services non marchands et/ou d'effectuer des opérations de redistribution des revenus ou du patrimoine national. Elles tirent la majeure partie de leurs ressources de contributions obligatoires (impôts).En France, les administrations publiques (APU) se regroupent en trois sous-secteurs :Les APU centrales (APUC) : composées de l'État et des organismes divers APUC (ODAC) ; les universités, le CNRS, l'ANPE… ;Les APU locales (APUL) : régions, départements, communes + OAL (régie de transport municipal, chambre de commerce…) ;Les ASSO (Administration de sécurité sociale) : unités qui distribuent des prestations sociales à partir de cotisations sociales obligatoires + ODASS ; les ressources proviennent des assurances sociales (ex. : hôpitaux publics).Les institutions sans but lucratif au service des ménages (ISBLSM) regroupent diverses structures dont certaines associations (ex. : association de consommateurs, parti politique, syndicat, Église, organisme de charité, etc.). Leurs points communs sont que, d'une part, elles produisent des services pour les ménages, d'autre part, elles sont financées par des cotisations volontaires et parfois par la vente de biens et services marchands, mais dont le but n'est pas d'en tirer de bénéfice.D'un point de vue économique et du fait de la façon dont la comptabilité nationale les prend en compte, les ISBLSM affichent un rôle négligeable ; il en résulte que dans les statistiques globales, leur consommation est ajoutée à celle des ménages. La majorité des organismes à but non lucratif, qui regroupent l'ensemble des entreprises de l'économie sociale, n'est cependant pas regroupée dans cette catégorie des ISBLSM, ce qui contribue à minorer leur importance. Les différentes études menées situent l'importance de l'ensemble du secteur non lucratif (ISBLSM et économie sociale) à environ 10 % des emplois en France.Ce n'est pas un secteur institutionnel et à ce titre on le qualifie parfois de faux secteur, dans la mesure où les opérations ne sont pas décomposées en distinguant des catégories d'agents : il n'y a pas de compte des ménages ou des SNF du reste du monde. Ce secteur « plus un » regroupe ainsi les unités non résidentes qui effectuent des opérations avec l'économie nationale.Les flux sont enregistrés au moment de la réalisation de l'opération. Les flux financiers sont comptabilisés en « flux nets d'acquisition d'actifs » et « flux nets d'engagements contractuels » alors que les autres flux le sont en « emplois » et « ressources ».Il s'agit de l'ensemble des opérations qui concernent la création et l'utilisation des biens et des services.Parmi elles on distingue :La production, qui a évolué dans le temps; les entreprises y jouent un rôle majeur, mais les ménages ainsi que les administrations sont eux aussi considérés comme des producteurs ;La consommation ;La formation brute de capital fixe — FBCF — (c'est-à-dire l'investissement) ;Les opérations avec l'extérieur (c'est-à-dire les importations et les exportations de biens et de services). Ces opérations sont regroupées dans le TRE (tableau des ressources et des emplois).Ce sont les opérations par lesquelles la valeur ajoutée créée par la production est distribuée entre les salariés, les propriétaires d'entreprises et les administrations publiques, puis redistribuée du fait de l'action des administrations publiques (versements d'allocations financées par des prélèvements…).Pour simplifier on peut considérer ici la valeur ajoutée (VA) comme l'ensemble des richesses créées.VA = P - CI : Production - Consommations IntermédiairesUn indicateur, le taux de marge, résume pour l'essentiel la répartition des richesses créées entre les salariés et les propriétaires d'entreprises. Il mesure la part des profits des entreprises (EBE, excédent brut d'exploitation) dans la VA : taux de marge = EBE / VA x 100. Comme la valeur ajoutée se répartit principalement entre salaires et profits, à une hausse du taux de marge correspond une baisse de la part des richesses créées qui revient aux salariés, et une hausse de celle qui revient aux propriétaires des moyens de production (capital).Ces opérations sont regroupées dans le TCEI (tableau des comptes économiques intégrés).Les opérations financières représentent les engagements pris par les agents économiques les uns envers les autres, en contrepartie de monnaie ou de produits. Par exemple les prêts faits par certains représentent des emprunts pour les autres. La comptabilité nationale retrace ces opérations entre les principaux secteurs institutionnels dans le cadre du TOF « tableau des opérations financières ».(Cette partie de l'article fait la liste des principaux comptes. C'est une ébauche à compléter car chacun d'eux reste à présenter). Le compte de production                      P        −        C        I        =        V        A              {\displaystyle P-CI=VA}  Le compte de production décrit les flux qui composent le processus de production à savoir les consommations intermédiaires qui sont des opérations sur biens et services : son solde est la valeur ajoutée ou la richesse créée. Le compte d'exploitation EBE (excédentaire brut d'exploitation) = Valeur Ajoutée - Salaires - Impôt (production) + Subvention (exploitation)ouEBE= PIB - Salaires - Impôts (production + produit) + Subvention (exploitation + produit) Le compte d'affectation des revenus primaires EBE + Revenus de la propriété reçus + revenus salariés + impôts sur la production - subventions - revenus de la propriété versés = SRPCe compte s'intéresse aux ressources des secteurs c'est la répartition des revenus liés directement au processus de production (revenus primaires). En emploi on a les revenus de la propriété que les secteurs versent. Le compte de distribution secondaire du revenu Srp + Impôts sur le revenu reçus + impôts sur le patrimoine reçus + Prestations sociales reçues + Autres transferts courants reçus+ Cotisations reçues - impôts sur le revenu versés - impôts sur le patrimoine versé - prestations sociales versées - cotisations sociales versés - autres transferts courants versés = RDBCe compte de répartition des revenus secondaires décrit les flux entre les différents secteurs que sont les ménages et les administrations publiques. En ressource de compte les impôts et cotisations sociales sont versés aux administrations publiques. Les ménages reçoivent des prestations sociales. Les autres transferts courants sont versés à l'ensemble des secteurs. En emplois on a les impôts versés et reçus par l'ensemble des secteurs institutionnels. Les cotisations sociales sont versées par les ménages et les entreprises. Le solde obtenu est le revenu disponible brut. Le compte d'utilisation du revenu disponible                     R        D        B        −        C        F        =        E        B              {\displaystyle RDB-CF=EB}  Ce compte permet de distinguer la part du revenu disponible (RDB=revenu disponible brut) qui sera consacrée à la consommation de biens finaux (CF = consommation finale) de celle qui sera réservée à l'épargne (EB = épargne brute). Ce compte constitue en fait la charnière entre les comptes de résultat (ceux qui représentent des flux) d'une part et les comptes d'accumulation (parfois appelés comptes patrimoniaux et qui représentent des stocks). En effet c'est au départ de l'épargne que se constituent les masses capitalistiques. Le compte de capital Emplois+ FBCF (P51)+ CCF+ VS (P52)+ OV (P53 acquis - cédés)+ AF (NP1 + NP2 acquis - cédés)Ressources+ EB(B8) [solde précédent]+ TC(D9 reçu - D9 versé)Solde : Capacité/Besoin de financement (B9A)FBCF : Formation Brute de Capital Fixe Le compte financier le compte financier mesure la variation de l'actif et le passif financier du secteur institutionnel et du reste du monde.Il permet d'évaluer le patrimoine financier des secteurs institutionnels, en dressant un état de la valeur des actifs détenus et des engagements contractés (passif) à un moment donné. Cette opération a souvent lieu au 31 décembre de l'année.Le TEE est un tableau de synthèse qui donne une présentation simultanée des comptes de flux des secteurs institutionnels et des comptes d'opérations. Il rassemble les opérations économiques et financières de l'économie nationale pour une année donnée. Le TEE permet ainsi de mesurer les résultats économiques globaux, la contribution de chaque secteur institutionnel à ces résultats, ainsi que l'importance des relations entre l'économie nationale et le reste du monde. Il constitue également un outil très important pour la prévision économique.La comptabilité nationale utilise le « tableau économique d’ensemble » (TEE) qui rassemble l’origine et l’utilisation des ressources de chaque secteur (sociétés non financières, instituts de crédit, entreprises d’assurance, administrations publiques, administrations privées, ménages et reste du monde).Il est construit en valeur d'une part, en brut, cvs (corrigé des variations saisonnières) et cjo-cvs (corrigé de l'effet des jours ouvrables et des variations saisonnières) d'autre part. Ainsi que pour le TES, les comptes du TEE ne sont pas publiés.Le TEE se décompose en une succession de lignes et de colonnes qui aboutissent chacune à la mesure d'un solde correspondant. Chaque compte est séparé en emplois (actif) et en ressources (passif). Excepté dans le compte de production, les soldes des différents comptes sont évalués dans les comptes trimestriels tout simplement par solde.Le tableau entrées-sorties distingue les branches et secteurs. La branche est constituée par l'ensemble des activités qui élaborent un produit donné. Ainsi, il y a autant de branches que de produits. Un secteur est constitué par l'ensemble des entreprises ayant la même activité principale. Le TES indique le montant de chaque produit utilisé par les diverses branches de l'économie. Il permet de retrouver l'équilibre pour chaque branche entre les emplois et les ressources. Il permet d'expliquer a posteriori et de simuler a priori les incidences d'une modification des conditions économiques générales.La comptabilité nationale utilise le « tableau entrées-sorties » (TES) qui décrit l’équilibre des opérations sur biens et services pour toutes les branches de l’économie. On entend par branche l’ensemble des unités de production qui fabriquent un même produit. Ainsi le TES permet pour chaque branche et pour l’ensemble de l’économie, de faire ressortir un équilibre entre les emplois et les ressources de la branche. Sa structure repose sur une division par branches et par produits. Il constitue un outil utile aux comptables nationaux. Dans une perspective keynésienne, s’inspirant du tableau économique de Quesnay, le TES a été mis en évidence par l'analyse entrée-sortie de Wassily Leontief pour représenter l’ensemble des opérations des agents économiques au cours d’une période donnée.On va donc tout d’abord rappeler l’égalité de base, puis voir la structure du TES, et enfin son utilité. Rappel de l’égalité de base Ressources=Production_(P) + Importation_(M) + Impôts_(M)Emplois = Consommation intermédiaire (CI) + Consommation finale (CF) + FBCF + Exportations (X) + Variation des stocks (VS)Le TES présente l’équilibre emploi/ressources : P + M = CI + CF + FBCF + X + VSCet équilibre est toujours vérifié dans les comptes en T. La structure du TES En ligne : répartition des produits entre les branches c’est-à-dire le volume de produits utilisés par chaque branche.En colonne : volumes des produits nécessaires à chaque branche pour sa production.Le total des ressources de chaque branche est égal au total des emplois des produits correspondants.Le TES se compose :d'un tableau des emplois intermédiairesd'un tableau des emplois finauxd'un tableau des comptes de productiond'un tableau total des ressources L’utilité du TES Le TES donne une représentation cohérente de la production nationale et permet de représenter les branches qui contribuent le plus à la production nationale. Il permet de faire apparaître le degré d’indépendance des branches en faisant le calcul : (Total des consommations intermédiaires de branche/Production de la branche)*100Ainsi, toute modification de la production dans une branche entraîne des répercussions dans les autres branches.Le TES est aussi un instrument de prévision économique. On peut calculer des coefficients techniques : (Consommation intermédiaire en produit x / Production de la branche y)*100.L’ensemble des coefficients techniques donne une matrice sur laquelle on peut baser des prévisions relativement fiables à court terme. Il est notamment possible de prévoir :l’effet d’entraînement d’une branche sur les autres ;les conséquences sur les branches d’une augmentation globale de la production, des exportations, de la consommation des ménages… ;les conséquences de l’interdépendance des branches (goulets d’étranglement).On peut bien entendu critiquer la difficulté de construction d’un tel tableau pour une économie nationale, ainsi que les erreurs de mesure des grandeurs économiques qu’il renferme.Le TES peut servir de base à la construction d'une matrice de comptabilité sociale, entrée utile pour un modèle d'équilibre général calculable.Abréviations :P : production ;M : importation ;C : consommation ;CI : consommation intermédiaire ;CF : consommation finale ;FBCF : formation brute de capital fixe ;X : exportation ;VS : variation des stocks.Le TOF réunit l'ensemble des statistiques financières relatives aux secteurs institutionnels (SI) et permet d'analyser les aspects financiers de l'économie.En dépit de leur taille et de la masse d'informations qu'ils contiennent, ces tableaux sont d'une structure très simple et leur lecture est assez facile et posée.Comptes nationaux et régionaux de la Belgique publiés par la BNBComptes économiques et financier du CanadaComptes nationaux de la FranceBalance des paiements de la France en 2002Comptes économiques du QuébecDes organismes spécialisés sont chargés de vérifier les comptes nationaux : les Cours des comptes.En France, la loi organique relative aux lois de finances (LOLF), promulguée en août 2001 et mise en œuvre depuis le 1er janvier 2006, modifie en profondeur les finances publiques..La comptabilité nationale est assujettie à un principe de sincérité.Le rapport de la Cour des comptes de juin 2006 fait état de manques de précisions dans le système français de comptabilité nationale :« II ne comprend pas les passifs implicites ; il ignore bon nombre d'actifs ayant une utilité sociale, mais qui ne sont pas valorisés faute d'une valeur marchande de référence ; peu d'actifs incorporels sont recensés ; enfin, il se fonde sur une notion d'actif restrictive, excluant la plus grande partie du capital immatériel – éducation, recherche, santé. »La comptabilité nationale, a été conçue dans les années de reconstruction qui ont suivi la Seconde Guerre mondiale. Il fallait vérifier que le pays retrouvait le niveau de production d'avant guerre, puis qu'il rattrapait celui de l'Amérique. L'attention était focalisée sur le quantitatif, et les contraintes environnementales étaient ignorées.Ainsi conçu, et rigidifié par les institutions de la comptabilité nationale, le PIB ne serait pas adapté à l'économie actuelle, dont le but est différent.Concevoir la comptabilité nationale qui répondrait à des objectifs de développement durable suppose un gros effort intellectuel.Du point de vue environnemental, la comptabilité nationale tient compte actuellement de la consommation de ressources naturelles en tant que consommations intermédiaires.Edith Archambault, La Comptabilité nationale, Economica, 2003,  (ISBN 271784712X)Jean-Paul Piriou, La Comptabilité nationale, Repères, La Découverte, 2004,  (ISBN 2707143367)André Vanoli, Une Histoire de la comptabilité nationale, La Découverte, 2002,  (ISBN 2707137022)Gilbert Abraham-Frois, Économie politique, Economica, 2001],  (ISBN 2717842675) (l'ouvrage comporte une annexe sur la comptabilité nationale, claire et synthétique)Dictionnaire d’économie, J-Y Capul, Olivier Garnier, Hatier, 2005,  (ISBN 2218740591)DJ. Muller, P. Vanhove, PECF. Économie, Dunod, 1999Michel Braibant, Vers un tableau « entrées-sorties » idéal et mondial, Edilivre, 2018  (ISBN 9782414267040)Histoire de la pensée économique Normalisation ÉconométrieComptabilitéPlan comptable Mesure économique Produit intérieur brutTaux d'investissementFormation brute de capital fixe Belgique StatbelBudget fédéral de BelgiqueBanque nationale de BelgiqueBureau fédéral du PlanCour des comptes (Belgique) France InseeBudget de l'État françaisCour des comptes (France)Abrégé de comptabilité nationaleComptes nationaux et régionaux belgesLes définitions de l'InseeUNSTATS Portail de l’économie"
économie;""
économie;"En comptabilité nationale, la notion de déficit budgétaire s'utilise lorsque le budget de l'État est en déficit : les recettes de l’État (hors emprunt) sont inférieures à ses dépenses (hors remboursement d'emprunt) d'où un solde budgétaire négatif,.De même, les administrations publiques (ensemble composé de l’État, des ODAC, de l'administration territoriale et des administrations de sécurité sociale) connaissent un déficit public lorsque les dépenses publiques pour une année sont supérieures aux recettes publiques ; le solde des finances publiques est alors négatif.Le déficit budgétaire peut se traduire par de nouveaux emprunts contractés par l'État au cours de l’année, en plus de ceux destinés à amortir les emprunts antérieurs arrivés à échéance. Ces emprunts viennent alimenter la dette de l'État, de même que le déficit public augmente la dette publique. Ainsi, ces deux données sont liées mais se distinguent par leur nature : le déficit est un flux alors que la dette est un stock.Concrètement, les budgets publics (mesurés avec une périodicité annuelle) sont très souvent déficitaires, dans la majorité des pays ; dans le cas opposé, on parle d’excédent budgétaire. En France, dans le cadre de la loi organique relative aux lois de finances, le solde budgétaire de l'année à venir fait l'objet d'une prévision inscrite dans le projet de loi de finances.Pour équilibrer les comptes, le déficit peut être compensé :par l'emprunt (ce qui déplace le « problème » dans le temps, suppose la confiance des créanciers, et a de toute façon un coût puisqu'il faut payer des intérêts) ;par le recours à des réserves préalablement accumulées à partir d'excédents budgétaires réalisés les années antérieures ;plus généralement, en puisant dans le patrimoine ;par des hausses d'impôts, à supposer qu'elles n'atteignent pas le point de rupture où elles détruisent le gisement fiscal, ou par des baisses d'impôts, qui peuvent éventuellement augmenter les rentrées fiscales (voir courbe de Laffer) ;par une émission monétaire (« planche à billets »), qui ne déplace pas la difficulté dans le temps, mais en change la nature en modifiant la valeur de la monnaie. Cette méthode n'est plus utilisée depuis des décennies dans les pays développés (quoique l'assouplissement quantitatif, auquel les États-Unis ont notamment eu recours dans le cadre de la crise financière de la fin des années 2000, s'en rapproche), elle est même impossible dans les États qui ont confié la gestion de la monnaie à une banque centrale indépendante. C'est le cas par exemple des États de l’Union européenne avec la Banque centrale européenne.par une réduction des dépenses publiques.En macroéconomie, on distingue, au sein du solde public, deux éléments : la composante structurelle (dénommée solde structurel), et la composante conjoncturelle. Le solde conjoncturel est traditionnellement calculé à partir des sensibilités moyennes des dépenses et des recettes publiques à la position de l’économie dans le cycle économique (écart de production ou output gap en anglais). Le solde structurel est alors obtenu en retranchant du solde public le solde conjoncturel ainsi construit,. Une autre définition plus simple consiste à dire que le solde structurel est le solde budgétaire qui serait obtenu si la croissance atteint le PIB potentiel. Le solde conjoncturel étant alors obtenu par différence entre le solde réel et le solde structurel. Ainsi, le déficit public structurel est le solde négatif des finances publiques sans tenir compte de l’impact de la conjoncture sur la situation des finances publiques.La variation du solde structurel est aussi appelée « ajustement structurel », qui comprend :l'effort structurel, qui regroupe les effets des mesures prises par l'État concernant les dépenses (réduction du ratio structurel de dépenses publiques) et les recettes (mesures nouvelles en prélèvements obligatoires) ;une composante non discrétionnaire, qui dépend de l'évolution des élasticités des différents prélèvements obligatoires ainsi que la contribution des évolutions des recettes hors prélèvements obligatoires.Parallèlement, on qualifie de solde primaire la situation budgétaire d'un État endetté avant le paiement de la charge de la dette. On parlera donc de déficit primaire ou d'excédent primaire, même si au bout du compte, le budget accuse un déficit. Ce type de solde est utilisé comme révélateur de l'équilibre budgétaire réel de l'État à un moment donné, en lui retranchant le poids de ses déficits passés qu'incarne la dette.Il est important de comprendre le lien entre déficit public et dette publique. Le budget de l'État est en déficit lorsque l'excédent primaire des finances publiques ne suffit pas au paiement des intérêts sur la dette. La dette publique augmente donc en valeur.Dans tous les cas, augmenter le déficit budgétaire a un impact économique qui peut, selon certains économistes, être un stimulant pour l'activité économique par l'intermédiaire d'une politique de relance selon les principes du keynésianisme, ou simplement être un moindre mal dans certaines situations de récession. Pour d'autres économistes, un déficit budgétaire est toujours le signe d'une mauvaise gestion des fonds publics et de l'argent du contribuable et à ce titre il doit être évité (voir politique budgétaire).Si le taux de rendement des investissements publics est supérieur au taux d'intérêt payé sur la dette publique, il peut être rationnel de s’endetter. Toutefois, pour la France par exemple, le déficit actuel (en 2007) ne finance que des dépenses courantes.Lorsque la dette publique est mesurée en pourcentage du PIB, elle peut baisser d'une année à l'autre, même en présence d'un déficit budgétaire. En effet, lorsque la dette et le PIB augmentent, le ratio de la dette publique sur le PIB diminuera si le PIB croît plus vite que la dette.Les pays participant à la monnaie unique européenne sont soumis à une discipline économique et budgétaire visant à empêcher les déficits publics excessifs. Sont considérés comme excessifs les déficits cumulés des administrations publiques dépassant le seuil de 3 % du produit intérieur brut. Cette limite a été définie dans le cadre du traité de Maastricht (1992) et du pacte de stabilité et de croissance (Amsterdam, 1997 ; Bruxelles, 2005).Depuis 2004, des procédures visant à la réduction des déficits excessifs ont concerné dix États membres de l'Union européenne, dont quatre dans la zone euro (Grèce, France, Allemagne et Pays-Bas) et six autres hors zone euro (Hongrie, République tchèque, Slovaquie, Pologne, Chypre, Malte).La procédure a depuis été légèrement assouplie[Comment ?][Quand ?][Qui ?].Par un phénomène purement mécanique, le déficit budgétaire se réduit en période de forte croissance économique, dans la période faste du cycle économique (les recettes de l'État augmentent fortement, alors que ses dépenses ont une volatilité plus faible, donc augmentent moins rapidement).Les différents gouvernements ont également tendance à présenter un budget en fort déficit en début de mandat (application des programmes électoraux, dépenses mises au compte du précédent gouvernement), et à présenter un déficit budgétaire réduit en fin de mandat, à des fins électorales (report à l'année suivante, déplacement de créances sur des organismes publics divers, utilisation de jeux comptables).En France, les déficits publics sont plafonnés à 3 % du PIB depuis 1982 : c'est le ministre de l'Économie et des Finances Jacques Delors sous la présidence de François Mitterrand qui a voulu faire du non-dépassement des 3 % une règle pour le tournant de la rigueur adopté un an plus tard. 1982 voit en effet le déficit budgétaire dépasser les 100 milliards (signal négatif aux marchés économiques mais qui, ramené au ratio, fait 3 % du PNB français),. Le chargé de mission à la Direction du budget, Guy Abeille, explique que : « On a imaginé ce chiffre de 3 % en moins d’une heure, il est né sur un coin de table, sans aucune réflexion théorique. (…) Mitterrand [voulait] qu’on lui fournisse rapidement une règle facile, qui sonne économiste et puisse être opposée aux ministres qui défilaient dans son bureau pour lui réclamer de l’argent ». Selon le néokeynésianisme mis en place en 1981 (programme de relance par la croissance), « limiter le déficit à 3 % permettait de renouer avec la croissance sans pour autant accroître la dette en points de PIB ». Dès lors, ce chiffre revient comme une antienne et, sous l'influence française, l'Union européenne l'impose à l’ensemble des pays membres par le pacte de stabilité et de croissance adopté en 1997 (et révisé en 2005).Par exemple, en France, la crise économique de 1993 a contribué à creuser le déficit budgétaire, et la bonne conjoncture autour de l'année 2000 a réduit mécaniquement le déficit. En 2000, il a été question d'une « cagnotte budgétaire » alors que le déficit global n'était pas comblé.En 2010, le déficit public de la France « au sens de Maastricht » s'est élevé à 136,5 milliards d'euros, soit 7,1 % du produit intérieur brut (PIB).Déficit public de la Zone euro : +0,1 % en 2000,[réf. nécessaire]-1,8 % en 2001,[réf. nécessaire]-2,5 % en 2002,[réf. nécessaire]-3,1 % en 2003,[réf. nécessaire]-2,9 % en 2004.[réf. nécessaire]En 2016, le Luxembourg (+ 1,6 %), Malte (+ 1,0 %), la Suède (+ 0,9 %), l'Allemagne (+ 0,8 %), la Grèce (+ 0,7 %), la République tchèque (+ 0,6 %), Chypre et les Pays-Bas (+ 0,4 % chacun) ainsi que l’Estonie et la Lituanie (+ 0,3 % chacun) ont affiché un excédent public, tandis que la Bulgarie et la Lettonie avaient un solde budgétaire à l'équilibre.Les déficits publics les plus faibles, par rapport au PIB, ont été enregistrés en Irlande (- 0,6 %), en Croatie (- 0,8 %) et au Danemark (- 0,9 %). Quatre États membres ont affiché un déficit supérieur ou égal à 3 % du PIB : l'Espagne (- 4,5 %), la France (- 3,4 %) ainsi que la Roumanie et le Royaume-Uni (- 3,0 % chacun).Lorsque les dépenses dans le secteur privé sont jugées insuffisantes pour relancer l'activité économique et l'emploi, l'État intervient. En recourant au déficit budgétaire, il peut augmenter les revenus disponibles des fonctionnaires civils et militaires, augmenter ses achats et/ou relancer les travaux publics. L'augmentation de la consommation, de l'investissement et de la production nationaux qui en résultent permettent de stimuler la croissance économique et l'emploi.L'inflation. Lorsque les autorités publiques financent le déficit budgétaire par simple émission de monnaie fiduciaire sans tenir compte des besoins de l'activité économique en liquidités, les prix des biens et des services augmentent causant ainsi une baisse, du pouvoir d'achat de la population et de la compétitivité des produits nationaux.""L'effet d'éviction"" . À l'inverse, si l'État veut éviter l'inflation et ses effets néfastes, il peut financer son déficit en se faisant prêter auprès du grand public. Le drainage de l'épargne ainsi réalisé va le raréfier en augmentant son prix : le taux d'intérêt. La conséquence et la baisse de l'investissement privé provoquée par la baisse de l' effet de levier.Dette publiqueBudget de l'ÉtatConseil budgétaireEn FranceFinances publiques en FranceBudget de l'État françaisDéficit budgétaire et déficit publicDéficit public de la FranceDéficit budgétaire de la FranceDéficit de la Sécurité sociale en FranceDette publique de la FranceDirection du budgetAgence France TrésorDans l'UECrise de la dette souveraine | Dette publique de la zone euro | Budget de l'Union européenne Portail de l’économie   Portail de la finance   Portail de la politique"
économie;""
économie;"En économie, un indicateur est une statistique construite afin de mesurer certaines dimensions de l’activité économique, ceci de façon aussi objective que possible. Leurs évolutions ainsi que leurs corrélations avec d'autres grandeurs sont fréquemment analysées à l'aide de méthodes économétriques.Les indicateurs sont construits par l'agrégation d'indices qui figurent dans un document appelé « tableau de bord ». La construction des indicateurs découle d'un choix de conventions qui traduisent plus ou moins bien certaines priorités et valeurs éthiques et morales. Le « Tableau économique » de François Quesnay, l'un des premiers physiocrates qui a vécu au XVIIIe siècle, constitue l'un des premiers exemples d'un tel indicateur visant à mesurer la richesse d'un pays. Depuis les développements des comptes nationaux après la Seconde Guerre mondiale, le produit intérieur brut (PIB) et le produit national brut (PNB) sont les indicateurs les plus courants.Par ailleurs, il existe d'autres indicateurs qui prennent en compte d'autres facteurs ignorés par le PNB et le PIB afin de mesurer le bien-être des habitants d'un pays ; en incluant par exemple des indicateurs de santé, d’espérance de vie, de taux d'alphabétisation. Le Programme des Nations Unies pour le développement (PNUD) a ainsi créé l'indice de développement humain (IDH) dans les années 1990.Des tentatives pour prendre en compte d'autres dimensions telles la sécurité ou pour inclure la « soutenabilité écologique » de l'activité économique dans des indicateurs ont aussi été menées plus récemment.Parmi les nombreux indicateurs économiques très souvent utilisés figurent en premier lieu le Produit intérieur brut (PIB), dont on surveille le taux de croissance afin de mesurer la croissance économique, et le Produit national brut qui permet de comparer les puissances économiques des différentes nations. Sont aussi souvent utilisés le taux d'inflation et des indices du niveau des revenus, de celui de la richesse, ou encore le salaire minimum, le salaire moyen et l'indice de Gini, lesquels fournissent divers aperçus de la répartition et de l'inégalité des revenus. De nombreux indicateurs financiers sont enfin d'usage de plus en plus courant avec l'essor de la mondialisation financière.La mesure de la production d'un pays se fait généralement par le Produit national brut (PNB) et le Produit intérieur brut (PIB). Le PIB est défini comme la valeur totale de la production interne de biens et services dans un pays donné au cours d'une année donnée par les agents résidents à l’intérieur du territoire national. C'est aussi la mesure du revenu provenant de la production dans un pays donné. Ces indicateurs correspondent au développement des comptes nationaux mis en place après la Seconde Guerre mondiale. Ils sont limités du fait des conditions historiques de leur apparition, à la fois dans leur mesure et au niveau conceptuel.Le Produit national brut (PNB) vise à évaluer la valeur des productions nationales réalisées aussi bien sur le territoire d'un pays qu'à l'étranger. Pour ce faire, il retranche du PIB les productions et services réalisés sur le territoire par les non-résidents (donnant lieu au versement de revenus hors du pays) et lui ajoute la valeur des produits et services effectués à l'étranger par des résidents (entreprises ou personnes qui ont donc reçu des paiements de revenus à l'étranger). En dehors de ces ajustements comptables correspondant à la balance des paiements, le PNB présente les mêmes défauts et qualités que le PIB.Pour évaluer la richesse, on utilise souvent le Revenu national brut (RNB) qui fournit une mesure des revenus monétaires acquis durant l'année par les ressortissants d'un pays. Cet agrégat comptable est, au niveau d'un pays, peu différent de la production nationale brute du fait que le PNB est égal à la somme des revenus bruts des secteurs institutionnels, à savoir de la rémunération des salariés, des impôts sur la production et des importations moins les subventions, de l'excédent brut d’exploitation (assimilé au revenu des entreprises) et du solde de revenu avec l'extérieur.Mais les données de patrimoine constituent de meilleures mesures de la richesse proprement dite. Il est difficile toutefois d'obtenir des évaluations comparables du patrimoine quand bien même on se limite seulement aux valeurs monétaires. Le problème devient encore plus ardu si on veut inclure des évaluations du patrimoine physique (immeubles, usines, outils de production, etc.), du patrimoine culturel (monuments, œuvres d'art présentes dans des musées, etc.), et plus encore du patrimoine social. Il serait nécessaire pour cela d'établir des conventions comptables et, si l'on veut effectuer des comparaisons internationales, de se mettre d'accord au niveau mondial sur leur utilisation. Or de telles opérations nécessitent des négociations et des accords internationaux très longs à réaliser.La montée de la mondialisation financière depuis les dérégulations impulsées par les administrations Reagan et Thatcher s’est traduite depuis les années 1980 dans un développement considérable des besoins d’information sur les évolutions des marchés financiers internationaux et les données financières relatives aux obligations d'information et aux états financiers des sociétés cotées. Depuis cette époque, les chiffres de la croissance et du PNB voisinent de plus en plus avec le spectacle des évolutions de l’euro, du dollar et du yen d’un côté, du Dow Jones, du NASDAQ, du Nikkei ou du CAC 40 de l’autre. En effet, les acteurs de la mondialisation que sont les cadres des entreprises financières ou non financières tournées vers l'exportation ont besoin de suivre quotidiennement ces variables de base de leurs arbitrages que sont les taux de change et les niveaux de valorisation boursière. Ainsi, avec la multiplication des expositions des firmes et des nations aux risques de change et aux risques financiers se développent des besoins d’indicateurs en tout genre, de « risque client » ou de « risque pays émergent », associés à chaque type de transactions. La Caisse des dépôts et consignations a créé ainsi des indicateurs synthétiques de libéralisation financière et de crise bancaire informant sur les vulnérabilités associées aux opérations financières mondialisées dans les pays émergents.Les critiques ont été historiquement nombreuses vis-à-vis des indicateurs économiques « classiques ». Marilyn Waring, première femme députée au Parlement néo-zélandais, a souligné que les tâches ménagères et le temps consacré par les parents à l’éducation des enfants, en particulier par les femmes et surtout les femmes dites « inactives », étaient occultés par les mesures de production par individu. En outre, des indicateurs comme le PIB mesurent mal l'économie informelle ou les services domestiques comme le faisait remarquer Alfred Sauvy. Enfin, ils se concentrent sur la valeur ajoutée, et non sur la richesse possédée (stock de capital). Dès lors, une catastrophe naturelle qui détruit de la richesse va pourtant contribuer au PIB à travers l'activité de reconstruction qu'elle va générer. Cette contribution ne reflète pas la destruction de capital, ni le coût de la reconstruction. Cette contradiction était dénoncée dès 1850 par l'économiste français Frédéric Bastiat qui, dans Sophisme de la vitre cassée, écrivait que « la société perd la valeur des objets inutilement détruits », ce qu'il résumait par : « destruction n'est pas profit. »Depuis la fin des années 1980, de multiples mouvements ont mis en cause les capacités du PNB à représenter toutes les dimensions du niveau de vie. Ainsi, au début des années 1990, certaines institutions internationales du système de l'Organisation des Nations unies ont fait un travail de pionnier en proposant de nouveaux indicateurs de développement. La collaboration d'économistes comme Amartya Sen, avec le Programme des Nations unies pour le développement (PNUD), a permis de proposer successivement toute une batterie de nouveaux indicateurs multidimensionnels du développement qui incluent, en plus du PNB, des critères sociaux. Le plus connu est l'Indice de développement humain (IDH). Depuis, de nombreuses autres initiatives se sont multipliées.Le PIB (défini ici comme ""la valeur monétaire des biens et services produits durant une certaine période dans un pays""), est un indicateur très superficiel car :- c'est un indicateur global, qui ne tient pas compte de la répartition de la richesse créée (même en le divisant par la population on obtient qu'un indicateur de la répartition potentielle de la richesse produite, et non de la répartition réelle) → le PIB/population peut s'accroître avec la richesse d'une minorité de la population tandis qu'une majorité devient plus pauvre ;- c'est un indicateur à court terme, qui ne tient pas compte de l'impact de la production sur la déplétion du capital naturel.- il repose sur l'hypothèse que le prix donne une mesure ""exacte"" de la qualité de la richesse produite globalement, c'est-à-dire de la mesure dans laquelle elle répond aux besoins de l'ensemble de la population; or les sondages d'opinions montrent que les budgets militaires (qui représentent une part importante du PIB et qui sont financés par les impôts payés par la population) ont toujours été supérieurs au niveau souhaité par la population, qui préfère que l'on consacre plus de ressources au sport, à la culture ou aux transports en commun; d'autre part une partie importante du PIB résulte d'achats provoqués par le conditionnement imposé que constitue la publicité dans les lieux publics, et qui donc gonflent artificiellement le PIB.Indice de la puissance économique d'une nation, le PNB mesure la richesse d'un pays. Mais il ne fournit qu'une mesure très approximative du bien-être des habitants qui y vivent. Il ne fournit en effet qu'une agrégation comptable des valeurs des différents biens et services marchands produits, quelles que soient les utilités de ces productions. Par exemple, le PNB ne prend pas en compte les externalités négatives de la production (les dégâts causés à l'environnement, les prélèvements sur le patrimoine, etc.). Il ne mesure pas non plus l'impact de toutes les activités non monétarisées et réalisées hors du champ économique proprement dit (travaux domestiques, éducation des enfants, activités artistiques, etc. – ensemble théorisés par l'opéraïsme italien sous le nom de « travail social », et qui concerne souvent les femmes), lesquelles augmentent le bien-être général.Dans le cas des États-Unis, par exemple, le PNB agglomère indistinctement la production de biens qui ne contribuent pas directement au bien-être des habitants (aides au développement, etc.), avec celles des biens ou services produits et consommés par les américains.L'indice de développement humain (IDH) est le premier des indices créés par le Programme des Nations Unies pour le développement (PNUD). Utilisé depuis les années 1990, l'IDH combine trois facteurs permettant d'apprécier les « capacités » des résidents de ces pays (leurs capabilities selon l'économiste Amartya Sen) : l'espérance de vie à la naissance,l'accès à l'éducation, mesuré à partir de la durée moyenne de scolarisation des adultes (en années) et de la durée attendue de scolarisation des enfants en âge scolaire (en années).ainsi que le niveau de vie réel par habitant calculé à partir du logarithme du revenu national brut par habitant en parité de pouvoir d'achat (PPA).L'IDH classe les pays en établissant la moyenne entre ces trois indices principaux « normalisés » (c'est-à-dire ramenés à une échelle de 0 à 1).Le PNUD publie trois indicateurs synthétiques intégrant d'autres dimensions que l'activité économique :D'abord, à partir de 1995, l'Indicateur Sexospécifique (ou sexué) de développement humain (ISDH), qui permet de corriger l'IDH d'un facteur d'autant plus positif que les différences entre les situations des femmes et des hommes sont moins importantes du point de vue des trois critères pris en compte dans le développement humain.Puis, à partir de 1995 également, l'Indicateur de Participation des Femmes (IPF) à la vie économique et politique, lequel complète le précédent en faisant la moyenne d'un certain nombre de taux de participation des femmes à des postes politiques ou économiques valorisés.L'Indicateur de pauvreté humaine (IPH) est introduit à partir de 1997. Il est construit sous un autre principe que celui des capabilities de Amartya Sen. Il signale les manques, privations ou exclusions fondamentales d'une partie de la population en tenant compte de quatre facteurs : longévité, éducation, emploi et niveau de vie. Deux variantes de calculs sont distinguées :une variante 1 (IPH-1) pour les pays économiquement en développementune variante 2 (IPH-2) pour les pays économiquement développés. Pour les pays développés, l’IPH-2 tient compte de quatre critères auxquels il accorde le même poids : la probabilité de mourir avant soixante ans, l'illettrisme, le pourcentage de personnes en deçà du seuil de pauvreté, soit 50 % du revenu médian, le pourcentage de chômeurs de longue durée.Au début des années 2000, dans la lignée du mouvement impulsé par le PNUD, de nombreuses institutions se sont mises à discuter des limites du PNB pour tenter de les dépasser. En novembre 2004 à Palerme, l'Organisation de coopération et de développement économiques (OCDE) a organisé un premier Forum mondial de l’OCDE sur ce thème. Fin juin 2007, l'OCDE a organisé un second colloque à Istanbul portant sur « les statistiques, les connaissances et les politiques ». Il a débouché sur une déclaration énergique exhortant les bureaux statistiques du monde entier à « ne plus se limiter aux indices économiques classiques comme le produit intérieur brut (PIB) ».Face aux mises en cause multiformes de la mondialisation, il s’agissait d’abord, comme le déclarait le secrétaire général de l'OCDE Angel Gurria, de « mesurer en quoi le monde est devenu meilleur ». Afin de mettre en œuvre et généraliser cette déclaration signée par l’ONU et le PNUD, la Commission européenne a réuni les 19 et 20 novembre 2007 à Bruxelles un colloque international dénommé Beyond the GDP (Au-delà du PIB), durant laquelle son président, José Manuel Durão Barroso, défendait la mise en place de nouveaux indices pour mesurer les problèmes contemporains.Ces réunions institutionnelles ont rassemblé une large part des nombreux indicateurs alternatifs mis au point dans le monde entier afin d'évaluer le bien-être social et environnemental. Parmi ces indicateurs synthétiques alternatifs, certains concernent les problèmes sociaux contemporains, d'autres les inégalités et la pauvreté, la sécurité économique et sociale ou le patrimoine écologique.L’Indice de santé sociale (ISS) a été mis au point aux États-Unis par deux chercheurs, Marc et Marque-Luisa Miringoff. L’ISS est un indice social synthétique visant à compléter le PIB pour évaluer le progrès économique et social. C'est une sorte de résumé des grands problèmes sociaux présents dans le débat public aux États-Unis dans les années 1990. Il se traduit dans seize indicateurs sociaux dont il fait une sorte de moyenne. Sont ainsi regroupés dans cet indice des critères de santé, d'éducation, de chômage, de pauvreté et d'inégalités, d'accidents et de risques divers. L'ISS a acquis une grande réputation internationale en 1996, année de la parution d'un article majeur dans la revue économique Challenge montrant le décrochage des courbes de progression du PNB et de l'ISS aux États-Unis, le premier continuant à progresser alors que le second plongeait durablement après les années 1973-1975. Ce graphique montre ainsi en quoi les années Reagan et Bush père ont porté un rude coup à la santé sociale des États-Unis, laquelle se trouvait en 1996 à un niveau nettement inférieur à celui de 1959, en dépit d’une très belle courbe de croissance économique.Le BIP 40 est un indicateur synthétique de l'évolution des inégalités en France dont le nom est une référence ironique à la fois au PIB (inversé) et au CAC 40. Cet indicateur a été mis au point et présenté à la presse en 2002 par réaction au fait que la santé économique et la santé boursière ont droit à des indices synthétiques fortement médiatisés, alors que ce n'est pas le cas pour ceux de la « santé sociale ». Cela même si l’Insee publie de nombreuses études et indicateurs sur le sujet. L'équipe de militants syndicalistes, d'économistes et de statisticiens français qui ont agrégé des indicateurs pour former le BIP 40 est associée à un réseau associatif militant pour la réduction des inégalités, le Réseau d’alerte sur les inégalités (RAI).De façon récente, des chercheurs de grandes institutions internationales (comme Guy Standing au BIT à Genève) et de pays développés (tels Lars Osberg et Andrew Sharpe au Canada ou Georges Menahem en France) ont mis au point des indicateurs visant à cerner le degré de protection économique des personnes contre les principaux risques de perte ou de diminution forte de leurs revenus, par exemple en matière de chômage, de maladie, de retraite, etc.  L'indicateur de bien-être économique de Osberg et Sharpe Osberg et Sharpe prennent ainsi en compte quatre composantes caractérisant le bien-être des populations dans la construction d’un Indicateur du bien-être économique (IBEE) :les flux effectifs de consommation par habitant, qui incluent la consommation de biens et services marchands, les flux effectifs par habitant de biens et services non marchands et les changements dans la pratique des loisirs ;l’accumulation nette dans la société des stocks de ressources productives, y compris l’accumulation nette de biens corporels et de parcs de logements, l’accumulation nette de capital humain et des investissements en Recherche & Développement (RD), les coûts environnementaux et la variation nette du niveau de l’endettement extérieur ;la répartition des revenus, selon l’indice de Gini sur l’inégalité, ainsi que l’ampleur et l’impact de la pauvreté ;la sécurité économique contre le chômage, la maladie, la précarité des familles monoparentales et des personnes âgées.Grâce à leur indicateur ils sont en mesure de comparer les tendances d’évolution du bien-être économique dans six pays de l’OCDE : États-Unis, Royaume-Uni, Canada, Australie, Norvège et Suède. Des comparaisons sont ainsi données sur le site du laboratoire de ces deux chercheurs canadiens. Une application de l'IBEE au cas de la France a été proposée par Florence Jany-Catrice et Stephan Kampelmann en juillet 2007. L'indicateur de sécurité de Standing à l'OIT Dans les travaux de Guy Standing effectués dans le cadre du Bureau international du travail (BIT), la vision est centrée sur le travail et vise à cerner la sécurité économique dans sept domaines. Dans un deuxième temps, un indice synthétique permet d'effectuer une moyenne de ces sept domaines : les revenus (y compris les prestations sociales), la participation à l’activité économique, la sécurité d’emploi, la sécurité du travail (contre les risques d'accidents ou de maladies professionnels), la sécurité des compétences et qualifications, la sécurité de carrière, et enfin celle de la représentation syndicale et d’expression des salariés. Une série de grandes enquêtes ont ainsi été menées par les missions locales du BIT dans une vingtaine de pays. Les pays scandinaves sont à nouveau aux premières places pour cet indicateur. L'indicateur de sécurité de Menahem En France, Georges Menahem a mis au point en 2005 un indicateur baptisé taux de sécurité économique (TSE). Selon ses dernières publications, la sécurité économique peut être décomposée entre une partie « marchandisée » dépendante des relations salariales et de la vente des produits, et une partie ""démarchandisée"" relative aux prestations et aides auxquelles les individus ont droit indépendamment de leurs relations actuelles avec le marché (comme la retraite, les allocations familiales, de logement, de chômage ou le RMI). Ses estimations sur une trentaine de pays montrent que le taux de sécurité démarchandisée est un bon indicateur de l'efficacité du système de protection sociale : il est maximum en Suède et dans les pays Nordiques, il est encore important dans les pays continentaux tels l'Autriche, l'Allemagne ou la France, mais il est faible au Royaume-Uni et dans les pays Européens du Sud comme l'Italie, la Grèce ou l'Espagne et très limité dans les pays d'Europe Centrale et Orientale tels la Lettonie ou la Lituanie. Quant aux États-Unis, leur taux de sécurité démarchandisée est négatif, ce qui témoigne du mauvais état des protections sociales dans ce pays présenté comme un modèle de l'économie de marché. Ce taux n'est que faiblement positif dans deux autres exemples du modèle « libéral » selon le sociologue danois Gosta Esping-Andersen : en Australie et au Canada, à un niveau à peine plus élevé pour ce dernier car les programmes sociaux y sont plus étendus.L'Empreinte écologique est un indicateur visant à mesurer les pressions économiques sur l'environnement. L’empreinte écologique d’une population est la surface de la planète, exprimée en hectares, dont cette population dépend compte tenu de ce qu’elle consomme. Les principales surfaces concernées sont consacrées à l’agriculture, à la sylviculture, à la pêche, aux terrains construits et aux forêts capables de recycler les émissions de CO2. Il s’agit d’un indicateur synthétique, qui « convertit » en surfaces utiles de multiples pressions humaines sur l’environnement, mais pas toutes.On peut calculer cette empreinte pour une population allant d’un seul individu à celle de la planète, et par grands « postes » de la consommation. Par exemple, la consommation alimentaire annuelle moyenne d’un Français exige 1,6 hectare dans le monde ; son empreinte totale (alimentation, logement, transports, autres biens et services) est de 5,3 hectares. Pour un Américain, on obtient 9,7 hectares : le record du monde. Or l’empreinte par personne « supportable » par la planète aujourd’hui, compte tenu des rythmes naturels de régénération des ressources était de 2,9 hectares en 1970, et elle ne cesse de diminuer sous l’effet de la progression de la population, de la régression des terres arables, des forêts, des ressources des zones de pêche, etc. Elle est passée à 2 hectares en 1990 et elle n’est plus que de 1,8 hectare en 2001. Si tous les habitants de la planète avaient le mode de vie des Américains, il faudrait 5,3 planètes pour y faire face. Si tous avaient le niveau de vie moyen des Français, il en faudrait près de trois.De nombreux rapports ont déjà été produits, dont ceux particulièrement documentés et fiables du Global environmental conservation organization (soit le World Wide Fund for Nature ou WWF). Mais leurs conséquences sont limitées compte tenu de la faible visibilité dans la sphère publique de ce problème, ses conséquences négatives sur la vie quotidienne ne touchant pas encore vraiment les acteurs économiques, politiques et médiatiques dominants et les nations les plus favorisées même si leur empreinte écologique est pourtant de loin la plus importante. De ce fait, elles peuvent croire encore dans les bénéfices d’une croissance matérielle soutenue et indéfinie, les indicateurs des limites de notre planète matériellement finie étant difficiles à percevoir.L'empreinte écologique est un indicateur abstrait et synthétique qui ne traduit qu'une faible part des conséquences du dérèglement du climat et des dégradations des écosystèmes. La comparaison de l'empreinte de l'Afrique et de l'Europe montre certes que les pays les plus pauvres ont encore, pour quelque temps, une empreinte écologique par personne très supportable par la planète, ce qui permet aux pays favorisés d'utiliser bien plus que leur surface. Ainsi, les dommages restent au faible niveau des premiers signes que nous observons actuellement. Selon le WWF, ce résultat traduit une dette écologique des pays riches par rapport aux pays pauvres : les premiers « empruntant » aux seconds d’énormes surfaces de ressources naturelles, terres arables, forêts, aux pays du Sud. Tout se passe comme s'ils y exportaient leur pollution, au moins celle qui ne connait pas de frontière, à commencer par celle des gaz à effet de serre.Mais l'empreinte écologique est limitée car elle ne permet d'illustrer que très indirectement l'importance des conséquences du réchauffement climatique :L’accélération du réchauffement climatique dans la période récente directement liée aux émissions d’origine humaine de gaz à effet de serre.La dimension des catastrophes humaines mondiales prévisibles au-delà d’un réchauffement de deux degrés : sècheresses, inondations et tempêtes, élévation du niveau des mers, etc.L'importance et la diversité de ces catastrophes à venir suggère qu'il faudrait compléter l'empreinte écologique par une batterie d'indicateurs d'inégalités économiques et sociales afin d'évaluer en quoi certaines populations plus pauvres sont davantage touchées par ce que les populations riches nomment les « aléas » climatiques. La moitié de la population mondiale vit ainsi dans des zones côtières qui risquent d'être submergées si le niveau des mers s’élevait d’un mètre, évolution possible pour le siècle à venir selon le Groupe d'experts intergouvernemental sur l'évolution du climat (GIEC) si les tendances actuelles persistent.Jean Gadrey et Florence Jany-Catrice, Les nouveaux indicateurs de richesse, Édition La Découverte, 2005Dominique Méda,  Au-delà du PIB. Pour une autre mesure de la richesse, Champs-Flammarion, 2008Rapport mondial sur le développement humain 2005 : La coopération internationale à la croisée des chemins : l’aide, le commerce et la sécurité dans un monde marqué par les inégalités, New York, Programme des Nations unies pour le développement, janvier 2005, 385 p. (ISBN 2-7178-5114-3, lire en ligne), chapitre 2 « Inégalité et développement humain ».WWF: Global environmental conservation organization (ou Organisation de conservation de l'environnement mondial)PIB et développement durableHappy planet indexIndice rouge à lèvresIndicateur du développementLes grands indicateurs de l'économie française, site de l'InseeLes indicateurs économiques du Loiret , iD Loiret Portail de l’économie"
économie;"Selon le vocabulaire de la comptabilité nationale, l’investissement (mesuré par la formation brute de capital fixe, en abrégé FBCF), peut être le fait de différents agents économiques :pour les entreprises : c'est la valeur des biens durables acquis pour être utilisés pendant au moins un an dans leur processus de production. Il peut avoir trois formes : capacité, remplacement et productivité ;pour les ménages : la FBCF dans le cadre de leur activité domestique ne concerne que l'acquisition ou la production pour leur propre compte de logements ;pour les entrepreneurs individuels : la formation brute de capital fixe (FBCF) des ménages en tant qu'entrepreneurs est comptée dans la FBCF des entreprises.Les investissements financiers, les acquisitions de terrains et les investissements immatériels (publicité, etc.) ne sont pas comptabilisés dans la FBCF, bien que ces investissements aient pris depuis les années 1980 une grande importance dans la stratégies des sociétés.Le rendement d'un investissement fait l'objet d'un calcul prenant en compte sa durée de vie ou sa durée d'utilisation (avec dans ce cas la prise en compte de valeur finale résiduelle de l'investissement).L'investissement durable stratégique désigne un type d'investissement se voulant plus vertueux, dans la perspective de développement durable, ce qui implique de prendre en compte dans la prise de décision d'investissement de nouveaux paramètres comme la durabilité et la soutenabilité.L'impact d'un investissement sur une entreprise, qui produit et/ou met à disposition des biens et des services, peut être financier ou uniquement en propriété.Cet impact est financier si l'entreprise reçoit effectivement le montant de l'investissement. Cet investissement va donc augmenter son capital social.Seul un investissement sur le marché primaire (ex : lors de la fondation d'une société ou lors d'une émission d'actions d'une société existante) a un impact financier sur l'entreprise. Cet investissement sert en général à acquérir ou améliorer des moyens de production (machines, locaux, informatique, etc.).Un investissement sur le marché secondaire (ex : un produit financier d'une assurance vie composé d'un « panier » d'actions) est un échange financier (ex : entre un particulier et sa banque) dont pas un sous ne va aux entreprises dont les actions composent le panier.Dans tous les cas, que ce soit un investissement sur le marché primaire ou secondaire, il a un impact sur la propriété. Ainsi, les actions rachetées par d'autres lors d'une OPA peuvent provoquer des changements importants d'actionnaires qui, en vertu des pouvoirs que donne la propriété des actions, sont en mesure de modifier profondément les destinées de l'entreprise et de ses salariés, alors même que l'entreprise elle-même n'a pas reçu un sous : la transaction est uniquement entre investisseurs. Il en est de même lorsque des membres d'un C.A. de sociétés sont des gestionnaires de fonds d'investissement ou de pensions tenus de rentabiliser les placements de leurs petits ou gros épargnants.Ces deux marchés (primaire et secondaire) nous suggèrent une typologie des investissements :Investissements ayant une finalité d'accroissement du capital technique (ou capital fixe, ou capital productif) ;Investissement financier dont la finalité est de rechercher une contrepartie (placement) ou à plus ou moins long terme un gain financier (plus-value).Marx parle de cet investissement d'une manière que certains qualifieraient de manichéenne dans « Das Kapital, Band 2, Abschnitt 1, 1.4 Der Gesamtkreislauf » :« Geldmachen ist das treibende Motiv. Produktion erscheint nur als notwendiges Übel dazu. » soit « Gagner de l'argent est le motif moteur. Pour cela, la production n'apparaît que comme un mal nécessaire ». … à défaut de pouvoir s'en défaire ou d'en rêver comme il le précise dans la parenthèse ensuite : « Alle kapitalistischen Nationen ergreift periodisch ein Schwindel, den sie zur Geldmacherei frei von lästiger Produktion nutzen. » soit « Toutes les nations capitalistes ont périodiquement une chimère, celle de pouvoir faire du fric en se passant d'une production pesante ennuyeuse ».Ce rêve de légèreté et de vitesse des investissements se réalise justement dans la sphère financière, dans le marché secondaire, avec des « produits » financiers de toute sorte et les systèmes « électroniques » pour les transactions internationales. Il se réalise également dans la sphère de l'économie réelle, parfois au détriment de PdG trop adeptes d'une logique industrielle ou sociale et pas assez d'une logique « financière ».Ainsi, Pierre Suard, ancien PdG d'Alcatel, a été nommé par des investisseurs dont la finalité était productive. Il a créé un empire industriel à l'image de Siemens, son concurrent le plus semblable. Il a été débarqué et remplacé par Serge Tchuruk en 1995 après l'arrivée de nouveaux actionnaires dont les investissements étaient plutôt à finalité « financière ». Ce dernier a concentré Alcatel sur son « cœur de métier », escompté le plus rentable, et a vendu le reste. Le changement de slogan qui a suivi, même du « cœur de métier », est révélateur d'un changement de finalité des investissements, moins industriel et plus financier : le slogan « être un architecte d'un monde internet » est remplacé par « apporter de la valeur ajoutée aux actionnaires ».En mars 2021, la même mésaventure arrive à Emmanuel Faber, PdG de Danone débarqué sous l'impulsion d'actionnaires anglo-saxons insatisfaits des résultats financiers de leurs investissements, obérés, d'après eux, par la politique sociale de ce PdG.À la vue des réalités économiques actuelles, il semble que l'influence des investissements « financiers », y compris sur le marché primaire, soit de plus en plus grande.Cette domination des investissements « financiers » peut aussi s’apprécier en considérant les flux financiers : (1-) les flux financiers correspondant au marché primaire (à savoir investissements productifs) sont beaucoup moins importants que ceux correspondant au marché secondaire (à savoir investissements « financiers ») ; (2-) même au niveau du marché primaire, il semble que l'investisseur souhaite minimiser, rendre ""marginale"" son investissement et il a à sa disposition les outils juridiques pour le faire.En effet, le plus souvent, les entreprises investissent directement soit en recyclant une partie de leurs bénéfices, soit surtout en empruntant directement sur les marchés bancaires ou obligataires. La part d’investissement par le marché primaire (ex : par émission d'actions) est minime au regard de leur investissement direct : en 2016 investissement par émission d'actions : 22 M€ ; par emprunt des entreprises : 297 M€ (source : LaTribune et Insee). De plus, il faut déduire des investissements sur le marché primaire la part de plus en plus importante de « rachat d'actions » par l'entreprise sur ordre de ses « investisseurs », ce « rachat » consiste à reverser à ceux-ci les montants de la valorisation d'une partie de leurs actions pour les « annuler ». Souvent l'entreprise doit emprunter pour cela.Enfin, l'investissement, au regard des investissements directement faits par les entreprises, est à considérer en tenant compte du concept de « responsabilité limitée » conjugué avec la non réalité juridique de l'entreprise : les investisseurs d'une entreprise ont de fait la propriété et le contrôle de TOUS les moyens de production de celle-ci alors même qu'ils n'y ont que peu contribué par leur argent.Le concept de « responsabilité limitée » et sa mise en œuvre dans les lois au XIXe siècle (ex : en France, lois du 23 mai 1863 puis du 24 juillet 1867 ; en Angleterre lois de 1856 à 1862 sur les Joint-Stock Company limited) compte, d'après Y.N. Harari dans son célèbre ouvrage Sapiens, « parmi les inventions les plus ingénieuses de l’humanité » : « Peugeot est une création de notre imagination collective. Les juristes parlent de « fiction de droit ». Peugeot appartient à un genre particulier de fictions juridiques, celle des « sociétés anonymes à responsabilité limitée ». L’idée qui se trouve derrière ces compagnies compte parmi les inventions les plus ingénieuses de l’humanité ». Harari en explique les avantages : « Si une voiture tombait en panne, l’acheteur pouvait poursuivre Peugeot, mais pas Armand Peugeot. Si la société empruntait des millions avant de faire faillite, Armand Peugeot ne devait pas le moindre franc à ses créanciers. Après tout, le prêt avait été accordé à Peugeot, la société, non pas à Armand Peugeot, l’Homo sapiens » actionnaire !Cette explication montre que la « responsabilité limitée » est en fait non pas une limitation des risques mais un véritable transfert de responsabilité et des risques de l'investisseur-actionnaire à la société-entreprise, à son collectif de travail, responsabilité pénale et économique. Toutefois ce transfert ne s'accompagne pas en retour d'un transfert de propriété du fait de la non réalité juridique de l'entreprise : quel que soit le montant investi par l'investisseur-actionnaire il a toujours le pouvoir et est propriétaire de fait (de par sa possession des actions) de tous les moyens de production (locaux, machines, moyens informatiques, etc.), y compris de ceux acquis grâce aux « millions » empruntés : c'est l'entreprise, qui acquiert en empruntant, qui rembourse, et qui entretient à ses frais les moyens de production en plus, bien entendu, de payer les salaires, charges et taxes.Grâce à cette « responsabilité limitée » conjuguée avec la non-réalité juridique de l'entreprise, plusieurs procédés permettent aux investisseurs-actionnaires d'accroître les moyens de production qu'ils contrôlent en minimisant au maximum leur mise (le capital social) : investissement par effet de levier, achat à effet de levier, rachat d'actions. Il est donc très compréhensible que les investisseurs-actionnaires recourent à ces procédés plutôt que d’émettre des actions supplémentaires provoquant l'arrivée d'autres investisseurs-actionnaires avec qui certes les risques sont partagés mais également le pouvoir et la propriété. Si l'entreprise était, comme une association 1901, sujet de droit, la « responsabilité limitée » serait remplacée par les « responsabilités et propriétés partagées » entre actionnaires et le collectif de travail de l'entreprise, chacun selon sa contribution. Les procédés « à effet de levier » et autres au profit de certains ne seraient plus et beaucoup d'autres s'en réjouiraient.Sous la finalité générale d'accroissement du capital technique (ou capital fixe, ou capital productif) des objectifs plus précis peuvent être visés :l'investissement de remplacement ou de renouvellement, a pour but de maintenir l'activité à son niveau actuel ;l'investissement de modernisation ou de productivité, a pour but d'accroître la productivité en introduisant des équipements modernes et perfectionnés ;l'investissement de capacité ou d'expansion, a pour but d'augmenter la capacité de production de l'entreprise en ajoutant par exemple des unités de production que ce soit d'un produit déjà existant, il s'agit alors d'une expansion quantitative, ou d'un nouveau produit - on parle alors d'expansion qualitative ;l'investissement total.L'investissement peut être qualifié de :productif : attention double sens possible :soit renvoie à l'idée qu'il s'agit d'un investissement de nature directement productive,soit renvoie à l'idée de l'efficacité de son rendement : la valeur cumulée des biens et des satisfactions obtenues est supérieure voire très supérieure au coût investi ;non directement productif (voire improprement qualifié d'improductif): il concerne des biens et des services d'utilité publique (écoles, hôpitaux, etc) ;matériel : il se traduit par la création d'un bien ou actif réel (un bien de production, par exemple) ;immatériel : il concerne des services : formation, recherche-développement, innovation, marketing, technologies de l'information, publicité, etc., susceptibles d'apporter un développement futur ;financier : il doit être considéré à part compte tenu de ce que sa finalité est de rechercher une contrepartie (placement) ou à plus ou moins long terme un gain financier (plus-value) ;stratégique, lorsqu'il est jugé essentiel pour la survie ou l'avenir de l'investisseur ;réputationnel, lorsqu'il contribue ou est nécessaire à la réputation de l'entreprise ou à son maintien, avec par exemple la publicité, l'acquisition de certains labels et certifications, certaines formes de mécénat, le rappel de produit.L'investissement « productif » se décompose d'abord en bâtiments puis en équipements.La manière dont sont enregistrées et répertoriées les dépenses d'investissement peut conduire à des difficultés pratiques :Par exemple, les dépenses en technologies de l'information sont habituellement rattachées à des centres de coût dans les entreprises. Or dans ce type de dépenses, 50 % en moyenne[réf. nécessaire] concerne la maintenance d'applications existantes, (dépenses d'exploitation) les 50 % restant concernent les développements (dépenses d'investissement). Or la distinction est souvent perdue dans la comptabilité des entreprises (avec un impact fâcheux sur l'évaluation objective de l'effort d'investissement et/ou d'innovation).On parle d'investissement brut quand le flux d'investissement comprend l'investissement neuf et l'investissement de remplacement.Le calcul de l'Investissement net s'obtient par différence entre : Capital technique de fin de période - Capital technique en début de période. Il représente l'investissement brut moins l'amortissement. Selon la théorie économique L'investissement doit être fait jusqu'au point où son bénéfice marginal égale son coût marginal. Ceci suppose évidemment que les biens d'investissements nécessaires soient disponibles. Selon le critère de la rentabilité Investir revient à engager de l'argent dans un projet, en renonçant à une consommation immédiate ou à un autre investissement (coût d'opportunité) et en acceptant un certain risque, pour accroître ses revenus futurs.La rentabilité est mesurable selon différentes méthodes qui ne donnent pas toutes toujours exactement le même résultat, tout en restant globalement cohérentesle retour sur investissement, qui peut s'exprimer en taux ou en temps, mesure le ratio des sommes rapportées par l'investissement sur le montant investi ;la valeur actuelle nette : l'investissement rapporte la différence entre son coût et la VAN, qui dépend du taux d'actualisation retenu ; elle diffère du retour sur investissement en ce qu'elle tient compte du montant total investi : par exemple, le retour sur investissement peut être meilleur pour l'achat d'une bicyclette que d'une maison (rendement respectif de 100 % et 1 %) et la VAN en sens inverse (VAN respective de 200 € et 100 000 €) ;le taux de rentabilité interne (TRI) : l'investissement est d'autant plus rentable que ce taux est élevé (cependant, pour un taux d'actualisation donné et connu, la VAN est un indicateur plus significatif, alors qu'on peut trouver deux investissements A et B tels TRIA > TRIB et VANB > VANA).On peut également assimiler à la rentabilité des critères tels que le temps nécessaire pour atteindre le point mort (durée nécessaire pour que les flux générés soient égaux au montant de l’investissement initial).Le risque pris par l'investisseur est aussi un critère important, dont un indicateur est le ratio de la capacité d'autofinancement par rapport au montant investi ; il est souvent fait à titre prévisionnel pour déterminer si un investissement proposé est adapté, et dans quelle mesure il satisfera l'investisseur.Quelle que soit la méthode utilisée, les paramètres suivants doivent être convenablement appréciés et intégrés dans le calcul :le capital investi a une durée prévue d'utilisation à la fin de laquelle il peut encore présenter une valeur résiduelle ;le prix relatif du capital par rapport à celui du travail influe sur l'investissement. Lorsque le prix du capital baisse par rapport à celui du travail, il est intéressant d'engager des investissements de productivité, qui permettent de substituer du capital (moins cher) au travail (plus cher) ;les taux d'intérêt déterminent le coût des emprunts contractés pour effectuer un investissement et peuvent donc freiner l'investissement s'ils sont élevés ;le niveau d'endettement de l'entreprise joue aussi : une entreprise endettée devra consacrer ses profits à son désendettement au risque de disparaître ;les entreprises cherchent à anticiper la demande avant d'investir pour savoir s'il est nécessaire d'augmenter leurs capacités de production. Ainsi, des anticipations favorables où l'on prévoit une hausse de la demande, favorisent l'investissement tandis que les anticipations défavorables qui prévoient une stagnation ou une baisse de la demande, le freinent. C'est le principe de la demande anticipée ou effective évoquée par Keynes. C'est la demande anticipée des entrepreneurs qui va déterminer l'offre. Autres méthodes financières D'autres méthodes existent qui s'inscrivent dans la mouvance des théories financières intégrant davantage l'incertitude future liée aux valorisations découlant du marché :James Tobin a proposé un critère, appelé le Q de Tobin qui compare la valeur boursière de l'investissement avec son coût de remplacement ;Dixit et Pindyck(1994) proposent de faire l'analogie avec les options: pour la décision d'investissement, l'entrepreneur a le choix entre ne rien faire (attendre) ou investir tout de suite, choix dont l'irréversibilité joue un rôle important dans la productivité.Dans sa décision d'investir, l'entrepreneur compare le cout de l'investissement (I) et la somme des valeurs, actualisées et pondérées par les risques, des rentrées de trésorerie obtenues grâce à l'investissement (R). Le projet d'investissement sera réalisé si R > I. Dans l'analyse keynésienne, l'efficacité marginale du capital désigne le taux de rendement interne de l'investissement. Elle sert de taux d'actualisation des recettes tirées de l'investissement. À savoir, l'investissement est d'autant plus important que le taux d'intérêt est faible. Pour Keynes, l'investissement dépend de la comparaison entre l'efficacité marginale r de l'investissement et le taux d'intérêt pratiqué sur le marché des capitaux, i. Si r > i, la décision de réaliser l'investissement est justifiée. Il peut être financé soit à partir de fonds dont dispose l'entreprise, soit à partir d'emprunt dont le coût est inférieur au taux de rendement de l'investissement. La formule de Keynes n’est valable que pour un investissement financé uniquement par la dette. Si une partie du financement est apportée en fonds propres, il est nécessaire d’en calculer le coût puis de calculer le coût moyen pondéré du capital, qui sera substitué à i. Par ailleurs, l’entrepreneur prendra une marge de sécurité car en pratique, le rendement de l’investissement ne sera pas égal à celui anticipé.Dans l'analyse macro-économique, le terme d'investissement est réservé à la seule création de biens capitaux nouveau (machines, immeubles...). Pour Keynes, l'investissement dépend de l'efficacité marginale du capital et du taux d'intérêt. En fait, les dépenses en biens d'investissement dépendent principalement de deux variables :le rendement attendu de l'investissement, dit ""efficacité marginale du capital"",le taux d'intérêt i ou coût d'emprunt contracté pour financer l'acquisition de biens d'investissement.Pour une efficacité marginale donnée, l'investissement apparaît comme une fonction décroissante du taux d'intérêt. Le niveau du taux d'intérêt est donc la variable incitatrice ou désincitatrice privilégiée du processus d'investissement. Dans l'analyse Keynésienne, l'investissement est considéré comme autonome, c'est-à-dire indépendant du revenu.Avant toute chose, le dirigeant doit faire tout d'abord son métier en restituant l'investissement dans la stratégie d'entreprise et l'organisation d'entreprise. À défaut, il risque de prendre des décisions hâtives en matière de moyens mais sans chemin pertinent et/ou dans une facilité trompeuse qui juge inutile la nécessité de cette réflexion.Avant d'engager ses ressources propres à l'investissement, l'entreprise doit en effet examiner toutes les solutions possibles pour financer son besoin de financement : autofinancement, recours à l'emprunt, leasing, aides publiques (pour la R&D), augmentation de capital ou financement par prélèvement sur fonds propres. Ces sources de financement peuvent être combinées.Il faut aussi noter que les investissements peuvent aussi être financés par cession d'actifs , (dans l'hypothèse où l'entreprise désinvestit dans le cadre d'une stratégie de réorientation ou de recentrage de ses activités).Le législateur offre des possibilités de réduction d'impôt sur le revenu et impôt de solidarité sur la fortune (ISF) pour les particuliers qui investissent dans les PME. La PME doit répondre à des critères quantitatifs (CA < 50M€, emploie de moins de 250 salariés) et à la définition de PME communautaire.Pour l'ISF la réduction fiscale ne concerne que la souscription au capital initial de la société ou la souscription à une augmentation de capital de celle-ci. La possibilité de réduire son ISF risque donc de ne profiter principalement qu'à un cercle réduit de contribuables, sollicités par leur entourage pour participer à ce type d'opérations souvent réalisée en cercle restreint.L'autofinancement est le financement des investissements par des moyens internes à l'entreprise. L'autofinancement se mesure de deux manières : le taux de marge qui donne une indication sur les ressources de l'entreprise (excédent brut d'exploitation / valeur ajoutée) et le taux d'autofinancement : EB/FBCF (Formation Brute de Capital Fixe) qui mesure la part de l'investissement qui est financée par l'épargne brute (partie de l'EBE, hors dividendes, intérêts et impôts, servant à financer la FBCF).Cela consiste à lever des capitaux sous forme de prêt auprès de tiers. La durée de l'emprunt doit être en accord avec la durée d'amortissement du bien acheté (en général l'emprunt est un peu plus court que celle-ci). L'emprunt peut être de 2 types : bancaire ou obligataire.Il s'agit d'augmenter les capitaux propres de l'entreprise en faisant souscrire de nouvelles parts (SARL) ou actions (SA). Il est demandé, via une opération d'augmentation de capital en numéraire,aux actionnaires de mettre la main à la poche pour financer les investissements ;et/ou à de nouveaux actionnaires d'entrer dans le capital de l'entreprise.Cette méthode a l'avantage de renforcer la solvabilité de l'entreprise, laquelle de toute façon ne peut dépasser un certain montant de recours à l'emprunt sans perdre la confiance de ses banques et fournisseurs. Cela dit, cette opération est assez souvent mal vue par les actionnaires, car l'émission de nouvelles actions va « diluer » la valeur de leurs actions actuelles.Cette méthode n'est donc utilisable que si les actionnaires acceptent de remettre de l'argent dans la société. Cela dépendra en grande partie :de la rentabilité des fonds propres affichée ou visée par l'entreprise. Cette rentabilité et le risque qui lui est associé doit être comparée aux autres couples rentabilités/risques disponibles par ailleurs ;et, pour les sociétés cotées, du cours de bourse, qui doit être supérieur au prix d'émission des nouvelles actions pour qu'il y ait intérêt à souscrire celles-ci ;ou encore, facteur plus négatif mais qui entraîne une pression forte sur les actionnaires, d'une situation d'endettement critique risquant de faire sombrer l'entreprise si elle ne trouve pas de l'argent frais pour conforter ses capitaux propres.L'augmentation de capital en numéraire ne doit pas être confondue avec celle par incorporation de réserves (il ne s'agit alors que d'un transfert de poste comptable à l'intérieur des capitaux propres) ni celle par échange de titres (cas de fusion-acquisition)On parle de mal-investissement lorsque l'investissement est inadéquat : trop élevé (sur-investissement), trop faible (sous-investissement), ou les deux à la fois (i.e. : mal orienté).La décision d'investir ou de ne pas le faire, est toujours une forme de pari sur l'avenir : il n'est donc pas étonnant de rencontrer des investissements inadéquats. Lorsqu'une accumulation d'investisseurs se trouvent commettre la même erreur, plus ou moins simultanément, celle-ci peut générer - au niveau macro-économique, dans une filière d'activité ou dans une zone géographique - des situations pouvant aller de la simple récession à la crise économique de plus grande ampleur (voir l'analyse du cycle économique).En régime d'économie libre, la variable essentielle en la matière est le taux d'intérêt. Trop élevé, il rend impossible l'investissement même dans des projets a priori rentables. Trop bas, il favorise l'investissement dans des projets à la rentabilité trop faible.Des agents économiques trop optimistes peuvent sur-investir et créer des capacités de production excédentaires par rapport à la demande effective exprimée par le marché. À l'échelle d'un pays, ou d'une branche d'activité, l'insuffisance constatée des débouchés par rapport à l'offre ainsi créée va provoquer un effet déflationniste et la faillite des entreprises marginales (celles dont le prix de revient est le plus élevé).Goal-based investingDirection générale des entreprises (France)Oséo (France)Small Business Administration (États-Unis)Direction générale des entreprises (Espagne)Ressource relative à la santé : (en) Medical Subject Headings Ressource relative aux beaux-arts : (en) Grove Art Online  Portail de l’économie   Portail de la finance"
économie;"La macroéconomie est une discipline de l'économie qui étudie le système économique au niveau agrégé à travers les relations entre les grands agrégats économiques que sont le revenu, l'investissement, la consommation. La macroéconomie constitue l'outil essentiel d'analyse des politiques économiques des États ou des organisations internationales. Il s'agit d'expliquer les mécanismes par lesquels sont produites les richesses à travers le cycle de la production, de la consommation, et de la répartition des revenus au niveau national ou international.La macroéconomie est une branche de la science économique. Elle se consacre à l'étude des grandes variables économiques nationales ou internationales, et aux relations entre ces variables. Elle repose sur une approche globale, quoiqu'elle puisse se fonder sur des comportements microéconomiques et micro-ondes.Parce qu'elle fonctionne par la comparaison d'agrégats, la macroéconomie est avant-tout une représentation hiérarchisée de l'économie, articulée entre ses agents, via des flux. Elle cherche à expliciter ces relations et à prédire leur évolution face à une modification de certaines variables. La macroéconomie permet par exemple d'estimer la réaction d'un système économique possédant telles caractéristiques face à un choc pétrolier, ou à une politique économique particulière.Contrairement à la microéconomie, qui favorise les raisonnements en équilibre partiel, la macroéconomie se place toujours dans une perspective d'équilibre général, ce qui l'amène à accorder plus d'attention au bouclage des modèles et à la dynamique de création et de maintien d'institutions essentielles, comme les marchés, la monnaie.La macroéconomie a évolué à travers le temps pour devenir plus précise et plus sûre. Ses origines se trouvent dans les premiers travaux économiques du XVIIIe siècle (voir Histoire de la pensée économique), mais elle prend véritablement forme grâce aux travaux de John Maynard Keynes. Sa théorie, le keynésianisme, se fonde sur ce qui fut appelée la macroéconomie keynésienne, qui est l'interprétation keynésienne de la macroéconomie. Elle crée des outils de base de la macroéconomie (le modèle IS/LM, la courbe de Phillips, etc.).La macroéconomie n'est aujourd'hui plus rattachée à une quelconque école de pensée. Elle a évolué vers la construction de modèles économiques complexes, incluant à la fois des relations supposées entre variables et des relations comptables servant à définir les agrégats. Très utilisés pour analyser et prévoir les résultats des politiques économiques, ces vastes modèles qualifiés, le plus souvent, d'économétriques (les plus frustes comportent une dizaine d'équations, les plus complexes dépassent les 1 500) sont à l'heure actuelle employés par la plupart des gouvernements, institutions statistiques (comme l'INSEE), organisations internationales (OCDE) et certains acteurs privés voulant disposer de leurs propres prévisions quant à la conjoncture.Les penseurs de la Grèce antique, tels que Platon, Aristote et Xénophon, avancent des idées économiques. Celles-ci sont toutefois souvent liées à la gestion de la maisonnée, et l'économie désigne l'art de bien administrer sa maison. La microéconomie primitive naît ainsi avant la macroéconomie. Certains auteurs font toutefois remarquer que, chez Platon notamment, la place de l'économie de la cité est parfois pensée dans le cadre des relations géopolitiques,. Il y aurait donc des bribes de pensée macro chez les Grecs.Au XVIIe siècle, des premières réflexions sur la monnaie et l'économie agricole émergent. Les pensées développées sont assez primitives, et il faut attendre le XVIIIe siècle, avec son courant physiocrate, pour qu'un premier système macroéconomique soit ébauché. Cette représentation se trouve dans l'ouvrage de François Quesnay appelé le Tableau économique. Quesnay, médecin de la famille royale, cherchait à représenter l'économie sur les bases de la circulation du sang, comme un système cohérent et dynamique,. Certains auteurs rechignent toutefois à utiliser le terme de macroéconomie pour désigner la pensée de Quesnay, et préfèrent celui de circuit économique.Les Classiques pensent à la fois le niveau micro et le niveau macro. Adam Smith étudie les questions de répartition des richesses et de fonctionnement global de l'économie, tandis que David Ricardo marque l'histoire de la théorie du commerce international. Ils posent donc les jalons de la macroéconomie, conceptualisant le marché et sa concurrence, et mobilisant une loi générale (la loi des débouchés élaborée par Jean Baptiste de Say) qui explique l'économie comme un système. Ils ont toutefois également une pensée micro, car ils réfléchissent à la théorie de la valeur. La pensée des Classiques va jusqu'à créér des relations entre la micro et la macroéconomie. Adam Smith considère, ainsi, que la satisfaction des intérêts particuliers (niveau micro) permet la réalisation de l'intérêt général (niveau macro).Karl Marx propose à son tour une représentation schématique de l'économie industrielle de son époque. Il conceptualise le cycle monnaie-marchandise-monnaie et la baisse tendancielle du taux de profit. Il raisonne en classes sociales, comme ses prédécesseurs, quoiqu'il les redéfinisse. S'attachant à traiter d'agrégats afin de ne pas se perdre dans l'individualité, il adopte une approche macro à l'économie. Il est à ce titre cité dans les manuels de macroéconomie de référence comme un précurseur.Parallèlement, les fondateurs de l'école néoclassique ont utilisé la théorie marginaliste, pour agréger les comportements des agents économiques, c'est-à-dire les consommateurs et les producteurs. Cette microéconomie agrégée, approche souvent à la base de certaines théories macroéconomiques, est à la base de la théorie de l'équilibre général de Léon Walras, et complété par Kenneth Arrow et Gérard Debreu. Cette vision de l'économie ne peut toutefois pas se confondre avec la macroéconomie, étant donné qu'elle ne se base que sur des comportements individuels, et n'analyse pas l'économie dans son ensemble.La distinction systématique entre la microéconomie et la macroéconomie n'émerge vraiment qu'au cours des années 1930. Les travaux de John Maynard Keynes, dont notamment la Théorie générale de l'emploi, de l'intérêt et de la monnaie de 1936, indiquent un intérêt renouvelé pour l'étude agrégée des grandes variables, détachées des questionnements propres à l'économie du niveau micro. Il est question pour cette macroéconomie d'étudier les grandes variables économiques agrégées afin de fournir aux décideurs les clefs de compréhension du monde et des moyens d'action.Keynes écrit ainsi, en 1939, dans la préface à l'édition française de la Théorie générale : « J'ai appelé ma théorie une théorie générale. Par là, j'ai voulu signifier que j'étais principalement intéressé par le comportement du système économique dans son ensemble, avec des revenus globaux, des profits globaux, un produit global, un emploi global, un investissement global, une épargne globale, plutôt qu'avec des revenus, des profits, un produit, l'emploi, l'investissement d'industries, d'entreprise set d'individu particuliers. » Il continue : « je soutiens que l'on a commis d'importantes erreurs en étendant au système dans son ensemble ce qui a été établi correctement pour une de ses parties prise isolément. »L'après-guerre voit l'extension du keynésianisme et sa mise à jour. Repris aux États-Unis, la synthèse néoclassique, macroéconomique, forme le socle des politiques économiques occidentales des Trente Glorieuses. Le monde académique accepte la séparation nette entre microéconomie et macroéconomie. La microéconomie se spécialise alors sur les problèmes d'allocation des ressources par le moyen des prix relatifs (prix d'un produit, ou de plusieurs produits, exprimé par un ou plusieurs autres produits), alors que la macroéconomie étudie la production globale et le niveau des prix.Les échecs de la synthèse néoclassique à prévoir et à enrayer la stagflation (hausse des prix et baisse de la croissance) conduit, au XXe siècle, à l'émergence de nouvelles écoles de pensée qui rénovent les théories macroéconomiques dominantes. Le monétarisme, la nouvelle économie classique et la nouvelle économie keynésienne font partie de ces mouvements de pensée.Robert E. Lucas de la nouvelle économie classique critique la macroéconomie pour son manque de fondements microéconomiques. Cette critique dite de Lucas souligne que la synthèse néoclassique a formé un cadre macroéconomique qui a trop gommé la microéconomie, c'est-à-dire l'étude de la réaction des agents. En effet, l'école de la synthèse les considère généralement comme passifs et incapables d'anticiper les effets des politiques économiques.Parallèlement, les progrès réalisés par le monde académique permet la construction de modèles de plus en plus complexes et élaborés. Rendue possible par l'augmentation des capacités de calcul des ordinateurs ainsi que la généralisation des techniques d'optimisation dynamique, l'explosion des données économiques permettent à la macroéconométrie d'affiner ses capacités prédictives. La critique de Lucas mène aussi à la création de la microéconométrie.Au début du XXIe siècle, des économistes cherchent à dépasser la distinction entre microéconomie et macroéconomie. La plupart des modèles macroéconomiques actuels font l'hypothèse qu'ils ne constituent qu'une simplification de la réalité, dont ils étudient un aspect particulier, comme l'effet de l'innovation sur la croissance, ou des structures monétaires sur l'investissement. De ce fait, ils mélangent relations macroéconomiques et extensions au niveau macroéconomique de relations microéconomiques pour autant que ces extensions soient compatibles avec les faits stylisés qu'on cherche à analyser.La croissance désigne l'augmentation de la richesse produite dans un système économique. Ses causes sont l'un des sujets majeurs de la macroéconomie, qui a proposé différents modèles explicatifs. Le premier grand modèle fut le modèle de la croissance exogène. Il a été dépassé par les modèles de la croissance endogène. Au niveau de ces derniers, le prix Nobel d'économie, l'américain Paul Romer a apporté des contributions majeures.La consommation est l'une des variables majeures étudiées en macroéconomie. Elle est l'une des composantes de la croissance. Elle peut être issue des ménages, des entreprises ou de la puissance publique. La consommation peut être issue de l'extérieur, auquel cas il s'agira d'exportations. En cas d'un manque d'autosuffisance, la consommation peut s'adresser au reste du monde et correspond à des importations.La macroéconomie s'intéresse aux stocks et aux flux. Le stock est une quantité à un moment donnée. Le flux est exprimé en unité de temps (par exemple, sur un an). La dette est un stock constitué de l'ensemble des flux de déficits.La macroéconomie étudie les grands déséquilibres internationaux. Les déficits commerciaux, les déficits budgétaires, les situations de déséquilibres sur les différents marchés internationaux..Toutes les pages avec « macroéconomie » dans le titre(fr) Bernard Guerrien, Une brève histoire de la macroéconomie [lire en ligne (page consultée le 11/09/2015)]Paul Krugman et Robin Wells, Macroéconomie, De Boeck, coll. « Ouvertures économiques », 2009, 2e éd., 998 p.Gregory N. Mankiw, Macroéconomie, De Boeck, coll. « Ouvertures économiques », 2003, 3e éd., 655 p.Olivier Blanchard, Daniel Cohen, Macroéconomie, Pearson éducation, 2002,  (ISBN 2-84211-121-4)David Romer, Macroéconomie approfondie, McGraw Hill / EdiscienceMichel De Vroey, Pierre Malgrange, La théorie et la modélisation macroéconomiques, d’hier à aujourd’hui Document de travail PSE Lire en ligne(en) Gregory Mankiw,""The Macroeconomist as Scientist and Engineer"", NBER Working Paper 12349, juin 2006 Lire en ligneMichael Wickens, Analyse Macroéconomique Approfondie, De Boeck, 2010,  (ISBN 2-8041-6193-5)Gilbert Abraham-Frois, « La macroéconomie en l’an 2000 », Revue économique, vol. 52, no 3,‎ 2001 (lire en ligne, consulté le 28 juin 2011)Pascal Salin, Macroéconomie, PUF, coll. « Premier cycle », 1991, 400 p. (ISBN 978-2-13-043530-3, lire en ligne) Portail de l’économie"
économie;Le marché primaire est le marché financier où les agents économiques peuvent acheter et vendre des actifs financiers qui viennent d'être émis. Il s'agit d'un « marché du neuf », contrairement au marché secondaire, qui est un marché de l'occasion. Le marché primaire est le marché sur lequel a lieu l'émission d'actifs financiers (actions, des obligations, warrants, etc.). Par opposition au marché secondaire, le marché primaire est parfois appelé « marché du neuf ». Les actions d'une entreprise qui vient d'être introduite en bourse sont pour la première fois émises sur le marché primaire. Il en va de même en ce qui concerne l'augmentation de capital d'une société existante, ou encore d'une émission d'obligations souveraines qui permettent à celui-ci de se financer.C'est l'un des métiers de la banque d'investissement que d'être « arrangeur » de ces émissions, en organisant et en centralisant les souscriptions des épargnants et des organismes financiers.Le marché primaire a pour rôle d'assurer la rencontre entre l'offre et la demande de capitaux. Le marché primaire contribue à ce titre au financement de l'économie.Marché secondaireHistoire des bourses de valeurs Portail de la finance
économie;Le marché secondaire est le marché financier où les agents économiques peuvent acheter et vendre des actifs financiers déjà existants. Il s'agit d'un « marché de l'occasion », contrairement au marché primaire, qui est un marché de neuf. Les marchés secondaires sont des marchés où s'échangent des actifs financiers déjà émis et achetés une première fois. Il s'agit à ce titre d'un marché d'occasion, où un actif change de main. Les marchés secondaires comportent à la fois un segment organisé (la bourse), et un segment de gré à gré (marchés privés, over-the-counter). On peut acheter sur les marchés secondaires des instruments financiers comme des actions, des obligations, des options et des contrats à terme. Ces instruments ont déjà été émis et achetés une première fois sur le marché primaire. Toute revente se fait sur les marchés secondaires.Le cours de chaque instrument sur les marchés secondaires fluctue selon l'offre et la demande sur ce marché. La valeur des titres peut ainsi s'écarter de la valeur initiale du titre lors de l'émission.Le marché secondaire a plusieurs rôles. Son premier est un rôle de liquidité, car le marché permet de liquider aisément des actifs financiers. Le deuxième est un rôle de veille, car le marché secondaire agit comme une sorte de thermomètre de la santé financière.Marché primaireHistoire des bourses de valeurs Portail de la finance
économie;"La macroéconomie est une discipline de l'économie qui étudie le système économique au niveau agrégé à travers les relations entre les grands agrégats économiques que sont le revenu, l'investissement, la consommation. La macroéconomie constitue l'outil essentiel d'analyse des politiques économiques des États ou des organisations internationales. Il s'agit d'expliquer les mécanismes par lesquels sont produites les richesses à travers le cycle de la production, de la consommation, et de la répartition des revenus au niveau national ou international.La macroéconomie est une branche de la science économique. Elle se consacre à l'étude des grandes variables économiques nationales ou internationales, et aux relations entre ces variables. Elle repose sur une approche globale, quoiqu'elle puisse se fonder sur des comportements microéconomiques et micro-ondes.Parce qu'elle fonctionne par la comparaison d'agrégats, la macroéconomie est avant-tout une représentation hiérarchisée de l'économie, articulée entre ses agents, via des flux. Elle cherche à expliciter ces relations et à prédire leur évolution face à une modification de certaines variables. La macroéconomie permet par exemple d'estimer la réaction d'un système économique possédant telles caractéristiques face à un choc pétrolier, ou à une politique économique particulière.Contrairement à la microéconomie, qui favorise les raisonnements en équilibre partiel, la macroéconomie se place toujours dans une perspective d'équilibre général, ce qui l'amène à accorder plus d'attention au bouclage des modèles et à la dynamique de création et de maintien d'institutions essentielles, comme les marchés, la monnaie.La macroéconomie a évolué à travers le temps pour devenir plus précise et plus sûre. Ses origines se trouvent dans les premiers travaux économiques du XVIIIe siècle (voir Histoire de la pensée économique), mais elle prend véritablement forme grâce aux travaux de John Maynard Keynes. Sa théorie, le keynésianisme, se fonde sur ce qui fut appelée la macroéconomie keynésienne, qui est l'interprétation keynésienne de la macroéconomie. Elle crée des outils de base de la macroéconomie (le modèle IS/LM, la courbe de Phillips, etc.).La macroéconomie n'est aujourd'hui plus rattachée à une quelconque école de pensée. Elle a évolué vers la construction de modèles économiques complexes, incluant à la fois des relations supposées entre variables et des relations comptables servant à définir les agrégats. Très utilisés pour analyser et prévoir les résultats des politiques économiques, ces vastes modèles qualifiés, le plus souvent, d'économétriques (les plus frustes comportent une dizaine d'équations, les plus complexes dépassent les 1 500) sont à l'heure actuelle employés par la plupart des gouvernements, institutions statistiques (comme l'INSEE), organisations internationales (OCDE) et certains acteurs privés voulant disposer de leurs propres prévisions quant à la conjoncture.Les penseurs de la Grèce antique, tels que Platon, Aristote et Xénophon, avancent des idées économiques. Celles-ci sont toutefois souvent liées à la gestion de la maisonnée, et l'économie désigne l'art de bien administrer sa maison. La microéconomie primitive naît ainsi avant la macroéconomie. Certains auteurs font toutefois remarquer que, chez Platon notamment, la place de l'économie de la cité est parfois pensée dans le cadre des relations géopolitiques,. Il y aurait donc des bribes de pensée macro chez les Grecs.Au XVIIe siècle, des premières réflexions sur la monnaie et l'économie agricole émergent. Les pensées développées sont assez primitives, et il faut attendre le XVIIIe siècle, avec son courant physiocrate, pour qu'un premier système macroéconomique soit ébauché. Cette représentation se trouve dans l'ouvrage de François Quesnay appelé le Tableau économique. Quesnay, médecin de la famille royale, cherchait à représenter l'économie sur les bases de la circulation du sang, comme un système cohérent et dynamique,. Certains auteurs rechignent toutefois à utiliser le terme de macroéconomie pour désigner la pensée de Quesnay, et préfèrent celui de circuit économique.Les Classiques pensent à la fois le niveau micro et le niveau macro. Adam Smith étudie les questions de répartition des richesses et de fonctionnement global de l'économie, tandis que David Ricardo marque l'histoire de la théorie du commerce international. Ils posent donc les jalons de la macroéconomie, conceptualisant le marché et sa concurrence, et mobilisant une loi générale (la loi des débouchés élaborée par Jean Baptiste de Say) qui explique l'économie comme un système. Ils ont toutefois également une pensée micro, car ils réfléchissent à la théorie de la valeur. La pensée des Classiques va jusqu'à créér des relations entre la micro et la macroéconomie. Adam Smith considère, ainsi, que la satisfaction des intérêts particuliers (niveau micro) permet la réalisation de l'intérêt général (niveau macro).Karl Marx propose à son tour une représentation schématique de l'économie industrielle de son époque. Il conceptualise le cycle monnaie-marchandise-monnaie et la baisse tendancielle du taux de profit. Il raisonne en classes sociales, comme ses prédécesseurs, quoiqu'il les redéfinisse. S'attachant à traiter d'agrégats afin de ne pas se perdre dans l'individualité, il adopte une approche macro à l'économie. Il est à ce titre cité dans les manuels de macroéconomie de référence comme un précurseur.Parallèlement, les fondateurs de l'école néoclassique ont utilisé la théorie marginaliste, pour agréger les comportements des agents économiques, c'est-à-dire les consommateurs et les producteurs. Cette microéconomie agrégée, approche souvent à la base de certaines théories macroéconomiques, est à la base de la théorie de l'équilibre général de Léon Walras, et complété par Kenneth Arrow et Gérard Debreu. Cette vision de l'économie ne peut toutefois pas se confondre avec la macroéconomie, étant donné qu'elle ne se base que sur des comportements individuels, et n'analyse pas l'économie dans son ensemble.La distinction systématique entre la microéconomie et la macroéconomie n'émerge vraiment qu'au cours des années 1930. Les travaux de John Maynard Keynes, dont notamment la Théorie générale de l'emploi, de l'intérêt et de la monnaie de 1936, indiquent un intérêt renouvelé pour l'étude agrégée des grandes variables, détachées des questionnements propres à l'économie du niveau micro. Il est question pour cette macroéconomie d'étudier les grandes variables économiques agrégées afin de fournir aux décideurs les clefs de compréhension du monde et des moyens d'action.Keynes écrit ainsi, en 1939, dans la préface à l'édition française de la Théorie générale : « J'ai appelé ma théorie une théorie générale. Par là, j'ai voulu signifier que j'étais principalement intéressé par le comportement du système économique dans son ensemble, avec des revenus globaux, des profits globaux, un produit global, un emploi global, un investissement global, une épargne globale, plutôt qu'avec des revenus, des profits, un produit, l'emploi, l'investissement d'industries, d'entreprise set d'individu particuliers. » Il continue : « je soutiens que l'on a commis d'importantes erreurs en étendant au système dans son ensemble ce qui a été établi correctement pour une de ses parties prise isolément. »L'après-guerre voit l'extension du keynésianisme et sa mise à jour. Repris aux États-Unis, la synthèse néoclassique, macroéconomique, forme le socle des politiques économiques occidentales des Trente Glorieuses. Le monde académique accepte la séparation nette entre microéconomie et macroéconomie. La microéconomie se spécialise alors sur les problèmes d'allocation des ressources par le moyen des prix relatifs (prix d'un produit, ou de plusieurs produits, exprimé par un ou plusieurs autres produits), alors que la macroéconomie étudie la production globale et le niveau des prix.Les échecs de la synthèse néoclassique à prévoir et à enrayer la stagflation (hausse des prix et baisse de la croissance) conduit, au XXe siècle, à l'émergence de nouvelles écoles de pensée qui rénovent les théories macroéconomiques dominantes. Le monétarisme, la nouvelle économie classique et la nouvelle économie keynésienne font partie de ces mouvements de pensée.Robert E. Lucas de la nouvelle économie classique critique la macroéconomie pour son manque de fondements microéconomiques. Cette critique dite de Lucas souligne que la synthèse néoclassique a formé un cadre macroéconomique qui a trop gommé la microéconomie, c'est-à-dire l'étude de la réaction des agents. En effet, l'école de la synthèse les considère généralement comme passifs et incapables d'anticiper les effets des politiques économiques.Parallèlement, les progrès réalisés par le monde académique permet la construction de modèles de plus en plus complexes et élaborés. Rendue possible par l'augmentation des capacités de calcul des ordinateurs ainsi que la généralisation des techniques d'optimisation dynamique, l'explosion des données économiques permettent à la macroéconométrie d'affiner ses capacités prédictives. La critique de Lucas mène aussi à la création de la microéconométrie.Au début du XXIe siècle, des économistes cherchent à dépasser la distinction entre microéconomie et macroéconomie. La plupart des modèles macroéconomiques actuels font l'hypothèse qu'ils ne constituent qu'une simplification de la réalité, dont ils étudient un aspect particulier, comme l'effet de l'innovation sur la croissance, ou des structures monétaires sur l'investissement. De ce fait, ils mélangent relations macroéconomiques et extensions au niveau macroéconomique de relations microéconomiques pour autant que ces extensions soient compatibles avec les faits stylisés qu'on cherche à analyser.La croissance désigne l'augmentation de la richesse produite dans un système économique. Ses causes sont l'un des sujets majeurs de la macroéconomie, qui a proposé différents modèles explicatifs. Le premier grand modèle fut le modèle de la croissance exogène. Il a été dépassé par les modèles de la croissance endogène. Au niveau de ces derniers, le prix Nobel d'économie, l'américain Paul Romer a apporté des contributions majeures.La consommation est l'une des variables majeures étudiées en macroéconomie. Elle est l'une des composantes de la croissance. Elle peut être issue des ménages, des entreprises ou de la puissance publique. La consommation peut être issue de l'extérieur, auquel cas il s'agira d'exportations. En cas d'un manque d'autosuffisance, la consommation peut s'adresser au reste du monde et correspond à des importations.La macroéconomie s'intéresse aux stocks et aux flux. Le stock est une quantité à un moment donnée. Le flux est exprimé en unité de temps (par exemple, sur un an). La dette est un stock constitué de l'ensemble des flux de déficits.La macroéconomie étudie les grands déséquilibres internationaux. Les déficits commerciaux, les déficits budgétaires, les situations de déséquilibres sur les différents marchés internationaux..Toutes les pages avec « macroéconomie » dans le titre(fr) Bernard Guerrien, Une brève histoire de la macroéconomie [lire en ligne (page consultée le 11/09/2015)]Paul Krugman et Robin Wells, Macroéconomie, De Boeck, coll. « Ouvertures économiques », 2009, 2e éd., 998 p.Gregory N. Mankiw, Macroéconomie, De Boeck, coll. « Ouvertures économiques », 2003, 3e éd., 655 p.Olivier Blanchard, Daniel Cohen, Macroéconomie, Pearson éducation, 2002,  (ISBN 2-84211-121-4)David Romer, Macroéconomie approfondie, McGraw Hill / EdiscienceMichel De Vroey, Pierre Malgrange, La théorie et la modélisation macroéconomiques, d’hier à aujourd’hui Document de travail PSE Lire en ligne(en) Gregory Mankiw,""The Macroeconomist as Scientist and Engineer"", NBER Working Paper 12349, juin 2006 Lire en ligneMichael Wickens, Analyse Macroéconomique Approfondie, De Boeck, 2010,  (ISBN 2-8041-6193-5)Gilbert Abraham-Frois, « La macroéconomie en l’an 2000 », Revue économique, vol. 52, no 3,‎ 2001 (lire en ligne, consulté le 28 juin 2011)Pascal Salin, Macroéconomie, PUF, coll. « Premier cycle », 1991, 400 p. (ISBN 978-2-13-043530-3, lire en ligne) Portail de l’économie"
économie;""
économie;"La parité de pouvoir d'achat (PPA) (on parle de valeurs mesurées en parité de pouvoir d'achat) est une méthode utilisée en économie pour établir une comparaison, entre pays, du pouvoir d'achat des devises nationales, ce qu’une simple utilisation des taux de change ne permet pas de faire.Le pouvoir d'achat d’une quantité donnée d’argent dépend en effet du coût de la vie, c’est-à-dire du niveau général des prix. La PPA permet de mesurer combien une devise permet d’acheter de biens et services dans chacune des zones que l’on compare.Les économistes forment un « panier » normalisé de biens et de services, dont le contenu peut être sujet à discussion (à ce sujet, voir en:Discussion and clarification of PPP).La monnaie couramment utilisée comme référence est le dollar américain, pris à une année donnée.Dans un marché global et unifié, sans coûts de transport, les produits identiques ont tous le même prix au même instant et à tous les endroits de ce marché : c'est la loi de prix unique.Cette loi, de nature microéconomique, est théorique. Elle se définit produit par produit, manufacturé ou non (par exemple le cuivre, le café, le ciment, les pneus d'une dimension donnée, une canette de boisson, un hamburger, ce dernier parfois utilisé comme indice rudimentaire de PPA à lui tout seul — l'indice Big Mac). Le monde réel fournit des exemples d'autant plus proches de cette situation théorique que les produits considérés sont mieux standardisés et moins coûteux à transporter.Pour la plupart des produits au contraire, les hypothèses sur lesquelles cette loi repose ne sont pas vérifiées, car le monde est loin d'être un marché unique : les coûts de transport ne sont pas nuls, les réglementations diffèrent en fonction des pays, les droits de douane appliqués aux importations augmentent leurs prix de vente. Par ailleurs, les coûts de fabrication varient fortement en fonction des pays : certaines ressources naturelles sont plus ou moins abondantes, le climat varie, le coût de la main-d'œuvre varie fortement. Les prix sont donc différents d'un endroit à l'autre.Cependant on peut considérer que le consommateur d'un pays substitue à certains produits plus chers dans son pays certains autres moins chers[réf. souhaitée]. Il y a donc non pas un seul produit, mais un ensemble de produits nécessaires à la vie du consommateur moyen. C'est le « panier », qui reflète les habitudes de consommation : au Japon la quantité de protéines nécessaire à la vie est apportée par du soja et du poisson alors qu'en France elle est apportée par de la viande de volaille ou de bovidés.La loi de parité du pouvoir d'achat exprime un coût égal du panier dans tous les pays ayant un niveau de vie raisonnablement comparable. C'est une loi de nature macroéconomique.Les taux de change PPA sont utilisés avant tout dans les comparaisons internationales de niveau de vie. La comparaison internationale des PIB conduit à ne pas prendre en compte les différences de prix existant entre les pays. Les écarts entre les taux de change réels et les taux de change PPA peuvent être significatifs. Ainsi, lorsque le yen, la monnaie japonaise, est surévalué, comme en 1999, le PIB par habitant paraît beaucoup plus élevé que son équivalent américain, alors que mesuré en PPA, il est en réalité beaucoup plus bas.Cette méthode permet de s'affranchir de trois problèmes :les taux de change des devises peuvent connaître des variations subites et brutales sans qu'il y ait modification des conditions économiques. Une comparaison internationale des évolutions à court terme serait faussée par une utilisation des taux de change du marché ;les devises des pays pauvres sont systématiquement sous-évaluées sur le marché des changes du fait de leur moindre productivité (c’est l’effet Balassa-Samuelson) ;certains pays fixent administrativement le taux de change de leur devise. Cela a pour effet de fausser les statistiques et les comparaisons internationales. C'était en particulier le cas des pays d’Europe de l'Est avant 1989.L'utilisation des PPA permet de s'affranchir de ces trois effets.La PPA est parfois utilisée comme un indicateur de la sous-évaluation ou surévaluation d'une devise par rapport à une autre sur le marché des changes. L'exercice est hasardeux, compte tenu des incertitudes inhérentes à cet instrument de mesure.La PPA absolue définit un cours de change entre deux monnaies. Elle est déterminée en définissant un panier de consommation dans un pays A et en évaluant le prix d’un panier « semblable » dans un pays B par la formule :                    P        P                  A                      t                          =                                            P                              t                                                    P                              t                                            ∗                                                    .              {\displaystyle PPA_{t}={\frac {P_{t}}{P_{t}^{*}}}.}  où PPAt est la PPA absolue entre les deux pays sur la période t, et Pt est le prix sur la période t du panier de référence dans le pays A. L'autre pays, B, est marqué par un astérisque.Pour prendre un exemple chiffré, fictif, si un panier de produits évalués à 1 000 $ aux États-Unis a un coût moyen de 900 € en France, alors le taux de change en PPA du dollar par rapport à l'euro sera de 0,90. Ce taux est calculé indépendamment du cours de l’euro en dollar sur les marchés des changes, qui peut fortement fluctuer.La PPA relative mesure la variation de la PPA entre deux périodes. Elle s'exprime ainsi :                                                        P              P                              A                                  t                                                                    P              P                              A                                  t                  −                  1                                                                    =                                                            P                                  t                                                            /                                            P                                  t                  −                  1                                                                                    P                                  t                                                  ∗                                                            /                                            P                                  t                  −                  1                                                  ∗                                                                          {\displaystyle {\frac {PPA_{t}}{PPA_{t-1}}}={\frac {P_{t}/P_{t-1}}{P_{t}^{*}/P_{t-1}^{*}}}}  où PPAt est le taux de change et Pt est le prix à la période t. Le pays étranger est marqué par un astérisque.Une variation de la PPA relative permet de mettre en évidence un différentiel d’inflation entre deux régions du monde.Plusieurs arguments limitent la pertinence et l’usage des PPA :les PPA peuvent varier de façon très importante suivant le choix du panier de produits. En ce sens, il est soumis aux mêmes limitations que les indices des prix ;les habitudes de consommation et les choix sont parfois très variables entre pays. Les produits consommés par les populations en dépendent et construire deux paniers équivalents est un travail très subjectif ;les différences de qualité pour deux produits mis en équivalence sont difficiles à évaluer ;les prix peuvent beaucoup varier à l’intérieur d’un même pays. Le prix d’un verre de bière est beaucoup plus élevé dans un bar sur les Champs-Élysées que dans un village du Massif central ;les prix des produits importés dépendent du taux de change. Une modification du taux de change a donc une influence sur la PPA alors que celle-ci est construite pour définir une parité de change décorrélée du marché des changes.Des indicateurs comme l’indice Big Mac, construit initialement par The Economist, ou l’indice iPod, assez frustes, sont parfois utilisés à des fins pédagogiques.Liste des pays par PIB (PPA)Liste historique des régions et pays par PIB (PPA)Programme de comparaison internationale(en) Penn World Table: tableaux de référence sur la parité de pouvoir d'achat et les revenus nationaux convertis en prix internationaux pour plus de 169 pays sur la période 1950 jusqu'à récemment. Site de l'université de Groningue.(fr) Le marché ou la PPA : Quelle base de comparaison choisir ? - Étude du FMI, mars 2007 [PDF](fr) Parités de pouvoir d’achat : mesure et utilisations - Cahier statistique de l'OCDE sur les PPA, mars 2002 [PDF](fr) Prix et salaires 2015 - Est-ce que je gagne assez pour me permettre la vie que je veux ? [PDF] Portail de l’économie"
économie;"Le prix, exprimé en un montant de référence (en général monétaire), est la traduction de la compensation qu'un opérateur est disposé à remettre à un autre en contrepartie de la cession d'un bien ou un service. Le prix mesure la valeur vénale d'une transaction et en constitue l'un des éléments essentiels.Le mécanisme de formation des prix est un des concepts centraux de la microéconomie, spécialement dans le cadre de l'analyse de l'économie de marché, où les prix jouent un rôle primordial dans la recherche et la définition d'un prix dit « d'équilibre » (alors qu'ils jouent un rôle plus mineur dans une économie administrée).Les niveaux de prix possibles sont en nombre potentiellement infini, selon les acteurs économiques, selon leurs estimations de la valeur de la chose pour eux-mêmes et pour les autres (spéculation). Si une transaction se réalise effectivement, le prix traduit le compromis entre les estimations de l'acheteur et celles du vendeur (reflet de l'offre et la demande).Le mécanisme de détermination des prix peut être affecté par d'autres facteurs :éventuelles imperfections régnant sur le marché (monopole, oligopole, pénurie, marché noir, etc.),contraintes légales lorsqu'il en existe (les prix n'étant pas toujours libres : « prix imposés » ou « administrés »),considérations techniques, telles que la méthode de mise en marché (commerce national et international, commerce de gros, commerce de détail, enchères, etc.) ou les contraintes que cela implique (délais de transmission des offres, définitions des priorités entre offres, ...).Selon l'objet concerné, le périmètre et la méthode de détermination du prix varie. On rencontre ainsi différentes sortes de prix :le prix d'achat.le prix de vente, qui indique le prix auquel un commerçant déclare être disposé à céder la chose et qui ne doit pas être inférieur au coût de revient (interdiction légale de la vente à perte) ;le prix de revient, censé refléter l'ensemble des dépenses liées aux intrants et à la fabrication d'un produit ou d'un service ; Le prix de revient ou coût de revient est égal au coût de production majoré des frais de transport ;le prix d'acceptabilité ou prix psychologique, qui définit le prix qu'une grande partie de la clientèle trouve justifié pour l'acquisition d'un bien ou d'un service ;le prix de cession, qui indique le prix auquel est facturée une cession entre deux services d'une même entreprise ou entre deux filiales d'un même groupe. En matière de comptabilité des entreprises, les prix de cession concernent les biens immobiliers (qui ont une longue durée de vie à l'instar des constructions ou des terrains non bâtis) par opposition aux prix de vente qui concernent, eux, les produits courants c'est-à-dire ceux qui sont relatifs à l'activité normale de l'entreprise.Raymond Barre distingue plusieurs types de prix en fonction du degré de liberté du marché (prix libres, prix administrés), du stade d'élaboration du produit (prix du gros ou de détail) ou encore de la nature des produits vendus (prix des produits agricoles, industriels ou des services)L'importance du système de prix libres a été mise en avant et débattue en particulier dans les années 1920-1930.Une vive controverse sur la question du calcul économique oppose les économistes de l'école autrichienne d'économie, Ludwig von Mises puis, ultérieurement Friedrich Hayek, aux tenants du socialisme de marché, Oskar Lange au premier chef. Pour Ludwig von Mises, le système de prix libres est le seul moyen de coordination des actions des millions d'individus qui composent l'économie d'un pays. Friedrich Hayek relaie cette idée et insiste pour sa part sur le rôle des prix comme vecteur de transmission de l'information disponible aux individus.L'économiste Milton Friedman résume cela en écrivant que le système de prix libres remplit trois fonctions :transmission de l'information sur l'offre et la demande ;.incitation pour les producteurs à s'orienter vers les secteurs aux prix élevés et, partant, à permettre un retour à l'équilibre ;répartition des revenus.Dans une économie planifiée, les prix n'ont pas la même importance. L'appareil productif peut s'en passer : au lieu de chercher à maximiser la valeur ajoutée de sa production comme il le ferait dans une économie de marché, un producteur peut se voir attribuer un quota de matières premières et un objectif de production ; les prix sont fixés par les pouvoirs publics à un niveau considéré comme « souhaitable », mais ils ne sont pas directement connectés aux décisions d'allocations des matières premières ou d'objectif de production, qui sont fixés par ailleurs. Il peut en résulter une pénurie (file d'attente et marché noir) ou un rationnement, si le prix est inférieur à l'utilité pour les consommateurs, ou des excès de production dans le cas contraire.En outre, certaines situations (par exemple, la guerre) incitent les autorités à recourir au contrôle des prix (ou du moins du prix de certains produits jugés nécessaires), ou à influer sur l'offre (protectionnisme, subvention...) et la demande.En réalité, la liberté totale des prix est rarement constatée, même dans les économies réputées les plus libérales, notamment à cause de l'impact de la fiscalité, de lois anti-dumping, des subventions, des engagements pris dans le cadre de contrats pluri-annuels, etc.Sur un marché libre le prix reflète l'équilibre entre l'offre et la demande. Mais les auteurs classiques (Adam Smith, David Ricardo, John Stuart Mill, ...) et Karl Marx considèrent qu'il est soumis plus aux influences de l'offre (coût exprimé par une certaine quantité de travail) que de la demande. Pour Karl Marx l'équilibre tend à se fixer autour de la valeur du travail incorporé. Ricardo estime également que le ""prix réel"" correspond à la quantité de travail incorporé mais constate que le ""prix courant"" est fonction de l'offre et de la demande. Le prix courant aurait tendance à se rapprocher du prix naturel. Selon Adam Smith le prix se dissocie de la ""valeur réelle"" car il tient compte de la valeur de la monnaie qui, elle, est variable. À partir de la fin du dix-neuvième siècle, les auteurs marginalistes (Léon Walras, Stanley Jevons, ...) estiment que le prix ou ""la valeur d'échange"" ne dépend pas de l'offre mais de la demande et donc de l'utilité exprimée par le consommateur. Malgré les influences de la demande sur la détermination du prix du marché admises par les marginalistes, Alfred Marshall considère que, de toute façon, on ne peut pas se passer du concours des deux (i.e l'offre et la demande) pour la fixation du prix. André Orléan estime que la fixation d'un prix peut s'établir par mimétisme et non en fonction du travail incorporé (côté offre) ou de l'utilité (côté demande),. Pour Jacques Perrin, les institutions jouent ou doivent jouer un rôle dans la constitution des prix en prenant en compte l'utilité sociale. Le prix n'est pas donc déterminé par l'unique confrontation de l'offre et de la demande qui sont exprimées par des agents économiques ""rationnels"". Elles subissent d'autres influences (psychologiques, sociologiques et institutionnelles). Par ailleurs, l'offre et la demande sont exprimées dans le temps. Ce dernier peut avoir une influence capitale dans les décisions des producteurs et des acheteurs. Avant de produire, de vendre ou d'acheter, ils procèdent notamment à des études futures du marché pour exploiter les opportunités avantageuses et éviter les menaces sources de risques majeurs.Les libéraux, en faisant appel au concept du consommateur-roi de Paul A. Samuelson, considèrent que les consommateurs, par leur pouvoir d'augmenter ou de baisser librement la demande exprimée sur le marché des biens de consommation, déterminent les prix et donnent le signal aux entreprises d'augmenter ou de baisser l'offre conséquente. Par conséquent, toute chose étant égale par ailleurs, les entreprises vont augmenter ou diminuer les demandes portant sur les marchés du travail, des biens de production et des capitaux déterminant ainsi les taux de salaire, d'intérêt et les prix sur le marché des biens de production. Cependant, John K. Galbraith a montré, dans les années 1960, que le fonctionnement réel de l'économie contemporaine ne correspond pas à ce schéma théorique. Les entreprises, en agissant sur le marché des biens de consommation (études des besoins du consommateur, étude de la concurrence, promotion des ventes) conditionnement la demande du consommateur aussi bien sur ce marché que sur ceux des biens de production, du travail et des capitaux et le privent de toute initiative. De plus, la liberté du consommateur est contrariée par la dépenses budgétaires de l'État visant à accroître les investissements publics pour augmenter la croissance économique. Cette nouvelle stratégie des entreprises est appelée par John K. Galbraith la "" filière inversée "".L'évolution des prix n'est pas l'inflation (l'augmentation du niveau général des prix), qui ne mesure le prix que par de la monnaie, alors que l'évolution des prix en général dépend du fonctionnement de l'économie, qui modifie le prix relatif des biens (i.e le prix d'un bien exprimé par d'autres biens). Cependant la mesure du prix de la monnaie ne peut être fait qu'indirectement, par mesure du prix d'un panier représentatif de biens : si le prix de ce panier augmente, c'est que la valeur (relative) de la monnaie diminue, et inversement.Il existe différents indices de prix pour différentes classes de biens et pour différents usages :les prix à la consommation sont mesurés par l'Indice des prix à la consommation (IPC ou, en anglais, CPI) ;les prix à la production sont mesurés séparément, et correspondent aux Coûts de production ;l'indice du coût de la construction ou l'indice de référence des loyers mesurent l'évolution du prix du logement ;etc.Pour un bien, on parle de « prix nominal » lorsque l'on fait référence au prix exprimé dans une monnaie donnée. On parle de « prix réel » lorsque l'on extrait du prix nominal la part due à l'évolution de la valeur de la monnaie, c'est-à-dire l'inflation.Les prix définissent une distance dans l'espace des commodités préservant la valeur des échanges.Soit A et B, Alphonse et Brigitte, deux agents économiques qui possèdent chacun les vecteurs commodités a et b, par exemple                     a        =        (        1        ,        1        ,        3        ,        0        ,        10        )              {\displaystyle a=(1,1,3,0,10)}   et                     b        =        (        0        ,        1        ,        4        ,        1        ,        100        )              {\displaystyle b=(0,1,4,1,100)}   avec l'ordre (voiture,table,chaises,machine à laver,monnaie), et p le vecteur des prix, la distance comparant la richesse des deux agents est définie par                    d        (        A        ,        B        )        =        d        (        a        ,        b        )        =                  |                          ∑                      i                                    p                      i                          (                  a                      i                          −                  b                      i                          )                  |                      {\displaystyle d(A,B)=d(a,b)=|\sum _{i}p_{i}(a_{i}-b_{i})|}  Cette pseudo-distance définit une relation d'équivalence dans l'espace des commodités qui préserve les écarts de richesse lors d'un échange. Par exemple si d(A,B) est de 100 et A et B échangent une même valeur de commodités d(a',b')= 0, i.e. A donne a' à B et B donne b' à A, après l'échange d(A,B) est toujours égale à 100. Cette préservation de la valeur ne serait pas vérifiée si d était une autre distance.Contrôle des loyersJuste prixLoi du maximum généralMarginalismeOffre et demandeParadoxe de l'eau et du diamantPrix prédateursValeurDistinction entre l'acompte et les arrhes(en) Price Theory par Milton Friedman(en) Theory of Price par George StiglerAndré Orléan, L'empire de la valeur, Seuil, 2011(en) Le système de prix libres, Henry Hazlitt(en) Four Thousand Years of Price Control, Ludwig von Mises Institute Portail de l’économie   Portail du management   Portail du commerce"
économie;"La production est l'action d'un sujet qui transforme une matière première  pour faire exister un nouvel objet. On rencontre ce phénomène de production dans la société, mais aussi bien dans la nature. C'est pourquoi on peut l'étudier soit sous l'angle économique et sociologique, soit sous l'angle biologique.Le terme « production » dérive du latin classique qui signifie « prolonger, mettre en avant ». Dans l'Antiquité, il désigne aussi bien les créations de la nature (l'arbre producteur de fruits) que celles de l'homme (l'artisan producteur d'objets utiles). Ce n'est qu'au début de l'ère industrielle qu'il entre dans le discours économique.Selon John Stuart Mill, « l'économie décrit les lois des phénomènes de société qui se produisent du fait des opérations conjointes de l'humanité pour la production de richesses ». L'économie est donc la discipline scientifique qui étudie la production comme élément fondamental, mais aussi l'échange, la distribution et la consommation des biens et des services. C'est ainsi qu'on étudie la production selon les méthodes, les lieux et les marchés. On compare la production d'un même produit à partir de modèles différents d'organisation. On calcule le volume de production par pays et par époques. On sépare l'analyse par secteurs économiques. On distingue la production marchande de la production non marchande. La production marchande est celle qui est réalisée et vendue essentiellement par les entreprises sur le marché des biens de consommation achetés par les ménages ou sur celui des biens de production achetés par les entreprises. La période de référence est généralement l'année. Elle est différente de la production annuelle. Grâce à la variation des stocks (stockage lorsque la production annuelle n'est pas totalement vendue ou déstockage dans le cas où celle-ci est insuffisante), la production permet de répondre au besoin annuel du marché national. Cette production est celle réalisée sur le territoire national (par des entreprises nationales ou étrangères) et ne tient donc pas compte de la production réalisée par des entreprises nationales dans le reste du monde. Par contre, la production est dite non marchande lorsque le prix payé par l'utilisateur est inférieur à la moitié de son coût de production. La production non marchande est réalisée essentiellement par l'État et accessoirement par les administrations privées (syndicats et partis politiques, par exemple) et les ménages. Les services concernés sont, essentiellement, de défense nationale, de sécurité, de justice, religieux et de spectacle public. La production non marchande sous forme d'autoconsommation des ménages n'est pas prise en compte par la comptabilité nationale car elle n'est pas justifiée par des documents (factures ou bulletins de paie, par exemple) justifiant son existence. Par contre, les travaux domestiques (services de jardinage ou d'éducation des enfants) sont comptabilisés lorsque le paiement des domestiques est prouvé par des pièces justificatives et non effectué uniquement gratuitement ou par remise d'une somme d'argent de la main à la main.La première approche économique de la production fut celle des physiocrates au XVIIIe siècle, qui considéraient que seule l'agriculture était vraiment productrice puisque le végétal apporte plus de graines qu'il n'en consomme, les autres activités ne faisant que transformer les produits de la terre. Au siècle suivant, David Ricardo va mettre l'accent sur la théorie de la valeur fondée sur le travail, approfondissant la distinction entre valeur d'usage et valeur d'échange. Henry Charles Carey est un célèbre économiste américain qui s'est opposé à Ricardo et au libre-échange en faisant l'éloge du capitalisme protectionniste et interventionniste américain,.Aujourd'hui, la production est l'activité socialement organisée exercée par une unité institutionnelle qui combine des facteurs de production (facteur travail et facteur capital) afin de transformer les consommations intermédiaires en biens ou en services s'échangeant sur le marché.Depuis les travaux de Colin Clark, on regroupe les activités économiques de production  selon trois grands secteurs :le secteur primaire : l'ensemble des activités qui exploitent les ressources naturelles : agriculture, mines, pêche...le secteur secondaire : toutes les activités de transformation d'une matière première : industries manufacturières, construction...le secteur tertiaire : principalement marchand : commerce, transports, hébergement-restauration... ; ou non-marchand : administration publique, enseignement...Selon une enquête de 2016, en France, le secteur primaire représente 2,8 % des 26 millions de personnes possédant un emploi (au sens du Bureau international du travail) ; le secteur secondaire 20,6 % et le secteur tertiaire 75,7 %. La France est le pays européen où le poids du tertiaire est le plus élevé.Selon l'INSEE, l'industrie regroupe « les activités économiques qui combinent des facteurs de production (installation, approvisionnement, travail, savoir) pour produire des biens matériels destinés au marché. » En France, l'industrie représente 12,4 % du PIB (20,3 % en Allemagne, 8,7 % au Royaume-Uni). La part de l'industrie manufacturière dans l'économie française a diminué de moitié depuis 1970 (5,7 millions de salariés contre 2,7 millions aujourd'hui).On distingue la production marchande de la production tout court.La production marchande peut se subdiviser en deux catégories :la production marchande simple où le producteur vend son produit sur le marché ou rend un service marchand à titre individuel ;la production marchande capitaliste où le produit ou le service créé par des salariés est propriété du capitaliste. Il est ensuite vendu en tant que marchandise dans le but de réaliser un bénéfice.La production non-marchande se définit comme la production de biens ou services proposés gratuitement ou à un prix inférieur au coût de production, par des organisations publiques, ou des associations.La production réelle d'une entreprise ne correspond pas normalement à sa production vendue. Celle-ci comprend en effet, en plus de la production propre de l'entreprise, celle issue d'autres entreprises, qui correspond aux matières premières et aux autres produits achetés (appelés consommations intermédiaires) pour fabriquer le produit vendu. La production réelle de l'entreprise, appelée « valeur ajoutée », est donc sa production vendue, de laquelle il faut retrancher les consommations intermédiaires.Lorsque la production n'est pas vendue sur le marché (l'essentiel de la production des administrations), sa valeur correspond, par définition, à son coût de production. Comme dans le cas de la production marchande, la valeur ajoutée des administrations est obtenue après avoir retranché les consommations intermédiaires de la production.Différentes organisations permettent de produire un bien ou un service. Certaines sont des espaces où sont concentrés les moyens de production et les ressources humaines pour produire à grande échelle, en grande quantité et d'une manière répétitive avec une division des tâches poussée. D'autres sont des structures plus éclatées et plus mobiles comme l'entreprise en réseau, (l'entreprise étendue) mise en place dans le cadre de l'économie post-industrielle.Trois grands modes d’organisation de la production peuvent être observés : organisation de type « série unitaire »,  les industries process, la production manufacturière.La sociologie économique considère que la production est une activité de création, de rencontre, d'échange et de partage de nombreux éléments tels que le temps, l'espace, les biens, les idées et les émotions.Les économistes ont modélisé la production en identifiant les éléments qui contribuent à sa réalisation, à savoir les facteurs de production. L'un des facteurs de production est constitué par le travail, ce qui représente la dimension sociale de la production du point de vue des théories économiques.Depuis les années 1970 environ, où sont apparus et se sont développés les mouvements écologistes, on se rend compte que la production, surtout industrielle, est grosse consommatrice de ressources naturelles, ce qui pose le problème de la rareté ou de l'épuisement de ces ressources, et qu'elle peut engendrer d'importantes pollutions. C'est pourquoi est apparue la notion de développement durable, qui combine deux aspects : ne pas abuser des ressources naturelles ; régler la production pour qu'elle ne détruise ni ne pollue l'environnement.Du point de vue biologique, tous les êtres vivants, végétaux comme animaux, sont des producteurs : ils produisent de la matière vivante en prélevant des éléments dans leur milieu de vie. L'animal comme le végétal produit sa propre matière à partir des aliments qu'il consomme. On distingue deux types de producteurs :les producteurs primaires : ce sont les végétaux verts qui contiennent de la chlorophylle grâce à laquelle en présence de lumière et uniquement à partir de matières minérales, ils fabriquent de la matière organique carbonée ;les producteurs secondaires : ce sont tous les autres êtres vivants qui fabriquent leurs substances organiques à partir de la matière d'un autre être vivant végétal ou animal.L'histoire de la production est marquée par deux grandes ruptures. La première est la Révolution  néolithique caractérisée par la transition de tribus de chasseurs-cueilleurs vers des communautés  d'agriculteurs. La première émergence eut lieu au Proche-Orient, il y a 5000 ans environ, où les hommes passèrent graduellement de la cueillette de céréales sauvages, à la production de plantes et d'animaux domestiqués. Les hommes ne se contentent plus de prendre ce que la nature leur offre, ils modifient radicalement leur environnement par des techniques agricoles nouvelles pour obtenir d'importants surplus de production. Une société sédentaire remplace progressivement les groupes nomades.La seconde rupture majeure, à partir du XVIIe siècle, est la Révolution industrielle qui transforme une société à dominante agraire et artisanale en une société commerciale et industrielle. Le caractère dominant de cette mutation est le passage de l'outil (prolongement de la force musculaire de l'ouvrier) à la Machine (dispositif autonome mû par une énergie naturelle), ce qui permet la mise en place de la production en série, c'est-à-dire d'une production de masse, production d'objets tous identiques à  grande échelle.Dans le cadre du capitalisme, la production est généralement conçue comme l'activité destinée à satisfaire non plus les besoins du producteur (autoconsommation), mais à être vendue sur le marché. Cette dernière est appelée « production marchande ». De plus, la vente n'est pas effectuée pour satisfaire les besoins jugés nécessaires ou urgents. Ceux-ci doivent être armés d'un pouvoir d'achat ; autrement dit, la production est destinée aux consommateurs qui sont capables de payer.Dans le second Discours, Jean-Jacques Rousseau cherche à cerner l'origine de la civilisation, qui est aussi selon lui l'origine du malheur de l'homme. Il affirme : « La métallurgie et l'agriculture furent les deux arts dont l'invention produisit cette grande révolution ». Au XIXe siècle les archéologues et les historiens ont parlé de ""révolution néolithique"" pour caractériser ""la période de la préhistoire marquée par l'émergence des premières sociétés agricoles sédentaires (...) qui ont éliminé, en quelques millénaires les sociétés de chasseurs-cueilleurs"", et qui ont installé ""une économie de la production"".C'est aussi au XIXe siècle que Karl Marx a élaboré une philosophie qui donne une grande importance à la production :d'une part, il en fait la base de la compréhension de l'homme : les hommes ""commencent à se distinguer des animaux dès qu'ils commencent à produire leurs moyens d'existence, pas en avant qui est la conséquence même de leur organisation corporelle"". La production n'est donc pas seulement une action économique; elle a un sens plus profond car elle est ""la façon dont les individus manifestent leur vie"". ""Ce qu'ils sont coïncide donc avec leur production"" (ibid.). À la question philosophique : ""qu'est-ce que l'homme ?"", Marx répond donc : ""l'homme c'est le monde de l'homme, l'État, la société"". Il faut donc dire que l'homme se produit lui-même dans l'histoire : ""par son activité historique, l'homme se donne une valeur humaine, il produit ses propriétés d'homme"", il se met en valeur. Marx écrivait : « Tout ce qu'on appelle l'histoire universelle n'est rien d'autre que l'engendrement de l'homme par le travail humain. »d'autre part, l'étude de la production fournit la base scientifique qui permet de comprendre la structure et l'évolution des sociétés humaines. C'est la théorie du matérialisme historique selon laquelle les rapports de production (relations entre les classes sociales) sont liés  aux forces productives (techniques, outillage et machines): ""les rapports sociaux sont intimement liés aux forces de production. En acquérant de nouvelles forces productives, les hommes changent leur mode de production et en changeant leur mode de production,ils changent la manière de gagner leur vie, ils changent tous leurs rapports sociaux"". Dans un fameux raccourci, il écrit : « Prenez le moulin à bras et vous aurez la société féodale avec le suzerain; prenez le moulin à vapeur et vous aurez la société avec le capitaliste industriel». Les rapports de production  sont d'abord en accord avec l'état de développement des forces productives, mais l'évolution de ces dernières finit par créer le besoin de nouveaux rapports de production, ""alors commence une ère de révolution sociale"" qui se conclut par l'apparition d'un nouveau mode de production et donc d'un nouveau type de société. L'histoire de l'humanité se définit par celle des modes de production. Il distingue les suivants : asiatique, antique, féodal et capitaliste auxquels devrait succéder le mode de production communiste débarrassé de la lutte entre les classes sociales qui a caractérisé les précédents.D'une manière plus générale, Michel Henry crédite Marx d'avoir pensé ""l'activité productive des hommes"" comme une praxis: ""C'est dans la pratique qu'il faut que l'homme prouve la vérité"". Selon Adolfo Sanchez-Vasquez, le concept de praxis signifie : ""activité orientée vers la transformation d'un objet (nature ou société) en tant que fin tracée par la subjectivité consciente et agissante des hommes et, par conséquent, activité objective et subjective à la fois"", et en ce sens, il s'oppose à toutes les philosophies précédentes, car comme le dit la XIe thèse des thèses sur Feuerbach : « Les philosophes n'ont fait qu'interpréter le monde de diverses manières, il s'agit maintenant de le transformer ».Biens et services marchandsCapital productifConsommationÉconomie post-industrielleEntrepriseEntreprise étendueFacteur de productionMatérialisme historiqueMode de productionProduction audiovisuelleProductivismeProductivité Portail de l’économie   Portail des entreprises   Portail de la philosophie   Portail de la sociologie   Portail de la production industrielle"
économie;En économie, le produit national brut (PNB) correspond à la production annuelle de richesses (valeur des biens et services créés, moins valeur des biens et services détruits ou transformés durant le processus de production) créées par un pays, que cette production se déroule sur le sol national ou à l'étranger.En comptabilité nationale française, le PNB n'est plus mesuré depuis 1993 : le choix a été fait de mesurer le revenu national brut, qui en est très proche. Le produit intérieur brut (PIB) est également beaucoup plus utilisé.Le PNB est la valeur totale de la production finale de richesses (valeur des biens et services créés - valeur des biens et services détruits ou transformés durant le processus de production) des acteurs économiques d'un pays donné au cours d'une année donnée. À la différence du PIB, il inclut les produits nets provenant de l'étranger, c'est-à-dire le revenu sur les investissements nets réalisés à l'étranger (cet élément étant négatif si les revenus des investissements de l'étranger sur le territoire national sont supérieurs aux revenus des investissements du pays à l'étranger).Le terme « national », dans « produit national brut », reflète ainsi la prise en compte de la valeur ajoutée produite par les résidents du pays en question (principe de nationalité) mais il n'est pas intérieur parce qu'une partie de cette valeur ajoutée est produite à l'étranger (le PIB est basé sur le principe de territorialité). Le PNB, de même que le PIB, inclut la TVA du pays, ce dont la légitimité est contestée.PNB = PIB + (revenus des facteurs du travail et du capital en provenance de l'extérieur - revenus des facteurs du capital et du travail versés à l'extérieur)PNB = PIB + revenus nets des facteurs versés au reste du mondeUne des questions pièges posées par des professeurs d'économie à leurs étudiants est la suivante : « Si l'on transforme un parking gratuit en parking payant, l'effet est-il d'augmenter, de diminuer ou de laisser inchangé le PNB du pays ? »La première réponse à laquelle on pense est que le PNB augmente en raison de l'apparition d'une nouvelle valorisation d'échanges. C'est oublier que les budgets ne sont pas élastiques et que ce qui est payé sur le parking par ses utilisateurs ne le sera pas par eux-mêmes ailleurs… mais le sera peut-être par les bénéficiaires de ces nouveaux revenus. Après analyse, la réponse est que le problème posé de cette façon ne permet pas de savoir si le PNB va augmenter, diminuer, ou éventuellement rester stationnaire.Sur le plan comptable, les échanges valorisés ne sont plus les mêmes. De nouveaux échanges apparaissent (le prix payé par les utilisateurs du parking), mais d'autres échanges s'en trouvent diminués et d'autres encore augmentés.Sur le plan économique, si on fait l'hypothèse d'un comportement similaire des bénéficiaires de ces nouveaux revenus, alors le PNB aura augmenté à hauteur de cette nouvelle valorisation. Si leur comportement est radicalement différent, alors l'analyse devient plus complexe : dans certains cas, le PNB étudié diminue : délocalisation du PNB local vers un autre PNB (rémunération de capitaux étrangers), investissements moins productifs, ou encore dépenses dans des cycles d'échanges plus lents.Les indices d'évolution des prix, (sur lesquels reposent par exemple les taux d'inflation) sont censés tenir compte des augmentations de prix, mais ce n'est pas toujours possible. Ainsi, le parking nouvellement payant fait probablement nouvellement l'objet d'une surveillance, d'une organisation, d'un nettoyage, d'une publicité, d'une signalisation, toutes choses qui contribuent à sa valeur et ne peuvent être distinguées de façon certaine d'une vulgaire augmentation de prix.Revenu national brutRevenu par têteProduit intérieur brut (PIB)Indicateur économique Portail de l’économie
économie;Le revenu national (RN) au prix du marché représente l’ensemble des revenus primaires reçus par les différents secteurs institutionnels du pays.Le revenu national = Produit intérieur brut + (Revenu Net du travail, de propriété, des entreprises reçues du reste du monde) - la consommation de capital fixe - les impôts (net de subvention versé aux institutions de l'Union européenne).Le revenu national est le revenu perçu par les agents économiques nationaux du fait de leur participation à l'activité de production.Produit intérieur brutProduit national brut Portail de l’économie
économie;"En économie, le secteur primaire est le premier secteur défini dans la loi des trois secteurs. Il est parmi les trois secteurs économiques définis dans la comptabilité nationale et regroupe les activités liées à l’exploitation de ressources naturelles : agriculture, sylviculture, pêche et activités minières. Le secteur primaire rassemble l'ensemble des activités qui produisent des matières premières non transformées.Le secteur primaire comprend l'agriculture, la pêche, l'exploitation forestière et l'exploitation minière. On désigne parfois les trois dernières par le terme « autres industries primaires ». Les industries primaires sont liées à l'extraction des ressources de la terre.Selon Fortune, le secteur de l’extraction représenterait 27 % de l'économie mondiale comprenant notamment les activités relatives à l’énergie ou les minières. Les vingt plus gros négociants du secteur ont engrangé $191 milliards de profit entre 2003 et 2012.C'est un processus par lequel les gens aménagent leurs écosystèmes pour satisfaire les besoins de leurs sociétés. Elle désigne l’ensemble des savoir-faire et activités ayant pour objet la culture des terres, et, plus généralement, l’ensemble des travaux sur le milieu naturel (pas seulement terrestre) permettant de cultiver et prélever des êtres vivants (végétaux, animaux, voire champignons ou microbes) utiles à l’être humain. En France En 1700, il fallait environ trois heures pour produire un kilogramme de blé, d'où la malnutrition et les famines. À cette période, 80 % de la population active travaillait dans l'agriculture; en 1880, il fallait encore un peu plus d'une heure ; en 1950, 30 minutes. Aujourd'hui, environ une minute. Cela explique l'évolution de la part de l'agriculture : en 1995, l'agriculture représentait en France 6 % de part de la population active ayant un emploi, contre 40 % en 1913[réf. souhaitée]. En 2008, l'agriculture en France pesait 3,5 % du PIB (2008), soit 66,8 milliards d'euros. En 2012, elle ne serait plus que de 2 % du PIB français. En Belgique En 1846, les cultivateurs représentent encore 52 % de la population économiquement active :en 1880, 22 % ;en 1913, 16 % ;actuellement 2 %.En 1846, intervient encore pour plus de 50 % dans le PNB :en 1880, 29 % ;en 1913, 15 % ;actuellement, 0,7 %. En Europe L'emploi dans le secteur agricole est en forte régression pour l'amont de la filière (agriculture) depuis plus d'un siècle, et dans l'UE27 il a encore diminué au XXIe siècle sous l'effet de l'industrialisation et de l'augmentation de la productivité ; selon Eurostat. Ceci correspond à la perte de 3,7 millions d’emplois à temps plein en 10 ans. L'emploi a ainsi baissé de 17 % dans l’UE152 et de 31 % dans les 12 États-membres (NEM122) ayant rejoint l’UE en 2004 et en 2007. En 2009, le secteur de l’agriculture employait dans l’UE27 l’équivalent de 11,2 millions de personnes travaillant à temps plein, dont 5,4 millions dans l’UE15 et 5,8 millions dans les NEM12. Dans le même temps (2000 → 2009), le revenu réel moyen par actif a augmenté de 5 % (il a même doublé en Lettonie, Estonie et Pologne) de 2000 à 2009. Dans le monde En 2011, la production agricole mondiale était estimée à 4 949 milliards, soit 6,2 % de l'économie mondiale.Dans le secteur primaire, la section des mines est définie comme l'exploitation de différents roches ou minéraux.Secteur économiqueSecteur secondaireSecteur tertiaireSecteur quaternaire Portail de l’économie"
économie;"En économie, le secteur secondaire ou secteur industriel est le second secteur défini dans la loi des trois secteurs. Il est parmi les trois secteurs économiques définis dans la comptabilité nationale et regroupe les activités liées à la transformation des matières premières issues du secteur primaire (industrie manufacturière, construction)Ce secteur, même s’il représente une part relativement modeste du PIB des pays développés (par exemple 13,2 % aux États-Unis en 2006, 20,6 % en France en 2006 et 26,3 % en Suisse en 2005), est considéré comme stratégique ; il fournit des emplois d’ingénieur et fournit du travail de recherche et développement à des entreprises du secteur tertiaire.Selon la CIA, le secteur industriel représentait 30,7 % de l'économie mondiale en 2012. Mais, selon Fortune, le secteur industriel représenterait 13,2 % de l'économie mondiale en 2012 si l'on intègre les activités d'extraction au secteur primaire.Secteur économiqueIndustrieSociété industrielleSecteur secondaire en FranceLe secteur tertiaireSecteur quaternaire Portail de la production industrielle   Portail de l’économie"
économie;"Le secteur tertiaire produit des services, il fait partie du domaine de l'économie. C'est le troisième secteur défini dans la loi des trois secteurs. Il est parmi les trois secteurs économiques définis dans la comptabilité nationale et est de fait défini par complémentarité avec les activités agricoles et industrielles (secteurs primaire et secondaire respectivement).Le secteur tertiaire est composé du :tertiaire principalement marchand (commerce, transports, activités financières, services rendus aux entreprises, services rendus aux particuliers, hébergement-restauration, immobilier, information-communication) ;tertiaire principalement non marchand (administration publique, enseignement, santé humaine, action sociale).Dans les pays développés, c’est de loin le secteur le plus important en nombre d'actifs occupés. En 2012, le secteur tertiaire représentait près de 60 % de l'économie mondiale.La « tertiarisation » de l’économie pose des problèmes statistiques, conceptuels et méthodologiques : les notions de volume, de qualité et de productivité du travail sont probablement à revoir dans le « tertiaire moderne ». L'objet des études futures sera aussi d'examiner si les trois critères d'homogénéité (une part croissante de l'emploi, une relative insensibilité aux crises économiques, et surtout un progrès technique faible), sont aujourd'hui respectés dans un ensemble qui comprend 79 %[réf. nécessaire] de la population active française.Le secteur tertiaire de l'économie, généralement appelé secteur des services, est le troisième des trois secteurs économiques de la théorie des trois secteurs. Les autres sont le secteur secondaire (à peu près identique à celui de la fabrication) et le secteur primaire (matières premières).Le secteur des services consiste en la production de services au lieu de produits finis (en) . Les services (également appelés «biens immatériels») comprennent les soins, les conseils, l'accès, l'expérience et le travail affectif. La production d'informations a longtemps été considérée comme un service, mais certains économistes l'attribuent désormais à un quatrième secteur, le secteur quaternaire .Le secteur tertiaire de l'industrie implique la fourniture de services à d'autres entreprises ainsi qu'aux consommateurs finaux. Les services peuvent impliquer le transport, la distribution (en) et la vente de marchandises du producteur au consommateur, comme cela peut se produire dans la vente en gros et au détail, la lutte contre les ravageurs ou le divertissement. Les produits peuvent être transformés au cours du processus de fourniture du service, comme cela se produit dans l'industrie de la restauration. Cependant, l'accent est mis sur les gens en interagissant avec les gens et servant le client plutôt que des transformations les biens physiques.Il est parfois difficile de définir si une entreprise donnée fait partie intégrante du secteur secondaire ou tertiaire. Et ce ne sont pas seulement les entreprises qui ont été classées comme faisant partie de ce secteur dans certains régimes; le gouvernement et ses services tels que la police ou l'armée, et les organisations à but non lucratif telles que les organismes de bienfaisance et les associations de recherche peuvent également être considérés comme faisant partie de ce secteur.Afin de classer une entreprise en tant que service, on peut utiliser des systèmes de classification tels que la norme de classification industrielle standard internationale des Nations Unies, le système de codes de la classification industrielle standard (SIC) des États-Unis et son nouveau système de remplacement, le système de classification des industries de l'Amérique du Nord. (SCIAN), la nomenclature statistique des activités économiques dans la Communauté européenne (NACE) dans l'UE et des systèmes similaires ailleurs. Ces systèmes de classification gouvernementaux ont un premier niveau de hiérarchie qui indique si les biens économiques sont tangibles ou intangibles.Aux fins de la finance et des études de marché, des systèmes de classification basés sur le marché tels que le Global Industry Classification Standard et l'Industry Classification Benchmark (en) sont utilisés pour classer les entreprises qui participent au secteur des services. Contrairement aux systèmes de classification gouvernementaux, le premier niveau des systèmes de classification basés sur le marché divise l'économie en marchés ou industries fonctionnellement liés. Le deuxième ou le troisième niveau de ces hiérarchies indique alors si des biens ou des services sont produits.Les groupes sociaux non productifs les plus anciens sont les propriétaires fonciers, marchands, militaires et le clergés, etc. Ils établissent leur assise sur le foncier, moyen fondamental de production, sur la régulation, le contrôle et la commercialisation de ses produits, les derniers sur le spirituel, « l'établissement d'une relation au cosmos à travers les religionse »:« Les activités de services sont nées à partir du moment où les activités de survie de l'espèce humaine ont été assurées de façon suffisante pour permettre de dégager un surplus, même temporaire, susceptible d'être attribué à d'autres fonctions et, conjointement, à d'autres individus ou groupes humains, qui ont ainsi acquis la possibilité de s'abstraire, partiellement ou en totalité, des contraintes de la production. En fait, il y a une liaison directe entre le développement d'activités non directement productives et l'organisation d'une société permettant l'appropriation de ces activités par certaines classes sociales. »L'assise du pouvoir de ces classes sociales est toujours justifiée par le « service rendu », et par l'extraction d'une fonction confiée à un corps de « spécialistes », détenteurs d'une compétence qui est de fait déniée aux autres groupes sociaux. Les activités non directement productives se font dans une hiérarchisation croissante.Au cours des 100 dernières années, il y a eu un glissement substantiel des secteurs primaire et secondaire vers le secteur tertiaire dans les pays industrialisés. Ce changement s'appelle la tertiarisation. Le secteur tertiaire est désormais le plus grand secteur de l'économie du monde occidental, et c'est aussi le secteur qui connaît la croissance la plus rapide. En examinant la croissance du secteur des services au début des années 90, le mondialiste Ken'ichi Ohmae a noté le secteur tertiaire représenterait « 70 % de la force de travail aux États-Unis, 60 % au Japon et 50 % à Taïwan »:« In the United States 70 percent of the workforce works in the service sector; in Japan, 60 percent, and in Taiwan, 50 percent. These are not necessarily busboys and live-in maids. Many of them are in the professional category. They are earning as much as manufacturing workers, and often more. »Les économies ont tendance à suivre une progression de développement qui les fait passer d'une forte dépendance à l'agriculture et à l'exploitation minière au développement de l'industrie manufacturière (par exemple, automobiles, textiles, construction navale, acier) et finalement à une structure davantage axée sur les services. La première économie à suivre cette voie dans le monde moderne a été le Royaume-Uni. La vitesse à laquelle d'autres économies ont fait la transition vers des économies de services (ou « post-industrielles ») a augmenté avec le temps.Historiquement, le secteur manufacturier avait tendance à être plus ouvert au commerce international et à la concurrence que les services. Cependant, avec une réduction spectaculaire des coûts et des améliorations de la vitesse et de la fiabilité dans le transport des personnes et la communication de l'information, le secteur des services comprend désormais une partie de la concurrence internationale la plus intense, malgré un protectionnisme résiduel.Vous trouverez ci-dessous une liste de pays par production de services aux taux de change du marché en 2016:Les prestataires de services sont confrontés à des obstacles dans la vente de services auxquels les vendeurs de biens sont rarement confrontés. Les services sont intangibles, ce qui rend difficile pour les clients potentiels de comprendre ce qu'ils recevront et quelle valeur cela représentera pour eux. En effet, certains, comme les consultants et les prestataires de services d'investissement, n'offrent aucune garantie de la valeur pour le prix payé.Étant donné que la qualité de la plupart des services dépend en grande partie de la qualité des personnes qui fournissent les services, les « coûts de personnel » représentent généralement une fraction élevée des coûts des services. Alors qu'un fabricant peut utiliser la technologie, la simplification et d'autres techniques pour réduire le coût des produits vendus, le fournisseur de services est souvent confronté à un schéma constant d'augmentation des coûts.La différenciation des produits est souvent difficile. Par exemple, comment peut-on choisir un conseiller en placement plutôt qu'un autre, étant donné qu'ils sont souvent perçus comme fournissant des services identiques ? La facturation d'une prime pour les services n'est généralement une option que pour les entreprises les plus établies, qui facturent des frais supplémentaires en fonction de la reconnaissance de la marque.Des exemples d'industries tertiaires peuvent inclure:TélécommunicationHôtellerie TourismeMédias de masseSoins de santé/hôpitauxSanté publiquePharmacieTechnologie de l'informationGestion des déchetsConsultantJeu d'argentCommerce de détailBien de grande consommation (FMCG)FranchisageImmobilierÉducationServices financiersBanqueAssuranceInvestment management (en)Services professionnels (en)Services juridiques (en)Conseil en managementTransportÉducationAprès s'être développé jusqu'en 1960 selon un rythme annuel moyen de 1 % en France, l'emploi des branches tertiaires progresse très vivement de 1960 à 1980 (2 % par an), puis encore assez fortement de 1980 à 2000 (+1,7 %). Il ralentit ensuite entre 2000 et 2011 (+0,9 %), avec une quasi-stagnation de 2008 à 2011, voire une légère baisse dans certains services traditionnels aux ménages ou les télécommunications. Ceci est le résultat de deux tendances : une accélération de la croissance de la demande intérieure tertiaire (+ 4,3 % par an en volume entre 1959 et 2012) et des gains de productivité du travail plus faibles que dans le reste de l'économie (+ 2,5 % par an dans les services marchands contre + 4,5 % dans l’industrie). Face à une demande croissante, un secteur dont la productivité progresse relativement plus lentement ne peut que se développer en terme d'emploi. Depuis le début du siècle dernier, le progrès technique a toujours été plus faible dans le tertiaire que dans les autres secteurs. Expliquer « l'explosion » récente de l'emploi tertiaire revient donc à analyser les raisons de l'accélération de la demande tertiaireLa balance commerciale des services est excédentaire en France. Finance On parle dans ce cas là, des garages de voitures ou encore des magasins de ventes qui comprend en particulier le secteur bancaire et celui de l'assurance au point d'être parfois désigné par « Secteur Banque Assurance ». Information  Sécurité Cette branche du secteur tertiaire comprend notamment les activités de police, de milice et d'armée assurant la sécurité des biens et des personnes. Justice Aux États-Unis, 2 % du PIB est obtenu en justice. Bénévolat Selon une étude de l’Insee parue en 2004, le bénévolat représentait aux alentours de 1 point de PIB.Le tertiaire « supérieur » (ou mixte) regroupe les « métiers du savoir » qui fournissent aux entreprises et aux particuliers des prestations intellectuelles complexes. Essentiels au fonctionnement de l’économie et au développement stratégique des entreprises, élément clef du rayonnement et de l’attractivité du territoire, ces métiers constituent également par eux-mêmes un secteur économique majeur, en fort développement. Recherche et éducation Le secteur tertiaire, privé et public, via ses infrastructures (bâtiments tels que bureaux, hôtels, commerces, d'enseignement et les bâtiments administratifs, réseaux internet, serveurs..) est devenu un grand consommateur d'énergie et de foncier ; En France, à la suite de la loi Grenelle 2 de 2010, un décret annoncé pour fin 2012 mais un temps repoussé par le Conseil d’État avant d'être publié en mai 2017 et entrant en vigueur le 11 mai 2017, afin de « favoriser l'efficacité et la sobriété énergétiques », impose aux « collectivités territoriales services de l'Etat, propriétaires et occupants de bâtiments à usage tertiaire privé, professionnels du bâtiment, maîtres d'ouvrage, maîtres d'œuvre, bureaux d'études thermiques, sociétés d'exploitation, gestionnaires immobiliers, fournisseurs d'énergies » une « obligation d'amélioration de la performance énergétique dans les bâtiments à usage tertiaire », sur la base d'une « étude énergétique portant sur tous les postes de consommations du bâtiment », avec un niveau d'économie d'énergie à atteindre avant 2020 et « un ou plusieurs scénarios permettant de diminuer, d'ici 2030 » la consommation d'énergie. Un observatoire doit recueillir les données permettant d'évaluer les résultats et de mettre à jour les données. L'étude énergétique est accompagnée de propositions de travaux d'économie d'énergie et de « recommandations hiérarchisées selon leur temps de retour sur investissement » avec présentation des « interactions potentielles entre ces travaux ». Ce décret permet de mutualiser l'obligation sur l'ensemble d'un patrimoine, et prévoit le cas d'un changement de propriétaire ou de preneur (un dossier dédié sera annexé au contrat de vente ou de bail). Il demande aussi une sensibilisation des occupants au thème des économies d'énergie. La « non-atteinte » des objectifs malgré les actions et travaux entrepris devra pouvoir être justifiée auprès des services de l’État. Un propriétaire d'un ensemble de bâtiments ou de parties de bâtiments concernés peut remplir globalement ses obligations sur l'ensemble de son patrimoine.Michel Braibant, De la désindustrialisation à la tertiairisation, vers un mélange des genres, Paris, Société des écrivains, 2 mai 2015, 188 p. (ISBN 9782342034738).Secteur économiqueBranche d'activitéSecteur quaternaireTertiarisation du travailSecteur bénévoleSociété post-industrielleEffet de structure de fret Portail de l’économie"
économie;"Un système économique est le mode d’organisation de l'activité économique d’une société ou d'une aire géographique donnée, qui détermine la production, la consommation, l'utilisation des ressources, etc. Dans un sens élargi, il peut être également compris comme l'organisation sociale induite par le système. Le système économique influence de nombreux facteurs, comme le niveau de vie des habitants, le niveau des inégalités, les relations avec les autres pays, ou la puissance économique.Un système économique est un mode d'organisation de la société analysé par le prisme économique. Un tel système regroupe l'ensemble des institutions, sociales comme normatives, qui agissent comme une courroie ou un récipiendaire de l'activité économique au niveau micro. Le système forme le niveau macroéconomique de la vie du pays. Les normes du système (via le droit du pays) détermine également la répartition des richesses.Ce système n'est pas neutre économiquement, dans le sens où ses structures conditionnent l'affectation des ressources et la productivité (production unitaire d'un facteur de production utilisé dans la combinaison productive) des facteurs. Sa modulation joue donc un rôle essentiel dans le cadre des politiques publiques visant à générer de la croissance ou à modifier le niveau d'inégalités.Les systèmes économiques varient en fonction des régions et des époques. Les pays occidentaux suivent une organisation fondée sur le capitalisme caractérisé par la propriété privée des moyens de production. Le système économique des pays de l'ancien bloc de l'Est était fondé sur le principe de l’économie communiste dont l'objectif est la propriété collective des moyens de production.Un système économique est lui-même composé d'une multitude de sous-systèmes, plus petits. Les agents qui le composent peuvent y être spécialisés : le système économique vigneron dispose de spécificités face au système économique de l'agroalimentaire auquel il appartient, qui, lui-même, n'est qu'un sous-système d'un ensemble plus grand.Du fait des combinaisons politiques et des modalités que peuvent prendre les institutions, il n'y a pas un seul modèle de système économique. Dans Les trois mondes de l'Etat-providence, Gøsta Esping-Andersen identifie trois types de systèmes économiques, correspondant à un système continental, un système nordique social, et un système anglo-saxon. Des recherches ultérieures ont identifié des systèmes économiques arabo-musulmans et asiatiques.Raymond Aron transpose par ailleurs le concept à la société internationale, en écrivant : « les États, par leurs politiques, contribuent à former le système économique mais celui-ci, inégalement déterminé par les États selon les pesanteurs de chacun d'eux, constitue un système différent du système interétatique, qu'on devrait plutôt qualifier de transnational que d'interétatique ».Un système économique est composé par des relations entre ses acteurs. Pierre-Joseph Proudhon note ainsi en 1850 que le système économique capitaliste d'Europe de l'Ouest est dominé par la figure du travailleur et celle du patron. Les relations entre les agents sont modulées par les politiques publiques, qui peuvent par exemple décider d'un niveau de centralisation plus ou moins élevé de l'activité économique.Le passage à un mode de développement durable suppose d'évoluer vers un nouveau modèle économique. Un système économique peut être refusé par certains de ses agents, voire remplacé. Le communisme est ainsi un système économique concurrent au système économique capitaliste. Un système économique se fond dans les institutions sociales ; dès lors, le changement de système économique requiert une modification desdites institutions.Le concept de souveraineté économique permet de qualifier un système économique dont les principales sources d'approvisionnement de matériaux et de biens stratégiques sont endogènes au système. Un système économique peut, à l'extrême, être autarcique, c'est-à-dire fonctionner sans relations avec d'autres systèmes économiques.Certains auteurs comme Jean-Marie Albertini constatent que le mot système ou modèle est utilisé par les économistes pour déterminer les différences théoriques essentielles entre les pays appartenant au même système (Japon et États-Unis, par exemple) ou, a fortiori entre des pays qui appartiennent à des systèmes économiques et sociaux totalement opposés (URSS et États-Unis lors de l'époque de la Guerre froide). Toutefois, ce mot ne permet pas de décrire l'évolution historique et réelle d'un pays ou d'un système. Pour cela, Jean-Marie Albertini propose d'utiliser le mot « régime ».Développement économique et social Portail de l’économie"
économie;"L'organisation industrielle (ou économie industrielle ou Concurrence imparfaite et industrial organization  en anglais) est la branche de la microéconomie qui étudie le fonctionnement des marchés et les comportements des entreprises sur ces marchés. Elle traite notamment des situations dans lesquelles les entreprises disposent d'un pouvoir de marché, ce que les économistes appellent la concurrence imparfaite. Elle ne se réduit toutefois pas à l'analyse de la concurrence imparfaite. Un de ses objectifs est d'évaluer la performance des marchés en matière d'efficacité et de bien-être collectif. À cet égard, l'économie industrielle comporte une dimension importante d'aide à la décision publique, pour tout ce qui touche à la régulation des marchés.L’économie industrielle a pour objet l’étude de « l’organisation et du fonctionnement des entreprises et des marchés dans le monde réel » (Médan et al, 2000). L'organisation industrielle tire l'essentiel de ses outils de la microéconomie et de la théorie des jeux.Les questions posées par l'organisation industrielle visent à ouvrir un certain nombre de boîtes noires de la microéconomie néoclassique. Elle se demande ainsi pourquoi il existe des entreprises (plutôt qu'un monde de travailleurs indépendants) ?pourquoi la taille et la structure varient en fonction des produits, des marchés et du temps ?pourquoi la prédiction du prix au coût marginal n'est que très rarement vérifiée ?Dans ce cadre, l'organisation industrielle a absorbé l'étude des monopoles et des oligopoles, ainsi que la problématique schumpetérienne du lien entre la capacité d'extraire des profits et la capacité à supporter des dépenses liées à la recherche et à l'innovation. Elle s'interroge ainsi également sur les raisons de la diversité des biens et par voie de conséquence à la dynamique de l'innovation.En 1988, Richard Schmalensee définit ainsi l'économie industrielle par trois thèmes essentiels :L'étude des déterminants du comportement, de la taille, de l'échelle et de l'organisation des entreprises privées ;La concurrence imparfaite, c'est-à-dire dans quelle mesure le fonctionnement et la performance du marché (en tant que moyen d'allocation des ressources entre agents) est affecté lorsque les conditions de la concurrence pure et parfaite (CPP) ne sont pas respectées ? Ce thème couvre en particulier les questions de choix de prix, de quantité et de capacité, ainsi que la concurrence hors-prix : sélection des produits, publicité, changement technique.L'étude des politiques publiques concernant l'activité économique, en particulier en matière de droit de la concurrence, de dérégulation, et de privatisations, ainsi que des politiques industrielles affectant le progrès technique.Dans son ouvrage Théorie de l’organisation industrielle, Jean Tirole souligne la difficulté empirique qu’il y a à définir un marché en écrivant que « la notion de marché est loin d’être simple,… [si] la définition d’un marché ne saurait être trop étroite, la définition ne doit pas être [non plus] trop large. La « bonne » définition dépend de l’usage qui en sera fait. Il n’y a pas de recette facile pour définir un marché ». Au-delà de cette plaisante réflexion, l’idée est de décrire une norme (que l’on pourrait présenter comme un idéal-type au sens de Max Weber) pour examiner, ensuite, comment et pourquoi la réalité s’écarte de cette référence. Au sens économique, un marché est le lieu où se rencontrent une offre et une demande, où s’établissent des contrats (qui peuvent porter sur les quantités, la qualité ou le prix) et où se concluent des échanges.En économie industrielle, il convient aussi, pour rester pertinent, de définir « l’étendu d’un produit », c’est-à-dire ses caractéristiques qui font que les biens entre eux ne sont pas parfaitement substituables mais similaires.  Par exemple définir, selon les cas, si une chemise et un tee-shirt sont similaires (ce sera alors le même marché) ou s’il faut considérer deux marchés distincts.En pratique, il faut s’intéresser à l’organisation de la production qui va influer sur la nature du marché. En raison des spécificités sectorielles, il s’agit aussi souvent d’adopter une approche au cas par cas pour étudier la concurrence. L’organisation industrielle s’intéresse donc à des équilibres partiels et non à l’équilibre général, cher aux macroéconomistes.L’économie industrielle s’intéresse aux interactions entre les différents acteurs du marché (i.e., entreprises, consommateurs et l'Etat).  L'entreprise Selon Jean Tirole, « une entreprise doit être capable de produire (ou vendre) plus efficacement que ne le pourraient ses parties constituantes en agissant séparément ». Une entreprise doit optimiser en permanence pour maximiser son profit (ou minimiser ses pertes), mettre en œuvre différentes combinaisons d’activité, s’adapter à son environnement. Ce comportement peut l’amener à rechercher un pouvoir de marché allant jusqu’au monopole, par l'élimination progressive de ses concurrents, ou à profiter des interactions avec les autres producteurs pour s’inscrire dans un oligopole. Le profit étant défini comme l’écart entre le chiffre d’affaires et les coûts engagés (fixes et variables), sa maximisation suppose de minimiser les coûts sous la contrainte de la fonction de production (pour vendre, il faut produire un bien ou un service). La fonction de coût d’une entreprise est la donnée clé de l’économie industrielle. L'entreprise comme processus de production Dans l'analyse microéconomique traditionnelle, ou standard, l'entreprise est abordée au travers de ses caractéristiques techniques. Elle est une organisation dont le but est de produire certains biens ou services. Pour produire ces biens, elle combine des facteurs de production, tels que la force de travail, le capital matériel (locaux, machines, etc.) et immatériel (savoir-faire, connaissance, etc.), des matières premières ou des biens intermédiaires. Les contraintes techniques de l'entreprise sont représentées par une fonction de production qui détermine les niveaux de production accessibles pour différentes combinaisons des facteurs de production.  L'entreprise comme organisation L'entreprise remplit deux fonctions.Elle est une instance de coordination du processus de production. Dans La Richesse des nations (1776), Adam Smith présente la firme moderne comme une réponse à la complexité croissante des activités, en particulier à la division du travail.Elle repartit la valeur créée entre les parties impliquées dans le processus, ainsi que les risques liés aux aléas de la production.Cependant, cela n'explique pas la forme particulière d'organisation qu'est la firme et pourquoi ces deux fonctions doivent être remplies au sein de celle-ci. Comme le souligne Ronald Coase dès 1937, la nature fondamentale des échanges qui s'opèrent au sein d'une entreprise n'est pas différente de celle des échanges qui s'opèrent sur les marchés. Telle entreprise peut faire effectuer la même opération en interne ou, en externe, en faisant appel à un sous-traitant. Seul le mode de transaction change. Selon Coase, la question est donc de comprendre pourquoi tel type d'échange se fait au sein d'une entreprise, et tel autre sur les marchés. Il fonde alors son approche sur le fait que tout échange implique des coûts de transaction (principalement des coûts de recherche du meilleur partenaire et de négociation des contrats), L'échange se fera au sein d'une entreprise si le coût de transaction (i.e., en externe) de cette opération est moins élevé dans l'entreprise que sur le marché. Les travaux de Coase ont eu une profonde influence sur l'analyse moderne de l'entreprise, telle qu'elle apparaît, dès le milieu des années 1970, en particulier dans les travaux d'Oliver Williamson qui a cherché à définir la nature de ces coûts de transaction.Au-delà des coûts directs (la négociation des échanges par exemple), cette notion de coûts englobe tous les facteurs qui limitent les parties dans leur capacité d'améliorer l'efficacité des échanges. Une des sources d'inefficacité, mise en avant par Herbert Simon (1976), provient des limites aux capacités cognitives des individus, la rationalité limitée (par opposition à la rationalité illimitée chère aux économistes) des acteurs. Une deuxième limitation provient de ce que Williamson appelle l'opportunisme des acteurs. Une entreprise est une organisation mettant en présence des individus aux intérêts multiples et parfois contradictoires. Les objectifs des cadres dirigeants (managers, en anglais), des propriétaires et des employés sont différents. Dans un tel contexte, les divers acteurs ne sont enclins à révéler l'information dont ils disposent et à agir dans le sens de l'intérêt général que si cela sert leur objectif propre. La bonne marche de la coopération nécessite la mise en place de mécanismes visant à limiter les comportements opportunistes, c'est-à-dire à fournir de bonnes incitations. Et cela induit nécessairement une certaine perte d'efficacité.Le système de prix constitue le mécanisme utilisé par le marché tandis que l'entreprise se fonde sur une forme d'organisation plus hiérarchisée. À cet égard, la firme est vue comme un ensemble de contrats liant les parties dans le but de mettre en place la production.  Le consommateur La fonction de demande reflète le comportement du consommateur qui, en tenant compte de sa contrainte budgétaire, maximise son utilité. Ce comportement a un effet sur le prix d’équilibre et donc sur le chiffre d’affaires de l’entreprise. La fonction de demande est déterminée par un « prix de réserve » au-dessus duquel le consommateur a décidé qu’il n’achèterait pas. Si ce prix est proposé, le consommateur A achète ; si le prix est inférieur A et B, qui a un « prix de réserve » inférieur, achètent et A voit sa satisfaction (son bien-être) augmenter. Plus le prix est bas, plus nombreux et contents sont les consommateurs. Le « surplus social » augmente. Les pouvoirs publics En économie industrielle, les pouvoirs publics mettent en place des politiques de règlementation ou de régulation pour corriger les imperfections de marché.L'organisation industrielle étudie le pouvoir de marché des entreprises qui peut découler des différentes structures de marché et des interventions publiques mises en oeuvre pour limiter la capacité des entreprises à abuser de leur pouvoir de marché. Dans une première approche, la taille et le nombre d'entreprises dans un secteur d'activité reflètent l'ampleur des économies d'échelle, terme utilisé pour indiquer le lien entre le niveau global de la production d'une entreprise et son coût unitaire (coût total rapporté au volume de la production). On dit qu'il y a des économies d'échelle si le coût unitaire baisse lorsque la production totale augmente. Dans ce cas, il est plus efficace de concentrer la production sur un nombre restreint d'entreprises que de la répartir entre de nombreuses entreprises produisant peu, puisque ainsi on réduit la dépense par unité produite.Tout démarrage d'une nouvelle activité industrielle nécessite des dépenses initiales spécifiques : recherche et développement, mise en place des capacités de production et du circuit de distribution, information des consommateurs et construction d'une image de marque. Le concept de coûts d'entrée regroupe ces dépenses. À la différence des coûts fixes, les coûts d'entrée ne sont engagés qu'une fois. Lorsqu'elle envisage d'entrer sur un marché, une entreprise doit évaluer si les perspectives de profit futur justifient l'engagement de ces coûts. Cela signifie que si les coûts d'entrée sont élevés, il y aura peu d'entreprises actives sur le marché.L'intervention des pouvoirs publics peut aussi freiner l'entrée sur un marché en bloquant par des barrières à l'entrée les nouveaux entrants ou les produits substituables au sens de M. Porter. Ces barrières peuvent être tarifaires (impôts et taxes, droits de douane) ou non tarifaires (quotas, normes). Dans ce cas, elle a un coût social (prix élevés, qualité insuffisante, etc.) qu'il convient de mettre en balance avec les justifications de l'intervention, qu'elles soient d'ordre économique, social, juridique ou prudentiel.Voir monopole pour l'article détaillé.Les oligopoles s’observent en pratique dans de nombreux secteurs d’activité comme la construction automobile ou l’industrie du tabac. Dans la plupart des cas, un oligopole est non-coopératif et chaque entreprise prend ses décisions (sur le prix ou sur les quantités) en fonction de ce qu’elle suppose être le comportement des autres membres de l’oligopole. En langage économique, il s’agit de faire des hypothèses sur la fonction de réaction des concurrents. Cette étape vient s’ajouter à la connaissance de la fonction de demande des consommateurs. Voir oligopole pour l'article détaillé.Les biens offerts sur un même marché sont rarement homogènes, rigoureusement interchangeables. Leurs différences résultent d'une politique de production délibérée. Les offreurs souhaitent souvent se distinguer les uns des autres pour capter une clientèle, la fidéliser. Et les concurrents vantent leurs produits en matière de qualité et de performances d'utilisation pour conquérir et défendre des micro-monopoles.La différenciation du bien est verticale si, à prix égal, les acheteurs adressent unanimement leurs demandes au bien de qualité présumée supérieure. La qualité du bien est, en ce cas, synonyme d'excellence. La différenciation est horizontale si, à prix égal, les acheteurs se répartissent entre les différentes versions du produit, en fonction de leurs qualités respectives et des goûts personnels des consommateurs. Dans ce cas, la qualité définit une étroite conformité aux attentes de la clientèle visée.Lorsqu'il existe des barrières technologiques à l'entrée, les firmes en place sur un marché peuvent en plus développer des stratégies industrielles qui dissuadent les concurrents en puissance de venir sur le marché. Puisqu'une entreprise n'entre sur un marché que si elle anticipe des profits suffisants pour compenser ses coûts d'entrée, toute stratégie qui permet de réduire les profits futurs des concurrents permet de barrer l'entrée. Dans ce contexte, on dit que la firme en place met en place des barrières stratégiques à l'entrée. Ces stratégies soulèvent deux difficultés : elles doivent d'abord être crédibles, et elles ne doivent pas être trop coûteuses (pour la firme en place).On parle de comportement de prédation lorsqu'une entreprise « agresse » des concurrents afin de les évincer du marché ou tout au moins de les fragiliser pour accroître son pouvoir de marché. Les stratégies de surinvestissement déjà discutées peuvent être adoptées dans ce but. Une alternative consiste à baisser fortement ses prix de façon à diminuer la rentabilité des concurrents. Les relations verticales entre entreprises fait également l'objet de travaux en économie industrielle. Le problème de double marginalisation est une illustration.Les régulateurs cherchent à contrôler le comportement des firmes disposant d’un pouvoir de marché (monopoles, monopoles naturels, oligopoles). Il s’agit d’aligner l’objectif de la firme et l’intérêt collectif. La tâche est rendue délicate du fait de l’asymétrie d’information entre régulateur et régulé, lequel par essence connaît mieux sa propre situation (fonction de coûts, technologie utilisable pour abaisser les coûts, effort de réduction des coûts…) que le régulateur. C’est une information privée qu’il n’a pas toujours intérêt à révéler.Laffont et Tirole (1986) ont construit un modèle qui montre que le problème peut être résolu par l’offre d’un menu de contrats. En choisissant un contrat, le régulé révèle son information privée (auto révélation). Dans la réalité cela se concrétise à l’occasion de processus de négociation entre régulateur et régulé (généralement pas explicitement par un menu de contrats). Après avoir rappelé la nature et les conséquences essentielles de l’asymétrie d’information, on examinera la logique qui préside à l’offre d’un menu de contrats pour enfin présenter brièvement les principes du modèle fondamental de Laffont et Tirole.Les relations étudiées supposent un déficit informationnel chez l'autorité (le principal). Généralement, l'opérateur (l'agent) dispose d'une connaissance privée de la technologie utilisée, de l'état des coûts d'exploitation ou de la demande du marché. La nouvelle théorie de la régulation propose donc de considérer systématiquement les rapports entre régulateur et opérateur à travers le cadre normatif principal-agent. S'inspirant des techniques de la conception de mécanismes, cette approche élabore les mécanismes régulateurs optimaux et requiert pour cela une définition précise des objectifs de l'autorité et de la firme et une prise en compte rigoureuse des contraintes économiques et informationnelles. Les contraintes informationnelles constituent l'enjeu majeur de cette théorie. Leur existence gêne le contrôle de l'autorité et empêche celle-ci de mettre en place la politique réglementaire qui s'avère être la meilleure pour la société.L’économie industrielle est le fruit d’une longue tradition, initiée par des ingénieurs économistes français, dont Cournot et Dupuit sont les grands noms. Elle s’est ensuite tournée vers les politiques publiques, avec l’entrée en vigueur du Sherman Act aux Etats-Unis et la construction du droit de la concurrence. L'économie industrielle s'est imposée en tant que discipline à partir des années 1940, sous l'impulsion d'économistes tels qu'Edward Mason et Joe Bain. Ce courant initial, baptisé Tradition de Harvard, a adopté une approche descriptive, et sa fameuse « Structure-Conduct-Performance » qui conforte et affine l’intervention publique dans l’organisation des marchés,. On l'oppose généralement à la Tradition de Chicago, plus théorique et non- interventionniste. Cette dernière adopte une approche sceptique critiquant l’absence de fondement théorique de l’économie industrielle, mais qui, réticente face à la régulation en général, n’a pas développé d’autres doctrines en la matière.[1]En effet, avant les années 1980, pour l’économiste, une entreprise était, de fait, une boîte noire (qui permet de transformer divers entrants en produit final et dont la fonction de coût permet d’établir un prix de vente). La portée des études systématiques était de ce fait limitée : il est possible de constater une corrélation entre, par exemple, le taux de concentration d’une industrie et les profits réalisés par les entreprises du secteur sans saisir les causalités à l’œuvre. Les conclusions qui pouvaient être tirées de telles études étaient souvent rudimentaires (lutter contre les cartels, casser les monopoles) et sans véritable dimension prospective, ce qui réduisait leur efficacité.C'est avec les avancées en théorie des jeux et théorie de l'information, au début des années 1980, que l'économie industrielle se dote d'outils pour modéliser les comportements complexes des entreprises. Cela donna naissance à une littérature très majoritairement théorique. Cette approche permet de tenir compte des asymétries d’information et de comprendre de quelle façon un changement de règlementation peut les corriger. La comparaison d'un texte de référence des années 1970, Industrial Market Structure and Economic Performance de F. Michael Scherer, et du texte de Jean Tirole, Théorie de l'organisation industrielle (1988), montre cette évolution sensible, due à l'irruption de la théorie des jeux. La théorie de l’organisation industrielle a permis de passer du constat comportementaliste statique (telle industrie se trouve dans telle situation) à une approche cognitive de l’entreprise (relations avec les concurrents, politique de recherche et développement, différenciation des produits). La compréhension des choix stratégiques (passés et futurs) permet de mieux spécifier les caractéristiques particulières du secteur et de rendre ainsi la réglementation plus efficace (ouverture à la concurrence, régulations de firmes dominantes).  En permettant, toujours à travers des modèles, ces études rigoureuses des interactions stratégiques permettent à l’économiste d'apporter des conseils plus pertinents (à l’entreprise pour son organisation ou à l’Etat pour la règlementation) qui seront adaptés à une industrie spécifique.   L’économie industrielle — et plus encore l’économie de l’innovation — ne se sont développées que tardivement en France, par rapport à l’émergence de l’industrial organization aux États-Unis. Il faut en effet attendre les années 1970 pour que les premiers manuels soient publiés en français et que cet enseignement ne soit introduit dans le cursus de licence ou maîtrise de sciences économiques des universités. L'école d'économie de Toulouse a contribué fortement à cette discipline.   Après un développement important de la théorie, la discipline semble maintenant s'orienter vers une phase de travaux plus appliqués (empirical industrial organization).   Définitions L'organisation industrielle étudie les comportements stratégiques entre acteurs économiques, et donc n'entre pas dans le cadre de la concurrence pure et parfaite (CPP). Ce champ est parfois nommé ""concurrence imparfaite"". Ces deux dénominations correspondent aux titres des versions française et anglaise de l'ouvrage de Jean Tirole, Concurrence imparfaite, Economica, Paris, 1986 et Industrial Organization, MIT press 1988, qui constituent le point de départ de ce champ en France.Par ailleurs, l'utilisation d""industrielle"" dans la dénomination de cette discipline peut être trompeuse. Elle n'est pas spécifique à l'industrie et concerne tous les acteurs et secteurs économiques dont celui des services. Cela provient du fait qu'en anglais, le terme ""industry"" désigne plutôt un secteur ou une filière qu'une structure proprement industrielle (au sens du secteur industriel, secondaire, transformatif par opposition aux secteurs primaire - extractif - et tertiaire - services). En particulier, l'étude économique du fonctionnement d'une industrie se rapproche plutôt de la recherche opérationnelle, de l'ingénierie de production et de la logistique que de l'économie industrielle. Industrial organization versus Industrial Dynamics L’économie industrielle se diffuse en France selon deux principales orientations correspondant aux deux voies suivies par la discipline:Un premier courant prolonge l’approche américaine où l’économie industrielle reste centrée sur les questions des structures de marché et sur l’évaluation de leur efficacité en matière d’allocation des ressources. L’Institut d’économie industrielle (IDEI) à Toulouse constitue le fleuron de cette démarche, grâce à l’implication de deux universitaires français, Jean-Jacques Laffont et Jean Tirole, qui feront le lien entre leurs deux attaches : l’une au Massachusetts Institute of Technology (Boston) aux États-Unis et l’autre à Toulouse, où ils vont créer l'un des meilleurs pôles européens en matière de recherches et de formation de docteurs dans le champ de l’industrial organization, connu aujourd’hui sous le nom de Toulouse School of Economics (TSE). Pour une grande part, l’économie industrielle concerne la microéconomie en situation d’information imparfaite.Un deuxième courant, prenant appui sur les différentes approches structuralistes très présentes parmi les économistes français et parmi certains gestionnaires travaillant sur la firme, va développer une approche de l’économie industrielle plus orientée vers la question productive que sur celle du marché. Prenant appui sur une association, l’Association pour le développement d’études sur la firme et l’industrie (ADEFI, association aujourd’hui en sommeil) et sur une revue scientifique (la Revue d’économie industrielle), proposant des outils de politique industrielle (notamment les politiques de filière des années 1980) et, plus tard, sur une école d’été (l’école méditerranéenne d’économie industrielle, Cargèse), l’étude des systèmes productifs devient centrale dans ces travaux. La publication du volumineux Traité d’économie industrielle en 1988 constitue le point d’orgue de cette démarche.Ces travaux s’efforcent de fédérer un ensemble d’équipes dispersées (Paris, Nice, Strasbourg, Grenoble, Montpellier, Lyon, Bordeaux). Par opposition au courant précédent, ces analyses entendent se concentrer sur les processus de création des ressources en les replaçant dans un cadre contextuel large : stratégies des acteurs, analyses organisationnelles des firmes, prise en compte des technologies et de leurs modifications mais aussi place centrale des Institutions dont l’État. La montée des questionnements scientifiques associés aux différentes formes de l’informatisation de la société va réactualiser l’analyse de l’innovation, longtemps négligée, comme le propose Jean-Luc Gaffard (1990).Quand Bo Carlsson (1987, 1992) proposera d’opposer l’Industrial Organization (IO) centré sur la question de l’allocation des ressources existantes à l’Industrial Dynamics (ID) orienté vers la création de ressources et l’innovation technologique, le second courant se revendiquera clairement du champ de la Dynamique industrielle (Arena, 1990). Ce dernier se décline cependant dans des approches hétérodoxes diversifiées (évolutionnisme, régulationisme, institutionalisme, etc.) et veut s’opposer à la « nouvelle économie industrielle ».En français, il est assez classique de reprendre cette opposition en proposant les termes d'organisation industrielle et d'économie des organisations.Les années 1980 et début des 1990 correspondent à l’âge d’or de l’économie industrielle et de l’innovation, dans la diversité des approches développées. Plusieurs facteurs expliquent pourquoi elle va à cette époque prendre une position dominante dans l’ensemble du champ de la science économique et participer à son renouvellement.Premièrement, elle est productrice de concepts novateurs et de nouvelles approches (économie de l’innovation, théorie des coûts de transaction, théorie des marchés parfaitement disputables, économie de la connaissance, etc.).Deuxièmement, elle est en prise avec les problèmes auxquels sont alors confrontées les économies en matière de concurrence par l’innovation ou de reconversion industrielle (retour à Schumpeter et développement de l’évolutionnisme) : il s’agit de problèmes productifs.Troisièmement, la crise du keynésianisme a laissé la place libre aux approches néoclassiques en macro-économie et les courants hétérodoxes ont trouvé dans la méso-économie un espace et des outils pour penser dans leur propre champ (jusqu’au retour de l’institutionnalisme).Les approches initiées dans le champ de l’économie industrielle et de l’innovation vont trouver des applications dans de nouveaux champs disciplinaires qu’ils vont contribuer à renouveler : économie bancaire, nouvelle économie internationale, l’économie spatiale et, plus tard, l’économie géographique, l’économie du développement, etc. On peut même parler d’une banalisation tant aujourd’hui il est communément admis que la rivalité concurrentielle ou l’avantage compétitif (ou concurrentiel cher à Michael Porter) passe par l’aptitude des firmes et des économies à innover.L'économie industrielle reste aussi très proche de la pratique par son influence sur la conduite de la politique de la concurrence et sur celle de la politique de réglementation des monopoles.Cet article est partiellement ou en totalité issu de l'article intitulé « Économie industrielle » (voir la liste des auteurs).Bruno JULLIEN, « ÉCONOMIE INDUSTRIELLE », dans Encyclopædia Universalis (lire en ligne)Jean Tirole (trad. de l'anglais), Théorie de l'organisation industrielle, Paris, Economica, 1993, 419 p. (ISBN 2-7178-2218-6 et 2-7178-2217-8)Yves Morvan, Fondements d'économie industrielle, Paris, Economica, 1991(en) Luis Cabral, Introduction to industrial organization, MIT Press, 2000, 354 p. (ISBN 0-262-03286-4, lire en ligne)Les trois volumes du Handbook of Industrial Organization constituent la référence de ce champ à un niveau recherche (M1 et au-delà).(en) Handbook of Industrial Organization, vol. 1, Amsterdam/London/New York etc., Elsevier Science Pub. Co., coll. « Handbooks of Economics », 1989, 947 p. (ISBN 0-444-70434-5)(en) Handbook of Industrial Organization, vol. 2, Elsevier Science Pub. Co., coll. « Handbooks of Economics », 1989, 1555 p. (ISBN 0-444-70435-3)(en) Handbook of Industrial Organization, vol. 3, Elsevier Science Pub. Co., coll. « Handbooks of Economics », 2007, 2440 p. (ISBN 978-0-444-82435-6 et 0-444-82435-9, lire en ligne)(en) Richard Schmalensee, « Industrial Economics: An Overview », The Economic Journal, vol. 98, no 392,‎ septembre 1988, p. 643 - 681 (ISSN 0013-0133, résumé)Carlsson B., 1987, Reflections on industrial dynamics: the challenges ahead, International Journal of Industrial Organization, 5(2)Carlsson B., 1992, Industrial Dynamics: A Framework for Analysis of Industrial Transformation. Revue d’économie industrielle, no.61, 7-32.Arena R., 1990, La dynamique industrielle : tradition et renouveau, Revue d’économie industrielle, no.53, 5-17.Arena R., de Bandt, J., Benzoni L., Romani P.M. (coord.), 1988, Traité d’économie industrielle, Economica, Paris. *Gaffard J.L., 1990, Économie industrielle et de l’innovation. Précis Dalloz, Paris(en) Dennis W. Carlton, Modern Industrial Organization, Boston, Pearson / Addison Wesley, 2005, 4e éd., 822 p. (ISBN 9780321223418)Revue d'économie industrielleIJIO Portail de l’économie   Portail de la production industrielle"
économie;"Une économie du marché est un système économique où les décisions de produire, d'échanger et d'allouer des biens et services rares sont déterminées majoritairement à l'aide d'informations résultant de la confrontation de l'offre et de la demande telle qu'établie par le libre jeu du marché. Confrontation qui détermine les informations de prix, mais aussi de qualité, de disponibilité. Au cœur de l'économie de marché, le mécanisme de l'offre et de la demande concourt à la découverte et à l'établissement des prix. Ce mécanisme opère par arbitrage pour un horizon donné et pour une qualité donnée entre des valeurs représentatives du bien ou du service concerné : d'une part la valeur de son coût intrinsèque (prix de revient) mais aussi d'autre part sa valeur d'échange (prix relatif, c'est-à-dire du prix d'un produit ou d'un service par rapport aux autres).Pour Robert Gilpin la dynamique de l'économie de marché fait intervenir également d'autres facteurs comme la concurrence et l'aptitude à la survie des acteurs dans l'activité économique .Cette dynamique propre au marché représente un facteur expliquant la diffusion de la croissance économique et l'extension géographique des échanges dans un espace plus large, au-delà des frontières politiques des États.Pour Roger Guesnerie « À l'aune de l'esquisse qui est faite ici d'une économie de marché -des marchés appuyés sur la monnaie et le droit-, nombre d'économies historiquement datées ont droit au label d'économies de marché ».D'une manière générale, il serait plus exact de parler des économies de marchés plutôt que de l'économie de marché, tant le système est dépendant des contextes et institutions très diverses qui accompagnent et soutiennent les marchés.Dans cette perspective, la volonté de prendre en compte les aspects sociaux en Europe après la Seconde Guerre mondiale a conduit à l'émergence du concept d'économie dite « sociale de marché », qui a été décliné selon différentes variantes propres aux pays concernés.Aujourd'hui, l'importance croissante accordée à l'environnement peut laisser entrevoir une évolution vers une « économie durable de marché » voire une « économie sociale et durable de marché » .Certains auteurs posent clairement une distinction entre économie de marché et capitalisme.Pour Fernand Braudel, les régimes de production/répartition des biens et services ont évolué selon trois formes historiques successives : celle de la vie matérielle primitive où le processus d'auto-suffisance et d'auto-consommation se déroule de manière très locale, à l'échelle de l'individu, de la famille ou de petits groupes. Ici on produit pour se suffire, uniquement, à soi-même. L'échange et donc le marché n'existent pas.celle de l'économie de marché, telle qu'elle découle des échanges rendus nécessaires par une plus grande spécialisation et une plus large division du travail : chacun produit une catégorie spécifique de bien et doit fatalement échanger avec les autres pour se procurer les biens qu'il ne produit plus et ainsi satisfaire l'ensemble de ses besoins.celle du capitalisme, amorcée par les entreprises de « commerce ou de négoce au long cours » et qui se financiarise inéluctablement pour engendrer un système où l'échange commercial n'est plus que le support ou le prétexte de gains financiers. Pour lui, « le capitalisme dérive par excellence des activités économiques au sommet ou qui tendent vers le sommet. En conséquence, ce capitalisme de haut vol flotte sur la double épaisseur sous-jacente de la vie matérielle et de l'économie cohérente de marché, il représente la zone de haut profit ». D'une façon générale, Braudel distingue deux types d'échanges : « l'un terre à terre, concurrentiel puisque transparent » qui relève de l'économie de marché et « l'autre supérieur, sophistiqué, dominant » qui relève du capitalisme.Pour Robert Gilpin, l'essence du marché réside dans le rôle des prix relatifs dans le processus d'allocation des ressources tandis que celle du capitalisme réside dans la propriété privée des moyens de production. Au niveau théorique, une économie socialiste de marché composée d'acteurs publics et de travailleur non libres est pour lui concevable comme cela est envisagé dans le concept d'économie socialiste de marché.Issue d'un concept et d'une pratique liée à l'ordo-libéralisme, l'expression recouvre aujourd'hui un sens plus large. Ainsi, Mario Monti, le commissaire européen, distingue les économies de marché de type anglo-saxon des économies sociales de marché allemande ou française. Pour lui, l'économie de marché doit non seulement être compatible, mais aussi être en mesure de financer la protection sociale par une imposition redistributive, de même que promouvoir un certain volontarisme des gouvernements et des institutions européennes en faveur de l'économie dans le respect des règles européennes de la concurrence. En effet, dans l'optique libérale la concurrence entraîne la baisse des prix. Cela protège le pouvoir d'achat des individus et favorise l'innovation,. Le capitalisme, lui, encouragerait en réalité des comportements criminels, crapuleux et opportunistes .Il n'y a pas sur le plan théorique d'unanimité quant à la définition précise de l'économie de marché. On constate en revanche l'existence et la pratique de modèles les plus divers où le mécanisme d'économie de marché est amené à coexister et à composer avec des logiques ou contraintes plus ou moins compatibles.Le régime de la concurrence pure et parfaite n'étant pas concrétisé dans la réalité,il n'existe pas « un » marché général où se produit la confrontation de toutes les offres et de toutes les demandes, mais « des » marchés plus ou moins inter-connectés sinon cloisonnés qui donnent lieu à des confrontations partielles. (ex : marché des biens, des matières premières, des services, du travail, des changes, des capitaux, marché monétaire, marché immobilier, etc.)les imperfections de marché prospèrent :Monopole, duopole, oligopole, cartel, entente, position dominante, etc.asymétrie de l'information, délai et effet retard, goulet d'étranglement, etc.la demande exprimée sur un marché est perçue la plupart du temps à travers le prisme déformant de la demande solvable, c'est-à-dire celle qui émane des opérateurs disposant du pouvoir d'achat monétaire suffisant.Des actions collectives peuvent être organisées pour promouvoir ou défendre des valeurs positives ou des règles sociales, culturelles, morales, voire religieuses :l'économie publique ayant une place complémentaire (ex : les services publics), ou centrale (ex: le capitalisme d’État) ;l'économie sociale pour pallier les insuffisances ou déficiences du marché (ex : les mécanismes de protection sociale, d'assistance, de solidarité, d'État providence) ;l'interventionnisme d'État ou dirigisme via l'économie planifiée ou l'économie administrée ;le commerce équitable comme projet d'organisation visant à faire une meilleure place à certains producteurs en danger d'être marginalisés ou évincés du marché courant ;l'élaboration de normes - à caractère volontaire, incitatif ou obligatoire - peuvent contribuer à préciser ou encadrer la définition des pratiques de conception, de production ou de distribution des biens et services pour des motifs de protection de la qualité ou de la sécurité dues au consommateur/usager.l'économie dite non marchande ou domestique ne donne pas lieu à échange rémunéré(ex : jardins familiaux, babysitting non rémunéré, femmes au foyer, aide des grands-parents…).l'économie de troc et/ou l'économie de subsistance se situe relativement à l'écart des flux économiques (ex. : pratiques de certaines zones rurales du tiers monde ou très déshéritées)l'économie spéculative ou exclusivement financière qui introduisent des logiques de type « argent ⇒ marchandise ⇒ argent » dénoncées par certains comme représentant des déconnexions forcées de l'économie réelle. (ex. : spéculation sur les matières premières)l'économie souterraine ou les « trafics » opérés sur des marchés parallèles ou occultes (ex: le trafic de drogue, le travail ou le marché au noir, ou le proxénétisme…).les phénomènes de corruption ou de délit d'initié qui visent par leur nature à fausser le libre jeu des forces du marché.L'économie de marché s'arrête difficilement au niveau d'un seul pays, si vaste soit-il. Au niveau international, elle est d'autant plus développée que les divers pays pratiquent le libre-échange.Cela dit, en pratique beaucoup de pays revendiquent pour leurs exportations les règles applicables à l'économie de marché (sinon la clause de la nation la plus favorisée), en organisant par ailleurs vis-à-vis des importations des règles fort peu réciproques (protectionnisme) :Certains pays issus du collectivisme — comme la Chine — se veulent « économie socialiste de marché », alors qu'ils sont encore des économies marquées par l'empreinte du Capitalisme d'État.D'autres pays bénéficient de conditions de couts qui leur permettent de pratiquer une concurrence jugée « déloyale » par leurs rivaux plus développés.D'autres pays se trouvent dans une situation où la structure des flux échangés les entraine vers la détérioration des termes de l'échange.Un débat donne lieu à un fort questionnement assorti de multiples prises de position :Échanges entre socialistes « réformistes » et socialistes « fondamentalistes » Michel Rocard se targue souvent « d'avoir mis des décennies à apprendre l'économie de marché aux socialistes ».le tournant de la rigueur survenu en 1983 motive la gauche réformiste pour accepter de facto l'économie sociale de marché.L'économie de marché non régulée n'est pas forcément compatible avec les exigences du développement durable. En effet, la recherche de la maximisation du profit par les entreprises ne va pas spontanément dans le sens d'un développement durable, car elle conduit à des raisonnements de court terme (voire de spéculation), et elle tend à la satisfaction des intérêts des seuls actionnaires des entreprises.L'encyclique Caritas in Veritate de Benoît XVI (juillet 2009) indique que les acteurs de la vie économique ne peuvent se limiter au marché seul, mais « que l'économie doit aussi impliquer l'État et la société civile » :« La vie économique a sans aucun doute besoin du contrat pour réglementer les relations d’échange entre valeurs équivalentes. Mais elle a tout autant besoin de lois justes et de formes de redistribution guidées par la politique, ainsi que d’œuvres qui soient marquées par l’esprit du don. L’économie mondialisée semble privilégier la première logique, celle de l’échange contractuel mais, directement ou indirectement, elle montre qu’elle a aussi besoin des deux autres, de la logique politique et de la logique du don sans contrepartie. »« Mon prédécesseur Jean-Paul II avait signalé cette problématique quand, dans Centesimus annus, il avait relevé la nécessité d’un système impliquant trois sujets : le marché, l’État et la société civile. »— Encyclique Caritas in Veritate, chapitre III, § 37 et 38L'intervention de l'État, qui représente les « intérêts publics » (notion à définir), est considérée par certains comme nécessaire. Elle se fait actuellement de la façon suivante :Être exemplaire en matière de développement durable :Définir une stratégie nationale de développement durable,Mettre en œuvre cette stratégie en mettant en place des organisations dédiées au développement durable dans les ministères et les collectivités territoriales (ex. : ministère de l'environnement ou du développement durable),Lancer des actions concrètes comme le Grenelle de l'environnement.Encadrer le marché par l'application forcée des règles le concernant (concurrence, propriété privée, droit du travail, ...).Participer aux différentes initiatives qui ont lieu au niveau international sur le développement durable, sommets de la Terre, sommets de l'eau, protocole de Kyoto et ses suites, réunions sur la biodiversité…Définir de nouvelles règles du jeu :En France, les entreprises cotées en bourse doivent rendre compte des conséquences sociales et environnementales de leur activité (article 116 de la loi sur les Nouvelles Régulations Économiques),Principe du bonus-malus écologique pour les véhicules automobiles,Mise en place d'une finance du carbone.Toutefois, les évaluations portant sur la mise en œuvre des Nouvelles Régulations Économiques en France montrent qu'assez peu d'entreprises se conforment réellement aux exigences de la loi. En effet, le non-respect de la loi n'entraîne aucune sanction vis-à-vis des entreprises. Il s'agit d'un droit mou.On peut imaginer d'autres actions des États :Changer les règles de comptabilisation de la richesse. Par exemple, des études ont montré que le produit intérieur brut (PIB) ne prend pas en compte la diminution du capital naturel. L'INSEE a retenu le PIB comme indicateur de développement durable, alors qu'en réalité les effets à long terme de la croissance économique sur l'environnement ne sont pas pris en compte par le PIB. Il est clairement de la responsabilité des États de définir des instruments de mesure de la croissance économique, en l'occurrence des indices macroéconomiques, qui rendent compte efficacement de la conformité des agents économiques par rapport aux principes de développement durable (PIB vert).Mettre en place une fiscalité favorable aux produits durables (taxe carbone),Adapter l'enseignement,Sensibiliser la société civile en donnant le feu vert à tous les moyens permettant de montrer les dangers sur l'homme des activités qui menacent l'environnement et le développement durable.etc.La société civile intervient par l'intermédiaire de ses représentants, organisés en parties prenantes (organisations professionnelles, organisations syndicales, organisations non gouvernementales…). Par exemple, en matière environnementale, les parties prenantes représentatives sont les ONG (organisations non gouvernementales) (environnementales (WWF, Greenpeace, Amis de la Terre…).Les parties prenantes peuvent se concevoir par rapport aux autorités politiques, ou bien par rapport aux entreprises.Par rapport à la perspective catholique, les sociaux-libéraux s'interrogent sur la notion même d'État. La distinction société civile/État leur pose un problème car elle suppose, à la manière de ce qui existe dans l'Église, une prépondérance donnée à la hiérarchie de l'État sur les citoyens.L'Organisation mondiale du commerce (OMC) octroie le « statut d’économie de marché » (SEM) aux États. Un pays qui importe des produits depuis un pays qui n'en bénéficie pas est autorisé à ne pas tenir compte du prix pratiqué sur le marché intérieur de l’État exportateur.La Chine s'est ainsi vue attribuer ce statut en 2016, conformément à l'accord convenu lors de son adhésion en 2001,. Avant même cette décision de l'OMC, plus de 80 pays dans le monde avaient reconnu le statut d'économie de marché à la Chine. Cependant, les États-Unis s'y opposent,. De son côté, l'Union européenne a mis en place une nouvelle méthodologie anti-dumping qui ne cible plus spécifiquement la Chine : Jean Quatremer estime ainsi qu'« en clair, l’Union va continuer à considérer que la Chine n’est pas un pays à économie de marché, mais sans le proclamer et en évitant les foudres de l’OMC »,. D'autre part, un rapport détaillé de la Commission européenne émettait des doutes en décembre 2017 sur la nature d'« économie de marché » de l'« économie socialiste de marché » de la Chine,.Robert Gilpin, 1987, The Political Economy of International Relation, Princeton University Press.Fernand Braudel, 1985, La Dynamique du capitalisme, Paris, Arthaud, 1985  (ISBN 2080811924)Roger Guesnerie 2006, L'Économie de marché, Le Pommier.John Kenneth Galbraith, Et le système fut rebaptisé. dans Les Mensonges de l'économie, traduction française de Paul Chemla, Paris, Grasset, 2004Michel Lafitte, 2007, Développement durable et économie de marché  (ISBN 2863254782) Portail du libéralisme   Portail de l’économie"
économie;La conjoncture est la situation économique d’un pays à un moment donné.Le terme de conjoncture fait référence aux évolutions économiques de court terme d’un ensemble économique, en général un pays. Elle s’apprécie à l'aide d'indicateurs économiques tels que le taux de croissance du PIB, le taux d'inflation, l’évolution du taux de chômage, la balance commerciale, etc.La conjoncture est liée aux cycles économiques, et ses durées types des changements conjoncturels vont de quelques mois à quelques années. La conjoncture est étudiée au niveau d'un pays, d'une région, ou, à un niveau plus fin, au niveau d'un secteur économique particulier. On peut parler alors de la conjoncture économique d'une entreprise.Le gouvernement et la banque centrale d'un pays tentent généralement de limiter l'influence néfaste des trop grandes variations de conjoncture économique sur l'activité économique au moyen de politiques conjoncturelles.Par exemple, en période de « surchauffe économique », la banque centrale peut augmenter ses taux directeurs pour éviter les bulles spéculatives, l’accélération de l'inflation et le sur-investissement, c'est-à-dire les investissements dans des projets non-rentables ou l'accumulation de capacités excédentaires de production dans certains secteurs économiques. À l'inverse quand se présente un risque de récession économique elle peut abaisser ses taux, dans la mesure toutefois où cela ne risque pas de provoquer des tensions inflationnistes.Inversement, le gouvernement et la banque centrale encourageront l'activité dans les périodes de creux économique.ÉconomieCycle économique Portail de l’économie
économie;"La croissance économique désigne la variation positive de la production de biens et de services dans une économie sur une période donnée, généralement une longue période. En pratique, l'indicateur le plus utilisé pour la mesurer est le produit intérieur brut (PIB). Il est mesuré « en volume » ou « à prix constants » pour corriger les effets de l'inflation. Le taux de croissance, lui, est le taux de variation du PIB. On utilise souvent la croissance du PIB par habitant comme indication de l'augmentation de la richesse individuelle, assimilée au niveau de vie (à distinguer de la qualité de vie).La croissance est un processus fondamental des économies contemporaines, reposant sur le développement des facteurs de production, lié notamment à la révolution industrielle, à l'accès à de nouvelles ressources minérales (mines profondes) et énergétiques (charbon, pétrole, gaz, énergie nucléaire, etc.) ainsi qu'au progrès technique. Elle transforme la vie des populations dans la mesure où elle crée davantage de biens et de services. À long terme, la croissance a un impact important sur la démographie et le niveau de vie des sociétés qui en sont le cadre. De même, l'enrichissement qui résulte de la croissance économique peut permettre de faire reculer la pauvreté, à condition que les richesses créées soient redistribuées vers les plus bas revenus.Certaines conséquences de la croissance économique comme la pollution et les atteintes à l'environnement, l'accentuation des inégalités sociales ou l'épuisement des ressources naturelles (pétrole, métaux notamment) sont souvent considérés comme des effets pervers qui obligent à distinguer croissance et progrès.Le rapport commandé en 1970 par le Club de Rome à une équipe du Massachusetts Institute of Technology (MIT), intitulé The Limits To Growth met en évidence l'impossibilité d'une croissance illimitée dans un monde fini. Les tenants de la décroissance estiment que la poursuite de la croissance amènerait inévitablement à un effondrement de la civilisation.Les économistes utilisent le terme de croissance conventionnellement pour décrire une augmentation de la production sur le long terme (une durée supérieure à un an). Selon la définition de François Perroux, la croissance économique correspond à « l'augmentation soutenue pendant une ou plusieurs périodes longues d’un indicateur de dimension, pour une nation, le produit global net en termes réels ». La définition de Simon Kuznets va au-delà et affirme qu'il y a croissance lorsque la croissance du PIB est supérieure à la croissance de la population[réf. nécessaire].À court terme, les économistes utilisent plutôt le terme d'« expansion », qui s'oppose à « récession », et qui indique une phase de croissance dans un cycle économique. La croissance potentielle est le niveau de croissance qui serait obtenu avec une pleine utilisation de tous les facteurs de production (travail, capital et savoir) ; l'écart entre la croissance effective (celle effectivement mesurée) et la croissance potentielle est minimal au plus fort d'une expansion.Au sens strict, la croissance décrit un processus d'accroissement de la seule production économique. Elle ne renvoie donc pas directement à l'ensemble des mutations économiques et sociales propres à une économie en développement. Ces transformations au sens large sont, conventionnellement, désignées par le terme de développement économique. Selon François Perroux, « le développement est la combinaison des changements mentaux et sociaux d'une population qui la rend apte à faire croître, cumulativement et durablement, son produit réel global ». Le terme de « croissance » s'applique alors plus particulièrement aux économies déjà développées.La Commission du développement durable (en) du gouvernement britannique souligne qu'il est important de distinguer trois notions qui « ne sont absolument pas les mêmes choses » :la croissance des flux biophysiques (énergie et matériaux) ;la croissance de la valeur monétaire de la production (PIB) ;la croissance du bien-être économique de la population.Le croissantisme économique est considéré comme étant l'idéologie de la croissance par opposition à la philosophie décroissantiste.La croissance économique est généralement mesurée par l'utilisation d'indicateurs économiques dont le plus courant est le produit intérieur brut (PIB). Il offre une certaine mesure quantitative du volume de la production. Afin d'effectuer des comparaisons internationales, on utilise également la parité de pouvoir d'achat, qui permet d'exprimer le pouvoir d'achat dans une monnaie de référence. Au niveau international, cette monnaie de référence est le dollar américain. Pour comparer la situation d'un pays à des époques différentes on peut également raisonner à monnaie constante en se référant aux prix d'une date antérieure appelée date de référence (afin d'eviter les effets de l'inflation).L'indicateur du PIB reste cependant imparfait comme mesure de la croissance économique. Il est pour cela l'objet de plusieurs critiques :le PIB ne mesure que partiellement l'économie informelle. Une part importante des transactions, non déclarée, était perdue pour les statistiques comme pour l'administration fiscale. Mais en 2014, plusieurs pays (l'Italie, le Royaume-Uni, l'Espagne et la Belgique) ont décidé d'intégrer dans leur PIB des estimations de l'économie souterraine (drogue, prostitution, trafics divers) en application des nouvelles normes comptables européennes publiées par Eurostat ; les États-Unis l'avaient déjà fait en 2013 ; la comptabilité nationale française, qui effectuait déjà des redressements pour prendre en compte les activités dissimulées (travail au noir, contrebande), a décidé d'intégrer des estimations du trafic de drogue, mais pas de la prostitution clandestine ;le PIB ne mesure que de façon imparfaite les productions qui ne sont pas commercialisées : ainsi, la production des administrations est supposée égale aux salaires des fonctionnaires ; une évaluation des productions agricoles auto-consommées est intégrée au PIB. D'autre part, même s'il prend en compte la production des activités non marchandes, le PIB ne mesure pas l'activité de production domestique (ménage, cuisine, bricolage, éducation des enfants, etc.), des activités majoritairement réalisées par des femmes,. En 2009, la commission Stiglitz estime que la valeur du travail domestique équivaut à 35 % du PIB en France sur la période 1995-2006 ;le PIB ne mesure que les apports de valeur ajoutée dans l'immédiat (sur une année). Les effets de long terme, notamment dans des services tels que l'éducation ou la santé, ne sont pas ou mal comptabilisés à travers leur impact sur la production ;le PIB ne mesure que la valeur ajoutée produite par les agents économiques résidents. Il ne prend donc pas en compte les transferts de ressources internationaux (les sorties de fonds du pays par les résidents étrangers vers leurs pays d'origine et les entrées en provenance de l'étranger qui correspondent aux envois de fonds vers leur pays d'origine par les résidents nationaux à l'étranger), alors que ces derniers représentent souvent une part importante de leur richesse nationale. Il est possible d'utiliser un outil plus pertinent tel que le revenu national brut ;enfin, le PIB ne prend en compte que les valeurs ajoutées, et non la richesse possédée par un pays, sans distinguer les effets positifs ou négatifs sur le bien-être collectif. Une catastrophe naturelle (l'ouragan Katrina détruisant La Nouvelle-Orléans, par exemple), qui détruit de la richesse, va pourtant contribuer au PIB à travers l'activité de reconstruction qu'elle va générer. Cette contribution ne reflète pas la destruction antérieure, ni le coût du financement de la reconstruction. Cette contradiction était dénoncée dès 1850 par l'économiste français Frédéric Bastiat qui, dans son Sophisme de la vitre cassée, écrivait que « la société perd la valeur des objets inutilement détruits », ce qu'il résumait par : « destruction n'est pas profit ».Cette contradiction apparente provient probablement du fait que le PIB ne mesure pas réellement le développement, le progrès en lui-même ; il ne mesure pas non plus l'activité économique, pourvoyeuse d'emploi, car l'activité peut fort bien croître sans augmentation de valeur ajoutée, si l'on remplace du capital ou des matières premières par du travail. Dans ce cas, la croissance économique tiendrait compte aussi bien de la production formelle que celle informelle (cf. supra). Mais pour parvenir à cet objectif, tant la comptabilité des entreprises que la comptabilité nationale doivent complètement quitter le critère de mesure par la valeur ajoutée (i.e. production vendue moins consommations intermédiaires) et adopter celui du travail. La croissance ne mesure en fait que l'augmentation de la consommation de facteurs de production : travail, capital et ressources naturelles (matières premières, potentiel productif des terres agricoles, énergie...). La société peut progresser[pas clair] sans croissance, en modifiant la répartition des facteurs.Dans son acception classique, le développement économique ne se résume pas à la seule croissance économique et des indicateurs ont été proposés pour mesurer plus finement celui-ci, comme l'indice de développement humain, mis au point par l'économiste du développement Amartya Sen, prix Nobel d'économie.Dans un certain nombre de cas, les données de la comptabilité nationale ne sont pas disponibles ou sont de mauvaise qualité. C'est notamment un problème lorsqu'on s'intéresse à des périodes anciennes, à des pays en voie de développement avec une mauvaise comptabilité nationale ou encore lorsqu'on s'intéresse au développement économique à un niveau infra-national, par exemple au niveau d'une ville ou d'une région. Dans ce cas, plusieurs indicateurs ont été proposés.Les économistes Daron Acemoglu, Simon Johnson et James Robinson utilisent des données sur l'urbanisation et sur la densité de population pour approximer le degré de développement en l'an 1500.Les économistes David Weil, Vernon Henderson et Adam Storeygard (2011) proposent d'utiliser des images satellites pour mesurer l'augmentation de l'intensité lumineuse de nuit et utiliser cette donnée pour estimer la croissance économique.Grâce au développement des statistiques nationales, les économistes, les historiens et les démographes ont constaté qu'avant la Révolution industrielle, la croissance économique est essentiellement liée à celle de la population : on produit plus parce qu’il y a plus d'individus pour produire, mais le niveau de vie reste le même. Cela peut s'expliquer par le fait que la croissance démographique est strictement supérieure à la croissance économique (cf. graphe). Au sens du prix Nobel d'économie, l'américain Simon Kuzsnets, à ce stade il n'y a pas de croissance économique. Pour lui, la croissance n'est possible que lorsque le taux de ce dernier est strictement supérieur à celui de la croissance démographique.À partir du XVIIIe siècle, la croissance économique se déconnecte de celle de la population (et devient supérieure à celle-là, après qu'elle lui était égale sur plusieurs décennies) et l’augmentation du niveau de vie devient exponentielle, mais très irrégulière. Cela s'explique essentiellement par le progrès technique (amélioration des facteurs de production, capital et  travail, notamment) devenu plus important depuis la révolution industrielle (renouvellement colossal à la fois économique, culturel et social) de la fin du dix-huitième siècle. Après les très forte croissance mondiale des années 1830 et croissance mondiale des années 1850, la Grande Dépression (1873-1896) donne un sérieux coup de frein. De même, la grande dépression des années 1930 fait suite à la croissance économique de la Belle Époque et à la puissante expansion des années 1920. Plus généralement, les périodes de reconstruction suivant une guerre sont favorables, comme lors de la très forte croissance des années 1950, socle des Trente Glorieuses (1945-1973).Les historiens[Qui ?] s’accordent sur le fait que le niveau de vie sur l’ensemble du globe a peu évolué de l’Antiquité jusqu’au XVIIIe siècle (entre l'an 1 et l'an 1000 l'économie mondiale aurait même décliné), mis à part une embellie en Europe occidentale entre les Xe et XIIIe siècles, annulée par les épidémies et les famines des XIVe et XVe siècles. Ils s'accordent aussi à constater qu'il y a de grandes disparités selon les peuples et selon les époques. Sachant qu'on a affaire à des sociétés où presque toute la population est rurale, il est de toutes façons presque impossible d'obtenir la statistique de leur production, puisque celle-ci est presque complètement locale, voire familiale (bâtiment, mobilier, confection, alimentation, services…), et très marginalement commerciale, de telle sorte qu'il est impossible de reconstituer un standard moyen de consommation et de l'évaluer en monnaie.Selon l'économiste français Thomas Piketty, du XVIIIe siècle au XXe siècle, « la production mondiale a progressé en moyenne de 1,6 % par an, dont 0,8 % par an au titre de la population et 0,8 % au titre de la production par habitant », soit « un rythme très rapide, dès lors qu'il se prolonge durablement ».La croissance économique, aussi bien comme phénomène que comme donnée objectivable, est donc quelque chose de récent, lié à l'urbanisation (création des villes et déplacement de la population vers celles-là) des sociétés et à l'apparition de statistiques nationales. Selon la théorie du déversement, l'industrialisation provoque un déplacement des activités des campagnes vers les villes, les effectifs suivent. Jusqu'aux années 1970, c'était aussi un phénomène géographiquement limité, qui concernait surtout les pays occidentaux et le Japon[réf. nécessaire]. Même avec l'ouverture du commerce International à de nouveaux entrants et notamment à la Chine (Doha, 2002), la part de la croissance des pays de la triade (l'Union européenne, les États-Unis et le Japon) dans la croissance mondiale est encore la plus importante jusqu'à présent.Les Pays-Bas sont la première société à connaître un phénomène de croissance, au XVIIe siècle. Comme le note Henri Lepage en reprenant les analyses de Douglass North, « pour la première fois dans l'histoire connue de l'humanité, un pays se trouvait en mesure d'offrir un niveau de vie croissant à une population croissante, et cela un siècle avant que se manifestent les premiers signes réels de la Révolution industrielle ».Le phénomène s'est ensuite progressivement étendu. La phase de développement économique depuis la Révolution industrielle n'a aucun précédent historique. Après le XVIe siècle, lorsque différentes parties du monde développent des relations commerciales, on constate des périodes de croissance économique, mais éphémères et marginales. Les écarts entre conditions de VIe au XVIIIe siècle étaient réduits, pour certains auteurs comme Paul Bairoch : l'Inde possédait même un niveau de vie supérieur à l'Europe. On estime que la croissance globale de l'économie entre 1500 et 1820 n'est que d'un trentième de ce qu'elle a été depuis (de 247 milliards de dollars internationaux en 1500 à 695 en 1820, puis 33 725 en 1998). Les revenus en Europe ont été multipliés par 20 entre 1820 et les années 1990. L'Asie accélère aussi son rythme de croissance depuis un demi-siècle[Quand ?] : le niveau de vie en Chine a été multiplié par six et celui du Japon par huit[réf. nécessaire].Cependant, au XIXe siècle le développement économique entraîne des bouleversements sociaux comme l'exode rural (cf. supra, théorie du déversement). Le niveau de vie et le développement n'ayant commencé à être étudiés rigoureusement qu'au XIXe siècle, il est cependant difficile, faute de données, de faire une comparaison entre le XVIIIe et le XIXe siècle[réf. nécessaire].En 1913, le PIB/hab français était de 3 485 dollars internationaux (base 1990). En 1998, il était de 19 558 $. Le taux de croissance moyen du PIB/hab était donc de 2,0 % sur cette période. S'il avait été de 1,0 %, le niveau de vie aurait été de 8 200 $ en 1998, soit un peu moins que le niveau de vie réel de l'Uruguay (8 314 $).L'évolution en pourcentage du PIB en volume d'une année à l'autre. Les données sont mesurées en monnaie constante de 2005 d’après les données de l’OCDE (organisation du commerce et du développement en Europe).Source : Banque Mondiale.Selon Jean-Marie Albertini, si dans les économies capitalistes la croissance est réalisée en recourant au progrès technique et l'exploitation du travail devenu libre grâce à la séparation du travailleur de ses moyens de production qui a donné lieu à la propriété privée de ces moyens, elle est le résultat de la prépondérance du facteur politique sur le facteur économique dans les sociétés socialistes. La propriété collective des terres agricoles à permis de réaliser une production agricole à bas prix. Les produits alimentaires obtenus sont vendus dans les villes à des prix plus élevés (les revenus de la population urbaine étaient plus élevés que dans les compagnes). Les profils obtenus permettent de financer l'industrie lourde et donc de réaliser la croissance.Dans Le Capital au XXIe siècle, Thomas Piketty fait l'hypothèse que la période de forte croissance économique est terminée (depuis la fin des trente glorieuses et le début de la crise en 1973) et qu'il y a toutes les raisons de penser que la croissance devrait revenir à un niveau plus faible dans un régime stationnaire.Dans The Rise and Fall of American Growth (2016), l'économiste Robert J. Gordon défend la thèse que la forte croissance aux États-Unis et dans les pays développés entre 1870 et 1970 a été une exception et que les innovations qui ont eu lieu depuis 1970 génèrent moins de croissance que par le passé.En 2021, le quotidien français Le Monde observe que la croissance fait l'objet, en France, d'un « nouveau clivage politique ». Frédéric Dabi, directeur général de l’Institut français d’opinion publique (IFOP), indique : « En 2021, la moitié des Français se disent favorables à plus de croissance et à ce qu’une priorité absolue soit donnée à la création d’emplois, tandis que l’autre moitié défend un autre modèle de développement ayant pour objectif la préservation des ressources. En 2017, les premiers étaient majoritaires ».On peut distinguer plusieurs types de déterminants à la croissance : les richesses naturelles, l'environnement extérieur, la population, l'innovation au sens de Joseph A. Schumpeter (concept qui ne concerne pas seulement le progrès technique), l'investissement, la connaissance, la cohérence du développement. Les principales conclusions des travaux de Xavier Sala-i-Martin, économiste espagnol spécialiste de la croissance, confirment qu'il n'y a pas qu'un seul déterminant simple de la croissance économique.Xavier Sala-i-Martin avance que le niveau initial est la variable la plus importante et la plus robuste. C'est-à-dire que, dans la plupart des cas, plus un pays est riche, moins il croît vite. Cette hypothèse est connue sous le nom de convergence conditionnelle. Il considère également que la taille du gouvernement (administration, secteur public) n'a que peu d'importance. Par contre la qualité du gouvernement a beaucoup d'importance : les gouvernements qui causent l'hyperinflation (taux d'inflation extrêmement élevé), la distorsion des taux de change, des déficits excessifs (ceux de la balance des paiements et du budget de l'État) ou une bureaucratie inefficace ont de très mauvais résultats. Il ajoute également que les économies plus ouvertes tendent à croître plus vite (cf. croissance de la Chine depuis 2002). Enfin, l'efficience des institutions est très importante : des marchés efficients, la reconnaissance de la propriété privée et l'état de droit sont essentiels à la croissance économique. Il rejoint en cela les conclusions d'Hernando de Soto,.Sur une plus longue période, l'expérience historique[réf. nécessaire], notamment celle du XVIIIe siècle, suggère que l'extension des libertés économiques (liberté d'entreprendre, de circulation des idées, des personnes et des biens) est une condition de la croissance. En occident, la culture, la religion et la société ont connu des changements très profonds. Tout d'abord, le conformisme de l'homme précapitaliste a été vaincu par une volonté de rendre l'environnement dépendant de l'homme plutôt que de lui être dépendant. Ensuite, le puritanisme et le calvinisme (qui est une variante principale du premier courant) ont fait du travail une vertu divine et considèrent la misère comme un péché. Enfin, les restrictions sociales sont vigoureusement combattues : les vielles corporations qui constituent l'une des stratégies fondamentales du mercantilisme et qui limitent la liberté des hommes (et l'innovation) sont levées et aussi bien le "" laisser-faire "" que le "" laisser-passer "" sont devenus des moyens au service de plus de croissance économique. Au XXe siècle, il existe plusieurs cas où une population partageant les mêmes antécédents historiques, la même langue et les mêmes normes culturelles a été divisée entre deux systèmes, l'un étant une économie de marché et l'autre une économie planifiée : les deux Allemagne, les deux Corée, la république populaire de Chine et Taïwan. Dans chaque cas, les zones ayant pratiqué l'économie de marché ont obtenu une croissance nettement supérieure sur le long terme. Cependant, l'enrichissement de l'Allemagne de l'Ouest s'explique par l'aide des États-Unis, l'enrichissement de la Corée du Sud et de Taïwan par l'aide des États-Unis et du Japon et que Taïwan a attiré les Chinois les plus qualifiés. Les États-Unis et l'Europe de l'Ouest étant beaucoup plus développés que l'URSS, leurs pays alliés ont été beaucoup plus aidés. La très forte croissance de l'URSS avant les années 1960 et la très forte croissance de la Chine depuis les années 1980 sont des exemples de pays dont l'économie planifiée a augmenté la croissance. Aucun pays n'a eu une croissance telle que celle de la Chine et l'URSS sans bénéficier d'aide extérieure ou d'une exploitation massive de ressources naturelles très lucratives, telles le pétrole, par rapport au nombre d'habitants. L'effondrement de l'URSS témoigne également des meilleurs résultats des économies de marché par rapport aux économies de type collectiviste[Interprétation personnelle ?].Sur le très long terme, Angus Maddison identifie trois processus interdépendants qui ont permis l'augmentation conjointe de la population et du revenu : la conquête ou la colonisation d'espaces fertiles et relativement peu peuplés, le commerce international et les mouvements de capitaux, et enfin l'innovation technologique et institutionnelle.Quant à Daron Acemoglu, dans An Introduction to Modern Economic Growth (2008), il distingue quatre causes fondamentales de la croissance : l'environnement naturel, la culture, les institutions et la chance.Une étude empirique publiée en 2010 affirme avoir établi un lien entre un manque de croissance économique et la consanguinité.Pour Jean-Marie Albertini, maître de recherche au CNRS français, les déterminants de la croissance économique, sont, au moins, au nombre de trois. Le chercheur, précise, cependant, que ces paramètres ne sont pas les mêmes selon qu'il s'agisse d'une économie capitaliste développée ou d'un régime socialiste. Dans les sociétés industrialisées modernes, la croissance est le fruit, à la fois, de l'exploitation du travail des femmes et des enfants à la fin de la révolution industrielle de 1789 et du progrès technique d'une part, et de l'exploitation des pays colonisés de l'autre. Dans une société marxiste, la croissance économique est la conséquence d'une socialisation des moyens de production. La production collective des terres agricoles a permis au régime stalinien en URSS de réaliser une production agricole à bas prix, de vendre, ensuite, le produit ainsi obtenu à la population urbaine à prix élevé. Les bénéfices obtenus permettent de financer la croissance (production militaire, construction de logements sociaux, de barrages d'irrigation, ...).Les théories explicatives de la croissance sont relativement récentes dans l'histoire de la pensée économique. Ces théories, sans négliger le rôle de l'ensemble des facteurs de production tendent à mettre en avant parmi ceux-ci le rôle primordial du progrès technique dans la croissance. Sur le long terme, seul le progrès technique est capable de rendre plus productive une économie (et donc de lui permettre de produire plus, c'est-à-dire d'avoir de la croissance)[réf. nécessaire]. Toutefois, ces théories expliquent encore mal d'où provient ce progrès, et en particulier en quoi il est lié au fonctionnement de l'économie.La plupart des économistes de l'école classique, écrivant pourtant au commencement de la révolution industrielle, pensaient qu'aucune croissance ne pouvait être durable, car toute production devait, selon eux, inexorablement converger vers un état stationnaire. C'est ainsi le cas de David Ricardo pour qui l'état stationnaire était le produit des rendements décroissants des terres cultivables, ou encore pour Thomas Malthus qui le liait à son « principe de population », mais aussi pour John Stuart Mill.Toutefois, Adam Smith, à travers son étude des effets de productivité induits par le développement de la division du travail, laissait entrevoir la possibilité d'une croissance ininterrompue. Et Jean-Baptiste Say écrivait « Remarquez en outre qu’il est impossible d’assigner une limite à la puissance qui résulte pour l’homme de la faculté de former des capitaux ; car les capitaux qu’il peut amasser avec le temps, l’épargne et son industrie, n’ont point de bornes. ».Nikolai Kondratiev est un des premiers économistes à montrer l'existence de cycles longs de 50 ans, et Joseph Schumpeter développe la première théorie de la croissance sur une longue période. Il considère que l'innovation portée par les entrepreneurs constitue la force motrice de la croissance. Il étudie en particulier le rôle de l'entrepreneur dans Théorie de l'évolution économique en 1913.Pour Schumpeter, les innovations apparaissent par « grappes », ce qui explique la cyclicité de la croissance économique. Par exemple, Schumpeter retient les transformations du textile et l'introduction de la machine à vapeur pour expliquer le développement des années 1798-1815, ou le chemin de fer et la métallurgie pour l'expansion de la période 1848-1873. De façon générale il retient trois types de cycles économiques pour expliquer les variations de la croissance :les cycles longs ou cycles Kondratieff, d'une durée de cinquante ans ;les cycles intermédiaires ou cycles Juglar, d'une durée de dix ans environ ;les cycles courts ou cycles Kitchin, d'une durée de quarante mois environ.Schumpeter introduit enfin le concept de « destruction créatrice » pour décrire le processus par lequel une économie voit se substituer à un modèle productif ancien un nouveau modèle fondé sur des innovations. Il écrit ainsi :« L'impulsion fondamentale qui met et maintient en mouvement la machine capitaliste est imprimée par les nouveaux objets de consommation, les nouvelles méthodes de production et de transport, les nouveaux marchés, les nouveaux types d'organisation industrielle - tous éléments créés par l'initiative capitaliste. […] L'ouverture de nouveaux marchés nationaux ou extérieurs et le développement des organisations productives, depuis l'atelier artisanal et la manufacture jusqu'aux entreprises amalgamées telles que l’US Steel, constituent d'autres exemples du même processus de mutation industrielle — si l'on me passe cette expression biologique — qui révolutionne incessamment de l'intérieur la structure économique, en détruisant continuellement ses éléments vieillis et en créant continuellement des éléments neufs. Ce processus de destruction créatrice constitue la donnée fondamentale du capitalisme : c'est en elle que consiste, en dernière analyse, le capitalisme et toute entreprise capitaliste doit, bon gré mal gré, s'y adapter. »Après la Seconde Guerre mondiale, les économistes Harrod et Domar, influencés par Keynes, cherchent à comprendre les conditions dans lesquelles une phase d'expansion peut être durable. Ainsi, s'il ne propose pas à proprement parler une théorie de la croissance (expliquant son origine sur une longue période), le modèle de Harrod-Domar permet, néanmoins, de faire ressortir le caractère fortement instable de tout processus d'expansion. En particulier, il montre que pour qu'une croissance soit équilibrée — c'est-à-dire que l'offre de production augmente ni moins (sous-production) ni plus (surproduction) que la demande —, il faut qu'elle respecte un taux précis, fonction de l'épargne et du coefficient de capital (quantité de capital utilisée pour produire une unité) de l'économie. Or, il n'y a aucune raison que la croissance, qui dépend de décisions individuelles (en particulier des projets d'investissement des entrepreneurs), respecte ce taux. De plus, si la croissance est inférieure à ce taux, elle va avoir tendance non pas à le rejoindre, mais à s'en éloigner davantage, diminuant progressivement (en raison du multiplicateur d'investissement). La croissance est donc, selon une expression d'Harrod, toujours « sur le fil du rasoir »[réf. nécessaire].Ce modèle, construit après guerre et marqué par le pessimisme engendré par la crise de 1929, a toutefois été fortement critiqué. Il suppose, en effet, que ni le taux d'épargne, ni le coefficient de capital ne sont variables à court terme, ce qui n'est pas prouvé[réf. nécessaire].Robert Solow propose un modèle néoclassique de croissance. Ce modèle repose essentiellement sur l'hypothèse d'une productivité marginale décroissante du capital dans la fonction de production. Le modèle est dit néoclassique au sens où les facteurs de production sont utilisés de manière efficace et rémunérés à leur productivité marginale. Solow montre que cette économie tend vers un état stationnaire. Dans ce modèle, la croissance de long terme ne peut provenir que du progrès technique (et non plus de l'accumulation du capital).Si on pense que tous les pays convergent vers le même état stationnaire, alors le modèle de Solow prédit un phénomène de convergence : les pays pauvres devraient croître plus vite que les pays riches.L'une des faiblesses théoriques du modèle de Solow vient du fait qu'il considère le progrès technique comme exogène. Autrement dit, il ne dit rien sur la façon dont le progrès technique apparaît.Les théories de la croissance endogène cherchent à endogénéiser le progrès technique, c'est-à-dire à construire des modèles qui expliquent son apparition. Ces modèles ont été développés à partir de la fin des années 1970 notamment par Paul Romer, Robert E. Lucas et Robert Barro. Ils se fondent sur l'hypothèse que la croissance génère par elle-même le progrès technique. Ainsi, il n'y a plus de fatalité des rendements décroissants : la croissance engendre un progrès technique qui permet que ces rendements demeurent constants. Si tel est le cas, la croissance n'a donc plus de limite. À travers le progrès technique, la croissance constitue un processus qui s'auto-entretient.Ces modèles expliquent que la croissance engendre du progrès technique par trois grands mécanismes. Premièrement, le learning by doing : plus on produit, plus on apprend à produire de manière efficace. En produisant, on acquiert en particulier de l'expérience, qui accroît la productivité. Deuxièmement, la croissance favorise l'accumulation du capital humain, c'est-à-dire les compétences possédées par la main d'œuvre et dont dépend sa productivité. En effet, plus la croissance est forte, plus il est possible d'accroître le niveau d'instruction de la main-d'œuvre, en investissant notamment dans le système éducatif. D’une manière générale, la hausse du niveau d'éducation de la population – par des moyens publics ou privés – est bénéfique. Troisièmement, la croissance permet de financer des infrastructures (publiques ou privées) qui la stimulent. La création de réseaux de communication efficaces favorisent, par exemple, l'activité productive.« La principale [des] conclusions [de ces nouvelles théories] est qu'alors même qu'[elles] donnent un poids important aux mécanismes de marché, elles en indiquent nettement les limites. Ainsi il y a souvent nécessité de créer des arrangements en dehors du marché concurrentiel, ce qui peut impliquer une intervention active de l'État dans la sphère économique. »En particulier ce « retour de l'État » se traduit par le fait qu'il est investi d'un triple rôle : encourager les innovations en créant un cadre apte à coordonner les externalités qui découlent de toute innovation (par exemple"
médecine;""
médecine;"Un agent infectieux est un agent biologique pathogène responsable d'une maladie infectieuse.Les agents infectieux sont majoritairement des micro-organismes, notamment des bactéries et des virus. Cependant, certains agents pathogènes ne sont pas des organismes (les prions), d'autres ne sont pas microscopiques (les vers parasites).Le pouvoir pathogène d'un agent infectieux mesure sa capacité à provoquer une maladie chez un organisme hôte.La virulence d'un agent infectieux mesure sa capacité à se développer dans un organisme (pouvoir invasif) et à y sécréter des toxines (pouvoir toxique).Chez les humains, 1 415 espèces infectieuses ont été inventoriées,. Environ 600 sont connues pour le bétail et environ 400 pour les chats et les chiens. Les données concernant la faune sauvage sont trop parcellaires pour qu'un chiffre puisse être donné. Parmi ces agents infectieux on compte :des protéines : les prions (agents dits non conventionnels car exempts d'acide nucléique) ;des virus (217 espèces infectieuses) ;des organismes unicellulaires :procaryotes : certaines bactéries (538 espèces infectieuses),eucaryotes : les protozoaires parasites et certaines levures (66 espèces infectieuses) ;des organismes pluricellulaires :certaines moisissures (307 espèces infectieuses),les vers parasites ou helminthes (287 espèces infectieuses),certains arthropodes (acariens, pou).Le pouvoir pathogène (grec ancien πάθος [pathos], « souffrance » ; id. γένος [genos], « naissance ») — ou pathogénicité — d'une bactérie mesure sa capacité à provoquer des troubles chez son hôte. Il varie selon la souche (sérovar) et dépend de son pouvoir invasif (capacité à se répandre dans les tissus et à y établir un ou des foyers infectieux), de son pouvoir toxicogène (capacité à produire des toxines) et de sa capacité à se reproduire.On distingue trois catégories de bactéries pathogènes :les bactéries pathogènes strictes (ou spécifiques), qui provoquent des troubles quel que soit le patient (à l'exception des porteurs sains) ; par exemple : Salmonella Typhi et Vibrio cholerae ;les bactéries pathogènes opportunistes, qui provoquent des troubles lorsque les défenses immunitaires de l'hôte sont affaiblies ou que la personne est âgée (on parle aussi de sujets immunodéprimés) ; par exemple : Pseudomonas aeruginosa.les bactéries pathogènes occasionnelles, qui sont le plus souvent inoffensives mais dont certaines souches sont pathogènes. On retrouve dans cette catégorie des bactéries commensales comme Escherichia coli ou Staphylococcus aureus.Le pouvoir invasif d'une bactérie (ou d'une souche bactérienne) est sa capacité à se multiplier et à se répandre dans un organisme hôte, malgré les défenses immunitaires. Facteurs exogènes La température de l'eau, de l'air et du sol, le pH, le taux d'oxygène, la teneur en certains nutriments de l'environnement, l'exposition aux UV solaires, à certains toxiques ou polluants ou à la radioactivité, la présence d'un vecteur biologique, d'une lésion ou d'une primo-infection, etc. peuvent ou non favoriser l'agent infectieux.Par exemple le réchauffement climatique pourrait permettre à des microbes d'être plus présents et infectieux plus haut en altitude, plus près des pôles nord et sud ou plus fréquemment dans les eaux douces, estuariennes ou salines.La radioactivité ambiante, l'augmentation des UV induite par le trou de la couche d'ozone, la dispersion de biocides et d'antibiotiques dans l'environnement, ou encore l'exposition à l'ozone troposphérique pourraient être de nouveaux facteurs de mutation et donc d'apparition de souches plus agressives ou plus résistantes, ou de maladies émergentes. De même de nombreux agents mutagènes dispersés par l'Homme dans l'environnement (radionucléides, certains métaux lourds et divers produits chimiques) pourraient favoriser l'apparition de nouvelles souches pathogènes. Facteurs liés à la bactérie La constitution et le métabolisme de la bactérie définissent en partie son pouvoir invasif ; ainsi, celui-ci dépend :des facteurs d'adhésion : la présence de fimbriae (ou pili), d'adhésines, et/ou de glycocalyx facilite la fixation de la bactérie sur sa cellule cible ;de la résistance à la phagocytose grâce à la présence d'une capsule, mais aussi de la résistance aux enzymes lysosomales (censées détruire la bactérie phagocytée) ;de la production d'enzymes :les collagénases, qui dégradent le collagène des tissus conjonctifs (chez Clostridium perfringens par exemple),les coagulases, qui permettent la formation d'un caillot autour du corps bactérien, qui le protège des cellules du système immunitaire (chez S. aureus par exemple),les hyaluronidases, qui dégradent l'acide hyaluronique, constituant des tissus conjonctifs,les DNases (ou ADNases) qui dégradent l'ADN des cellules infectées,les kinases (ou fibrinolysines). Facteurs liés à l'hôte Le pouvoir invasif dépend également du terrain infecté (c'est-à-dire le milieu environnant la bactérie), à savoir :l'état du système immunitaire de l'hôte (une diminution des défenses immunitaires due à l'âge, à la fatigue, à la maladie, etc. favorise l'invasion) ;les facteurs physico-chimiques de l'environnement.Une toxine est une molécule synthétisée par un organisme vivant, ayant un effet nocif ou létal pour l'organisme hôte.Les toxines protéiques sont les poisons les plus actifs : 250 g de toxine (tétanique ou botulinique) suffirait à tuer toute la population humaine.Le pouvoir pathogène peut être quantifié par trois données : la dose minimale mortelle (DMM), la dose létale 50 (DL50) et la dose minimale infectante (DMI).La DMM est la dose la plus faible qui tue dans un délai déterminé un groupe expérimental.La DL50 est la dose qui tue dans un délai déterminé 50 % d'un groupe expérimental.La DMI est la dose minimale permettant la contamination et le développement de la maladie.Les toxines peuvent agir de plusieurs manières : sur le système immunitaire, en provoquant une allergie (effet allergène), ou encore un choc septique ; sur le système nerveux (effet neurotoxique) ; sur le système musculaire (effet myotoxique) ; sur le système reproductif (effet reprotoxique) ; etc. Une toxine peut agir seule ou en synergie avec d'autres. Selon la bactérie en cause et le mode de contamination, la production et l'action de la toxine se feront différemment.Dans ou autour d'une plaie, une bactérie peut se multiplier et libérer sa toxine protéique. Celle-ci peut agir localement et éventuellement à distance, au niveau de la moelle épinière, en diminuant la quantité de neuromédiateurs libérés, et au niveau des synapses neuromusculaires en augmentant la libération d'acétylcholine. Elle provoque une paralysie de contracture. Exemple : toxine tétaniqueÀ la suite d'une ingestion ou inhalation ou de son développement dans le tube digestif, la toxine passe dans le sang et diminue la quantité d'acétylcholine au niveau des jonctions neuromusculaires. Elle provoque une paralysie flasque.Exemple : toxine botuliniqueÀ la suite d'une ingestion, la bactérie adhère à l'épithélium intestinal et produit la toxine qui se fixe sur les entérocytes. Elle empêche l'absorption des ions Na+ et Cl−, et provoque donc une fuite hydrominérale.Exemple : toxine cholériqueLes toxines protéiques ont souvent un pouvoir toxique très élevé. Elles provoquent l'apparition d'anticorps dans l'organisme : les anti-toxines.Certaines peuvent être transformées en anatoxines par un traitement au formol, et une incubation à 40 °C (Méthode de Ramon). Ces anatoxines sont utilisées pour :fabriquer des vaccins ;fabriquer des sérums utilisés en sérothérapie.Certaines souches d'Escherichia coli sont sources de diarrhées, d'infections urinaires, d'infections nosocomiales, de septicémies, de la méningite du nouveau-né ; la souche EHEC O157 est une source du syndrome hémolytique et urémique.Staphylococcus aureus, Pseudomonas aeruginosa sont sources d'infections des yeux, de plaies et de la gastro-entérite aiguë).La salmonelle, Yersinia enterocolitica provoquent des gastro-entérites.Campylobacter jejuni est devenue la première source de pathologies entériques bactériennes dans les pays industrialisés.Vibrio cholerae cause le cholera.Shigella dysenteriae cause des dysenteries.L'eau épurée doit donc être débarrassée de ces germes pathogènes lorsqu’elle est rejetée dans le milieu naturel pour ne pas contaminer celui-ci et causer une épidémie pouvant être mortelle au sein des populations en aval.Groupe 1 : non pathogènes pour l'homme.Groupe 2 : pathogènes pour l'homme, contagiosité faible, prophylaxie et traitement.Groupe 3 : pathogènes pour l'homme, contagiosité, prophylaxie ou traitement.Groupe 4 : pathogènes pour l'homme, forte contagiosité, sans prophylaxie ni traitement.(en) « Que devient une bactérie pathogène après avoir tué son hôte ? » Gazettelabo 13 avril 2012, sur PLOSToxinePlasmide et Conjugaison bactérienneCulture axéniqueConservation de la viandeVarioleRessource relative à la santé : (no + nn + nb) Store medisinske leksikon  Portail de la médecine   Portail des maladies infectieuses   Portail de la microbiologie"
médecine;"Une anesthésie locale  consiste à inhiber de façon réversible la propagation des signaux le long des nerfs. Si cette anesthésie est réalisée au niveau de voies nerveuses spécifiques, elle est susceptible de produire des effets tels que l'analgésie (diminution de la sensation de douleur) et la paralysie (perte de puissance du muscle). Elle s'oppose à l'anesthésie générale où le patient est endormi.En 1884, Carl Köller utilise déjà la cocaïne pour l’anesthésie par contact en ophtalmologie et en otorhinolaryngologie. La même année, Richard Hall inaugure son emploi en chirurgie dentaire et William Halsted introduit la technique du « bloc nerveux ».En le greffant sur d’autres alcaloïdes, comme la quinine ou la morphine, Filehne démontre que c’est le noyau benzoyle de la cocaïne qui est responsable de son activité anesthésique. Mais ces esters benzoïques, tous actifs, restent trop irritants pour être utilisables.Entrepris sur la base du modèle moléculaire proposé pour la cocaïne par Alfred Einhorn en 1892, les travaux de Georg Merling à Berlin aboutissent à la commercialisation de la bêta-eucaïne par Schering AG. Mais c’est Richard Willstätter, élève d’Einhorn, qui, en 1898, élucide définitivement la structure de la cocaïne, qu’il synthétise en 1901. La fonction phénolique liée à une fonction carboxylique estérifiée s’étant révélée essentielle, Paul Ehrlich met alors au point l’orthoforme puis le néoorthoforme, auxquels leur fonction phénolique prête aussi une action antiseptique. En 1902, E. Ritsert, cherchant à sa benzocaïne (Anesthésine) des dérivés plus solubles, parvient à la Nirvanine, immédiatement rendue obsolète par l’arrivée de nouvelles molécules.En effet, dès l’année suivante Ernest Fourneau, directeur des recherches chez Poulenc frères, revenant d’Allemagne où il a travaillé avec Willstätter, synthétise la Stovaïne, premier substitut non irritant de la cocaïne en anesthésie locale. Un an plus tard, les laboratoires Hoechst commercialisent la Novocaïne synthétisée par Einhorn et qui sera pendant des décennies le principal des anesthésiques locaux.Parallèlement à ces découvertes, des étapes essentielles dans le développement de l’anesthésie locale ont été franchies. Elles ont abouti aux techniques d’anesthésie locorégionale.À la suite des travaux de l’Allemand Heinrich Braun, puis des Anglais George Oliver et Edward Sharpey-Schafer en 1894, et enfin des Américains John Jacob Abel et Albert Cornelius Crawford en 1898, l’adrénaline est introduite en anesthésie locale comme vasoconstricteur pour ralentir l’élimination du médicament et compléter ainsi les effets du garrottage, pratiqué par James Leonard Corning dès 1885.Mais l’adrénaline reste insuffisamment efficace et le garrottage n’est utilisable que sur les territoires facilement accessibles. S’appuyant sur les observations faites par Edward Feinberg en 1886, et que François-Franck a reprises en 1887 pour établir que « le contact direct d’une solution de cocaïne avec un tronc nerveux détermine l’abolition des propriétés fonctionnelles de ce nerf », Corning et Oberst inaugurent alors la technique de l’anesthésie locorégionale : au lieu d’agir dans la région concernée, ils opèrent sur le nerf correspondant. Enfin, les Français Jean Anasthase Sicard et Fernand Cathelin mettent au point, en 1901, l’anesthésie péridurale en injectant le médicament dans le Liquide cérébrospinal,.Il existe différents types d'anesthésies locales :anesthésie topique = de surface : l'anesthésique sous forme de gel ou pommade est déposé sur la muqueuse.anesthésie par infiltration : l'anesthésique est déposé à proximité du ou des nerfs à endormir, grâce à une aiguille.Les anesthésies locorégionales, plus efficaces que la simple anesthésie locale, anesthésient un nerf ou un territoire donné, souvent plus large que la zone chirurgicale concernée. Ces techniques permettent d'effectuer des chirurgies de plus grande envergure. Elles nécessitent des doses d'anesthésiques locaux modérées pour une grande efficacité. Les différents types d'anesthésies locorégionales sont:bloc tronculaire : consiste à infiltrer un tronc nerveux pour obtenir l'anesthésie de son territoire ; par exemple le bloc du nerf cubital entraine l'anesthésie du bord interne de la main.bloc plexique : consiste à infiltrer un plexus (ensemble de nerfs) pour obtenir une anesthésie d'une région entière. Par exemple, l'infiltration du plexus brachial entraine une anesthésie de tout le membre supérieur.blocage épidural (ou infiltration de l'espace péridural). En fonction du niveau infiltré peut donner une anesthésie de la moitié inférieure du corps, ou simplement de plusieurs métamères (péridurale suspendue)rachianesthésie : injection d'anesthésique local dans le liquide céphalorachidien, donne une anesthésie de la moitié inférieure du corps.Les techniques d'anesthésie locorégionale font appel à l'utilisation de neurostimulateurs afin de faciliter le repérage des nerfs et d'améliorer le pourcentage de succès de ces anesthésies. L'utilisation du repérage des nerfs par échographie (technique indolore et beaucoup plus confortable pour le patient) est en pleine expansion et semble être la technique d'avenir.Il existe de nombreux anesthésiques locaux. Jusqu'à la mise au point de la Stovaïne et de la Novocaïne, la cocaïne avait été pratiquement seule en usage. En 1946, Löfgren introduisit la lidocaïne. Puis vinrent la scandicaïne, la prilocaïne, l'étidocaïne et la bupivacaïne. Les plus modernes sont la ropivacaïne, la lévobupivacaïne, l'articaïne et la mépivacaïne.péri-apicales, para apicales ou supra-périostée : les plus fréquentes. L'anesthésique est déposé en regard de l'apex de la dent sur la face externe du périoste. Le produit diffuse au travers de la corticale pour rejoindre l'apex. Cette anesthésie se fait à la jonction gencive attachée/gencive libre. Il faut piquer avec une angulation de 45° et faire glisser le biseau en direction apicale jusqu'au-dessus de l'apex. Le point d'infiltration sera toujours distal par rapport à la dent. Utilisé pour toutes les dents maxillaires, et les dents mandibulaires antérieures (jusqu'à la première prémolaire). Au-delà son efficacité est relative. Elle se caractérise par un engourdissement plus ou moins important de la lèvre au niveau du site d’injection. Une para- apicale n'est, si elle est bien faite, pas douloureuse.intraseptales : l'anesthésique est déposé à l'intérieur de l'os alvéolaire, dans la crête osseuse. La solution diffuse, par capillarité, dans l'os spongieux et atteint l'extrémité radiculaire. On vise le milieu de la papille pour entrer dans la table osseuse. Il faut tarauder l'os avec une aiguille courte, bipointe car il faut forcer pour atteindre le septum. Cette technique peut être indifféremment utilisée à la mandibule et au maxillaire. Elle pose quelques problématiques :il faut passer la corticale,il y a risque de torsion et d'obstruction de l'aiguille,c'est une anesthésie de courte durée,s'il y a peu d'os spongieux et beaucoup d'os cortical, la vasoconstriction peut générer une nécrose du septum.Il faut y avoir recours en dernier lieu.intraligamentaires : l'anesthésique est déposé à l'intérieur du desmodonte ou ligament alvéolodentaire. l'aiguille va passer entre le septum inter-dentaire et la racine. L'anesthésie arrive par toutes petites doses mais sous pression. Le biseau de l'aiguille est toujours tourné vers la racine. C'est une anesthésie de courte durée (15 minutes) donc pour des soins conservateurs courts. C'est une technique contre-indiquée pour les gros actes chirurgicaux (exodontie) et lésions parodontales car la vascularisation locale est perturbée et il y a un risque d'alvéolite.intrapulpaires : l'anesthésique est déposé dans la pulpe de la dent. Utilisée en complément, notamment lors d'actes endodontiques (dévitalisation). Cette technique a pour caractéristique d’être douloureuse.anesthésie loco-régionale (ou « tronculaire » ou « anesthésie à l'épine de Spix ») : permet d'endormir les molaires mandibulaires (impossibles à endormir par une para-apicale). On anesthésie le nerf mandibulaire (V3) avant qu'il n'entre dans l'os mandibulaire. On obtient une anesthésie des molaires et prémolaires, qui s’accompagne généralement d’une perte de sensibilité et de motricité de plusieurs heures de la lèvre du côté où a été faite l'infiltration. Cette anesthésie est souvent utilisée pour l’extraction des dents de sagesse. Elle nécessite un peu d’attente pour faire effet.anesthésie transcorticale : l'anesthésique est déposé dans l’os spongieux qui entoure la dent. Cette technique est immédiate et permet d’anesthésier 2 à 6 dents sans gêne postopératoire pour le patient.anesthésie ostéocentrale : l’anesthésique est également déposé dans l’os spongieux, mais à proximité immédiate des apex. De ce fait, l’anesthésie est instantanée et très efficace. Elle peut être utilisée sur tous les secteurs et permet d’obtenir l’anesthésie de 2 à 8 dents, sans engourdissement de la lèvre et sans suite postopératoire.Repose sur le principe de modification des perméabilités membranaires de l'axone.Dans la carpule, l'anesthésique est sous forme non ionisée donc inactif mais diffusible. Il ne s'active qu'en milieu acide (intérieur de l'axone).Donc lorsque le produit est injecté, il est inactif (car le milieu n'est pas acide) mais se diffuse jusqu'à l'axone qu'il pénètre grâce aux canaux sodiques. Une fois rentré, il est en milieu acide, s'ionise donc s'active mais n'est plus diffusible. Il bloquera les récepteurs sodiques, ce qui entraînera la perte de l'excitabilité de la fibre nerveuse et l'abolition de la conduction du potentiel d'action.Lors de phénomènes inflammatoires, le milieu dans lequel on injecte l'anesthésique est déjà acide, le produit est donc immédiatement ionisé, activé et ne diffuse pas jusqu'à l'axone, donc l'anesthésie ne prend pas. D'où l’intérêt dans ces cas là de passer de l'anesthésie locale à locorégionale.La section « Histoire » a pour source principale :François Chast, « De Freud à la péridurale : Anesthésie locale », dans Histoire contemporaine des médicaments, La Découverte, 2002 Portail de la médecine"
médecine;"La douleur est une « expérience sensorielle et émotionnelle désagréable », une sensation subjective normalement liée à un message, un stimulus nociceptif transmis par le système nerveux. D'un point de vue biologique et évolutif, la douleur est une information permettant à la conscience de faire l'expérience de l'état de son corps pour pouvoir y répondre. On distingue principalement deux types de douleur, aiguë et chronique :la douleur aiguë correspond à un « signal d'alarme » de l'organisme pour inciter à une réaction appropriée en cas de remise en cause de son intégrité physique, soit par un traumatisme (brûlure, plaie, choc), soit par une maladie ;la douleur chronique, l'installation durable de la douleur, est considérée comme une maladie qui peut notamment être le signe d'un dysfonctionnement des mécanismes de sa genèse, on parle alors de douleur neurogène ou psychogène.Cette sensation, de désagréable à insupportable, n'est pas nécessairement exprimée. Pour l'identifier chez autrui on peut faire le diagnostic de la douleur en se référant à des effets observables, par exemple les mouvements réflexes de retrait au niveau des membres et des extrémités pour les douleurs aiguës, ou des changements de comportement, d'attitudes et de positions du corps pour les douleurs chroniques.Les traitements de la douleur sont multiples, les études sur le sujet pour une meilleure compréhension se poursuivent, en particulier pour la reconnaître quand elle n'est pas exprimée. Ainsi la douleur de l'enfant ne l'est pas toujours, la douleur chez le nouveau-né étant même officiellement inexistante jusqu'à la démonstration du contraire en 1987, et son identification dans le Règne animal reste un sujet de recherche.Une définition de référence de la douleur a été donnée en 1979 par L'IASP (International Association for the Study of Pain) :La douleur apparait ainsi comme une expérience subjective. C'est un événement neuropsychologique pluridimensionnel. Il convient alors de distinguer :la composante sensori-discriminative qui correspond aux mécanismes neurophysiologiques de la nociception. Ils assurent la détection du stimulus, sa nature (brûlure, décharges électriques, torsion, etc.), sa durée, son évolution, son intensité, et l’analyse de ses caractères spatiaux ;la composante affective qui exprime la connotation désagréable, pénible, rattachée à la perception douloureuse. La représentation mentale de la douleur chronique (les états mentaux aversifs provoqués par les émotions causées par la douleur) serait chargée d'une valeur négative capable de transformer les états neuronaux ;la composante cognitive référant à l’ensemble de processus mentaux qui accompagnent et donnent du sens à une perception en adaptant les réactions comportementales comme les processus d’attention, d’anticipation et de diversion, les interprétations et valeurs attribuées à la douleur, le langage et le savoir sur la douleur (sémantique) avec les phénomènes de mémoire d’expériences douloureuses antérieures personnelles (mémoire épisodique) décisifs sur le comportement à adopter.Beecher en 1956 a démontré l’influence de la signification accordée à la maladie sur le niveau d’une douleur. En étudiant comparativement deux groupes de blessés, militaires et civils, qui présentaient des lésions identiques en apparence, il a observé que les militaires réclamaient moins d’analgésiques. En effet, le traumatisme et son contexte revêtent des significations tout à fait différentes : comparativement positives pour les militaires (vie sauve, fin des risques du combat, bonne considération du milieu social, etc.), elles sont négatives pour les civils (perte d’emploi, pertes financières, désinsertion sociale, etc.) ;la composante comportementale qui correspond à l’ensemble des manifestations observables :physiologiques (paramètres somato-végétatifs ex. : pâleur),verbales (plaintes, gémissements…),motrices (immobilité, agitation, attitudes antalgiques).En 1994 L'IASP propose cinq critères distincts de classification :la région du corps impliquée : l'abdomen, membre inférieur…le système dont la dysfonction cause la douleur : digestif, nerveux…la durée et la fréquence ;l'intensité et la durée depuis le début ;l'étiologie.Dans les voies nerveuses de la nociception on distingue le circuit de la perception et celui de la régulation :les voies nociceptives ascendantes, comme toutes les voies nerveuses sensitives, véhiculent l'information de la périphérie du corps vers le cortex cérébral en passant par la moelle épinière ;les voies descendantes, à l'inverse, portent un message depuis le cortex vers la périphérie, à la rencontre du message nociceptif dont elles peuvent alors limiter l'intensité en agissant sur les voies ascendantes.Le rôle de ces circuits descendant est le rétrocontrôle, ici la régulation de l’intensité du message sensitif afin de moduler la sensation douloureuse. Ce mécanisme inhibiteur est aussi appelé théorie de la porte ou gate control et est notamment utilisé dans le contrôle inhibiteur diffus.Ces voies nociceptives transmettent l'information du stimulus nociceptif grâce à des mécanismes électrobiochimiques faisant intervenir de nombreuses molécules, dont des acides aminés. La douleur est véhiculée en premier lieu par les fibres A-delta qui conduisent le message nocicepteur à une vitesse de 15 à 30 m/s.La vulnérabilité à la douleur ou la sensibilité à l'« effet placebo » dépendent en partie de facteurs génétiques qui contrôlent le système dopaminergique du cerveau, lequel est en cause dans l'anticipation de la douleur et de la confiance en la guérison. De même pour la production par le cerveau lui-même de certains opiacés naturels (les endorphines) jouant un rôle de neurotransmetteur.La douleur compte trois grands mécanismes de genèse (qui peuvent se combiner) : la douleur de nociception, la douleur neurogène et la douleur psychogène.La douleur nociceptive est générée par un récepteur spécifique, un nocicepteur, dont le rôle est de signaler les atteintes à l'intégrité de l'organisme. C'est un signal d'alarme normal, et même souhaitable dans la mesure où il induit une attitude appropriée dont l'absence est potentiellement dangereuse pour l'organisme.La douleur neurogène est générée par le nerf lui-même et non un récepteur spécifique, c'est donc une pathologie nerveuse classée parmi les neuropathie. Elle est ressentie comme des décharges électriques, des élancements, des sensations de brûlures, des sensations de froid douloureux et des picotements dans le territoire des nerfs atteints. C'est aussi la douleur que ressentent les malades amputés et en particulier la sensation perçue dans un membre qui a disparu (membre fantôme).La douleur psychogène est générée par le psychisme, mais n'est pas imaginaire, elle est réellement ressentie par l'individu mais existe en l'absence de lésion. Les mécanismes physiologiques de ces douleurs, étudiés par la psychopathologie, ne sont pas encore clairement compris. L'utilisation d'antalgique semble dans ce cas inefficace.Outre le sentiment de souffrance, la douleur peut provoquer un malaise vagal par stimulation des nerfs vagues (nerfs pneumogastriques). Les symptômes de cette excitation vagale sont tout ou partie des signes incluant notamment une baisse du débit sanguin par bradycardie et hypotension ; une syncope ; un myosis (diminution du diamètre des pupilles par contraction de l'iris) ; une transpiration aux extrémités des membres ; une sécrétion excessive de salive ; une hyperchlorhydrie (excès de sécrétion d'acide chlorhydrique par la muqueuse de l'estomac) ; une constipation ou des diarrhées ; des spasmes et des troubles de la respiration.La douleur prolongée est inhibée par le corps par sécrétion d'endorphines (ou endomorphines). La production d'endorphine se fait initialement aux niveaux des nerfs proches du siège de la douleur ; lorsque cette production ne suffit plus (douleur prolongée), c'est un site plus proche du cerveau qui prend le relais dans la sécrétion. La douleur revient ainsi par vagues.Les états de douleur sont le résultat de la sélection naturelle. La souffrance peut être un trait adaptatif et améliorer la capacité de survie d’un individu.L'évaluation et le diagnostic de la douleur étant complexe, l'IASP précise que « L'incapacité à communiquer verbalement n'infirme pas la possibilité que l'individu éprouve de la douleur et nécessite un traitement approprié pour soulager la douleur. La douleur est toujours subjective… » L'OMS (Organisation Mondiale de la Santé) le précise bien dans ces recommandations en ce qui concerne la douleur chez l'enfant car elle est communément sous estimée.Divers organismes définissent le cadre sémantique, répertorient les connaissances physiologiques, et livrent des recommandations de traitement souvent liées aux différentes classes d'âges. Par exemple dans la francophonie, on peut citer l'INSERM sur l'aspect scientifique, le CNRD pour l'archivage des informations, la SFETD pour l’exploration médicale des voies de traitement, ou encore l'AQDC au sujet de la douleur chroniqueCependant, malgré l'émergence de moyens techniques, le diagnostic reste malaisé car il existe une tendance naturelle[Information douteuse] à se protéger de la perception de la douleur d'autrui[réf. nécessaire], c'est entre autres la raison de la mise en place d'échelles d'évaluation de la douleur.En 2014, un moyen technique pour « mesurer » la douleur relativement à la dilatation réflexe de la pupille est en cours d'évaluation. La pupillométrie permettrait d'adapter au mieux les traitements anti-douleur, en particulier chez les personnes endormies où la dilatation de la pupille n'est pas sensible à d'autres facteurs, comme le stress, mais son évaluation sur les enfants semble donner de bons résultats.D'après Nicolas Danziger,la vision de la douleur chez l'autre crée une « émotion aversive » par un mécanisme dit de « résonance émotionnelle ». Mais il précise que ce mécanisme connaît des lacunes, par exemple en cas de différence ethnique ou religieuse, que d'autre part il peut donner lieu à « une volonté de fuite ou d’éloignement de celui qui souffre », et que « de nombreux travaux scientifiques ont démontré, ces dernières années, que le corps médical avait encore tendance à mésestimer la douleur des patients ».On trouve ainsi aisément les preuves d'un déni collectif de la douleur chez autrui en particulier en ce qui concerne la douleur chez l'enfant qui est largement sous-estimée en milieu hospitalier. Daniel Annequin affirme même : « Chez l'enfant on revient de loin, pendant des années on a voulu ignorer que l'enfant ressentait de la douleur […] On disait que les fibres C n’étaient pas myélinisées, mais elles ne sont jamais myélinisées, on avait comme ça tout une série d'argumentaires pseudo-scientifiques ». Et en effet la démonstration scientifique de la capacité du nourrisson à ressentir la douleur n'a été faite qu'en 1987, et donc sa prise en charge au préalable n'existait qu’exceptionnellement, même pour les interventions les plus lourdes.Ce relatif refus de voir la douleur de l'autre n'est ni propre au milieu médical ni universel comme le montre une étude issue du plan douleur 2006 qui distingue deux types d'attitudes réparties aussi bien chez les soignants que chez les parents : les « réservés » et les « sensibilisés », chacun reprochant respectivement à l'autre groupe le trop ou le trop peu de prise en charge, par « sensiblerie » ou par « déni ». Cette distinction entre en résonance avec celle d'une autre étude sociologique qui divise les médecins également en deux groupes : les « compatissants » et les « négateurs »,.La perception de la douleur, de son intensité, est subjective. Le même phénomène (traumatisme, maladie) sera ressenti différemment selon la personne et selon la situation. La douleur peut aller d'une simple incommodation jusqu'à un malaise, voire la mise en danger du pronostic vital ou psychiatrique de la personne.L'évaluation pour l'autre est donc complexe, c'est pourquoi on s'appuie de préférence sur le témoignage grâce à un support d'auto-évaluation et des échelles d'évaluation de la douleur spécifiques quand c'est impossible ou insuffisant. Auto-évaluation L'auto-évaluation consiste à demander directement à la personne souffrante le niveau de sa douleur. Elle nécessite une coopération et une bonne compréhension, et s'appuie sur des échelles médicales standardisées (numériques, visuelles analogiques, verbales simples et verbales relatives…).L'auto-évaluation n'est pas qu'une évaluation de la douleur, c'est également une manière de communiquer avec l'équipe médicale. Dans le cas de douleurs chroniques notamment, la cotation de la douleur n'indique pas uniquement la douleur ressentie, mais globalement l'altération de la qualité de vie et la détresse émotionnelle. Hétéro-évaluation Il existe aussi des échelles d'évaluation spécifiques fondées sur l'observation du comportement du patient. Contrairement aux échelles d'auto-évaluation elles ne nécessitent pas la participation du patient et sont de ce fait recommandées dans l'évaluation de la douleur chez les personnes chez qui l'auto-évaluation pose problèmes pour différentes raisons.En plus du témoignage de l'entourage qui peut évaluer les différences au quotidien, les changements survenus, il existe des échelles d'évaluations spécifiques comme l'échelle San Salvadour. Enfants et nourrissons Le signe habituel de l'expression de la douleur pour le petit enfant est le cri que le ou les parents arrivent souvent à distinguer des autres cris (peur, faim…), mais à un stade supérieur de douleur, le nourrisson est souvent prostré.Plusieurs échelles existent, bien que peu utilisées en pratique, il s'agit de la grille DESS (Douleur enfant San Salvadour), de l'échelle NCCPC (Non communicating children’s pain checklist) ou GED-DI (Grille d’évaluation de la douleur déficience intellectuelle) et de l'échelle EDINN (Échelle de douleur et d'inconfort du nouveau-né et du nourrisson). Le problème principal de ces échelles et qu'elles comportent des items longs à répertorier et ne sont pas utilisables en urgence. Personne âgée Chez les personnes âgées, et notamment atteintes de troubles cognitifs comme la maladie d'Alzheimer, on peut utiliser l’échelle Algoplus, et fréquemment utilisée, l'échelle ECPA.Il existe deux grands types de douleurs : les douleurs par excès de nociception, et les douleurs neuropathiques.La douleur par excès de nociception est déclenchée par une stimulation des récepteurs nociceptifs. L'examen neurologique est par ailleurs normal.La douleur neuropathique est une douleur causée par une lésion des voies sensitives du système nerveux, central ou périphérique. Elle est généralement associée à une dysesthésie et une allodynie. Il peut exister une zone gâchette déclenchant les douleurs. Les douleurs neuropathiques peuvent être à type de brûlure, de froid, ou de sensations de piqûres, et s'accompagner de sensations de fourmillement.On parle de douleur discrète ou aiguë, éventuellement chronique ou récidivante, mais « habituellement, la douleur est divisée en deux catégories en fonction de la durée ».La douleur aiguë est une douleur vive immédiate, et généralement brève. Elle est causée par une stimulation nociceptive de l'organisme, telle une lésion tissulaire, pouvant se produire sous la forme d'un stimulus thermique (contact de la peau avec du feu) ou mécanique (un pincement, un coup). « La douleur aiguë joue donc un rôle d’alarme qui va permettre à l’organisme de réagir et de se protéger face à un stimulus mécanique, chimique ou thermique. »Sa fonction d'alerte est alors justifiée, ce qui n'est plus nécessairement le cas avec une douleur chronique.« La douleur est dite chronique ou pathologique, lorsque la sensation douloureuse excède trois mois et devient récurrente. »La douleur chronique est une maladie grave et invalidante. Les conséquences des douleurs chroniques sont autant organiques (hypertension artérielle secondaire, atrophie musculaire) que psychologiques, avec une modification comportementale pouvant aller de l'anxiodépression jusqu'à des troubles de la dépersonnalisation avec risque suicidaire accru.Plusieurs sociétés savantes, dont la Société française d'étude et de traitement de la douleur (SFETD), l'Association internationale d'étude de la douleur ou la Société internationale de neuromodulation, soulèvent l'importance de la douleur chronique dans la population générale ; de 15 à 25 % de la population seraient victimes de douleurs chroniques.Les douleurs chroniques sont principalement des douleurs neuropathiques dans le cadre de maladies générales avec une atteinte du système nerveux. Par exemple le diabète insulinique génère principalement une destruction des nerfs périphériques avec une hypoesthésie, mais dans certains cas, l'atteinte des nerfs périphériques va tendre vers un état d'hyperesthésie. Les atteintes post-opératoires des nerfs périphériques sont aussi parmi les principales causes de douleurs neuropathiques. En fait, toute atteinte d'un nerf périphérique ou atteinte d'une structure du système nerveux central peut s'exprimer par des douleurs neuropathiques chroniques. Le mécanisme de ces douleurs est actuellement basé sur la perte du gate control (Le gate control est schématiquement l'inhibition des voies nociceptives Aδ et C par les grosses fibres sensitives-motrices). Pathologies en cause Il est difficile de dresser une liste complète des syndromes douloureux chroniques comptant par exemple :les migraines et les syndromes migraineux réfractaires ;les céphalées cervico-géniques ;le Failed Back Surgery Syndrome ou « syndrome d'échec de chirurgie du dos » ou encore syndrome post-laminectomie (en) ;les douleurs neuropathiques post-opératoires chroniques ;les douleurs neuropathiques ;les douleurs de l'artérite ;Les syndromes douloureux régionaux complexes de type I et II ;les douleurs du membre fantôme ;les douleurs des pathologies ostéo-articulaires chroniques ;les lombalgies chroniques ;l'algie vasculaire de la face ;etc.Par exemple on dénombre 150 000 personnes en France qui souffrent de migraines réfractaires ou rebelles au traitement et à peu près le même nombre de personnes souffrant de céphalées cervicogéniques.Les autres mécanismes de douleur chronique sontdes douleurs inflammatoires par hyperstimulation des voies nociceptives sans atteinte directe de celles-ci ;les douleurs mécaniques par destruction des articulations ;l'ischémie d'origine vasculaire avec une composante neuropathique par ischémie des nerfs des membres. Effets Les douleurs chroniques, quelles que soient leurs origines qui peuvent être multiples, vont amputer de façon plus ou moins profonde et intense la sphère comportementale par atteinte de l'activité physique, le sommeil, la concentration et les fonctions cognitives[réf. souhaitée] (schématiquement par manque de sommeil réparateur). Progressivement le comportement va être modifié vers des signes de dépression avec anxiété, agressivité envers l'entourage, pouvant aller jusqu'à de réels troubles dépressifs majeurs et une dépersonnalisation de la personne. Parallèlement la personne atteinte de douleur chronique peut se désocialiser (arrêts de travail itératifs, fin de droits…) tout en ayant éventuellement l'image de quelqu'un ayant acquis certains « bénéfices secondaires » durant la période de chronicisation de la douleur. Douleur cancéreuse Une autre forme de douleur dite chronique est la « douleur cancéreuse » qui est liée soit au cancer lui-même soit aux conséquences des traitements, qui peuvent induire des douleurs neuropathiques ou compressives en fonction du mécanisme.La forme la plus rare de douleur chronique est la douleur sine materia qui est un diagnostic d'élimination. C'est une douleur qui n'a pas d'origine organique apparente. Ce diagnostic ne devrait être évoqué que devant une douleur dont les explorations complémentaires morphologiques (IRM, TDM) et neurophysiologiques (électromyogrammes, électroneurogrammes, potentiels évoqués somesthésiques) sont et restent normales.Lors de l'examen médical des muscles, en particulier en médecine du sport, ces différents temps de l'examen permettent de faire la distinction entre les différentes pathologies possibles.Le médecin examinant recherchera par l'interrogatoire ainsi que par l'examen clinique à individualiser certains types particuliers de douleurs musculaires qui peuvent orienter vers leurs causes qui peuvent être des accidents sportifs, ou bien certaines maladies bien individualisées qui se manifestent par différentes types de douleurs musculaires.Si la douleur musculaire est présente à l'effort. L'arrêt de l'effort physique ou la baisse de son intensité fait diminuer ou disparaître la douleur. Elle est présente au repos, lorsque les muscles sont ""froids"". La palpation du muscle concerné provoque ou augmente la douleur : rictus douloureux sur le visage du sujet examiné, réaction de retrait. La contraction volontaire provoque ou augmente la douleur. L'étirement du muscle provoque ou augmente la douleur[réf. nécessaire].L'inflammation : la douleur inflammatoire est plus importante le soir et en début de nuit (lorsque le taux sanguin de cortisol naturel est au plus bas). Elle diminue ou disparaît après échauffement et à l'effort (activité professionnelle ou sportive) : douleur de dérouillage.La douleur mécanique est constante, ne diminue pas voire s'accentue à l'effort. Elle n'augmente pas le soir, ni en début de nuit, et diminue lorsque la mobilisation s'arrête.Certaines toxines bactériennes, végétales, fongiques ou animales (venins) peuvent être sources de vives douleursIl existe différents types de traitements tels que médicamenteux, chirurgicaux, psychologiques.Le traitement inadéquat de la douleur est très répandu à travers le domaine chirurgical et dans le domaine hospitalier et d'urgence en général,,,,,,. Cette négligence s'étend depuis toute époque. Les Africains et Latino-américains seraient les plus nombreux à souffrir entre les mains d'un médecin,; et la douleur chez les femmes est moins traitée que chez les hommes. Quant à la douleur chez l'enfant, et en particulier chez les plus petits, elle a été niée officiellement et scientifiquement jusqu’au milieu des années 1980, ces derniers étant régulièrement opérés sans anesthésie.La réaction à la douleur est utilisée pour évaluer l'état neurologique d'un patient, et notamment son état de conscience. Il fait partie du bilan des secouristes ainsi que de l'échelle de Glasgow. Si la victime n'a pas de réaction spontanée, ni au bruit ou au toucher, sa réaction à la douleur est testée. Il convient d'exercer une stimulation qui ne cause pas de blessure ni d'aggravation de l'état. Plusieurs méthodes peuvent être employées.Un pincement de la peau a longtemps été pratiqué ; celui-ci doit être évité. Sur une personne consciente, un léger pincement aux extrémités est utilisé (dos de la main ou dessus du pied, face interne du bras) pour vérifier si la personne ressent ce qui lui est fait, mais pas comme méthode de stimulation d'une personne sans réaction. Une pression avec les doigts sur l'arrière de la mâchoire inférieure (nomenclature internationale = mandibule), sous les oreilles et une pression appuyée au niveau sus-orbitaire.La douleur est la principale cause de visite dans les milieux hospitaliers dans 50 % des cas, est une pratique de visite présente dans 30 % des familles. De nombreuses études épidémiologiques de différents pays rapportent une prévalence élevée de douleur chronique présente chez 12-80 % de la population. Elle devient plus évidente à l'approche du décès chez les individus. Une étude de 4 703 patients affirme que 26 % des patients souffrant de douleurs durant les deux dernières années de leur vie, guérissent à 46 % le mois d'après.Une enquête de 6 636 enfants (âgés entre 0–18 ans) affirme que, sur 5 424 enfants interrogés, 54 % ont fait l'expérience de douleurs durant les trois derniers mois. Un quart d'entre eux rapportent qu'ils font l'expérience de douleurs présentes ou prolongées depuis trois mois voire plus, et un tiers d'entre eux rapportent qu'ils font l'expérience de douleurs fréquentes et intenses. L'intensité des douleurs chroniques était plus élevée chez les filles, et la douleur chronique augmente chez les filles âgées entre 12 et 14 ans.La perception de la douleur peut être augmentée ou diminuée selon le contexte et l'état d'esprit et/ou par certains médicaments. Sans médication, la nociception dépend fortement du type de douleur, du contexte et de la culture du patient. Dans un contexte rassurant, ou au contraire très difficile (situation de guerre, d'accident, de catastrophe) l'intensité de la douleur peut diminuer.Expérimentalement, on montre que la simple présence de plantes vertes dans une chambre diminue l’intensité perçue d'une douleur et de l'état psychologique du patient.Une hypothèse est qu'en fonction de la personnalité et du contexte, chacun peut plus ou moins dramatiser sa douleur (c'est-à-dire tendre à décrire son expérience de douleur en termes plus exagérés que ne le ferait une personne moyenne), ce qui influe durablement sur la perception de la douleur par le sujet qui modifi l'attention qu'il y porte, l'anticipation de la douleur attendue, en augmentant les réponses émotionnelles à cette douleur. La douleur n’est pas du tout considérée ni prise en compte de la même manière selon les cultures, les religions ou les époques. Chaque peuple a sa propre conception de la douleur, et plus généralement de la souffrance. Cette notion s’applique aussi bien aux bénéficiaires de soins qu’aux valeurs des soignants. En effet, « ce ne sont pas seulement les malades qui intègrent leur douleur dans leur vision du monde, mais également les médecins et les infirmières qui projettent leurs valeurs, et souvent leurs préjugés, sur ce que vivent les patients dont ils ont la charge. ».La prise en charge de la douleur peut s’expliquer par le fait que « (…) la pratique quotidienne d’actes douloureux oblige le soignant à mettre en œuvre un certain nombre de mécanismes de défense visant à le protéger, à le prémunir contre l’enlisement et la contamination par la souffrance de l’autre… ». Un aspect intéressant de l’écho que peut produire la douleur de l’enfant est noté chez le soignant : le déni. « Reconnaître, admettre la réalité de la douleur de l’enfant est un exercice difficile pour beaucoup d’équipes accueillant des enfants. D’autant que la non-reconnaissance de la douleur est plus facile chez l’enfant car ses moyens d’expression sont plus limités. » (…) « Ce déni est souvent le reflet d’un malaise chez les soignants, d’une incompréhension de l’attitude de l’enfant, d’un dysfonctionnement au sein d’un service. » . Dans les services[Lesquels ?], il est dit que : « Ce n’est pas de la douleur, c’est de la peur ou de l’anxiété… », ou bien : « C’est de la douleur mais il oubliera… », ou bien encore : « C’est dans la tête, c’est psychologique… ». Le déni de la réalité est un mécanisme de défense des soignants qui nient totalement une part plus ou moins importante de la réalité externe. « Le déni est un mécanisme psychologique où la personne réagit comme si sa pensée était toute puissante et qu’il suffisait de refuser la pensée d’une chose pour que cette chose n’existe pas. Mécanisme pathologique quand il est prévalent et rigide mais qui se retrouve sous une forme atténuée chez tout un chacun sous la forme : « il ne faut pas penser au malheur, à la mort, etc. » ; héritage de la pensée magique chez les jeunes enfants. Dans la relation de soin, ce déni se manifeste rarement de façon ouverte mais plutôt de manière inconsciente qui peut se traduire par la persistance d’attitudes nocives (le déni favorise les conduites à risque)… ».Il existe une autre notion qui peut entrer en ligne de compte dans ce déni des soignants face à la douleur de l’enfant : le concept d’amnésie infantile qui fait partie du développement psychologique de l’enfant. Il est vrai « que nous avons tous été des enfants ». Mais cette période de notre vie que nous avons tous en commun est recouverte « d’un voile d’étrangeté », peu, voire aucun souvenir de cette époque ne nous revient consciemment à la mémoire. « Qu’il est donc difficile de comprendre ce que veut, ce que cherche, ce que demande un enfant ! » : cela explique cette facilité des soignants à ne pas prendre en compte la douleur de l’enfant qu’il soigne, ne se souvenant pas eux-mêmes de ce qu’ils ont ressenti et vécu à cette période de leur vie. Un autre concept intéressant concernant le vécu de la douleur par les soignants est le transfert. Les soignants adultes résistent mieux à la douleur en général, et donc transfèrent leurs ressentis et leurs émotions sur la personne qu’ils soignent. Ils pensent que l’enfant supporte la douleur de la même façon qu’ils le feraient[réf. nécessaire].Pousser un juron peut également avoir un effet anti-douleur.Il existe une association internationale pour l'étude de la douleur (International Association for the Study of Pain ou IASP), basée à Seattle puis à Washington. Elle soutient la recherche dans ce domaine, publie une lettre mensuelle et a notamment publié une nouvelle classification des douleurs chroniques pour permettre aux chercheurs et cliniciens traitant la douleur d'utiliser un vocabulaire commun, codifié et approuvé, incluant une taxonomie des formes de la douleur et leurs abréviations (en 1986, actualisé en 1994 2011). Cette classification inclut des syndromes douloureux régionaux complexes (SDRC) et des sections spécialisées sur la douleur abdominale, pelvienne, et urogénitales (révisée en 2012).Les connaissances concernant la nociception et la douleur chez les animaux invertébrés sont encore très fragmentaires.L'une des méthodes pour repérer la douleur chez les humains est de poser une question : une personne peut exprimer une douleur qui ne peut être détectée par des mesures physiologiques connues. Cependant, comme chez les nourrissons, les animaux non-humains ne peuvent poser de question sur ce qu'ils ressentent ; ainsi les critères définis aux humains ne peuvent être attribués aux animaux. Les philosophes et scientifiques se sont penchés sur ces difficultés d'expression. René Descartes, par exemple, explique que les animaux manquent de conscience et font l'expérience d'une douleur différente de celle ressentie par les humains. Bernard Rollin de l'Université d'État du Colorado, principal auteur de deux lois fédérales concernant la douleur animale, rédige que les chercheurs, durant les années 1980, restaient incertains concernant l'expérience de la douleur ressentie par les animaux, et que les vétérinaires, formés aux États-Unis avant 1989, apprenaient à ignorer la douleur chez les animaux. Lors de ses discussions avec des scientifiques et autres vétérinaires, il lui était demandé de « prouver » que les animaux sont conscients et de fournir des preuves « scientifiquement acceptables » qui permettraient de mettre en avant la douleur animale. Carbone rédige que la perception dans laquelle les animaux souffrent différemment des humains reste peu répandue. La capacité des espèces invertébrées chez les animaux, telles que les insectes, à ressentir la douleur et la souffrance reste également incertaine,.La présence de la douleur chez les animaux reste incertaine pour quelques-uns, mais elle peut être repérée à l'aide de réactions comportementales ou physiques. Les spécialistes croient actuellement que tout animal vertébré peut ressentir la douleur, et que certains invertébrés, comme la pieuvre, le peuvent également,. Quant aux autres animaux, plantes et autres entités, la capacité physique à ressentir la douleur reste une énigme dans la communauté scientifique, car aucun mécanisme par lequel la douleur peut être ressentie n'a été détecté. En particulier, il n'existe aucun nocicepteur connu dans des groupes tels que les plantes, champignons et la plupart des insectes.L'évaluation relève parfois de la gageure. Suivant l'espèce animale et le type de douleur, l'évaluation peut être relativement facile ou impossible.En général, les douleurs chroniques sont silencieuses et se manifestent par des troubles fonctionnels plus ou moins marqués (position antalgique, comportements d'évitement, irritabilité, anorexie et parfois apathie). Les douleurs aiguës sont plus visibles et faciles à mettre en évidence par une palpation-manipulation appropriée.Il existe des grilles de notations pour certaines affections et espèces mais elles sont surtout employées en recherche.La douleur animale a longtemps été négligée pour diverses raisons : sous médicalisation de plusieurs espèces, un sondage Insee a donné il y a quelques années un taux de médicalisation d"
médecine;"L'endocrinologie est une discipline de la médecine qui étudie les hormones. Son nom signifie : l'étude (logos) de la sécrétion (crine) interne (endo). Elle étudie de très nombreux phénomènes physiologiques, car les hormones interviennent dans de nombreuses fonctions chez de nombreux organismes dont l'Homme :la nutrition :les hormones régulatrices de la glycémie comme l'insuline et le glucagon,la leptine qui régule les réserves de graisses dans l'organisme,la ghréline qui stimule l'appétit, et la PYY-36 qui donne une sensation de satiété pendant plusieurs heures ;la croissance, avec les différentes hormones de croissance ;la reproduction : la puberté, mais aussi les cycles menstruels de la femme, la grossesse et la lactation (fabrication de lait) ;la régulation de la température corporelle, avec les hormones thyroïdiennes ;la régulation des cycles circadiens avec la mélatonine.Chez certaines autres espèces animales, les hormones ont encore d'autres effets : le changement de sexe chez certains poissons, le changement de comportement chez les abeilles, la mue chez certains insectes (régulée par l'ecdysone)…Les hormones sont un moyen pour l'organisme de communiquer des informations en son sein, grâce principalement à des molécules transportées par le sang.L'endocrinologie a commencé en Chine au IIe siècle av. J.-C. Les Chinois isolaient à des fins thérapeutiques des hormones sexuelles et pituitaires à partir de l'urine humaine. Ils ont utilisé des méthodes nombreuses et complexes, comme la sublimation des hormones stéroïdiennes. Une autre méthode décrite par des textes chinois, le plus ancien datant de 1110, évoque l'utilisation de saponine, issue de fèves de Gleditsia sinensis (en), pour extraire les hormones. Le gypse, contenant du sulfate de calcium, a également été utilisé.La plupart des tissus et des glandes du système endocrinien avaient déjà été identifiés par les premiers anatomistes. Mais les Grecs anciens et penseurs romains, tels Aristote, Hippocrate, Lucrèce, Celse, Galien, etc. ont privilégié la théorie humorale pour expliquer le fonctionnement biologique et la maladie. Cette approche est restée dominante jusqu'au XIXe siècle, où la théorie microbienne, la physiologie et les bases de la physiopathologie se sont développées.En 1849, Arnold Berthold remarque que la crête et les barbillons ne se développent pas chez les coqs castrés et qu'ils adoptent un comportement moins masculin. Il constate que le fait de réintroduire des testicules dans la cavité abdominale du même coq ou d'un autre coq castré permet un développement comportemental et morphologique normal. Il en conclut que les testicules sécrètent une substance, transportée par le sang, qui agit sur la physiologie du coq. Mais deux autres hypothèses sur l'action des testicules étaient également envisageables : la modification ou l'activation d'un constituant du sang, ou bien le retrait d'un facteur inhibiteur contenu dans le sang. L'existence d'une substance qui engendre les caractéristiques masculines a été démontrée ultérieurement avec de l'extrait de testicules sur des animaux castrés. Puis la testostérone cristalline pure a été isolée en 1935.La maladie de Basedow reprend le nom d'un médecin allemand, Carl von Basedow, qui a décrit un cas de goitre avec exophtalmie en 1840. Cependant, des rapports antérieurs sur cette maladie avaient déjà été publiés par le médecin irlandais Robert James Graves, avec la même constellation de symptômes en 1835, par les Italiens Giuseppe Flajani et Antonio Giuseppe Testa, en 1802 et 1810 respectivement, et par le médecin anglais Caleb Hillier Parry (un ami d'Edward Jenner) à la fin du XVIIIe siècle. Thomas Addison a été le premier à décrire la maladie d'Addison en 1849.En 1902, William Bayliss et Ernest Starling ont observé dans une expérience que l'instillation d'un acide dans le duodénum provoquait un début de sécrétion du pancréas, et ce même après avoir supprimé toutes les connexions nerveuses entre les deux. Le même résultat pouvait être obtenu par l'injection d'un extrait de muqueuse du jéjunum dans la veine jugulaire, ce qui montrait que certains facteurs contenus dans cette muqueuse en étaient la cause. Ils ont appelé cette substance ""sécrétine"" et ont inventé le terme d'""hormone"" pour désigner les facteurs chimiques qui agissent de cette façon.Joseph von Mering et Oskar Minkowski ont observé en 1889 que le retrait du pancréas conduit à une augmentation de la glycémie, suivi d'un coma et enfin de la mort, symptômes de diabète sucré. En 1922, Frederick Banting et son assistant Charles Best ont remarqué que l'homogénéisation d'un pancréas et l'administration par injection de l'extrait qui en dérive améliore au contraire l'état du sujet. La mystérieuse hormone responsable de cet effet, l'insuline, n'a été séquencée par Frederick Sanger qu'en 1953.Les neurohormones ont été identifiés pour la première fois par Otto Loewi en 1921. Il a fait incuber le cœur d'une grenouille (innervé, avec son nerf vague) dans un bain d'eau salée, et a laissé reposer la solution pendant un certain temps. Un second cœur non innervé était ensuite introduit dans cette solution. Si le nerf vague du premier cœur était stimulé, l'activité inotrope (amplitude de battement) et chronotrope (fréquence cardiaque) était observable sur les deux cœurs. Si le nerf vague n'était pas stimulé, aucun des deux cœurs ne réagissait. L'effet pouvait être bloqué en utilisant de l'atropine, un inhibiteur connu de la stimulation du nerf vague. De toute évidence, quelque chose était sécrété par le nerf vague dans la solution saline et agissait sur les deux cœurs. Ce qui provoquait cet effet régulateur (myotropique) a été identifié plus tard: l'acétylcholine et la noradrénaline. Loewi a reçu un prix Nobel pour cette découverte.Des travaux plus récents en endocrinologie se sont axés sur les mécanismes moléculaires responsables du déclenchement des effets produits par les hormones. Earl Sutherland a mené les premiers travaux sur ce sujet en 1962, pour savoir si les hormones pénètrent dans les cellules pour provoquer une action, ou si elles restent en dehors. Il a étudié la noradrénaline, qui agit sur le foie pour convertir le glycogène en glucose grâce à l'activation de l'enzyme phosphorylase. Il a homogénéisé le foie en deux fractions, l'une membranaire et l'autre soluble (le phosphorylase est soluble), puis il a ajouté de la noradrénaline à la fraction membranaire, en a extrait les produits solubles, et les a ajoutés à la première fraction soluble. Le phosphorylase s'est activé, ce qui indiquait que le récepteur à noradrénaline cible se trouvait sur la membrane cellulaire et non dans la cellule. Il a plus tard nommé ce composé ""AMP cyclique"" (AMPc). Cette découverte a provoqué la naissance du concept de médiation par un messager secondaire. Il a reçu également un prix Nobel.À la fin du XXe siècle apparaissent des recherches sur les perturbateurs endocriniens, des molécules d'origine parfois naturelle mais le plus souvent artificielle — plastifiants, détergents, pesticides, médicaments, phénols, PCB, dioxines, agents ignifuges bromés, plomb, mercure, etc. — qui imitent ou altèrent le fonctionnement des hormones et qui peuvent avoir un effet néfaste sur la physiologie, le fonctionnement cognitif, et les capacités reproductives des espèces animales, y compris l'espèce humaine. Certains travaux montrent une augmentation récente et rapide de troubles d'origine endocrinienne liés au mode de vie et à la pollution de l'environnement,,.L'hypothalamusL'hypophyseL'épiphyseLa thyroïdeLes parathyroïdesLe thymusLe pancréasLes surrénalesLes ovairesLes testiculesLes revues scientifiques ci-dessous sont les principales revues spécialisées en endocrinologie clinique et fondamentale (en anglais) :EndocrinologyDiabetesJournal of Clinical Endocrinology and MetabolismMolecular EndocrinologyJournal of NeuroendocrinologyEndocrine ReviewsEuropean Journal of Endocrinology : document utilisé comme source pour la rédaction de cet article.Jean-Didier Vincent La biologie des passions. Un exposé clair du fonctionnement hormonal et de son importance dans la détermination des passions. L'homme humoral en quelque sorte distinct de l'homme neuronal.Jean Gautier (docteur), L'enfant ce glandulaire inconnu, 1961. Exposé des différentes étapes du développement endocrinien.Emmanuelle Lecornet - Sokol et Caroline Chaminadour, Et si c'était hormonal ?, Hachette, 20 février 2019, 240 p. (ISBN 978-2017077725)Robert Temple (trad. de l'anglais), Le génie de la Chine : 3 000 ans de découvertes et d'inventions, Arles, P. Picquier, 2007, 288 p. (ISBN 978-2-87730-947-9) Chirurgie endocrinienneCentre scientifique d'endocrinologie« Minimum vital » en endocrinologie (polycopié de la Pitié-Salpêtrière) Portail de la médecine"
médecine;"Un examen médical est une procédure de diagnostic réalisée pour des motifs de santé. Par exemple :pour diagnostiquer des maladiespour mesurer la progression, la régression ou la guérison des maladiespour confirmer chez quelqu'un l'absence de maladieQuelques-uns se composent d'un simple examen physique, appelé également examen clinique : il ne requiert que de simples instruments entre les mains d'un médecin, et peuvent être réalisés dans son cabinet. D'autres requièrent un équipement plus sophistiqué et/ou l'usage d'un environnement stérile.Quelques examens requièrent des tests sur échantillons de tissu ou des liquides corporels qui seront envoyés à un laboratoire médical pour analyse. Quelques tests chimiques simples (comme la mesure du pH de l'urine) peuvent être mesurés directement dans le cabinet du médecin. Parfois la possibilité d'un faux-négatif ou faux positif doit être confirmé par une mise en culture à partir d'échantillons.Certains examens peuvent également être effectués sur une personne morte dans le cadre d'une autopsie.Les examens médicaux peuvent être classés en deux catégories :examen invasifexamen non invasif.En règle générale, le premier requiert une effraction de la peau plus importante qu'une simple ponction veineuse, peut être désagréable, nécessite parfois une anesthésie locale  ou générale, peut  nécessiter une hospitalisation et comporte un certain nombre d'effets secondaires, voire, de risque d'accident. La distinction entre ces deux types d'examen n'est pas si tranchée dans un certain nombre de cas.Il existe également des examens ionisants. Généralement les examens de radiologie.La compilation de l'ensemble des données de l'examen médical constitue le dossier médical qui peut être manuscrit ou informatisé.Tout examen clinique débute par un interrogatoire du patient, permettant de déterminer :les antécédents :personnels : anciennes maladies, anciens examens,familiaux : à la recherche de maladies héréditaires,chez la femme : antécédents gynéco-obstétricaux, utilisation ou non d'une méthode contraceptive en faisant préciser laquelle (cette information est importante pour éviter de prescrire certains examens ou traitements contre-indiqués en cas de grossesse débutante).le motif de la consultation ;les symptômes actuels et  leur évolution, retraçant ainsi l' histoire de la maladie ;les traitements actuels et passés, médicaux et chirurgicaux, y compris les vaccinations ;le mode de vie : prise ou non de toxiques (tabac, alcool…), travail, situation familiale, prise en charge par un organisme de sécurité sociale et/ou une mutuelle.L'examen clinique comprend  classiquement 4 phases : l'inspection, la palpation, la percussion et l'auscultation.L'inspection : le médecin regarde le patient déshabillé.La palpation recherche d'éventuels points douloureux, masses anormales d'adénopathies (gros ganglions), augmentation de volume de certains organes comme le foie ou la rate… Elle permet également la ""prise du pouls"" du patient, habituellement au niveau de l'artère radiale, afin de mesurer la fréquence cardiaque, de dépister une anomalie du rythme cardiaque et de reconnaitre d'éventuelles anomalies de la pulsatilité artérielle (abolie, faible ou au contraire exagérée).La percussion permet de détecter d'éventuelles modifications au sein du thorax ou de l'abdomen (anormalement ""mat"" ou ""tympanique"")L'auscultation, peut être pratiquée de façon ""immédiate"", le médecin collant son oreille directement sur le corps du patient, ou ""médiate"" par l'intermédiaire d'un stéthoscope (cas le plus fréquent). Elle permet d'analyser les bruits provoqués par certains organes ou appareils : le cœur, l'appareil respiratoire, l'appareil digestif et les vaisseaux dans diverses localisations (crâne, cou, abdomen, aines...).Il est en règle générale complété par :la détermination du poids et de la taillela mesure de la pression artérielleEt de façon moins systématique par : des tests respiratoires ;une étude des examen des réflexes ;un examen de la vue :ophtalmoscopie (ophtalmoscope) ;un examen de l'ouïe ;un toucher rectal ;un examen du vagin et du col de l'utérus chez la femme.Voir articles : biopsiesponction lombaireVoir articles :examens sanguins, dontvitesse de sédimentation(hémogramme)test de dépistage du VIH/SIDA (le Western Blot est un test de détection d'anticorps spécifiques du VIH dans le sang, par une technique d’électrophorèse spéciale)tests urinairestest d'ADN, test d'ARNanalyse des sellesgaz sanguinsVoir articlesmicrobiologieVoir articlesradiologielavement barytéurographie intra-veineuseultrasonséchographiedopplerÉchographie Dopplerélectrocardiographie (ECG)électroencéphalographie (EEG)scannertomographie à émission de positronimagerie par résonance magnétique (RNM, IRM)IRM fonctionnelleendoscopiecoloscopiecystoscopiesigmoïdoscopiehystéroscopiecœlioscopiecolposcopieexamen de l'appareil respiratoirepléthysmographie Portail de la médecine"
médecine;"La glande thyroïde ou thyroïde est une glande endocrine régulant, chez les vertébrés, de nombreux systèmes hormonaux par la sécrétion de triiodothyronine (T3), de thyroxine (T4) et de calcitonine. Dans l'espèce humaine, elle est située à la face antérieure du cou, superficiellement.Ses déformations (on parle de goitre quand le volume de la thyroïde est augmenté) sont visibles sous la peau. Elle peut être le siège de diverses affections : hyperthyroïdie, hypothyroïdie, tumeur maligne ou tumeur bénigne. On peut l'étudier grâce à l'échographie et à la scintigraphie.La thyroïde, moulée sur l'axe trachéo-laryngé, est de consistance ferme, de couleur rosée, et pèse de 25 à 30 grammes généralement mais en cas de goitre sa masse peut augmenter jusqu'à 100-150 grammes. Elle est entourée d'une capsule avasculaire (ou gaine viscérale péri-thyroïdienne) qui lui est propre et qui est différente de la loge thyroïdienne.La thyroïde se compose de deux lobes droit et gauche situés verticalement de part et d'autre du larynx. Une partie intermédiaire horizontale, l'isthme thyroïdien, forme un pont entre les deux lobes. Généralement, la glande thyroïde répond aux 2e et 3e anneaux trachéaux ; mais elle peut avoir une position haute : 1er et 2e anneaux trachéaux, ou une position basse : 3e et 4e anneaux trachéaux. Les deux lobes ont un sommet supérieur, ainsi qu'une grande base inférieure. On leur décrit trois faces : médiale, postérieure et antéro-latérale. Sa hauteur est d'environ 6 cm pour une longueur de 6  à   8 cm. On trouve souvent entre les deux lobes, donc au niveau de l'isthme, le lobe pyramidal de Lalouette, souvent déporté vers la gauche : c'est un reliquat du canal thyréoglosse.Il existe des variations morphologiques, s'expliquant par l'embryologie : en effet les deux lobes sont parfois éloignés l'un de l'autre sans qu'il n'y ait d'isthme, ou au contraire peuvent être soudés donnant une thyroïde en forme de V. Provenant d'un bourgeon de cellules endodermiques naissant près de la racine de la langue, différentes positions de la glande thyroïde peuvent cependant survenir durant l'ontogenèse : une mauvaise migration de cette ébauche conduit alors à la détection de cette glande (fonctionnelle ou non fonctionnelle) dans la région linguale, cervicale, voire endo-thoracique.La thyroïde présente les rapports anatomiques suivants :ventralement : muscles cervicaux superficielslatéralement : nerfs récurrents et axes vasculaires jugulo-carotidiensdorsalement : larynx au pôle supérieur et trachée cervicale au pôle inférieurLes quatre parathyroïdes ont des positions variables, mais se situent généralement aux quatre pôles thyroïdiens.La thyroïde est un organe richement vascularisé. En effet on retrouve :Deux artères principales :artère thyroïdienne supérieure, première branche de l'artère carotide externe ; elle se divise en 3 branches (latérale, médiale et postérieure) une fois la glande atteinte.artère thyroïdienne inférieure, naissant du tronc thyro-cervical, branche collatérale de l'artère subclavière. Se divise également en trois branches (mêmes situations) dans la thyroïde.Dans de très rares cas il est possible qu'une 3e artère vienne vasculariser la thyroïde dans sa portion basse appelée artère de Neubauer qui est une branche de la crosse de l'aorte.Les deux artères principales de la thyroïde sont anastomosées ; l'ATS droite avec l'ATS gauche et l'ATI droite, et l'ATI droite avec l'ATS droite et l'ATI gauche.Il existe néanmoins d'autres artères, moins volumineuses, inconstantes, naissant directement de l'arc aortique. Par exemple l'artère thyroïdea ima vascularisant la partie isthmique. Celle-ci est présente chez environ 5 à 10 pour cent des sujets et peut provoquer une hémorragie en cas de trachéotomie.Trois veines principales :veine thyroïdienne supérieure, résultant de la confluence de trois veines dans la glande, et formant avec les veines linguale et faciale le tronc thyro-lingo-facial qui se jette dans la veine jugulaire interne.veine thyroïdienne moyenne, réunion de plusieurs branches pas très volumineuses se jetant dans la veine jugulaire interne.veine thyroïdienne inférieure, formée par la confluence de trois veines dans la glande et se jetant dans le tronc veineux brachio-céphalique.De même que pour les artères, certaines veines accessoires vascularisant préférentiellement l'isthme vont rejoindre les troncs veineux brachio-céphaliques droit et gauche.Chez nos mammifères domestiques, on parle de 2 glandes thyroïdes (droite et gauche), car contrairement à l'homme, les 2 lobes thyroïdiens ne sont pas réunis par un isthme.  Chez les autres vertébrés, la glande thyroïde est diffuse, formée de groupes dispersés de follicules, et situés latéralement à des distances variables de l'œsophage.Chez les agnathes et la plupart des téléostéens (poissons), les groupes de follicules se distribuent sur toute la partie ventrale de la tête.Malgré cette diversité morphologique, la structure histologique folliculaire de la thyroïde est hautement conservée chez tous les vertébrés, ce qui témoigne d'un processus original et commun de production hormonale.L'unité morpho-fonctionnelle de la glande thyroïde est le follicule thyroïdien (ou vésicule thyroïdienne), composé d'un épithélium unistratifié de cellules folliculaires (les thyréocytes), produisant les hormones thyroïdiennes, disposées autour d'une lumière centrale contenant la colloïde : la colloïde est principalement constituée du précurseur des hormones thyroïdiennes, la thyroglobuline. Le follicule thyroïdien est un véritable piège à iode (ion iodure), élément rare à la surface de la terre, et indispensable au fonctionnement de l'organisme; l'iode sera ainsi capté et stocké dans la colloïde : la biosynthèse des hormones thyroïdiennes pourra alors  se dérouler, l'iode venant se coupler à la thyroglobuline ; la thyroglobuline iodée est ensuite réintégrée dans le follicule thyroïdien, et sécrétée dans le courant sanguin.Le follicule thyroïdien, en dehors d'une majorité de cellules folliculaires, contient 1 à 2 % de cellules dites parafolliculaires (ou cellules C, ou cellules claires), produisant la calcitonine : elles n'ont cependant jamais de contact avec la colloïde.On trouve aussi des amas de cellules (ilots de Woffler), cellules jointives pouvant se transformer en vésicule thyroïdien.La thyroïde est issue de trois ébauches :deux ébauches latérales issues du 4e sillon branchial interne et qui forme une partie des lobes latérauxet une ébauche centrale issue de l'évagination du pharynx buccal constituant ainsi le tractus thyréoglosse qui forme l'isthme ainsi  que la majeure partie des lobes latéraux. Rappelons que le tractus thyréoglosse est l'axe de migration de la thyroïde chez l'embryon.La thyroïde sécrète :la T3 ou triiodothyronine en très faible quantité ;la T4 ou thyroxine ;la calcitonine intervenant dans le métabolisme du calcium.La production de ces hormones est régie par la thyréostimuline (TSH, « thyroid-stimulating hormone »), produite par l'hypophyse et nécessite un apport en iode. La plus grande production de la T3 est obtenue par la conversion de la T4 au niveau du foie, pour la plus grosse quantité et les intestins pour le reste. La thyroïde ne produit, elle, de la T3 directement que pour à peine 10 à 20 %.De par sa position superficielle, la thyroïde est explorée en premier lieu par une échographie cervicale, qui recherchera des nodules ou un goitre. L'image ultrasonore permet d'évaluer le volume de la thyroïde ; à l'échelle du diagnostic individuel ou de population (suivi épidémiologique).La tomodensitométrie avec injection de produit de contraste iodé est peu utilisée, généralement dans le cadre du bilan pré-opératoire des goitres volumineux, notamment des goitres plongeants.La scintigraphie thyroïdienne à l'iode 123 est un examen fonctionnel. L'injection d'un traceur d'iode radioactif mettra en évidence des zones du parenchyme plus ou moins actives, et permettra la distinction entre un nodule hypersécrétant et un nodule « froid ».Un bilan thyroïdien standard comporte le dosage de la TSH et de la T3 ou de la T4. La calcitonine n'est pas systématiquement dosée.Les dysthyroïdies peuvent avoir des origines génétiques, être liées à des carences nutritionnelles en iode, mais aussi être induites par des toxiques (plomb, ou iode radioactif par exemple - on parle alors de « thyrotoxicoses »). Hypothyroïdie Situation d'imprégnation insuffisante de l'organisme en hormones thyroïdiennes, le plus souvent à cause d'un mauvais fonctionnement de la glande thyroïde.Les symptômes de l'hypothyroïdie découlent d'un ralentissement métabolique général : fatigue, difficultés de concentration, troubles de la mémoire, frilosité, myxœdème, prise de poids malgré un appétit stable voire diminué, diminution de la pilosité avec perte de cheveux ou cheveux devenant cassants, éclaircissement des sourcils, sécheresse ou épaississement cutané, pâleur, crampes musculaires, fourmillement ou engourdissement des extrémités, inappétence, tendance à la dépression, insomnies, tendance à la constipation.L'examen clinique recherche une augmentation de la taille de la thyroïde qui peut être importante (goitre), un ralentissement de la fréquence cardiaque, la bradycardie, et parfois, de la tachycardie ou des symptômes ressemblant à ceux de l'hyperthyroïdie.Le traitement est une substitution journalière à vie en hormones thyroïdiennes, par voie orale. Hyperthyroïdie Symptomatologie due à un excès de production d'hormones thyroïdiennes :cardio-vasculaire : tachycardie, éréthisme cardio-vasculaire (frémissement du choc de la pointe du cœur) ;digestif : syndrome polyuro-polydipsique (boit et urine en grande quantité), amaigrissement, diarrhée, flatulences ;neuro-psy : tremblement, agitation, trouble de l'humeur (irritabilité allant à la dépression), trouble du sommeil, trouble du comportement alimentaire (mange en quantité excessive, perte de poids) ;généraux : hypersudation (mains souvent moites, transpiration), hyperthermie, thermophobie (température élevée et n'apprécie pas les températures élevées) ;musculaire et articulaire : douleur et fatigue musculaire, ostéoporose, augmentations des glandes lactogènes.Le tabac multiplie par dix le risque de survenance de la maladie de Basedow, la forme la plus fréquente de l'hyperthyroïdie, et augmente les risques de complications. Thyroïdites La thyroïdite est une inflammation de la glande thyroïde.Il existe plusieurs types de thyroïdites: Maladie d'Hashimoto La cause la plus fréquente d’hypothyroïdie.Elle consiste en une destruction de la glande thyroïde causée par des taux d’anticorps antithyroïdiens anormalement élevés dans le sang et des globules blancs. La thyroïde ne sécrète alors plus suffisamment d’hormones thyroïdiennes.Cette maladie nécessite donc très souvent un supplément hormonal.Pour confirmer le diagnostic, il faut réaliser une prise de sang qui dosera les hormones thyroïdiennes et la capacité du corps à les gérer (T4, T3, TSH), ainsi que les auto anticorps thyroïdiens (AC anti TPO, et AC anti thyroglobuline).  Thyroïdite du post-partum La thyroïdite du post-partum peut survenir dans l'année qui suit un accouchement. Dans ce cas, la glande a tendance à récupérer, et le traitement de remplacement des hormones thyroïdiennes n’a besoin d’être administré que durant quelques semaines. Une évolution vers une hypothyroïdie permanente est possible. Thyroïdite silencieuse Elle porte ce nom, car elle n’entraîne aucun signe ni symptôme d’inflammation de la thyroïde. De prime abord, le patient présente une hyperthyroïdie pouvant donner lieu aux mêmes symptômes que ceux de la maladie de Basedow-Graves, qui laisse place à une phase d’hypothyroïdie aboutissant à une guérison complète. La présence d’anticorps antithyroïdiens comparables à ceux décelés au cours de la maladie de Hashimoto est un facteur de risque de persistance de l’hypothyroïdie. Thyroïdite subaiguë dite de Quervain Il s’agit d’une forme passagère de thyroïdite provoquant une hypothyroïdie. La thyroïdite subaiguë serait causée par une infection virale, car la majorité des patients atteints ont présenté une infection de la gorge dans les semaines précédant son apparition. Cette affection se manifeste sous forme d’épidémies de faible envergure et est généralement associée à des infections virales connues. Dépression et thyroïde L'hypothyroïdie peut parfois être confondue avec un état de dépression et l'hyperthyroïdie pour un état d'excitation. Le diagnostic thyroïdien permettra d'éliminer ces faux diagnostics.Cependant des travaux récents ont montré qu'une hypothyroïdie traitée uniquement avec de la thyroxine peut davantage encore se rapprocher d'un état dépressif. Plutôt que de prescrire un antidépresseur, le médecin peut parfois proposer une simple substitution d'une partie de la dose de thyroxine (T4) par de la tri-iodo-thyronine (T3) ou de flavinine (substitut générique du B52).Un goitre est une thyroïde globalement augmentée de volume. Il est dit toxique lorsqu'il sécrète des hormones thyroïdiennes de façon excessive, entraînant une hyperthyroïdie. Les goitres sont rarement homogènes et le plus souvent multinodulaires ; chaque lobe thyroïdien présente un nombre important de nodules bénins de volume variable.Les goitres peuvent être en situation cervicale normale ; ils sont dits « plongeants » lorsque le pôle inférieur d'au moins un lobe pénètre dans le médiastin à travers l'orifice supérieur du thorax. Plongeants ou non, les goitres peuvent être (rarement) compressifs, lorsque le volume trop important de la thyroïde comprime les organes de voisinage, principalement la trachée, mais aussi l'œsophage et parfois étirant les nerfs récurrents, entraînant alors une paralysie de la corde vocale homolatérale.Les tumeurs de la thyroïde se présentent sous la forme d'un nodule thyroïdien, qui peut être bénigne (adénome de la thyroïde) ou maligne (carcinome de la thyroïde). Tumeurs bénignes Adénome vésiculaire de la thyroïdeAdénome oncocytaire de la thyroïdeLipoadénome de la thyroïdeAdénome à cellules claires de la thyroïdeAdénome vésiculaire à cellules en bague à chaton de la thyroïdeAdénome vésiculaire mucosécrétant de la thyroïdeAdénome vésiculaire à noyaux bizarres de la thyroïdeAdénome vésiculaire atypique de la thyroïde Tumeurs malignes Il existe deux types histologiques principaux de cancers thyroïdiens :carcinomes différenciés folliculairescarcinomes différenciés papillaires (le plus fréquent, 80 % des cas)La thyroïde est généralement abordée par une cervicotomie médiane, qui peut être élargie latéralement en cervicotomie en U s'il existe une nécessité de curage ganglionnaire cervical. En cas de volumineux goitre plongeant, un refend cutané en Y en regard de l'extrémité crâniale du sternum sera souvent pratiqué. Au maximum, une simple manubriotomie (on parle alors de cervicomanubriotomie) ou une sternotomie médiane pourra être pratiquée.livret d'information ""Cancer de la thyroïde"" ; Institut Gustave Roussy.ARC, Monographie Volume 79 (2001) Some Thyrotropic AgentsFini J.B et Demeneix B (2019) Les perturbateurs thyroïdiens et leurs conséquences sur le développement cérébral. Biologie Aujourd’hui, 213(1-2), 17-26.Remaud S et Demeneix B (2019) Les hormones thyroïdiennes régulent le destin des cellules souches neurales. Biologie Aujourd’hui, 213(1-2), 7-16 (résumé). Portail de l’anatomie   Portail de la physiologie"
médecine;"Les hormones thyroïdiennes, c'est-à-dire la thyroxine (T4) la triiodothyronine (T3) et la diiodothyronine (T2), sont des hormones produites par les cellules folliculaires de la thyroïde à partir de la thyroglobuline et d'iodure. Il existe également la thyrocalcitonine, hormone produite par les cellules parafolliculaires de la thyroide et qui joue un rôle dans le métabolisme phospho-calcique. La thyrocalcitonine induit une hypocalcémie et une hypophosphoremie. Celles-ci sont produites majoritairement sous forme de T4. La T4 agit comme une prohormone, relativement peu active, qui est convertie en T3, plus active. La conversion de la T4 en T3 a lieu dans les cellules cibles, sous l'effet d'une enzyme, la thyroxine 5'-désiodase.Les hormones thyroïdiennes sont essentielles à la croissance et au développement corrects, à la multiplication et à la différenciation de toutes les cellules de l'organisme, notamment dans le système nerveux central, le squelette et les bourgeons dentaires. À divers degrés, elles régulent le métabolisme basal des protéines, des lipides et des glucides. Toutefois, c'est sur l'utilisation des composés riches en énergie que leur impact sur les cellules est le plus prononcé. Elles ont également un effet permissif sur l'action d'autres hormones et de neurotransmetteurs.De nombreux stimuli physiologiques et pathologiques influent sur la synthèse des hormones thyroïdiennes. L'hyperthyroïdie est le syndrome clinique causé par un excès de thyroxine libre ou de triiodothyronine libre circulante, ou des deux. Une carence en iode provoque une augmentation de la taille de la thyroïde, d'où l'apparition d'un goitre, en réponse au ralentissement de la biosynthèse des hormones thyroïdiennes.Les hormones thyroïdiennes sont biosynthétisées dans la thyroïde. Cette biosynthèse est stimulée indirectement par l'hormone thyréotrope (TRH, de l'anglais : thyrotropin-releasing hormone), un tripeptide de structure (pyro)Glu–His–Pro–NH2 synthétisé par l'hypothalamus. La TRH induit la synthèse de la thyréostimuline (TSH, de l'anglais : thyroid-stimulating hormone) par l'anté-hypophyse, lobe antérieur de l'hypophyse. La TSH agit en augmentant l'expression du gène de la thyroperoxydase (TPO, de l'anglais : thyroid peroxidase).La thyroïde est très vascularisée. Les cellules de la thyroïde sont organisées en follicules autour de vésicules thyroïdiennes qui contiennent une substance gélatineuse qu'on appelle généralement la colloïde. Ces cellules sont orientées, c’est-à-dire qu'elles possèdent un pôle apical du côté de la colloïde et un pôle basal du côté des vaisseaux sanguins. Le noyau des cellules folliculaires est relativement actif, la présence d'un réticulum endoplasmique rugueux, riche en ribosomes, démontre une forte activité de biosynthèse des protéines, et l'appareil de Golgi est lui-même très actif car on peut observer de nombreuses vésicules au pôle apical.Les cellules folliculaires permettent l'échange de molécules entre le sang et la colloïde. Le sang fournit les acides aminés nécessaires à la synthèse, dans le réticulum de ces cellules, de la thyroglobuline (Tg), une protéine dimérique de 660 kDa qui contient environ 120 résidus de tyrosine. La thyroglobuline passe ensuite dans l'appareil de Golgi pour être internalisée dans les vésicules, lesquelles fusionnent avec la membrane apicale des cellules folliculaires en libérant la thyroglobuline dans la colloïde par exocytose.L'iode absorbé par l'alimentation est présent dans l'organisme sous forme d'anions iodure I−, qui sont concentrés dans les cellules folliculaires à partir du sang à l'aide du symport Na/I (NIS), qui utilise le gradient électrochimique en cations sodium Na+ pour accumuler les ions I−. Ces derniers traversent ensuite la membrane apicale grâce à la pendrine, qui joue le rôle d'antiport Cl−/I−, pour rejoindre la colloïde, où ils sont oxydés par la thyroperoxydase (TPO) à l'aide de peroxyde d'hydrogène H2O2 pour former du diiode I2 susceptible de réagir directement avec les résidus de tyrosine de la thyroglobuline : ceux-ci peuvent être iodés une fois pour former des résidus de monoiodotyrosine (MIT), ou deux fois pour former des résidus de diiodotyrosine (DIT).La condensation de deux résidus de DIT donne — outre un résidu d'alanine — un résidu de thyroxine (T4), tandis que la condensation d'un résidu de MIT sur un résidu de DIT donne un résidu de triiodothyronine (T3) ; la condensation d'un résidu de DIT sur un résidu de MIT donne en revanche un résidu de 3,3',5'-triiodothyronine (rT3 ou « T3 inverse »), qui est biologiquement inactive.La thyréostimuline (TSH) se lie au récepteur de la TSH, un récepteur couplé à la protéine Gs, ce qui provoque l'endocytose de fragments de colloïde dans des vésicules qui fusionnent avec des lysosomes. Les hormones thyroïdiennes T4 et T3 sont libérées par digestion de la colloïde par des peptidases, à raison de seulement cinq ou six molécules d'hormone thyroïdienne libérées par molécule de thyroglobuline digérée, le ratio étant d'environ une molécule de T3 pour 20 molécules de T4.En raison de leur caractère lipophile, la T4 et la T3 sont transportées dans le sang en étant liées à des protéines telles que les globulines liant la thyroxine (TBG, des glycoprotéines qui fixent préférentiellement la T4), la transthyrétine (TTR, une autre glycoprotéine, qui ne transporte pratiquement que la T4) et des albumines sériques, qui ont une affinité relativement faible pour les T3 et T4 mais sont abondantes et donc contribuent significativement à leur diffusion dans l'organisme.Les molécules de T4 et T3 présentes dans l'organisme sont généralement liées à une protéine transporteuse, seuls les 0,03 % libres de T4 et les 0,3 % libres de T3 étant biologiquement actives. Ce mode de transport a pour effet d'accroître la demi-vie des hormones thyroïdiennes dans le sang — environ 6,5 jours pour la T4 et 2,5 jours pour la T3 — et de réduire la vitesse à laquelle elles sont absorbées dans les tissus. C'est la raison pour laquelle la mesure de la concentration en hormones thyroïdiennes libres, désignées par T4L et T3L, revêt une grande importance clinique, tandis que la concentration totale, incluant les hormones liées aux protéines transporteuses, n'est pas significative.Malgré leur nature lipophile qui devrait leur permettre de passer les membranes cellulaires, les hormones T3 et T4 ne diffusent pas passivement à travers la bicouche de phospholipides de la membrane plasmique des cellules cibles, et font appel pour cela à des transporteurs membranaires spécifiques.Parmi les deux hormones thyroïdiennes, la prohormone T4 doit en fait être désiodée en T3 par une thyroxine 5'-désiodase dans les cellules cibles pour être pleinement active : la T3 est typiquement entre trois et cinq fois plus active que la T4, qui sert in fine essentiellement au transport de cette hormone dans le sang. Il existe deux isozymes de cette iodothyronine désiodase :le type 1 (D1), présent dans le foie, les reins, la thyroïde et, dans une moindre mesure, l'hypophyse, dont le rôle exact dans l'organisme n'est pas entièrement compris ;le type 2 (D2), présent dans l'hypophyse, le muscle squelettique, le cœur (artères coronaires), le système nerveux central et le tissu adipeux brun, responsable de l'essentiel de la formation de T3 dans la thyroïde, mais capable également de désioder la 3,3',5'-triiodothyronine, ou T3 inverse, en 3,3'-diiodothyronine, ou T2.Pour mémoire, il existe également un troisième type d'iodothyronine désiodase, la thyroxine 5-désiodase (D3), qui convertit respectivement la T4 et la T3, qui sont biologiquement actives, en T3 inverse et en T2, biologiquement inactives, ce qui a pour effet d'inactiver globalement les hormones thyroïdiennes.Une fois dans le cytoplasme, les hormones thyroïdiennes se lient aux récepteurs des hormones thyroïdiennes, qui sont des récepteurs nucléaires. Les récepteurs thyroïdiens se lient, sur l'ADN des cellules cibles, à des éléments de réponse des promoteurs de certains gènes dont ils régulent la transcription. Ces récepteurs thyroïdiens conditionnent la sensibilité relative des différents tissus aux hormones thyroïdiennes.Les hormones thyroïdiennes agissent sur l'organisme pour augmenter le métabolisme de base, agir sur la biosynthèse des protéines et rendre le corps plus réceptif aux catécholamines (telles l'adrénaline, d'où l'intérêt des bêta-bloquants dans l'hyperthyroïdie). L'iode est un composant important dans leur synthèse.Les hormones thyroïdiennes accélèrent le métabolisme de base et, par conséquent, accroissent la consommation de l'organisme en énergie et en oxygène. Elles agissent sur presque tous les tissus, hormis la rate. Elles accélèrent le fonctionnement de la pompe sodium-potassium et, d'une manière générale, raccourcissent la demi-vie des macromolécules endogènes en activant leur biosynthèse et leur dégradation.Les hormones thyroïdiennes stimulent la production de l'ARN polymérase I et II, et, par conséquent, augmentent l'activité de biosynthèse des protéines. Elles augmentent également la vitesse de dégradation des protéines, et, lorsqu'elles sont trop abondantes, la dégradation des protéines peut être plus rapide que leur biosynthèse ; dans ce cas, le corps peut tendre vers un équilibre ionique négatif.Les hormones thyroïdiennes potentialisent les effets des récepteurs adrénergiques β sur le métabolisme du glucose. Par conséquent, elles accélèrent la dégradation du glycogène et la biosynthèse du glucose par la néoglucogenèse[réf. nécessaire].Les hormones thyroïdiennes stimulent la dégradation du cholestérol et augmentent le nombre de récepteurs de LDL, ce qui accélère la lipolyse.Les hormones thyroïdiennes accélèrent le rythme cardiaque (Chronotrope positif - Tachycardie) et accroissent la force des systoles (Inotrope positif), augmentant ainsi le débit cardiaque à travers une augmentation du nombre de récepteurs adrénergiques β dans le myocarde et aussi de la sensibilité aux catécholamines. Il en résulte une augmentation de la pression artérielle systolique et une diminution de la pression artérielle diastolique.Les hormones thyroïdiennes ont un effet profond sur le développement de l'embryon et les nourrissons. Elles affectent les poumons et influencent la croissance postnatale du système nerveux central. Elles stimulent la production de myéline, de neurotransmetteurs, et la croissance des axones. Elles sont également importantes dans la croissance linéaire des os.Les hormones thyroïdiennes peuvent accroître le taux de sérotonine dans le cerveau, en particulier au niveau du cortex cérébral, et inhiber les récepteurs 5-HT2, comme l'ont montré des études sur la réversibilité, sous l'effet de la T3, de comportements d'impuissance apprise chez des rats, et des études physiologiques de cerveaux de rats[pas clair] .Les hormones thyroïdiennes sont prescrites dans les cas d'hypothyroïdie ou de thyroïdectomie (ablation chirurgicale de la glande thyroïdienne). Les hormones thyroïdiennes utilisées sont la T3 et la T4. L'hormone T3 est plus efficace que l'hormone T4 au niveau des récepteurs mais la T4 est transformée en T3 par les tissus périphériques et un traitement par la T4 permet d'avoir un taux normal de T3. La demi-vie de la T3 n'est que de 24 heures et elle nécessiterait deux à trois prises quotidiennes, alors que la demi-vie de la T4 est de 6 à 8 jours et autorise une seule prise quotidienne, ce qui explique son utilisation préférentielle.Le diagnostic du fonctionnement thyroïdien se fait en médecine nucléaire par injection d'iode 123 (isotope radioactif de l'iode) produit dans un cyclotron. Sa période radioactive est relativement faible puisqu'elle est de 13,21 h (c'est-à-dire 13 heures, 12 minutes et 36 secondes). Sa désintégration radioactive émet des rayons γ d'énergie caractéristique équivalent à 159 keV et 27 keV. La dose injectée pour le diagnostic ne dépasse pas les 10 mégabecquerels (MBq).La thyroxine (T4) a été isolée par l'Américain Edward Calvin Kendall en 1910 à partir de trois tonnes de thyroïde de porc, tandis que la triiodothyronine (T3) a été découverte en 1952 par le Français Jean Roche. Portail de la biologie   Portail de la chimie   Portail de la médecine"
médecine;"L'hyperthyroïdie (appelée aussi dans des cas très prononcés — graves et rares — thyréotoxicose ou thyrotoxicose) est le syndrome clinique causé par un excès de thyroxine libre circulante (FT4) ou de triïodothyronine libre (FT3), ou les deux. Chez les humains, les causes principales sont la maladie de Basedow (cause la plus fréquente : 70-80 % des cas), l'adénome toxique de la thyroïde, le goitre multinodulaire toxique, et la thyroïdite sub-aiguë.La glande thyroïde, stimulée par la TSH (thyroid-stimulating hormone), secrète deux hormones, la thyroxine (= tétraiodothyronine) ou T4 et la triiodothyronine (T3). La première est une prohormone, transformée en la seconde qui constitue la forme active.L'hyperthyroïdie consiste en l'augmentation des taux de T3 et de T4 dans le sang. Si cette hypersecrétion est secondaire à une maladie de la thyroïde (ce qui est vrai dans la quasi-totalité des cas), la TSH est effondrée (par rétrocontrôle)L'incidence annuelle est de 0,6 pour 1 000 femmes. Elle est quatre fois moindre chez les hommes. La prévalence aux États-Unis est de 1,3 %.La cause la plus fréquente chez le sujet jeune est la maladie de Basedow et chez le sujet âgé, le nodule toxique ou le goitre multinodulaire, surtout si l'apport iodé de la nourriture est pauvre. Les thyroïdites, entraînant le relargage d'hormones thyroïdiennes à la suite de la destruction cellulaire, comptent pour 10 % des hyperthyroïdies. Les autres causes sont rares.C'est la première cause d'hyperthyroïdie en termes de fréquence. Elle est plus fréquente chez la femme jeune. On retrouve de manière non constante un souffle à l'auscultation de la glande thyroïde qui est augmenté de volume, un discret gonflement des parties molles de la jambe (myxœdème prétibial) ou des globes oculaires légèrement proéminents (exophtalmie). Le diagnostic est fait en présence de TSI (Thyroid stimulating immunoglobulins) dans le sang des patients. La structure de cette TSI est proche de celle de la TSH et stimule ainsi la production d'hormones thyroïdiennes par la glande.Le nodule toxique de Plummer est évoqué devant le nodule isolé de la glande thyroïde qui peut parfois être palpé et surtout, par la fixation d'iode radioactif de ce dernier de manière exclusive à la scintigraphie thyroïdienne, le reste de la glande n'étant plus visualisé. Il devient une cause importante d'hyperthyroïdie chez la personne âgée. Son traitement demande l'éradication du nodule, que cela soit par chirurgie ou par iode radioactif.Elle peut être :infectieuse (thyroïdite de De Quervain dans un contexte grippal) ou post opératoire ;auto-immune comme lors de la thyroïdite de Hashimoto avec la présence d'anticorps anti-TPO ;survenir après un accouchement (assez fréquente puisqu'elle concerne jusqu'à 10 % des parturientes, le plus souvent très discrète et guérissant sans séquelle).Elle évolue parfois vers une hypothyroïdie (diminution des hormones thyroïdiennes) régressive.La scintigraphie montre alors l'absence totale de fixation de l'iode radioactif (scintigraphie blanche).Parmi les autres causes possible, on distingue :le goitre multinodulaire : le goitre est révélé à l'examen clinique de la glande, il peut être suffisamment important pour causer des compressions des structures adjacentes. La fonctionnalité des nodules est affirmée par la scintigraphie thyroïdienne. Le traitement est essentiellement chirurgical : l'utilisation d'iode radioactif peut faire disparaître l'hyperthyroïdie clinique mais ne parvient pas, en règle générale, à faire diminuer le goitre ;l'association d'une maladie de Basedow et de nodules fonctionnels (syndrome de Marine-Lenhart) ;le cancer de la thyroïde évolué ;l'adénome hypophysaire à TSH ;la prise d'hormone thyroïdienne en quantité trop élevée ;effet secondaire de la prise de certains médicaments, surtout du fait de leur richesse en iode dans le principe actif ou les excipients : antiseptiques contenant de l'iode (polyvidone), produits de contraste de radiologie, etc. L'amiodarone peut donner également des hypothyroïdies. L'hyperthyroïdie de l'amiodarone est plus fréquente dans les régions avec apports iodés insuffisants. Elle impose l'arrêt de ce médicament lorsque c'est possible, en sachant que sa demi-vie prolongée (plus de 100 jours) fait que l'imprégnation en médicament va persister très  longtemps.La plupart des signes restent non spécifiques ou peuvent être discrets. La sévérité des signes est corrélée avec les taux hormonaux. Ils sont toutefois plus frustes chez la personne âgée.L'hyperthyroïdie peut se manifester par tout ou partie des signes ci-dessous.Une perte de poids malgré un appétit conservé ou accru (polyphagie).Une prise de poids dans environ 10 % des cas.Une chaleur ressentie comme insupportable (thermophobie).Une polydipsie, soif excessive.Une asthénie, fatigue, à l'instar de l'hypothyroïdie, pouvant avoir comme conséquence des troubles de l'érection dans la moitié des cas, chez l'homme, réversible sous traitement.Une fréquence cardiaque élevée (tachycardie) avec des palpitations ou des extrasystoles auriculaires.Un essoufflement (dyspnée) ;Un pouls irrégulier pouvant correspondre à une fibrillation auriculaire, cette dernière pouvant être présente même en cas d'hyperthyroïdie dite sub-clinique.Des tremblements fins des extrémités, conséquence de l'excès de circulation sanguine rapide du sang (Attention, ce tremblement n'est pas d'origine neurologique !).Le tout peut se compliquer soit :d'une insuffisance cardiaque typiquement à haut débit, régressive le plus souvent après normalisation des hormones thyroïdiennes mais pouvant aboutir à des séquelles dans un tiers des cas ;de douleurs thoraciques pouvant évoquer une angine de poitrine.Diarrhée chronique.Nausées ou vomissements.Il existe une diminution de la force musculaire (myopathie endocrinienne) avec parfois diminution de la taille des muscles (atrophie musculaire).La maladie peut se présenter sous forme de dépression ou irritabilité.Dans les formes graves, l'hyperthyroïdie peut entraîner un coma, des mouvements anormaux sous forme de chorée, des troubles du comportement pouvant ressembler à une psychose.Peau luisante, chaude et humide.Démangeaison isolée.Plusieurs symptômes sont décrits :impuissance ;augmentation de la taille des seins (gynécomastie) ;infertilité ;absence totale ou partielle de menstruations.L'hyperthyroïdie, même modérée (dite sub-clinique) peut se compliquer d'une décalcification osseuse (ostéoporose secondaire).Certaines thyrotoxicoses peuvent ainsi faciliter un saturnisme inattendu, via une contamination de l'organisme par relargage du plomb antérieurement stocké dans les os. Dans ce dernier cas, l'augmentation de la plombémie est accompagnée d'une augmentation du taux sérique d'ostéocalcine qui reflète l'augmentation du remodelage osseux qui accompagne souvent l'hyperthyroïdie.Inversement ou en retour le plomb pourrait affecter la thyroïde en inhibant la captation d'iode, phénomène d'abord observé chez l'animal puis confirmé chez l'Homme dans les années 1960,, y compris dans un cas d'intoxication saturnine liée à la présence d'une balle en plomb non extraite de l'organisme.On observe une élévation de l'hormone TSH ou une chute de la thyroxine sérique et libre, en cas d'exposition chronique et plutôt quand la plombémie dépasse 60 µg/100 mL.Diminution de la concentration de cholestérol sanguin (hypocholestérolémie).Anémie (diminution de la concentration d'hémoglobine dans le sang).Neutropénie (diminution du nombre de polynucléaire neutrophiles sanguin).Selon la cause de l'hyperthyroïdie on observe un goitre, un nodule thyroïdien, une hypertrophie thyroïdienne...Le diagnostic est établi par un examen sanguin : mesure du taux de TSH dans le sang. Un taux effondré de TSH est spécifique d'une hyperthyroïdie périphérique (l'immense majorité des hyperthyroidies, secondaire à une atteinte de la thyroïde). Le diagnostic est confirmé par une mesure du taux de T3 libre et T4 libre sanguin que l'on retrouve augmenté. L'augmentation de ces deux hormones peut cependant être dissociée avec des cas rares d'hyperthyroïdie à T3, la T4 étant normale. Si la TSH est basse et la T4 et T3 sont normales sur des dosages répétées, on parle d'« hyperthyroïdie infraclinique ».Une fois le diagnostic fait, il reste à rechercher la cause. Il est indispensable de doser les anticorps spécifiques (Anticorps anti-récepteur de la TSH, anti-thyroglobuline, anti-thyropéroxydase « anti-TPO ») et de réaliser un examen d'imagerie de la thyroïde : échographie (par ultrasons) ou  scintigraphie (par injection d'un isotope radioactif qui se fixe sur la glande thyroïde et dont le rayonnement est détecté par une caméra à scintillations). Ces examens précisent l'aspect de la glande et la répartition géographique de son activité (fixation à la scintigraphie).La prise en charge de l'hyperthyroïdie a fait l'objet de la publication de recommandations par l'American Thyroid Association en 2011.Le choix du traitement dépend de la cause, de la sévérité et du terrain.Un traitement d'urgence est l'ingestion de solution saturée d'iodure de potassium (SSKI), un fort taux d'ion iodure permettant de stopper temporairement la sécrétion de thyroxine par la thyroïde.Elle consiste en l'ablation de la totalité ou d'une grande partie de la glande thyroïdienne. La chirurgie se doit de respecter les glandes parathyroïdes de petite taille et situées en arrière de la thyroïde. Elle doit également passer en dehors du nerf récurrent qui remonte en arrière de la glande. La section de ce nerf peut entraîner un changement de la voix (dysphonie) du fait qu'il innerve les cordes vocales.Dans le cas de l'hyperthyroïdie, une radiothérapie métabolique peut être prescrite. Il s'agit de l'ingestion d'iode 131 radioactif qui va se fixer sur la glande thyroïde et la détruire. Ce traitement n'est proposé qu'à certaines formes de maladie de Basedow et est naturellement inefficace en cas de non fixation de l'iode sur la glande (scintigraphie blanche). Elle expose à un risque d'hypothyroïdie (comme la chirurgie par ailleurs) qui est facilement traitée par la prise d'hormones thyroïdiennes.Il s'agit de  médicament inhibant la production d'hormones thyroïdiennes, comme le méthimazole (alias thiamazole), le carbimazole ou le propylthiouracile. Le délai d'efficacité peut être long. Deux techniques de prescription peuvent être utilisés : la titration (ou recherche de la dose minimale efficace) ou la prescription à doses importantes conduisant à une hypothyroïdie et à une substitution hormonale associée.En cas de douleurs, il est possible de donner un antalgique et un antipyrétique en cas de fièvre.Les bêta-bloquants ralentissent le cœur et diminuent les palpitations ainsi que les tremblements.Elle est définie par un taux bas de TSH et un taux normal de T4 libre et de T3 totale. Plus de la moitié des hyperthyroïdies sont sub cliniques et leur prévalence serait de 0,7 % aux États-Unis.Ce syndrome est associé avec un risque majoré d'ostéoporose chez la femme âgée mais aussi de maladies cardiovasculaires, de mortalité cardiaque et de fibrillation auriculaire.Un traitement systématique d'emblée n'est pas recommandé : une surveillance régulière du taux des hormones doit être faite et le traitement débuté à l'élévation de ces dernières.C'est l'une des maladies hormonales les plus fréquentes chez le chat, souvent provoquée par une tumeur bénigne (non cancéreuse) de la thyroïde. Cette maladie a été décrite pour la première fois dans les années 1970. Portail de la médecine"
médecine;"L'hypothyroïdie est une situation pathologique d'imprégnation insuffisante de l'organisme en hormones thyroïdiennes (normalement produites par la glande thyroïde). Les symptômes, d'intensité variable, sont notamment une fatigue, une somnolence, une frilosité, une constipation, une prise de poids, une pâleur cutanée, une raideur musculaire, des œdèmes (« myxœdème »).Elle peut se compliquer d'insuffisance cardiaque ou de dépression et classiquement lorsque l'évolution est avancée, d'un coma myxœdémateux. In utero et chez le nouveau-né elle peut entraîner un retard mental (autrefois dénommé « crétinisme »).La thyroïde étant sous le contrôle de la glande hypophysaire, les causes de l'hypothyroïdie relèvent de deux mécanismes principaux. Elle est dite primitive lorsque seule la thyroïde est atteinte, et secondaire lorsque c'est l'hypophyse qui est atteinte. Le diagnostic d'hypothyroïdie est établi par une prise de sang qui montre un taux anormalement bas d'une part de la thyroxine (taux normal dans les formes frustes) et d'autre part de thyréostimuline (TSH), dans des proportions variant selon mécanisme. L'hypothyroïdie est fréquente et le plus souvent d'origine primaire, d'expression fruste et affectant une femme. Les causes sont alors généralement une carence en iode, une pathologie auto-immune (telles que la thyroïdite de Hashimoto ou la thyroïdite atrophique), ou médicamenteuse (par exemple en rapport avec l'amiodarone). La plupart des hypothyroïdies secondaires sont dues à des tumeurs de la région hypophysaire comprimant la glande, ou à des séquelles locales de chirurgie ou de radiothérapie.Le traitement est celui de la cause, lorsqu'il est possible. Le traitement substitutif de l'hypothyroïdie est la lévothyroxine, prescrite par un médecin et dont la surveillance est à la fois clinique (signes d'hypothyroïdie et d'hyperthyroïdie) et biologique (dosage de TSH). La prévention repose en premier lieu sur la supplémentation alimentaire en iode dans les zones déficitaires, notamment sous la forme de sel iodé.Cette maladie affecte plus souvent les femmes, surtout après 50 ans ; et des personnes qui ont des antécédents personnels ou familiaux de maladie de la thyroïde ou de maladie auto-immune (diabète de type 1, maladie cœliaque, etc.), et les femmes qui ont enfanté au cours de l’année.Son incidence est de 0,3 % chez la femme et sa prévalence est de près de 3 % de la population (étude réalisée dans la population anglaise).La grossesse peut causer une affection auto-immune transitoire de la glande thyroïde. L’hypothyroïdie peut alors survenir dans l’année suivant un accouchement (auquel cas elle dure de 6 à 12 mois en moyenne).Les causes sont multiples. La grande majorité est représentée par l'hypothyroïdie primaire, autrement dit un dysfonctionnement au niveau de la glande thyroïde même. L'hypothyroïdie secondaire est due à un dysfonctionnement de l'hypophyse qui secrète alors en quantité insuffisante la TSH ou « hormone de stimulation de la thyroïde ». Enfin, cas très rare, l'hypothyroïdie peut être due à une résistance périphérique aux hormones thyroïdiennes.À l’origine, l’hypothyroïdie était due essentiellement à une carence en iode. Depuis l’ajout de l’iode dans le sel de table, cette cause est devenue rare dans les pays industrialisés (mais reste fréquente dans les pays en voie de développement).En 1986, la catastrophe nucléaire de Tchernobyl rappelait au monde les dangers du nucléaire. Mais le lien entre cette pollution et l'augmentation des cancers de la thyroïde n'est toujours pas établi. Un nouveau rapport officiel dresse un premier bilan 20 ans après la catastrophe.[réf. souhaitée]carence chronique en iode ;thyroïdite qu'elle soit auto-immune (thyroïdite de Hashimoto) ou infectieuse (thyroïdite de De Quervain) ;cause iatrogène (thyroïdectomie au radioiode, effets secondaires de certains médicaments comme l'amiodarone, l'hypothyroïdie pouvant être transitoire, cédant à l'arrêt du traitement, ou définitive) ;maladie infiltrative (sarcoïdose, amyloïdose, hémochromatose…) ;trouble enzymatique de la thyroïde (génétique) ;dysgénésie thyroïdienne congénitale ;consommation d'aliments goitrogènes.insuffisance hypophysairelésion hypothalamiquedéficit en TSHC'est un syndrome rare caractérisé par des anomalies des récepteurs tissulaires des hormones thyroïdiennes qui altèrent la liaison entre l'hormone et sa cible.Cette anomalie est partiellement compensée par une hyperproduction hormonale, ce qui limite l'hypothyroïdie.Les premiers signes d'une hypothyroïdie sont souvent asymptomatiques et très légers. Cette affection peut être associée a un grand nombre de symptômes. Ils peuvent être dus à une pathologie causale sous-jacente à l'hypothyroïdie, à un effet de masse dû à un goitre, ou directement à la carence en hormones thyroïdiennes. Les signes sont listés ci-après,,.Dans les formes débutantes, on peut rencontrer des signes :généraux : ongles cassants, peau sèche, démangeaisons, gain de poids, rétention d'eau,,, myxœdème, chute de cheveux ;neuropsychiques : dépression, pensées incessantes et rapides, insomnie ;neuromusculaires : réflexes déprimés, hypotonie, des crampes musculaires, douleurs articulaires, l'instabilité de l'humeur, l'irritabilité, constipation ;métaboliques : fatigue, somnolence, frilosité et intolérance au froid, sudation diminuée, ralentissement métabolique général ;cardiovasculaires : bradycardie ;endocriniens : infertilité (féminine), règles irrégulières et galactorrhée.Lorsque la maladie est plus évoluée, peuvent exister des signes :généraux : amincissement des sourcils (signe de Hertoghe), dessèchement de la peau du visage ;neuropsychiques : élocution ralentie et dysphonie, migration de la voix dans les graves à cause de l'œdème de Reinke (pseudomyxome des cordes vocales) ;neuromusculaires : syndrome du canal carpien et paresthésie bilatérale (fourmillement, picotements, engourdissement des extrémités tactiles) ;métaboliques : hypothermie ;cardiovasculaires : hypotension ;endocriniens : goître (dépendant de la cause de l'hypothyroïdie), baisse de la libido principalement chez les hommes en raison d'une insuffisance de synthèse de la testostérone testiculaire.Plus rarement, peuvent être présents des signes :généraux : jaunissement de la peau causé par une déficience de conversion du bêta-carotène en vitamine A (caroténodermie), visage/mains/pieds bouffis, grossissement du volume de la langue ;neuropsychiques : troubles de la mémoire, psychose aiguë/schizophrénie dysthymique[réf. nécessaire], déficit d'attention, diminution du sens du goût et de l'odorat, surdité ;neuromusculaires : difficultés à avaler, essoufflement avec un rythme respiratoire profond et lent. Examens généraux Il peut exister une hyperlipémie, plus rarement, une hypoglycémie.Il peut exister une anémie due à une anomalie de synthèse de l'hémoglobine en rapport avec la diminution des niveaux d'érythropoïétine (EPO), la diminution de l'absorption du fer et du folate au niveau intestinal, ou à une carence en vitamine B12 consécutivement à l'anémie.Il peut exister une altération de la fonction rénale avec diminution du débit de filtration glomérulaire. Examens spécifiques Le diagnostic repose sur le dosage de la TSH qui est augmentée dans les formes primaires (de loin les plus courantes).Le dosage des hormones thyroïdiennes montre des taux bas, mais peut être normal dans les formes débutantes.La recherche d'anticorps anti-peroxydase et anti-thyroglobuline est utile afin de détecter un mécanisme auto-immun.Échographie du cou.Pas de scintigraphie thyroïdienne en cas d'hypothyroïdie.Les formes graves peuvent aller jusqu'à des troubles de la conscience, voire un coma, appelé « coma myxœdémateux ». L'hypothyroïdie peut également se compliquer d'une péricardite, d'épanchements pleuraux.L'hypothyroïdie, même fruste, semble être corrélée avec le risque de survenue de maladies cardio-vasculaires. La correction de celle-ci n'entraîne cependant pas une modification du risque.Le diagnostic est affirmé par un dosage hormonal guidé par la clinique. La T4 est abaissée sauf en cas de forme fruste (avec peu de symptômes). La TSH peut être basse ou augmentée selon la cause. En pratique seul le dosage de la TSH est réalisé dans un premier temps, et c'est en cas d'anomalie de celui-ci ou de forte conviction clinique que le dosage de T4 est fait.Le contexte clinique peut orienter vers une cause. Cependant, le dosage des anticorps (anti-thyroperoxydase voire anti-thyroglobuline) est souvent réalisé, une échographie n'est quant à elle, et contrairement au bilan d'hyperthyroidie,  pas nécessaire au bilan initial.La plupart du temps, le traitement doit faire appel à une substitution journalière en hormones thyroïdiennes, par voie orale. Il s'agit d'un traitement à vie qui exige un suivi médical impliquant également un dosage annuel de la TSH.Le médicament le plus utilisé est la lévothyroxine qui doit être donnée à doses progressives chez le patient âgé ou porteur d'une maladie cardiaque. Il entraîne la normalisation du taux de TSH, mais qui peut être différée de plusieurs mois.L'ajout d'iode dans le sel de cuisine est une mesure efficace pour prévenir la carence en iode, en particulier dans les pays en voie de développement.L'hypothyroïdie congénitale est dépistée systématiquement à la naissance en France, ce qui permet, avec un traitement approprié, d'éviter une affection autrefois dénommée crétinisme. Ce dépistage a été mis en place en 1975.Le dépistage d'une hypothyroïdie chez l'adulte est conseillé en France :pour les femmes de plus de 60 ans avec antécédent de pathologie thyroïdienne ;pour toute personne avec antécédent de traitement à risque : chirurgie, radiothérapie, médicament (amiodarone, lithium, interféron).(en) Cet article est partiellement ou en totalité issu de l’article de Wikipédia en anglais intitulé « hypothyroidism » (voir la liste des auteurs).HyperthyroïdieThyroxineThyroïdite d'HashimotoRessources relatives à la santé : ICD-10 Version:2016 Orphanet (en) Diseases Ontology (en) DiseasesDB (sv) Internetmedicin (en) Medical Subject Headings (en + es) MedlinePlus (en) NCI Thesaurus (no + nn + nb) Store medisinske leksikon (cs + sk) WikiSkripta Association française des malades de la thyroïde(en) « Hypothyroidism Booklet », sur American Thyroid Association, 2003 Portail de la médecine"
médecine;""
médecine;"L'inflammation est la réaction stéréotypée du système immunitaire, face à une agression externe (infection, trauma, brûlure, allergie, etc.) ou interne (cellules cancéreuses) des tissus. C'est un processus dit ubiquitaire ou universel qui concerne tous les tissus, faisant intervenir l'immunité innée et l'immunité adaptative. Elle est cependant inhibée dans le système immunitaire des muqueuses, dont le mécanisme d'action est spécifique.L'inflammation est identifiée en médecine par le suffixe -ite. Traditionnellement, les symptômes associés à l'inflammation sont décrits en latin par « dolor, calor, rubor, tumor, et functio laesa » (douleur, chaleur, rougeur, œdème et perte de fonctionnalité). Le plus gros problème qui découle de l'inflammation est que la défense de l'organisme attaque à la fois les agents nocifs et non nocifs, d'une manière qui endommage les tissus ou les organes sains.L'inflammation chronique est une réponse à de nombreuses transformations de l'environnement et du comportement modernes (elle est favorisée par la sédentarité, la mauvaise hygiène alimentaire (malbouffe), la pollution, les altérations du microbiote humain) et un facteur important dans le développement de maladies de civilisation telles que la résistance à l'insuline, l'obésité, les maladies cardiovasculaires, les maladies immunitaires, et même les troubles de l'humeur et du comportement.Dès les premières civilisations, on trouve des témoignages de sa connaissance et de sa guérison. Les premiers écrits sont apparus dans des papyrus égyptiens datant de 3000 av. J.-C. En Grèce et à Rome, un livre a été conservé, l'un des nombreux écrits de Aulus Cornelius Celso, encyclopédiste, ""De Medicinae"", dans lequel quatre signes cardinaux d'inflammation sont identifiés.Traditionnellement, à sa suite, les symptômes associés à l'inflammation sont décrits en latin par « dolor, calor, rubor, tumor, et functio laesa » (douleur, chaleur, rougeur, œdème et perte de fonctionnalité — la formulation des quatre premiers étant attribués à Aulus Cornelius Celsus et celle du cinquième souvent attribuée à Claude Galien). Si l’inflammation est connue depuis l’Antiquité, l’impotence fonctionnelle a été rajoutée à sa définition par Rudolf Virchow en 1858.En 1793, le chirurgien écossais John Hunter a souligné ce qui est aujourd'hui considéré comme une évidence : « L'inflammation n'est pas une maladie, mais une réponse non spécifique qui produit un effet curatif sur le corps dans lequel elle se produit ».Le pathologiste Julius Cohnheim a été le premier chercheur à utiliser le microscope pour examiner les vaisseaux sanguins enflammés dans des membranes minces et translucides, telles que le mésentère et la langue de la grenouille. Il observa la réorganisation initiale du flux sanguin, la formation de l'œdème après augmentation de la perméabilité vasculaire,la migration des leucocytes. En 1867, il démontra que l'émigration des globules blancs était à l'origine du pus. La contribution de Cohnheim fut fondamentale pour comprendre l'ensemble du processus inflammatoire.Le biologiste russe Ilya Ilitch Metchnikov découvrit le processus de phagocytose, en observant l'ingestion d'épines de rose par les amibocytes de larves d'étoiles de mer, et de bactéries par les leucocytes de mammifères (1882) ; la conclusion de ce chercheur était que l'objet de l'inflammation était de faire en sorte que des cellules ayant une capacité phagocytaire atteignent la zone lésée afin qu'elles puissent phagocyter des agents infectieux. Cependant, il est vite devenu clair que les facteurs cellulaires (phagocytes) et les facteurs sériques (anticorps) étaient essentiels à la défense contre les micro-organismes. En reconnaissance de cela, Metchnikoff et Paul Ehrlich (qui a développé la théorie humorale) ont partagé le prix Nobel de médecine en 1908. .A ces noms il faut ajouter celui de Sir Thomas Lewis qui, par de simples expériences sur la réponse inflammatoire de la peau, a établi le concept que divers produits chimiques induits localement par la stimulation d'une lésion, comme l'histamine, sont des facteurs médiateurs des altérations vasculaires. d'inflammation. Ce concept fondamental est à la base des importantes découvertes de médiateurs chimiques de l'inflammation et de la possibilité d'utiliser des médicaments anti-inflammatoires.L'inflammation aiguë peut être considérée comme la première ligne de défense contre les blessures infligées à un tissu. Elle fait partie du système immunitaire inné.Le processus d'inflammation aiguë est initié par les cellules immunitaires résidentes déjà présentes dans le tissu impliqué, principalement les macrophages résidents, les cellules dendritiques, les histiocytes, les cellules de Kupffer et les mastocytes. Ces cellules possèdent des récepteurs de surface appelés récepteur de reconnaissance de motifs moléculaires (PRR), qui peuvent reconnaître (c'est-à-dire se lier à) deux sous-classes de molécules, qui agissent comme signal déclencheur :les motif moléculaire associé aux pathogènes (PAMP). Les PAMP sont des composés qui sont associés à divers agents pathogènes, mais qui se ne sont pas présent dans les cellules de l'organisme. Ils signent donc la présence d'un agent pathogène ;les motif moléculaire associé aux dégâts (DAMP). Les DAMP sont des signaux de danger émis par les cellules de l'hôte, associés à des blessures ou à des dommages cellulaires.Au début d'une infection, d'une brûlure ou d'autres blessures, ces cellules reconnaissent un signal de danger pour l'organisme (l'un des PRR reconnaît un PAMP ou un DAMP) et s'activent, libérant des médiateurs inflammatoires responsables des signes cliniques de l'inflammation du tissu concerné.Les cellules immunitaires réagissent aux stress physiques détectés dans les tissus (chaleur, froid, pression) et produisent les médiateurs sérotonine et histamine, qui sont de puissants agents vasoactifs qui agissent sur la contraction et la perméabilité des vaisseaux artériels et veineux.Telle que définie, l'inflammation aiguë est une réponse immunovasculaire à des stimuli inflammatoires. Cela signifie que l'inflammation aiguë peut être largement divisée en une phase vasculaire, qui se produit en premier, suivie d'une phase cellulaire impliquant des cellules immunitaires (plus spécifiquement des granulocytes myéloïdes dans le cadre aigu).La réponse inflammatoire aiguë nécessite une stimulation constante pour être soutenue. Les médiateurs inflammatoires sont de courte durée et se dégradent rapidement dans les tissus. Par conséquent, l'inflammation aiguë commence à se résorber dès que le stimulus est supprimé.L'inflammation est déclenchée par l'action de médiateurs chimiques, qui déclenchent la phase vasculaire, ou vasculo-exsudative. On constate :La libération d'amines vaso-actives préformées par les mastocytes (histamine et sérotonine) ;L'activation de protéines plasmatiques inactives (facteur XII (Hageman), bradykinine, kallikréine, complément) ;La sécrétion de médiateurs lipidiques (prostaglandines dont prostacycline, leucotriènes, facteur d'activation plaquettaire (PAF)).En plus des médiateurs dérivés des cellules, plusieurs systèmes de cascade biochimiques acellulaires - constitués de protéines plasmatiques préformées - agissent en parallèle pour initier et propager la réponse inflammatoire. Ceux-ci incluent le système du complément, activé par les bactéries, et les systèmes de coagulation et de fibrinolyse, activés par la nécrose (par exemple, brûlure, traumatisme).Les facteurs chimiques produits durant l'inflammation (histamine, bradykinine, sérotonine, leucotrienes et prostaglandines) augmentent la sensation de douleur, induisent localement la vasodilatation des vaisseaux sanguins et le recrutement de phagocytes, en particulier les neutrophiles[réf. nécessaire]. Les neutrophiles peuvent également produire des facteurs solubles contribuant à la mobilisation d'autres populations de leucocytes. Les cytokines produites par les macrophages et les autres cellules du système immunitaire inné constituent un relais de la réponse immunitaire. On compte, parmi ces cytokines, le TNFα, HMGB1, et l'interleukine-1.Les trois cytokines majeures de l'inflammation sont l'interleukine-1, l'interleukine-6 et le facteur de nécrose tumorale,. On les nomme le trio pro-inflammatoire.Sous l'influence de médiateurs chimiques, les cellules endothéliales (formant les vaisseaux sanguins) s'activent. Cela entraîne une vasodilatation locale artériolaire puis capillaire qui provoque :une augmentation de l'apport sanguin ;une diminution de la vitesse du flux sanguin.Ce gonflement local des vaisseaux sanguins provoque la rougeur (rubor) et la sensation de chaleur (calor).L’augmentation de l'apport sanguin permettra d’évacuer les cellules mortes et les toxines (détersion), et d’apporter les éléments nécessaires à la guérison, notamment des globules blancs pour combattre les corps étrangers.Les molécules médiatrices modifient également les vaisseaux sanguins pour permettre la migration des leucocytes, principalement des neutrophiles et des macrophages, hors des vaisseaux sanguins (extravasation) et dans les tissus. Dans des conditions normales, l'endothélium ne permet pas la sortie des protéines et l'échange se fait par pinocytose. Au cours de l'inflammation, les bases morphologiques de l'endothélium sont altérées par l'action de médiateurs chimiques, produisant une altération des jonctions cellulaires et des charges négatives de la membrane basale. Généralement, cet effet se produit dans les veinules, mais s'il est très intense, il atteint les capillaires et une extravasation se produit en raison de la rupture.Parallèlement, les cellules endothéliales activées expriment des molécules d'adhésion (nécessaires à la diapédèse). De son côté, la diminution de la vitesse (stase) permet aux leucocytes de se marginaliser le long de l'endothélium, un processus essentiel à leur recrutement dans les tissus.Le vaisseau devenant plus perméable, l’eau du plasma sanguin s'épanche par osmose vers les tissus.La perméabilité accrue des vaisseaux sanguins entraîne une exsudation (fuite) de liquide dans les tissus. La fuite de liquide provoque une augmentation de la viscosité du sang, ce qui augmente la concentration des globules rouges (congestion veineuse).L’œdème inflammatoire (tumor) est donc la conséquence du passage du plasma (plus précisément d'un exsudat) dans la zone lésée. Il se traduit par un gonflement du tissu touché, et comprime les nerfs alentour, provoquant la sensation douloureuse et les démangeaisons (dolor). Certains des médiateurs libérés comme la bradykinine augmentent la sensibilité à la douleur (hyperalgésie). La perte de fonction (functio laesa) est probablement le résultat d'un réflexe neurologique en réponse à la douleur.L'œdème a plusieurs fonctions : il permet l'apport jusqu'à la lésion de moyens de défense (immunoglobulines, protéines du complément…), la dilution de l'agent pathogène, et la limitation du foyer inflammatoire.Le mouvement du liquide plasmatique entraîne avec lui des protéines importantes, telles que la fibrine et les immunoglobulines (anticorps), dans le tissu enflammé.Ce fluide tissulaire exsudé contient divers médiateurs antimicrobiens du plasma tels que le système du complément, le lysozyme, des anticorps, qui peuvent immédiatement endommager les microbes et opsoniser les microbes en vue de la phase cellulaire.Si le stimulus inflammatoire est une plaie lacérée, les plaquettes exsudatives, les coagulants, la plasmine et les kinines peuvent coaguler la zone blessée et assurer l'hémostase dans un premier temps.Ces médiateurs de la coagulation fournissent également un cadre structurel de mise en scène au site du tissu inflammatoire sous la forme d'un réseau de fibrine - comme le ferait un échafaudage de construction sur un chantier de construction - dans le but de faciliter le débridement phagocytaire et plus tard la réparation des plaies.Une partie du liquide tissulaire exsudé sera également acheminée par les vaisseaux lymphatiques vers les ganglions lymphatiques régionaux. Dans des conditions normales, le système lymphatique filtre et contrôle les petites quantités de liquide extravasculaire qui ont été perdues par les capillaires. Au cours de l'inflammation, la quantité de liquide extracellulaire augmente, et le système lymphatique participe à l'élimination de l'œdème. De plus, dans ce cas, une plus grande quantité de leucocytes, de débris cellulaires et de microbes passe dans la lymphe, pour lancer la phase de reconnaissance et d'attaque du système immunitaire adaptatif. Comme pour les vaisseaux sanguins, les lymphatiques prolifèrent également dans les processus inflammatoires, pour répondre à la demande accrue. Les vaisseaux lymphatiques peuvent devenir secondairement enflammés (lymphangite) ou les ganglions lymphatiques peuvent devenir enflés (lymphadénite), en raison d'une hyperplasie des follicules lymphoïdes et d'un nombre accru de lymphocytes et de macrophages.Les molécules d’adhésion (CAM, intégrines, sélectines) libérées par les cellules endothéliales sont un signal pour les leucocytes présents dans les vaisseaux sanguins, qui dans la région inflammatoire ont tendance à quitter le milieu du courant pour s’accoler à la paroi de l’endothélium du vaisseau, par « margination », favorisée par le ralentissement du flux sanguin.La diapédèse leucocytaire est le phénomène permettant le passage des leucocytes de la circulation sanguine jusqu'au foyer de l'inflammation. La traversée de l'endothélium par les leucocytes ou diapédèse intervient dans un segment particulier du système circulatoire : les veinules post-capillaires. On peut distinguer différentes étapes :Margination leucocytaire ;Rolling : interaction des leucocytes et des cellules endothéliales par l'intermédiaire de sélectines, adhérence faible entre une sélectine E capillaire et un carbohydrate de la cellule immunitaire ;Activation (activation) par interaction entre une chimiokine capillaire et une intégrine cellulaire ;Adhérence ferme (adhesion) entre l'intégrine cellulaire (LFA-1) et son ligand vasculaire (ICAM-1). La cellule est fixée sur l'endothélium ;Diapédèse : passage de la paroi endothéliale par les leucocytes, qui commence par l'adhésion cellulaire par l'intermédiaire des intégrines et de molécules d'adhésion (ICAM, en anglais : intercellular adhesion molecule).Après la migration des leucocytes hors des vaisseaux sanguins (extravasation), les cellules inflammatoires, dont les leucocytes, se dirigent ensuite de façon unidirectionnelle par chimiotaxie, le long d'un gradient créé par les cellules locales, pour atteindre le site de la lésion.À l'issue de la phase vasculaire ou vasculo-exsudative, la phase cellulaire fait suite à la diapédèse, lorsque les leucocytes sont amassés dans le tissu interstitiel.Elle correspond à la formation du granulome inflammatoire. Il participe à la détersion (rôle des granulocytes et des macrophages) et permet le développement de la réaction immunitaire adaptative. Les cellules composant le granulome ont également un rôle de sécrétion de médiateurs chimiques.Les leucocytes engloutissent les microbes et les détruisent, générant la production de pus. Le pus sera éliminé vers l'extérieur si la lésion est en contact avec l'extérieur, ou il générera un abcès si la zone où s'est formé le pus est à l'intérieur d'un organe.La réparation des lésions tissulaires s'effectue grâce aux macrophages, qui stimulent les fibroblastes pour synthétiser le collagène et les cellules endothéliales pour générer de nouveaux vaisseaux, grâce à la sécrétion de facteurs de croissance.L 'Inflammation systémique implique trois organes (le foie, le système nerveux central et les glandes surrénales) et le trio pro-inflammatoire (l'interleukine-1, l'interleukine-6 et le facteur de nécrose tumorale).Les molécules inflammatoires sensibilisent les terminaisons nerveuses. Les neurones relarguent la substance P et la CGRP des peptides qui ont des actions vaso-dilatatrices puissantes.L'inflammation peut :aboutir à la guérison de l'individu ;donner une cicatrice ;être limitée dans une partie du corps et persister sous forme de granulome comme le granulome pulmonaire de la tuberculose ;se diffuser dans tout l'organisme sous forme de septicémie aboutissant à un choc toxique souvent mortel.L’inflammation bien contrôlée est une réponse normale du corps qui nait s’amplifie et s’éteint. Elle est consécutive à une agression interne (comme un cancer) ou externe (comme une infection). Lorsque le corps n’arrive plus à maîtriser l’inflammation, celle-ci peut engendrer des maladies diverses comme le diabète, le cancer ou devenir chronique comme l’arthrite, la maladie de Crohn par exemple.Des efforts importants ont été déployés pour comprendre les mécanismes moléculaires inflammatoires et comment les combattre. En effet, une inflammation de trop longue durée ou trop intense peut avoir des effets délétères sur l’organe où elle siège et potentiellement entraver sa fonction. Les mécanismes de la phase d’initiation de l’inflammation sont maintenant bien compris. En revanche, les mécanismes de la phase d’arrêt de l’inflammation n’étaient jusque récemment pas connus. Ces dernières années, les travaux de l’équipe du professeur Charles Serhan, de l’École de Médecine de Harvard CETRI (Center for Experimental Therapeutics and Reperfusion : injury), ont permis de comprendre cette phase appelée résolution caractérisée par l’arrêt de la réponse inflammatoire, la réparation du tissu enflammé pour permettre finalement le retour à l’état initial du tissu appelé homéostasie.De nombreuses études scientifiques démontrent ainsi que le corps humain dispose de mécanismes naturels pour contrôler et programmer l’arrêt de l’inflammation. Ces mécanismes portent le nom de Résolution, processus associé à la synthèse d’une famille de molécules spécifiques appelée SPM (en) pour médiateurs spécialisés de la résolution ou Specialized Pro-resolving Mediators en anglais.On peut définir la résolution de l’inflammation comme le processus biologique naturel et indispensable pour stopper naturellement l’inflammation. Ce mécanisme est piloté par des médiateurs appelés SPM issus des acides gras polyinsaturés (AGPI) comme les oméga 3.Les AGPI qui donnent naissances aux SPM sont l’acide arachidonique (AA), l’acide docosahexahenoique (DHA), l’acide eicosapentaénoique (EPA) et l’acide docosapentaénoique (DPA). Ainsi l’AA va donner naissance aux lipoxines, l’EPA au résolvines de type E, le DHA aux résolvines de type D, aux marésines, aux protectines et le DPA aux résolvines de la famille n-3.Dans certains cas, le corps ne produit pas ces molécules en quantité suffisante ou au bon moment. L’arrêt de l’inflammation est alors altéré et peut s’accompagner de complications telles que fibrose, cicatrices ou perdurer de façon chronique.De nombreux travaux ont ainsi permis de mieux comprendre la finesse des mécanismes mis en place naturellement par notre organisme et de démontrer que les réponses inflammatoires chroniques semblent être dues à un défaut de résolution.A la faveur de ces découvertes, l’enjeu pour arrêter l’inflammation n’est donc plus de la bloquer, mais de la réguler en favorisant sa phase de résolution. Ce nouveau champ de recherches porte le nom de pharmacologie de la résolution.Les SPM agissent de façon différente aux anti-inflammatoires et représentent donc une alternative thérapeutique très prometteuse pour arrêter de façon programmée l’inflammation sans la bloquer.Ils agissent en :Contrebalançant l’effet des médiateurs pro-inflammatoires,Diminuant la pénétration des polynucléaires dans les tissus inflammés,Stimulant la défense innée par phagocytose (cellules apoptotiques, pathogènes…),Atténuant la douleur,Favorisant la régénération des tissus,Les SPM présentent des activités biologiques très bénéfiques car ils sont synthétisés au niveau du site inflammatoire et passent dans la circulation sanguine pour exercer leur activité à distance (activité autacoïde). Ils vont permettre ainsi d’arrêter l’inflammation (en inhibant les voies dépendantes de NF-kB par exemple). En permettant un arrêt programmé de l’inflammation, ils évitent le versant fibrotique d’une mauvaise cicatrisation et favorisent les capacités de défense de l’organisme (ils sont non immunosuppresseurs).Il existe actuellement deux méthodes principales pour mesurer les SPM :Les tests EIA (Enzyme Immunosorbant assay). Cette méthode est limitée car peu de kits sont disponibles et ils ne permettent de quantifier qu’un marqueur à la fois alors que les SPM sont une famille de molécule.La chromatographie liquide associé à la spectrométrie de masse (ou LC/MS/MS). Il s’agit de la technique de référence qui permet de coupler sensibilité et robustesse pour l’identification de plusieurs SPM en une analyse unique. Des méthodes de quantification des SPM par LC/MS/MS ont été décrites dans la littérature.Des études cliniques montrent que l’augmentation des SPM dans le corps est corrélée à une amélioration clinique de l’état inflammatoire.C’est le cas par exemple de l’étude Barden publiée en 2016qui a montré une corrélation négative entre la douleur perçue par le patient et la présence de RVE2 dans le liquide synovial suggérant que la production de SPM peut être associée à la gestion naturelle de l’inflammation et de la douleur par le corps humain.En effet dans les modèles animaux d’autres SPM ont également montré des effets atténuateurs sur la douleur,. Ces effets analgésiques sont médiés par des récepteurs spécifiques couplés aux protéines G.Pour aller plus loin dans les recherches sur le rôle des SPMs dans les maladies inflammatoires, l’Union européenne via son programme H2020, a sélectionné et financé le projet immunAID (H2020-EU.3.1.1. Grant agreement no: 779295). Ce projet est coordonné par l’INSERM et il est composé de 24 partenaires dans 12 pays.L’inflammation peut se manifester par :une rougeur (érythème correspondant à une vasodilatation locale) ;un gonflement (œdème) : augmentation de la formation de liquide interstitiel et d'œdème. ;une sensation de chaleur : Augmentation de la température de la zone enflammée. Elle est due à une vasodilatation et à une augmentation de la consommation locale d'oxygène ;une douleur qui semble pulser : La douleur apparaît comme une conséquence de la libération de substances capables de provoquer l'activation de nocicepteurs, telles que les prostaglandines ;une éventuelle altération du fonctionnement de l’organe concerné (ex. : difficulté à bouger (impotence fonctionnelle) dans le cas d'une articulation).On fait parfois référence aux noms latins, notamment dans les langues étrangères, pour décrire les manifestations de l’inflammation. Ces manifestations ont été décrites il y a 2 000 ans par Celsus : rubor (rougeur), calor (chaleur), tumor (gonflement), dolor (douleur), functio laesa (impotence fonctionnelle). Si l’inflammation est connue depuis l’Antiquité, l’impotence fonctionnelle a été rajoutée à sa définition par Rudolf Virchow en 1858.Le phénomène inflammatoire s'accompagne de modifications biologiques telles que l'augmentation de la concentration sanguine de plusieurs protéines dont l'haptoglobine, la céruloplasmine, des globulines, ou la protéine C réactive (CRP). Une électrophorèse des protéines plasmatiques permet d'objectiver ces changements dans leur globalité[pas clair].L'élévation des « protéines inflammatoires » accroît la vitesse de sédimentation.La ferritine augmente, reflétant la séquestration tissulaire du fer sérique. Cette séquestration est secondaire à l'augmentation de la sécrétion d'hepcidine, médiée notamment par l'interleukine 6. Cette séquestration est un des facteurs concourant à l'installation d'une anémie sur le long terme (anémie inflammatoire).Dans certains cas, une polynucléose neutrophile est présente.Des études récentes ont lié l'inflammation chronique à plusieurs types de pathologies, dites « maladies de civilisation » : maladies cardio-vasculaires, diabète et obésité...L'état inflammatoire chronique est reconnu favoriser le développement des tumeurs et a fortiori des tumeurs cancéreuses,.Inflammation aiguëInflammation chroniqueInflammation non spécifiqueInflammation spécifiqueInflammation granulomateuseL’inflammation, est une réaction de défense généralement bénéfique, mais pose parfois problème, par la douleur qu'elle engendre ou lorsqu'elle perdure et devient chronique, risquant alors de nuire à la structure ainsi qu'à la fonction de l'organe touché.Le froid (glace à travers un tissu par exemple) suffit parfois à combattre l’inflammation (il induite une vasoconstriction, diminuant l’œdème et calme la douleur).Des médicaments anti-inflammatoires peuvent calmer les symptômes ou limiter les effets délétères de l'inflammation sur l'organisme. On distingue les anti-inflammatoires non stéroïdiens et les glucocorticoïdes. Ces médicaments existent sous de nombreuses formes (orale, suppositoire, inhalation, perfusion ou bien locale par pommade, collyre...) selon les indications.Des thérapies récentes (biothérapies) bloquent spécifiquement certains médiateurs de l'inflammation (anti-TNFα, anti-IL4…). Elles ont révolutionné la prise en charge de maladies inflammatoires telles que la polyarthrite rhumathoïde ou les spondylarthrites ankylosantes mais avec des effets secondaires.Certains aliments contribuent à réduire l'inflammation - ou ses marqueurs sanguins -, en particulier les omega-3,,, (contenus dans les poissons gras et l'huile de lin ou de colza par exemple), les anthocyanes (contenus dans les fruits rouges et la betterave par exemple), le bêta-glucane (contenu par l'avoine et les grains entiers par exemple), le riz complet, le thym, le curcuma, le gingembre, le chou, l'ananas, l'huile d'olive, les noix, l'ail, les oignons. À l'inverse, les aliments à fort indice glycémique ou à forte charge glycémique (sucre, amidon par exemple) contribuent à augmenter ces marqueurs sanguins.La restriction calorique semble réduire l'inflammation. Dans une étude de restriction calorique portant sur 218 personnes pendant 2 ans, dénommée CALERIE, le taux de Protéine C réactive a baissé de 47 %.Basée sur l’approche développé par le Professeur Charles Serhan, Professeur à l’école de médecine de Harvard, qui a établi le concept de résolution de l’inflammation à la suite de la découverte des SPM, une approche micronutritionnelle peut également être développée pour lutter contre l’inflammation.Elle consiste en premier lieu à apporter à notre corps le substrat qui lui permet d'augmenter la synthèse de SPM lors d’une réponse inflammatoire, qu’elle soit aiguë, chronique ou de bas grade. Cet apport peut se faire via l’ingestion d’acides gras polyinsaturés spécifiques enrichis en SPM ou favorisant la production de SPM.Selon l’endroit où est située l’inflammation, elle peut prendre différents noms, en général en -ite :(en) « Inflammation », Science, vol. 374, no 6571 (numéro spécial),‎ 26 novembre 2021 (lire en ligne, consulté le 26 novembre 2021)InflammasomeMacrophageCytokineRessources relatives à la santé : (en) Medical Subject Headings (no + nn + nb) Store medisinske leksikon (cs + sk) WikiSkripta  Portail de la biologie   Portail de la médecine"
médecine;Une antibiothérapie est un traitement par antibiotique.Les indications à l'antibiothérapie sont les infections bactériennes.Il existe deux types d'antibiothérapie, la curative et la prophylactique ou préventive (antibioprophylaxie).Si besoin est, un prélèvement à visée bactériologique (afin de déterminer quel est le germe responsable de l'infection) est fait avant le début de l'antibiothérapie. Le choix du ou des antibiotiques est fait par le médecin en fonction du germe en cause, du foyer infectieux, de l'état du malade (terrain), des antibiotiques éventuellement déjà prescrits.Une antibiothérapie peut être le plus souvent une monothérapie à base d'un seul antibiotique ou parfois une bithérapie (deux antibiotiques) voire une trithérapie (trois antibiotiques) dans certains cas. L'association de plusieurs antibiotiques permet dans certains cas d'être plus efficace sur un germe identifié, ou d'avoir un maximum de chance d'être efficace si le germe n'est pas identifié.La durée de l'antibiothérapie varie selon le germe, sa localisation, le terrain, l'évolution de la maladie. Elle doit être décidée par le médecin et est poursuivi très souvent au-delà de l'amélioration des symptômes du malade afin d'éviter toute récidive.La surveillance de l'antibiothérapie se fait sur sa tolérance et l'apparition éventuelle d'effets secondaires, sur l'évolution clinique et biologique, sur les prélèvements devenus négatifs ou non, sur les dosages sanguins d'antibiotiques.Antibiotique | Résistance aux antibiotiques Portail de la médecine   Portail de la pharmacie
médecine;"Une maladie auto-immune est consécutive à une anomalie du système immunitaire conduisant ce dernier à s'attaquer aux composants normaux de l'organisme (le « soi », d'où la racine auto- pour parler de ce trouble de l'immunité).Parmi ces maladies peuvent être citées la sclérose en plaques, le diabète de type 1 — jadis appelé « diabète juvénile » ou « diabète insulino-dépendant » —, le lupus, les thyroïdites auto-immunes, la polyarthrite rhumatoïde, le syndrome de Goujerot-Sjögren, la maladie de Crohn, etc. On distingue classiquement les maladies auto-immunes spécifiques d'organes, qui touchent un organe en particulier (comme par exemple les maladies auto-immunes de la thyroïde), et les maladies auto-immunes systémiques, telles que le lupus, qui peuvent toucher plusieurs organes. Au début du XXIe siècle en Occident, les maladies auto-immunes sont devenues la 3e cause de mortalité/morbidité après le cancer puis les maladies cardiovasculaires et à peu près dans les mêmes proportions.Le système immunitaire est un ensemble de cellules et voies métaboliques conduisant à l'élimination d'une grande variété de pathogènes. Ce système repose sur la notion très centrale du soi opposé au non-soi ainsi qu'au soi modifié. Cette distinction s'effectue grâce à des marqueurs chimiques du soi (c'est-à-dire la reconnaissance de motifs antigéniques plus ou moins spécifiques) mais elle n'est pas véritablement innée : les cellules immunitaires naïves sont d'abord sensibilisées et sélectionnées en fonction de leur réactivité vis-à-vis de ces marqueurs du soi,. Cela explique notamment le fait que les individus chimériques n'expriment pas forcément « plus » d'auto-immunité que des individus monozygotes.Il existe donc chez tous les vertébrés une auto-immunité latente, laquelle est en temps normal inhibée par les mécanismes de régulation de la maturation des cellules immunitaires.On connaît ou suspecte diverses causes possibles :prédispositions génétiques ;facteurs environnementaux diffus :manque d'exposition-immunisation à des entités du non-soi (flore, pathogènes, parasites intestinaux, etc. ; par exemple les personnes non exposées aux parasites intestinaux ont plus de risques de développer une maladie de Crohn et les enfants parasités par des helminthes et dont l'hygiène est moins rigoureuse risquent moins de développer des allergies),dérégulation par des toxiques inhalés, ingérés ou acquis en passage percutané. L'exposition chronique et/ou périodiquement intense par inhalation aux poussières de silice (par exemple issues du sciage, ponçage ou meulage de ciment/béton en est une cause reconnue, confirmée par l'ANSES en 2019) ;séquelles infectieuses ;séquelles allergiques.Dans les pays industrialisés, les maladies auto-immunes touchent environ 8 % de la population, dont 78 % de femmes. Une forte prévalence de maladies auto-immunes (lupus érythémateux disséminé (SLE pour les anglophones) est constatée, sclérose en plaques (SEP),, cirrhose biliaire primitive, polyarthrite rhumatoïde (PR), et thyroïdite de Hashimoto notamment) chez les femmes. L'évolution de nombreuses maladies auto-immunes, leur gravité et leur pronostic varie aussi selon le sexe. Ceci n'est pas encore clairement expliqué, bien qu'il ait été prouvé que les taux d'hormones sont liés à la gravité de certaines maladies auto-immunes dont la sclérose en plaques.Chez les humains, et dans le modèle animal, le système hormonal semble avoir une importance majeure dans plusieurs phénomènes liés à ces maladies ; par exemple, les maladies auto-immunes sont plus fréquentes chez les personnes ayant une dysthyroïdie que dans la population générale, ce qui peut laisser supposer des mécanismes physiopathologiques communs et « justifie une surveillance des patients ayant une dysthyroïdie auto-immune et la réalisation d'un bilan initial et d'un suivi thyroïdien régulier chez les patients ayant une maladie auto-immune »,.En laboratoire chez le modèle murin, les femelles sont également plus touchées que les mâles par des maladies telles que le lupus érythémateux disséminé spontané (souris de souches (NZB×NZW)F1 et NZM.2328), l'encéphalomyélite allergique expérimentale (EAE, pour Experimental autoimmune encephalomyelitis) chez la souris SJL, la thyroïdite, le syndrome de Sjögren chez les souris de souche MRL/Mp-lpr/lpr, et pour le diabète chez les souris NOD. Les hormones sexuelles et/ou le patrimoine génétique hérité lié au sexe semblent donc être responsables de la sensibilité accrue des femmes à ces maladies auto-immunes.Chez l’animal, certains œstrogènes, la progestérone et les androgènes préviennent ou atténuent les signes cliniques des maladies auto-immunes alors que la castration chez le mâle les aggrave.Du fait de leurs propriétés immunologiques, promyélinisantes, neurotrophiques et neuroprotectrices des œstrogènes, progestatifs et androgènes, une régulation hormonale pourrait peut-être moduler l'évolution de maladies telles que la sclérose en plaques (qui est plus rare et plus tardive chez l'homme que chez la femme, mais plus grave). Chez les femmes, le rythme des poussées de cette maladie diminue en fin de grossesse, puis progresse après l'accouchement, alors que les sécrétions hormonales chutent. D'autres maladies auto-immunes semblent pouvoir répondre à une médication de type hormonal. L'influence de perturbateurs endocriniens pourrait possiblement être l'un des facteurs explicatifs de la récurrence croissante de certaines maladies auto-immunes.D'autre part — de manière générale — les femmes ont une réponse immunitaire différente que celle des hommes,, ; elles répondent notamment à l'infection, à une vaccination ou à des traumatismes avec une production plus importante d'anticorps et une production accrue de lymphocytes T auxiliaires Th2 (réponse immunitaire humorale prédominante), alors qu'une réponse par les lymphocytes T auxiliaires Th1 et l'inflammation sont généralement plus sévères chez les hommes. Cette différence d'intensité et de qualité de réponse immunitaire semble au moins en partie responsable de la plus grande vulnérabilité des femmes à un nombre important de maladies auto-immunes. À l'importance du sexe sur la réponse immunitaire s'ajoutent parfois les additionnels de l'importance du sexe sur les organes cibles de ces maladies auto-immunes, telles que le CNS dans la MS (Cerghet et al. 2006 ; Spring et al. 2007).Chez les deux sexes, les maladies auto-immunes commencent par une phase aiguë associée à une réponse immunitaire inflammatoire pour évoluer vers une phase chronique associée à la fibrose, mais des différences marquées existent selon le sexe :les maladies auto-immunes qui sont plus fréquentes chez les hommes se manifestent habituellement cliniquement avant cinquante ans. Et elles sont caractérisées par une inflammation aiguë, l'apparition d'autoanticorps, et une réponse immunitaire pro-inflammatoires de type Th1 ;les maladies auto-immunes qui prédominent chez les femmes se manifestent avec une phase aiguë (ex. : maladie de Basedow, lupus érythémateux systémique, sont des maladies connues pour être des pathologies médiées par des anticorps. Et les maladies auto-immunes qui ont une incidence accrue chez les femmes semblent cliniquement actives après l'âge de 50 ans et associés à une maladie chronique, fibrosique et « Th2-médiée ». Les Th17 augmentent l'inflammation par les neutrophiles et la fibrose chronique.Le sexe du sujet est donc un facteur à considérer comme particulièrement important dans les études sur l'auto-immunité, concernant les processus physiopathologiques du système immunitaire et des organes-cibles concernés.Les auto-anticorps sont des anticorps (Ac) dirigés contre des éléments de l'organisme qui les a fabriqués ; leur nombre est élevé.Certains de ces auto-anticorps sont plus fréquemment retrouvés dans certaines maladies appelées maladie auto-immune.Ces maladies sont :Anémies hémolytiques auto-immunes ;Anémie de Biermer (anémie pernicieuse) : Ac anti-facteur intrinsèque, anti-cellule pariétale ;Cholangite sclérosante primitive : Ac anti-cytoplasme des polynucléaires neutrophile ;Dermatite herpétiforme : Ac anti-gliadine, Ac anti-endomysium ;Diabète de type 1 : Ac anti-cellules Bêta du pancréas ;Épidermolyse bulleuse acquise : Ac anti-collagène VII ;Hypothyroïdie : Ac anti-thyropéroxydase ;Lupus érythémateux : Ac anti-ADN natif, Ac anti-Sm ;Maladie cœliaque : Ac anti-endomysium, Ac anti-gliadine, Ac anti-transglutaminase ;Maladie de Berger : Ac anti-glomérule rénal ;Maladie de Basedow : Ac anti-récepteurs de la TSH ;Maladie de Crohn : Ac anti-Saccharomyces cerevisiae ;Myasthénie : Ac anti-récepteur de l'acétylcholine (Ac anti-RACh) ;Pemphigoïde bulleuse : Ac anti-glycoprotéine intégrine ;Pemphigus profond : Ac anti-desmogléine ;Polymyosite : Ac anti-Jo 1, PL7, PL12, OJ, EJ ;Purpura thrombopénique idiopathique : Ac anti-plaquettes.Rectocolite hémorragique : Ac anti-cytoplasme des polynucléaires neutrophilesSyndrome de CREST (Le terme CREST tend à disparaître on parle actuellement de sclérodermie limitée) : Ac anti-centromères ;Syndrome de Stiff Man (ou maladie de l'homme raide) : Ac anti-GAD 65 et Ac anti-GABA ;Sclérodermie systémique : Ac anti-Scl 70 ;Syndrome myasthénique de Lambert-Eaton : Ac anti-canaux calciques voltage-dépendants (Ac anti-VGCC) ;Syndrome de Goujerot-Sjögren : Ac anti-SSA, Ac anti-SSB. ;Thyroïdite d'Hashimoto : Ac anti-thyroglobuline, Ac anti-thyropéroxydase ;Les maladies suspectées d'être des maladies auto-immunes sont :L'hidradénite suppurée ou Maladie de VerneuilÀ ce jour, il n'existe pas de traitement curatif. Il existe plusieurs traitements « suspensifs », qui limitent l'expression des symptômes mais qui ont leurs limites en raison de leur toxicité pour le système immunitaire et certaines cellules.De nouveaux traitements sont envisagés pour bloquer l'effecteur même. Ce sont souvent les mêmes médicaments que ceux utilisés pour éviter les rejets de greffe d'organes.Les principales molécules utilisées visent à supprimer l'activation de cellules à problème et/ou à les tuer ; ce sont ;les glucocorticoïdes (traitement de 1re ligne, et le plus ancien). Il est peu ciblant : le médicament aussi dit anti-inflammatoire freine toutes les réponses immunitaires, en affectant la prolifération des lymphocytes T ou B à dose normale et pouvant même les tuer à forte dose ; le médicament sert ici in fine à inhiber la production de cytokines. Les effets secondaires des corticoïdes sont nombreux, ce qui pousse les chercheurs à trouver de nouvelles molécules immunosuppressives, dont :le cyclophosphamide (qui tue les cellules en train de se diviser, et qui agit sur les lympohcytes B et moindrement T) ;l'Azathioprine ;l'acide mycophénolique ;des molécules agissant au stade de l'activation (plus précocement, en inhibant la calcineurine au moment de la reconnaissance des antigènes) ; elles sont utilisées depuis le début des années 1980 ; elles présentent l'avantage de ne pas tuer les cellules et d'avoir des effets plus réversibles, mais l'inconvénient d'être moins efficace sur des patients ayant déjà une maladie auto-immune avancée ; le traitement doit être précoce (exemple : Tacrolimus...) ;des inhibiteurs de la voie M-Tor 1 (ex. : Sirolimus) qui ont un effet antiprolifératif sur les lyphocytes T surtout.des inhibiteurs du protéasome (ex. : Bortézomib) ; uniquement actif contre les lymphocytes B et les plasmocytes (initialement créé pour traiter le cancer du plasmocyte) ;des anticorps (essentiellement monoclonaux, mais aussi polyclonaux), sous forme de « sérums anti-lymphocitaires », mais ils tuent tous les lymphocytes, y compris ceux qui ne sont pas impliqués dans l'autoimmunité ;des anticorps ; ce sont par exemple l'Alemtuzumab qui cible la molécule CD52 pour détruire des lymphocytes T. On utilise aussi des anticorps ciblant la molécule CD20 pour détruite des lymphocytes B, ou encore l'anti-IL2R contre les lymphocytes T activés (prévu pour traiter les greffes d'organes), ou des anticorps ciblant la cytokine TNF (utile pour certaines maladies de Crohn ou la polyarthrite rhumatoïde), des anticorps ciblant des cytokines telles que l'IL17 (interleukine 17), ou l'IL23, ou l'IL6R et/ou leur récepteur. On utilise aussi un anticorps ciblant la CTLA4 Ig (limitent les molécule régulatrice CD80 eet CD 86 en cause dans certains phénomènes auto-immuns).La recherche vise à utiliser des anticorps ciblant mieux leurs cibles (lymphocytes T et/ou B), ce qui permettrait de traiter certaines maladies auto-immunes avec moins de toxicité pour le patient.Les effets iatrogènes (secondaires) liés à la toxicité des médicaments sont un problème majeur du traitement des maladies auto-immunes, car ils sont par exemple source d'hypertension, de risque de cancer (si traitement au long-cours) ; l'immunosuppression facilite les infections virales, moindre production de cellules sanguines, neurotoxicité.4 approches se dessinent au début du XXIe siècle :La première vise à neutraliser des cellules effectrices telles que la TH17 (qui produit l'IL17) qui semblent impliquées et retrouvées dans les lésions de l'arthrite rhumatoide, du syndrome de Sjögren (ou syndrome de Gougerot-Sjögren), de la sclérose en plaques ou de la maladie de Crohn. Encore mieux serait de convertir des cellules effectrices en cellules normales, ce qui semble possible d'après quelques données expérimentales sur le modèle animal ;Une seconde approche consiste à tenter de moduler la présentation des antigènes ;La troisième approche vise à inhiber les effets des interférons ;La quatrième voie cherche à induire et amplifier (le nombre et/ou la fonction) des lymphocytes T régulateurs ; sur le modèle murin en laboratoire on a montré en 2015 que dans un même organisme malade (y compris au pic de la maladie), on peut trouver certaines TH17 sont des effectrices pathogènes et responsables de la maladie, alors que d'autres sont non-pathogènes et même régulatrices.(en) Ronald Asherson (éd.): Handbook of Systemic Autoimmune Diseases, Elsevier, en 10 volumes [3] :Ronald Asherson, Andrea Doria, Paolo Pauletto: The Heart in Systemic Autoimmune Diseases, Volume 1, 2004,  (ISBN 978-0-444-51398-4),  (ISBN 0-444-51398-1)Ronald Asherson, Andrea Doria, Paolo Pauletto: Pulmonary Involvement in Systemic Autoimmune Diseases, Volume 2, 2005,  (ISBN 978-0-444-51652-7),  (ISBN 0-444-51652-2)Ronald Asherson, Doruk Erkan, Steven Levine: The Neurologic Involvement in Systemic Autoimmune Diseases, Volume 3, 2005,  (ISBN 978-0-444-51651-0),  (ISBN 0-444-51651-4)Michael Lockshin, Ware Branch (éd.): Reproductive and Hormonal Aspects of Systemic Autoimmune Diseases, Volume 4, 2006,  (ISBN 978-0-444-51801-9),  (ISBN 0-444-51801-0)Piercarlo Sarzi-Puttini, Ronald Asherson, Andrea Doria, Annegret Kuhn, Giampietro Girolomoni (éd.): The Skin in Systemic Autoimmune Diseases, Volume 5, 2006,  (ISBN 978-0-444-52158-3),  (ISBN 0-444-52158-5)Rolando Cimaz, Ronald Asherson, Thomas Lehman (ed.): Pediatrics in Systemic Autoimmune Diseases, Volume 6, 2008,  (ISBN 978-0-444-52971-8),  (ISBN 0-444-52971-3)Justin Mason, Ronald Asherson, Charles Pusey (éd.): The Kidney in Systemic Autoimmune Diseases, Volume 7, 2008,  (ISBN 978-0-444-52972-5),  (ISBN 0-444-52972-1)Ronald Asherson, Manel Ramos-Casals, Joan Rodes, Josep Font: Digestive Involvement in Systemic Autoimmune Diseases, Volume 8, 2008,  (ISBN 978-0-444-53168-1),  (ISBN 0-444-53168-8)Ronald Asherson, Sara Walker, Luis Jara: Endocrine Manifestations of Systemic Autoimmune Diseases, Volume 9, 2008,  (ISBN 978-0-444-53172-8),  (ISBN 0-444-53172-6)R. Cervera, Ronald Asherson, Munther Khamashta, Joan Carles Reverter (éd.): Antiphospholipid Syndrome in Systemic Autoimmune Diseases, Volume 10, 2009,  (ISBN 978-0-444-53169-8),  (ISBN 0-444-53169-6)Carole Émilea: Comment faire le diagnostic de maladie auto-immune systémique ?, in: ""Immunologie"", mai 2009, volume 20, numéro 418-9, page 29 DOI:10.1016/S0992-5945(09)70132-1 [4](en) Vinay Kumar, Abul K. Abbas, Nelson Fausto, Jon Aster: Robbins and Cotran Pathologic Basis of Disease, Elsevier, 8e édition, 2010, 1464 pp.,  (ISBN 978-1-4160-3121-5)Stephani Sutherland, « Une nouvelle vision de l'auto-immunité », Pour la science, no 531,‎ janvier 2022, p. 24-32Melinda Wenner Moyer, « Les femmes surexposées ? », Pour la science, no 531,‎ janvier 2022, p. 34-39Frédéric Rieux-Laucat et Loïc Mangin, « Il y a un fort parallèle entre auto-immunité et cancer », Pour la science, no 531,‎ janvier 2022, p. 40-45Alain Fischer (2016), Comment traiter les maladies autoimmunes ?, Cours/conférence du 31 mai 2016 15:00 16:30 Cours Amphithéâtre Guillaume Budé - Marcelin BerthelotSystème immunitaireMaladie autoinflammatoire Portail de la biologie   Portail de la médecine"
médecine;"La maladie de Basedow ou Graves-Basedow est une hyperthyroïdie auto-immune (maladie de la thyroïde). La personne atteinte produit des anticorps anormaux (stimulant le recepteur de la TSH) dirigés contre les cellules folliculaires de la thyroïde. Plutôt que de détruire ces cellules, comme le ferait tout anticorps normal, ces anticorps reproduisent les effets de la TSH et stimulent continuellement la libération d'hormones thyroïdiennes, provoquant une hyperthyroïdie accompagnée de signes cliniques spécifiques. La maladie de Basedow ou de Graves, plus fréquente chez la femme que chez l'homme, se manifeste le plus souvent par une accélération du métabolisme basal, une diaphorèse, des pulsations cardiaques rapides et irrégulières, une augmentation de la nervosité et une perte pondérale. Il s'agit de sa forme la plus fréquente.Elle doit son nom à Carl von Basedow.La maladie de Basedow peut toucher tout le monde, mais essentiellement les individus entre 40 et 60 ans et plus rarement à l'adolescence. Elle est cinq à dix fois plus fréquente chez les femmes. Elle est la cause de près des trois quarts des hyperthyroïdies.Il existe un facteur génétique comme l'atteste une atteinte concomitante chez de vrais jumeaux ainsi que la présence d'antécédents familiaux.Comparativement au tabagisme actif persistant, la cessation de consommation de tabac diminue le risque de développer une maladie de Basedow, notamment dans sa forme oculaire, et plus particulièrement chez les femmes.Décrit à de nombreuses reprises, ce syndrome doit son nom à Carl von Basedow mais peut avoir plusieurs dénominations :goitre exophtalmique ;goitre toxique diffus ;hyperthyroïdie auto-immune ;maladie de Graves ;maladie de Graves-Basedow.La maladie est probablement de cause multifactorielle. Il existe une participation génétique et plusieurs gènes sont impliquées : CD40, CTLA4, PTPN22, FCRL3, gènes de la thyroglobuline et au récepteur à la TSH.Le stress peut avoir un rôle provocateur. Le tabagisme est un facteur de risque.La maladie est plus fréquente chez la femme, faisant suspecter une participation génétique et/ou hormonale. D'autres facteurs ont été identifiés : infection à Yersinia enterocolitica, déficit en sélénium ou en vitamine D.L'auto-immunité se développe à partir des anticorps anti-récepteurs de la TSH, dans lequel le corps fabrique des anticorps pour le récepteur de la thyroïde-stimulant hormone (TSH-R). Ces anticorps se lient aux récepteurs TSH qui se trouvent sur les cellules qui produisent des hormones thyroïdiennes, ce qui entraîne une production anormalement élevée de T3 et T4. C'est une hypersensibilité de type V.Ils comprennent l'association de signes d'hyperthyroïdie et de signes plus spécifiques de la maladie.Elle se caractérise par une asthénie, un amaigrissement contrastant avec un appétit conservé voire augmenté, une hypersudation, des attitudes d'évitement de la chaleur. Il peut exister des troubles psychologiques, une agitation, une nervosité, un tremblement, une soif permanente avec augmentation des mictions (polyurie-polydipsie)L'examen clinique montre une fréquence cardiaque accélérée (tachycardie), voire un rythme irrégulier.Les signes et symptômes sont les suivants :goitre (augmentation de volume de la thyroïde) ;exophtalmie (déplacement de l'œil hors de son orbite) ;myxœdème (infiltration cutanée) au niveau des tibias ;gynécomastie, (développement excessif des glandes mammaires chez l'homme) ;augmentation du rythme cardiaque ;augmentation de l'activité métabolique ;plus rarement, hippocratisme digital (déformation du doigt et des ongles).L'hyperthyroïdie est démontrée biologiquement par l'effondrement du taux de TSH dans le sang, couplé à un taux élevé de triiodothyronine (T3) et de thyroxine (T4). En cas de maladie de Basedow, il existe une élévation du taux d'anticorps anti-récepteur de la TSH (« TRAK »), hautement spécifique et sensible, et des TSI (Thyroid stimulating immunoglobulins).La scintigraphie thyroïdienne consiste en la visualisation du captage (fixation) par cet organe d'un composé radioactif. Elle montre typiquement une thyroïde augmentée de volume et hyperfixante. Elle permet de différencier la maladie de Basedow d'autres causes d'hyperthyroïdie, comme un nodule, dit « toxique », par exemple.La vascularisation du goitre est augmentée et peut être démontrée par un doppler de la thyroïde. L'échographie de la glande, couplée au doppler, a des résultats comparables à ceux de la scintigraphie. L'échographie permet par ailleurs de détecter les nodules et distinguer une thyroïdite d'un Basedow (baisse du débit dans le premier cas).L'imagerie peut ainsi mettre en évidence dans près de 35 % des cas de maladie de Basedow des nodules thyroïdiens associés, généralement non fonctionnels. Dans environ 1% des cas, il est retrouvé un ou des nodules fonctionnels en plus de la maladie de Basedow (syndrome de Marine-Lenhart).Insuffisance surrénalienneMyasthénieThyroïdite d'HashimotoDiabète de type 1Polyendocrinopathie auto-immuneSyndrome de MeansUne rémission spontanée se fait dans près d'un tiers des cas. Le risque de rechute à court terme après arrêt des antithyroïdiens de synthèse peut être prédit par la persistance d'un taux élevé d'anticorps anti-récepteur de la TSH. Il reste toutefois élevé, dépassant 50 %.Les trois options sont la prescription de médicaments antithyroïdiens de synthèse (méthimazole et propylthiouracile), l'ablation chirurgicale de la thyroïde et l'emploi d'iode radioactif (iode 131 : irathérapie), détruisant ainsi sélectivement la glande. Ces trois traitements ont une efficacité comparable mais le taux de rechute est plus élevé avec les médicaments (près de 40 %) qu'avec les deux autres méthodes. Le choix de l'une ou l'autre des options dépend d'un certain nombre de paramètres, dont font partie les habitudes locales (les États-Unis recourant de manière beaucoup plus fréquente que l'Europe à l'iode radioactif en première intention).Pour les antithyroïdiens de synthèse, deux techniques de prescription peuvent être utilisés : la titration (ou recherche de la dose minimale efficace) ou la prescription à doses importantes conduisant à une hypothyroïdie et à une substitution hormonale associée. dans tous les cas, un dosage des hormones thyroïdiennes doit être fait au premier mois, puis tous les trois mois si l'hyperthyroïdie est équilibrée. Le traitement est prolongé jusqu'à 18 mois, voire plus.La prise en charge de l'atteinte oculaire a fait l'objet de la publication de recommandations par l'European Group on Graves' Orbitopathy en 2008. L'exophtalmie nécessite une prévention des lésions oculaires, secondaire à une couverture insuffisante des paupières avec un risque de lésion de la cornée. Plus rarement, elle doit être traitée pour elle-même.Louis-Ferdinand Céline en 1933, met au point un produit, La Basedowine, enregistré au Laboratoire National de contrôle des médicaments sous le no 343-4 et commercialisé par les Laboratoires Gallier jusqu'en 1971.Glafira Ziegelmann, première femme interne de Montpellier et la première femme admissible à l'agrégation de médecine, consacre en 1898 sa thèse au traitement de la maladie de Basedow. Portail de la médecine"
médecine;"Une maladie infectieuse (ou infection) est une maladie provoquée par l'invasion d'un ou plusieurs micro-organismes ou agent infectieux (bactéries, champignons, parasites, protozoaires, virus) dans un tissu où ils se multiplient, et par une réaction générale des cellules et des tissus infectés pour éliminer ces agents pathogènes ou leurs toxines (processus impliquant notamment le système immunitaire des plantes et des animaux).L'étude des agents infectieux relève de la biologie, de la microbiologie médicale, de l'épidémiologie et de l'écoépidémiologie. Dans la nature, des maladies infectieuses se développent chez tous les organismes vivants (animaux, végétaux, fongiques, micro-organismes… il existe également des virus de virus). En tant qu'interactions durables, les maladies infectieuses font partie des boucles de rétroaction qui entretiennent la stabilité relative (équilibre dynamique) des écosystèmes, la plupart des pathogènes coévoluant avec leur hôte depuis des millions d'années. Leur mode de transmission est variable et dépend de leur réservoir (humain, animal, environnemental) et parfois de vecteurs (maladies vectorielles).Elles sont plus ou moins contagieuses. Par exemple, le tétanos est une toxi-infection causée par Clostridium tetani, une bactérie qui se trouve dans la terre. Il n’y a pas de transmission interhumaine, l’infection se produit lorsque la bactérie entre dans l’organisme par une plaie souillée. Un vaccin existe contre cette affection et est obligatoire en France pour tous les enfants d’âge scolaire. Autre exemple, le paludisme est dû à un parasite, le Plasmodium falciparum (il existe d’autres Plasmodii), transmis d’homme à homme par l’intermédiaire d’un moustique, l’anophèle. Le réservoir du parasite est humain mais il n’y a pas de transmission interhumaine. Il n’existe à l'heure actuelle pas de vaccin. La tuberculose se transmet d’homme à homme par mécanisme aéroporté : le réservoir est humain et c’est une maladie contagieuse. Les infections sexuellement transmissibles (ou encore MST pour maladies sexuellement transmissibles) se transmettent à l’occasion de rapports sexuels ou par le sang.De nombreux microbes vivent normalement et nécessairement dans notre tube digestif et sur notre peau, et ne deviennent infectieux qu'à certaines occasions. Le contact avec les microbes est nécessaire à l'entretien et au bon fonctionnement de la digestion et du système immunitaire.L'infection est le terme désignant soit une maladie infectieuse en général, soit la contamination par un germe. C'est la conséquence pathologique au niveau d'un tissu ou d'un organisme de la présence anormale et/ou de la réplication d’un germe bactérien, viral ou mycosique. La contamination est la pénétration du germe dans un organisme.L'infectiologie est la branche de la médecine concernant les maladies infectieuses. Le médecin spécialiste est un infectiologue. Suivant le type de germe, il est également question de bactériologie, de virologie, de parasitologie ou de mycologie.Un sepsis est une infection grave. L'adjectif septique se rapporte à un organisme ou un objet contaminé par un germe (fosse septique par exemple). Une septicémie est la contamination grave et durable (sans traitement) du sang par un germe. Une bactériémie est une contamination transitoire du sang par un germe. Lorsque les cas se multiplient dans un lieu et une période limitée, il est question d’épidémie. Si la diffusion est beaucoup plus généralisée, il est alors question de pandémie. Lorsque l'épidémie concerne le milieu animal, il est question d'épizootie. Lorsque le germe se transmet de l’animal à l’homme, il est question d'anthropozoonose ou plus simplement de zoonose.Le contage désigne la contamination par le germe.La période d’incubation est le délai entre le contage et la première manifestation de la maladie. Le malade peut être contagieux durant ce temps.La période de contagion est le temps pendant lequel le patient excrète le germe et peut le transmettre. Elle dépend de chaque maladie infectieuse.Les infections nosocomiales (ou iatrogènes) sont des infections attrapées à l’hôpital. Elles sont particulièrement complexes et dangereuses car elles surviennent chez des sujets affaiblis et concernent souvent des germes résistants aux antibiotiques. Il s’agit d’un problème de santé publique majeur.Comme le résumait en 1935 le bactériologiste français Charles Nicolle : « Malheureusement, les signes des maladies infectieuses sont presque tous les mêmes : fièvre, maux de tête, agitation ou stupeur, éruption. Seuls leur groupement, leur succession, une observation minutieuse ont pu, après de longs tâtonnements, permettre d'établir des tableaux symptomatiques particuliers et les distinguer entre eux. »Les maladies infectieuses sont responsables dans le monde de 17 millions de décès par an, soit un tiers de la mortalité et 43 % des décès dans les pays en voie de développement (contre 1 % dans les pays industrialisés). Les six maladies suivantes représentent 90 % des décès par maladies infectieuses dans le monde.Depuis les années 2000, de nombreuses urgences sanitaires reliées à l’émergence de nouveaux agents étiologiques responsables de maladies respiratoires sévères sont survenues : le syndrome respiratoire aigu sévère (SRAS), les infections d’influenza aviaire A (H5N1) chez les humains dans plusieurs pays de l’Asie, la pandémie de grippe A (H1N1) et, plus récemment, le virus influenza aviaire A (H7N9) en Chine, le coronavirus du syndrome respiratoire du Moyen-Orient (MERS-CoV) et la pandémie de Covid19. La pathogénicité et la létalité élevées de la plupart de ces virus génèrent des répercussions sociales et une pression importante sur les services de santé.La population mondiale infectée par le VIH continue de croître : rien qu’en 2000, 5,3 millions de nouveaux cas se sont déclarés dans le monde, dont la moitié parmi les jeunes de plus de 25 ans.Après une phase de forte régression (époque pastorienne et hygiéniste), les maladies infectieuses sont revenues ou sont devenues plus résistantes (antibiorésistance). Des maladies infectieuses émergentes ou réémergentes inquiètent périodiquement les épidémiologistes et les autorités sanitaires en raison de leurs impacts sanitaires, économiques et socio-politiques actuels ou potentiels. Le Haut Conseil de la santé publique (HCSP) a récemment fait 25 recommandations (sur la recherche et l'enseignement, la surveillance sanitaire et la gestion raisonnée des crises sanitaires notamment).Les progrès de l'hygiène et de la vaccination ont fourni un espoir de pouvoir les éradiquer, mais elles sont encore en France, la troisième cause de mortalité :Il est également noté que certaines infections sont aussi à l’origine de maladies inflammatoires chroniques (telles que l’asthme) et de cancers.Les maladies infectieuses entravent la santé de base des individus et ont une influence négative sur chaque indice du développement humain et plus particulièrement sur l'espérance de vie à la naissance, l'éducation et le PIB réel. Elles sont responsables d'une forte mortalité dans les régions où l'hygiène connaît un déficit et où l’accès aux soins est difficile. La malnutrition ainsi qu'un accès limité à l'eau potable sont autant de facteurs aggravants qui diminuent les chances de survie des malades mais aussi des enfants en bas âge de même que leurs conditions de développement. Ces deux facteurs désarment le système immunitaire et peuvent être vecteurs de maladies infectieuses.Ces maladies ont des conséquences négatives importantes sur le développement cognitif et les performances scolaires chez l’enfant. La malaria, entre autres, peut causer de graves séquelles, dont des troubles comportementaux, des problèmes moteurs et un manque d’autonomie. Une telle infection est donc un frein à l’éducation. Dans le cas des épidémies, il peut arriver que les enseignants soient eux aussi touchés par la maladie. Un manque de corps enseignant réduirait de façon directe la qualité de l’éducation en affaiblissant le système scolaire. Par ailleurs, si dans une famille, les responsables de l'éducation des enfants (souvent la mère) sont touchés par la maladie, c’est l’éducation dans son ensemble qui va être affectée. Le coût du traitement réduit le budget qui aurait pu être accordé à la scolarisation mais également les conditions de vie de l’enfant. Ce qui crée un cercle vicieux : les couches les plus éduquées de la population sont de moins en moins atteintes par des maladies infectieuses telles que le sida. En effet ces personnes qui sont les plus éduquées sont les mieux informées sur les modes de transmission et de prévention. Or, plus de 80 % des personnes atteintes par ces maladies vivent dans les pays en développement.D'un point de vue macroéconomique, les maladies infectieuses ont un impact sur la croissance économique et le PIB. Dans les pays en développement, la main d’œuvre est le facteur-clé de la production et donc du PIB. Néanmoins, le bon fonctionnement des entreprises et la possibilité d'être concurrent sur le marché international nécessitent avant tout une bonne santé et une éducation de base. Lorsque la santé de la personne génératrice de revenu pour la famille est affectée, toute la famille en souffre. Les maladies infectieuses aggravent donc la pauvreté, réduisent la croissance économique, le capital humain et contribuent à l’augmentation des inégalités entre les pays en voie de développement et les pays riches.La prévention des maladies infectieuses vise à limiter le risque infectieux (y compris professionnel, notamment pour les métiers de la santé, de contact avec les animaux, des déchets, des cadavres, des eaux usées, des échantillons à analyser en laboratoires de biologie, etc.).Elle s’articule en trois volets : éviter l’infection, renforcer les défenses immunitaires et prendre des traitements préventifs (prophylaxie) en cas de risque d’exposition.La maladie infectieuse est provoquée par la pénétration dans l’organisme d’une bactérie ou d’un virus. La première précaution consiste donc à « fermer les portes d’entrée », à savoir :les voies respiratoires : tousser ou éternuer dans un mouchoir, dans le coude, ou dans les mains (en se les lavant immédiatement après) pour éviter de contaminer l’entourage ; porter un masque facial lorsque des personnes vulnérables sont rencontrées (par exemple dans certaines zones des milieux hospitaliers, personnes immunodéprimées) ou porteuses de virus très contagieux (comme le sras) ; pour la ventilation artificielle, utiliser un filtre antibactérien ;les voies digestives : se laver les mains avant de manger ou de préparer un repas, ou après une exposition à des liquides biologiques (par exemple en sortant des toilettes), voire les désinfecter lorsqu’il s’agit de liquides d’une autre personne (par exemple accident d'exposition au sang) ; porter des gants fins (latex, ou pour les personnes allergiques en PVC ou nitrile) lorsqu’une telle exposition est probable ; en général laver les mains régulièrement pendant la journée ;effraction cutanée : toute plaie grave devra être montrée à un médecin qui prendra les mesures nécessaires ; toute plaie simple doit être nettoyée, ou mieux désinfectée (voir l’article bobologie) ; mais la première précaution est bien sûr d’éviter de se faire une plaie, en respectant les règles de sécurité de certaines activités et en portant des protections adaptées (gants de travail…) ;voie oculaire : éviter de se frotter les yeux et se laver les mains avant au cas où cela arriverait ; en cas de risque d’exposition à des liquides biologiques, porter des lunettes de protection ;sexualité : utiliser un préservatif pour réduire les risques de transmission des maladies sexuellement transmissibles.Le port d'équipements de protection individuelle dépend de l’évaluation des risques. Au travail outre des gants de protection, un appareil de protection respiratoire et des lunettes masques ou une visière sont parfois nécessaires, voire un vêtement de protection intégral.Les gants fins sont recommandés en cas de risque d’exposition à des liquides biologiques ou chimiques, mais déconseillé pour les activités courantes : en effet, la peau est alors dans une atmosphère chaude et humide propice au développement de germes, et par ailleurs, il vaut mieux des mains propres que des gants sales. À noter qu’au bout d’une vingtaine de minutes, certains gants fins deviennent poreux ou sont incompatibles avec certaines substances.Il faut aussi limiter le développement de germes pathogènes sur et dans le corps et dans l’habitation, par une hygiène suffisante :hygiène corporelle : se laver, se brosser les dents ;hygiène ménagère : avoir un réfrigérateur créant un froid suffisant, décongelé et nettoyé régulièrement, laver les couverts, assiettes et verres après utilisation, stocker les ordures dans des poubelles dédiées et ramassées régulièrement par les services municipaux, évacuation des eaux usagées vers une fosse septique vidangée régulièrement ou vers les égouts, rangement et nettoyage de l’habitation, aérer pour limiter la pollution intérieure (acariens, composés organiques volatils) et donc les allergies et les maladies respiratoires ;surveiller et traiter les parasitoses (certaines facilitent les maladies infectieuses, virales ou bactériennes). Par exemple, chez le porc, l'ascaris augmente le risque de bronchopneumonie, la trichocéphalose l'entérite hémoragique, l'oesophagostomum les salmonelloses, les strongyloides le rouget, les metastrongylus la grippe porcine, etc.Les collectivités territoriales jouent un rôle important en ce qui concerne l’hygiène collective, avec la gestion des eaux pour fournir de l’eau potable, l’organisation de la collecte et du traitement des ordures, l’équarrissage des cadavres d’animaux et la police des funérailles et des lieux de sépulture (condition de transport et de conservation des corps avant crémation ou inhumation, gestion des cimetières et crématoriums).La première mesure consiste à avoir une bonne hygiène de vie : alimentation saine, exercice physique régulier, sommeil suffisant, éviter les comportements à risque (tabagisme, excès d’alcool), ce qui permet d’avoir un meilleur état de santé général donc de mieux résister aux infections.Par ailleurs, il convient de respecter les vaccinations préventives obligatoires, ou recommandées comme la vaccination des personnes âgées contre la grippe.Il faut aussi prendre précautionneusement les médicaments prescrits par un médecin, en lisant systématiquement les notices accompagnatrices, riches en informations (effets secondaires, interactions avec d’autres médicaments, recommandations…) et ne pas hésiter à questionner le médecin ou le pharmacien en cas de doute. Les effets peuvent ne pas être immédiats, et il faut continuer le traitement jusqu’à la fin même en cas d’amélioration et disparition des symptômes, notamment dans le cas des antibiotiques : la disparition des symptômes signifie la diminution du nombre de germes, mais pas leur disparition, si le traitement est interrompu trop tôt, ceux-ci peuvent se redévelopper, et devenir résistants à l’antibiotique.Il ne faut pas que le médecin prescrive systématiquement d’antibiotique : ils ne sont pas efficaces contre les maladies virales.Les mesures d’hygiènes simples sont les meilleurs traitement préventifs : lavage des mains, pour éviter la transmission des infections alimentaires, éternuer dans ses coudes lors d'un Éternuement et non pas dans ses mains afin de ne pas les « contaminer » par d'éventuels microbes… Il est parfois nécessaire de prendre des médicaments à titre préventif, comme les médicaments contre le paludisme lors d’un voyage dans un pays impaludé.La détection précoce d’une maladie permet de démarrer son traitement plus tôt et donc de réduire la mortalité ; il est recommandé de faire au moins une visite médicale par an. En cas de doute sur une infection (par exemple plaie souillée, accident d’exposition au sang, rapport sexuel non protégé), le médecin pourra mettre en place un traitement préventif pour diminuer les risques de développement d’une maladie. Pour les maladies sexuellement transmissibles, il existe en France des centres anonymes et gratuits de dépistage.Certains patients doivent être isolés (voire mis en quarantaine) pour éviter la dissémination du germe : ainsi, lors d’une varicelle, l’enfant ne doit pas aller à l’école pendant 15 jours à partir de la première éruption. Il s’agit de l'éviction scolaire. La prévention hospitalière des infections nosocomiales est un sujet complexe. Elle repose essentiellement sur l’hygiène des soignants et des soignés (lavage des mains), sur l’isolement des patients porteurs de germes résistants aux antibiotiques, mais aussi sur une antibiothérapie ciblée et adaptée.Une nouvelle approche en phase d'étude est d'utiliser la phagothérapie à des fins préventives pour la santé humaine comme cela se fait déjà dans l'agriculture et l'industrie alimentaire.Leur étude relève de l'épidémiologie et pour les zoonoses ainsi que de l'écoépidémiologie.Certaines situations (crises sanitaires ou alimentaires…) ou lieux (ports, aéroports) sont des facteurs de risques.Le traitement par antibiotiques est le traitement qui a permis de vaincre les maladies infectieuses jusqu'à l'apparition des bactéries multi-résistantes.Il présente de nombreux avantages dont la possibilité d'une fabrication en masse, rapide et bon marché des médicaments antibiotiques.Il trouve ses limites avec l'apparition de bactéries de plus en plus résistantes.La phagothérapie est apparue au début du XXe siècle avec le développement par le Français Félix d'Hérelle de médicaments bactériophagiques réalisés à partir de virus bactériophages (simplement appelés bactériophages ou même phages) lytiques afin de traiter certaines maladies infectieuses d’origine bactérienne. D'Hérelle a ainsi traité des épidémies de peste et de choléra avec succès.La phagothérapie a été largement utilisée dans le monde avant la découverte des antibiotiques. Si elle a été progressivement abandonnée par les pays occidentaux séduits par les avantages de l’antibiothérapie, la phagothérapie traditionnelle est toujours employée et développée dans les pays de l'ancienne Union Soviétique. Dans les pays occidentaux, des patients victimes d'infection par bactéries multi-résistantes se regroupent pour faciliter l'accès aux traitements bactériophagiques étrangers,,.Elle connaît un regain d'intérêt en Occident avec l'émergence de l'antibiorésistance. Elle fait l'objet de recherches à l'Institut Pasteur mais son utilisation demeure soumise à ATUn par l'ANSM.Antoine van Leeuwenhoek (1632-1723) voit pour la première fois des agents bactériens en microscopie.Louis Pasteur permet le rapprochement entre maladie et agents infectieux. Première vaccination contre la rage.Robert Koch est célèbre pour sa découverte du bacille de la tuberculose qui porte son nom : le bacille de Koch.Jonas Salk et Albert Sabin assurent le développement de la vaccination anti-polio.Charles Nicolle, Destin des maladies infectieuses, PUF 1939Brown L. (2010), ""Le plan B pour un pacte écologique mondial"", Paris, Calmann-Lévy Souffle Court Editions, 509 pages.Contrepois A. L'invention des maladies infectieuses. Édition des Archives Contemporaines. 2001. Naissance et développement institutionnel de la bactériologie médicale en France et en Allemagne au XIXe siècle.Flahaut A. et Zylberman P. Des épidémies et des hommes. Édition de la Martinière. 2008. Une bonne vulgarisation par deux experts de la question, avec nombreuses photos et illustrations.INSTITUT PASTEUR , ""Le défi des maladies infectieuses"", http://www.pasteur.fr/ip/easysite/pasteur/fr/presse/dossiers-de-presse/sante-en-voyage/le-defi-des-maladies-infectieuses, dernière visite le 8 mars 2014.Nicolle C. Le destin des maladies infectieuses. Édition France Lafayette. 1993. Réédition d'un grand classique de 1933. Conférences au Collège de France par Charles Nicolle, Prix Nobel de Médecine 1928. Toujours d'actualité.Orth G. et Sansonetti P. (sous la direction de). La maitrise des maladies infectieuses. Académie des Sciences. EDP Sciences. 2006. État des lieux et recommandations adressées aux pouvoirs publics et à l'ensemble des acteurs de santé. Un ouvrage collectif à l'aspect sévère, mais une actualisation pointue de tous les aspects (médico-scientifiques, socio-culturels, etc.) du problème.Raoult D. (1999), ""Les nouvelles maladies infectieuses, que sais-je ?"", Presses universitaires de France, 128 pages.Dossier documentaire Société Française de Santé PubliqueRessources relatives à la santé : ICD-10 Version:2016 ICD9Data.com (en) Diseases Ontology (en) Medical Subject Headings (en) NCI Thesaurus (no + nn + nb) Store medisinske leksikon (cs + sk) WikiSkripta Infectiologie.com Portail de la médecine   Portail de la microbiologie   Portail des maladies infectieuses"
médecine;Les maladies inflammatoires chroniques de l’intestin (MICI) regroupent les maladies liées à l'inflammation de l'intestin à caractère chronique.Leur origine est inconnue, mais qui dans un certain nombre de cas au moins pourraient être liées à une prédisposition génétique (conséquence de l'évolution humaine et de l'aseptisation de l'environnement actuel) et aux faibles doses de nombreux résidus de produits toxiques contenus dans l'alimentation moderne. Des études de 2015 ont montré un accroissement du nombre de bactériophages du virobiote qui implique une baisse de la diversité microbienne.La maladie peut se définir comme une inflammation chronique de l'intestin qui induit à la fois une modification du microbiote qui en retour entretient l'inflammation. La modification du microbiote (déficit dans certaines bactéries) est par ailleurs multifactorielle: génétique, alimentation, interaction avec le virobiote (phages), traitements antibiotiques.Parmi les maladies concernées, il faut distinguer ses deux principales formes que sont :la maladie de Crohn pouvant concerner tout le tube digestifla rectocolite hémorragique (ou colite ulcéreuse) limitée aux régions du rectum et parfois du côlonBien que différentes maladies existent, différents symptômes sont similaires :douleurs abdominalesdiarrhéevomissementrectorragie ou hémorragie rectale (hématochézie)perte ou gain de poidsainsi que des manifestations extradigestives dans 25 % des cas, comme l'arthrite par exemple.L'âge moyen de découverte d'une MICI se situe entre 15 et 35 ans, en général à la suite de différents examens, tels que :IRM fonctionnelleendoscopie digestive, telle que coloscopie (ou colonoscopie), anuscopie ou rectoscopie par exemplebiopsiedéfécographiemanométrie anorectaleendosonographieélectromyographieéchographie endoanalepet scancapsule vidéo endoscopiqueDifférent traitements sont prescrits, selon le niveau de gravité de la maladie. Dans les cas les plus graves les MICI peuvent exiger une immunosuppression afin de contrôler les symptômes, via des médicaments tels que l'azathioprine, le méthotrexate ou la mercaptopurine, voire une forme de mésalazine.L'inflammation du tube digestif amène des complications nutritionnelles et en particulier des carences que le traitement cherchera à compenser. Chez 126 patients atteints de maladies inflammatoires de l'intestin les carences concernaient l'hémoglobine (40 %), la ferritine (39,2 %), la vitamine B6 (29 %), le bêta-carotène (23,4 %), la vitamine B12 (18,4 %), la vitamine D (17,6 %), l'albumine (17,6 %) et le zinc (15,2 %). Toutes ne sont pas liées aux apports alimentaires inadéquats (fréquents dans ces maladies amenant à des choix alimentaires particuliers) : vitamine E (63 %), vitamine D (36 %), vitamine A (26 %), calcium (23 %), acide folique (19 %), fer (13 %), et vitamine C (11 %).Chez les enfants récemment diagnostiqués, une carence en zinc est aussi observée, tout particulièrement chez les enfants atteints de la maladie de Crohn.Des compléments alimentaires et un bilan régulier des teneurs en vitamine B6 sont recommandées.L'anémie, fréquente dans les MICI, doit être avérée avant la prise de compléments de fer.L'Inserm a mis en évidence un déficit en Elafine dans la pathologie, et envisage une bactérie probiotique génétiquement modifiée. Une étude anglaise révèle que la consommation de fructose peut aggraver l'inflammation intestinale des personnes atteintes de maladies inflammatoires chronique de l'intestin.Les MICI peuvent limiter grandement la qualité de vie en raison des douleurs et autres conséquences pouvant nécessiter une hospitalisation. Toute personne atteinte d'une MICI ne peut guérir, mais doit suivre un traitement à vie et fait l'objet d'un suivi médical pour surveiller et traiter l’évolution de la maladie : risque de fistule intestinale et risque accru de cancer colorectal.Microbiote intestinalRessources relatives à la santé : Orphanet (en) Diseases Ontology (en) DiseasesDB (en) Medical Subject Headings (en) NCI Thesaurus «MICI, chronique d’un intestin malade», La Méthode scientifique, France Culture, 20 janvier 2020 Portail de la médecine
médecine;"La maladie est une altération des fonctions ou de la santé d'un organisme vivant.On parle aussi bien de la maladie, se référant à l'ensemble des altérations de santé, que d'une maladie, qui désigne alors une entité particulière caractérisée par des causes, des symptômes, une évolution et des possibilités thérapeutiques propres.Un ou une malade est une personne souffrant d'une maladie, qu'elle soit déterminée ou non. Lorsqu'elle fait l’objet d'une prise en charge médicale, on parle alors de patient(e).La santé et la maladie sont liées aux processus biologiques et aux interactions avec le milieu social et environnemental. Généralement, la maladie se définit comme une entité opposée à la santé, dont l'effet négatif est dû à une altération ou à une désharmonisation d'un système à un niveau quelconque (moléculaire, corporel, mental, émotionnel…) de l'état physiologique ou morphologique considérés comme normal, équilibré ou harmonieux. On peut parler de mise en défaut de l'homéostasie.Les termes maladie et malade proviennent du latin male habitus signifiant qui est en mauvais état.Ce terme est unique en français, italien et espagnol, alors que l'anglais et l'allemand disposent de doublons tels que illness et disease, Erkrankung et Krankheit qui expriment des distinctions particulières de sens.Il n'existe pas de terme commun désignant la maladie dans le groupe des langues indo-européennes, on note l'existence de nombreux synonymes dont la signification étymologique appartient à quatre champs sémantiques :la faiblesse, la perte de force, l'incapacité à travailler ;la difformité et la laideur ;la gêne, le trouble, le malaise ;la souffrance et la douleur.Le concept initial d'état morbide ou de maladie s'appuie sur un critère objectif (incapacité de fournir un travail pour soi ou pour la société), et un critère subjectif (de la gêne ou indisposition à la douleur aiguë).Ce concept n'est pas socialement neutre, car il implique un jugement moral et esthétique : il y a la maladie, mais aussi le mal, le mauvais, et le laid. Disease, illness, sickness En français, les termes « maladie » et « malade » sont utilisés de façon indistincte pour signifier « avoir une maladie » (reconnue par un médecin), « être malade » (se sentir mal), « être un malade » (être reconnu comme tel par l'entourage ou la société).L'anglais utilise trois termes, plus ou moins interchangeables, mais en principe utilisés le plus souvent dans un contexte spécifique. Disease se rapporte à une perturbation biomédicale, objectivée par une maladie reconnue par un médecin, dans le cadre d'une pathologie référencée (nosologie).Illness se rapporte à l'expérience vécue, personnelle et intime, de la maladie : « je me sens, ou je suis, malade ».Sickness se rapporte à la perception de la maladie dans le cadre de l'entourage non-médical (social ou culturel) : «je suis un malade» (reconnu comme tel). Limites et extensions Il a été montré en 1989 que plus les étudiants en médecine étaient avancés dans leur cursus plus ils avaient tendance à qualifier de maladie les conditions parmi 38 qui leur étaient présentées, sans que cette qualification n'ait de lien fort avec les propriétés de gravité, curabilité, responsabilité du patient ou causalité externe. L'idée de maladie, plutôt qu'être parfaitement définie, évolue donc chez l'étudiant en fonction de son avancement dans le cursus.Classifier un certain état comme une maladie est aussi un fait social d'évaluation. Ainsi, certains états ne sont reconnus comme des maladies que dans certaines cultures, ou à certaines époques, et pas dans d'autres. On parle alors de syndromes culturels. Parfois la catégorisation d'un état comme une maladie est controversé au sein d'une même société. L'hyperactivité et l'obésité sont par exemple des états de plus en plus considérés comme des maladies par l'opinion publique dans les pays occidentaux mais n'étaient pas ainsi considérés il y a encore quelques décennies, et ne le sont toujours pas dans certains pays.La maladie est à différencier des blessures, handicaps, syndromes et affections.Une blessure est une lésion, physique ou psychique.Un handicap est une déficience qui peut aussi bien être due à une maladie qu'à une blessure.Un syndrome est un ensemble de signes ou de symptômes qui apparaissent simultanément. Ainsi l'usage médical distingue une maladie, qui a une cause spécifique connue, d'un syndrome, qui ne se préoccupe pas des causes.Une affection désigne une altération de fonctions qui est rattachée à un organe spécifique et qui ne prend en compte ni les causes, ni les symptômes, ni le traitement. Tout comme les syndromes, elle est parfois distinguée d'une maladie.Par extension, on peut associer la maladie à des entités non biologiques pour signifier qu'elles sont altérées ou que leur fonctionnement n'est plus considéré comme bon. Il est ainsi habituel d'entendre les termes « société malade » ou « entreprise malade » par exemple.La maladie humaine est le noyau fondateur de la médecine, une grande partie de la connaissance médicale étant orientée vers la maladie et ses solutions.La science vétérinaire concerne les maladies qui affectent les animaux, dont les zoonoses.La phytopathologie est la science qui concerne les maladies qui affectent les plantes et autres sujets botaniques.La pathologie est la branche de la médecine traitant des causes et des symptômes des maladies dans leur ensemble. Le terme est souvent utilisé fautivement pour désigner la maladie elle-même, ou ses manifestations, y compris par des médecins.La pathogénie est l'étude des mécanismes responsables du déclenchement et du développement d'une maladie.L'ontologie est l'étude de la genèse des entités médicales telles que les maladies, les signes cliniques et les syndromes.L'étiologie est l'étude spécifique des causes et des facteurs d'une maladie.La séméiologie, ou sémiologie médicale, est la branche de la médecine qui traite des signes cliniques et des symptômes des maladies et de la façon de les présenter.Le diagnostic est la réflexion menant à l'identification de la nature d'une maladie à partir des symptômes relevés par les observations.Le pronostic est la prévision de la progression de la maladie et des chances éventuelles de guérison.La prophylaxie désigne le processus ou l'ensemble de mesures visant à prévenir la propagation ou l'apparition d'une maladie.La thérapeutique est la section de la médecine s'occupant de l'étude des traitements.La nosologie est la branche de la médecine qui étudie les critères de classification systématique des maladies.Les facteurs des maladies sont le domaine d'étude de l'étiologie et physiologie. Catégorisation des facteurs  Facteurs intrinsèques et extrinsèques Il existe de nombreux facteurs différents pouvant entraîner l'apparition d'une maladie.Ces facteurs peuvent être aussi bien intrinsèques qu'extrinsèques à l'organisme concerné par la maladie.La présence d'un facteur intrinsèque n'exclut pas celle d'un facteur extrinsèque, et inversement. Ainsi, de nombreuses maladies résultent d'une combinaison de facteurs intrinsèques et extrinsèques. Liste Les facteurs peuvent être répartis dans les catégories suivantes :Facteurs chimiquesFacteurs économiquesFacteurs sociauxFacteurs psychologiquesFacteurs biologiquesFacteurs environnementauxLes facteurs environnementaux incluent les produits chimiques toxiques (par exemple les acétaldéhydes dans la fumée de cigarette et les dioxines relâchées lors de l'utilisation d'Agent orange) et les agents infectieux (par exemple les virus de la varicelle ou de la polio).Certains facteurs peuvent faire partie de plus d'une catégorie. Facteurs biochimiques C'est le cas des causes biochimiques de maladies qui peuvent être considérées comme un spectre où à l'une des extrémités la maladie est causée exclusivement par des facteurs génétiques (par exemple les répétitions CAG dans le gène HD (ou gène huntingtine ou encore gène IT15) qui cause la maladie de Huntington) et à l'autre causée entièrement par des facteurs environnementaux.Entre ces deux extrêmes, gènes et facteurs environnementaux interagissent pour causer la maladie comme c'est le cas pour la maladie inflammatoire appelée maladie de Crohn où les gènes NOD2/CARD15 et la flore intestinale jouent chacun un rôle. L'absence de facteur génétique ou environnemental dans ce cas a pour résultat l'absence de manifestation de la maladie. Étude des facteurs environnementaux Les postulats de Koch peuvent être utilisés pour déterminer si une maladie est causée par un agent infectieux. L'émergence de nouvelles maladies infectieuses est liée aux activités humaines perturbant l'équilibre des écosystèmes.Par exemple, l'Institut de recherche pour le développement indique que « le déboisement des forêts primaires reste l'une des causes principales de l'apparition de nouveaux agents infectieux et de leur circulation épidémique dans les populations humaines ». En effet, les forêts jouent un rôle essentiel pour la biodiversité terrestre, élément stabilisateur des agents pathogènes. Étude des facteurs génétiques Pour déterminer si une maladie est causée par un facteur génétique, les chercheurs étudient la présence de la maladie dans l'arbre généalogique familial.Cela fournit des informations qualitatives à propos de la maladie, c'est-à-dire comment elle est héritée.Un exemple classique de cette méthode de recherche est l'héritage de l'hémophilie dans la famille royale britannique. Plus récemment cette méthode a été utilisée pour identifier le gène Apoliprotéine E (ApoE) comme un gène susceptible d'être lié à la maladie d'Alzheimer, bien que certaines formes de ce gène (ApoE2) en soient moins susceptibles.Pour déterminer jusqu'à quel point une maladie est causée par des facteurs génétiques, c'est-à-dire pour obtenir des informations quantitatives, des études sur des jumeaux sont effectuées. Les jumeaux monozygotes sont génétiquement identiques alors que les jumeaux dizygotes sont seulement génétiquement similaires. De plus des jumeaux, qu'ils soient monozygotes ou dizygotes, partagent souvent un environnement similaire. Ainsi en comparant l'incidence de la maladie (nommée taux de concordance) chez des jumeaux monozygotes avec l'incidence de la maladie chez des jumeaux dizygotes, la contribution de chaque gène à la maladie peut être déterminée.Les gènes suspects peuvent être identifiés grâce à plusieurs méthodes. L'une d'entre elles est la recherche de mutation d'un organisme modèle (par exemple les organismes Mus musculus, Drosophila melanogaster, Caenhorhabditis elegans, Brachydanio rerio et Xenopus tropicalis) qui possèdent un phénotype similaire à la maladie étudiée. Une autre approche est la recherche de ségrégation de gènes ou l'utilisation de marqueurs génétiques (par exemple les polymorphismes nucléotidiques et marqueurs de séquences exprimées). Maladies complexes Les maladies complexes sont dues à l'interaction entre un profil génétique particulier et un environnement particulier. Quelques exemples :ObésitéDiabète sucréHypertension artérielleAthérome et athéroscléroseAsthmeMaladies dysimmunitaires ou auto-immunesMaladies neurodégénératives (maladie d'Alzheimer, maladie de Parkinson, sclérose latérale amyotrophique)Un symptôme se distingue d'un signe. Le symptôme est l'expression subjective des effets ressentis par le malade alors que les signes en sont l'expression objective déduite par le médecin, ou plus généralement de la personne réalisant un diagnostic.Certaines maladies sont contagieuses ou infectieuses, comme c'est le cas par exemple de l'influenza (ou grippe). Les maladies infectieuses peuvent être transmises par un grand nombre de mécanismes, incluant l'expulsion de particules dans l'air lors d'un éternuement ou d'une toux, les fomites (objets contaminés par des pathogènes), les morsures et piqûres d'insectes ou autres animaux vecteurs porteurs de la maladie, et l'absorption d'eau ou de nourriture contaminée.Il existe également des infections ou maladies sexuellement transmissibles (MST ou IST). Ce sont des maladies infectieuses qui se transmettent au cours de rapports sexuels, ou de contacts sanguins. Au début du XXIe siècle, un des principaux représentants de ces maladies est le SIDA. Un représentant plus ancien est la syphilis.Certaines maladies sont dites non transmissibles, elle ne se transmettent pas directement. Il y a par exemple les maladies liées à l'environnement.Une des principales mesures permettant d'éviter la propagation d'une maladie parmi une population ou seulement le développement d'une maladie chez un individu est la prévention.Elle peut se décomposer en trois parties :La prévention, qui a pour but de réduire la probabilité d'apparition de la maladie (ex : vaccination).La prévision, qui doit prévoir des mesures pour combattre le sinistre si celui-ci survient.La protection, qui a pour but de limiter l’étendue et la gravité de la maladie ou de l'épidémie, lorsqu'elle est déjà présente (ex : amputation, quarantaine).En médecine, on parle plus particulièrement de prophylaxie, le processus qui vise à prévenir les épidémies et la propagation d'une maladie. La prophylaxie est, plutôt qu'un traitement médical, une promotion de la prise de conscience générale des bonnes conduites à adopter face à la maladie.Les principales mesures de prévention de la maladie sont l'amélioration de l'hygiène et la vaccination.La thérapeutique est la section de la médecine s'occupant de l'étude des traitements.Les traitements consistent souvent, suivant le niveau évolutif de la société humaine concernée, en la prise de médicaments à base de molécules de synthèse ou bien de remèdes produits à partir de l'environnement naturel. Il existe toutefois de nombreuses autres thérapies, telles la radiothérapie ou la kinésithérapie, n'ayant pas recours à l'ingestion et à l'injection de substances extérieures.L'identification d'un état comme une maladie, plutôt que comme une simple variation de la structure humaine ou de fonctions, peut avoir des implications sociales et économiques significatives et peut changer le statut social de l'être concerné.La maladie peut parfois entraîner l'exclusion sociale des personnes touchées. Un exemple est l'exclusion des lépreux, courante en Europe depuis le Moyen Âge, et leur regroupement dans des établissements appelés léproseries dans le but de limiter la propagation de la maladie par contagion.La peur de la maladie a été et est encore un phénomène social très répandu, bien que toutes les maladies, notamment les plus bénignes, n'aient pas ce genre de répercussions sociales.Dans certains pays, les maladies infectieuses les plus dangereuses, du point de vue du risque épidémique, sont des maladies à déclaration obligatoire, c'est-à-dire qu'elles doivent être déclarées aux autorités dès qu'elles sont diagnostiquées par le médecin ou le vétérinaire.Certains dispositifs ont également été mis en place dans de nombreux pays pour éviter ou compenser les effets néfastes de la maladie. C'est dans cette optique qu'est apparue l'assurance maladie, qui est un dispositif chargé d'apporter une compensation financière à un individu subissant ou ayant subi une maladie.Une dérive consiste à élargir les descriptions nosographiques des maladies tout en y sensibilisant le grand public afin d'augmenter le marché de certains fournisseurs de traitements contre ces mêmes maladie. Cette pratique est appelée le disease mongering.L'étude des différentes classifications de la maladie concerne la branche de la médecine appelée « nosologie ».Il existe différentes tentatives de classification des maladies. Toutefois, du fait de la constante évolution de la médecine, elles ne sont pas figées. Les maladies peuvent être catégorisées en fonction de leurs causes et facteurs, de leurs symptômes ou des fonctions et organes touchés. On parle alors respectivement de classification étiologique, nosographique et fonctionnelle. Classification étiologique Maladies par agents physiques (froid, chaleur, etc.)Maladies toxiques (produits chimiques, poisons, etc.)Maladies parasitaires (champignons, vers, etc.)Maladies infectieuses (virus, bactéries, etc.)Maladies traumatiques (chocs psychologiques ou physiques, brûlures, etc.)Maladies dyscrasiques (troubles des métabolismes, troubles génétiques, etc.)Maladies psychiques (facteurs psychiques, bien que ces maladies puissent aussi avoir les mêmes facteurs que les maladies précédentes) Classification fonctionnelle Dysfonctionnements moléculaires (au niveau de la molécule)Dysfonctionnements cellulaires (au niveau de la cellule)Dysfonctionnements organiques (au niveau de l'organe)Dysfonctionnements corporel (au niveau d'un système d'organes)Dysfonctionnements mental (au niveau psychologique)On peut également séparer les maladies en :maladies aiguës et maladies chroniques, suivant qu'elles aient un développement rapide ou étalé ;en maladies bénignes et maladies malignes, suivant leur gravité ;en maladies locales et maladies générales, suivant l'étendue de la zone touchée ;en maladies évitables et inévitables.L'Organisation mondiale de la santé publie et est responsable de l'évolution de la Classification internationale des maladies, poursuite des travaux de Jacques Bertillon. Cette classification permet le codage des maladies, des traumatismes et de l'ensemble des motifs de recours aux services de santé grâce aux codes CIM (ou ICD en anglais). Elle permet également l'analyse systématique et l'interprétation des causes de morbidité et de mortalité dans le monde entier. Son but est notamment l'organisation et le financement des services de santé.De nombreuses cultures ont tenté de donner une signification et une origine à la maladie.Dans la mythologie grecque, l'apparition de la maladie est expliquée par l'ouverture de la boîte de Pandore. Zeus, qui voulait se venger des hommes à la suite du vol du feu par Prométhée, ordonne la création de Pandore, femme qu'il envoie auprès du frère de ce dernier. Pandore apporte avec elle une boîte qu'il lui est interdit d'ouvrir. La curiosité la pousse à le faire tout de même et c'est ainsi qu'elle libère la maladie et les autres maux de l'humanité que la boîte contenait.Au Proche-Orient ancien, l'origine naturelle de la maladie est concevable, mais elle se rajoute à une origine surnaturelle, par exemple la colère des dieux, la première étant la conséquence de la seconde.À partir de 1860, la pensée tendait vers l'idée que les homosexuel(le)s souffraient plutôt d'une maladie. Cette position de la communauté médicale et scientifique a perduré jusque vers les années soixante, où plusieurs voix se sont manifestées pour remettre en question cette vision de l'homosexualité. En 1974, l'Association américaine de psychiatrie a éliminé l'homosexualité de sa liste des maladies mentales, le Manuel diagnostique et statistique des troubles mentaux. Le 17 mai 1990, c'était au tour de l'Organisation mondiale de la santé de prendre la même position et de retirer l'homosexualité de sa Classification internationale des maladies dans sa dixième version (CIM-10).La maladie a inspiré de nombreuses créations artistiques.Le personnage du malade tient par exemple la place centrale dans Le Malade imaginaire, la dernière comédie écrite par Molière.Mais aussiHôpital général, de Slaughter : Tous les aspects de la médecine y sont représentés : l’organisation hospitalière…L’Hôpital, d’Alfonse BoudardLe Pavillon des cancéreux de Soljenitsyne : le cancerLa Mort du pantin, de Pierre MoustiersUn cri, de Michèle LoriotLa Peste, de Camus : les épidémiesLe Hussard sur le toit de Giono : Le choléraOpération épidémie, de SlaughterLa Montagne magique de Thomas Mann : La tuberculoseUn grand patron, de Pierre Véry : la formation médicaleLe Destin de Robert Shanon, de CroninLe Médecin de campagne de Balzac : l’exercice de la médecineLe Docteur Pascal de ZolaVoyage au bout de la nuit de L.-F. CélineLes Hommes en blanc, d’André SoubiranLe Livre de San Michele, d’Axel MuntheSept morts sur ordonnance, de Georges Conchon : les problèmes morauxKnock, de Jules Romain : l’arrivisme, les tentations et dérives de la médecineLes Grandes Familles, de Maurice Druon : la tentation des honneurs avec le professeur LartoyLes Thibault, de Roger Martin du GardOscar et la Dame rose, d’Eric-Emmanuel Schmitt.Philippe Adam et Claudine Herzlich, Sociologie de la maladie et de la médecine (1994), Paris, Armand Colin, 2014.Marc Augé et Claudine Herzlich (dir.), Le Sens du mal. Anthropologie, histoire, sociologie de la maladie, Bruxelles, Éditions des archives contemporaines, coll. « Ordres sociaux », 1984.Philippe Batifoulier, Capital-Santé. Quand le patient devient client, Paris, La Découverte, 2014.Frédéric Bauduer, Histoires des maladies et de la médecine, Paris, Ellipses, coll. « Sciences humaines en médecine », 2017.Henri Bergeron et Patrick Castel, Sociologie politique de la santé, Paris, PUF, coll. « Quadrige », 2014.Max Blecher, Aventures dans l’irréalité immédiate, suivi de Cœurs cicatrisés, trad. d’Elena Guritanu, Paris, L’Ogre, 2015.Max Blecher, La Tanière éclairée, trad. par Georgeta Horodinca et Hélène Fleury, Paris, Maurice Nadeau, 1989.Norbert Elias, La Solitude des mourants (1982), trad. Sybille Muller, suivi de Vieillir et mourir : quelques problèmes sociologiques, trad. Claire Nancy, Paris, Christian Bourgois éditeur, 1987.Dr. Christophe Fauré, Vivre ensemble la maladie d'un proche, Albin Michel, 2002.Jean-Claude Fondras, Santé des philosophes, philosophes de la santé, Nantes, éditions nouvelles Cécile Defaut, 2014.Elodie Giroux et Maël Lemoine (dir.), Philosophie de la médecine. Santé, maladie, pathologie, Paris, Vrin, 2012.Xavier Guchet, La Médecine personnalisée. Un essai philosophique, Paris, Les Belles Lettres, 2016.Hervé Guibert, À l'ami qui ne m'a pas sauvé la vie, Paris, Gallimard, coll. « Folio », 1990.Céline Lefève, Lazare Benaroyo et Frédéric Worms (dir.), Les Classiques du soin, Paris, PUF, 2015.Thomas Mann, La Montagne magique (1924), trad. Maurice Betz, Paris, Le Livre de poche, 1991.Claire Marin, Violences de la maladie, violence de la vie, Paris, Armand Colin, 2008.Ruwen Ogien, Mes Mille et Une Nuits : la maladie comme drame et comme comédie, Paris, Albin Michel, 2017.Roselyne Rey, Histoire de la douleur, Paris, La Découverte, coll. « Histoire des sciences », 1993 ; nouvelle édition avec des postfaces de Jean Cambier et Jean-Louis Fischer, Paris, La Découverte, 2011.Susan Sontag, La Maladie comme métaphore (1977, 1978), trad. Marie-France de Paloméra, suivi de Le Sida et ses métaphores, trad. Bruce Matthieussent, Paris, Christian Bourgois éditeur, 1993.Virginia Woolf, De la maladie (1930), trad. Élise Argaud, Paris, Payot & Rivages, 2007.Ressources relatives à la santé : Orphanet (en) Classification internationale des soins primaires (en) Diseases Ontology (en) Medical Subject Headings (en) NCI Thesaurus (no + nn + nb) Store medisinske leksikon (cs + sk) WikiSkripta (fr) Site officiel de l'Organisation mondiale de la santé(fr) Site officiel du Ministère de la Santé, de la Jeunesse et des Sports français.(fr) (en) Site officiel de Santé Canada.(fr) Site officiel du Ministère de la Santé et des Services sociaux du Québec(fr) Site officiel du Service public fédéral de la santé belge.(fr) Site officiel du Ministère de la Santé du Congo-Kinshasa(fr) Site officiel du Ministère de la Santé du Luxembourg(fr) (en) (de) (it) Site officiel de l'Office fédéral de la santé publique suisse.(fr) Classification étionosographique des pathologies sur psychobiologie.ouvaton.org Portail de la médecine"
médecine;"La médecine (du latin : medicina, qui signifie « art de guérir, remède, potion »), au sens de pratique (art), est la science témoignant de l'organisation du corps (anatomie), son fonctionnement normal (physiologie), et cherchant à préserver la santé (physique comme mentale) par la prévention (prophylaxie) et le traitement (thérapie) des maladies. La médecine humaine est complémentaire et en synergie avec la médecine vétérinaire.La médecine contemporaine utilise l'examen clinique, les soins de santé, la recherche et les technologies biomédicales pour diagnostiquer et traiter les blessures et les maladies, habituellement à travers la prescription de médicaments, la chirurgie ou d'autres formes de thérapies.Il n'existe pas suffisamment de données fiables pour déterminer le début de l'usage des plantes à des fins médicinales (phytothérapie). Les données médicales contenues dans le Papyrus Edwin Smith peuvent être datées du XXXe siècle av. J.-C.. Les premiers exemples connus d’interventions chirurgicales ont été réalisés en Égypte aux alentours du XXVIIIe siècle av. J.-C. (voir chirurgie). Imhotep sous la troisième dynastie est parfois considéré comme le fondateur de la médecine en Égypte antique et comme l'auteur originel du papyrus d’Edwin Smith qui énumère des médicaments, des maladies et des observations anatomiques. Le papyrus gynécologique Kahun traite des maladies des femmes et des problèmes de conception. Nous sont parvenues trente-quatre observations détaillées avec le diagnostic et le traitement, certains d'entre eux étant fragmentaires. Datant de 1800 av. J.-C., il s’agit du plus ancien texte médical, toutes catégories confondues. On sait que des établissements médicaux, désignés par l’expression Maisons de vie ont été fondés dans l’Égypte antique dès la première dynastie.Les plus anciens textes babyloniens sur la médecine remontent à l’époque de l’ancien empire babylonien dans la première moitié du IIe millénaire av. J.-C. Cependant, le texte babylonien le plus complet dans le domaine de la médecine est le Manuel de diagnostic écrit par Esagil-kin-apli le médecin de Borsippa, sous le règne du roi babylonien Adad-ALPA-iddina (1069-1046 av. J.-C.).Hippocrate, est considéré comme le père fondateur de la médecine moderne et rationnelle,, et ses disciples ont été les premiers à décrire de nombreuses maladies. On lui attribue la première description des doigts en baguette de tambour, un signe important pour le diagnostic de la bronchopathie chronique obstructive, du cancer du poumon et des cardiopathies cyanogènes congénitales. Pour cette raison, le symptôme des doigts en baguette de tambour est parfois appelé hippocratisme digital . Hippocrate a également été le premier médecin à décrire la face hippocratique. Shakespeare fait une allusion célèbre à cette description dans sa relation de la mort de Falstaff dans Henry V, acte II, scène III,. Le Corpus hippocratique popularise la théorie des humeurs. La médecine rationnelle grecque et latine coexiste cependant pendant toute l'Antiquité avec les cultes des Dieux guérisseurs.Agnodice (Hagnodice) ou Hagnodikè (en grec ancien : Ἁγνοδίκη) fut, selon une légende grecque rapportée par Hygin (Caius Julius Hyginus) dans la 274e de ses Fabulae, l'une des premières femmes médecin et gynécologue. Issue de la haute société athénienne, elle se déguisa en homme pour suivre les cours de médecine du célèbre médecin Hérophile. Vers 350 av. J.-C., elle passa l'examen et devient gynécologue, mais sans révéler qu'elle était une femme.La médecine pratiquée et enseignée en occident a ses racines dans les connaissances acquises et protocolées de l'Antiquité au Ier millénaire av. J.-C. de l'Orient à l'Empire romain.Elles proviennent de la Torah, étonnement rationnelle en la matière, car tenant compte des conditions climatiques. En effet, les cinq livres de Moïse qui la constituent, contiennent diverses « lois » ayant des conséquences directes sur la santé à travers différents rituels, tels que l'isolement des personnes infectées (Lévitique 13:45-46), le lavage des mains après avoir manipulé un cadavre (Livre des Nombres 19:11-19) et l’enfouissement des excréments à l’extérieur du campement (Deutéronome 23:12-13).La traduction dans les années 830-870 de 129 œuvres du médecin grec Galien (1er siècle av J.C.) en arabe par Hunayn ibn Ishaq et ses élèves sert de modèle à la médecine des civilisations islamiques et se propage rapidement à travers l’Empire arabe, reprenant en particulier, l'insistance de Galien sur une approche rationnelle et systématique de la médecine. Qusta ibn Luqa joua aussi un rôle important dans la traduction et la transmission des textes grecs. Les médecins musulmans ont mis en place certains des premiers hôpitaux, institution qui importée en Europe à la suite des croisades.En Europe occidentale, l'effondrement de l'autorité de l’empire romain a conduit à l’interruption de toute pratique médicale organisée. La médecine était exercée localement, alors que le rôle de la médecine traditionnelle augmentait, avec ce qui restait des connaissances médicales de l'antiquité. Les connaissances médicales ont été préservées et mises en pratique dans de nombreuses institutions monastiques qui s’étaient souvent adjoint un hôpital et disposaient de carrés d'herbes médicinales. Une médecine professionnelle organisée est réapparue, avec la fondation de l’école de médecine de Salerne en Italie au XIe siècle qui, en coopération avec le monastère du Mont Cassin, a traduit de nombreux ouvrages byzantins et arabes.À partir du XIe siècle, l'Église veut dissocier la vocation de moine de la profession de médecin. La volonté d'encadrer le savoir aboutit à la formation d'universités aux mains des ecclésiastiques. Les médecins de l'université de médecine de Montpellier, dépositaires des doctrines des médecins juifs et arabes, privilégient les plantes, ceux de l'Ancienne université de Paris privilégient la purge et la saignée.Au XIXe siècle, Karl August Wunderlich publie Das Verhalten der Eigenwärme in Krankheiten, qui établit que la fièvre est seulement un symptôme et met fin au credo d'une maladie infectieuse jusqu'alors nommée « fièvre intermittente ». En 1881 Theodor Billroth réalise la première gastrectomie, il révolutionne la chirurgie du pharynx et de l'estomac. En utilisant l'analyse statistique, le médecin Pierre-Charles Alexandre Louis (1787-1872) montre que l'utilisation des saignées chez les malades atteints de pneumonie n'est pas bénéfique mais néfaste. Ceci esquisse la notion d'étude randomisée en double aveugle.Madeleine Brès (1842-1921) est la première femme de nationalité française à accéder aux études de médecine en 1868, mais sans avoir le droit d'accéder aux concours. Elle obtient son doctorat en médecine, en 1875 et devient gynécologue et pédiatre. Elle démontre dans sa thèse que le lait du nourrisson se modifie au cours de l'allaitement et crée une des premières crèches parisiennes. Elizabeth Garrett Anderson, britannique la devance de cinq ans en France dans l'obtention de son doctorat.En 1854, Florence Nightingale est la première à utiliser les statistiques pour réorganiser les soins aux blessés de la guerre de Crimée et faire baisser la mortalité des soldats,,.Le 25 novembre 1901, Aloïs Alzheimer décrit le tableau clinique de la maladie qui porte son nom, dont il n'existe toujours aucun traitement connu à ce jour. Les traitements médicaux font des progrès spectaculaires avec l'invention de nouvelles classes de médicaments. Felix Hoffmann dépose le brevet de l'aspirine le 6 mars 1899. En 1909, le Nobel de médecine Paul Ehrlich invente la première chimiothérapie en créant un traitement à base d'arsenic contre la syphilis. En 1921 Frederick Banting de l'université de Toronto isole l'insuline et invente un traitement du diabète sucré. Le premier antibiotique date de 1928 avec la découverte de la pénicilline par Alexander Fleming.Selon la psychanalyste argentine Raquel Capurro, la médecine a été le premier domaine influencé par le positivisme d'Auguste Comte, à partir du milieu du XIXe siècle, à travers des personnalités telles que le docteur Robinet parmi d'autres.La délimitation de ce qui est médecine et de ce qui ne l'est pas est source de débat.La plus grande partie de cet article traite de la médecine telle qu'elle s'est développée à partir de l'époque moderne, et pratiquée à partir du XIXe siècle. Les innovations majeures apportées par la médecine occidentale à partir du XIXe siècle (anesthésie et asepsie puis vaccination et antibiotiques au XXe siècle), ses succès, ainsi que sa diffusion à travers le monde par le biais notamment de la colonisation par l'Occident vont inciter à poser, dès la fin du XIXe siècle, la médecine scientifique occidentale comme modèle de médecine faisant autorité, lequel s'est diffusé au niveau mondial à travers son industrialisation au XXe siècle.Certains chercheurs réhabilitent de même certains aspects de la médecine médiévale occidentale. Ainsi l'historien de la médecine Roger Dachez qui met en valeur l'aspect préventif et la vision globale qu'avait de la médecine le Moyen Âge.De même, toujours à la fin du XXe siècle, notamment sous l'effet de la mondialisation, les médecines traditionnelles ou non occidentales ont vu leur place reconnue au sein de la médecine mondiale : en 2002, l'organisation mondiale de la santé a ainsi mis en place sa première stratégie globale en matière de médecine traditionnelle.On identifie ainsi, à côté de la médecine occidentale, d'autres types de médecines, dites « alternatives » incluant : médecine chinoise, médecine tibétaine traditionnelle, médecine ayurvédique, médecine traditionnelle, et médecine non conventionnelle.En Occident, l'usage de médecines alternatives et complémentaires est constaté dans certaines conditions où les traitements de biomédecine semblent inefficaces, notamment dans le cas de maladies chroniques.Les étapes de l'acte médical sont formées de :l'étiologie qui désigne l'étude des causes de la maladie ;la pathogénie ou pathogenèse qui désigne l'étude du mécanisme causal ;la physiopathologie qui désigne l'étude des modifications des grandes fonctions au cours des maladies ;la sémiologie qui désigne l'étude de l'ensemble des signes apparents. Elle est apparentée à ce qui est nommée la clinique, opposée à la para-clinique qui sont les résultats des examens complémentaires. Face à la complexité croissante des techniques d'imagerie, il s'est développé une sémiologie des examens complémentaires ;le diagnostic qui désigne l'identification de la maladie ;le diagnostic différentiel qui désigne la description des maladies comportant des signes proches et qui peuvent être confondues ;la thérapeutique qui désigne le traitement de la maladie ;le pronostic qui désigne l'anticipation de l'évolution de celle-ci ;la psychologie qui désigne la partie de la philosophie qui traite de l’âme, de ses facultés et de ses opérations. La psychologie du patient est un élément important de la réussite du processus médical. Comme le dit dès 1963 l'historien de la médecine Jean Starobinski, « une médecine vraiment complète ne se borne pas à cet aspect technique ; s'il accomplit pleinement son métier, le médecin établit avec son patient une relation qui satisfera les besoins affectifs de ce dernier. L'acte médical comporte donc un double aspect : d'une part les problèmes du corps et de la maladie font l'objet d'une connaissance qui n'est pas différente de celle que nous prenons du reste de la nature - et l'organisme du patient est alors considéré comme une « chose » vivante capable de réagir conformément à des lois générales ; d'autre part, le rapport thérapeutique s'établit entre deux personnes, dans le contexte d'une histoire personnelle - et la médecine devient alors cette fois un art du dialogue, où le patient s'offre comme un interlocuteur et comme une conscience alarmée ». Georges Canguilhem écrivait lui que « l’acte médicochirurgical n’est pas qu’un acte scientifique, car l’homme malade n’est pas seulement un problème physiologique à résoudre, il est surtout une détresse à secourir ». Une décision médicale doit tenir compte à la fois des données de la science, mais également des préférences des patients et de l’expérience du praticienEn travaillant ensemble comme une équipe interdisciplinaire, de nombreux professionnels de la santé hautement qualifiés sont impliqués dans la prestation des soins de santé modernes. Voici quelques exemples : les infirmiers, les techniciens médicaux d'urgence et les ambulanciers, les scientifiques de laboratoire, pharmaciens, podologues, physiothérapeutes, inhalothérapeutes, psychologues, orthophonistes, ergothérapeutes, radiologues, des diététiciens, des bioingénieurs, des chirurgiens et des vétérinaires.Un patient admis à l'hôpital est habituellement sous les soins d'une équipe spécifique en fonction de leur problème de présentation principale, par exemple, l'équipe de cardiologie, qui peut ensuite interagir avec d'autres spécialités, par exemple, la chirurgie, la radiologie, pour aider à diagnostiquer ou traiter le problème principal ou des complications ultérieures. Les médecins ont de nombreuses spécialisations et sous-spécialisations dans certaines branches de la médecine, qui sont énumérés ci-dessous. Il existe des variations d'un pays à l'autre en ce qui concerne les spécialités et les sous-spécialités.Les principales branches de la médecine sont :les sciences fondamentales ;les spécialités médicales ;les domaines interdisciplinaires, comme les humanités médicales.L'anatomie : étude de la structure physique des organismes. Contrairement à l'anatomie macroscopique ou brute, la cytologie et l'histologie sont concernés par des structures microscopiques.La biochimie : étude de la chimie qui se déroule dans les organismes vivants, en particulier la structure et la fonction de leurs composants chimiques.La biologie moléculaire : étude des mécanismes moléculaires des processus de réplication, de transcription et de traduction du matériel génétique.La biomécanique : étude de la structure et des mouvements des systèmes biologiques au moyen de la mécanique.La biophysique : science interdisciplinaire qui utilise les méthodes de la physique et de la chimie physique pour étudier les systèmes biologiques.La biostatistique : application des statistiques à des champs biologiques dans le sens le plus large. Une connaissance de la biostatistique est essentiel dans la planification, l'évaluation et l'interprétation de la recherche médicale. Il est également fondamental de l'épidémiologie et de la médecine fondée sur des preuves (EBM).La cytologie : étude des cellules.L'embryologie : étude du développement précoce des organismes.L'épidémiologie : étude de la démographie des processus de la maladie, et inclut, mais sans s'y limiter, l'étude des épidémies.La génétique : étude des gènes, et leur rôle dans l'héritage biologique.L'histologie : étude des structures des tissus biologiques par microscopie optique, la microscopie électronique et l'immunohistochimie.L'immunologie : étude du système immunitaire, qui comprend le système immunitaire inné et adaptatif.La microbiologie : étude des micro-organismes, y compris les protozoaires, les bactéries, les champignons, les virus et les prions.La neuroscience : étude du système nerveux.La nutrition (mise au point théorique) et la diététique (orientation pratique) : étude de la relation entre la nourriture et des boissons à la santé et à la maladie, en particulier dans la détermination d'une alimentation optimale. thérapie nutritionnelle médicale se fait par des diététistes et est prescrit pour le diabète, les maladies cardiovasculaires, le poids et les troubles alimentaires, les allergies, la malnutrition et les maladies néoplasiques.La pathologie en tant que science : étude des maladies, de leurs causes, progressions et traitements.La pharmacologie : étude des médicaments et de leurs actions.La physiologie : étude du fonctionnement normal de l'organisme et les mécanismes de régulation sous-jacents. La physiologie peut être subdivisée (physiologie cardiaque, endocrinienne…).La physique médicale : étude des applications des principes de physique en médecine.La toxicologie : étude des effets nocifs des médicaments et des poisons. Par pratique l'anatomopathologie : étude microscopique des tissus malades ;l'anesthésie-réanimation : l'anesthésie qui est la médecine péri-opératoire, la réanimation qui est la prise en charge des malades présentant au moins deux défaillances d'organe ou une nécessitant une technique de suppléance ;la biologie médicale ;la chirurgie : thérapeutique médicale qui comporte une intervention mécanique au sein même des tissus ;l'éducation de la santé ;la médecine esthétique : type de soins visant à améliorer l'aspect plastique du patient ;la médecine générale (médecine de famille) ;la médecine du travail : médecine préventive consistant à éviter toute altération de la santé des travailleurs du fait de leur travail, notamment en surveillant les conditions d'hygiène du travail, les risques de contagion et l'état de santé des travailleurs ;la médecine d'urgence : médecine hospitalière (service des urgences) et extrahospitalière (Samu), traitement des urgences vitales ;la nutrition : prise en charge du métabolisme et de l'alimentation ;la pharmacie : dispensation des médicaments et prise en charge pharmaco-thérapeutique ;la radiologie, spécialité de l'imagerie médicale. Par type de patient L'andrologie : médecine de l'homme, prise en charge des maladies spécifiques du sexe masculin ;la gynécologie : spécialité médicochirurgicale, dont l'activité variée inclut notamment la médecine de la femme, le suivi gynéco-obstétrical et les cancers des organes génitaux féminins ainsi que des seins ;l'obstétrique : médecine de la femme enceinte. À noter la pratique médicale à part entière des sages-femmes, qui se consacrent à la surveillance de la grossesse normale ;la médecine fœtale : médecine du fœtus grâce à l'apparition de méthodes d'explorations de la vie intra-utérine (échographie, Doppler, amniocentèse) ;la médecine légale : recherche des causes de la mort sur un cadavre (nécropsie) et rédaction d'un rapport pour la Justice ;la pédiatrie : médecine des enfants, domaine très large et englobant généralement la génétique clinique ;la néonatologie : médecine et réanimation des nouveau-nés et des prématurés ;la gériatrie : médecine des personnes âgées ;la médecine des gens de mer : médecine des marins et travailleurs de la mer.la médecine vétérinaire : médecine des animaux. Par organe L'angiologie : médecine des vaisseaux ;la cardiologie : médecine des maladies du cœur et du système vasculaire ;la dermatologie : médecine des maladies de la peau ;l'endocrinologie : médecine des maladies des glandes, des anomalies hormonales, des troubles de la nutrition et des métabolismes ;l'hématologie : médecine des maladies du sang ;l'hépato-gastro-entérologie : aussi appelée gastroentérologie, médecine des maladies de l'appareil digestif dans son ensemble, incluant celles du tube digestif et celles du foie, du pancréas, ainsi que de la paroi abdominale. La gastroentérologie comprend également les activités d'endoscopie digestives, soit haute (endoscopie œsogastroduodénale), soit basse (iléocoloscopie) ;l'immunologie : médecine des maladies ou des troubles du système immunitaire ;la néphrologie : médecine des maladies des reins ;la neurologie : médecine des maladies du système nerveux ;l'odontologie : soins des dents ;l'ophtalmologie : médecine des maladies des yeux, de l'orbite et des paupières ;l'orthopédie : discipline chirurgicale traitant les affections de l'appareil locomoteur ;l'oto-rhino-laryngologie (ORL) : médecine des maladies des oreilles, du nez et de la gorge ;la pneumologie : médecine des maladies de la plèvre, des bronches et des poumons ;la proctologie : médecine des maladies du rectum et de l'anus ;la rhumatologie : discipline médicale traitant les affections de l'appareil locomoteur ;la stomatologie : médecine des maladies de la bouche ;l'urologie : médecine de l'appareil urinaire. Par affection L'addictologie : médecine des dépendances, regroupant l'alcoolisme, le tabagisme et la toxicomanie (branche de la psychiatrie selon certains) ;l'alcoologie : médecine des troubles liés à l'alcool ;l'allergologie : médecine des allergies ;la cancérologie ou oncologie : médecine des cancers (comprenant la chimiothérapie des tumeurs) associée avec la radiothérapie : traitement des tumeurs par radiations ionisantes ;la diabétologie : médecine des diabètes ;l'infectiologie : médecine des maladies infectieuses ;la psychiatrie : médecine des troubles comportementaux, psychiques et des maladies mentales ;la toxicologie : traitement des empoisonnements et intoxications ;la traumatologie : traitement des patients ayant subi de graves blessures, généralement accidentelles ;la vénérologie : médecine faisant l'étude des maladies transmises par l'acte sexuel. Types de chirurgie Chirurgie cardiaqueChirurgie digestiveChirurgie de la face et du cou (cervico-faciale)Chirurgie généraleChirurgie pédiatriqueChirurgie orthopédiqueChirurgie dentaireChirurgie plastique, reconstructrice et esthétiqueChirurgie thoraciqueChirurgie urologique (Urologie)Chirurgie vasculaireChirurgie viscéraleNeurochirurgieTechniques chirurgicales Divers Anatomie et cytologie pathologiques (voir anatomopathologie)Anesthésie-réanimationBiologie médicaleGénétiqueGynécologie obstétriqueInformatique Médicale et Technologies de l'InformationMédecine généraleMédecine interneMédecine hyperbareMédecine nucléaireMédecine nutritionnelle (voir nutrition)Pathologie (pays anglophones)PédopsychiatrieMédecine physique et de réadaptationSanté publiqueLes académies de médecineles Centers for Disease Control and Prevention, soit « centres de contrôle et de prévention des maladies »les hôpitauxles organismes de recherche médicaleles organismes publicsles Conseils de l'Ordre de médecinsl'Agence européenne des médicamentsUne profession de la santé est une profession dans laquelle une personne exerce ses compétences ou son jugement ou fournit un service lié au maintien ou l'amélioration de la santé des individus, ou au traitement ou soins des individus blessés, malades, souffrant d'un handicap ou d'une infirmité. Des exemples de profession peuvent notamment inclure : médecin, pharmacien, chirurgien-dentiste, sage-femme, masseur-kinésithérapeute, physiothérapeute, ergothérapeute, psychomotricien, infirmier, podologue, aide-soignant, ambulancier, et attaché de recherche clinique.Chaque profession possède son propre cursus de formation. En plus des études permettant d'exercer la profession de médecin dont l'organisation varie selon les pays, on trouve donc notamment les études en soins infirmiers, et les études de pharmacie.L'étudiant en médecine s'appelle carabin.Les apports de la médecine, particulièrement de la médecine occidentale depuis le XIXe siècle, se mesure notamment par l'allongement de la durée de la vie, l'espérance de vie en bonne santé, la réduction de la mortalité infantile, et l'éradication ou la capacité technique d'éradication de très anciennes épidémies (tuberculose, peste, lèpre, etc.). Ces progrès se poursuivent comme avec les succès de nouvelles thérapies (ou actes chirurgicaux) sur des pathologies considérées encore incurables il y a une quinzaine d'années (comme certains cancers et maladies auto-immunes).La médecine n'est pas une science exacte, et l'acte médical peut parfois affecter la personne humaine de manière négative, par exemple via :des « effets secondaires » ou indésirables de médicaments ou traitements, qui devront pour certains (Distilbène par exemple) être supportés par plusieurs générations. La recherche de ces effets se fait par pharmacovigilance ;l'antibiorésistance est due à la sélection de souches bactériennes résistantes à divers antibiotiques à cause d'un usage non raisonné de ces derniers ;les maladies nosocomiales peuvent apparaître en hôpital à cause de la concentration de malades. La forte pression exercée par les traitements ainsi que par les désinfectants et antiseptiques sur ce « pot pourri » de germes amène à long terme à l’émergence d'agents infectieux résistants qui pourront infecter facilement les malades déjà affaiblis ;les résultats de maladresses, d'erreurs médicales, de défauts d'organisation, de prises excessives de médicaments ou de traitements inadaptés. Un trouble ou une maladie est dite iatrogène lorsqu'elle est provoquée par un acte médical ou par les médicaments, même en l’absence d’erreur du médecin, du soignant, du pharmacien ou tout autre personne intervenant dans le soin. En France, 4 % des hospitalisations sont consécutives à des soins, et 40 % de ces cas seraient évitables. Ces problèmes comprennent une partie des maladies nosocomiales dont les plus fréquentes sont les infections nosocomiales.De nombreux progrès sont annoncés ou espérés dans les années à venir, en matière de santé-environnement, d'épidémiologie, d'allongement de la durée de vie, si ce n'est de la durée de vie en bonne santé. La médecine prédictive, le clonage, les cellules-souches posent des questions nouvelles en termes de bioéthique.Des défauts d'anticipation font que, par exemple en France, en 2025, alors que la population aura augmenté (et la population âgée plus encore), le nombre de médecins aura diminué de 10 % et la densité médicale de 15 %, à la suite du non-remplacement des médecins baby-boomers induit par les quotas d’accès aux études de médecine dans les années 1970 à 1990. La médecine libérale devrait perdre 17 % de ses effectifs, et le secteur salarié 8 %, sauf en milieu hospitalier où le ministère envisage une hausse de 4 % ; 13 % des généralistes auront disparu, contre 7 % pour les spécialistes (ophtalmologistes, oto-rhino-laryngologistes et psychiatres surtout). La faible « densité médicale » augmentera aussi le coût des soins, l’impact des déplacements en termes de pollution (et secondairement de santé) et pourrait diminuer l'efficience médicale (une moindre densité médicale augmente la mortalité), d'autant plus que les patients sont plus pauvres.Cet article est partiellement ou en totalité issu de l'article intitulé « Histoire de la médecine » (voir la liste des auteurs).(en) Charles Singer et E. Ashworth Underwood, A Short History of Medicine, New York et Oxford, Oxford University Press, 1962(en) Roberto Margotta, The Story of Medicine, New York, Golden Press, 1968.(en) Roberta Bivins, Alternative Medicine? : A History, Oxford University Press, 5 octobre 2007, 264 p. (ISBN 978-0-19-156881-7, lire en ligne)(en) Robert A. Schwartz, Gregory M Richards et Supriya Goyal, « Clubbing of the Nails », Medscape Reference,‎ 28 février 2012 (lire en ligne, consulté le 11 juin 2012).Stanis Perez, Histoire des médecins. Artisans et artistes de la santé de l'Antiquité à nos jours, Perrin, 2015, 470 pages.Ressource relative à la littérature : (en) The Encyclopedia of Science Fiction Ressource relative à la santé : (en) Medical Subject Headings Haute Autorité de Santé : recommandations, conférences de consensus, etc. (France)Code de la santé publique (France)Service Public Fédéral (SPF) Santé publique, Sécurité de la Chaîne alimentaire et Environnement (Belgique)CISMeF : annuaire de sites médicaux Internet francophoneBase de données de publications médicales(en) Medline, base de données de publications médicales Portail de la médecine"
médecine;""
médecine;"Un organisme (du grec organon, « instrument »), ou organisme vivant, est, en biologie et en écologie, un système vivant complexe, organisé et est le produit de variations successives au cours de l'évolution. Il est constitué d'une ou plusieurs cellules vivantes (on parle alors, respectivement, d'organisme unicellulaire ou pluricellulaire). Les organismes vivants sont classifiés en espèces partageant des caractéristiques génétiques, biologiques et morphologiques communes.Les organismes complexes, multicellulaires, sont constitués d'un ensemble de cellules vivantes différenciées, assurant des fonctions spécialisées et opérant de manière concertée. Ces cellules dérivent en général d'une progénitrice unique et partagent le même patrimoine génétique. Elles interagissent de façon à fonctionner comme un ensemble stable dynamiquement.Un organisme vivant se trouve en effet dans un état thermodynamique de non-équilibre, mais conservant un environnement interne approximativement constant, grâce à l’apport continu d'énergie et, le cas échéant, de nutriments. Ce phénomène d'équilibre dynamique maintenu par l'organisme est appelée homéostasie.Quelques centaines d'espèces, dites « organismes modèles », sont utilisées comme modèles d'étude par les scientifiques et les laboratoires de recherche pour comprendre les mécanismes fondamentaux du vivant.Un organisme est un être organisé, qui peut être un organisme unicellulaire ou un organisme multicellulaire. Le terme d'organisme complexe s'applique à tout organisme vivant ayant plus d'une cellule.Il est difficile de définir avec précision ce qu'est un être vivant. On peut donner quelques caractéristiques du vivant :la capacité de se maintenir en vie en puisant dans l'environnement l'énergie et les composants nécessaires. Cette capacité s'appuie sur le métabolisme qui inclut diverses fonctions, telles que la nutrition, la respiration, la photosynthèse... ;la capacité de se développer selon une certaine organisation (croissance, morphologie, division cellulaire, développement) ;la capacité de se reproduire et de donner naissance à d'autres organismes vivants (reproduction végétative ou sexuée) ;la nécessité d'un environnement favorable pour survivre (température, pression, oxygène, eau...).La matière vivante est fondée sur la chimie organique avec comme base le carbone.Tout organisme vivant est mortel, par définition.Selon la source d'énergie utilisée, on distingue les organismes chimiotrophes, tirant leur énergie de molécules et les phototrophes, tirant leur énergie de la lumière.Tous les organismes vivants sont composés d'un nombre plus ou moins grand de cellules. Un organisme se développe en général à partir d'une cellule unique, par divisions cellulaires successives. Au cours de ce développement, les cellules subissent des étapes de différenciation, ce qui leur permet d'acquérir des spécialisations associées à des fonctions particulières. Un ensemble de cellules spécialisées de même type qui s'associent forment un tissu et l'organisation structurée de différents tissus constitue un organe. À l'intérieur d'un organisme vivant complexe, on trouve ainsi différents types cellulaires et différents tissus, variables suivant le type d'organisme considéré (animaux, plantes...).Ce type d'organisation hiérarchique : cellule, tissu, organe peut s'étendre aux grandes fonctions de l'organisme, on parle alors de système ou d'appareil, qui sont une collection d'organes participant à la même grande fonction : système nerveux, système respiratoire, appareil reproducteur, système racinaire (chez les plantes)...L'ensemble de l'organisme suit en général un plan d'organisation commun à tous les individus d'une même espèce. Ce plan d'organisation détermine la disposition relative des organes et des tissus, l'existence et le positionnement de membres ou d'appendices. Il est déterminé génétiquement et partagé en général par des espèces voisines sur le plan évolutif. Certains organismes vivants ont au cours de leur cycle de vie des stades d'existence très différenciés (stade larvaire, stade adulte...) avec des morphologies et des plans d'organisation qui peuvent varier, au travers d'étapes de métamorphose.Georges Chapouthier a proposé d'interpréter la complexité des organismes par l'application répétée de deux principes généraux, compatibles avec la sélection darwinienne : le principe de « juxtaposition » d'unités identiques, puis le principe d'« intégration » de ces unités dans des ensembles plus complexes, dont elles constituent alors des parties.EspèceOrganisme modèleOrganicismeRessource relative à la santé : (en) NCI Thesaurus  Portail de la biologie   Portail de l’écologie"
médecine;"En médecine, un patient est une personne physique recevant une attention médicale ou à qui est prodigué un soin.Le mot patient est dérivé du mot latin patiens, participe présent du verbe déponent pati, signifiant « celui qui endure » ou « celui qui souffre ».Il existe plusieurs dénominations communes au terme patient, dont personne soignée, bénéficiaire de soins , ""usager"" ou encore client employé notamment dans la culture anglophone[réf. souhaitée]. Dans la recherche médicale, le patient est parfois appelé sujet. On commence même à utiliser le terme d’actient (patient qui agit) du fait de l'évolution des patients à se renseigner par eux-mêmes et à poser de plus en plus de questions au praticien.[réf. souhaitée].En médecine, le patient bénéficie d'examens médicaux, de traitements prodigués par un médecin ou un professionnel de la santé pour faire face à une maladie ou à des blessures. Le patient peut également bénéficier d'actes de prévention.Knock ou le Triomphe de la médecine, pièce de théâtre de Jules Romains, ayant fait l'objet de plusieurs adaptations cinématographiques, qui illustre de manière humoristique les relations entre médecins et patients.Luc Perino, Patients zéro. Histoires inversées de la médecine, La Découverte, 2020Jean-Philippe Pierron, « Une nouvelle figure du patient ? Les transformations contemporaines de la relation de soins », Sciences sociales et santé, vol. 25, no 2,‎ 2007, p. 43-66 (DOI 10.3406/sosan.2007.1858)Relation médecin-patientCharte du patient hospitaliséÉducation thérapeutique du patientDossier médical du patientPatient zéro Portail de la médecine"
médecine;"La santé est « un état de complet bien-être physique, mental et social, et ne consiste pas seulement en une absence de maladie ou d'infirmité ». Dans cette définition par l'Organisation mondiale de la santé, OMS, depuis 1946, la santé représente « l’un des droits fondamentaux de tout être humain, quelles que soient sa race, sa religion, ses opinions politiques, sa condition économique ou sociale »,. Elle implique la satisfaction de tous les besoins fondamentaux de la personne, qu'ils soient affectifs, sanitaires, nutritionnels, sociaux ou culturels.. Mais cette définition confond les notions de santé et de bien-être.Par ailleurs, « la santé résulte d’une interaction constante entre l’individu et son milieu » et représente donc « cette « capacité physique, psychique et sociale des personnes d’agir dans leur milieu et d’accomplir les rôles qu’elles entendent assumer d’une manière acceptable pour elles-mêmes et pour les groupes dont elles font partie ». René Dubos présente en 1973 la santé comme « la situation dans laquelle l'organisme réagit par une adaptation tout en préservant son intégrité individuelle. C'est l'état physique et mental relativement exempt de gênes et de souffrances qui permet à l'individu de fonctionner aussi longtemps que possible dans le milieu où le hasard ou le choix l'ont placé. »,Pour René Leriche en 1936, « la santé c'est la vie dans le silence des organes. »,Dans les sociétés traditionnelles (« primitives »), la santé relève généralement autant de l'individu que du groupe. Elle est imbriquée avec les croyances animistes et religieuses, et le rôle des guérisseurs (chamans, sorciers, etc.) qui utilisent à la fois la pharmacopée locale, le toucher et des pratiques relevant de la magie, de la divination, ou de la psychologie.À partir du XVIIIe siècle, la maladie cesse progressivement d'être considérée comme une fatalité et le corps redevient un sujet de préoccupation. Ce mouvement concerne d'abord les élites, puis s'étend progressivement à l'ensemble de la société. La santé devient alors un droit que les États se doivent de garantir.L'état de santé se recherche à la fois pour chaque individu, avec la médecine clinique, ou pour une population, avec la santé publique.La santé d'une population est classiquement évaluée d'abord par les taux de mortalité et de morbidité, avec l’espérance de vie. La santé est une notion relative, « parfois non présentée comme corollaire de l'absence de maladie : des personnes porteuses d'affections diverses sont parfois jugées « en bonne santé » si leur maladie est contrôlée par un traitement. A contrario, certaines maladies peuvent être longtemps asymptomatiques, ce qui fait que des personnes qui se sentent en bonne santé peuvent ne pas l'être réellement. »« État de santé ressentie » : c'est l'un des indicateurs d'état de santé. Il est publié tous les deux ans depuis 2002, pour les pays de l'OCDE. Après une tendance à la hausse de 2002 à 2008, il a chuté de plusieurs points en 2010 « Quelles que soient les tranches d’âge, le pourcentage des femmes et des hommes s’estimant en bonne ou très bonne santé baisse en 2010. Et lorsque l’on considère l’ensemble des sexes, il en est de même pour le quintile de revenu le plus élevé ». En 2008, 74,9 % des hommes se jugeaient en bonne ou très bonne santé, contre 70,6 % en 2010. Pour les femmes ce taux est passé de 70,1 % à 66,5 %.La santé mentale peut être considérée comme un facteur très important de la santé physique pour les effets qu'elle produit sur les fonctions corporelles. Ce type de santé concerne le bien-être émotionnel et cognitif ou une absence de trouble mental. L'Organisation mondiale de la santé (OMS) définit la santé mentale en tant qu'« état de bien être dans lequel l'individu réalise ses propres capacités, peut faire face aux tensions ordinaires de la vie, et est capable de contribuer à sa communauté ». Il n'existe aucune définition officielle de la santé mentale. Il existe différents types de problèmes sur la santé mentale, dont certains sont communément partageables, comme la dépression et les troubles de l'anxiété, et d'autres non communs, comme la schizophrénie ou le trouble bipolaire.Pour l'Organisation mondiale de la santé (OMS), la santé reproductive est une composante du droit à la santé.Cette notion récente évoque la bonne transmission du patrimoine génétique d'une génération à l'autre. Elle passe par la qualité du génome, des spermatozoïdes et des ovules, mais aussi par une maternité sans risque, l'absence de violences sexuelles et sexistes, l'absence de maladies sexuellement transmissibles (MST), la planification familiale, l'éducation sexuelle, l'accès aux soins, la diminution de l'exposition aux perturbateurs endocriniens, etc.Un certain nombre de polluants (dioxines, pesticides, radiations, leurres hormonaux, etc.) sont suspectés d'être, éventuellement à faibles ou très faibles doses, responsables d'une délétion de la spermatogenèse ou d'altération des ovaires ou des processus de fécondation puis de développement de l'embryon. Certains sont également cancérigènes ou mutagènes (ils contribuent à l'augmentation du risque de malformation et d'avortement spontané).Les soins de santé reproductive recouvrent un ensemble de services, définis dans le Programme d’action de la Conférence internationale sur la population et le développement (CIPD) tenue au Caire (Égypte) en septembre 1994 : conseils, information, éducation, communication et services de planification familiale ; consultations pré et postnatales, accouchements en toute sécurité et soins prodigués à la mère et à l’enfant; prévention et traitement approprié de la stérilité ; prévention de l’avortement et prise en charge de ses conséquences ; traitement des infections génitales, maladies sexuellement transmissibles y compris le VIH/SIDA ; le cancer du sein et les cancers génitaux, ainsi que tout autre trouble de santé reproductive ; et dissuasion active de pratiques dangereuses telles que les mutilations sexuelles féminines.La santé au travail fait partie des principaux thèmes de santé identifiés par l'OMS.Un déterminant de santé est un facteur qui influence l’état de santé d'une population soit isolément, soit en association avec d’autres facteurs.L'hygiène est l'ensemble des comportements concourant à maintenir les individus en bonne santé. Ils demandent de pouvoir notamment faire la part entre les « bons microbes » et ceux qui sont pathogènes ou peuvent le devenir dans certaines circonstances. Ces circonstances l'hygiène cherche à les rendre moins probables, moins fréquentes ou supprimées. Après une phase hygiéniste, dont l'efficacité de court terme est indiscutable, sont apparus une augmentation des allergies, des maladies auto-immunes, des antibiorésistances et des maladies nosocomiales jugées préoccupantes. La recherche de juste équilibre entre exposition au risque et solution médicale usuelle est rendue difficile dans un contexte d'exposition accrue à des cocktails de polluants complexes (pesticides en particulier) et perturbateurs hormonaux, de modifications sociétales et climatiques planétaires (cf. maladies émergentes, risque pandémique, zoonoses, risque de bioterrorisme, etc.).La lutte contre les infections nosocomiales à l'hôpital, ou contre les toxi-infections alimentaires par exemple, est née après la découverte de l'asepsie sous l'influence par exemple de Ignace Semmelweis ou Louis Pasteur. Les comportements individuels et collectifs sont de toute première importance dans la lutte contre les épidémies ou les pandémies.Cette discipline de l'hygiène vise donc à maîtriser les facteurs environnementaux pouvant contribuer à une altération de la santé, comme la pollution par exemple, avec des problèmes paradoxaux à gérer : par exemple, l'amélioration des conditions d'hygiène semble avoir paradoxalement pu favoriser la réapparition de maladies comme la poliomyélite et diverses maladies auto-immunes et allergies.De nombreux facteurs de risque sont intrinsèquement liés au mode de vie. Les soins corporels, l'activité physique, l'alimentation, le travail, les problèmes de toxicomanie, notamment, ont un impact global sur la santé des individus.Nutrition : Aliments - Oligo-élément - AlicamentProduits d'hygiène : Crème solaire - Dentifrice - Préservatif - SavonToxicomanies & dépendances : Alcool - Cannabis - Cocaïne - Tabac - Jeu pathologiqueDe nombreux risques et dangers sont liés au domaine de la santé, l'évolution humaine et également les changements de son mode de vie ne sont pas sans conséquences. L'alimentation et les nouvelles technologies sont également des facteurs de risques en France et dans le reste du monde. Les rythmes, les cadences de travail ; les gestes inadaptés sont des facteurs très importants sur la santé. Ils entraînent des troubles psychosomatiques et parfois des handicaps pour la vie.Quatre facteurs permettraient d'allonger considérablement la durée de la vie : absence de tabac, consommation d'alcool égale ou inférieure à un demi verre par jour, consommation de 5 fruits et légumes par jour, exercice physique d'une demi-heure par jour. Le tout donnerait une majoration de l'espérance de vie de 14 ans par rapport au non-respect de ces facteurs.Du strict point de vue de l'alimentation, de nombreuses études concordantes concluent qu'une alimentation exclusivement végétarienne permet de limiter les risques de cancer et de maladies cardio-vasculaires, et donc d'avoir une espérance de vie en bonne santé plus longue,. Les études mettent à la fois en évidence les bénéfices d'une alimentation riche en légumes et fruits et les risques relatifs liés à la consommation de viande, poisson et produits laitiers,,,. Les compléments alimentaires synthétiques ne seraient absolument pas nécessaires,.D'autres pistes sont explorées pour allonger la durée de vie en bonne santé : le jeûne, le jeûne intermittent et la restriction calorique.Par ailleurs, l'« hygiénisme moral » trans-national débuté au XIXe siècle (à ne pas confondre avec la médecine alternative créée par Herbert Shelton) est une doctrine contre le « relâchement des mœurs », ce qui serait le meilleur moyen de garantir la santé. C'est ce courant qui a par exemple déclaré la lutte contre la syphilis ou l'alcoolisme comme priorité nationale. C'est également lui qui déclare que si les obèses sont gros, c'est qu'ils sont gourmands et paresseux, ou encore que les fumeurs n'ont pas de volonté; Il semble persister dans certaines politiques et campagnes d'information et d'éducation des citoyens à l'hygiène.C'est un domaine, parfois nommé « santé environnementale », qui se développe depuis la fin du XXe siècle, à la suite de la prise de conscience du fait que l'environnement, notamment lorsqu'il est pollué, est un déterminant majeur de la santé.La pollution aiguë ou chronique, qu'elle soit biologique, chimique, due aux radiations ionisantes, ou due aux sons ou la lumière (ces facteurs pouvant additionner ou multiplier leurs effets) est une source importante de maladies.Dans l'Union européenne, la Commission a adopté (11 juin 2003) une « stratégie communautaire en matière de santé et d'environnement », traduite le 9 juin 2004, en un « Plan d'action » (2004-2010), qui vise notamment les maladies dites « environnementales ». Cela concerne l'asthme et les allergies respiratoires, en cherchant plus généralement à « mieux prévenir les altérations de la santé dues aux risques environnementaux » (dont l'exposition aux pesticides et à leurs résidus). Des systèmes de veille sanitaire permanente doivent identifier les menaces émergentes (dont nanotechnologies, OGM, maladies émergentes, impacts des modifications climatiques, etc.) et en évaluer l'impact sanitaire selon des actions réalisées au niveau communautaire mais aussi national. Un « plan d'action environnement et santé » va être développé afin de mettre en œuvre cette stratégie ; de plus un processus de consultation a été lancé. Le plan d'action vise à faire le point sur les connaissances scientifiques existantes et à évaluer la cohérence et les progrès réalisés dans l'installation du cadre législatif communautaire en matière de santé et d'environnement. Un nouveau système d'information sur la santé est prévu « qui fonctionnera également dans le domaine de l'environnement » et veut devenir « la plus importante source de données fiables pour l'évaluation de l'impact des facteurs environnementaux sur la santé ». Ces aspects seront coordonnés avec les systèmes de réaction rapide et une approche intégrée « visant à juguler les déterminants environnementaux de la santé ».En ce qui concerne plus spécifiquement la France, un premier Plan national santé-environnement a été lancé en 2004 et un second en 2009, à la suite du Grenelle de l'environnement. Le bilan des actions menées devrait être fait en 2013.La santé publique désigne à la fois l'état sanitaire d'une population apprécié via des indicateurs de santé (quantitatifs et qualitatifs, dont l'accès aux soins) et l'ensemble des moyens collectifs susceptibles de soigner, promouvoir la santé et d'améliorer les conditions de vie.La notion de santé publique regroupe plusieurs champs :la santé au travail incluant la médecine du travail et parfois des démarches épidémiologiques ;la gestion des campagnes de prévention, qui doivent influencer les autres secteurs de la société pour y promouvoir la santé (économie, écoles, trafic, habitation, environnement, style de vie, etc.), la vaccination... ;l'organisation des réseaux de soins : premiers secours, hôpitaux, médecine libérale, médecine d'urgence... ;la formation initiale et continue des professions médicales et paramédicales ;la sécurité sociale et l'assurance maladie (Sécurité sociale en France) ;la recherche médicale et pharmacologique.Les règles en matière de santé font l'objet de textes internationaux édictés par l'OMS ou la FAO (Codex alimentarius pour l'alimentation).L'Union européenne a produit de nombreuses directives, règlements ou décisions pour protéger la santé des consommateurs ou d'animaux consommés.La promotion de la santé telle que définie par l'OMS est le « processus qui confère aux populations les moyens d'assurer un plus grand contrôle sur leur propre santé, et d'améliorer celle-ci ». Cette démarche relève d'un concept définissant la « santé » comme la mesure dans laquelle un groupe ou un individu peut d'une part réaliser ses ambitions et satisfaire ses besoins, et d'autre part évoluer avec le milieu ou s'adapter à celui-ci.La santé est prise en compte par le droit, y compris du point de vue des Conditions de travail.Les crises sanitaires sont des pandémies importantes, qui touchent entre une dizaine de personnes (cas des crises très médiatisées qui touchent les pays développés, comme certaines crises alimentaires) et des millions de personnes. Elles peuvent avoir des coûts économiques, sociaux et politiques considérables.L'OMS a d'ailleurs été créée pour qu'une pandémie telle que celle produite par la grippe espagnole ne se reproduise pas avec les mêmes effets (30 à 100 millions de morts selon les sources).Les sommes en jeu dans le domaine de la santé sont considérables, tant pour les coûts induits par les maladies, les pollutions et l'absentéisme, que par le marché des soins et des médicaments (en 2002, le marché mondial du médicament a été évalué à 430,3 milliards de dollars, contre 220 milliards en 1992). Le marché pharmaceutique a augmenté de 203 milliards d'euros. Et la consommation médicale progresse plus rapidement que le PIB dans les pays développés.Des crises sanitaires telles qu'une pandémie peuvent avoir des coûts économiques, sociaux et politiques considérables.La santé comme concept peut être un objet d’étude anthropologique. Tel que rapporté par Roy, elle est souvent conceptualisée comme une construction sociale par les anthropologues puisque le rapport que les sociétés ont avec elle est très variable d’une à l’autre, et selon les époques également. Le travail anthropologique cherchera donc à mieux comprendre l’expérience que font les groupes sociaux et culturels de la santé. Cet objet d’étude, pour faire preuve de rigueur méthodologique, doit être replacé dans son contexte global, notamment à travers les changements sociaux. On cherche alors à comprendre les phénomènes de relation santé/maladie, bien que de plus en plus le schéma santé/vie prend place. Pour dire autrement, selon Massé, l’anthropologie médicale s’intéresse à comment les acteurs sociaux définissent la bonne ou la mauvaise santé, et comment les maladies sont soignées dans ce contexte.Quelques approches théoriques sont nées en anthropologie médicale, rapportées par Roy. Parmi elles, celle de la théorie médico-écologique, celle de la phénoménologie et celle de la critique de la médecine et de la santé internationale.La théorie médico-écologique est formulée par Alexander Alland au début des années 1970, mais est reprise par d’autres quelques années après. Elle part du principe que les groupes humains adaptent leur culture à l’environnement. Cette théorie propose l'idée que l’adaptation culturelle est intimement liée à l’adaptation biologique en fonction de l'environnement et du milieu dans lequel le groupe se trouve. Ainsi, la santé est liée à ces transformations externes.L’approche phénoménologique se développe en parallèle à cette dernière. Des auteurs comme Kleinman et Good en sont un point d’origine, en cherchant à redonner une subjectivité à l’expérience humaine de la santé, s’éloignant de l’objectivité préconisée par la médecine. Pour ce faire, des perspectives expérientielles et sémantiques sont mobilisées.L’approche critique de la médecine et de la santé internationale se développe dans les années 1960. Elle a pour objet les conditions notamment politiques et économiques, donc globales, dans lesquelles sont vécues la santé et la maladie : les inégalités sociales façonnent l’accès à l’information, aux ressources de maintien de la santé et aux traitements. Un texte clé pour comprendre ce mouvement est notamment celui de Baer, Singer et Johnsen.De nombreux médias et émissions sont spécialisés dans les thèmes de la santé. En voici une sélection :Le Magazine de la santé, sur France 536.9, sur la Radio télévision suisseQuoi de neuf doc ?, sur TV5 MondeRadio Public SantéRadio France internationale, émission Priorité santéRadio Canada première chaîne, émission RDI SantéPlace à la santéSanté MagazineAlternative santéEnvironnement, Risques et SantéHealth On the Net Foundation est fondation qui indique aux internautes dans quels sites internet, ils peuvent obtenir des informations justes et sérieuses dans le domaine de la santé.PubMedSantepratiquePortail Santé-UEFasosante.netCarenity (réseau social santé sur internet destiné aux malades et à leurs proches).André Rauch, Histoire de la santé, PUF, Que-sais-je ?, 1995Georges Canguilhem, La santé, concept vulgaire et question philosophique, Sables, Pin-Balma, 1990Ressources relatives à la santé : (en) Medical Subject Headings (en) NCI Thesaurus Ressource relative à la recherche : Horizon 2020 Liste des thèmes de santé, site de l'OMS(en) Global Health, site Our World in Data Portail des soins infirmiers   Portail de la médecine   Portail de la société"
médecine;"En médecine, un symptôme (du grec συμπίπτω, « rencontrer ») ou signe fonctionnel est un signe qui représente une manifestation d'une maladie, tel qu'il est observé chez un patient. En général, pour une pathologie donnée, les symptômes sont multiples, et parfois il peut ne pas y avoir de symptôme (la maladie ou le malade est dit dans ce cas asymptomatique) ou peu de symptômes (maladie ou malade paucisymptomatique). Inversement, un même symptôme peut très souvent être attribué à différentes maladies : on ne peut donc en général pas conclure automatiquement qu'un symptôme (par exemple, le mal de gorge) est dû à une maladie donnée (par exemple, la grippe) ; ce serait commettre le sophisme de l'affirmation du conséquent.Le mot σύμπτωμα, en grec, signifie « accident », « coïncidence » ; il est constitué du préfixe σύν, « avec » et de πίπτω, « arriver », « survenir ». Le symptôme est donc, à l'origine, « ce qui survient ensemble », ce qui « concourt » ou « co-incide », au sens littéral du terme.Les symptômes sont les signes cliniques dont le malade se plaint (comme la douleur, la toux, le vertige, la tristesse). Les symptômes sont les éléments d'alerte d'un processus pathologique en cours, motivant ainsi le recours à une consultation médicale permettant d'objectiver la plainte en retrouvant des signes, qui, rassemblés en syndrome, puis en maladie en établissant un diagnostic, permettront de guider l'attitude thérapeutique.Les symptômes sont donc à différencier :des autres signes cliniques :les signes physiques, découverts en examinant le malade : contracture abdominale, souffle cardiaque,certains signes généraux : fièvre, hypotension artérielle ;des signes paracliniques obtenus à l'aide d'examens complémentaires :les signes radiologiques à la suite de radiographies,les signes biologiques à la suite de prélèvements.Par exemple, dans l'arthrose de hanche, le patient peut se plaindre de douleur à la marche (symptôme), et le praticien pourra objectiver à l'examen une limitation de mobilité de la hanche (signe physique), et sur une radiographie du bassin (signe radiologique).En créant la psychanalyse, Sigmund Freud va donner un sens au symptôme. À la suite des Études sur l'hystérie (1895), il n'a plus cesse de l'interroger dans les manuscrits à une époque « où la psychiatrie le réduisait à un phénomène hétérogène et opaque de la vie psychique ».Le symptôme peut être une manifestation somatique : une paralysie, des troubles du langage.Il peut être aussi une manifestation psychique : angoisse, hydrophobie.En étudiant le cas d'Anna O. (Bertha Pappenheim), une hystérique soignée par Josef Breuer grâce à la méthode cathartique, Freud a d'abord vu dans le symptôme un résidu mnésique d'expériences émotives (c'est-à-dire de traumatismes psychiques).Ensuite, en formulant sa nouvelle compréhension du système psychique, il a interprété différemment le symptôme.L'appareil psychique est composé de différentes instances en conflit : le moi, le ça et le surmoi.Quand une représentation (pulsionnelle) tombe sous le coup d'un interdit, elle est refoulée dans l'Inconscient par la censure opérée par le moi, mais jamais anéantie. Un processus alors de tentative de réapparition des éléments refoulés se met en place : c'est le retour du refoulé. Il y a plusieurs façons de déjouer la censure : le rêve, les lapsus, les oublis et les actes manqués ou bien les symptômes. Ces formations substitutives sont des formes de déguisement de la représentation, rendus acceptables pour la conscience pour pouvoir réinvestir son champ. Ainsi, ils permettent la satisfaction du désir sans éveiller la censure en formant un compromis entre les désirs et les interdits. Ce sont tous ces déguisements qui sont investigués, interprétés dans la cure psychanalytique.Remarque : il y a des liaisons associatives entre le symptôme et ce à quoi il se substitue.Le symptôme est le substitut de représentations tombées sous le coup d'un interdit et refoulées dans l'Inconscient. Il est le déguisement de ces représentations pour qu'elles puissent réinvestir le champ de la conscience, en étant acceptable. Et, il apporte une satisfaction de remplacement au désir inconscient, sans éveiller la censure et même en satisfaisant les exigences défensives. Cette double-satisfaction explique la capacité de résistance du symptôme car il est maintenu des deux côtés.Récapitulatif :il est formation de compromis en tant qu'il est le produit du conflit défensif ;il est formation substitutive dans la mesure où c'est le désir qui cherche à se satisfaire ;il est formation réactionnelle dans la mesure où c'est le processus défensif qui prévaut.Le symptôme est satisfaction, décharge pulsionnelle, il offre un bénéfice primaire. On ne saurait chercher à retirer au malade mental son symptôme, en ce qu'il en jouit, et que le psychologue doit reconnaitre comme jouissance.Ce bénéfice primaire correspond à la signification que porte le symptôme, signification qui seule permet l'expression d'un désir inconscient - le symptôme se rattache donc à la représentation, voire au discours. Pour Jacques Lacan, le symptôme est donc métaphore (Le symptôme est une métaphore que l'on veuille ou non se le dire).Le symptôme peut également engendrer un bénéfice secondaire, plaisir supplémentaire qui ne se relie donc pas directement au sens que veut énoncer ce signe de la maladie, mais qui provient plus d'un hasard relatif cette fois à la nature même du symptôme. Ainsi, le procédurier paranoïaque ralliant à lui un mouvement de soutien.Du point de vue psychosociologique, le symptôme est la façon particulière dont un individu trouve sa place dans le monde et règle son rapport à celui-ci, en fonction des contraintes et des stimulations psychosociales qui lui parviennent. Le symptôme est un prolongement de la personnalité, qui permet à cette dernière d'appréhender le monde mais aussi de s'en distancier, par un ensemble de protections constitutives dudit symptôme.Ainsi le symptôme est-il, du point de vue du sujet :stratégie d'individualisation ;matériau de la personnalité ;interprétation continue du monde ;modalité comportementale dynamique ;dispositif protecteur du Moi (ou ego) ;routine pathologique sitôt qu'il étouffe la créativité du sujet ou porte atteinte à l'intégrité d'autrui.Cet article est partiellement ou en totalité issu de l'article intitulé « Symptôme fonctionnel » (voir la liste des auteurs). Médecine et biologie  Psychanalyse Augustin Jeanneau, « symptôme », dans Alain de Mijolla (dir.), Dictionnaire international de la psychanalyse 2. M/Z, Paris, Hachette, 2005 (ISBN 2-0127-9145-X), p. 1769-1770.Augustin Jeanneau et Roger Perron, « symptôme (formation de -) », dans Alain de Mijolla (dir.), Dictionnaire international de la psychanalyse 2. M/Z, Paris, Hachette, 2005 (ISBN 2-0127-9145-X), p. 1770-1772.Yves Morhain, « Permanence du corps et variations du symptôme hystérique et/ou psychosomatique », Psychothérapies, 2011/2 (Vol. 31), p. 131-141. DOI : 10.3917/psys.112.0131. [[ lire en ligne]]Valentin Nusinovici, « symptôme,sinthome », dans Alain de Mijolla (dir.), Dictionnaire international de la psychanalyse 2. M/Z, Paris, Hachette, 2005 (ISBN 2-0127-9145-X), p. 1772-1774.Marcel Scheidhauer, « Le symptôme, le symbole et l'identification dans l'hystérie dans les premières théories de Freud », in: Enfance, tome 40, n°1-2, thématique : « Identités, Processus d'identification. Nominations », 1987, p. 151-162, sur le site de Persée, consulté le 30 mars 2021 [lire en ligne].Liste des symptômes en médecine humaineSémiologie médicaleTableau cliniqueSigne physiquePathomimieSymptôme (pathologie végétale)Inhibition, symptôme et angoisseSinthome Portail de la médecine   Portail de l’agriculture et l’agronomie   Portail de la psychologie"
médecine;""
médecine;La thyroxine ou T4 est une hormone thyroïdienne agissant comme une prohormone devant être désiodée en triiodothyronine, ou T3, par la thyroxine 5'-désiodase pour être pleinement active. Elle est biosynthétisée chez les mammifères dans la thyroïde par iodation de la thyroglobuline sous l'action de l'iode introduit dans les cellules par la pendrine et oxydé en iode atomique par la thyroperoxydase, une enzyme dont l'expression est accrue par la thyréostimuline (TSH). Elle est inactivée par la thyroxine 5-désiodase, qui la convertit en 3,3',5'-triiodothyronine ou « T3 inverse », isomère inactif de la T3.Les hormones thyroïdiennes jouent un rôle important dans le métabolisme énergétique et agissent en relation avec d'autres hormones, telles que l'insuline, le glucagon, l'adrénaline ou encore l'hormone de croissance.La L-thyroxine (énantiomère S-(–), ou lévothyroxine) est synthétisée en laboratoire comme médicament contre l'hypothyroïdie ou comme traitement à vie en cas de thyroïdectomie. Elle est prise sous forme de comprimés à raison de 12,5 μg à 200 μg par jour, habituellement une demi-heure avant le petit déjeuner afin d'en maximiser l'absorption dans la mesure où elle est mal absorbée par l'intestin. Elle peut également être administrée par intraveineuse dans les cas d'hypothyroïdie sévère.Liste d'hormones Portail de la chimie   Portail de la biochimie   Portail de la médecine
médecine;La thyroïdite de Hashimoto ou thyroïdite chronique lymphocytaire est une thyroïdite chronique auto-immune particulièrement fréquente caractérisée notamment par la présence d'anticorps anti-thyroperoxydase et par une infiltration lymphoïde de la glande thyroïde. Généralement évoqué à l'examen clinique devant un goitre et une hypothyroïdie, le diagnostic de la maladie nécessite la réalisation d'examens complémentaires biologiques et morphologiques. Le traitement de la maladie fait généralement appel à une hormonothérapie substitutive.C’est en examinant des pièces de thyroïdectomie obtenues chez quatre femmes d’âge moyen dans un contexte de goitre compressif que le médecin japonais Hakaru Hashimoto (1881−1934) découvre la maladie en 1912. Il publie sa découverte avec l’article Kōjōsen rinpa-setsu shushō-teki henka ni kansuru kenkyū hōkoku ou Zur Kenntnis der lymphomatösen Veränderung der Schilddrüse (Struma lymphomatosa) dans « Archiv für klinische Chirurgie », y décrivant alors l’infiltration lymphocytaire de la glande thyroïde.En 1957, la thyroïdite de Hashimoto devient la première maladie auto-immune spécifique d’organe à être reconnue. Initialement sous-diagnostiquée et considérée comme une maladie rare,, la thyroïdite de Hashimoto est aujourd'hui reconnue comme une des pathologies thyroïdiennes les plus fréquentes.Les estimations de l’incidence et de la prévalence de la thyroïdite de Hashimoto sont variables. Une incidence de 1/1000 a été proposée ainsi qu’une prévalence de 8/1000. La maladie est ainsi la première cause d’hypothyroïdie dans les pays où les apports en iode sont satisfaisants,.Il existe une nette prédominance féminine de la maladie de Hashimoto avec un rapport estimé entre 8 et 20 femmes pour 1 homme. Cette thyroïdite survient généralement aux alentours de l’âge de 40 ans mais peut se voir à tout âge y compris en population pédiatrique. Elle serait également plus fréquente chez les populations caucasiennes et asiatiques.Une histoire familiale de maladies de la thyroïde est fréquente (20 % des cas), en faveur d'une prédisposition génétique. Celle-ci est le plus souvent associée à des haplotypes particuliers tels que HLA-DR3, HLA-DR4 et HLA-DR5. Par ailleurs, la thyroïdite de Hashimoto pourrait être liée, avec un niveau de risque plus faible, avec le polymorphisme de gènes impliqués dans la réponse immunitaire comme le gène CTLA-4 (Cytotoxic T-lymphocyte Associated-4) (en) qui entraîne une diminution du fonctionnement des produits du gène et une régulation négative de l'activité des lymphocytes T,. Ce mécanisme est également retrouvé dans le diabète de type 1.La carence iodée serait un facteur de protection contre le risque de thyroïdite de Hashimoto, et une correction excessive en iode ,, une carence en sélénium, des maladies infectieuses et quelques médicaments ont été impliqués comme facteurs de risque chez les personnes avec un risque génétique déjà prédéterminé.L'incidence est augmentée chez les patients avec des anomalies chromosomiques, comme dans le syndrome de Turner,, le syndrome de Down (ou trisomie 21) et le syndrome de Klinefelter.Des recherches récentes suggèrent un potentiel rôle du virus HHV-6 (possiblement variant A) dans le développement ou la stimulation de la thyroïdite de Hashimoto.Le rôle du tabac est discuté. Des études indiquent un risque plus élevé chez les fumeurs, d'autres tendent à montrer un effet protecteur du tabac avec une réduction des taux sériques en auto-anticorps ainsi qu'une évolution moins fréquente vers l'hypothyroïdie. Le mécanisme physiopathologique de cet effet protecteur n'est cependant pas compris.La thyroïdite de Hashimoto est associée à la survenue d'autres maladies auto-immunes : diabète de type 1,, maladie cœliaque,, vitiligo, maladie de Biermer, insuffisance surrénale (notamment dans le cadre d'un syndrome de Schmidt) et polyarthrite rhumatoïde.La maladie de Basedow peut être associée à la thyroïdite de Hashimoto et il existe des formes de passage entre ces deux maladies. On parle ainsi de « Hashitoxicose », entité décrite pour la première fois en 1971.Sur le plan physiopathologique, les anticorps dirigés contre la thyroperoxydase et/ou la thyroglobuline causent une destruction progressive des follicules thyroïdiens de la glande thyroïde.Macroscopiquement, le goitre est symétrique, non adhérent aux éléments péri-thyroïdiens et présente une surface capsulaire discrètement bosselée.En microscopie les lésions consistent en une association de fibrose interstitielle, d'infiltration lymphoïde et de destruction épithéliale,. Le degré de fibrose est très variable. L'infiltration lymphoïde présente une organisation en follicules avec des lymphocytes B au centre et des lymphocytes T dans le cortex. Les cellules épithéliales thyroïdiennes sont également modifiées, apparaissant élargies et acidophiles (cellules de Hürthle).Ces signes sont liés à la présence du goître induit par la thyroïdite. Celui-ci est diffus et sa surface est le plus souvent régulière. Sa consistance est très particulière : ferme, « suiffée » ou « caoutchoutée ». Parfois ces changements peuvent ne pas être palpables. Le goitre peut éventuellement être responsable de signes compressifs (dysphonie, dysphagie et dyspnée). La palpation cervicale ne retrouve généralement pas d'adénomégalie. Il n'est pas mis en évidence non plus de douleur ou de signes inflammatoires locaux.Ces derniers sont liés à la dysthyroïdie avec au premier plan l'hypothyroïdie. L'hyperthyroïdie peut également être présente, en particulier au début de l'évolution de la maladie. On soulignera que les signes de dysthyroïdie peuvent être absents initialement, la fonction thyroïdienne n'étant pas nécessairement perturbée. En revanche plus la maladie évolue et plus l'hypothyroïdie devient fréquente.Parmi les signes cliniques induits par l'hypothyroïdie on retrouve notamment : constipation, bradycardie, myxœdème, anémie, règles irrégulières, asthénie, troubles de la concentration et de la mémoire, dépression, peau sèche et épaissie, perte de cheveux.La positivité à un taux élevé des anticorps anti-TPO, retrouvée dans 95 % des cas, est le meilleur signe biologique pour diagnostiquer la thyroïdite de Hashimoto. Elle survient préférentiellement chez des sujets HLA B8-DR3 (en). Le titre en anticorps est de plus associé au degré d'infiltration lymphoïde de la glande. On notera que la positivité des anticorps anti-TPO est par ailleurs très rare chez les sujets sains.En cas de négativité des anticorps anti-TPO, on peut retrouver une augmentation des anticorps anti-thyroglobuline.Il n'y a pas nécessairement, au début, de trouble de la fonction hormonale, mais la maladie évoluera toujours vers une hypothyroïdie avec des taux de T4 anormalement bas et secondairement des taux de TSH élevés.Enfin, la thyroïdite de Hashimoto n'est pas associée à la présence de marqueurs sériques de l'inflammation.L'échographie de la thyroïde montre un goitre hypoéchogène,. Le parenchyme thyroïdien devient plus hétérogène au cours de l'évolution. On peut notamment mettre en évidence des pseudo-nodules et des nodules de régénérations hyperéchogènes (white knight). Des ganglions récurrentiels peuvent être visualisés. La vascularisation est hétérogène en Doppler couleur. L'étude en Doppler pulsé retrouve une élévation des vitesses systoliques, toutefois moindre que dans la maladie de Basedow.La scintigraphie est inutile dans ce contexte d'hypothyroïdie. Lorsqu'elle est réalisée elle montre des résultats très variables ne contribuant donc pas au diagnostic.La thyroïdite de Hashimoto peut être associée à toute autre maladie auto-immune : collagénose, insuffisance surrénalienne, à un cancer de la thyroïde ou entraîner des complications cardio-vasculaires. Elle peut aussi entraîner des symptômes laissant penser à tort à un virage maniaque (manie) caractéristique d'un trouble bipolaire.La thyroïdite de Hashimoto peut se compliquer d'une encéphalopathie (encéphalopathie de Hashimoto). Cette entité a été décrite pour la première fois en 1966 et seuls une centaine de cas ont été rapportés dans la littérature depuis.Le lymphome thyroïdien complique moins de 1 % des thyroïdites auto-immunes,. Toutefois celui-ci doit être évoqué devant toute augmentation de volume du goitre ou en cas de survenue d'adénopathie.Le traitement chirurgical n'a aujourd'hui que rarement sa place dans la thyroïdite de Hashimoto. On le réservera essentiellement aux goitres compressifs.La prise en charge est avant tout médicale, consistant en une hormonothérapie thyroïdienne substitutive. Les hormones thyroïdiennes permettraient une diminution du volume du goitre tout en corrigeant l'hypothyroïdie latente ou évidente.Endocrionologie, diabète et maladies métaboliques. Collège des enseignants d'endocrinologie, diabète et maladies métaboliques p361InfoThyroAFMT : site officiel de l'Association française des malades de la thyroïde Portail de la médecine
médecine;"Un vaccin est une préparation biologique administrée à un organisme vivant afin d'y stimuler son système immunitaire et d'y développer une immunité adaptative protectrice et durable contre l'agent infectieux d'une maladie particulière. Un vaccin est administré lors d'une vaccination à un individu sain.La substance active d’un vaccin est un agent antigénique soit à pathogénicité atténuée par une forme tuée ou affaiblie du micro-organisme pathogène, ou par une de ses toxines, ou par une de ses composantes caractéristiques, par exemple une protéine d'enveloppe, soit un acide nucléique. Plusieurs types de vaccins existent selon le procédé utilisé pour obtenir des anticorps neutralisants : virus entiers atténués ou inactivés, vecteurs viraux génétiquement modifiés réplicatifs ou non réplicatifs (adénovirus, vaccine), sous-unités vaccinales obtenues par recombinaison génétique, anatoxines et acides nucléiques (ADN, ARN messager).La réaction immunitaire primaire met en mémoire l'antigène menaçant présenté pour que, lors d'une contamination ultérieure, l'immunité ainsi acquise puisse s'activer plus rapidement et plus fortement.L'Organisation mondiale de la santé (OMS) signale que des vaccins homologués sont disponibles pour plus de vingt infections différentes ainsi évitables. Les vaccins les plus connus sont ceux contre la poliomyélite, antidiphtérique, antitétanique, contre la coqueluche, la tuberculose, la rougeole, la grippe saisonnière, les fièvres hémorragiques Ebola et la Covid-19.En dépit d'un solide consensus scientifique, il existe au niveau mondial une controverse sur l'intérêt des vaccins et de la vaccination, variable selon les pays et les contextes sanitaires et sociétaux.Le mot « vaccin » dérive du mot « vaccine » (lui-même issu du latin vaccinus qui signifie « de vache »), utilisé par Edward Jenner en 1798 pour désigner sa formulation médicale recueillie à partir des pustules de variole présentes sur le pis des vaches (variole de la vache, appelée en français vaccine, bénigne) puis inoculée aux humains pour les préserver de la variole humaine. Cette formulation constitue ainsi le premier de tous les vaccins.En 1881, pour honorer Jenner, Louis Pasteur proposa que les termes « vaccin » et « vaccination » soient étendus pour couvrir les nouvelles inoculations protectrices alors en cours de développement.Avant l'introduction de la vaccination avec des éléments provenant de cas de variole de la vache, la variole pouvait être prévenue par l'inoculation délibérée du virus de la variole, pratique appelée plus tard variolisation pour la distinguer de la vaccination antivariolique. La pratique de l'inoculation de la variole a ses premiers indices au Xe siècle en Chine et la plus ancienne utilisation documentée de 1549, également chinoise. Les Chinois ont alors mis en œuvre une méthode d'« insufflation nasale » administrée en soufflant du matériel de variole en poudre, généralement des croûtes, dans les narines. Diverses techniques d'insufflation ont été enregistrées au cours des XVIe et XVIIe siècles en Chine. Deux rapports sur la pratique chinoise de cette inoculation ont été reçus par la Royal Society de Londres en 1700.À la fin des années 1760, alors qu'il étudiait son futur métier de chirurgien / apothicaire, Edward Jenner connut l'histoire, courante dans les zones rurales, que les travailleurs laitiers n'avaient jamais la variole humaine, souvent fatale ou défigurante, parce qu'ils avaient déjà contracté la variole de la vache (la vaccine) qui était beaucoup moins violente chez l'homme. En 1796, Jenner a pris du pus de la main d'une laitière ayant la vaccine, l'a gratté dans le bras d'un garçon de 8 ans, James Phipps. Six semaines plus tard, il lui a inoculé la variole humaine : celui-ci ne l'a pas développé. Jenner a rapporté en 1798 que l'inoculation de son produit était sans risque, chez les enfants comme chez les adultes, cette vaccination étant beaucoup plus sûre que l'inoculation de la variole humaine. Cette dernière pratique pourtant alors usuelle, a ensuite été interdite en Angleterre en 1840.La seconde génération de vaccins a été introduite dans les années 1880 par Louis Pasteur qui a développé des vaccins contre le choléra et contre l'anthrax du poulet. À partir de la fin du XIXe siècle, les vaccins étaient considérés comme une question de prestige national et des lois sur la vaccination obligatoire ont été adoptées.Le XXe siècle a vu l'introduction de plusieurs vaccins efficaces, notamment ceux contre la diphtérie, la rougeole, les oreillons, le tétanos et la rubéole. Les principales réalisations comprennent le développement du vaccin contre la poliomyélite dans les années 1950. Grâce au vaccin l'éradication mondiale de la variole humaine a été obtenue dans les années 1970. Maurice Hilleman a été le plus prolifique des développeurs de vaccins au XXe siècle. Comme les vaccins sont devenus plus courants, de nombreuses personnes ont commencé à les tenir pour acquis. Cependant, les vaccins restent insaisissables pour de nombreuses maladies importantes, notamment l'herpès simplex, le paludisme, la gonorrhée et le VIH.Un vaccin est une préparation administrée pour provoquer l’immunité protectrice et durable de l'organisme contre une maladie en stimulant la production d’anticorps. Un ou plusieurs antigènes microbiens doivent être utilisés pour induire une nette amélioration de cette immunité.Le but principal des vaccins est d'obtenir, par l'organisme lui-même, la production d'anticorps et l'activation de cellules T (lymphocyte B ou lymphocyte T à mémoire) spécifiques à l'antigène. Une immunisation réussie doit donc procurer une protection contre une future infection d'éléments pathogènes identifiés. Un vaccin est donc spécifique à une maladie mais pas à une autre.Outre le vaccin actif lui-même, les excipients et les composés de fabrication résiduels suivants peuvent être présents ou ajoutés dans certaines préparations vaccinales :des adjuvants, pour stimuler la réponse immunitaire (plus précoce, plus puissante, plus persistante) au vaccin, de façon non spécifique, du système immunitaire inné ; ils permettent de réduire la dose de vaccin, comme :des sels ou gels d'aluminium,une association aluminium-lipides (AS04),des émulsions eau-squalène,des virus végétaux ; début mai 2008, Denis Leclerc a proposé d'utiliser un virus végétal (qui ne peut se reproduire chez l'homme) comme pseudovirion jouant le rôle d'adjuvant, pour rendre des vaccins plus longuement efficaces contre des virus qui mutent souvent (virus de la grippe ou de l'hépatite C, voire contre certains cancers). Le principe est d'associer à ce pseudovirion une protéine-cible interne aux virus, bactéries ou cellules cancéreuses à attaquer, et non comme on le fait jusqu'ici une des protéines externes qui sont celles qui mutent le plus. Ce nouveau type de vaccin, qui doit encore faire les preuves de son innocuité et de son efficacité, déclencherait une réaction immunitaire à l'intérieur des cellules, au moment de la réplication virale,des produits antimicrobiens :des antibiotiques, pour empêcher la croissance de bactéries pendant la production et le stockage du vaccin,le formaldéhyde, utilisé pour inactiver les produits bactériens des vaccins anatoxines ; Le formaldéhyde est également utilisé pour inactiver les virus indésirables et tuer les bactéries qui pourraient contaminer le vaccin pendant la production,le thiomersal, antimicrobien controversé contenant du mercure qui est ajouté aux flacons de vaccin qui contiennent plus d'une dose pour empêcher la contamination et la croissance de bactéries potentiellement nocives,le glutamate monosodique (MSG) et le 2-phénoxyéthanol sont utilisés comme stabilisants dans quelques vaccins pour aider le vaccin à rester inchangé lorsque le vaccin est exposé à la chaleur, à la lumière, à l'acidité ou à l'humidité ;la protéine d’œuf, présente dans les vaccins contre la grippe et contre la fièvre jaune car ils sont préparés à partir d’œufs de poule ; d'autres protéines peuvent parfois être présentes.L'immunogénicité (ou efficacité sérologique) est la capacité d'un vaccin à induire des anticorps spécifiques. Les anticorps sont produits par des lymphocytes B se transformant en plasmocytes. Le temps nécessaire à l'induction d'anticorps est de 2 à 3 semaines après la vaccination. Cette production d'anticorps diminue progressivement après plusieurs mois ou années. Elle est mesurable et cette mesure peut être utilisée dans certains cas pour savoir si le sujet est vacciné efficacement (vaccin anti-hépatite B et anti-tétanos en particulier).Le nombre de lymphocytes B mémoire, non sécrétant, mais qui réagissent spécifiquement à la présentation d'un antigène, semble, lui, ne pas varier au cours du temps. Ce qui permet d'induire une protection de longue durée, jusqu'à des décennies (ou tant que le sujet reste immunocompétent), car la réactivation de l'immunité mémoire lors d'une nouvelle infection s'effectue alors en quelques jours.Cependant, certains vaccins ne provoquent pas la formation d'anticorps mais mettent en jeu une réaction de protection d'immunité cellulaire, c'est le cas du BCG (« vaccin Bilié de Calmette et Guérin », vaccin anti-tuberculeux).Selon certaines études, notamment sur la grippe A (H1N1), la vaccination d'un individu ne le rend pas non contagieux pour autantL'efficacité clinique d'un vaccin se mesure par la réduction de la fréquence de la maladie chez les sujets vaccinés (taux de protection effectif de la population vaccinée). Elle est parfois estimée par des marqueurs de substitution (taux d'anticorps connus protecteurs), mais l'efficacité sérologique (mesurée en laboratoire) ne concorde pas toujours avec l'efficacité clinique (mesurée en épidémiologie de terrain).Le suivi et la surveillance d'une politique vaccinale s'effectuent par l'épidémiologie des maladies vaccinables (surveillance par des réseaux de laboratoire hospitaliers, centres de référence, réseaux sentinelles, notification systématique ou obligatoire…), la surveillance des effets indésirables (pharmacovigilance, registres de suivi…) et par des études séro-épidémiologiques (séroprévalence). Ces études permettent d'évaluer l'immunité collective des populations, dont la situation et la localisation des sujets non-vaccinés, réceptifs ou vulnérables.Selon le type de vaccin, et l'état de santé du sujet, les vaccins peuvent être contre-indiqués ou fortement recommandés.Lors d'une vaccination, les effets indésirables pouvant être reliés au vaccin administré dépendent d'abord de l'agent infectieux combattu, du type de vaccin (agent atténué, inactivé, sous-unités d'agent, etc.), de ses excipients (nature du solvant, adjuvants, conservateurs chimiques antibactériens, etc.) utilisés.Suivant les vaccins, certains effets indésirables, en général bénins, se retrouvent de manière plus ou moins fréquente. L'une des manifestations les plus courantes est la fièvre et une inflammation locale qui traduisent le déclenchement de la réponse immunitaire recherchée par la vaccination. Dans de très rares cas, la vaccination peut entraîner des effets indésirables sérieux et, exceptionnellement, fatals. Un choc anaphylactique, extrêmement rare, peut par exemple s'observer chez des personnes susceptibles avec certains vaccins (incidence de 0,65 par million, voire 10 par million pour le vaccin rougeole-rubéole-oreillons (RRO)). En France, la loi prévoit le remboursement des dommages et intérêts par l'Office national d'indemnisation des accidents médicaux lorsqu'il s'agit de vaccins obligatoires. Liés à l'hydroxyde d'aluminium La myofasciite à macrophages a été associée à la persistance pathologique de l'hydroxyde d'aluminium utilisé dans certains vaccins. Cependant lors de sa réunion de décembre 2003, le Comité consultatif mondial sur la sécurité des vaccins, après avoir examiné les données d’une étude cas témoins réalisée en France, a conclu, en accord avec ses précédentes déclarations, que la persistance de macrophages contenant de l’aluminium au site d’injection d’une vaccination antérieure n’est associée ni à des symptômes cliniques ni à une maladie spécifique. C'est aussi la conclusion à laquelle est parvenue l'agence française de sécurité du médicament, qui ne voit dans la myofasciite à macrophages qu'un phénomène histologique auquel aucun syndrome clinique spécifique ne peut être associé.Un sérum ne doit pas être confondu avec un vaccin. Mais ils peuvent parfois être associés lors de l'injection : c'est la sérovaccination.Par abus de langage, le terme de vaccination s'applique parfois à diverses inoculations et injections. Ainsi l'immunocastration des porcs est souvent présentée comme un vaccin (contre l'odeur de verrat). En 1837, Gabriel Victor Lafargue parla de « vaccination morphinique » pour ce qui n'était qu'une injection sous-épidermique. Dans cette catégorie se place également le vaccin de Coley (qui génère une hyperthermie destinée à détruire des tumeurs).Les vaccins sont souvent classés en deux grandes catégories selon qu'ils sont issus ou non d'agents infectieux. La première catégorie se répartit entre vaccins vivants atténués et vaccins inactivés. La seconde catégorie se répartit principalement entre vaccins conjugués, à anatoxine, à sous unité protéique, à ARN / à ADN.On les trouve aussi classés selon le type d'agent infectieux traité : vaccins bactériens / vaccins viraux.Les abréviations des noms de vaccins se sont relativement harmonisées au niveau mondial mais il n'y a pas encore au début de l'année 2020 de normalisation mondialement partagée. L'OMS, en collaboration avec l'institut norvégien de la santé publique, en propose une nomenclature. Les États-Unis utilisent une autre liste,.Plus de vingt vaccins font partie de la liste des médicaments essentiels de l'OMS,, dans la classe ATC J07, pour les adultes comme pour les enfants. Vivant atténué Les agents infectieux sont multipliés en laboratoire jusqu’à ce qu’ils perdent naturellement ou artificiellement, par mutation, leur caractère pathogène. Les souches obtenues ont perdu leur virulence (rendues incapables de développer la maladie), mais elles restent vivantes avec une capacité transitoire à se répliquer chez l'hôte. Ils créent donc une infection a minima.Ce genre de vaccin stimule l'immunité spécifique de façon généralement plus efficace et plus durable que celui composé d’agents infectieux inactivés. Ils peuvent parfois induire après vaccination des réactions locales ou générales qui sont des symptômes mineurs de la maladie qu'ils préviennent. Du fait de ce risque infectieux potentiel, ils sont contre-indiqués en principe chez la femme enceinte et les personnes immunodéprimées.Les vaccins vivants ne contiennent pas d'adjuvants : ils n'en ont pas besoin.Les principaux vaccins vivants disponibles sont le vaccin BCG (tuberculose)  le ROR (Rougeole, Oreillons, Rubéole), le vaccin contre la  varicelle, contre le zona, contre la fièvre jaune, le vaccin oral contre la poliomyélite, contre les gastro-entérites à rotavirus. Inactivé Les agents infectieux, une fois identifiés et isolés, sont multipliés en très grand nombre puis altérés, chimiquement ou par la chaleur. Ils conservent néanmoins une certaine capacité immunogène (aptitude à provoquer une protection immunitaire) moins ciblée. C'est pourquoi ils nécessitent l'ajout d'adjuvant et font souvent l'objet de plus de rappels de vaccination.Un vaccin inactivé peut être :entier ou complet, lorsqu'il est composé du micro-organisme complet mais tué ou inactivé ; par exemple, le vaccin « cellulaire » contre la coqueluche. Ces vaccins sont très efficaces mais plus « réactogènes » avec un risque plus élevé de réactions indésirables ;sous-unitaire ou « à sous-unités » lorsqu'il est composé d'une fraction du micro-organisme inactivé. Il peut être obtenu de façon classique ou à partir de biotechnologie ou de biologie de synthèse ; par exemple, le vaccin « acellulaire » contre la coqueluche. Cette fraction peut-être un peptide de surface du germe, un polysaccharide de paroi bactérienne, une anatoxine, une particule pseudo-virale ou tout autre composant immunogène du micro-organisme. Ces vaccins sont moins immunogènes, mais avec moins d'effets secondaires.Les vaccins polyosidiques ou à polysaccharides activent les seuls lymphocytes B. Ils sont inefficaces avant l'âge de deux ans. Par exemple, le vaccin polysaccharidique contre le pneumocoque. Ils ont une faible réponse mémoire et nécessitent plus de rappels.voir aussi Vaccin à ARN, Vaccin à ADN Conjugués Les vaccins conjugués se basent sur la liaison d'un polysaccharide (antigène capsulaire) avec une protéine porteuse. Cette conjugaison permet d'induire une bonne réponse mémoire et d'activer les lymphocytes T, ce qui les rend utilisables chez l'enfant de moins de deux ans. Le premier de ce type a été le vaccin contre Haemophilus influenza b ou Hib, agent de méningite purulente du nourrisson. D'autres vaccins conjugés de ce type sont le vaccin contre le méningocoque, le vaccin contre le pneumocoque. Anatoxine Un vaccin anatoxine est produit par inactivation physique ou chimique de la molécule initialement toxique qui cause la maladie et qui est produite par l'agent infectieux. La molécule ainsi inactivée perd ses propriétés toxiques mais conserve sa structure et des propriétés immunisantes. Les vaccins anatoxines sont connus pour leur efficacité.Les vaccins antitétaniques et antidiphtériques sont des vaccins à base d'anatoxines. Sous-unité protéique Une sous-unité protéique, un fragment protéique (ou un assemblage de plusieurs fragments) de la surface du micro-organisme pathogène, peut créer une réponse immunitaire.Le vaccin contre l'hépatite B, celui contre la grippe saisonnière, ceux contre le papillomavirus humain (HPV), sont des vaccins à sous-unité protéique. Issus du génie génétique (biologie de synthèse) Certaines de ces molécules peuvent être obtenues par génie génétique, et ainsi être produites en grande quantité. La stratégies de développement la plus connue consiste à insérer des gènes microbiens dans des Escherichia Coli, des levures ou des cellules animales en culture, de façon à leur faire produire des protéines microbiennes spécifiques, par exemple l'antigène de surface de l'hépatite B, qui est ensuite utilisée dans le vaccin contre l'hépatite B.D'autres stratégies sont la production de pseudoparticules virales, dépourvues d'ADN viral et incapables de se répliquer (vaccin contre les papillomavirus) ; la recombinaison génétique permet des virus atténués « réassortants » (vaccin contre la grippe, vaccin contre les rotavirus).Il existe également une recherche sur des vaccins oraux basés sur des plantes (production d'antigènes par des algues). Hétérologue Un vaccin hétérologue (ou hétérotypique) est fabriqué à partir d’un micro-organisme différent de celui de la maladie à combattre mais ayant des similitudes immunologiques suffisantes pour induire une protection croisée de qualité acceptable.L'exemple classique est l'utilisation par Jenner du virus de la vaccine (variole de la vache) pour protéger l'humain contre la variole. Un exemple actuel est l'utilisation du vaccin BCG préparé à partir d'une souche atténuée de bacille tuberculeux bovin (Mycobacterium bovis) pour protéger contre la tuberculose humaine.L'autovaccin (autogenous vaccine) est un vaccin élaboré à partir d'une souche spécifique de micro-organismes prélevée sur le malade lui-même. Ils connaissent un développement important en médecine vétérinaire dans les élevages de porc notamment,. « On entend par autovaccin à usage vétérinaire, tout médicament vétérinaire immunologique fabriqué en vue de provoquer une immunité active à partir d’organismes pathogènes provenant d’un animal ou d’animaux d’un même élevage, inactivés et utilisés pour le traitement de cet animal ou des animaux de cet élevage » (article L 5141-2 du Code de la santé publique).Les vaccins multivalents ou combinés, associent des combinaisons d'antigènes, permettant de cibler plusieurs maladies différentes en un seul vaccin (par exemple Rougeole-Oreillons-Rubéole ou Diphtérie-Tétanos-Poliomyélite-Coqueluche-Hib-Hépatite B). Ces vaccins permettent de diminuer le nombre d'injections et d'augmenter la couverture vaccinale.Plusieurs vaccins sont en cours de développement :Les vaccins combinant des cellules dendritiques avec des antigènes afin de présenter ceux-ci aux globules blancs du corps, stimulant ainsi une réaction immunitaire. Ces vaccins ont montré des résultats préliminaires positifs pour le traitement des tumeurs cérébrales et sont également testés dans le mélanome malin [réf. nécessaire].Vaccin à ADN : Le mécanisme proposé est l'insertion (et l'expression, renforcée par l'utilisation de l'électroporation, déclenchant la reconnaissance du système immunitaire) d'ADN infectieux, viral ou bactérien, dans des cellules humaines ou animales. Certaines cellules du système immunitaire qui reconnaissent les protéines exprimées monteront alors une attaque contre ces protéines et les cellules qui les expriment. Parce que ces cellules vivent très longtemps, si le pathogène qui exprime normalement ces protéines est rencontré plus tard, elles seront attaquées instantanément par le système immunitaire[réf. nécessaire]. Ces vaccins sont très faciles à produire et à stocker[réf. nécessaire]. Plusieurs vaccins à ADN sont disponibles en 2019 pour un usage vétérinaire mais aucun vaccin à ADN n'a encore été approuvé pour un usage chez l'homme. Comme le vaccin à ARN, le vaccin à ADN est parfois dit « vaccin génétique », car il introduit dans l'organisme un élément génétique du virus.Vecteur recombinant : en combinant la physiologie d'un micro-organisme et l'ADN d'un autre, l'immunité peut être créée contre les maladies qui ont des processus d'infection complexes[réf. nécessaire]. Le vaccin contre le virus Ebola en est un exemple.Le vaccin à ARN est composé d'un ARN messager conditionné dans un vecteur tel que des nanoparticules lipidiques. Ce type de vaccin a montré son efficacité pour lutter contre la maladie à coronavirus 2019(SARS-Cov-2) , voir Vaccin contre la Covid-19.Des vaccins peptidiques des récepteurs des lymphocytes T qui modulent la production de cytokines et améliorent l'immunité à médiation cellulaire[réf. nécessaire]Des vaccins utilisant le ciblage de protéines bactériennes impliquées dans l'inhibition du complément et qui neutraliseraient le mécanisme clé de virulence bactérienne[réf. nécessaire]. Vétérinaires La vaccination des animaux est utilisée à la fois pour prévenir certaines de leurs maladies infectieuses et pour prévenir la transmission de maladies aux humains. Les animaux de compagnie et les animaux élevés comme bétail sont quasi-systématiquement vaccinés.En cas de propagation de la rage, la vaccination antirabique des chiens peut être exigée par la loi. Des populations sauvages (renard, raton laveur) peuvent également être alors vaccinées.En plus de celui de la rage, les principaux vaccins canins sont celui contre la maladie de Carré, le parvovirus canin, l'hépatite canine infectieuse, l'adénovirus-CAV2, la leptospirose, la bordatella, la toux de chenil et la maladie de Lyme.Les vaccins DIVA (pour Differentiation of Infected from Vaccinated Animals), également connus sous le nom de SIVA (pour Segregation of Infected from Vaccinated Animals), permettent de différencier les animaux infectés des animaux vaccinés. Pour les végétaux Les vaccins actuels sont essentiellement faits pour les humains et d'autres animaux (vaccins vétérinaires) mais on sait maintenant que les plantes ont aussi un système immunitaire, et qu'il est possible de les vacciner. Un premier vaccin commercialisé pour les plantes a été créé en 2001 par la société Goëmar.Grâce à des tests moléculaires permettant d'identifier les siRNA efficaces contre le virus de la tomate (Tomato bushy stunt virus ou TBSV, de la famille des Tombusviridae), un vaccin a pu être produit et, en laboratoire, a donné les résultats espérés ; il peut en outre être pulvérisé sur les feuilles (pas besoin d'injection). Un projet est de faire un vaccin contre le virus de la mosaïque du concombre (capable de détruire des champs entiers de concombres, citrouilles ou  melons). La méthode est aussi plus simple et plus rapide que de concevoir une plante OGM résistant au virus. Selon une autre étude publiée sur un magazine spécialisé, des chercheurs ont testé leurs molécules sur des plantes, par spray, et 90% d’entre elles n’ont pas été infectées par le virus.Le développement complet est un processus le plus souvent très long, qui se compte habituellement en années, avec plusieurs étapes successives : une phase préclinique (hors expérimentation humaine), trois phases cliniques (avec expérimentation humaine), une phase d'autorisation administrative, une phase de production industrielle, une phase de vaccination et une phase finale de pharmacovigilance.Autrefois, ce processus débutait par des expérimentations animales qui se sont révélées décevantes pour prédire l'efficacité d'un vaccin. Actuellement[Quand ?], on débute beaucoup plus tôt l'expérimentation humaine : c'est ce qu'on appelle la médecine expérimentale ou translationnelle. Ce développement doit respecter les différentes phases d'un essai clinique de vaccin.Les chercheurs doivent d'abord choisir une voie d'administration : voie nasale, orale ou par injection. Ce choix peut dépendre du vecteur choisi, de l’antigène, de l'adjuvant ou d'autres paramètres. Si la voie par injection est choisie, il faut aussi choisir quelle injection : intradermique, sous-cutanée ou intramusculaire.Les phases 1 et 2 permettent d'établir l'innocuité du projet de vaccin. La phase 3, plus étendue, permet de tester son efficacité. Celle-ci se mesure uniquement vis-à-vis de la prévention de la maladie ou de l'infection, que le vaccin projeté est censé empêcher. Plusieurs moyens permettent d'évaluer cette efficacité :Surtout avec un essai d'efficacité contrôlé et aléatoire comprenant des critères cliniques adaptés. Son but doit démontrer la diminution de l'infection ou de la maladie après immunisation par rapport au groupe de référence non immunisé.On peut faire aussi une évaluation de l'efficacité par observation, toujours avec des critères cliniques, pour évaluer l'effet protecteur d'un vaccin dans des conditions réelles au sein d'une population ouverte.Dans certains cas, on peut se contenter de critères immunologiques comme par exemple un titrage des anticorps. Phase I Dans cette phase, on vérifie d'abord l'innocuité du produit avant de s'intéresser à son efficacité. Généralement on teste alors  le produit candidat-vaccin à dose croissante sur des petits groupes (rarement plus de 100 volontaires). Le nombre de doses peut varier en fonction du type de vaccin. Les effets secondaires sont soigneusement répertoriés. Mais à ce stade, certains effets secondaires graves comme la réaction anaphylactique sont rarement détectés en raison du très petit nombre de participants.Le protocole d'étude doit établir les effets secondaires spécifiquement au vaccin et les quantifier (injection peu douloureuse ou très douloureuse).Les chercheurs s’intéressent bien entendu à la réponse immunologique (par exemple, le dosage des anticorps). Mais ce dosage n'est pas forcément synonyme d'efficacité du vaccin. On parle alors d'immunogénicité du vaccin. On propose enfin un ""meilleur"" dosage du vaccin. Phase II Si la phase I est concluante (pas d'effets secondaires graves plus réponse immunitaire satisfaisante), on peut entamer la phase II, où l'on commence d'abord par augmenter la taille du groupe étudié : même protocole que la phase I mais plus de participants (phase II a) sur plusieurs centaines à quelques milliers de volontaires.On teste alors l'efficacité de la réponse immunitaire ainsi que la tolérance du projet de vaccin. On identifie largement les effets secondaires constatés. On cherche aussi à déterminer la posologie adaptée (quantité de produit, nombre de doses) et un premier calendrier vaccinal (durée entre vaccinations).Beaucoup de candidat-vaccins ne passent pas cette phase : ils ont une réponse immunitaire satisfaisante mais celle-ci n'est pas efficace ou suffisante pour empêcher la maladie ou leurs effets secondaires sont jugés trop graves. Phase III Si la phase II est satisfaisante le projet de vaccin peut passer en phase III. Les tests de sécurité et d'efficacité continuent avec une population beaucoup plus grande (de l'ordre de plusieurs dizaines de milliers de volontaires) et hétérogène (sexe, classes d'âge, diversité génétique, etc). Il s'y ajoute les études d'homogénéité d'un lot de vaccin à un autre qui consistent à vérifier l'homogénéité de la fabrication de plusieurs lots cliniques d'un point de vue clinique.Enfin, des études d'administrations simultanées vérifient l'absence d'interférence significative lorsqu'il est administré concomitamment à un vaccin déjà homologué et inclus dans les programmes courants de vaccination.Malgré la plus grande taille des groupes étudiés, les effets secondaires très rares ne seront pas forcément tous connus au cours de la phase III : les essais cliniques de sécurité en phase III sont normalement conçus pour observer les effets indésirables jusqu'à un taux de 1 pour 10 000.Cette phase est la plus longue et la plus coûteuse : entre 2 et 13 ans et environ 750 millions d'euros.Cette phase va définir le ratio risques/bénéfices qui est obligatoire pour l'enregistrement et l'autorisation de chaque vaccin.Alors que le vaccin est commercialisé et que la vaccination est en cours, cette dernière phase, souvent dénommée phase IV, est une étude de pharmacovigilance consistant notamment en une surveillance de la sécurité et des effets secondaires du vaccin sur une population beaucoup plus large. Ceci est effectué en détectant des possibles manifestations post-vaccinales indésirables (MAPI), en les analysant médicalement, en évaluant la causalité des effets observés vis à vis du vaccin et en restituant les résultats obtenus aux autorités.Cette phase peut remettre en cause la phase d'autorisation administrative.Le marché mondial des vaccins s'apparente à un oligopole. Quatre producteurs principaux se partageant l'essentiel du marché, même s'il existe un grand nombre d'autres demandeurs. Suivant les sources, la répartition en part de marché s’établit ainsi :2016 :1 Merck : 19 %, 2 Sanofi : 18 %, 3 GlaxoSmithKline : 16 % et 4 Pfizer : 13 % ;2016 : 1 Merck & Co : 24,5 %, 2 GlaxoSmithKline : 22,6 %, 3 Pfizer : 22 %, 4 Sanofi : 20,2 %.De nombreuses barrières à l'entrée sont présentes, cela signifie que les entreprises candidates à l’entrée sur ce marché ont un coût de production supérieur à celui des entreprises en place. Effectivement, des gros investissements sont nécessaires quant à la construction d'un laboratoire et à la recherche de nouveaux vaccins.Les efforts d'investissement dans ce milieu sont principalement concentrés en Europe et en Amérique du Nord. Plus de 50 % des investissements en recherche et développement ont été menés en Europe entre 2002 et 2010.Le dépôt de brevets sur les processus de développement de vaccins peut parfois être considéré comme un obstacle au développement de nouveaux vaccins. En raison de la faible protection offerte par un brevet sur le produit final, la protection de l'innovation concernant les vaccins se fait souvent par le biais du brevet de certains procédés alors utilisés ainsi que la protection du secret pour d'autres procédés[réf. nécessaire].Selon l'Organisation mondiale de la santé, le plus grand obstacle à la production locale de vaccins dans les pays moins développés n'a pas été les brevets, mais les exigences substantielles en matière financière, d'infrastructure et de main-d'œuvre nécessaires à l'entrée sur le marché. Les vaccins sont des mélanges complexes de composés biologiques et, contrairement au cas des médicaments, il n'existe pas de vrais vaccins génériques. Le vaccin produit par une nouvelle installation doit subir des tests cliniques complets de sécurité et d'efficacité similaires à ceux subis par celui produit par le fabricant d'origine. Pour la plupart des vaccins, des procédés spécifiques ont été brevetés. Ceux-ci peuvent être contournés par des méthodes de fabrication alternatives, mais cela nécessitait une infrastructure de recherche et développement et une main-d'œuvre qualifiée. Dans le cas de quelques vaccins relativement nouveaux comme le vaccin contre le papillomavirus humain, les brevets peuvent imposer une barrière supplémentaire[réf. nécessaire].Certaines plantes transgéniques ont été identifiées comme des systèmes d'expression prometteurs pour la production de vaccins. Des plantes complexes telles que le tabac, la pomme de terre, la tomate et la b"
médecine;"En médecine, l’étiologie (ou étiopathogénie) est l'étude des causes et des facteurs d'une maladie. Ce terme est aussi utilisé dans le domaine de la psychiatrie et de la psychologie pour l'étude des causes des maladies mentales. L'étiologie définit l'origine d'une maladie en fonction de signes ou symptômes, c'est-à-dire en jargon de ses manifestations sémiologiques.En littérature, on parle de récit ou conte étiologique lorsqu'une histoire, orale ou écrite, a pour but de donner une explication imagée à un phénomène ou une situation dont on ne maîtrise pas l'origine. En philosophie, l’étiologie est l'étude de l'ensemble des causes d'un phénomène.L’étiologie est l’étude de la causalité ou de l’origine. Le mot est dérivé du grec αἰτιολογία (aitiología) « donnant une raison pour » (αἰτία, aitía, « cause »; et -οοογνα, -logía). Ainsi, l’étiologie est l’étude des causes, des origines, des raisons pour lesquelles les choses sont telles qu'elles sont ou encore la façon dont elles fonctionnent; l'étiologie peut également se référer aux causes elles-mêmes. Le mot est couramment utilisé en médecine[réf. à confirmer] (concernant les causes et les facteurs de la maladie) et en philosophie, mais aussi en physique, psychologie (pour l'étude des causes des maladies mentales), géographie, analyse spatiale, théologie et biologie, en référence aux causes ou aux origines de divers phénomènes.L'étiologie (du grec ancien : αἰτιολογία, « recherche, exposition des causes ») concerne une école philosophique de l'Antiquité s'intéressant à l'étude des causes.Le sens étiologique est l'un des trois types de sens littéral identifiés par Thomas d'Aquin, lorsqu’un énoncé a été dit en fonction d’une condition particulière d’énonciation .En médecine, l’étiologie d’une maladie ou d’une condition se réfère aux études fréquentes pour déterminer un ou plusieurs facteurs qui se réunissent pour causer la maladie[source insuffisante]. De même, lorsque la maladie est répandue, les études épidémiologiques étudient quels facteurs associés, tels que l’emplacement, le sexe, l’exposition aux produits chimiques, et bien d’autres, rendent une population plus ou moins susceptible d’avoir une maladie, une condition ou une maladie, aidant ainsi à déterminer son étiologie.Parfois, la détermination de l’étiologie est un processus imprécis. Dans le passé, l’étiologie de la maladie marine dorénavant connue, le scorbut[réf. à confirmer], était longtemps inconnue. Lorsque de grands navires furent construits, les marins commencèrent à prendre la mer pendant de longues périodes et manquaient souvent de fruits et légumes frais. Sans connaître la cause précise, le capitaine James Cook soupçonnait le scorbut d’être causé par le manque de légumes dans l’alimentation.Intrigué et basé sur ses soupçons, il a forcé son équipage à manger de la choucroute tous les jours. Il y a eu des résultats positifs et il en a donc déduit qu'en mangeant cela, il empêchait le scorbut, même s’il ne savait pas exactement pourquoi. Il a fallu encore environ deux cents ans pour découvrir l’étiologie précise: le manque de vitamine C dans l’alimentation d’un marin.Conditions héréditaires : l'hémophilie, un trouble qui entraîne des saignements excessifsTroubles métaboliques et endocriniens ou hormonaux : ce sont des anomalies dans la signalisation chimique et l’interaction dans le corps. Par exemple, le diabète sucré ou mellitus est une maladie endocrinienne qui cause une glycémie élevée.Troubles néoplastiques ou cancer : les cellules du corps deviennent incontrôlées.Problèmes d’immunité : tels que les allergies, qui sont une réaction excessive du système immunitaire.Un mythe étiologique, ou mythe d’origine, est un mythe destiné à expliquer les origines des pratiques culturelles, des phénomènes naturels, des noms propres etc.Dans le passé, lorsque de nombreux phénomènes physiques n’étaient pas bien compris ou bien lorsque les histoires n’étaient pas notés, des mythes survenaient souvent pour fournir des étiologies.Ainsi, un mythe étiologique[réf. à confirmer], ou mythe d’origine, est un mythe qui a surgi, a été raconté au fil du temps ou a été écrit pour expliquer les origines de divers phénomènes sociaux ou naturels. Par exemple, l’Énéide de Virgile est un mythe national écrit pour expliquer et glorifier les origines de l’Empire romain.En théologie, de nombreuses religions ont des mythes de création expliquant les origines du monde ou sa relation avec les croyants.On parle de conte étiologique lorsqu'une histoire a pour but de donner une explication imagée à un phénomène ou une situation dont on ne maîtrise pas l'origine scientifiquement. Exemple : Pourquoi les chiens n'aiment-ils pas les chats ? Ce type de littérature est très ancien et attesté dans la tradition orale. Les premiers récits écrits sont souvent basés sur ces traditions. Les Métamorphoses, d'Ovide (43 av. J.-C.) en sont un exemple déjà tardif et érudit, inspiré des Heteroeumena de Nicandre de Colophon (IIe siècle av. J.-C.). Dans Histoires comme ça (Just so Stories), Rudyard Kipling renoue de façon fantaisiste avec la tradition étiologique, qui sert de ressort à chacun de ses contes : pourquoi les éléphants ont-ils une trompe ? (L'Enfant d'éléphant) ; comment est née l'écriture ? (Comment naquit la première lettre ?).Sémiologie médicaleTableau cliniqueRessources relatives à la santé : (en + es) MedlinePlus (cs + sk) WikiSkripta Cours d'étiologie de l'Université catholique de Louvain Portail de la médecine"
médecine;"La fièvre est l'état d'un animal, à sang chaud (endotherme) ou à sang froid (ectotherme), dont la température interne est nettement supérieure (hyperthermie) à sa température ordinaire, de façon contrôlée.Chez les endothermes (essentiellement les mammifères et les oiseaux), ce phénomène physiologique semble être principalement une réponse hypothalamique stimulée par des substances pyrogènes principalement libérées par les macrophages et/ou lors des phénomènes inflammatoires.Chez l'humain, la fièvre accroît les défenses par plusieurs voies complémentaires : elle stimule l'immunité spécifique et non spécifique et la microbiostase (inhibition de la croissance) en diminuant le fer disponible pour les micro-organismes pathogènes afin de diminuer leur virulence. Le phénomène se déroule suivant trois phases :montée thermique ;plateau d’hyperthermie ;défervescence.La température corporelle normale moyenne des humains est de 37 °C (entre 36,5 et 37,5 °C selon les individus et le rythme nycthéméral). La fièvre est définie par une température rectale au repos supérieure ou égale à 38,0 °C. S'il n'existe pas de consensus concernant un seuil à partir duquel la fièvre elle-même serait dangereuse, certains auteurs estiment en se basant sur des données animales que le système nerveux central pourraît présenter des signes de souffrance à partir de 41,5 °C. Lorsque la fièvre est modérée, entre 37,7 et 37,9 °C, on l'appelle fébricule.Chez certains ectothermes, la fièvre s'obtient en se déplaçant dans des zones plus chaudes ; cette fièvre est qualifiée de comportementale.Il n'existe pas de définition précise universellement admise de la fièvre notamment du fait de difficultés concrètes de mesure en situation clinique (la température mesurée dépend du moment de la journée, de la proximité d'un repas, de caractéristiques environnementales). Cependant, le Brighton Collaboration Fever Working Group s'accorde à la définir en 2004 comme relevant d'une température corporelle supérieure ou égale à 38 °C, et ce quelles que soient les modalités de mesure, l'âge ou les conditions environnementales. L'OMS de son côté, considère comme fiévreuse une température axillaire supérieure ou égale à 37,5 °C,,.La température corporelle se mesure à l'aide d'un thermomètre médical. Suivant le placement de celui-ci, on parle de :température buccale : thermomètre placé dans la bouche (la méthode la plus courante dans les pays anglo-saxons, sauf pour les jeunes enfants) ;température rectale : bout du thermomètre placé dans le rectum via l'anus (la méthode la plus précise, traditionnellement conseillée pour les jeunes enfants) ;température axillaire : sous le bras. Cette mesure est moins précise que la mesure rectale. Selon les sources, la température axillaire est entre 0,5 °C, et 0,9 °C de moins que température rectale. Le CHU de Rouen ajoute systématiquement 0,9 °C à la mesure par thermomètre axillaire électronique ;température tympanique : mesure infrarouge de la température du tympan.La température buccale et la température axillaire étant moins élevées que la température rectale, prise comme référence, des corrections doivent être appliquées (+0,5 °C pour la buccale, +0,7 °C pour l'axillaire[réf. nécessaire]).L'état d'homéothermie est maintenu par une commande centrale exercée par la partie antérieure de l'hypothalamus (« thermostat hypothalamique »). L'hypothalamus reçoit des informations provenant des neurones associés aux thermorécepteurs périphériques, et aussi du sang circulant. En retour l'hypothalamus envoie des informations vers les neurones périphériques qui contrôlent les pertes de chaleur (vasodilatation périphérique et sudation) ou la production de chaleur (frissons musculaires).La fièvre est une hyperthermie qui dépend du contrôle hypothalamique, et qui se traduit par un trouble de régulation des mécanismes de perte ou de production de chaleur. L'augmentation de température par le thermostat résulte de l'effet de substances sanguines dites pyrogènes, exogènes ou endogènes.Les pyrogènes exogènes proviennent de micro-organismes infectants, comme l'endotoxine des bactéries gram-négatives, ou les toxines des bactéries gram-positives, soit par action directe sur le thermostat, soit par action indirecte en activant la production de pyrogènes endogènes par les cellules de l'hôte (leucocytes).Les leucocytes peuvent produire des pyrogènes endogènes capables d'induire un état fébrile. Il s'agit de protéines solubles de la famille des cytokines, parmi les plus importantes : l'interleukine 1, le TNF, les interférons... La plupart des cellules de l'organisme, soumises à des stress cellulaires, peuvent produire des pyrogènes. Ceci explique que tout état fébrile n'indique pas forcément une maladie infectieuse.Les cytokines agissent sur des récepteurs spécifiques présents sur toutes les cellules de l'organisme, comme les récepteurs Toll qui activent les mécanismes inflammatoires. Ces derniers se traduisent notamment par une extravasation des leucocytes et leur migration vers les tissus pour neutraliser l'agent agresseur. Lorsque l'agression est maitrisée par les réponses inflammatoires et immunitaires, le thermostat hypothalamique induit un retour de la température corporelle à la normale.La fièvre est donc considérée comme une réaction de défense de l'organisme contre une agression microbienne, physique ou chimique, qui s'est conservée tout au long de l'évolution des vertébrés. Son avantage se voit le plus clairement chez les poissons et les poikilothermes (animaux « à sang froid ») qui résistent mieux aux infections en augmentant la température de leur corps. Chez les mammifères, les avantages sont plus faibles (l'augmentation étant relativement plus limitée). De plus une fièvre très élevée (supérieure à 41 °C) peut léser le système nerveux central, d'où un probable système de régulation empêchant que la fièvre ne dépasse un certain plafond.La fièvre est un signe médical fréquent. Il appartient au médecin d'essayer de la rattacher à une étiologie (diagnostic) et d'évaluer sa gravité.Pour établir un diagnostic devant ce signe le médecin recherchera ses caractéristiques sémiologiques : homme / femme - âge - antécédents - ethnie - facteurs de risques - caractère aiguë / prolongée - lieu de séjour - fièvre isolée ou regroupement syndromique - incidence et prévalence locales et saisonnières, etc.En médecine générale, le plus souvent (voir carré de White) la fièvre conduira vers un diagnostic de pathologie bénigne.La cause de la fièvre peut être infectieuse (bactérie, virus ou parasite) ou non infectieuse (par exemple Vascularite, thrombose veineuse profonde ou effet secondaire). La fièvre due à une infection bactérienne est généralement plus élevée que celle due à un virus.Malgré leurs faibles prévalences dans les pays occidentaux, il est indispensable que le médecin sache écarter des atteintes particulièrement graves :au retour d'un voyage : paludisme ;si fièvre persistante : tuberculose ;associée à des douleurs articulaires : arthrite septique ;isolée, chez l'homme : prostatite ;en cas de souffle cardiaque : endocardite infectieuse ;si présence d'un purpura associé : purpura fulminans.Voici ce que constate l'ANSM sur la fièvre de l'enfant, par la plume d'Aude Chaboissier : « Les bénéfices attendus du traitement sont désormais plus centrés sur l'amélioration du confort de l'enfant que sur un abaissement systématique de la température, la fièvre ne représentant pas, par elle-même et sauf cas très particuliers, un danger. »Au cas par cas (par exemple : antécédents convulsifs, allergie, comorbidité, fiabilité de l'entourage, traitements associés, etc.) le médecin doit peser le rapport entre les bénéfices attendus et les risques encourus (hépatotoxicité, cardiotoxicité, syndrome de lyell, choc anaphylactique, cellulite faciale, etc.) avant de prescrire ou non des antipyrétiques.Attitudes pratiques pour la prise en charge d'une fièvre persistante supérieure à 38,5 °C :recourir à des mesures physiques simples : avant toute prise de médicament, il faut éviter de couvrir l'enfant, ne pas le maintenir dans une pièce surchauffée et lui donner à boire autant et aussi souvent que possible ; ceci, sans qu'il soit indispensable de lui donner un bain tiède comme cela était classiquement conseillé ;si prescrit on peut donner une seule classe de médicaments antipyrétiques en tenant compte des contre-indications et des précautions d'emploi qui lui sont propres ;brumiser de l'eau sur le visage pour rafraîchir si besoin.Ces recommandations concernent le confort de l'enfant et ne font ni baisser, ni n’élèvent la température car le thermostat hypothalamique mettra en marche les mécanismes de thermogenèse (si l'enfant est trop refroidi) et de thermolyse (si l'enfant est dans un environnement trop chauffé) afin de laisser le corps à la température prévue tant que les mécanismes immunitaires seront activés.Devant une fièvre de l'enfant, dans les pays occidentaux, il est fréquent de demander un avis diagnostic auprès d'un médecin généraliste pour affirmer le caractère bénin de l'épisode afin que l'enfant puisse être pris en charge par des adultes rassurés.Toutefois, un diagnostic médical est primordial si la fièvre de l'enfant présente des caractéristiques inhabituelles : nourrissons, fièvre plus élevée qu’habituellement, durée et évolution inhabituelle, comportements inhabituels (pleurs continus, fatigue, agitation, etc.), teint inhabituel, éruptions cutanées, signes d'accompagnements (vomissements, etc.), épidémie locale de pathologies potentiellement graves (méningite, etc.). La poussée dentaire n'est pas une cause de fièvre.Au-dessus de 40 °C, la température peut être un signe de maladie grave et peut être mal tolérée par l'organisme : Exemple : chez les personnes ayant une prédisposition, le risque de convulsion augmente.Chez le jeune enfant, cette fièvre peut entraîner des convulsions qui, si elles sont impressionnantes, sont en général bénignes ; il faut toutefois impérativement éviter que cette situation ne se prolonge, il faut donc abaisser lentement la température de l'ensemble du corps.On préconisait auparavant de donner systématiquement des bains d'eau dont la température est de 2 °C en dessous de la température du bébé, et la prescription médicale consistait souvent en une bithérapie aspirine-paracétamol ; la chute de la température était une priorité avec trois objectifs : empêcher le développement de l'hyperthermie maligne, éviter les convulsions fébriles et améliorer le confort de l'enfant.Cependant, aucune étude récente n'a mis en évidence l'effet des antipyrétiques pour la prévention des convulsions, et par ailleurs, seuls certains enfants (2 à 5 %) sont sujets aux convulsions.Une fièvre réelle (supérieure à 38 °C) chez un enfant doit toujours donner lieu à une consultation médicale, mais rarement aux urgences de l'hôpital sauf pour les nourrissons de moins de 3 mois.Il convient de prendre contact rapidement avec un médecin (le Samu en France) afin d'avoir des conseils et éventuellement une intervention médicalisée en présence de signes de gravité tels que : température supérieure à 40 °C ;perte de poids ;convulsions qui se répètent ou durent malgré le refroidissement ;taches sur la peau ;troubles de la conscience ;pleurs incessants.Avant d'aller consulter un médecin, il est nécessaire d'attendre 24 h pour un enfant entre 4 mois et moins de 2 ans puis 48 h pour un enfant de 2 ans et plus, sauf si les symptômes s'aggravent.La fièvre ayant un rôle dans la lutte contre l'infection, pour un enfant n'étant pas sujet aux convulsions et hors urgence (voir ci-dessus), l'administration d'antipyrétique n'est plus systématique, et n'est envisagée qu'à partir de 38,5 °C. On conseille alors plutôt le paracétamol en monothérapie,,.L'utilisation de l'ibuprofène chez l'enfant est controversée,. Il peut y avoir des effets secondaires rares mais graves chez l'enfant varicelleux.Par le passé, la fièvre a pu être délibérément provoquée dans un but de guérison. C'est ce que l'on a appelé « la pyrothérapie ». C'est le Dr Konteschweller Titus qui forgea le mot « pyrétothérapie » en 1918 rappelant notamment à cette fin l'usage du vaccin contre la typhoïde. Cette approche obtint une certaine reconnaissance avec la mise au point par Julius Wagner-Jauregg de la malariathérapie pour la guérison de la syphilis (cela lui valut le Nobel en 1927). D'autres procédés ont été utilisés,,,, que l'avènement des antibiotiques notamment ont relégué dans un oubli presque total. Dans les dernières années cependant – notamment dans le domaine de la lutte anticancéreuse – la réévaluation de la littérature à l'aune des connaissances contemporaines suscite un regain d'intérêt pour la - fever therapy, pyrétothérapie ainsi que pour la thermothérapie (élévation de la température par voie externe),.Tout comme chez les organismes endothermes, chez les ectothermes, la température de la fièvre augmente la capacité défensive de l'hôte en diminuant le taux de réplication des pathogènes et en augmentant l'efficacité du système immunitaire. En effet, la fièvre est une défense immunitaire ancienne avec des mécanismes physiologiques apparemment bien conservés au sein d'une large diversité de taxons d'invertébrés et de vertébrés. Pour ce faire, certains ectothermes modifient leurs comportements habituels assurant leur thermorégulation : ils se placent dans des endroits chauds afin d'élever leur température. Ce mécanisme, nommé fièvre comportementale a été identifié dans les années 1970 chez les iguanes du désert, les crapets arlequins et les têtards. Il concerne aussi les poissons et les insectes. Il permet aux insectes fébriles d'acquérir une survie et une fécondité supérieures aux non fébriles, mais l'atteinte et le maintien de la température élevée exigent des efforts coûteux pour l'organisme, parfois mortels.Dans le cas d'une infection fongique par Beauveria bassiana de la Mouche domestique, les hautes températures ont un effet négatif sur la croissance du champignon. Au petit matin, lorsque le champignon s'est développé à sa température optimale tout au long du cycle de la nuit, les immunosuppresseurs sont à des niveaux élevés et la réponse fébrile est la plus intense, pendant au maximum deux heures. À mesure que les facteurs immunitaires exogènes sont réduits ou éliminés de l'hémolymphe, la mouche se déplace progressivement vers des zones plus fraîches. Pendant la nuit, le champignon se rétablit, car la mouche ne peut pas exprimer de fièvre pour supprimer la croissance fongique. Et le cycle recommence le lendemain. La Mouche domestique provoque également des intensités de fièvre différentes, sélectionnant des températures plus élevées lorsqu'elle est infectée par une dose fongique plus élevée, montrant ainsi une capacité à gérer le bénéfice-risque de la fièvre.Fièvre chez l'enfant, Esculape.com« Fièvre chez l'enfant », sur ameli.frRecherches sur les fièvres, selon qu'elles dépendent des variations des saisons, et telles qu'on les a observées à Londres pendant les vingt années consécutives  (avec des observations de pratique sur la meilleure manière de les guérir. Suivies de L'histoire des constitutions épidémiques de Saint-Domingue, et de la description de la fièvre jaune). Tome premier. William Grant, traduit par Jean-Baptiste-René Pouppé-Desportes, édition de 1821Du typhus d'Amérique ou fièvre jaune, François-Victor Bally (1775-1866). Édition : Paris : Imprimerie de Smith. 1814.De la dengue : fièvre éruptive des pays chauds, et de sa distribution géographique: thèse pour le doctorat en médecine présentée et soutenue le 25 juin 1875, Albert  Morice (1848-1877). Portail de la médecine"
musique;"L'accordéon est un instrument de musique à vent de la famille des bois. Le nom d'accordéon regroupe une famille d'instruments à clavier, polyphonique, utilisant des anches libres excitées par un vent variable fourni par le soufflet actionné par le musicien. Ces différents instruments peuvent être de factures très différentes.Une personne qui joue de l'accordéon est un accordéoniste.Le Sheng, instrument de musique polyphonique religieux utilisé dans les orchestres de cour et de théâtre en Chine ancienne, est le plus ancien instrument à anche libre connu : il est constitué d'une chambre à vent sur laquelle sont fixés des tuyaux de bambou où vibre l'anche. Cet orgue à bouche est présent dès 2700 à 2500 av. J.C.. On le retrouve dans le reste de l'Asie sous d'autres noms : Sompoton sur l'île de Bornéo, Khène au Laos, Sho au Japon. Marin Mersenne cite entre 1636 et 1644, un Khên du Laos.En 1674, un Khène fait partie de l'inventaire de la collection du royaume du Danemark. Johann Wilde (en) aurait ramené un Sheng à la cour de Saint-Pétersbourg en 1740. Le jésuite et missionnaire Joseph-Marie Amiot fait parvenir en 1777 deux paires de sheng à Monseigneur Bertin à Paris.C'est durant la seconde moitié du XVIIIe siècle et le début du XIXe siècle que le procédé sonore de l'anche libre est l'objet de toutes les attentions des inventeurs. S'il est souvent avancé que le Sheng fut à l'origine de l'accordéon, le lien entre l'instrument asiatique et les instruments occidentaux n'est cependant pas évident, d'autant que la guimbarde, autre instrument à anche libre, existe en Europe depuis au moins l'époque gallo-romaine.En 1769 est organisé un concours à Saint-Pétersbourg, dont l'objet est l'invention d'un instrument qui imiterait la voix humaine. Le physicien danois Christian Gottlieb Kratzenstein (de) remporte le concours avec l'invention de sa « machine parlante ». Néanmoins, à la lecture de son travail publié à Bordeaux en français, on peut constater qu'il ne fait aucune allusion aux instruments asiatiques. Et que la construction de sa machine est exclusivement née de l'étude anatomique du larynx.C'est dans ce bain obscur entouré de mystères et contradictions, que les brevets d'invention autour des instruments à anche libre vont naître, s'interpénétrer, s'influencer, se doter parfois de manière douteuse de paternité, mais permettant peu à peu l'émergence d'une nouvelle espèce d'instruments.En 1810, on assiste à la naissance de l'« orgue expressif » de Gabriel-Joseph Grenié qui introduit le soufflet à pédalier, dont le système prendra plus tard le nom d'harmonium. Il réinvente l'anche libre, comme on peut le lire dans son mémoire de brevet.En 1818, l'Autrichien Anton Haeckl invente le Physharmonica, premier instrument à anches libres clavier et à soufflet manuel. Un brevet lui a été accordé le 8 avril 1821. Dans le journal Allgemeine musikalische Zeitung du 14 avril 1821, la publicité du physharmonica dit entre autres : « Le maître fait aussi des versions très petites qui reposent confortablement sur le bras gauche, et dont on joue de la main droite. » Cet élément est primordial pour l'avenir.En 1821, inspiré par la guimbarde, l'Allemand Christian Friedrich Ludwig Buschmann invente un instrument à anches métalliques : l'« aura ». Cet instrument, qui deviendra l'harmonica, inspirera des fabricants se copiant, améliorant, inventant tout une multitude d'instruments dérivés.Anton Reinlein obtient en 1824 à Vienne un brevet pour son harmonica « à la manière chinoise », Christian Messner ouvrira l'une des premières usines à Trossingen en 1827 puis en 1832 lance la fabrication de ses « mundharmonika ».En 1822, Buschmann monte un soufflet sur son « aura » qui devient « handaeoline », l'éoline à main.En 1827, Marie Candide Buffet (1797-1859) fabrique des « harmonicas métalliques à bouche ».En 1829, Cyrill Demian, facteur de piano et orgues à Vienne (Autriche), fabrique un instrument dans la veine de Buschmann et Haeckl, dont il veut déposer le brevet sous le nom d'« Aeolina »[réf. nécessaire]. Ce nom étant déjà pris par un modèle Buschmann et ce nouvel instrument étant, contrairement à ses prédécesseurs, voué à l'accompagnement et, en ce sens, n'émettant que des accords, Demian et ses fils dépose leur brevet le 6 mai 1829 sous le nom d'« Accordion » ; cet instrument est muni d'un soufflet manié par la main gauche, la main droite se réserve à un clavier dont chacune des 5 touches émet un accord, différent en tirant ou en poussant.Le 23 juin 1829, la même année que le brevet de Demian, Charles Wheatstone invente le « symphonium », rebaptisé « concertina », dont le brevet sera déposé le 8 février 1844[réf. nécessaire]. Ce modèle est unisonore.En France, en 1830, Marie Candide Buffet positionne un clavier mélodique en main droite à la place des accords[réf. nécessaire]. Demian invente, vers 1834, la combinaison d’un deuxième clavier pour les accords, et d’un premier pour la mélodie[réf. nécessaire].En 1834, Carl Friedrich Uhlig crée le « konzertina » allemand, bisonore, après avoir rencontré Demian et ayant désiré créer un instrument mélodique[réf. nécessaire]. C’est ce modèle qui inspirera Heinrich Band (de) la même année, en faisant évoluer la forme des claviers[réf. nécessaire].En 1841, Louis Léon Douce dépose un brevet pour son « accordéon harmonieux », instrument unisonore[réf. nécessaire].À partir de 1847 Carl Friedrich Zimmermann (de) développe le même type de concertinas que Band,. Les termes de « bandonion » puis « bandonéon » arriveront en 1854 en hommage au fabricant à Henrich Band.En 1852, Philippe-Joseph Bouton conçoit l’instrument avec un clavier piano à la main droite. En Autriche, le « Schrammelharmonika » sera le premier instrument avec le clavier main droite moderne qui va inspirer les Italiens. En Italie, en 1863, Paolo Soprani fonde la première industrie du « fisarmonica » (nom italien de l'accordéon) à Castelfidardo, ville considérée comme l'un des berceaux de l'accordéon moderne. Autre berceau, Stradella, dans la province de Pavie où Mariano Dallapé invente un nouvel instrument encore plus proche de l'accordéon moderne en 1871. Le terme « fisarmonica » est très important, car Soprani ne va pas fabriquer des accordéons, mais des « physharmonika ». Cette distinction n'est pas anodine car, en 1861, le Maître de chapelle de Loreto (à proximité de Castelfidardo) expose un instrument décrit comme « accordéon par la forme, mais véritable fisarmonica ». À l'époque, fisarmonica et accordéon sont deux instruments différents en Italie. C'est l'origine de l'industrie italienne.La première génération d'instruments encore usités apparaît à la fin du XIXe siècle. Jusqu'à aujourd'hui, les modèles n'ont cessé de se perfectionner, d'évoluer, de se spécialiser selon les styles, selon les coutumes, selon les traditions culturelles ayant accueilli l'une ou l'autre forme de l'instrument à anche libre et à soufflet manuel.Dans l'accordéon, deux anches sont montées sur une même plaquette, une de chaque côté de la plaquette. Une anche ne fonctionne que dans un seul sens, lorsque l'air la pousse vers la plaquette, donc une seule des deux anches fonctionnera pour un sens donné du soufflet. Une « peau musique » (en cuir, en vinyle ou en matériau composite souple) empêche la perte d'air par l'interstice entre l'anche qui ne parle pas et la plaquette (on dit de l'anche qui produit du son qu'elle « parle »).La vibration est due à un phénomène dit « de relaxation » : elle n'est donc pas sinusoïdale et comporte de nombreux harmoniques responsables d'une famille typique de timbres. Les harmoniques sont utilisés pour faciliter l'accord des basses fréquences (< 100 Hz).La fréquence de vibration est pratiquement indépendante de la puissance du souffle d'air, l'anche vibrante jouant d'ailleurs, à pleine puissance, le rôle de limiteur de débit. Cependant, lorsque des anches de fréquences extrêmement proches (différence inférieure à 1 Hz, tout au plus) sont alimentées en air par un système commun, il arrive que l'anche la moins stable en fréquence s'accorde à la fréquence de l'autre par effet de « couplage » ou de « pilotage », masquant leur « désaccord », voire interdisant un vibrato différentiel intentionnel de fréquence inférieure à 1 Hz.Dans l'accordéon, les anches donnant les sons les plus graves (< 50 Hz environ) ont une longueur de 5 à 10 centimètres et sont chargées, près de leur extrémité vibrante, par une masse de laiton (généralement — ou d'étain sur les anches anciennes ou modifiées par un accordeur). Les anches produisant les sons les plus aigus (plus de 6 kHz dans l'aigu du piccolo) ont une longueur inférieure à 6 millimètres.En raison de la très courte longueur d'onde des sons les plus aigus produits (de l'ordre de quelques cm), on constate souvent des phénomènes d'ondes stationnaires dus aux « obstacles » à leur propagation (cases exiguës du sommier qui supporte les plaquettes, soupapes…) qui peuvent affaiblir, voire neutraliser totalement, le son produit. Des solutions empiriques de facture permettent d'éliminer ce phénomène.L'accord se fait en jouant sur les paramètres raideur et masse : on augmente la fréquence en diminuant la masse par enlèvement de matière (limage d'épaisseur) à l'extrémité libre de l'anche (ou de sa charge rapportée). On diminue la fréquence en diminuant l'épaisseur (raideur) de l'anche (enlèvement par grattage : (grattoir) près de sa partie fixe, flexible (le « ressort »).Une anche vibrante de grandes dimensions et de fréquence infrasonique, destinée à produire un vibrato en amplitude, a été utilisée dans l'accordéon de concert Cavagnolo : cette anche est placée dans une paroi séparatrice (équivalente à une « plaque ») disposée entre le soufflet et la « caisse du chant ». Ce système générateur de vibrato semble être resté sans suite en raison, sans doute, de sa fréquence invariable, de son effet trop systématique (un accord, grave ou aigu, vibre « en bloc ») et de sa limitation du débit d'air (contradictoire avec l'expressivité naturelle de l'instrument), en dépit de la présence d'un moyen de neutralisation : une très large soupape.Le musicien ouvre et referme le soufflet central, positionné entre les deux parties droite et gauche de l'instrument, munie chacune d'un clavier : une partie droite, qui reste statique, et une partie gauche, qui s'écarte et se rapproche de la partie droite à chaque va-et-vient du soufflet (on parle de « tiré » ou de « poussé » du soufflet). En même temps, l'instrumentiste appuie sur les touches des claviers de l'instrument pour décider des notes à produire. L'air du soufflet passe ainsi dans le mécanisme, et actionne une ou plusieurs anches accordées à la lime et au grattoir. L'anche au repos possède une courbure qui la porte « au vent » : le réglage de cette courbure a pour but de permettre et faciliter l'attaque, à toutes les puissances.Véritable homme-orchestre, l'accordéoniste peut exécuter le rythme aussi bien que la mélodie et l'harmonie, ce qui lui a valu une place importante dans les bals populaires français.Cet instrument aux accords tout faits et à la sonorité « désaccordée » ne suscita pas l'adhésion de tous d'où, dès les années trente, l'invention des basses chromatiques (clavier mélodique de main gauche similaire à celui de la main droite, remplaçant grâce à un convertisseur le clavier traditionnel basse-accord) et la présence possible de registres permettant de changer la sonorité de l'instrument en appuyant sur un bouton.Il existe plusieurs sortes d'accordéons qui se différencient d'une part, par l'organisation des notes sur les claviers et d'autre part, par la manière de produire des notes en actionnant le soufflet.L'accordéon chromatique possède les 12 demi-tons de la gamme chromatique. Une touche enfoncée produira la même note que l'on tire ou que l'on pousse le soufflet. Certains ont des boutons, d'autres des touches de piano. Suivant les modèles, la tessiture peut dépasser 4 ou 5 octaves.Les accordéons diatoniques peuvent jouer des gammes diatoniques. Une touche enfoncée ne produira pas la même note selon que le musicien tire ou pousse le soufflet. On dit qu'il est bi-sonore.Ces deux descriptions correspondent aux deux familles d'accordéons les plus répandues. De nombreuses variantes ont été réalisées (chromatique bi-sonore, diatonique uni-sonore, systèmes mixtes).L'accordéon est utilisé en musique populaire, musique traditionnelle, musique folklorique, dans les musiques actuelles, ainsi qu'en musique classique et contemporaine. « On peut tout jouer avec l'accordéon » déclare Yvette Horner.La plus ancienne pièce de concert est Thème varié très brillant pour accordéon, écrit en 1836 par Mlle Louise Reisner de Paris. Le compositeur russe Piotr Ilitch Tchaïkovski inclut (de façon optionnelle) quatre accordéons diatoniques dans sa Suite pour orchestre no 2 en Do Majeur, op. 53 (1883), simplement pour ajouter une petite couleur au troisième mouvement (Scherzo burlesque). Le compositeur italien Umberto Giordano inclut l'accordéon diatonique dans son opéra Fedora (1898). L'accordéoniste apparait sur la scène, avec également un joueur de piccolo et un joueur de triangle, trois fois dans le troisième acte (qui se déroule en Suisse), pour accompagner une courte et simple chanson qui est chantée par un petit savoyard.En 1915, le compositeur américain Charles Ives inclut un chœur d'accordéons diatoniques (ou de concertinas) avec également, entre autres, deux pianos, un célesta, une harpe, un orgue, un zither et un thérémine optionnel dans son Orchestral Set no 2. La partie d'accordéon, écrite pour la main droite seulement, consiste en dix-huit mesures à la fin de l'œuvre. Le premier compositeur à avoir écrit spécifiquement pour l'accordéon chromatique est Paul Hindemith. En 1921, il inclut l'harmonium dans sa Kammermusik No. 1, une œuvre de musique de chambre en quatre mouvements pour douze musiciens, mais plus tard il récrit la partie d'harmonium pour l'accordéon. D'autres compositeurs allemands ont aussi écrits pour l'accordéon.En 1922, Alban Berg inclut un accordéon dans son opéra Wozzeck. L'instrument, marqué Ziehharmonika bzw. Akkordeon dans la partition, apparaît seulement durant la scène de la taverne, avec un ensemble sur scène (Bühnenmusik) consistant en deux violons, une clarinette, une guitare et un bombardon en fa (ou tuba basse).D'autres compositeurs du XXe siècle ont écrit pour l'accordéon comme Kurt Weill dans L'Opéra de Quat'sous (1928), Sergueï Prokofiev et sa Cantate pour le 20e anniversaire de la révolution d'octobre, op. 74 (1936), Dmitri Chostakovitch l'utilise dans la Jazz Suite No. 2 (1938), ainsi que Jean Françaix dans Apocalypse According to St. John (1939) ou Darius Milhaud dans Prélude et Postlude pour Lidoire (1946), ainsi que John Serry Sr. dans American Rhapsody (1955) et Concerto pour accordéon (1964) ,,. L'accordéon est présent aujourd'hui dans le répertoire de musique contemporaine. Principalement en musique de chambre, des compositeurs comme Henk Badings (Sonate pour accordéon seul, 1981), Luciano Berio (Sequenza XIII pour accordéon seul, 1995) ou Jean Françaix, Concerto pour accordéon 1997) ont écrit pour l'instrument.L'accordéon et ses variantes sont présents dans de très nombreuses musiques traditionnelles ou musique folkloriques. L'Écosse, l'Irlande ou la Grande-Bretagne furent ouverts à intégrer l'accordéon à leur folklore adaptant et composant dans leurs styles respectifs, soit des reels, des jigues ainsi que des valses. En Amérique, on retrouve traditionnellement l'accordéon dans le folklore québécois composé principalement de reels et de riggodons ainsi que dans la musique cadienne, principalement des ballades. L'Autriche, la Suisse ou la Bavière sont parfois représentées par des valses, des marches) ou des polkas. Les ensembles de musiques tsiganes et klezmers ont aussi des formes d'accordéons spécifiques comme le Bayan que l'on retrouve dans la musique traditionnelle en Russie.En Amérique latine, de nombreux genres musicaux utilisent différentes sortes d'accordéons comme le norteña au nord-est du Mexique, le chamamé en Argentine, la cumbia et le vallenato en Colombie ou instrument musique brésilienne, le baião au nord-est.L'accordéon est présent dans la musique de jazz. Cela a commencé avec la collaboration de Django Reinhardt et Jo Privat à l'époque du jazz swing. L'accordéoniste Marcel Azzola a aussi réalisé des arrangements pour accordéon des plus grands standards de jazz comme All the Things You Are. Plus récemment, des accordéonistes se sont éloignés du musette traditionnel pour s'intéresser au jazz, comme les artistes Richard Galliano, ou Vincent Peirani.Avant la Seconde Guerre mondiale, des musiciens comme Gus Viseur ou Tony Murena font déjà le lien entre jazz et musette. Après la guerre, l'accordéon est utilisé par des auteurs-compositeurs-interprètes comme Léo Ferré ou Jacques Brel, et des virtuoses comme Aimable, qui promènera son instrument en tournées mondiales[réf. nécessaire]. Mais l'instrumentarium du jazz moderne (be-bop, free jazz), puis du rock dans les années 1960, tend à le ringardiser[réf. nécessaire].Bien que créé en Europe, cet instrument se répand au Moyen-Orient et en Afrique du Nord, et est adopté par la musique populaire, puis s'introduit dans de nouvelles tendances musicales. Le chanteur de raï Cheb Khaled explique ainsi : « Mon instrument, c'est l'accordéon. Je l'ai appris à l'école de la rue. De naissance […] Dans le temps, quand les gens fêtaient les mariages, il y avait le violon, l'accordéon, la darbouka, mais pas de trucs électroniques. Et l'accordéon donnait un son typique, oriental. C'est original, c'est un beau son. »L’accordéon chromatique, avec des touches piano, s’est ancré à partir des années 1960 dans divers types de chanson populaire en Algérie : la chanson oranaise, le chaâbi, et le raï. Il participe à une transition entre les instruments traditionnels et les claviers électroniques. On le trouve également dans la musique populaire égyptienne, par exemple dans le style Baladi (en), avec des techniques et des modes d’interprétation spécifiques, notamment dans le jeu du bourdon ou le jeu en contretemps rythmique, qui se rapprochent des arrangements instrumentaux pratiqués par les joueurs d’instruments plus traditionnels tels que le mizmār ou zurna,. Et en Égypte, une artiste des années 2010 comme Youssra el Hawary s'en empare,. De façon générale, l'accordéon n'est pas réservé à une musique populaire et festive, mais il sait prendre place dans une musique dite savante. L'oudiste Anouar Brahem s'est ainsi associé pour des spectacles et albums, à l'accordéoniste de jazz franco-italien Richard Galliano dans les années 1990.Le « piano du pauvre », ou « piano à bretelles », est entré dans la littérature française dès 1833, grâce au vicomte François-René de Chateaubriand dans Mémoires d'outre-tombe. L'accordéon en France est lié à l'histoire du bal musette. Il reste cependant pointé du doigt par certains : Octave Mirbeau le destine « aux polkas pour les bals ».L'histoire de l'accordéon est liée également à celle du swing manouche, avec dès les débuts de ce mouvement des collaborations répétées entre Jo Privat et Django Reinhardt, des compositions très en avance sur leur temps de Gus Viseur et aujourd'hui de nombreux artistes swing tels que Ludovic Beier … Dans les années 1950, l'accordéon devient l'instrument des bals populaires, Yvette Horner et André Verchuren étant alors les deux figures emblématiques de cet instrument.Dans les années 1970, l'accordéon redevient populaire grâce à l'attrait des musiques traditionnelles et folkloriques qui l'utilisent (musique bretonne, slave, musique cadienne…) ; par l'utilisation par des chanteurs français comme Renaud qui le remettent au goût du jour ; par l'apparition d'accordéonistes majeurs, se détournant du musette, comme Marc Perrone ou Richard Galliano ; et par son utilisation par des groupes de la scène alternative comme la Mano Negra ou Les Négresses vertes.L'accordéon a maintenant acquis ses lettres de noblesse en musique classique (même si cela reste méconnu du grand public). Il est enseigné dans les conservatoires de musique depuis les années 1970. L'accordéon est également présent dans la création contemporaine d'avant-garde. On peut citer Pascal Contet, qui contribue activement à développer le répertoire contemporain avec des compositeurs comme Bernard Cavanna, Vinko Globokar, Jacques Rebotier, Jean-Pierre Drouet, Bruno Giner, Marc Monnet, Sofia Goubaïdoulina,Jean Françaix, Poul Rovsing Olsen, Gérard Pesson… Citons également quelques membres de la jeune génération : Fanny Vicens, Vincent Lhermet, Jean-Etienne Sotty. Côté traditionnel, l'accordéon fait partie des instruments de la musique bretonne qui revient à partir des années 1970.Aujourd'hui, l'accordéon est largement utilisé aussi bien par des artistes de variétés (Patrick Bruel, Yann Tiersen…) que par des groupes « alternatifs » (Les Ogres de Barback, Les Têtes Raides, Red Cardell, Les Hurlements d'Léo, La Rue Ketanou, N&SK, Sagapool), les groupes de rap (Java, le Ministère des affaires populaires), le duo féminin Délinquante qui utilise cet instrument de façon notable, des groupes régionaux qui arrangent ces morceaux et/ou en composent de nouveaux tel qu'Accordé à vent, groupe du Pas-de-Calais, le duo Kof a Kof avec Roland Becker au saxophone et Régis Huiban à l'accordéon chromatique, avec des musiciens de jazz tels que Richard Galliano, Marcel Azzola, Marc Berthoumieux, Jacques Bolognesi ou Marcel Loeffler, Lionel Suarez, René Sopa.En 2005, Serge Lama a effectué une tournée avec un seul musicien, l'accordéoniste Sergio Tomassi jouant sur un accordéon numérique. Claude Parle développe l'accordéon dans le domaine des musiques improvisées et en relation avec la musique contemporaine, la danse, notamment la danse Buto (Masaki Iwana, Toru Iwashita, Atsushi Takenouchi (en)) ou le jazz contemporain (depuis les années 1970).Depuis avril 1990, San Francisco a pour instrument officiel l'accordéon.Lyon avec la firme Cavagnolo, et Tulle avec la fabrique Maugein sont des villes importantes pour l'accordéon chromatique français. Historiquement, la ville de Brive, avec l'usine Dedenis, fut très longtemps le siège de la première industrie de l'accordéon en France. Outre ces petites fabriques, plusieurs artisans fabriquent en France des instruments sur mesure ou commandés à l'unité, principalement des accordéons diatoniques, mais aussi des accordéons chromatiques.La ville de Castelfidardo est « la capitale mondiale de l’accordéon ». Une vingtaine d'entreprises familiales y sont regroupées dont Soprani, Pigini et Bugari ; 90 % des pièces détachées européennes y sont fabriquées.La manufacture d'accordéons la plus ancienne du monde, serait la société Harmona Akkordeon GmbH à Klingenthal en Saxe.Hohner, fabricant allemand d'harmonicas et d'accordéons situé à Trossingen, est présent en Europe et aux États-Unis.Les premiers accordéons des États-Unis ont été fabriqués à San Francisco.Il existe en France de nombreux festivals intégrant l'accordéon, ainsi qu'un certain nombre de festivals dédiés à l'instrument (qui peuvent être généralistes ou centrés sur un style de musique précis). Par exemple :L'Accordéon, moi j'aime !, en Belgique à TournaiQuelques exemples de surnoms de cet instrument (en France) : piano à bretelles, piano du pauvre, boîte à frisson, orgue portatif, branle-poumons, boîte à chagrin, soufflet à punaises, dépliant, calculette prétentieuse, boîte à soufflets et boîte du diable (boest an diaoul, en Basse-Bretagne et boueze en Haute-Bretagne). Ouvrages Pierre Monichon, L'Accordéon, éditions Van de Velde et Payot, 1985Pierre Monichon et Alexandre Juan, L’Accordéon, Éditions Cyrill Demian, 2012, 170 p.Joseph Amiot, Mémoire sur la musique des chinois, tant anciens que modernes, Paris, Nyon l'aîné, 1779 (lire en ligne) Articles Émile Leipp, Pierre Monichon, Alain Abbott et Étienne Lorin, « L'Accordéon — De l'accordion à l'accordéon de concert : éléments d'anatomie, de physiologie et d'acoustique », Bulletin du Groupe d'Acoustique Musicale (GAM), no 59,‎ 1972.Yves Defrance, « Traditions populaires et industrialisation : Le cas de l'accordéon », Ethnologie française, Presses universitaires de France, vol. 14, no 3,‎ 1984, p. 223-236 (résumé/1re page) Autres Thierry Benetoux, Comprendre et réparer votre accordéon, 2001, 243 p. (ISBN 978-2951718401)Interviews d'accordéonistes : Le souffle de l'accordéonCompositions pour accordéonHistorique des évolutions technologiques de l'accordéonTypes d'accordéonsAccordéon diatoniqueAccordéon chromatique à clavier « boutons » ou « piano »Accordéon à basses chromatiquesHarmonéon (aussi connu sous le nom d'accordéon de concert)Accordéon à touches pianoInstruments à anches libres, proches cousins de l'accordéon :BandonéonConcertinaAccordinaHarmonium indienRessources relatives à la musique : MusicBrainz (en) Musical Instruments Museums Online  Portail des musiques du monde   Portail de la musique   Portail de la musique classique"
musique;"Un chanteur ou une chanteuse est une personne qui utilise sa voix pour produire une succession de sons formant une mélodie. Ces sons peuvent être de simples phonèmes ou des textes littéraires. Le chant est présent dans tous les genres musicaux, qu'ils soient classiques ou modernes.La musique vocale couvre tous les genres musicaux, tant dans le domaine religieux (motet, oratorio, passion, etc. et plus généralement les différents rites et cérémonies) que profane (opéra, mélodie, chanson, etc.).Le chanteur peut utiliser comme support :un seul phonème sous forme de vocalises se rapprochant de l'art instrumental, comme dans les airs de la Reine de la nuit de La Flûte enchantée de Mozart ou la Bachianas brasileiras n° 5 de Villa-Lobos ;une série d'onomatopées, particulièrement dans le scat, popularisé par Louis Armstrong ou Ella Fitzgerald, mais aussi dans le rap du mouvement hip-hop;une comptine, un poème, un texte formant une chanson, un cantique, un hymne, une ode, un lied, une mélodie, etc. ;un livret spécialement créé ou inspiré d'une œuvre littéraire ou religieuse comme dans le motet, la cantate, l'oratorio, l'opéra, l'opérette, la comédie musicale, etc.Les thèmes et styles abordé peuvent être très divers, de la berceuse à l'épopée, en passant par la romance, couvrant tout l'éventail des sentiments humains. Ils peuvent avoir également une dimension politique ou sociale. On emploie parfois alors le terme de « chanteurs engagés » ou, dans le cas de l'humour et de la satire, de « chansonniers ».Si le timbre de la voix est relativement inné car en partie lié à la morphologie, un chanteur peut améliorer sa technique vocale en prenant des cours de chant.Dans le chant lyrique et le monde opératique, les chanteurs sont classés en plusieurs catégories en fonction du type et de la tessiture de leur voix :pour les femmes (du plus aigu au plus grave) :sopranomezzo-sopranocontralto (ou alto)pour les hommes (du plus aigu au plus grave) :sopranistecontreténorhaute-contreténorbarytonbasseCes classifications disposent de sous-classifications en fonction de la « couleur » du timbre, de l'« emploi » théâtral ou de l'agilité.Les castrats, hommes ayant subi à leur adolescence l'ablation des testicules afin d'empêcher la mue de leur voix selon une pratique en cours jusqu'à la fin du XIXe siècle, couvraient la tessiture actuelle des contre-ténors sopranistes et altistes. Mais l'absence de mue des castrats, par le moindre développement de sonorités graves et par la relative facilité à accéder au registre de tête qu'elle impliquait, apparentait leur voix à celles des femmes : les castrats conservaient ainsi tout au long de leur vie une voix « adolescente ».Le terme générique pour les chanteurs (hommes et femmes) spécialisés dans le genre lyrique (opéra, opérette) est « artiste lyrique ». Cette dernière notion fait appel également à des talents d'acteur. Les chanteuses solistes classiques sont souvent appelées « cantatrice », terme d'origine italienne . Certaines cantatrices célèbres  comme Maria Callas sont également appelées « divas » (litt. « déesses »).Note : Ces classifications n'ont en revanche pas de sens dans les genres musicaux qui n'utilisent pas la technique classique.Une formation à la musique spécialité chant se fait dès l’âge de 6 ans.Grâce au solfège et à la formation musicale, le chanteur peut accéder plus rapidement à des répertoires de style varié, transcrits sous forme de partitions.Il peut se produire en soliste ou en groupe (duo, trio, quatuor, etc.) jusqu'au chœur ou chorale pouvant réunir plusieurs centaines de personnes. On emploie alors le terme de « choriste ».Les chanteurs peuvent s'exprimer a cappella (sans accompagnement instrumental) ou être soutenu par l'accompagnement d'un instrument (piano, guitare, etc.), de plusieurs (quatuor à cordes, etc.), voire de tout un orchestre.Dans la tradition de ménestrels du Moyen Âge, le chanteur des rues se produit de façon itinérante a cappella ou s'accompagnant d'un instrument, le plus populaire au XIXe siècle étant l'orgue de Barbarie.Les droits des interprètes relèvent des droits voisins du droit d'auteur en France.Bernard Epin et Max Rongier, Profession chanteur, Paris, Éditions Farandole, 1977  (ISBN 2-7047-0031-1)ChansonChantChanteur de chanson (au Japon)Chœur (musique)Corde vocaleCrooner (variété américaine)Musique vocaleTechnique vocaleVoix (instrument)Voix (musique classique) Portail du travail et des métiers   Portail de la musique"
musique;"Dans la musique occidentale, et plus précisément, la musique classique, un contre-ténor (ou contreténor) est le type de voix masculine utilisant principalement sa voix de fausset (ou voix de tête), et dont la tessiture peut correspondre à celle d'un soprano (on parle alors de sopraniste), à celle d'un alto (altiste) ou à celle d'un contralto (contraltiste).Le contre-ténor, appelé aussi falsettiste, est à différencier de la haute-contre qui est un ténor utilisant sa voix de tête pour les aigus ou sur-aigus.Le contre-ténor a connu ses heures de gloire au cours de la Renaissance et pendant la période baroque, notamment en Espagne, en Angleterre ou en Allemagne, où ils étaient utilisés dans la musique sacrée. En Italie on n'a que très peu fait usage des contre-ténors dans l'opéra et, dans la musique sacrée également, ils furent complètement remplacés par les castrats au moins depuis la fin du XVIIe siècle. En France, c'est le règne de la haute-contre. À partir de la période classique, la technique vocale falsettiste des contre-ténors n'a pratiquement plus été utilisée à l'exception des chœurs des cathédrales anglicanes et du genre musical profane anglais du glee. On doit à Jean-Jacques Rousseau la disparition de ces voix en France si l'on se réfère à son dictionnaire de la musique.[réf. nécessaire]Ce n'est qu'au milieu du XXe siècle que les contre-ténors ont été remis à l'honneur, à l'occasion de la redécouverte du répertoire de la « musique ancienne » (c'est-à-dire la musique antérieure à la période classique). On leur fait alors chanter les rôles d'altos masculins dans les cantates de Bach, puis par extension, les rôles de castrats de l'opéra séria.Il existe, par ailleurs, un répertoire plus contemporain pour contre-ténors, notamment dans Le Songe d'une nuit d'été (1960) et Death in Venice (1973) de Benjamin Britten, Le Grand Macabre (1978) de György Ligeti, Akhnaten (1983) de Philip Glass ou The Tempest (2004) de Peter Tahourdin ; Péter Eötvös dans son opéra Trois Sœurs (1998, d'après Les Trois Sœurs de Tchekhov) confie les rôles des trois sœurs ainsi que de leur belle-sœur Natacha à quatre contreténors (deux contraltistes, deux sopranistes).Dans la polyphonie médiévale (et notamment dans le motet), on appelait contreteneur (lat. contratenor) la ou les voix disposées contre la teneur (tenor).Lorsque l'ambitus de ces voix rajoutées cessa de se confondre avec celui du ténor, on les distingua par les termes de :« contratenor bassus » (« contre la teneur, en bas »), vite abrégé en bassus (mais donnant aussi  basse-contre) ;« contratenor altus » (« contre la teneur, en haut »), abrégé ou traduit en contratenor, contra, altus (it. alto), contralto et haute-contre.La plupart de ces termes ont pris depuis des sens spécifiques.L'acception du terme de contreténor pour désigner une voix très aiguë provient plutôt de l'anglais countertenor. À la Renaissance, en France, la partie de contreténor désignait une ligne de chant qui sonnait contre celle de la ligne de ténor. Elle avait alors une tessiture assez comparable à la ligne de ténor. Peu à peu la ligne de contreténor s'est scindée en deux lignes de tessitures distinctes : la ligne de contreténor haute et la ligne de contreténor basse, qui ont donné les lignes de contralto et de basse.En français, la voix de contreténor peut être aussi appelée alto masculin ou falsettiste alto. Mais l’usage critique et musicologique actuel tend à faire de contreténor et de falsettiste des termes synonymes, si bien que la tessiture du contreténor peut s’étendre du registre de contralto à celui de soprano voire de soprano léger (comme chez Francesco Divito, Michael Maniaci ou André Vásáry).O Solitude de Henry PurcellMusic for a While de Henry PurcellA Chlorys de Reynaldo HahnOmbra mai fu de Georg Friedrich HaendelLascia ch'io pianga de Georg Friedrich HaendelChiamo il mio ben così de Christoph Willibald GluckChe farò senza Euridice de Christoph Willibald GluckNon so più cosa son, cosa faccio de Wolfgang Amadeus MozartVoi che sapete de Wolfgang Amadeus MozartVedrò con mio diletto de Antonio VivaldiArtaserse, un opéra par Leonardo Vinci, qui comporte un grand nombre de rôles pour contreténorsPour une liste plus exhaustive, voir la catégorie:contreténor, artistes disposant d'un article sur Wikipédia.MusiqueMusique classiqueVoix (instrument)Voix (musique classique)ChantSopranisteLe site des contreténors Portail de la musique classique   Portail de l’opéra"
musique;"Terme générique, une flûte (ou flute) est un instrument de musique à vent dont le son est créé par l'oscillation d'un jet d’air autour d'un biseau droit, en encoche ou en anneau.Ce souffle peut être dirigé librement par l'instrumentiste dans le cas des flûtes traversières, des instruments de type quena ou encore des flûtes de Pan, ou canalisé par un conduit en étant émis par le musicien lui-même dans le cas des différents types flûtes à bec ou en étant créé par une soufflerie mécanique dans le cas du jeu d'orgue.Les flûtes sont le plus souvent de forme tubulaire mais parfois globulaire, en graminée, en bois, en os ou en corne, mais aussi en pierre, en terre cuite, en plastique, en métal (or, argent…), en ivoire et même en cristal, la flûte peut être formée d'un ou de plusieurs tuyaux, avec ou sans trous, ou posséder une coulisse.Dès la Préhistoire, elle se retrouve partout dans le monde sous toutes sortes de formes. En septembre 2008, plusieurs morceaux d'une flûte datant du Paléolithique supérieur (environ 35 000 ans) ont été découverts dans la grotte d'Hohle Fels au sud-ouest de l'Allemagne, dans le Jura souabe. Cette flûte avait été fabriquée dans un radius de vautour fauve et témoigne du fait que les tout premiers Homo sapiens jouaient déjà de la musique.La flûte de pan était utilisée en Grèce dès le VIIe siècle av. J.-C. Le tin whistle est apparu au XIIe siècle, la flûte à bec au XIVe siècle. Certaines, à l'époque baroque, se virent ajouter un système de clés permettant d'obstruer les trous. Cette invention, dont il est impossible de tracer l'origine, fut notamment développée par Theobald Boehm au XIXe siècle.      Instruments de type  « flûte à bec » Le flageoletla flûte à bec et ses variantes historiques : le pipeau, le flageolet, le chalumeau, etc.le tin whistle, flûte droite en métal d'origine irlandaisela flûte harmonique (même si toutes ne sont pas à conduit)le flaviol, le galoubet, le txistule pinquillo andin (Bolivie et Pérou)la tarka ou l'anata andines (Bolivie, Pérou, Chili)le mohoceño andinle suling (flûte indonésienne)la sodina (flûte malgache)la fujara slovaquele salamouri géorgien Flûtes globulaires L'ocarinale sifflet Flûtes nasales La flûte nasale droitele sifflet à nez Flûtes à coulisse La jazzoflûte ou flûte à coulissela flûte Scoatariu Flûtes à embout coulissant Le glissando headjoint de Robert Dick (en) (pour flûte traversière).Le vibrabek de Jean-Pierre Poulin (pour tin whistle). Flûtes traversières L'Irish flute, flûte traversière en boisle fifrela flûte traversière classique, et ses variantes :flûte en sol (ou flûte alto) ;flûte basse ;flûte octobasse.le koudi chinoisle piccolola flûte traversière baroque (parfois appelée traverso de l'italien flauto traverso)le bansurî, flûte indiennele palahuito, flûte traversière andinele dízi (flûte traversière chinoise), incluant le bāngdí (piccolo) et le qudi (flûte),le daegeum, flûte traversière coréenne proche par son organologie du dizi chinois, utilisée dans la musique a'ak, équivalent coréen du gagaku japonaisles flûtes traversières japonaises (nom générique : fue ou yokobue) : ryūteki (flûte du gagaku), nohkan (flûte du théâtre nô), (flûte du gagaku), kagurabue (flûte du gagaku), dengakubue (utilisée dans les cérémonies liées au riz : dengaku), shinobue, misatobue…la flûte peule (appelée aussi Tambin) Flûtes à encoche La quena, flûte de roseau andine (principalement dans les pays andins), et ses variantes :le quenacho, modèle tenor, plus grand ;la quenilla, modèle soprano, plus petite.Le xiao chinoisle shakuhachi, flûte de bambou japonaisele danso coréen Flûtes obliques le ney (ou nay), flûte orientale, gasba (flûte algérienne et tunisienne), qawala égyptienle kaval turcle kaval bulgarele blul arménien Flûtes globulaires le xun chinois, Flûtes de Pan Le siku des Indiens Aymaras des Andes (Bolivie, Pérou et Argentine), également appelé antara en langue quechua et zampoña en espagnol, avec ses modèles de différentes tailles : toyo, chili, mala et sanka, de la plus grande à la plus petitele rondador de l'Équateurle naï roumainle paixiao chinoisLa flûte, outil de tissage dans la manufacture de basse lice de BeauvaisLa flûte à altérateurs : des cylindres amovibles bémolisent les notes en diminuant le diamètre des trous de jeu.l'aulos, des Grecs anciens : cet instrument à anche n'est pas une flûte, mais est cependant presque toujours appelé « flûte double » dans la littérature.                                                             Peu prisée pendant les quarante premières années de l'histoire du jazz en raison d'un volume sonore modeste vite étouffé par les sections de cuivres et d'autre part en concurrence directe avec la clarinette, ce n'est qu'à partir des années 1950 qu'elle éveille l'intérêt des jazzmen.Des musiciens comme James Moody, Gigi Gryce, Frank Wess, Eric Dolphy, Herbie Mann, des chefs d'orchestre comme Count Basie, Quincy Jones et Gil Evans ont su l'imposer comme un instrument de jazz à part entière. Roland Kirk élargira les possibilités expressives de l'instrument et nombre de musiciens l'adoptent dès lors comme instrument principal alors qu'au début elle n'était que le bonus des saxophonistes;John Coltrane ne s'y sera essayé qu'une seule fois dans To be. Longtemps utilisée par la musique classique pour son caractère pastoral et poétique la flûte jazz revendique sa place à part entière dans l'espace musical de la modernité.Ian Anderson, du groupe de rock progressif Jethro Tull, a utilisé la flûte dans ses compositions et sur scène, influencé par la technique de Roland Kirk, en pratiquant l'over-blowing (technique consistant à forcer le souffle pour obtenir une note plus haute sans la former par le doigté), mais aussi en chantant en superposition du son de la flûte.D'autres musiciens de rock progressif comme Peter Gabriel, Andrew Latimer ou Ray Thomas ont utilisé la flûte dans les compositions de leurs groupes respectifs (Genesis, Camel, The Moody Blues). Par ailleurs, le groupe de folk metal Ithilien allie des instruments traditionnels, tels que la flûte, avec une touche de metal moderne.D. Buisson, « Les Flûtes Paléolithiques d’Isturitz (Pyrénées-Atlantiques) », Bulletin de La Société Préhistorique Française,‎ 1990, p. 420–433. (lire en ligne).T. Clodoré et A-S. Leclerc (dir), Préhistoire de la musique, catalogue de l'exposition de Préhistoire de Nemours, éditions du musée de Préhistoire de Nemours, 2002.T. Clodoré-Tissot et P. Kersalé, Instruments et musiques de la Préhistoire, éditions Lugdivine, 2010, no 9.Flûte : jeu d'orgueFlûte longitudinale Portail des musiques du monde   Portail de la musique"
musique;"D'après l'encyclopédie Larousse, le genre musical est un « ensemble de formes de même caractère, réunies par leur destination (par exemple la musique de chambre) ou par leur fonction (par exemple la musique sacrée) ». Le genre musical est un concept sans limites précises, il est compliqué d'établir une liste exhaustive des genres. La dénomination d’un genre peut venir d’une expression qui a marqué une scène musicale (Krautrock), de techniques ou sources sonores utilisées par le genre musical populaire (techno, synthpop), de son origine géographique (Miami bass, UK garage), ou de l’intention que porte le style (rock psychédélique). Un genre musical peut être vocal ou instrumental.Selon le genre envisagé, diverses caractéristiques matérielles ou humaines peuvent être prises en considération.La source sonore est souvent déterminante pour appréhender un genre : celle-ci définit les instruments, les voix, les formations ou les effectifs de telle ou telle musique.La voix étant un instrument tout à fait particulier (elle réside dans le corps de l'exécutant et permet d'ajouter du texte à la musique, mais aussi des sons vocaux tels que des cris, des râles, des souffles...), la musique vocale et la musique instrumentale généreront des genres musicaux différents.Du point de vue du nombre des interprètes, on peut, par exemple, distinguer : le genre quatuor à cordes (pour quatre solistes), le genre sérénade pour orchestre de chambre (petite formation orchestrale), le genre concerto symphonique (pour orchestre symphonique et soliste).Le lieu de destination peut parfois déterminer un genre ou un autre. Il peut s'agir de musique destinée à être jouée en extérieur ou en intérieur et dans des types de lieux variés (type de lieux, dimension, volume, acoustique…). Par exemple, le genre marche militaire est normalement destiné à être joué en plein air, tandis qu'un trio de musique de chambre est plutôt destiné à une salle, de dimension réduite de préférence.La durée moyenne d'une œuvre musicale varie notablement d'un genre à l'autre et peut servir de caractéristique. Par exemple, les morceaux du genre opéra sont généralement plus longs que ceux du genre chanson.Lorsqu'on veut déterminer un genre musical, le critère sociologique est probablement le plus pertinent. Il permet de répondre aux questions suivantes : « à quoi sert cette musique ? » (sa fonction), « à qui s'adresse-t-elle ? » (quel groupe social) et « dans quelles circonstances est-il joué ? ». Ainsi la musique religieuse (ou musique sacrée) regroupera certains genres musicaux, tandis que son contraire (la musique profane) en regroupera d'autres. Quelques exemples : musique pour la scène, musique de film, musique de danse, musique militaire, musique funéraire, musique d'ascenseur, jingle, etc.Le genre doit ensuite être distingué du système musical, c'est-à-dire, de l'ensemble des usages propres à telle ou telle pratique musicale : échelles, modes rythmiques, règles d'écriture et aspects techniques divers.En effet, un système de composition peut créer différents genres musicaux. Le système tonal, depuis la fin de la Renaissance jusqu'à nos jours, a imprégné divers genres appartenant aussi bien à la musique instrumentale, à la musique vocale, à la musique sacrée ou à la musique profane.Réciproquement chaque genre peut appartenir à différents systèmes musicaux. Par exemple la musique vocale sacrée est susceptible d'exister dans un très grand nombre de systèmes : homophonie, hétérophonie, musique modale, musique tonale, musique atonale, musique sérielle, musique acousmatique, etc.Le genre musical doit également être distingué de la forme musicale.En effet, des œuvres musicales appartenant à un même genre peuvent revêtir différentes formes. Par exemple, une mélodie peut suivre la forme binaire (ABABA...), la forme rondo (ABACAD...), ou encore, une forme plus complexe et inhabituelle, la forme libre (ABCDE...).À l'inverse, des œuvres musicales appartenant à des genres différents peuvent revêtir la même forme. Par exemple, la forme fugue peut se retrouver dans une messe, dans une pièce pour orgue, dans une ouverture, dans un opéra, etc.Pour désigner le concept qui fait l'objet du présent article, le mot « forme » est souvent employé en lieu et place du mot « genre », ce qui ne manque pas de susciter de regrettables équivoques. Cette confusion provient du fait que dans un contexte donné — une époque, une esthétique, etc. — un genre revêt souvent une forme privilégiée, à tel point que le premier donne son nom au second — ou inversement. Quoi qu'il en soit, les deux concepts ne doivent pas être confondus. De même, le genre sonate est à distinguer de la forme sonate. Si l'emploi du mot « genre » a été retenu ici, c'est, d'une part parce que dans le domaine de la musique, ce mot revêt un sens analogue à celui qu'il reçoit dans d'autres arts — cinéma, littérature, peinture, etc. — d'autre part et surtout, parce que dans ce même domaine, le mot « forme » a déjà d'autres significations, ainsi qu'on vient de le voir.Allan Moore a répertorié quatre façons d'appréhender la relation entre le genre et le style musical :dans la première le style décrit la manière d'articuler les gestes musicaux (en) et le genre est lié à l'identité et au contexte de ces gestes ;dans la seconde le genre met l'accent sur le contexte des gestes et se rapporte à l'esthétique alors que le style qui met l'accent sur leur mode d'articulation se réfère à la poïétique (le processus de création) ;dans la troisième, le genre est normalement socialement contraint alors que le style est peu déterminé par le social et a un certain degré d'autonomie ;dans le quatrième, le style a plusieurs niveaux hiérarchiques, du plus général qui peut être socialement constitué au plus local ; le système des genres est lui aussi hiérarchique mais les « sous-genres » constituent des genres à part entière d'une façon différente des « sous-styles ».En dépit des distinctions ci-dessus, il n'est pas toujours facile de s'entendre sur la définition exacte de tel ou tel genre : certains ont des frontières floues, d'autres sont inventés par les critiques, tel le post-rock, ou plus récemment encore, le nu metal.Parfois, un nom de genre est susceptible d'évoluer en fonction de l'époque ou du lieu. Par exemple, dans la musique classique, le genre sonate, au XVIe siècle, désigne approximativement toute pièce musicale exclusivement instrumentale (par opposition à la cantate, pièce essentiellement vocale) ; tandis qu'au XIXe siècle, le même mot renvoie plus précisément à un « genre instrumental propre à la musique de chambre, généralement constitué de plusieurs mouvements ».D'autres fois au contraire, plusieurs mots désignent plus ou moins un même genre. Par exemple, dans la musique baroque, les termes suite, ordre, cassation et partita renvoient au même genre (avec éventuellement quelques nuances variant selon le compositeur).Le progrès de la distribution musicale électronique a créé la possibilité d’avoir accès à de très vastes catalogues musicaux, et, en dépit des limites évoquées ci-dessus, a augmenté la nécessité d’une classification cohérente des genres musicaux.En raison de l'incohérence des taxonomies musicales existantes, un projet de metadatabase (en) global de titres musicaux a été proposé par François Pachet et Daniel Cazaly. Cette proposition a comme objectif celui de décrire et classifier (dans le cadre de la musique occidentale) les titres musicaux (et non pas des albums ou artistes), en suivant les principes d’objectivité, indépendance, similarité et cohérence, en s’appuyant sur une série de descripteurs musicaux. Les créateurs de la plate-forme musicale Gracenote, dont le chercheur Oscar Celma, ont quant à eux répertorié plus de 2 000 genres musicaux, qui ont été créés par les labels musicaux pour des raisons de marketing ou bien par les fans (Celma et d'autres parlent alors de folksonomy, soit de taxonomie musicale créée par de simples auditeurs et non par des musiciens ou des professionnels de l'industrie musicale).Entre les différents genres, le nombre de caractéristiques communes est susceptible de varier. Certains genres sont très éloignés de nature (par exemple, la comédie musicale a peu de points communs avec le psaume) ; d'autres au contraire, peuvent être considérés comme très proches, sinon apparentés (par exemple, la sonate correspond à une évolution de la suite).Une classification arborescente peut être opérée par exemple en adoptant la classification suivante :musique vocalemusique vocale sacréemusique vocale profanemusique instrumentalemusique instrumentale sacréemusique instrumentale profaneCette double classification selon la source sonore et fonction sociale n'est pas exempte de reproches (par exemple, un même genre peut exister dans différents types de musique), mais elle évite d'employer trop de termes n'appartenant pas nécessairement au secteur musical qui intéresse l'utilisateur. Par ailleurs, elle est relativement facile à utiliser.[réf. nécessaire]Liste alphabétique des genres musicauxListe des genres musicaux par zone géographiqueListe des genres de la musique occidentalePrincipes de classement des documents musicaux (PCDM)Le format ID3 propose aussi une liste de genre musicaux.Le mot genre revêt un sens tout à fait spécifique dans le domaine de la musique de la Grèce antique. Dans ce contexte, le terme désigne alors une division particulière de chaque tétracorde constitutif d'une échelle musicale.Un tétracorde est la succession de quatre degrés conjoints — par exemple : mi, ré, do, si. Parmi ces degrés — qui correspondent aux quatre cordes de la lyre — les degrés extrêmes — mi et si dans notre exemple — sont fixes, tandis que les degrés intermédiaires — ré et do dans notre exemple — sont mobiles. Une gamme est donc composée de deux tétracordes consécutifs — « mi, ré, do, si » et « la, sol, fa, mi », par exemple — séparés par un ton, appelé « disjonction » — « si, la » dans notre exemple.Le genre diatonique divise chaque tétracorde de la manière suivante, de l'aigu au grave : deux tons (9/8) et un demi-ton diatonique ou limma (256/243). Il y a par exemple : « mi, ré, do, si ». C'est le seul genre dont la musique occidentale ait hérité, et qui est à la base de l'échelle diatonique pythagoricienne.Le genre chromatique divise chaque tétracorde de la manière suivante, de l'aigu au grave : tierce mineure (32/27), demi-ton chromatique ou apotome (2187/2048) et demi-ton diatonique ou limma (256/243). Il y a par exemple : « mi, do                    ♯              {\displaystyle \sharp }  , do                    ♮              {\displaystyle \natural }  , si ».Le genre enharmonique divise chaque tétracorde de la manière suivante, de l'aigu au grave : tierce majeure (5/4 ou 81/64, selon les théoriciens) et deux diésis, équivalent approximativement au quart de ton (selon les théoriciens : 36/35, 28/27, 39/38, 40/39, 31/30, 32/31...). Il y a par exemple : « mi, do, xx, si » — la note xx est à mi-chemin entre do et si.Gérard Denizeau, Les genres musicaux, vers une nouvelle histoire de la musique, Paris, Larousse, 1998. Portail de la musique"
musique;"En acoustique, un partiel harmonique est une composante d’un son périodique, dont la fréquence est un multiple entier d'une fréquence fondamentale.Si on appelle « ƒ » la fréquence fondamentale, les partiels harmoniques ont des fréquences égales à : ƒ, 2ƒ, 3ƒ, 4ƒ, 5ƒ, etc.Les partiels harmoniques sont des composants importants d’un son musical – du moins dans une conception traditionnelle du « son musical » : dans les musiques d'aujourd'hui, les bruits peuvent être aussi des sons musicaux. La fondamentale détermine la hauteur perçue, la puissance relative des harmoniques de rang supérieur influe, avec des caractères dynamiques, sur le timbre.En musique, par assimilation, on appelle « harmoniques » les sons qu'on obtient sur les instruments à cordes en forçant la vibration d'une corde à un mode supérieur à son mode fondamental. Par exemple, en effleurant la corde au tiers de sa longueur, on empêche son déplacement latéral à cet endroit, tout en la laissant osciller autour de ce point fixe, créant un nœud qui l'oblige à vibrer à une fréquence triple de celle qu'elle aurait, libre. Le son ainsi produit se trouve à un intervalle de douzième avec celui de la corde libre (une octave plus une quinte).Le mot « harmonique » est utilisé aussi de manière moins technique pour désigner des éléments de l'harmonie, par exemple dans l'expression « intervalle harmonique », qui désigne simplement un intervalle appartenant à l'harmonie.À hauteur (donc fréquence) identique, les sons émis par deux instruments différents (par exemple un violon et une flûte) ne sonnent pas de la même manière. Chacun se caractérise par ce qu’on appelle son timbre, qui permet de l’identifier. Cela traduit le fait qu’aucun son réel n’est absolument simple : il résulte de la combinaison de sons partiels. La fréquence d'ensemble de ceux-ci détermine la note de musique que l'on perçoit (voir  Fondamentale absente) et leur évolution au cours de l'émission de la note déterminent son timbre.Les sons produits par les instruments à vibration entretenue (ceux dont la vibration est soutenue par un apport constant d'énergie, par exemple par le frottement de l'archet pour les instruments à cordes, ou par un souffle constant pour l'orgue ou les autres instruments à vent) contiennent de nombreux harmoniques, alors que d'autres instruments comme les percussions émettent des fréquences inharmoniques (2,576ƒ, 5,404ƒ... par exemple pour un triangle). De plus, chaque harmonique possède une intensité relative par rapport aux autres. Le spectre harmonique révèle ainsi l’ensemble des fréquences qui déterminent le timbre de chaque instrument.La fréquence fondamentale est celle du premier partiel harmonique du son considéré, que l’on désigne comme harmonique 1 ou harmonique fondamental. La note que l’on perçoit correspond à cet harmonique, même s'il est absent du spectre sonore. Certains sons peuvent cependant tromper l’oreille, un harmonique aigu pouvant s’entendre plus que la fondamentale et la cacher[réf. souhaitée].Les harmoniques d’une note, forcément plus aigus que cette note, sont souvent appelés harmoniques supérieurs par opposition à la théorie erronée des harmoniques inférieurs avancée par certains théoriciens de la musique.Le tableau des fréquences de notes ci-dessous indique une correspondance entre les fréquences harmoniques d’une note et les notes qui s’accordent en consonance avec la fondamentale. Ainsi par exemple pour la note do, les notes constituant des intervalles consonants avec elle sont mi (la tierce), sol (la quinte), si  (la septième), do (l’octave), ré (la neuvième), etc. La raison en est qu'un nombre élevé des harmoniques supérieurs de ces notes consonances sont aussi des harmoniques de la fondamentale : la consonance se définit alors par la concordance entre les harmoniques. Les harmoniques d’une note sont donnés par les fréquences multiples de la fondamentale. Ainsi pour do à 32,7 Hz noté do−1, les harmoniques sont :N.B. : les 7e et 11e harmoniques n'ont été utilisées dans l'histoire récente de la musique occidentale que dans les musiques  microtonales ou les  musiques spectrales.L’image ci-dessous indique les harmoniques du do1 sur une portée et précise par les flèches et les chiffres (en cents) l’écart de hauteur entre chacun des 16 premiers harmoniques et la note la plus proche dans la gamme tempérée. Considérant que le demi-ton (du tempérament égal) fait 100 cents, la déviation de 31 cents de l'harmonique 7 est un tiers de ton plus bas que la moitié de la distance entre les harmoniques 6 et 8 et la déviation de 49 cents de l'harmonique 11 est quasiment à mi-chemin entre deux notes existantes, c’est-à-dire un quart de ton : c'est la raison pour laquelle elles ont eu peu d'usage dans la musique classique.Les écarts des harmoniques avec les notes de la gamme tempérée se retrouvent quelle que soit la note fondamentale et sont propres au rang de l'harmonique.Sur un piano, enfoncer doucement la pédale et faire résonner une note, permet de les écouter lorsque l’intensité sonore diminue : les cordes correspondant aux harmoniques vibrent par sympathie ; la série indiquée ci-dessus est alors relativement audible.C’est le bon ajustement des harmoniques de deux notes entendues simultanément qui validera la consonance de l’intervalle ou de l’accord entendu. On retrouve bien les raisons pour lesquelles un accord est parfait (do-mi-sol = 4-5-6) : les harmoniques des notes de l’accord sont en concordance avec les harmoniques de la fondamentale.Sur les instruments à cordes, on peut faire entendre un son harmonique en effleurant d’un doigt une division entière de la corde. Ci-contre, les divisions correspondant à la fondamentale F (corde à vide) et aux trois premiers harmoniques, et la façon de noter une note ainsi jouée.Harmonique est à l'origine un adjectif qualifiant ceux des partiels d'un son qui sont multiples d'une même fréquence, dite fondamentale. Les autres partiels sont dits inharmoniques. Seuls les partiels harmoniques contribuent à l'identification de la note de musique.Employé comme substantif, il remplace l'expression « partiel harmonique ». Il est par conséquent, comme partiel, du genre masculin.Si, dans un discours, on utilisait harmonique comme substantif pour remplacer « note harmonique » ou « division harmonique », il serait du genre féminin, comme les substantifs de l'expression à laquelle il se substituerait[réf. souhaitée].Par contamination du substantif féminin harmonie, il s'emploie aussi au féminin.Patrice Bailhache, Antonia Soulez et Céline Vautrin, Helmholtz du son à la musique, Paris, Librairie philosophique J. Vrin, 2011, 253 p. (ISBN 978-2-7116-2337-2, lire en ligne)Philippe Gouttenoire et Jean-Philippe Guye, Vocabulaire pratique d'analyse musicale, Delatour france, 2006, 128 p. (ISBN 978-2-7521-0020-7)André Calvet, Le Clavier Bien Obtempéré. Essai de Tempéramentologie, Montpellier, Piano e forte éditions, 2020, 470 p.PartielSpectre harmoniqueSérie de FourierJeux de mutations à l’orgueHarmonique artificielFondamentale manquanteFréquence fondamentale(fr) Site pédagogique pour voir et entendre les harmoniques sur un piano Portail de la musique   Portail de la physique"
musique;"La harpe est un instrument de musique à cordes pincées de forme le plus souvent triangulaire, muni de cordes tendues de longueurs variables dont les plus courtes donnent les notes les plus aiguës. C'est un instrument asymétrique, contrairement à la lyre dont les cordes sont tendues entre deux montants parallèles. L'instrumentiste qui joue de la harpe est appelé harpiste.Au début, il existait deux sortes de harpes : la harpe arquée et la harpe angulaire. Elle est, avec la flûte et certains instruments à percussion, l'un des plus anciens instruments de musique. Elle est peut-être née de l’arc musical dont la corde, tendue et relâchée, vibre et émet un son.L’origine de la harpe remonte à la Mésopotamie. Les premières harpes et lyres ont été trouvées à Sumer vers 3500 av. J.-C.. Plusieurs harpes ont été trouvées dans des sépultures et des tombes royales à Ur. Elle est connue des musiciens de l'Égypte antique, comme de Sumer (actuel Irak) et de Babylone. La harpe s'est répandue à travers les diverses civilisations et tous les continents sous des formes différentes.La harpe était un instrument universel : on la célèbre sur tous les continents et toutes les catégories sociales s'expriment à travers son art.En Europe, elle est signalée au sud-est de l'Écosse sur les pierres « pictes » aux alentours du IXe siècle apr. J.-C., et en Irlande pendant le haut Moyen Âge. Elle a alors pris sa forme moderne : triangulaire, apparemment posée sur la pointe, et dotée de la colonne qui relie la console (où s'accrochent les cordes) au bas de la caisse de résonance. Son usage se répand ensuite dans tout le continent.Le nombre de cordes et la forme variaient en fonction de l’évolution des civilisations, des besoins de la musique, de la technique de fabrication et de l'exigence d'inépuisables raffinements musicaux.La harpe médiévale reste immuablement diatonique, alors que le chromatisme envahit peu à peu la musique. À la Renaissance on utilise encore des harpes diatoniques (Gargantua de Rabelais apprend à jouer de la harpe). Mais le manque de chromatisme entraîne une désaffection de l'instrument au profit du luth et des instruments à clavier en train de naître. Pour pallier ce handicap, les luthiers italiens construisent la arpa doppia, la harpe double contenant deux rangées de cordes parallèles. C'est alors que, en 1697, un luthier bavarois, Hochbrücker, imagina un mécanisme qui, à l'aide de pédales permit d'effectuer certaines modulations. Cette harpe fut introduite en France en 1749. C'est une harpe à simple mouvement.C'est vers 1800 que le célèbre facteur de pianos, Sébastien Érard, invente le fameux mouvement à fourchettes qui va permettre à la harpe de rivaliser à nouveau avec les autres instruments chromatiques. Pour des raisons pratiques, en privilégiant de passer une pédale au lieu d'une autre, les harpistes ont souvent recours aux homophones ou notes enharmoniques. Pour répondre à ces critiques, en 1894, Gustave Lyon, directeur de la maison Pleyel, essaya de reprendre le principe des harpes chromatiques à double rangées de cordes croisées. Debussy composa pour cet instrument ses Danses sacrées et profanes. Le succès de cette harpe fut cependant de courte durée et à la mort de Gustave Lyon en 1936, elle disparut presque complètement de la vie musicale. Les danses sont maintenant jouées sur la harpe à pédales (double mouvement), car le système de fourchettes s'est considérablement amélioré. La harpe à pédales, ou harpe classique, est celle que l'on utilise dans les orchestres symphoniques et dans les formations de musique de chambre. C'est la harpe la plus sophistiquée.Elle possède de 40 à 46 cordes (pour les harpes d'étude) et 47 cordes (pour les harpes de concert), ce qui lui donne une tessiture de six octaves. Ces cordes sont principalement en boyau, à l'exception des cordes les plus graves (les deux dernières octaves) qui sont en métal, elles sont appelées cordes filées (filetage cuivre sur âme acier), les cordes les plus aiguës sont en nylon. Certaines harpes n'ont pas de cordes en boyau mais des cordes en nylon les remplacent, ce qui donne une autre sonorité à l'instrument ; les concertistes (et les instrumentistes) préfèrent souvent les cordes en boyau, qui donnent une sonorité plus « ronde » et franche, ce qui donne aussi une harmonie de matière à l'orchestre. Certaines cordes sont colorées pour permettre de repérer les notes principales : les do sont rouges et les fa sont noirs ou bleus. Les autres cordes sont incolores.La harpe à pédales peut être à simple mouvement ou à double mouvement. Dans les deux cas, on fait allusion au mécanisme reliant les pédales aux cordes pour en modifier la longueur et permettre de jouer les altérations musicales, c'est-à-dire les dièses et les bémols. Ces mécanismes ne font que réduire la longueur vibrante de la corde et n'en changent pas (idéalement) la tension.Sur une harpe double mouvement, inventée par Sébastien Érard en 1810, chaque corde peut jouer trois hauteurs : bémol si la pédale est relâchée (= en haut), bécarre si elle est bloquée sur le cran du milieu, et dièse si elle est tout à fait enfoncée.Il y a 7 pédales qui modifient les 7 notes de la gamme sur toutes les octaves. De gauche à droite, elles correspondent aux notes ré, do, si, mi, fa, sol, la pour la grande harpe. Les 3 premières pédales sont réservées au pied gauche, les 4 dernières au pied droit. Sur certains modèles, notamment sur les harpes Érard, une huitième pédale servait à actionner les volets de fermeture de la caisse de résonance. La harpe Erard de la photo ci-contre en possédait originellement (ouïes rectangulaires). Cette huitième pédale est appelé « pédale de renforcement ». L'ouverture ou la fermeture des volets changeaient la puissance du son propagé. Jean-Baptiste Krumpholtz a composé une sonate (Sonate dans le style pathétique Op. 14 N° 2) spécialement pour harpe à pédale de renforcement.La harpe à simple mouvement, tout comme la harpe celtique, ne permet que deux hauteurs par corde. L'invention de la harpe à simple mouvement est attribuée au facteur allemand Hochbrücker (1662/73 - 1763). On accorde la harpe à simple mouvement généralement en mi bémol majeur - toutes pédales relâchées - ce qui permet par la suite de jouer jusqu'à 3 bémols ou jusqu'à 4 dièses. Le nombre des tonalités est donc limité, mais le mécanisme, plus simple, permet la fabrication d'instruments moins coûteux.Inventée en 1894 par Gustave Lyon, directeur de la firme Pleyel, pour concurrencer la harpe diatonique à pédales, elle comporte deux plans de cordes croisés : un plan de cordes pour les bécarres, un plan pour les bémols et dièses. Elle permet l'exécution de tous les traits chromatiques avec une grande vitesse, mais contrairement à la harpe diatonique, elle ne permet pas les glissandi dans tous les modes et tonalités.Pour montrer les possibilités de l'instrument, la firme Pleyel commanda en 1904 une œuvre à Claude Debussy qui composa les Danses sacrée et profane pour harpe chromatique et orchestre à cordes. Mais cette œuvre est aussi jouable sur harpe diatonique, avec toutefois de très difficiles passages de pédales. Notons qu'André Caplet composa une première version de son Conte fantastique pour harpe chromatique et orchestre en 1908, intitulée Légende. Il adaptera ensuite l'œuvre pour harpe diatonique et quatuor à cordes en 1924.En riposte et afin de promouvoir les possibilités de la harpe diatonique, la firme Érard passa commande en 1905 d'une œuvre à Maurice Ravel qui composa l'Introduction et Allegro pour harpe avec accompagnement d'un quatuor à cordes, d'une flûte et d'une clarinette.Il avait été prévu une évolution de la harpe chromatique en y ajoutant des pédales, permettant ainsi à la fois les chromatismes rapides et les glissandi de la harpe diatonique. Cette harpe devait voir le jour en 1914, mais la Première Guerre mondiale mit fin au projet et la harpe chromatique tomba dans l'oubli progressivement dans les années d'après-guerre.Une classe de harpe chromatique a existé au Conservatoire national supérieur de musique de Paris de 1903 à 1933.Une classe de harpe chromatique a perduré au Conservatoire royal de Bruxelles jusqu'en 2005. Elle avait été ouverte en 1900, fermée en 1953, puis rouverte en 1978. Le départ à la retraite de Francette Bartholomée, titulaire de la classe de harpe au Conservatoire royal de Bruxelles (qui enseignait à la fois la harpe diatonique et la harpe chromatique) et son remplacement par un professeur qui ne pratique que la harpe diatonique a signifié la fermeture du Cours de harpe chromatique en 2005.Il est à noter qu'une association (Harpa Nova) a été nouvellement créée en Belgique à l'initiative de Vanessa Gerkens, une élève de Francette Bartholomée, pour soutenir l'enseignement de la harpe chromatique, promouvoir sa facture par de nouveaux luthiers (Pleyel ne construit plus de harpes chromatiques depuis 1930) et la sauvegarde des harpes Pleyel encore en existence. Un nouveau modèle de harpe chromatique de cinq octaves appelé « la Phoenix » a été récemment produit pour Vanessa Gerkens par le luthier français Marc Brûlé.Quelques citations irlandaises du XIIIe siècle :« tout gentilhomme doit avoir un coussin sur sa chaise, une femme vertueuse et une harpe bien accordée »« trois objets ne sont pas saisissables par voie de justice : le livre, la harpe et l'épée »La harpe celtique est un instrument central du monde celte ; plus que « traditionnelle », elle est une expression d'une culture classique celtique et, maintenant, d'une musique celtique contemporaine ; elle possède généralement 32 à 38 cordes. Elle est reconnaissable à son arc, toujours cintré. Les harpes celtiques cordées en métal que l'on trouvait au Moyen Âge, en Irlande et en Écosse notamment, sont toujours fabriquées et jouées de nos jours. Cet instrument médiéval qui se joue avec les ongles, a cependant fortement évolué, pour aboutir à ce qu'on peut appeler aujourd'hui les néo harpes celtiques, cordées en boyau ou nylon et qui se jouent avec la pulpe du doigt, ce qui implique une technique de jeu complètement différente qui se rapproche de la technique de jeu classique. Ce dernier type de harpe celtique sert parfois dans l'apprentissage de la harpe à pédales, sa taille la rendant plus accessible aux enfants, et son prix, plus accessible aux parents.De nos jours, les cordes sont le plus souvent en nylon dont la sonorité est un peu moins puissante et peut avoir une sonorité un peu « chinoise ». Les cordes en nylon ont en revanche l'avantage d'être moins sensibles aux changements de température et de casser moins souvent. Mais on trouve aussi des instruments montés en boyau (de mouton) ou encore en métal. Certaines cordes sont généralement colorées, comme pour la harpe à pédales, ce qui permet de repérer les notes de la gamme. Ainsi, les do sont rouges et les fa sont noirs ou bleus.Des taquets, crochets, clapets (ou palettes), fixés près de la partie supérieure de chaque corde, permettent de modifier la hauteur d'un demi-ton pour jouer les altérations (dièses/bémols). Ces clapets représentent en quelque sorte les touches noires d'un piano. On accorde généralement la harpe celtique en mi bémol majeur avec les taquets en position basse, ce qui permet ensuite de jouer dans les tonalités ayant jusqu'à quatre dièses ou jusqu'à trois bémols.La harpe celtique correspond à tout un répertoire, traditionnel ou savant, irlandais, écossais et, depuis les années 1950, breton. Mais elle s'adapte aussi à des répertoires classiques et contemporains (jazz, folk-rock, « world », électro-rock, pop, new age et métal). Elle accompagne idéalement le chant soliste. Sa petite taille en fait un instrument de choix pour débuter l'apprentissage de la harpe à pédales, bien qu'elle possède une technique de jeu propre, différente du jeu sur harpe classique. La plupart des instruments sont acoustiques mais il existe des harpes électro-acoustiques et purement électriques (cf Alan Stivell).Il existe également de petites harpes, pouvant être sanglées, dont on peut jouer debout et en se déplaçant. Traditionnellement, cette harpe dite bardique possède des cordes métalliques. Sa période de référence est le Moyen Âge, du Ve siècle au XVe siècle. Son répertoire s'oriente autour de la musique ancienne et traditionnelle celtique.On rencontre aussi un petit instrument moderne au son dynamique et brillant possédant le plus souvent 22 cordes en nylon, dans le registre aigu. Dite « harpe troubadour », elle fait référence aux musiciens qui utilisaient ce type d’instrument pour accompagner chants, danses et récits. Pour jouer des altérations sur cette harpe, il est nécessaire de l'accorder pendant le morceau, car elle ne possède pas de crochets.Cette harpe médiévale comporte deux rangs de cordes parallèles correspondant l'un aux notes naturelles et l'autre aux altérations (comme les touches blanches et les touches noires d'un clavier). Elle n'a rien à voir avec la grande harpe chromatique.On joue de cette harpe au Pays de Galles où c'est un instrument traditionnel. Son enseignement, hors académie, s'est transmis confidentiellement. Dans ce pays, elle bénéficie aujourd'hui d'un regain d'intérêt, avec des joueurs comme Llio Rhydderch ou Robin Huw Bowen qui interprètent un répertoire traditionnel tout en s'ouvrant à d'autres cultures musicales.Cette harpe est appelée harpe andine. Elle a été importée par les conquistadors au XVIe siècle. Elle possède 7 octaves et mesure 1,50 m de longueur et 76 cm de largeur.Cette harpe (arpa andina) est un instrument important du patrimoine musical péruvien et sur toute la Cordillère des Andes. Importée par les Conquistadores, elle a été adaptée pour répondre aux besoins d'expression musicale propres à la région. Elle est donc un produit du syncrétisme européo-andin, comme le charango par exemple. Notamment, sa caisse de résonance a été agrandie et la tension sur les cordes est plus faible que sur la harpe occidentale. Instrument diatonique, elle n'a pas de pédale. Essentiellement en bois, elle varie de taille et de forme selon les régions. Elle comporte de 32 à 38 cordes, et l'on trouve sur la même harpe aussi bien des cordages de nylon, de métal ou de boyaux (ces derniers, pour les notes basses seulement, sont en voie de disparition). Elle comporte un chevillier pour l'accordage, comme la guitare.La harpe andine est conçue pour être facile à transporter, et certains musiciens en jouent en marchant, pour accompagner les processions et les danses de carnaval. Elle est utilisée en instrument soliste, en accompagnement de petits ensembles ou avec de grands orchestres, essentiellement dans un registre de musique folklorique.Contrairement à la harpe occidentale qui utilise le système tonal, les musiciens andins jouent de la harpe en utilisant un système pentatonique. Elle est largement utilisée dans la musique vernaculaire du Pérou, surtout pour les genres de huayno des cordillères centrale et sud.Harpe des Andes                   Proche de la précédente, et donc à mi-chemin en taille entre la harpe des Andes et la harpe européenne, on trouve la harpe paraguayenne. Celle-ci comporte de 32 à 46 cordes (généralement 36), en nylon, qui sont également réparties autour de la ligne médiane de la tête de harpe : ainsi les forces de tension sont équilibrées, et la fabrication de ce type de harpe est moins lourde que celle des autres types. Elle aussi comporte un chevillier pour l'accordage.Harpe paraguayenne                Certainement l'une des plus anciennes formes de harpe connue, répandue en Égypte ancienne et en Asie. La caisse de résonance forme avec la console un angle plus ou moins droit qui n'est pas fermé par un joug.La harpe chinoise ou konghou : cette harpe a disparu depuis des siècles alors qu'elle avait pourtant une position importante en Chine.La harpe iranienne ou chang : cette harpe fut aussi prééminente en Iran mais a disparu depuis trois siècles.La harpe turque ou çeng : disparue elle aussi depuis deux siècles.la harpe géorgienne ou changi.Appelées aussi « harpes coudées », elles sont aussi fort anciennes et leurs conceptions assez rudimentaires.La harpe birmane ou saung : datant du VIIIe siècle, c'est l'une des plus anciennes harpes encore jouée de nos jours. C'est un instrument rare et précieux, très ornementé et réservé autrefois à la Cour.La harpe arquée des Khantys (Ostiaks).Les harpes africaines : C'est le type de harpe le plus courant rencontré sur le continent africain. Les harpes arquées sont fort nombreuses et réparties sur plusieurs ethnies et pays. Elles se déclinent en divers types elles-mêmes : naviforme, anthropomorphe, semi-ovoïde, ovale, rectangulaire, etc. Elles accompagnent le chant.Ce sont des hybrides de harpe arquée et de harpe angulaire avec deux ou plusieurs manches. On en rencontre en Nouvelle-Guinée et en Afrique. Il ne faut pas les confondre avec les lyres dont le sens des cordes est différent.On en rencontre plusieurs variétés en Afrique noire. Comme leurs noms l'indiquent, il s'agit de harpes hybrides ayant la forme et les caractéristiques d'un luth, mais utilisées avec une technique de jeu de harpe.Cet instrument hybride d'Afrique subsaharienne se présente sous la forme d'une variété d'arc musical ou de cithare mais avec des éléments propres à la technique de jeu de la harpe, notamment grâce à un haut chevalet similaire à celui des harpes-luths. Précisons que le terme de cithare ne convient pas à la harpe angulaire d'Afrique centrale qui se joue droite et verticale et non à plat sur les genoux comme se joue une harpe chromatique ou cithare.Il existe depuis deux siècles une multitude de variétés de guitares ornées d'un second manche non fretté et dont on joue en partie avec une technique de harpe.Harpe classiqueVoir la Catégorie:Harpiste classique.Harpe celtiqueVoir dans l'article Harpe celtique.Clotilde Cerdà i Bosch (1861-1926), jeune prodige de la harpe, connue par son nom de scène d'Esmeralda Cervantes avec le soutien de Victor Hugo.Charnassé Hélène et Vernillat France, Les instruments à cordes pincées - PUF Paris, 1970Élisabeth et Rémi Chauvet et alii (Myrdhin, Alan Stivell, Dominig Bouchaud, Tristan Le Govic…), Anthologie de la harpe : La harpe des Celtes, éditions de la Tannerie, avec un CD audio et un historique de la harpe.Alan Stivell et Jean-Noël Verdier,  Telenn, la Harpe Bretonne, éditions Le Télégramme.Michel Faul, Nicolas-Charles Bochsa : harpiste, compositeur, escroc, éditions Delatour 2003.  (ISBN 2-7521-0000-0).Michel Faul, Les tribulations mexicaines de Nicolas-Charles Bochsa, harpiste, éditions Delatour 2006.  (ISBN 2-7521-0033-7).Christine Y Delyn, dessins de Denis Brevet, Clairseach, la harpe irlandaise : aux origines de la harpe celtique, éd. Hent Telenn Breizh, 1998. Ouvrage de référence, abondamment illustré, sur l'histoire de la harpe irlandaise ancienne, et son rôle dans la civilisation gaélique, 175 p.H. Avelot, « L'art et la mode chez les Pahouins », in Arts d'Afrique noire, Villiers le Bel, 2001.Roslyn Rensch, The Harp, Its History, Technique and Repertoire,Harpes d'AfriqueLes harpes du Musée de la musique (Paris) Portail de la musique classique   Portail des musiques du monde"
musique;"En musique, la hauteur est l'une des caractéristiques essentielles d'un son ou note, les autres étant la durée, l'intensité, le timbre et l'expression.En musique occidentale, on désigne la hauteur par le nom d'une note sur une échelle ou gamme. La hauteur relative d'une note dans la gamme s'appelle degré. Le solfège écrit vers le haut de la portée les notes aiguës, et inversement, vers le bas, les graves. On écrit les signes correspondant aux instruments qui n'ont pas de hauteur précise, bien qu'ils puissent sonner aigu ou grave, comme les tambours, sur une ligne horizontale quelconque, l'important étant leur placement dans le temps, noté de gauche à droite.La perception des hauteurs est un des champs d'investigation de la psychoacoustique. La tradition musicale et les études de psychologie expérimentale convergent pour analyser cette perception en deux composantes distinctes, provenant de deux systèmes physiologiques différents. La perception de la hauteur spectrale place le son entendu assez approximativement sur une échelle entre graves et aiguës. La perception de la hauteur tonale rend capable de différencier deux sons harmoniques proches, indépendamment de leur richesse en graves et en aiguë.Dépendant de capacités d'interprétation et sujette à l'apprentissage, la perception des hauteurs apparaît comme une fonction cognitive.La sensation de hauteur est très bien partagée. Presque tout le monde peut reconnaître et chanter un air dont le contour mélodique n'est ni trop subtil, ni trop accidenté. Les médecins traitent l'incapacité à le faire comme un handicap, qu'ils appellent amusie. Cependant, le discernement des hauteurs et l'identification des notes est une capacité cognitive. Les musiciens la cultivent en s'entraînant à identifier une note à partir du son, dans un exercice appelé dictée musicale.Ils développent aussi leur capacité de discriminer des sons justes, c'est-à-dire dont la hauteur est précisément celle qu'exige la gamme dans laquelle la musique se joue. Cette capacité est particulièrement importante pour les musiciens qui pratiquent un instrument comme le violon, capable d'émettre un son dans un domaine continu de hauteurs. La notion de justesse est cependant dépendante des usages et de la culture, et ne peut guère être valide que lorsque le contexte est bien déterminé.On appelle intervalle l'écart de hauteur entre deux notes. Si elles sont émises simultanément, on parle d'intervalle harmonique ; si elles sont émises successivement, on parle d'intervalle mélodique.L'intervalle nul s'appelle l'unisson ; on parle d'unisson quand deux instruments ou deux voix émettent simultanément une note de même hauteur.L'octave est un intervalle de statut particulier dans toutes les cultures musicales. Dans la notation musicale occidentale, le nom des notes se répète à chaque octave, et on doit préciser, en cas d'ambiguïté, à quelle octave on se réfère. Le principe de l'identité des octaves indique que deux notes séparées par une octave sont équivalentes, et correspondent en pratique à une unisson.L'identification précise des intervalles entre notes, qu'on appelle oreille relative, est plus répandue que l'identification des fréquences en elles-mêmes, qu'on appelle oreille absolue. Cette dernière capacité exige probablement une disposition physiologique innée, et à coup sûr un entraînement commencé à un âge précoce.L'échelle des hauteurs des notes de musique se base sur l'octave. L'octave correspond à une division par deux de la longueur de l'élément vibrant qui produit le son, qu'il s'agisse d'une corde ou d'un tuyau. L'acoustique enseigne que cette division par deux correspond à une multiplication de la fréquence fondamentale par deux.Par exemple, pour un tuyau d'orgue :le tuyau d'orgue à bouche ouverte le plus long, do1, mesure 2,599 m ;un tuyau moitié moins long, de 1,30 m, donne un do à l'octave supérieure, dit do2 ;diviser encore la longueur par deux donne 0,65 m, et toujours do, dit do3 ;diviser encore la longueur par deux donne 0,325 m, et toujours do, dit do4.L'échelle des hauteurs est en progression arithmétique, quand la fréquence et les paramètres qui la gouvernent sont en progression géométrique, ce qui fait d'elle une échelle logarithmique.L'intervalle d'une octave est divisé en six tons ou douze demi-tons. Dans les systèmes modernes, tous les demi-tons tempérés sont égaux, et correspondent à une multiplication de la fréquence de la note par la même valeur. Pour des analyses plus fines, les notations du solfège peuvent être remplacés par les savarts ou les cents.La musique occidentale se base sur une théorie de la tonalité, qui privilégie certains rapports de hauteur. Les noms des notes se répètent à chaque octave. Dans une octave, sept niveaux de l'échelle des douze demi-tons correspondent à une note qui a un nom. Les autres en dérivent par une altération.Si les musiciens pensent les intervalles de façon linéaire, suivant la notation du solfège, les théoriciens de la musique rapportent les notes à des fréquences de vibration. Pour passer de l'échelle musicale à la fréquence, il suffit de connaître la fréquence d'une note. Les autres s'en déduisent par calcul.La Conférence internationale de Londres en 1953 donne comme référence, généralement admise, le la3 à 440 hertz.Dans la gamme tempérée, on peut calculer la fréquence des notes avec la formule suivante :                    f        =        55        ×                  2                      o            c            t            a            v            e            +                                                            d                  e                  m                  i                  t                  o                  n                  −                  9                                12                                                          {\displaystyle f=55\times 2^{octave+{\frac {demiton-9}{12}}}}  où octave et demiton sont des nombres entiers, correspondant à la note, de do (demiton=0) à si (demiton=11).Le tableau ci-contre donne les fréquences des notes dans l'octave du la de référence (octave 3). Il faut multiplier les fréquences par 2 pour une octave au-dessus, et les diviser par 2 pour une octave en dessous. La colonne de droite indique l'écart relatif par demi-tons, correspondant à l'intervalle musical.Cependant, ces calculs ne concernent que les notes de la musique occidentale, dans sa version moderne. Ils ne s'appliquent qu'à la gamme à tempérament égal et ne différencient pas les demi-tons diatoniques et chromatiques (voir aussi « Comma »).La hauteur relative d'une note de musique est l'intervalle qui la sépare d'une note considérée comme la base. Le changement de la note de base conserve les intervalles et les mélodies. Quand des musiciens réunis n'ont pas de diapason qui leur donne le la, ils peuvent jouer en se basant sur un la estimé.Dans la musique tonale, on indique le degré qu'occupe la note sur les sept que comporte l'échelle musicale. Le premier degré est la tonique et désigne la tonalité. Les degrés sont définis en termes de hauteur nominale, indépendamment de l'octave (ou du registre) où on peut les trouver.Quand les musiciens effectuent une transposition musicale, ils changent délibérément la note de base (la tonique), tout en conservant tous les intervalles (ce qu'on appelle le contour mélodique).Dans la musique tonale, la hauteur de référence permettant d'apprécier une hauteur relative, peut être :dans le cas d'un intervalle mélodique, soit la note précédente, soit la tonique ;dans le cas d'un intervalle harmonique, soit la basse, soit la fondamentale de l'accord en cours.La hauteur correspond à une sensation, c’est-à-dire à un phénomène psychoacoustique lié à une cause physique. Dans le contexte des études psychoacoustiques, qui ne concernent pas que la perception des sons musicaux, cette perception peut être appelée tonie.Le son est une vibration de l'air. Lorsque cette vibration est un périodique, c'est-à-dire qu'elle se répète identique à elle-même pendant un cycle d'une durée toujours égale appelée période, cette période, ou la grandeur inverse, la fréquence, en est un caractère d'autant plus important que le mathématicien Joseph Fourier a montré au début du XIXe siècle que toute fonction périodique, représentant numériquement un phénomène, peut s'analyser en une somme de sinusoïdes, dont les fréquences sont des multiples de la fréquence du phénomène périodique. On appelle son pur un son comportant une seule fréquence, et donc décrit par une fonction sinus. On savait déjà que la vibration des cordes et des colonnes d'air qui sont à la base des instruments de musique comportent des vibrations harmoniques ; Fourier montre que tout son peut se décomposer en une somme de sons purs.Toutefois, ce calcul vaut pour des sons qui s'étendent à l'infini, tant dans le passé que dans l'avenir. Mais une note de musique a une durée. La poursuite de l'analyse montre que plus la durée considérée est courte, moins la détermination de la fréquence est précise. Le produit de la durée par l'incertitude sur la fréquence est une constante, dans un système donné. Quand la durée diminue, l'incertitude augmente.Il faut garder à l'esprit cette incertitude quand on raisonne sur la musique. La musique est faite de sons changeants. Même l'instrument au son le plus régulier, comme l'orgue, a un temps d'attaque et une vibration interne, ne serait-ce que parce qu'il est joué et entendu dans un espace réverbérant. Par conséquent, en toute rigueur aussi bien qu'en pratique, la détermination d'une hauteur est une opération intellectuelle d'abstraction, et non une mesure.Les musiciens de la culture occidentale ont, depuis des siècles, accordé une importance privilégiée à la hauteur du son. Cette importance se reflète dans l'enseignement de la musique et dans la facture instrumentale. Ils ont aussi, souvent, voulu se rattacher philosophiquement à la science et aux mathématiques. Il faut donc, lorsqu'on étudie la question de la perception de la hauteur, examiner avec un soin critique le legs de la tradition musicale, qui reprend, dans ses principes, des opinions que l'on fait remonter à l'antiquité et à Pythagore en particulier.Notion intuitive pour les musiciens qui s'entraînent à en affiner leur perception dès le début de leur apprentissage, et apprennent au passage, en quelques lignes, une doctrine qui la rattache aux fréquences, la hauteur tonale n'est pas facile à définir rigoureusement.La hauteur d'un son pur correspond à sa fréquence de vibration, que l'on mesure en hertz (nombre de vibrations périodiques par seconde). Plus la vibration est rapide, plus le son est dit aigu ou haut ; plus la vibration est lente, plus le son est dit grave ou bas,.Si un son pur est une vibration sinusoïdale, avec une seule fréquence, les sons des notes musicales sont des sons complexes, qu'on peut décomposer en vibrations à plusieurs fréquences. Lorsque ces fréquences sont réparties selon une règle de distribution harmonique, que la fréquence fondamentale est dans le domaine audible et que leur enveloppe coïncide, un seul son est entendu avec une seule hauteur, correspondant à la fréquence fondamentale, même si elle n'est pas la plus forte, et que le son ne présente que ses multiples.Un son composé de multiples d'une même fréquence se perçoit comme à la hauteur de cette fréquence, même si celle-ci n'est pas présente dans ce mélange. On appelle ce paradoxe « perception de la fondamentale absente ».Exemple :un son pur de fréquence 110 Hz s'évalue à la hauteur d'un la1 ;le mélange des fréquences 3 × 110 = 330 Hz ; 4 × 110 = 440 Hz ; n × 110, s'évalue à la hauteur d'un la1, bien que toutes les fréquences soient largement supérieures.Parmi les instruments de musique, le basson a la propriété d'émettre des sons graves alors que la puissance sonore se trouve presque entièrement dans les harmoniques. Les facteurs d'orgue utilisent cette perception pour créer des notes basses avec des couples de tuyaux plus courts.L'échelle de Mel, proposée en 1937, montre que si l'on demande à des auditeurs de situer les sons purs successifs, en recherchant, pour un son de base, celui qui sonne à un intervalle d'une octave, l'écart de fréquence trouvé augmente significativement avec la fréquence.Cette augmentation se combine avec des facteurs mécaniques d'inharmonicité dans l'accordage inharmonique du piano.L'écart de fréquence entre les sons purs qu'on puisse à peine distinguer varie aussi selon la fréquence et l'intensité. Il est minimal dans la région des 1 500 Hz (octave 6). En tout, l'être humain peut différencier environ 1 800 fréquences de sons purs. Cette performance correspond à la capacité de distinguer deux sons de fréquence proche. Elle ne signifie pas que les sujets soient capables de situer la fréquence sur une échelle.La reconnaissance de la hauteur est la plus précise pour des sons d'une durée au-delà de une demi-seconde pour l'homme. Dans ces conditions, elle est de l'ordre de 1 savart (ou 5 à 10 cents) pour les fréquences les mieux discriminées, vers 1 500 Hz. Pour des sons plus brefs, le seuil de différenciation augmente. Un son très bref n’a pas de hauteur définie ; on parle de « claquement ».La hauteur perçue dépend faiblement de l'intensité. Stanley Stevens a montré qu’un son paraît baisser quand on augmente son volume pour les fréquences inférieures à 2 000 Hz. Pour des fréquences supérieures à 3 000 Hz, un accroissement d’intensité élève la hauteur perçue. Heureusement, ce phénomène ne concerne que les sons purs, et il n’affecte donc pas les instruments de musique[réf. souhaitée].Pour les sons qui n'ont pas de fréquence fondamentale audible, et donc ne peuvent s'associer à une note de musique, comme ceux des cymbales, on distingue des registres musicaux, dont les plus élémentaires sont le registre grave et le registre aigu ; entre les deux, on parle de registre médium.La perception du registre est indépendante de l'existence d'une note fondamentale. On ne peut attribuer une note à un tambour, mais on distingue un tambour grave d'un tambour aigu.D'autre part, les auditeurs identifient comme différents deux sons harmoniques stables, de même fondamentale mais de spectre sonore différents. On dit de celui dont les harmoniques aiguës sont plus puissantes qu'il a un timbre plus brillant, alors que les deux sons correspondent à la même note.Les études psychoacoustiques semblent confirmer qu'il s'agit de deux perceptions auditives distinctes, appelées la tonie ou hauteur brute, pour la hauteur spectrale, et le chroma, ou hauteur fondamentale ou nominale pour la capacité à distinguer des notes (avec parfois une erreur d'octave).La séparation de la variation de la hauteur tonale (par déplacement de la fondamentale) et de celle de la hauteur spectrale (par modification de l’enveloppe spectrale) dans un procédé de synthèse sonore permet de créer des variations de hauteurs paradoxales engendrant des effets inattendus, comme celui de la Gamme de Shepard.La résolution de la hauteur spectrale serait d'environ une tierce majeure, soit un rapport de l'ordre de 1 à 1,25. Le chroma, qui n'existe que pour les sons nettement périodiques, permet une résolution qui atteint 1/300 d'octave et donc l'identification des notes, mais il perd progressivement sa précision à partir de 2 000 Hz (do 6) et ne donne aucune indication au-delà de 4 500 Hz. La note la plus aiguë du piano est le do 7, aux alentours de 4 200 Hz, compte tenu de l'inharmonicité.Il serait bien agréable à l'esprit que ces sensations fussent indépendantes. Cela n'est pas tout à fait le cas. Diana Deutsch a fait écouter à des sujets des sons de Shepard, mélange de toutes les octaves audibles d'une fréquence musicale, écartés d'exactement trois tons, soit une demi-octave. Les sujets ont nettement désigné certaines notes comme plus aigües. Si les sensations de hauteur spectrale et de hauteur fondamentale étaient indépendantes, les auditeurs n'auraient pu choisir entre trois tons montants et trois tons descendants. Il s'est avéré, de plus, que le son de Shepard désigné comme le plus aigu varie d'une personne à l'autre, et se trouve corrélé avec le dialecte parlé par les sujets participants. Un tutti d'orchestre, avec de nombreux instruments jouant la note à des octaves différentes, présente des similitudes spectrales avec un son de Shepard. L'expérience montre que le principe de l'équivalence perceptive d'une transposition n'est pas universel, et que les sujets ont en général une certaine forme d'oreille absolue.Dès lors qu'on considère que la perception des hauteurs est une capacité cognitive, les conditions de la perception, qui fournissent au sujet des informations sur ce qu'il est censé percevoir, entrent en jeu. L'écoute est dirigée par une intention. C'est une critique récurrente faite aux expériences perceptives menées en laboratoire : elles conditionnent la sensation,.La fréquence des sons musicaux que nous entendons, à la différence de ceux du laboratoire, fluctue. La tenue de l'archet du violon, le souffle dans les instruments à vent, influencent irrégulièrement la fréquence. Pourtant, nous attribuons sans difficulté une hauteur, juste ou non, à ces sons. Des variations de fréquences dont la physique nous dit qu'elles sont de même proportion, sur un enregistrement de piano, nous semblent insupportables. La reconnaissance d'une hauteur dépend donc partiellement de l'identification préalable de la source.Les sons musicaux sont joués en séquence. L'expérience montre que les musiciens n'évaluent pas la hauteur juste à la même fréquence selon l'intervalle qui précède. S'il s'agissait d'un processus purement physiologique, celle-ci devrait correspondre toujours à la même vibration physique ; mais elle diffère notamment selon que la mélodie suit un mouvement ascendant ou descendantEnfin, l'échelle musicale exerce un effet d'attraction. Dans un contexte musical, l'évaluation des hauteurs est plus précise aux alentours de ses degrés.Une alternative aux expériences de psychologie expérimentale avec des sons fabriqués exprès consiste à mesurer, grâce aux ordinateurs, les hauteurs statistiquement présentes dans une exécution musicale considérée comme représentative ou exemplaire, impliquant des instruments capables d'une variation de l'accord, comme les violons et la voix humaine. Cette mesure met en évidence une variation du diapason corrélée avec l'interprétation.D'autre part, la qualité de timbre d'un instrument de musique, et celle de la voix humaine, inclut le vibrato, qui est une variation périodique perceptible de la hauteur. En physique, cette vibration équivaut à la combinaison de deux fréquences, de part et d'autre de celle de la note produite, dont la différence est égale à la fréquence de vibration. Lorsque cette vibration n'est que transitoire dans l'exécution musicale, elle n'affecte pas la perception de la justesse, qui est une perception de la hauteur, mais celle du timbre.La perception de la hauteur, orientée par ce que l'auditeur sait de la situation, résumée dans l'expression « scène auditive », ne se résume pas aux résultats obtenus par des expériences de laboratoireSi l'on excepte les musiques décoratives dont l'objet est de masquer des sons désagréables, en restant aussi peu remarquables que possible, l'écoute de la musique, comme celle de la parole, implique une attention active et dirigée vers ce que l'auditeur identifie comme éléments pertinents. Si la parole utilise, économiquement, une faible partie des capacités de perception et de discrimination des sons, la musique, joue avec l'ensemble de ces capacités. La reconnaissance d'une modulation des hauteurs est, dans toutes les cultures, un signe distinctif de l'écoute musicale. Dans la culture occidentale, les auditeurs, même non musiciens, sont imprégnés d'un système d'organisation des hauteurs divisées en notes, dont la succession paraît, ou non, bien formée, de la même façon que, pour un locuteur non grammairien, un énoncé paraît, ou non, bien formé.Claude Abromont et Eugène de Montalembert, Guide de la théorie de la musique, Librairie Arthème Fayard et Éditions Henry Lemoine, coll. « Les indispensables de la musique », 2001, 608 p. [détail des éditions] (ISBN 978-2-213-60977-5)Michèle Castellengo, « 6. Perception de la hauteur », dans Présentation des recherches - 1963-2002, Paris, Laboratoire d'acoustique musicale, 2002 (lire en ligne)Claude-Henri Chouard, L'oreille musicienne : Les chemins de la musique de l'oreille au cerveau, Paris, Gallimard, 2001, 348 p. (ISBN 2-07-076212-2).Charles Delbé, Musique, psychoacoustique et apprentissage implicite : vers un modèle intégré de la cognition musicale, Université de Bourgogne, 2009 (lire en ligne)Laurent Demany, « Perception de la hauteur tonale », dans Botte & alii, Psychoacoustique et perception auditive, Paris, Tec & Doc, 1999 (1re éd. 1989).(en) Hugo Fastl et Eberhard Zwicker, Psychoacoustics : Facts and Models, Springer, 2006 (ISBN 978-3-540-23159-2)Alain Goyé, La Perception Auditive : cours P.A.M.U., Paris, École Nationale Supérieure des Télécommunications, 2002, 73 p. (lire en ligne)Stephen McAdams (dir.) et Emmanuel Bigand (dir), Penser les sons : Psychologie cognitive de l'audition, Paris, PUF, coll. « Psychologie et sciences de la pensée », 1994, 1re éd., 402 p. (ISBN 2-13-046086-0)Stephen McAdams, « Introduction à la cognition auditive », dans McAdams & alii, Penser les sons, Paris, PUF, 1994Albert S. Bregman, « L'analyse des scènes auditives : l'audition dans des environnements complexes », dans McAdams & alii, Penser les sons, Paris, PUF, 1994Emmanuel Bigand, « Contributions de la musique aux recherches sur la cognition auditive humaine », dans McAdams & alii, Penser les sons, Paris, PUF, 1994Gérard Pelé, Études sur la perception auditive, Paris, L'Harmattan, 2012.Pierre Schaeffer, Traité des objets musicaux : Essai interdisciplines, Paris, Seuil, 1977, 2e éd. (1re éd. 1966), 713 p.Arlette Zenatti et al., Psychologie de la musique, Paris, Presses universitaires de France, coll. « Psychologie d'aujourd'hui », 1994Michèle Castellengo, « La perception auditive des sons musicaux », dans Arlette Zenatti, Psychologie de la musique, Paris, Presses universitaires de France, coll. « Psychologie d'aujourd'hui », 1994.Diana Deutsch, « La perception des structures musicales », dans Arlette Zenatti, Psychologie de la musique, Paris, Presses universitaires de France, coll. « Psychologie d'aujourd'hui », 1994.W. Jay Dowling, « La structuration mélodique : perception et chant », dans Arlette Zenatti, Psychologie de la musique, Paris, Presses universitaires de France, coll. « Psychologie d'aujourd'hui », 1994.Acoustique musicalePsychoacoustiqueOreille relative et oreille absolueTessitureAmbitusRegistre (musique)Système tonalCircularité de hauteur sonore Portail de la musique classique   Portail de la musique"
musique;"Un instrument de musique est un objet pouvant produire un son contrôlé par un musicien — que cet objet soit conçu dans cet objectif, ou bien qu'il soit modifié ou écarté de son usage premier. La voix ou les mains, même si elles ne sont pas des objets à proprement parler, sont considérées comme des instruments de musique dès lors qu'elles participent à une œuvre musicale.Hector Berlioz commence son Traité d'instrumentation et d'orchestration (1843) en déclarant que « tout corps sonore mis en œuvre par le compositeur est un instrument de musique ».L'ensemble des instruments utilisés pour une œuvre mais aussi et surtout dans une société donnée ou une époque est appelé « instrumentarium ». L'étude académique des instruments de musique est appelée organologie et prend le plus souvent ses sources dans l'ethnomusicologie.Les plus vieux instruments de musique connus, sont des flûtes à encoche de type quena à 5 trous datant d'au moins 35 000 ans. Elles ont été retrouvées dans des grottes du Jura Souabe, région située au sud-ouest de l'Allemagne. Dans la mesure où ces flûtes sont déjà techniquement évoluées et si on se base sur la prise en main complexe des quenas modernes, elles impliquent très certainement un savoir-faire musical bien antérieur.Tous les instruments de musique dont on a retrouvé la trace archéologique jusqu'à aujourd'hui sont le fait d'Homo sapiens.Il existe plusieurs lieux dans le monde dans lesquels des instruments de musique ont été trouvés ; par exemple, un triton nodifer a été trouvé à la Font Aux Pigeons (Châteauneuf-les-Martigues). On sait qu'il servait de trompe dans les régions de Grèce aux bergers pour appeler les troupeaux. Des flûtes percées paléolithiques ont aussi été trouvées au Pays basque dans la grotte d'Isturitz. Un autre instrument, le rhombe (instrument à vent) pouvait être en os, en bois de cervidé, en ivoire ou en bois. Cet instrument, à la forme foliacée, avait des extrémités percées ; ces perforations permettaient d'attacher un objet et de le faire tournoyer afin d'obtenir un son plus fort.Par son mode d'attaque, par la forme et la matière de sa caisse de résonance, par le soutien ou non de sa vibration, l'instrument de musique détermine le timbre — l'une des quatre caractéristiques du son avec la hauteur, la durée et l'intensité. Les progrès de l'acoustique musicale permettent de mieux comprendre les composantes du spectre harmonique spécifique à chaque source sonore.Un instrument de musique comporte souvent deux parties distinctes[réf. nécessaire] :celle qui crée la vibration ;celle qui transforme cette vibration en un timbre qui caractérise cet instrument.Peu importe leur matière, les instruments sont classés par leur méthode de production du son : l'organologie est l'étude détaillée de ces outils faiseurs de musique et de leurs différentes catégorisations. Le timbre de ces instruments peut être parfois transformé par un accessoire comme les sourdines pour les cordes et les cuivres, ou un kazoo pour la voix.Pour un son donné, la vibration peut provenir d'une corde, d'une colonne d'air ou d'une percussion ; des instruments peuvent combiner plusieurs systèmes, les plus récents vont de l'électromécanique jusqu'au virtuel.Les instruments à cordes sont également appelés « cordophones ».De matière, de grandeur et de grosseur variées, les cordes peuvent être frottées, pincées ou frappées. La classification traditionnelle distingue par conséquent :les instruments à cordes frottées, comme le violon, la trompette marine ou la vielle à roue ;les instruments à cordes pincées, comme la guitare, le clavecin ou la harpe ;les instruments à cordes frappées, comme le piano ou le cymbalum.Les instruments à vent, également appelés « aérophones », mettent en jeu une colonne d'air. Celle-ci peut être produite par le souffle du musicien, par une soufflerie mécanique ou par une poche d'air. On distingue :la voix, qui exploite toutes les possibilités des membranes muqueuses du larynx (cordes vocales) ;les bois, qui comportent un biseau ou une anche :les instruments à biseau, comme toutes les flûtes ou les jeux à bouche d'orgue,les instruments à anche libre, comme les harmonicas ou les accordéons,les instruments à anche simple, comme les clarinettes ou les saxophones,les instruments à anche double, comme les hautbois ou les bombardes,les cuivres, qui utilisent la vibration des lèvres dans une embouchure, comme les trompettes, les cornets à bouquin ou le didgeridoo, ainsi que les cors, les trombones et les tubas.N.B. Contrairement à ce que cette classification pourrait laisser penser, ce n'est pas la matière utilisée dans la facture instrumentale qui est déterminante, mais bien la manière de produire le son. Ainsi, s'il existe des flûtes et des clarinettes en métal et en bois, toutes font partie des « bois ». Le saxophone construit en cuivre est un « bois » car il est muni d'un bec à anche battante. Il existe également des « cuivres » fabriqués en bois, comme les cornets à bouquin et le serpent, et à l'origine, le cor est un olifant en corne ou fabriqué dans une défense d'éléphant.Les instruments de percussion — à hauteur déterminée ou non — englobent tout instrument par lequel un corps en frappe un autre. Cette catégorie d'instruments a été subdivisée par les théoriciens en membranophones et idiophones. Le développement de cette famille nombreuse au XXe siècle (plus de 500), particulièrement des instruments à claviers ou à lamelles, a imposé une nouvelle catégorisation autant pour les percussionnistes que pour les enseignants. À l'orchestre ou dans les classes de percussion, la distinction est faite entre :les claviers, constitués d'une série de lames accordées en bois ou en métal, frappées par des baguettes (comme le xylophone ou le steel drum) ou par l'intermédiaire d'un clavier (célesta) ;les peaux, naturelles ou synthétiques, elles sont constitués d'une membrane frappée par les mains ou par des baguettes, accordée ou non, comme le djembé ou la timbale ;les accessoires, c'est-à-dire toutes les autres percussions ne produisant généralement qu'un son, du triangle aux maracas en passant par les claves ou la crécelle.Les instruments de combinaison associent plusieurs modes de mise en vibration. On distingue :les instruments mécaniques, comme la serinette ou l'orgue de Barbarie ;le claviorganum, combinant orgue et clavecin actionnés par le(s) même(s) clavier(s).la Marble Machine, créée par le groupe Wintergatan, combinant guitare basse, vibraphone, cymbale ainsi que des percussions émulées à l'aide de microphones de contact, actionnée par des billes ou directement à la main. L'énergie est fournie par le musicien via une manivelle, et stockée dans un volant d'inertie. Un programmateur mécanique et des embrayages permettent au musicien d'activer des boucles « pré-enregistrées » sur chacun des instruments.Les instruments électromécaniques, comme l'orgue Hammond ou le Yamaha CP80.Les instruments de musique électronique, comme le Thérémine et les ondes Martenot.Les instruments électroanalogiques, comme le Moog Micromoog (synthétiseur analogique) ou les Yamaha DX7 et Roland AX-Synth (synthétiseurs numériques portables).Les instruments virtuels de l'Atari au Macintosh.Le Musikinstrumenten-Museum, musée des instruments de musique à Berlin ; il rassemble environ 3500 instruments.Le Musée des Instruments de musique (MIM), créé à Bruxelles en 1877, réunit dans les locaux d'un superbe immeuble Art nouveau une collection de plus de 8 000 instruments : instruments occidentaux mécaniques, électriques et électroniques, instruments traditionnels européens, instruments du monde.En 1999, fut ouvert à Ouagadougou (Burkina Faso), le Musée de la musique qui réunit une collection d'instruments de musique traditionnels des terroirs ethnoculturels du pays.Le musée des Beaux-Arts de Chartres abrite un instrumentarium, dont les représentations dans la cathédrale Notre-Dame sont au nombre de 320 pour 26 instruments différents. La pratique de ces instruments a donné lieu à l'enregistrement de deux albums,, ;Le musée des Instruments à vent, à La Couture-Boussey, centre de facture d'instruments à vent depuis le XVIIe siècle ;Le Musée de la Musique, à Paris, fait partie de la Cité de la musique.Le Musée national des instruments de musique de Rome ouvert à Rome en 1964, rassemble une collection exceptionnelle de 3 000 instruments, de l'Antiquité jusqu'à nos jours, couvrant tous les genres musicaux.Le Musée de la musique de Bâle (Musikmuseum en allemand) situé dans l’ancienne prison Lohnhof depuis l’an 2000.Le Musée national de la musique (České muzeum hudby) à Prague est installé dans l’ancienne église Sainte-Marie-Madeleine de style baroque. Il est situé à Malá Strana. Plus de 400 instruments de musique d'époque y sont exposés.* Liste des instruments de musiqueListe de fabricants d'instruments de musiqueHistoire de la catégorisation des instruments de musiqueInstrument de musique en bambouRessource relative aux beaux-arts : (en) Grove Art Online Musée virtuel avec extraits sonores et photosListe exhaustive et illustrée des instruments de musiqueInstruments du monde avec extraits sonores, explications et photosUstensiles & Instruments, Le ventre et l'oreille n°3, 2019, (ISSN 2650-3050). Portail de la musique   Portail des arts                    "
musique;"Un instrument à cordes est un instrument de musique dans lequel le son est produit par la vibration d'une ou plusieurs cordes. L'organologie les classe dans la catégorie des cordophones. L'histoire des instruments à cordes est vieille de plusieurs milliers d'années. Les premiers n'avaient probablement qu'une seule corde, comme l'arc musical. Dès l'Égypte ancienne, on connaissait les joueurs de harpe. Au Moyen Âge, les ménestriers s'accompagnaient au luth, etc.La vibration de la corde seule est peu audible. Une plaque couplée aux cordes, la table d'harmonie, prélève une partie de l'énergie vibratoire de la corde pour la transmettre à l'air et obtenir un son. La table d'harmonie peut être une peau de tambour, comme dans le banjo ou la kora.La fréquence fondamentale de la vibration dépend des caractéristiques de longueur, de masse et de tension de la corde. Les cordes ont subi de nombreuses évolutions, du fait notamment des techniques disponibles et de critères esthétiques et symboliques. On a utilisé des fibres végétales, des produits animaux comme le crin de cheval, le boyau ou la soie, des fils métalliques, et plus récemment les fibres synthétiques comme le nylon.Pour augmenter la masse de la corde sans en détériorer l’élasticité, les luthiers fabriquent des cordes où une âme souple est entourée d'un fil métallique (souvent un alliage de cuivre) qui en augmente la masse. Cette augmentation permet des sons plus puissants et plus graves.Il existe trois modes de jeu principaux sur les instruments à cordes, correspondant aux possibilités d'excitation de la vibration de la corde :par pincement des cordes, c'est-à-dire par un déplacement initial de la corde qui est ensuite relâchée, avec les doigts éventuellement armés d'onglets ou un plectre (ou médiator) ;par frappe avec des baguettes ou de petits marteaux ;par frottement avec un archet ou tout autre dispositif légèrement adhésif qui permet une  excitation continue par relaxation, alors que dans les deux premiers cas, la vibration décroît après l'excitation initiale.Dans chacun de ces trois cas, il existe des instruments où un mécanisme excite la corde.Certains instruments comportent de plus des cordes excitées indirectement, sans que le musicien ne les actionne, qu'on appelle cordes sympathiques (viole d’amour, Sitar).On peut aussi classer les instruments à corde par disposition. On distingue alorsceux dont les cordes sontperpendiculaires à la table d'harmonie comme les harpes,parallèles à la table d'harmonie comme les luths ;et sonnentsur toute leur longueur, comme le piano,sur une partie de leur longueur, sur une action du musicien, comme le violon. La guitare, la basse, le banjo, la mandoline, le luth... On joue de ces instruments en pinçant les cordes avec les doigts ou avec un plectre.La harpe et le clavecin utilisent des cordes chromatiques tendues sur une table d'harmonie en bois de résonance (souvent épicéa). Le clavecin possède de par son coffre un résonateur avec une rosace comme le luth.Dans la Bible, les civilisations orientales, la Grèce ainsi qu'au Moyen Âge, on retrouve les traces du psaltérion. Le psaltérion est joué avec une plume, c'est l'ancêtre de la famille d'instruments du genre clavecin.  L'ajout d'un clavier au psaltérion donne naissance à l'épinette, terme qui désignait autant le clavecin que l'épinette contrairement à l'acception moderne.En y adaptant un clavier et un mécanisme nommé sautereau muni d'un plectre pour pincer les cordes, le clavecin et l'épinette sont apparus au XIVe siècle. Les claviers de trois octaves se sont agrandis jusqu'à cinq octaves (63 notes) au cours des siècles ce qui a donné de grands instruments ; qui parfois ont voulu rivaliser avec l'orgue en multipliant les registres et en ajoutant, un deuxième clavier (Flandre, France) voire un troisième clavier ou un pédalier (Allemagne). Par exemple le piano, le cymbalum, le clavicorde. Les cordes sont frappées avec un marteau lorsqu'on appuie sur la touche.Le piano utilise des cordes tendues sur une caisse de résonance en bois. Pour faire sonner les cordes, le piano les frappe avec des petits marteaux. Le tympanon, le cymbalum apparaissent au Moyen Âge. Le tympanon, joué à l'aide de mailloches, donnera par la suite au XVe siècle le clavicorde muni d'un clavier. Au bout de la touche du clavier de clavicorde est fichée une lame métallique qui vient directement percuter la corde. Cette pièce est appelée tangente car elle divise la corde en deux parties, dont l'une est étouffée pour ne pas vibrer. Le clavicorde est le premier instrument à clavier et à cordes frappées. Au XVIIe siècle Bartolomeo Cristofori (1655-1731) invente le piano, mais ce piano-forte équipé d'une transmission clavier→marteau est très éloigné du piano actuel : à la place de la tangente est disposée une fourche dans laquelle s'articule un levier dont la grande extrémité est munie d'un marteau garni de peau, la petite extrémité est retenue par une barre fixe, il faut relâcher la touche pour répéter la note. Ce système sera perfectionné plus tard avec l'invention de l'échappement simple qui porte le nom de mécanique viennoise ; la barre fixe est remplacée par un élément muni d'un ressort qui se retranche dès que le marteau a frappé et permet ainsi de rejouer la note aussitôt. Ce système d'échappement va s'améliorer dans le courant du XIXe avec le double échappement. Ceci est la première génération de piano, qui en compte trois, jusqu'au piano actuel. Ce sont les violons et les instruments similaires de l'orchestre symphonique européen, et de nombreux instruments de musique populaire ou érudite de par le monde, comme le erhu chinois.Dans la plupart des cas, on frotte les cordes avec un archet. La surface légèrement adhésive déplace la corde, jusqu'à ce que la force de rappel à sa position de repos dépasse la limite d'adhérence. La corde revient alors vers sa position de repos, la dépasse et revient dans l'autre sens ; à un certain moment, la vitesse de la corde par rapport à l'archet est nulle, et elle adhère de nouveau. Ce processus produit une vibration, qui a la particularité d'avoir une fréquence fondamentale légèrement différente de celle de la corde vibrant sans excitation. Cette différence dépend des autres paramètres, comme la nature de l'enduit sur l'archet, en général, une résine appelée colophane, la force d'appui, la vitesse du mouvement, contrôlés par l'instrumentiste, comme le paramètre principal, la longueur de la corde, que le musicien règle, dans le cas du violon, en appuyant une ou plusieurs cordes sur le manche. Les vibrations se transmettent par le chevalet à la caisse de résonance.La corde à vide, dont la longueur et la tension sont fixes, varie un peu de fréquence fondamentale avec la force d'appui et la vitesse de l'archet, en même temps que change le volume sonore. Les instruments à cordes frottées de la musique orchestrale occidentale, de la famille des violons, n'ont pas de frettes. C'est ce qui permet à l'instrumentiste de jouer  juste les différentes notes, tout en faisant varier les trois causes qui affectent la puissance et la fréquence fondamentale des vibrations qui se transmettent par le chevalet à la caisse de résonance.La vielle à roue frotte les cordes avec la tranche d'un disque mu par une manivelle. La construction et le réglage de l'instrument déterminent la force d'appui sur la corde.Liste des instruments de musiqueListe des instruments à cordes (musique classique)Liste des instruments à cordesListe des cordophones dans le système Hornbostel-SachsHistoire de la catégorisation des instruments de musiqueAccordages d'instruments à cordes (en) avec illustrations Portail de la musique"
musique;"Un instrument à vent (ou aérophone) est un instrument de musique dont le son est produit grâce aux vibrations d'une colonne d'air provoquées par le souffle d'un instrumentiste (flûte, trompette… ), d'une soufflerie mécanique (orgue, accordéon) ou d'une poche d'air (cornemuse, veuze… ). Ils sont regroupés en deux grandes familles : les bois pour lesquels le son est produit par vibration d'une anche ou à travers un biseau ;les cuivres pour lesquels le son est produit par les lèvres du musicien.Ces catégories dépendent du mode de production du son d'un instrument et non du matériau utilisé pour sa conception. Ainsi les instruments à vent peuvent être fabriqués avec toutes sortes de matières (du bois, du métal, du plastique, du Plexiglas, du cristal, de l'ivoire ou de l'os), et certains utilisent des technologies mécaniques, électroniques ou informatiques.« Les plus vieux instruments à vent connus sont des flûtes fabriquées dans des os de vautour et datées de 35 000 ans pour celle d’Isturitz au Pays Basque, et de 40 000 ans pour celle de Hohle Fels, en Allemagne ».Les premiers instruments à embouchure en bassin (trompes irlandaises et danoises) datent de l'âge du bronze puis ont été utilisés depuis l'antiquité ; trompes et cors avaient essentiellement un usage militaire.Avec l'invention de l'anche (languette taillée directement dans la paroi de l'instrument ou indépendante en paille ou en roseau), la famille des bois s'est élargie avec les instruments à anches qui apparaissent au proche-orient ; des double clarinettes (Arghul) primitives sont présentes sur les représentations en ancienne Égypte. Ce type d'instrument s'est alors répandu en Afrique du nord et en Europe (aulos: instrument à deux chalumeaux en roseau...).Il existe de nombreux vestiges d'instruments à vents autour du bassin méditerranéen : « Les plus anciennes flûtes de Pan découvertes en Europe sont originaires des régions orientales du continent : d’une nécropole néolithique (2000 av. J.-C.) d’Ukraine méridionale et d’un site de la région de Saratov. Chacune se compose de sept à huit tuyaux en os creux d’oiseau… »...Au IIIe siècle av. J.-C., Ctésibios d’Alexandrie invente un orgue appelé hydraulos, réunion de plusieurs monaules (flûte grecque à une seule tige) à un clavier et alimentés avec de l’air comprimé créé par une colonne d’eau.Le Moyen Âge a été une période foisonnante pour la création de nouveaux instruments à vent.Dès lors, les instruments ont été constamment améliorés depuis la Renaissance.Les instruments sont classés par leur méthode de production du son et non par les matériaux qui les composent :la Voix :les femmes (soprano, alto… ) ;les hommes (ténor, baryton, basse… ).les Bois, une colonne d'air mise en vibration sur un biseau ou par une anche :Instrument à biseau,Instrument à conduit, comme la flûte à bec (en bois) ou le positif (tuyau en métal) ;Instrument à embouchure libre, comme les flûtes traversières, la quena (droite), le siku (polycalame) ou les flûtes obliques (comme le ney).Instrument à ancheInstrument à anche libre, comme l'harmonica, l'accordéon ou le bandonéon ;Instrument à anche simple, comme la clarinette (en ébène) ou le saxophone (en métal) ;Instrument à anche double, comme le hautbois, la bombarde ou le basson.les Cuivres, une colonne d'air mise en vibration par les lèvres du musicien, comme la trompette (en métal), le cornet à bouquin et le didgeridoo (en bois) ou l'olifant (en ivoire).Instruments mécaniques, combinant plusieurs systèmes comme le limonaire ou l'orgue de Barbarie.Instruments à vent électroniques, utilisant le souffle et les doigtés d'instruments à vent.Les instruments à vent, Georges Gourdet, Que sais-je ? n°267, Presses Universitaires de France, 1967Instrument de musiqueBois (musique)Cuivre (musique)Onde stationnaire dans un tuyauOrchestre d'harmonieBrass bandJean-Luc Matte, « Typologie des instruments à vent », sur jeanluc.matte.free.fr, 23 février 2011 (consulté le 23 février 2012). Portail des musiques du monde   Portail de la musique"
musique;"En musique, l'intervalle entre deux notes est l'écart entre leurs hauteurs respectives. Cet intervalle est dit harmonique si les deux notes sont simultanées, mélodique si les deux notes sont émises successivement.En acoustique, l'intervalle entre deux sons harmoniques est le rapport de leurs fréquences.Chaque intervalle est caractéristique[Comment ?] d'une échelle musicale, elle-même distinctive d'un type de musique (indienne, occidentale, musique orientale, etc.). La perception des intervalles diffère selon les cultures. Il n'existe pas de système musical universel contenant tous les intervalles de toutes les échelles musicales. Seul l'intervalle entre un son et sa répétition, l'unisson, peut être considéré comme n'appartenant pas en propre à un genre musical déterminé[réf. nécessaire].Lorsqu'un système musical[Quoi ?] ne possède pas de théorisation écrite[Quoi ?], les musicologues[Lesquels ?] utilisent[réf. nécessaire] la terminologie du solfège[Quoi ?] pour rendre compte des intervalles et des échelles propres à ce système.Historiquement, l'étude des intervalles a commencé par l'étude des rapports entre fréquences. L’école pythagoricienne a, grâce au monocorde, réussi à construire des micro-intervalles, quasi imperceptibles, par simple soustraction d’intervalles. Autrement dit par la division des fractions qui les représentent. La physique permet de comprendre les relations fractionnaires dues à la nature des ondes sonores, l'acoustique musicale et la psychoacoustique permettent de comprendre comment les intervalles et sons musicaux sont perçus.Ce type de représentation atteint vite ses limites : la perception d'un intervalle musical dépasse la notion de rapport de fréquences surtout avec l'usage d'instruments inharmoniques tels que le piano.Un intervalle est pur (ou « naturel ») lorsque le rapport des fréquences de ses deux notes est égal à une fraction de nombres entiers simples. En acoustique, la pureté se manifeste par l'absence de battement.L'addition de deux intervalles s'obtient en multipliant leurs rapports de fréquences. Une quinte pure (3/2) plus une quarte pure (4/3) donne une octave pure (2/1) :                                          3            2                          ×                              4            3                          =        2              {\displaystyle {\frac {3}{2}}\times {\frac {4}{3}}=2}  La soustraction de deux intervalles s'obtient en divisant leurs rapports. Une octave pure moins une quinte pure donne une quarte pure (complément à l'octave de la quinte) :                                          2                          3              2                                      =                              4            3                                {\displaystyle {\frac {2}{\frac {3}{2}}}={\frac {4}{3}}}  L'essence d'une mélodie (ou d'une harmonie) est déterminée par la nature des intervalles séparant les notes qui la constituent, et non pas par les notes elles-mêmes.Un intervalle mélodique est dit :ascendant si le deuxième son est plus aigu que le premier (par exemple, en musique occidentale : do puis sol dans la même octave),descendant si le deuxième son est plus grave que le premier (sol puis do dans la même octave),conjoint si ses notes sont deux degrés consécutifs de l'échelle considérée (do-ré ou sol-fa sont conjoints dans la même octave en gamme de do majeur),disjoint s'il n'est pas conjoint (do-mi, ou do-do si les deux do sont séparés par une ou plusieurs octaves ; do et do# sont deux notes différentes).Si l'intervalle est constitué du même son répété deux fois, c'est un unisson.En musique tonale, en musique modale, ou en musique atonale, la notion d'intervalle renvoie plus précisément à la distance entre deux degrés d'une gamme musicale.Dans la musique classique et donc dans le système tonal, les intervalles sont nommés et théorisés par le solfège et la fonction des différents degrés dépend de l'intervalle qui sépare chacun d'eux de la tonique. Aux différents intervalles sont associés les notions de consonance et dissonance.Les degrés de l'échelle diatonique sont séparés par des espaces conjoints (ou intervalles) inégaux, les tons et les demi-tons diatoniques.Les intervalles séparant deux degrés de l'échelle diatonique sont toujours nommés en utilisant un nom suivi d'un qualificatif (adjectif) :le nom est lié au nombre de degrés englobés ; ce nombre dépend de la gamme musicale utilisée ;le qualificatif dépend de l'étendue réelle de l'intervalle, compte tenu des tons et demi-tons : ainsi, une tierce est dite majeure lorsqu'elle englobe deux tons, mineure si elle n'englobe qu'un ton et un demi-ton diatonique. Nom Le nom de l'intervalle dépend de son étendue en degrés, c'est-à-dire du nombre de notes qui séparent la première note de la deuxième. Il dépend donc aussi de la tonalité choisie. L'intervalle s'appelle :une prime, lorsque l'on a un unissonune seconde, entre deux notes de noms successifs. exemple: Do-Ré, Do-Ré#une tierce. entre trois notes de noms successifs, exemple: Do-Mi, Do-Mi♭ ou encore Do♭-Mi# car Do, Ré et Mi se suiventune quarte, entre quatre notes de noms successifs, exemple: Do-Fa.Le raisonnement est le même pour la quinte, la sixte, la septième, l'octave, etc.Au-delà de l'octave, le noms deviennent neuvième, dixième, onzième, etc.Ainsi, do-sol constitue une quinte car l'intervalle constitué de do, ré, mi, fa, sol est long de cinq degrés.Jadis, le terme servant à désigner la longueur d'un intervalle servait également à désigner un degré par rapport à la tonique ou par rapport à une autre note de référence. Il est donc préférable d'indiquer la fonction des degrés, par exemple : « sol est la dominante de la gamme de do » ou « sol est le cinquième degré de la gamme de do » plutôt que « sol est la quinte de la gamme de do ». En revanche, il est correct de définir que « do-sol forme une quinte ».Cependant, en harmonie tonale, l'habitude est conservée de désigner les notes réelles d'un accord au moyen de l'intervalle qui sépare celles-ci de la basse, ou de la fondamentale, en fonction du contexte. Par exemple : « au premier renversement, la tierce (sous entendu : de la fondamentale) va à la basse ». Et inversement, « sur ce premier renversement, la sixte (sous entendu : de la basse, cette sixte est donc la fondamentale) est au soprano ».Les notes extrêmes d'un intervalle à chiffre pair — seconde, quarte, etc. — ont des positions différentes sur la portée : une sur la ligne, l'autre dans l'interligne ; au contraire, les notes extrêmes d'un intervalle à chiffre impair — unisson, tierce, etc. — ont des positions identiques sur la portée : soit sur deux lignes, soit dans deux interlignes. Qualificatif Le nom d'un intervalle ne donne pas son étendue exacte. Par exemple, les deux tierces do-mi et do-mi♭, bien qu'englobant le même nombre de noms de notes (Do-Ré-Mi : 3 notes), n'ont pas la même étendue tonale. La première est dite majeure (elle s'étend sur deux tons) ; l'autre est dite mineure (elle s'étend sur un ton et demi). Il existe cinq qualificatifs principaux :majeur,mineur,juste,augmenté,diminué.Plus rarement, on rencontre les qualificatifs « sur-augmenté » et « sous-diminué ».Au sein de l'échelle diatonique naturelle, les intervalles se partagent en deux familles :ceux qui, ni augmentés ni diminués, n'ont qu'une étendue « moyenne », qualifiée de juste,ceux qui ont deux étendues « moyennes » possibles : ils peuvent être soit majeurs soit mineurs (l'étendue d'un intervalle majeur est plus grande d'un demi-ton chromatique que celle de l'intervalle mineur). Intervalles justes La quarte, la quinte et l'octave peuvent être qualifiées de justes :la quarte juste fait exactement 2 tons et 1 demi-ton,la quinte juste fait exactement 3 tons et 1 demi-ton,l'octave juste fait exactement 5 tons et 2 demi-tons. Intervalles mineurs et majeurs La seconde, la tierce, la sixte et la septième peuvent être qualifiées de mineure ou de majeure :Par exemple, les intervalles do-ré et mi-fa sont tous deux des secondes, mais la première est majeure, car do et ré sont éloignés d'un ton, tandis que la deuxième est mineure, car mi et fa sont éloignés d'un demi-ton.Si on classe les intervalles par ordre croissant d'étendue, l'intervalle mineur précédera l'intervalle majeur correspondant. Selon ce classement, il est possible de reconstituer tous les intervalles de l'échelle diatonique naturelle en partant de do en utilisant uniquement des intervalles majeurs ou justes (les autres sont équivalents à l'un d'eux par enharmonie). Intervalles augmentés et diminués Quelle que soit la nature de l'intervalle, il est toujours possible de le rallonger ou le raccourcir d'un ou plusieurs demi-tons par l'ajout ou le retrait d'une altération. On parle alors d'intervalle augmenté et diminué si un demi-ton chromatique a été ajouté ou soustrait, et d'intervalle sur-augmenté ou sous-diminué si sa longueur a été modifiée de deux demi-tons chromatiques.Par exemple, do-sol#, est une quinte augmentée car la distance de do à sol est égale à cinq degrés, et qu'un demi-ton a été ajouté à l'intervalle. Cet exemple permet de voir que la quinte augmentée a un nom différent mais la même sonorité (dans un système à tempérament égal) que la sixte mineure (ici do-la bémol) ; cependant, ces intervalles sont différents en musique tonale ou modale, car bien que leurs sons soient identiques, leurs fonctions ne le sont pas. Cela influe sur le sens donné au discours, et peut également influencer l'interprétation musicale.Un intervalle simple (d'étendue inférieure à l'octave) peut être renversé par inversion de ses notes. Le renversement d'un intervalle est aussi appelé intervalle complémentaire, ou intervalle différentiel. Un intervalle ajouté à son renversement donne une octave juste. Par exemple, la quinte juste do-sol a pour renversement la quarte juste sol-do ; l'étendue de ces deux intervalles donne l'octave do-do ou sol-sol.Un intervalle mineur renversé donne un intervalle majeur, et inversement. De même pour les altérations : un intervalle augmenté a pour renversement un intervalle diminué. Par exemple, la tierce majeure fa-la a pour renversement la sixte mineure la-fa :Quant à un intervalle juste, son renversement est également un intervalle juste. Formule de renversement La formule suivante permet de trouver le renversement d'un intervalle donné :9 – étendue initiale = étendue du renversement.Par exemple, une septième renversée donne une seconde (9 – 7 = 2).La théorie du redoublement de l'intervalle repose sur le principe de l'identité des octaves. On définit :l'intervalle simple : de longueur inférieure ou égale à l'octave ;l'intervalle redoublé : de longueur supérieure ou égale à l'octave, c'est un intervalle formé d'une ou plusieurs octaves justes, plus un intervalle simple.L'octave juste est le seul intervalle pouvant être analysé à la fois comme un intervalle simple et comme un intervalle redoublé (le redoublement de l'unisson juste). L'octave diminuée est un intervalle simple, tandis que l'octave augmentée est un intervalle redoublé. Deux octaves forment une quinzième.En harmonie classique, les intervalles ont la signification de leur réduction à l'intervalle simple (par exemple seconde pour la neuvième). En jazz, les intervalles conservent leur sens propre dans la constitution des accords jusqu'à la treizième. Formule de redoublement Étendue de l'intervalle réduit + (7 × nombre d'octaves) = intervalle à l'octave.Ainsi, une seconde devient une neuvième à l'octave : 2 + (7 × 1) = 9.Cette même seconde devient une seizième à deux octaves : 2 + (7 × 2) = 16.Ce qui correspond bien à l'octave d'une neuvième : 9 + (7 × 1) = 16.Le qualificatif d'un intervalle redoublé est le même que celui de l'intervalle simple correspondant : par exemple, la dixième do-mi est majeure parce qu'elle est le redoublement de la tierce do-mi, qui est également majeure. Il suffit donc d'étudier les qualificatifs des seuls intervalles simples pour comprendre les qualificatifs de tous les intervalles.La transposition d'un intervalle est le déplacement de celui-ci en hauteur — au moyen des altérations sans modification de son étendue exacte.Si un demi-ton chromatique est ajouté, ou bien retranché, aux deux notes extrêmes d'un intervalle donné, le nom et le qualificatif de cet intervalle ne changent pas, en d'autres termes, les intervalles sont équivalents.Par exemple, do-mi est une tierce majeure, mais do                    ♯              {\displaystyle \sharp }  -mi                    ♯              {\displaystyle \sharp }  , ou encore, do                    ♭              {\displaystyle \flat }  -mi                    ♭              {\displaystyle \flat }  , sont aussi des tierces majeures ; fa-si                    ♭              {\displaystyle \flat }   est une quarte juste, mais fa                    ♭              {\displaystyle \flat }  -si                    ♭        ♭              {\displaystyle \flat \flat }  , ou encore, fa                    ♯              {\displaystyle \sharp }  -si, sont aussi des quartes justes ; etc.Les intervalles peuvent être identifiés à l'audition, par la perception acoustique de leur rapport de fréquences. Certaines ambiguïtés peuvent alors exister car une même différence de hauteurs peut être exprimée sous forme d'intervalles de noms différents suivant le contexte dans lequel elle se situe (voir : enharmonie).Chaque intervalle peut en revanche être identifié de façon non ambigüe à la lecture de sa notation sur une partition. Le musicien peut alors associer sonorités, fonction, et formalisme.Dans le solfège, trois intervalles simples peuvent servir de référence pour apprécier l'étendue de tous les autres :la seconde majeure, qui englobe un ton (exemple : do-ré),la tierce majeure, qui englobe deux tons (exemple : do-mi),la quarte juste, qui englobe deux tons et un demi-ton diatonique (exemple : do-fa).Grâce aux altérations qui augmentent ou diminuent un intervalle d'un demi-ton chromatique, et aux règles de renversement et de redoublement, il suffit, pour trouver le qualificatif d'un intervalle donné, de retenir les points suivants :Toutes les secondes sans altérations sont majeures — un ton —, sauf mi-fa et si-do, qui sont mineures — un demi-ton diatonique.Toutes les tierces sans altérations sont mineures — un ton et un demi-ton diatonique — sauf do-mi, fa-la et sol-si, qui sont majeures — deux tons.Toutes les quartes sans altérations sont justes — deux tons et un demi-ton diatonique — sauf fa-si qui est augmentée — trois tons (le triton).Les musiques traditionnelles (arabe, chinoise, indienne, turque, etc.) y compris en Europe (notamment dans les Balkans) utilisent parfois des micro-intervalles, comme des quarts de ton, ou des intervalles composites (par exemple, trois-quarts de tons), voire des divisions en deçà du quart de ton.Les musiciens occidentaux ont inventé des unités de mesure (tels que le cent et le savart) qui permettent de décrire ces micro-intervalles qui n'appartiennent pas traditionnellement à la musique classique occidentale (mais qui étaient reconnus au travers des commas).La musique arabe étant basée sur une gamme naturelle et non tempérée, elle repose sur un système d'intervalles différents des intervalles décrits au-dessus. Un intervalle de trois quarts de ton — dit « seconde neutre » — différencie de manière caractéristique les genres dits « zalzaliens » des genres diatoniques (à intervalles de secondes mineures et majeures) et des genres à seconde augmentée (type hijaz). On le trouve dans de nombreux Maqâms de la musique arabe (râst en particulier), dans la musique turque (musique classique ottomane - Türk sanat müziği), ouzbèke, ouïghoure, certaines formes de Musique kazakhe et kirghize, certaines musiques afghanes ou iranienne, etc., ou encore les musiques des Balkans (grecque, yougoslave et bulgare…).Les śruti utilisent un système musical qui divise l’octave en vingt-deux parties : ce système est difficile à percevoir pour une oreille habituée aux échelles occidentales. Inversement, la quinte, qui dans le solfège occidental est l'intervalle exprimant très précisément la distance entre cinq degrés — en référence à la gamme diatonique —, ne peut avoir la même valeur et jouer un rôle comparable dans une échelle musicale divisant l'octave en 22 degrésAccordConsonanceÉchelleMicro-intervalle(fr) Résumé des intervalles(fr) Liste des intervalles(fr) Expression des Intervalles d'après le théoricien Kirnberger, vers 1776(fr) Générateur de schémas pour la mémorisation des intervalles mélodiquesAdolphe Danhauser, Théorie de la musique : Édition revue et corrigée par Henri Rabaud, Paris, Henry Lemoine, 1929, 128 p. (ISMN 979-0-2309-2226-5)Claude Abromont et Eugène de Montalembert, Guide de la théorie de la musique, Librairie Arthème Fayard et Éditions Henry Lemoine, coll. « Les indispensables de la musique », 2001, 608 p. [détail des éditions] (ISBN 978-2-213-60977-5)Pierre-Yves Asselin, Musique et tempérament, Éditions JOBERT, 2000, 236 p. (ISBN 2-905335-00-9) Portail de la musique classique   Portail de la musique"
musique;"Un microphone (souvent appelé micro par apocope) est un transducteur électroacoustique, c'est-à-dire un appareil capable de convertir un signal acoustique en signal électrique.L'usage de microphones est aujourd'hui largement répandu et concourt à de nombreuses applications pratiques :télécommunications (téléphone, radiotéléphonie, Interphone, systèmes d'intercommunication) ;sonorisation ;radiodiffusion et télévision ;enregistrement sonore notamment musical ;mesure acoustique.On appelle également micro, par métonymie, les transducteurs électromagnétiques de guitare électrique (micro de guitare) et les transducteurs piézoélectriques (capteur piézo) utilisés pour des instruments dont le son est destiné à être amplifié.Le composant électronique qui produit ou module la tension ou le courant électriques selon la pression acoustique, est appelé capsule. On utilise aussi le terme microphone par synecdoque. Un tissu ou une grille protège généralement cette partie fragile.Le premier usage du terme microphone désignait une sorte de cornet acoustique. David Edward Hughes l'a le premier utilisé pour désigner un transducteur acoustique-électrique. Améliorant le dispositif de Graham Bell, Hugues fait valoir la capacité du dispositif qu'il a co-inventé à transmettre des sons beaucoup plus faibles.Une membrane vibre sous l'effet de la pression acoustique et un dispositif qui dépend de la technologie du microphone convertit ces oscillations en signaux électriques. La conception d'un microphone comporte une partie acoustique et une partie électrique, qui vont définir ses caractéristiques et le type d'utilisation. Capteurs de pression (omnidirectionnels) Si la membrane est au contact de l'onde sonore d'un seul côté, tandis que l'autre est dans un boîtier avec une pression atmosphérique constante, elle vibre selon les variations de pression. On parle d'un capteur de pression acoustique. Ce type de capteur réagit à peu près de la même manière aux ondes sonores quelle que soit la direction d'origine. Il est insensible au vent. Il est à la base des microphones omnidirectionnels.Les microphones à effet de surface sont des capteurs de pression fixés sur une surface de quelque étendue formant baffle, qui double la pression acoustique dans l'hémisphère limité par la surface d'appui (Voir PZM (microphone) (en)). Capteurs de gradient de pression (bidirectionnels ou directivité en 8) Si la membrane est au contact de l'onde sonore des deux côtés, elle ne vibre pas lorsqu'une onde arrive en travers, puisque les surpressions sont égales des deux côtés. On appelle ce type de membrane un capteur de gradient de pression acoustique. C'est la base des microphones bidirectionnels ou à directivité en 8. Types mixtes ou variables En associant ces deux types, soit par des moyens acoustiques, en contrôlant de façon plus subtile l'accès des ondes sonores à la face arrière de la membrane, soit par des moyens électriques, en combinant le signal issu de deux membranes, on obtient des directivités utiles, notamment cardioïde (dite aussi unidirectionnelle) :On construit des microphones de directivité cardioïde large, supercardioïde et hypercardioïde en changeant les proportions entre la composante omnidirectionnelle et la composante bidirectionnelle. Des microphones peuvent offrir un réglage ou une commutation de la directivité.Ces constructions permettent de donner plus d'importance à une source vers laquelle on dirige le micro et d'atténuer le champ sonore réverbéré, qui vient de toutes les directions. On définit un indice de directivité comme l'expression, en décibels du rapport entre un son venant dans l'axe du microphone et un son de même pression acoustique efficace venant d'une source idéalement diffuse (venant de partout autour du microphone).                          Tubes à interférences Les microphones à tube à interférences donnent des directivités accentuées, mais fortement dépendantes des fréquences. À cause de leur forme allongée, on les appelle micro canon. Taille de la membrane La taille de la membrane influe sur la conversion en vibrations, puis en signal électrique.Au contact d'une paroi perpendiculaire à la direction de propagation, une onde sonore développe une puissance proportionnelle à l'aire et au carré de la pression acoustique :                    P        =        S        .                                            p                              ′                                  2                                                                                    ρ                                  0                                                           c                                            {\displaystyle P=S.{\frac {p'^{2}}{\rho _{0}\ c}}}  S est la surface de la paroi ;p est la pression acoustique ;                              ρ                      0                                {\displaystyle \rho _{0}}   est la masse volumique de l'air (1,2 kg/m3 aux conditions normales de température et de pression) ;c est la vitesse du son, 343 m/s dans les mêmes conditions.On ne peut récupérer qu'une partie de cette puissance sous forme de signal électrique décrivant l'onde sonore. Plus la membrane est grande, moins il est nécessaire d'amplifier le signal, et par conséquent, moins on le soumet à un traitement amenant inévitablement une certaine quantité de bruit et de distorsion.La taille de la membrane détermine par conséquent la sensibilité maximale du microphone. Mais dès que la plus grande dimension de la membrane devient significative par rapport à la longueur d'onde d'un son, elle constitue, pour les ondes sonore qui n'arrivent pas perpendiculairement, un filtre en peigne. Bien entendu, d'autres phénomènes comme la diffraction sur les bords interviennent, rendant la réponse réelle plus complexe.La présence d'un entourage rigide autour de la membrane crée un effet de surface qui augmente la pression acoustique pour les fréquences dont la longueur d'onde est inférieure à la taille de l'ensemble membrane-entourage. Cet obstacle peut-être plat ou sphérique, il constitue autour d'une capsule capteur de pression un filtre acoustique, comme la grille de protection, qui délimite une cavité dont les caractéristiques influent sur la réponse du microphone, particulièrement aux plus hautes fréquences.Les applications (téléphone mobile, micro cravate) qui exigent des micros de petite taille limitent par là même la taille de la membrane. Microphone à charbon Les premiers microphones, employés d'abord dans les téléphones, utilisaient la variation de résistance d'une poudre granuleuse de carbone, quand elle est soumise à une pression. Quand on comprime la poudre, la résistance diminue. Si on fait passer du courant à travers cette poudre, il va être modulé suivant la pression acoustique sur la membrane qui appuie sur la poudre. On ne peut évidemment construire de cette manière que des capteurs de pression. Ces microphones sont peu sensibles, fonctionnent sur une plage de fréquence limitée, et leur réponse n'est que très approximativement linéaire, ce qui cause de la distorsion. Ils ont l'avantage de pouvoir produire une puissance assez élevée sans amplificateur. Ils ont été utilisés dans les combinés téléphoniques, où leur robustesse était appréciée, et à la radio avant l'introduction de procédés donnant de meilleurs résultats. Microphone dynamique à bobine mobile Dans les microphones électromagnétiques à bobine mobile, une bobine est collée à la membrane, qui la fait vibrer dans le fort champ magnétique fixe d'un aimant permanent. Le mouvement crée une force électromotrice créant le signal électrique. Comme la conversion de l'énergie sonore dégagée par l'action de la pression acoustique sur la membrane donne directement un courant utilisable, ces microphones sont dits dynamiques, car contrairement aux micros à charbon et aux micros électrostatiques, ils n'ont pas besoin d'alimentation.L'apparition dans les années 1980 d'aimants au néodyme a permis des champs magnétiques plus intenses, avec une amélioration de la qualité des microphones électromagnétiques. Microphone à ruban Dans les microphones électromagnétiques à ruban, la membrane est un ruban gaufré souple installé dans le champ magnétique d'un aimant permanent. Il fonctionne comme le microphone électromagnétique à bobine mobile, avec l'avantage de la légèreté de la partie mobile. Il ne requiert pas d'alimentation. L'impédance de sortie est bien plus faible que celle des autres types, et il est assez fragile. Microphone électrostatique Dans les microphones électrostatiques, la membrane, couverte d'une mince couche conductrice, est l'une des armatures d'un condensateur, chargé par une tension continue, l'autre armature étant fixe. La vibration rapproche et éloigne les armatures, faisant varier la capacité. La charge étant constante et égale au produit de la tension et de la capacité, la variation de la capacité produit une variation inverse de tension. L'impédance de sortie est très élevée. Les micros électrostatiques ont besoin d'une alimentation, d'une part pour la polarisation du condensateur, d'autre part pour l'amplificateur adaptateur d'impédance qui doit être proche de la membrane.L'alimentation peut être fournie par un conducteur spécial relié à un boîtier d'interface qui assure aussi l'adaptation d'impédance. Cependant, ce n'est le cas que pour quelques microphones très haut de gamme. La plupart des modèles utilisent une alimentation fantôme, ainsi nommée parce qu'elle ne nécessite aucun conducteur supplémentaire.La sensibilité des microphones électrostatiques est supérieure à celle des microphones dynamiques. Il y a besoin de moins de puissance sonore pour faire vibrer la membrane seule que l'appareil membrane-bobinage, et l'amplificateur adaptateur d'impédance prélève une puissance infime. Cet amplificateur est conçu pour le capteur et contrôle aussi la bande passante ; la réponse du condensateur seul est un filtre passe-bas (Rayburn 2012, p. 33). Ces amplificateurs furent d'abord composés d'un tube électronique et d'un transformateur. Plus récemment, leur niveau de bruit et de distorsion ainsi que leur sensibilité aux interférences ont été abaissés par l'emploi de transistors ou de transistors à effet de champ, sans transformateurs. Microphone électrostatique haute fréquence Le condensateur formé par la membrane et une armature fixe n'est pas polarisé par une tension continue, mais constitue, avec une résistance, un filtre dont la fréquence de coupure varie comme la capacité. Le niveau de modulation haute-fréquence suit donc la vibration de la membrane. L'étage suivant comporte une démodulation sur une diode qui conduit les transistors de sortie. Microphone électrostatique à électret Les microphones électrostatiques à électret tirent parti d'une propriété de certains matériaux de conserver une charge électrostatique permanente. Un matériau de cette sorte constitue une armature de condensateur, la membrane l'autre. Les microphones à électret n'ont pas besoin de tension de polarisation, mais ils ont néanmoins un amplificateur adaptateur d'impédance, qui requiert une alimentation. Si la tension de crête de sortie n'est pas trop élevée, cette alimentation peut être fournie par une pile.La charge de polarisation diminue dans le temps, ce qui se traduit par une perte de sensibilité du micro au fil des années.asymétrique sur courte distance (comme dans les téléphones mobiles ou les dictaphones). Le signal est la tension entre le conducteur unique et la masse.symétrique quand les câbles sont plus longs. Le signal est la différence entre le conducteur dit « chaud » ou « + » et le conducteur dit « froid » ou « - ». Les interférences, qui s'appliquent à peu près également aux deux conducteurs, sont réduites. Les applications professionnelles utilisent une transmission symétrique avec des connecteurs XLR. L'adaptation est en tension, les microphones ayant des impédances de sortie inférieures à 600 ohms et les entrées pour microphone des impédances de plusieurs kilohms. La ligne peut comprendre une alimentation fantôme.sans fil pour libérer les porteurs de micros. La transmission peut être analogique ou numérique. L'électronique se trouvant à proximité immédiate de la membrane est peu sensible aux interférences. L'usage d'antennes de réception doubles avec des récepteurs choisissant le signal le plus fort (diversity) assure la sécurité de la transmission. Le plan de fréquences limite le nombre de micros sans fil.La conception ou le choix d'un modèle existant doit tenir compte de l'usage auquel le microphone est destiné :directivité ;sensibilité ;sensibilité aux interférences (vent, interférences électromagnétiques) ;pression acoustique maximale ;bruit propre ;bande passante ;robustesse (résistance aux intempéries, à l'humidité, aux variations rapides de pression atmosphérique, aux surcharges acoustiques et électriques, aux mauvais traitements) ;système de fixation (micros tenus à la main, micro cravate, micros de studio, micros d'instrument, intégration dans un appareil) ;transmission, par câble standard, par câble spécial, sans fil (micro HF) ;poids ;encombrement ;prix.La qualité de la transcription du son dépend des caractéristiques et de la qualité du microphone mais aussi, et principalement, de l'emplacement du microphone par rapport à la source, ainsi que de l’environnement de la prise de son (bruits, vent…).La directivité est une caractéristique essentielle du microphone. Elle indique sa sensibilité selon la provenance du son par rapport à son axe.Le diagramme polaire représente la sensibilité du microphone selon la direction d'origine de l'onde sonore. La longueur du point central à la courbe indique la sensibilité relative en décibels. Dans la plupart des cas, la sensibilité ne dépend que de la direction par rapport à l'axe principal du microphone ; dans le cas contraire, deux diagrammes sont nécessaires. La directivité dépend aussi de la fréquence ; les diagrammes complets comprennent plusieurs courbes de valeurs relatives. En général, le diagramme est symétrique, et on peut mettre, pour une meilleure lisibilité, des demi-courbes de part et d'autre de l'axe.Le plus souvent, la réponse en fréquence est la plus égale quand le microphone est face à la source. Si d'autres sons ne viennent pas se mêler à celui de la source principale, on peut utiliser les différences de réponse hors de l'axe pour égaliser la sonorité.Omnidirectionnel : Le micro capte le son de façon uniforme, dans toutes les directions. Il s'utilise surtout pour enregistrer le son d'une source étendue, comme un orchestre acoustique, ou une ambiance. Il sert dans plusieurs systèmes de captation stéréophonique. Il capte la réverbération ; il est donc souhaitable que l'acoustique de la salle se prête à l'enregistrement. On met aussi à profit son insensibilité aux bruits de manipulation et au vent, par exemple pour enregistrer des déclarations ou du chant. On l'évite en sonorisation en raison de sa sensibilité à l'effet Larsen dès que la source est un peu éloignée. Les microphones omnidirectionnels le sont en réalité d'autant moins pour les hautes fréquences que leur membrane est grande ; c'est pourquoi il est préférable de les désigner comme des capteurs de pression, selon leur principe acoustique.Cardioïde : privilégie les sources sonores placées devant le micro. Utilisé pour la sonorisation, pour le chant, pour la prise de son d'instruments, le microphone unidirectionnel est le plus répandu. L'apparence de son diagramme directionnel le fait appeler cardioïde (en forme de cœur). Il rejette bien les sons provenant de l'arrière, et atténue ceux provenant des côtés. En contrepartie, il est plus sensible au vent, aux bruits de manipulation, aux ""plops"", et est plus affecté par l'effet de proximité, qui renforce les basses pour les sources proches. De nombreux modèles commerciaux sont traités pour limiter ces inconvénients.Super-cardioïde: le super cardioïde capte en priorité les sons venant de face, et sur un plan d'environ 140° de façon à éviter les bruits environnants, il est aussi appelé super unidirectionnel.Hypercardioïde : similaire au cardioïde, avec une zone avant un peu plus étroite et un petit lobe arrière. Il présente, accentués, les mêmes avantages et inconvénients que le cardioïde. Il est souvent utilisé en conférence, quand les orateurs s'approchent peu des micros.Canon : forte directivité vers l'avant, directivité ultra cardioïde permettant de resserrer le faisceau sonore capté. Utilisé pour enregistrer des dialogues à la télévision ou au cinéma, et pour capter des sons particuliers dans un environnement naturel. L'accroissement de directivité ne concerne pas les basses fréquences.Bi-directionnel ou directivité en 8 : deux sphères identiques. Le microphone bidirectionnel est utilisé le plus souvent en combinaison avec un microphone de directivité cardioïde ou omnidirectionnelle afin de créer un couple MS (voir Systèmes d'enregistrement stéréophonique). Les angles de réjection des microphones bidirectionnels permettent d'optimiser les problèmes de diaphonie lors de l'enregistrement d'instruments complexes comme la batterie par exemple.Les professionnels du son ont tendance à préférer les microphones électrostatiques aux dynamiques en studio. Ils offrent en général un rapport signal sur bruit largement supérieur et une réponse en fréquence plus large et plus étale.Pour les sources très puissantes, comme un instrument de percussion, les cuivres ou un amplificateur pour guitare électrique, un microphone dynamique a l'avantage d'encaisser de fortes pressions acoustiques. Leur robustesse les fait souvent préférer pour la scène.Avantages : robustesse, pas d'alimentation externe ni d'électronique, capacité à gérer de fortes pressions acoustiques, prix en général nettement inférieur à un microphone électrostatique de gamme équivalente.Inconvénients : manque de finesse dans les aigus le rendant inapte à prendre le son de timbres complexes : cordes, guitare acoustique, cymbales, etc.Quelques modèles de références : Les micros broadcast Shure SM7b, Electrovoice RE20 et RE27N/D très utilisés aux États-Unis et dans certaines radios nationales et locales françaises ; Shure SM-57, un standard pour la reprise d'instrument (notamment la caisse claire et la guitare électrique) et Shure SM-58 pour la voix (Micro utilisé entre autres par Mick Jagger sur Voodoo Lounge, Kurt Cobain sur Bleach etc.). Il est intéressant de savoir que ces deux micros sont identiques au niveau de la construction et que ce n'est qu'une courbe différente d'égalisation (due au filtre anti-pop qui n'existe pas sur le SM57) qui les différencie[réf. nécessaire]. Leurs versions hypercardioïdes, le BETA57 et BETA58, jouissent d'une notoriété moindre, malgré une qualité de fabrication nettement supérieure. Citons encore le Sennheiser MD-421 très réputé pour les reprises de certains instruments acoustiques (dont les cuivres) et d'amplis de guitare ou de basse.Le microphone électrostatique présente l'avantage d'excellentes réponses transitoire et bande passante, entre autres grâce à la légèreté de la partie mobile (uniquement une membrane conductrice, à comparer avec la masse de la bobine d'un microphone dynamique). Ils ont en général besoin d'une alimentation, en général une alimentation fantôme. Ils comportent souvent des options de traitement du signal telles un modulateur de directivité, un atténuateur de basses fréquences, ou encore un limiteur de volume (Pad).Les microphones électrostatiques sont plébiscités par les professionnels en raison de leur fidélité de reproduction.Les sonomètres professionnels utilisent tous des microphones à capteur de pression (omnidirectionnels) électrostatiques. Cet usage exige que le microphone soit étalonné ; le pistonphone est un appareil couramment utilisé à cette fin.Avantages : sensibilité, définition.Inconvénients : fragilité, nécessité d'une alimentation externe, contraintes d'emploi. Sauf les capteurs de pression, il est généralement fixé sur une monture à suspension faite de fils élastiques, généralement en zigzag, destinée à absorber les chocs et les vibrations. Il est rare qu'il soit utilisé comme microphone à main, sauf certains modèles qui incorporent une suspension interne.Ces caractéristiques font qu'ils sont en général plus utilisés en studio que sur scène.Quelques modèles de référence : Neumann U87ai, U89i et KM 184 (souvent en paire pour une prise stéréo), Shure KSM44, AKG C3000 et C414, Schoeps série Colette.Facilement miniaturisable, le micro à électret est très utilisé dans le domaine audiovisuel (micro cravate, micro casque, etc.) où on l'apprécie pour son rapport taille/sensibilité. Les meilleurs modèles parviennent même à rivaliser avec certains micros électrostatiques en termes de sensibilité.Les électrets actuels bénéficient d'une construction palliant cette fâcheuse espérance de vie limitée que l'électret connaît depuis les années 1970.Avantages : possibilité de miniaturisation extrême, sensibilité.Inconvénients : amoindrissement de la sensibilité au fil du temps.Quelques modèles de références : AKG C1000, Shure SM81 KSM32, Rode Videomic, Sony ECM, DPA 4006 4011.                                                Une capsule de microphone donne un signal correspondant à un point de l'espace sonore. Des agencements de capsules donnent plusieurs signaux qui permettent de représenter la direction de la source, ou d'obtenir des directivités particulières.Microphones stéréophoniques.Ensemble de 4 capsules en forme de tétraèdre donnant un goniomètre audio, et permettant de décider de la direction de l'axe et de la directivité à distance et après coup (Soundfield SPS200).Réseau de capsules alignées pour obtenir une directivité différente dans l'axe parallèle et l'axe perpendiculaire à l'alignement des capsules (Microtech Gefell KEM 970).L'hydrophone : il existe aussi des micros pour écouter les sons dans l'eau. Ces micros servent principalement à des usages militaires (écoute des bruits d'hélice pour la détection de sous-marins), à moins que l'on ne compte dans la catégorie les capteurs de Sonar.Le microphone de contact, qui capte les vibrations d'un solide comme le microphone piezzoélectrique.Un mouchard est un microphone de petite taille dissimulé afin de faire de l'espionnage.Les accessoires de microphone sontles filtres acoustiques (voir Taille de la membrane)les pieds de micro sur lesquels on peut les fixer ;les perches pour la prise de son pour l'image ;les suspensions élastiques pour éviter que le micro ne capte les vibrations de son support ;les écrans anti-pop pour éviter que le courant d'air produit par la bouche à l'émission de consonnes occlusives ou plosives « p », « b », « t » et « d » atteigne la membrane ;les bonnettes qui peuvent être en mousse de matière plastique ou des enveloppes en tissu, éventuellement double et avec poils synthétiques, pour éviter les bruits du vent et de la pluie ;les câbles de raccordement, qui doivent être de préférence souples pour éviter de transmettre des bruits ;les unités d'alimentation ;les réflecteurs paraboliques de prise de son ;les préamplificateurs de micros.Pierre Ley, « Les microphones », dans Denis Mercier (direction), Le Livre des Techniques du Son, tome 2 - La technologie, Paris, Eyrolles, 1988, 1re éd.Mario Rossi, Audio, Lausanne, Presses Polytechniques et Universitaires Romandes, 2007, 1re éd., p. 479-531 Chapitre 8, Microphones(en) Glen Ballou, Joe Ciaudelli et Volker Schmitt, « Microphones », dans Glen Ballou (direction), Handbook for Sound Engineers, New York, Focal Press, 2008, 4e éd.(de) Gehrart Boré et Stephan Peus, Mikrophone - Arbeitsweise und Ausführungsbeispiele, Berlin, Georg Neumann GmbH, 1999, 4e éd. (lire en ligne)(en) Gehrart Boré et Stephan Peus, Microphones - Methods of Operation and Type Examples, Berlin, Georg Neumann GmbH, 1999, 4e éd. (lire en ligne)(en) Ray A. Rayburn, Earle's Microphone Book : From Mono to Stereo to Surround — a Guide to Microphone Design and Application, Focal Press, 2012, 3e éd., 466 p.(en) Base de données de microphones Portail de la musique   Portail de l’électricité et de l’électronique   Portail des technologies   Portail de la radio   Portail de la télévision   Portail de l’informatique"
musique;"Un musicien ou une musicienne est une personne qui joue ou compose de la musique.En matière de musique vivante, on peut partir de l'opposition entre le musicien exécutant — chanteur ou instrumentiste — et l'auditeur de musique, dont le rôle est évidemment plus effacé. Cependant, lorsque chacun est à la fois exécutant et auditeur — par exemple, les membres d'une communauté religieuse en train de chanter des cantiques — cette distinction devient inopérante. Une deuxième opposition vient s'ajouter à la précédente et concerne les seuls musiciens : il s'agit de la distinction entre, d'une part le compositeur, qui « pense » et « écrit » la musique — pour les types de musique possédant un système de notation —, d'autre part l'interprète, qui exécute celle-ci — avec selon les cas, une plus ou moins grande part d'invention personnelle et d'improvisation. Certains musiciens célèbres, interprètes — individus ou groupes — ou compositeurs, ont laissé dans l'histoire des traces justifiant une étude biographique distincte du courant musical auquel ils appartiennent.Un chanteur utilise sa voix ;Un instrumentiste joue d'un instrument de musique ;Un compositeur compose de la musique ;Un parolier (ou auteur) écrit des textes ;Un chef, chef d'orchestre ou chef de chœur, dirige un ensemble musical ;Un auteur-compositeur-interprète écrit le texte et la musique de chansons qu'il chante lui-même.Les CFPM préparent au titre de musicien indépendant des musiques actuelles reconnu par l'État RNCP niveau IV.L'ONISEP distingue parmi les divers métiers de la musique les professionnels auxquels on peut donner la qualification de musiciens et ceux à qui on ne peut pas la donner. Chanteur Un chanteur est quelqu’un qui vocalise des sons musicaux avec un ton et une hauteur, et utilise sa propre voix pour produire de la musique. Les chanteurs peuvent chanter en solo ou en groupe et sont souvent accompagnés de musique instrumentale. La forme de chant la plus populaire aujourd’hui est le rap. En 2020, Forbes révèle que le chanteur le mieux payé au monde est un rappeur et c’est Kaynye West avec pas moins de 170 millions de dollars de revenus. Instrumentiste Techniquement parlant, un instrumentiste est toute personne qui ne joue que d’un instrument de musique, de manière professionnelle. Généralement, ils font partie d’un orchestre, groupe ou ensemble musical. En France, l’intervention d’un instrumentiste solo pour une performance coûte environ 540 euros. DJ Un disc-jockey, plus communément abrégé en DJ, est un animateur qui joue de la musique préenregistrée. En y apportant certaines modifications à l’aide d’outils électroniques, celui-ci devient un artiste qui propose ses créations ou mix à un public. En France, une prestation d’un animateur DJ pour un public privé coûte en moyenne 622 euros. Mais le DJ le mieux payé en France reste David Guetta avec un revenu annuel qui varie entre 10 et 30 millions de dollars par an. Chef d’orchestre Un chef d'orchestre dirige par ses gestes et son contact visuel une performance musicale de plusieurs instrumentistes ou chanteurs. Interprète Les interprètes sont des musiciens, chanteurs ou instrumentistes, professionnels, qui maîtrisent des répertoires d’autres artistes. Ils reproduisent des titres sur scène moyennant une rémunération sous forme de cachets ou de factures pour le musicien micro-entrepreneur ou en association. En France, une prestation d’interprète coûte entre 620 et 720 euros. Compositeur Un compositeur est un musicien qui crée des compositions musicales. Dans le langage courant ce titre est principalement réservé à ceux qui composent de la musique classique comme Mozart ou de la musique de film comme John Williams. Ceux qui écrivent la musique des chansons populaires peuvent être appelés auteurs-compositeurs. Auteur-compositeur-interprète Un auteur-compositeur-interprète écrit le texte et la musique de chansons qu'il présente lui-même à son public. Parolier Un parolier est un auteur-compositeur qui écrit des paroles pour des chansons, par opposition à un compositeur, qui compose des musiques pour des chansons. Le plus grand parolier de tous les temps est très certainement Bob Dylan, prix Nobel de littérature en 2016 « pour avoir créé dans le cadre de la grande tradition de la musique américaine de nouveaux modes d’expression poétique ».Liste de musiciensMusique Portail du travail et des métiers   Portail de la musique"
musique;"La musique classique désigne, pour le grand public, l'ensemble de la musique occidentale appelée « savante » par distinction avec la musique populaire, à partir de la musique médiévale et jusqu'à nos jours. Si une ambiguïté persiste dans la définition de ce que représente la musique classique, elle est due à l'usage systématique (et sans doute impropre) de l'adjectif « classique », qui sert également à définir une période spécifique de la musique savante occidentale, dont les œuvres datent d'entre 1750 (date de la mort de Bach) et les années 1820 (voir Classicisme et Musique de la période dite classique). On inclut ainsi dans le terme de « musique classique », dans le langage commun, l'ensemble des traditions écrites de la musique occidentale, à partir des chants grégoriens ou du système écrit de Guido d'Arezzo jusqu'aux différentes avant-gardes expérimentées depuis 1945, allant de Pierre Boulez à Thomas Adès.Le terme de « musique classique » est marqué d'une compréhension à deux niveaux. Il semble englober en son sein :Les musiques considérées comme nobles, solennelles ou vouées à des franges dominantes de la société (en témoignent l'acceptation en son sein des chants liturgiques et, plus généralement, des harmonies jugées  complexes en opposition avec une monodie plus « populaire »), qui semblent réservées à certaines salles de concert prestigieuses et certains instruments particuliers, et sont issues d'un héritage généralement voué aux classes supérieures.Les musiques qui présupposeraient une érudition et un apprentissage particulier, souvent dispensé dans les écoles de musique et les conservatoires, qu'il s'agisse d'un apprentissage pratique des instruments-phares (piano, violon, violoncelle) ou d'un apprentissage des matières théoriques (formation musicale, écriture, harmonie ou analyse).Une autre manière de distinguer la musique « classique » est d'évoquer l'importance de la tradition écrite dans son déroulement à travers les siècles : cette particularité du mode d'expression musical est moins présente dans l'expression des musiques populaires ou traditionnelles, notamment de la musique dite « légère ».L'acceptation du terme de musique ""classique"" correspond de nos jours à une synecdoque généralisée : on entend par ""musique classique"" l'ensemble des musiques savantes composées depuis le Moyen Âge, mais le terme se confond avec un adjectif qui, du point de vue musicologique, fait conventionnellement référence au « classicisme », période souvent considérée comme la plus représentative du genre par le grand public. Si l'utilisation du terme de ""musique savante"" permettrait d'évoquer la complexité de cette musique en comparaison aux musiques populaires tout en rappelant que ce genre continue d'évoluer encore aujourd'hui, le terme de ""musique classique"" reste le plus populaire, et est automatiquement assimilé à des œuvres comme Les Quatre Saisons de Vivaldi, ou aux compositions de Bach, Mozart ou Beethoven. Dans les faits, et tel que le genre est défini dans les écoles de musiques et conservatoires, la musique ""classique"" rassemble toutes les musiques savantes occidentales transmises par des partitions (à la différence des musiques populaires et traditionnelles qui se transmettent le plus souvent par l'oral), à partir des chants monodiques, avec le système tonal et, par la suite, avec les systèmes atonal, modal, polytonal ou dodécaphonique. L'utilisation du terme de musique classique est plutôt récente, et on suggère que l’expansion de l'industrie musicale de la seconde moitié de XXe siècle est à l’origine du changement de terminologie : l'adjectif « classique »  remplace l'adjectif « savante » utilisé auparavant, afin de rompre avec la perception des pratiques liées à ce genre musical.Pour définir la musique classique, qui est d'ordinaire considérée comme ""savante"", ""sérieuse"" ou ""grande"", il faudrait pouvoir la confronter à une musique qui n'entrerait dans aucun de ces trois adjectifs ; or, si on a souvent opposé musique savante et musique populaire, la frontière qui délimite leurs deux champs se trouve souvent être mince. La musique classique sait s'inspirer de thèmes populaires (on peut penser, au XVIIIe siècle, au concerto comique de Michel Corrette La servante au bon tabac qui s'inspire du thème populaire J'ai du bon tabac (1733), ou de façon bien plus renommée, à l'appropriation du thème Ah ! vous dirai-je, maman dans les variations de Mozart (1781/1782)), et l'influence des musiques populaires sur le genre est perceptible dès la Renaissance, en voyant par exemple les motifs musicaux de la villanelle être repris dans les livres de madrigaux. Dans le sens inverse, la musique savante peut s'ancrer dans le monde séculier, et elle le fait souvent grâce aux mélodies les plus connues de son répertoire : par exemple, le chœur Va, pensiero de Nabucco a été utilisé par les révolutionnaires italiens lors du Risorgimento, et  les supporters du FC Nantes ou des Girondins de Bordeaux utilisent la « Marche triomphale » d'Aïda du même Giuseppe Verdi dans les chants qu'ils entonnent pendant les matchs : cette conjonction immédiate de deux mondes que l'imaginaire commun a tendance à nettement séparer, renforce l'idée que musique classique et musique populaire ne sont pas deux termes satisfaisants pour rendre compte des phénomènes en jeu.La musique classique trouve son origine tant dans le chant grégorien que dans la musique profane des troubadours et trouvères médiévaux. Les musiciens sont d'abord des nobles, puis des roturiers éclairés, cultivés, pratiquant un art de la composition que l'on ne peut pas qualifier de populaire, car rarement adressé au peuple lui-même. L'art des troubadours et des trouvères ne se confond pas alors avec celui des « ménestrels », musiciens ambulants populaires, formés dans les nombreuses écoles de « ménestrandie », ancêtres des académies et conservatoires actuels. Dès les débuts, la distinction entre « populaire » et « savant » semble moins s'ancrer sur une différence liée à la musique elle-même, ou à sa qualité, mais sur une différence de statut social qui précéderait la création de la musique : les ménestrels pratiquant un art tout autant appris que celui des troubadours/trouvères, la seule vraie différence se situe dans le milieu social vers lequel ils s'adressent.En latin, le sens premier de ""classis"" est l'appel  des citoyens pour la défense du pays, puis par extension les différentes classes de citoyens susceptibles d'être appelés. Le pluriel de classis,  ""classici"", désigna les citoyens appartenant à la première des classes créées par Servius Tullus ; de là le sens de ""scriptores classici"" qui désignait les  ""écrivains de première classe"", d'où les ""classiques"". L'adjectif « classique » pourrait ainsi désigner, tel que le Dictionnaire de l'Académie française l'appliquait à la littérature, « un auteur ancien fort approuvé et qui fait autorité dans la matière qu’il traite ». L'autorité acquise par ces auteurs classiques (souvent les antiques grecs et romains) sur les créateurs des XVIIe et XVIIIe, pourrait peut-être se retrouver dans la relation entre l'opinion commune actuelle et les œuvres de Mozart ou Beethoven. Ainsi, l'opinion commune continue à identifier la musique ""classique"" de façon assez claire et stéréotypée, sans tenir compte des nombreuses évolutions dont a pu témoigner l'histoire de la musique savante occidentale, du Moyen Âge à nos jours : le terme désigne aujourd'hui, des œuvres souvent datées d'au moins un siècle, transmises à l'écrit par des partitions, et qui exigeraient une écoute « attentive ». Le degré de raffinement et d'éducation nécessaire pour écouter de la musique classique serait décisif : le décorum des concerts de musique classique en serait la preuve, puisqu'il se distingue par des règles très précises (l'absence de communication entre les interprètes et la musique en comparaison avec les musiques populaires, l'interdiction d'applaudir entre les mouvements d'une pièce, et l'interdiction communément admise de danser, parler ou chanter pendant que la pièce est jouée).Outre l'emploi conscient de techniques musicales et d'une organisation formelle hautement développées, c'est probablement l'existence d'un répertoire qui différencie le plus sûrement la musique classique de la musique populaire, et ce, depuis le début de la Renaissance. La musique d'essence populaire était alors peu ou pas écrite,  transmise à l'oral, ce qui a limité la constitution d'un répertoire fixé dans un temps long. De même que pour les contes collectés par les frères Grimm, le travail de Bartok et Kodaly au début du XXe siècle a consisté à ""récupérer"" les thèmes populaires des campagnes slaves pour les empêcher de tomber dans l'oubli : là où les frères Grimm ont écrit avec des mots, Bartok et Kodaly ont pu éditer des retranscriptions et permettre à ce patrimoine de subsister.De nos jours, le problème peut s'envisager autrement : la musique populaire gravée sur disque ne laisse place qu'à une seule interprétation admise par tous (les versions produites en concert, à l'exception des enregistrements de concerts eux-mêmes sortis en disques, ne sont considérées que comme des reproductions d'une autre musique), quand la tradition musicale savante différencie l'interprète du compositeur, et laisse les interprètes produire des versions à chaque fois différentes d'une même pièce, en attribuant à chaque interprétation le même crédit et la même légitimité. La musique savante occidentale ne semble donc pas se définir par une écoute ou une archive sonore comme la musique populaire actuelle : elle trouve sa source dans un répertoire écrit sans cesse renouvelé, autant par les compositeurs qui élargissent le catalogue disponible, que par les interprètes qui proposent sans cesse des nouvelles versions de pièces potentiellement déjà jouées des milliers de fois.La musique classique disposerait donc de ce que Nicholas Cook a appelé un « capital esthétique », c’est-à-dire un répertoire, de par la distinction entre interprète et compositeur, tandis que la musique populaire serait écrite pour ou par un musicien ou un groupe de musiciens pour lui-même,.Toujours d'après Nicholas Cook, la conception de la musique dont notre époque a hérité date du XIXe siècle, et tient principalement au personnage de Ludwig van Beethoven. La notion de répertoire, de « musée musical » dont Liszt réclamera la fondation en 1835 en tant qu'institution, n'aurait pas existé avant l'ère romantique. Ainsi, des compositeurs tels que Jean-Philippe Rameau, Jean-Sébastien Bach ou Joseph Haydn écrivaient leurs œuvres pour une occasion précise (la messe du dimanche ou le dîner du prince Esterházy par exemple), et le rituel du ""concert"" comme exacerbation des émotions du public, ne s'est développé que récemment. La Passion selon saint Matthieu, dont l'exécution en 1829 par Felix Mendelssohn était la première depuis la création de l'œuvre cent ans plus tôt, a été présentée au public moins comme un objet de culte et une représentation de la Passion du Christ, que comme un objet musical digne d'intérêt esthétique ""pur"". Ce que souligne ainsi Nicholas Cook, c'est que le terme de musique classique a été créé pour désigner les œuvres de ce musée musical imaginaire, musée qui n'existait pas avant que le XIXe siècle ne métamorphose la perception qu'avait le public de la majorité de la musique savante. La notion de musique classique aurait donc été formée a posteriori de la moitié de la musique qu'elle est censée désigner, et serait donc plus que sujette à caution.S'il est difficile de vraiment considérer la musique du Moyen Âge comme faisant partie de la musique « classique », il faut reconnaître que ses principes sont l'origine même des langages musicaux employés depuis lors. La musique savante occidentale commence (au-delà des rares traces de musiques antiques, comme l'Épitaphe de Seikilos) avec la monodie médiévale, et c'est à partir d'elle que l'ensemble des innovations suivantes s'ancrent.D'elle vient également la relation difficile à entremêler entre la musique savante et la musique cérémonielle, vouée aux cérémonies religieuses : la difficulté pour nous de différencier musique « classique » et musique « populaire » vient sans doute du manque de précision dont fait preuve un terme comme « musique médiévale », qui confond tous les styles, tous les ensembles d'instruments, et toutes les occasions différentes auxquelles la musique était dédiée.Jean-François Paillard, La Musique française classique - collection Que sais-je ?, no 878 - PUFNicholas Cook (trad. de l'anglais par Nathalie Getnili), Musique, une très brève introduction [« Music, a very short introduction »], Paris, Allia, 2006, 155 p. (ISBN 2-84485-206-8, OCLC 227201878, BNF 40094540)John Burrows, Charles Wiffen, La Musique classique, Gründ, 2006, Collection Le Spécialiste, 512 p.  (ISBN 978-2-7000-1347-4)Patrick Hauer, Dictionnaire des grands compositeurs et leurs œuvres, du XVIIe siècle au XXe siècle, éditions Dictionnaires d'aujourd'hui, 2007, 660 p.Régis Chesneau, Pour en finir avec le « classique », Paris, Éditions L'Harmattan, coll. « Logiques sociales : Série Musiques et champ social », 2019, 200 p. (ISBN 9782140123535, OCLC 1237118810, lire en ligne)Hyacinthe Ravet, « La petite musique du genre, ou comment combattre le sexisme dans la musique classique », sur The Conversation, 16 octobre 2019.Chronologie de la musique classique occidentale Portail de la musique classique   Portail des arts"
musique;"Une œuvre de musique de chambre est une composition musicale dédiée à un petit ensemble de cordes, bois, cuivres ou percussions, qui traditionnellement et avant l'affirmation des concerts publics, pouvait tenir dans la « grande chambre » d'un palais. Chaque partie est écrite pour un seul instrumentiste sans qu'il n'y ait à l'origine ni chef d'orchestre ni soliste attitré. Si certaines voix sont doublées ou triplées, particulièrement dans les cordes, on parle d'orchestre de chambre ; au-delà, on parle d'orchestre, avec le qualificatif de sa composition instrumentale (symphonique, à cordes, d'harmonie, de fanfare).La conception de ces ensembles -  duos, trios, quatuors, quintettes, sextuors, septuors, octuors, nonettes et dixtuors - demande aux compositeurs une connaissance de l'harmonie, de la polyphonie, du contrepoint, de l'organologie. En musique classique, la composition de quatuors à cordes est le plus souvent considérée comme l'un des summums de l'écriture musicale[réf. nécessaire].Ces ensembles de quelques solistes, menant leur voix indépendamment des autres, ont le plus souvent comme « meneur » l'instrument le plus aigu (le premier violon dans le quatuor à cordes, le flûtiste dans le quintette à vent), mais certaines œuvres plus complexes ou d'un effectif plus important, nécessitent la présence (toujours facultative, mais souvent essentielle) d'un chef d'orchestre (Gran Partita pour 13 instruments à vent en si bémol majeur de Mozart, Sérénade en ré mineur, op. 44 pour vents, violoncelle et contrebasse de Dvořák).En principe, le terme exclut les pièces comportant une partie chantée, malgré le caractère intimiste que revêtent souvent la mélodie et le lied. Certains compositeurs des XXe et XXIe siècles cependant, introduisent la voix dans des ensembles de musique de chambre, comme Schönberg dans son deuxième quatuor à cordes, op. 10 (1908), Ottorino Respighi dans Il tramonto (1914), Olivier Greif dans son Quatuor à cordes no 3 avec voix (1998) et Elena Ruehr dans son deuxième quatuor « Song of the Silkie » (2000). Dans certaines œuvres récentes, le compositeur choisit d'adjoindre une partie de sons fixés (bande magnétique ou CD par exemple) aux instruments acoustiques, voire de les amplifier à l'aide de microphones, ou même de transformer leur son grâce à l'ordinateur[réf. nécessaire].L'expression « musique de chambre » apparaît à la période baroque, les compositions qu'elle recouvre étant destinées à être jouées dans l'intimité des intérieurs de nobles ou d'amateurs fortunés. Dès le Moyen Âge et pendant la Renaissance de nombreux ensembles de cordes, de bois, de cuivres, mixtes ou en consort se forment et jouent indépendamment de la polyphonie vocale, mais suivant le plus souvent ses règles d'écriture à quatre voix (soprano, alto, ténor et basse). Ces compositions de camera s'opposent alors aux œuvres de la chiesa, de la chapelle et ainsi apparaissent comme l'expression d'une musique profane. Les sonates de Giovanni Legrenzi donnent leur titre définitif à ces compositions. A la fin du XVIIIe siècle, le quatuor à cordes devient l'expression la plus aboutie de la composition pour amateurs. Mozart et Haydn introduisent des formes qui séparent nettement la musique de chambre des symphonies pour orchestre.Au tournant du  XXe siècle, avec Gustav Mahler et surtout Arnold Schönberg, elle quitte les espaces privés pour rejoindre l'orchestre et de nouveaux territoires.Il existe de nombreux types de duos qui peuvent porter des titres les plus divers :du même instrument : souvent écrits dans un but pédagogique, certains ont dépassé le cadre d'une classe instrumentale, par exemple ceux pour 2 cors de Mozart, 2 violoncelles d'Offenbach, ceux pour 2 clarinettes ou 2 violons de Bartók ou pour 2 saxophones de Hindemith. Le piano est bien sûr un cas particulier avec des œuvres écrites soit pour piano quatre mains soit pour deux pianos ; il existe là un répertoire de concert spécifique et très varié, d'œuvres originales comme les Marches militaires de Schubert, les Danses hongroises de Brahms, les Danses slaves de Dvořák pour citer les plus connues) ou d'arrangements parfois faits par les auteurs eux-mêmes comme Ravel et son Boléro ;de deux instruments différents :pour deux instruments mélodiques, ce sont souvent des œuvres de commande pour deux amis (…),les plus fréquents associent un instrument mélodique et un instrument polyphonique comme l'immense répertoire des sonates pour instrument et clavier (piano, clavecin mais aussi orgue, harpe, guitare…). Les premiers exemples sont pour « dessus et basse continue » : le « dessus » pouvant être un violon, une flûte à bec ou traversière, un hautbois, une musette… ; la « basse continue » peut comprendre plusieurs instrumentistes (théorbe, viole de gambe, épinette, violoncelle, clavecin, basson ou guitare…) mais elle est toujours considérée comme une seule voix puisque tous doublent la partie de basse. Les sonates existent pour tous types de combinaisons et certains compositeurs comme Paul Hindemith se sont donné pour but d'en écrire pour chacun des instruments de l'orchestre.En musique baroque, la sonate en trio (sonata a tre, triosonate) désigne une écriture à trois parties, mais pouvant être jouée par :un musicien, par exemple les Six sonates en trio pour orgue de Jean-Sébastien Bach, (BWV 525 à BWV 530), une main par clavier plus la basse au pédalier ;deux musiciens, sonates pour violon et clavecin concertant (BWV 1014-1019), une voix pour le violon, une pour la main droite et une pour la main gauche du clavecin ; cette dernière, sortant du rôle de basse continue, peut être occasionnellement doublée par un violoncelle ou un basson ;trois musiciens, toutes solutions possibles sans basse continue ;quatre (ou plus) musiciens, la « sonate en trio » est alors écrite pour deux dessus et une basse continue, cette dernière étant jouée par une basse mélodique (viole de gambe, violoncelle, basson…) et un ou plusieurs instruments polyphoniques (théorbe, guitare, épinette, clavecin, positif, orgue…) ;cinq musiciens, la « sonate en trio » est alors écrite pour trois instruments mélodiques et une basse continue, cette dernière étant jouée par une basse mélodique (viole de gambe, violoncelle, basson…) et un ou plusieurs instruments polyphoniques (théorbe, guitare, épinette, clavecin, positif, orgue…). Exemples : les sonates en trio pour 2 hautbois, basson et basse continue de Fasch, Heinichen, Zelenka, avec une basse harmonisée au clavecin et / ou au théorbe, doublée par une contrebasse ou un violoncelle…Par la suite, ce sont surtout les trios avec piano qui dominent, mais on trouve aussi des ensembles d'instruments de même famille comme le trio à cordes (violon, alto et violoncelle), le trio d'anches (hautbois, clarinette, basson), le trio de cuivres (trompette, cor, trombone). Certains compositeurs ont usé de formules plus audacieuses comme Mozart avec ses « trios pour cors de basset », Haydn et ses 126 Barytontrios pour baryton à cordes, alto et basse, Beethoven et son trio pour deux hautbois, cor anglais… Au cours du XXe siècle, des trios beaucoup plus audacieux (…).Idéal de la musique occidentale classique, cette formule met en valeur au maximum le potentiel de la musique polyphonique et harmonique. Elle fut d'abord vocale, puis instrumentale et basée sur l'écriture à quatre voix superposées soprano, alto, ténor et basse. La formation reine de la musique de chambre est évidemment le quatuor à cordes, dont l'immense répertoire est à la fois un modèle, un but à atteindre et une source d'inquiétude pour chaque compositeur qui ose s'y risquer…Il existe de nombreux autres quatuors d'instruments de même famille : on trouve ainsi des quatuors de saxophones, de trombones, de clarinettes, de violoncelles, de flûtes, de percussions...Il existe des quatuors hétérogènes, comme le quatuor avec piano (violon, alto, violoncelle et piano), le quatuor d'anches (hautbois, clarinette, saxophone et basson), voire des ensembles combinant bois, cordes et instruments polyphoniques comme le Quartette op. 22 d'Anton Webern (violon, clarinette, saxophone et piano).Les quintettes sont souvent une extension des quatuors à un autre instrument, comme les quintettes pour quatuor à cordes et contrebasse (une sorte de « petit orchestre »), piano, ou clarinette par exemple.Il existe cependant deux formules entièrement originales : le quintette à vent (flûte, hautbois, clarinette, cor et basson) et le quintette de cuivres (deux trompettes, cor, trombone et tuba).Les formules les plus connues sont pour cordes, mais aussi pour percussions ou clarinettes. Ils sont souvent l'occasion pour un compositeur d'entremêler des sonorités très variées, comme dans le Sextetto mistico de Heitor Villa-Lobos (flûte, hautbois, saxophone, célesta, harpe et guitare) ou le Septuor pour cordes, piano et trompette de Camille Saint-Saëns. Au-delà de sept instruments, on rejoint souvent les « petits ensembles », qui nécessitent en général la présence d'un chef d'orchestre, quoique Mozart (entre autres) ait écrit des octuors d'instruments à vent.Florent Albrecht, Festivals de musique de chambre en France : dynamiques et enjeux contemporains, L'Harmattan, Paris, Budapest, Torino, 2003, 197 p.  (ISBN 2-7475-5191-1)Walter Willson Cobbett, Dictionnaire encyclopédique de la musique de chambre, complété sous la direction de Colin Mason, traduit de l'anglais par Marie-Stella Pâris, édition française revue et augmentée par Alain Pâris, Robert Laffont, coll. « Bouquins », Paris, 1999, 2 vol., 1627 p.  (ISBN 2-221-07847-0) (vol. 1 : A-J) ;  (ISBN 2-221-07848-9) (vol. 2 : K-Z)André Cœuroy et Claude Rostand, Les chefs-d'œuvre de la musique de chambre, Éd. Le Bon Plaisir, Plon, Paris, 1952, 282 p.Joël-Marie Fauquet, Les Sociétés de musique de chambre à Paris de la Restauration à 1870, Aux amateurs de livres, Paris, 1986, 448 p.  (ISBN 2-905053-25-9) (texte remanié d'une thèse de 3e cycle de Musicologie, Paris 4, 1981)Gérard Pernon, Dictionnaire de la musique, Paris, Jean-Paul Gisserot, Coll. Histoire de la musique, 2007, p. 50-51.Serge Gut et Danièle Pistone, La musique de chambre en France : de 1870 à 1918, H. Champion, Paris, 1985, 239 p.  (ISBN 2-85203-048-9)François-René Tranchefort (dir.), Guide de la musique de chambre, Fayard, Paris, 1989 (plusieurs rééd.), 995 p.  (ISBN 2-213-02403-0)Marc Vignal (dir.), Larousse de la musique, 1982 -  (ISBN 2-03-511303-2)Mozart et la musique de chambre, films documentaires réalisés par Alex Szalat, ADAV, Paris, 1991, 2 parties : 1. L'Enfant de l'Europe ; L'Art de la fugue (1 cassette vidéo VHS, 2 × 52 min) ; 2. À mon cher ami Haydn ; Histoire d'un texte ; Mozart et les quintettes (1 cassette vidéo VHS, 3 × 52 min) Portail de la musique classique"
musique;""
musique;La mélodie est une succession de sons ordonnés selon des rapports de rythme et de modulation par opposition à l'harmonie consistant dans l'accord de plusieurs sons exécutés simultanément. Le terme « mélodie » vient du latin melodia issu du grec ancien μελῳδία / melôidía, « chant », composé de μέλος / mélos, « arrangement musical », et ᾠδή / ôidế, « chant ».Dans la musique occidentale, chaque note d'une mélodie est déterminée par l'intervalle mélodique qui la sépare de la note fondamentale — ou note de référence — appelée « tonique » dans la musique tonale. Le mot mélodie s'oppose ainsi à la polyphonie — ou harmonie, ces deux termes pris dans leur sens le plus large de « procédé musical utilisant les simultanéités délibérées » —, cette technique d'écriture constituant l'une des singularités de la musique occidentale depuis le milieu du Moyen Âge.Du XIIIe au XVIe siècle, la musique occidentale est plus précisément dite polyphonique et modale, et son procédé de composition est appelé contrepoint. Une pièce musicale de cette époque, peut être considérée comme une « superposition de mélodies », chacune d'elles étant exécutée par les différentes parties de l'ensemble.Après la Renaissance, la musique est harmonique et son procédé de composition est le système tonal. Avec cette nouvelle technique, une partie se détache des autres pour exécuter la ligne mélodique principale (ou simplement mélodie) — généralement située à la partie supérieure de l'édifice. Dans ce nouveau sens, la mélodie s'oppose donc à la basse continue ainsi qu'aux différentes parties intermédiaires qui constituent l'accompagnement de la ligne principale — accompagnement pouvant être synthétisé en une série d'accords. On parle alors de « mélodie accompagnée ».ModesRythmeHarmoniePolyphonieMélodie (genre)Claude Abromont et Eugène de Montalembert, Guide de la théorie de la musique, Librairie Arthème Fayard et Éditions Henry Lemoine, coll. « Les indispensables de la musique », 2001, 608 p. [détail des éditions] (ISBN 978-2-213-60977-5) Portail de la musique classique   Portail de la musique   Portail des arts
musique;"En musique, une note est un symbole ou une lettre permettant de représenter un fragment de musique par une convention d'écriture de la hauteur et de la durée d'un son.Le symbole visuel d'une note de musique sur une partition est constitué : d'une « tête » qui indique la hauteur du son (sa fréquence en hertz),d'une « hampe » qui soutient les crochets et les barres de durée,d'une « durée », matérialisée par un ou des crochets ou par une ou plusieurs barres horizontales ou inclinées, qui indique sa longueur ou durée temporelle.La « tête » est généralement de forme ronde ovalisée et peut être évidée (ronde, blanche…) ou pleine (noire, croche…). Pour certains instruments, elle peut prendre diverses formes comme pour la percussion. Par exemple, le symbole  : croix est utilisé pour les cymbales,triangle pour les triangles,rectangle pour le tambourin,…La « hampe » est un trait vertical fin attaché à toutes les têtes de notes, sauf à la ronde. En règle générale, elle est placée au-dessus et à droite de la tête quand la tête de note est en dessous de la troisième ligne de la portée, et en dessous et à gauche quand la tête de note est sur ou au-dessus de la troisième ligne.La « durée » des notes est matérialisée par un crochet simple ou des crochets multiples superposés, ou par une barre ou plusieurs barres épaisses parallèles, horizontales ou inclinées. L'assemblage des « durées » de notes constitue le rythme : à partir de la croche, le nombre de crochets (pour une note) ou de barres (pour un ensemble de notes) détermine la durée rythmique de la note : un pour la croche, deux pour la double-croche, trois pour la triple-croche, etc. Pour la ronde, la blanche et la noire, pointées ou non, qui n'ont pas de symbole de « durée » (crochet ou barres horizontales), la longueur de la note est donnée par le fait que la tête de note soit remplie ou pas, et/ou par le fait qu'elle ait une hampe ou pas.L'assemblage des « têtes » de notes disposées à différentes hauteurs — qui donne la mélodie — et des « durées » de notes — qui donnent le rythme — forment la partition et le solfège, qui sont destinés à être lus et déchiffrés par le musicien interprète.En plus de la hauteur et de la durée, les notes font l'objet d'autres effets acoustiques créés par la manière de jouer du musicien, comme d'une part, l'intensité ou nuance, indiquée sur la partition par des lettres placées à proximité des notes, comme  pour piano,  pour fortissimo, etc., et du phrasé musical d'autre part (attaques douces, attaques brutales, notes piquées, accents, etc.) indiqué par des symboles musicaux spécifiques (points, traits, chevrons, etc.). Enfin, une même note jouée par divers instruments, certes de même hauteur, se différencie également par son timbre, spécifique à chaque instrument (timbre doux pour la flûte, percutant pour le piano quand il est martelé, féerique pour le glockenspiel, puissant et cuivré pour la trompette, plus sourd pour le cor, etc.).Des « têtes » de notes placées les unes après les autres, de gauche à droite, sont jouées successivement, ce qui forme une mélodie : chaque instrument d'un orchestre joue sa mélodie. Mais lorsque plusieurs « têtes » de notes sont superposées, elle doivent être jouées simultanément, ce qui crée un accord. C'est le cas pour un pianiste qui joue plusieurs notes en même temps avec une main, ou pour un ensemble instrumental où chaque musicien joue une des notes de l'accord. Tout son musical (ou note) possède une fréquence fondamentale (nombre de vibrations par seconde calculé en hertz) correspondant à sa hauteur.Tous les instruments de musique fonctionnent selon l'équation des cordes vibrantes, qui donne la fréquence du son joué en fonction des paramètres géométriques de construction de l'instrument (longueur de corde, diamètre de tuyau, etc.). Cette équation admet un ensemble de solutions de fréquences multiples de la fréquence dite fondamentale, qui correspond à la vibration du système vibrant (corde, tuyau, etc.).Lorsqu'un son donné de fréquence                               f                      0                                {\displaystyle f_{0}}   est joué, l'ensemble des fréquences                               f                      0                                {\displaystyle f_{0}}  ,                     2                  f                      0                                {\displaystyle 2f_{0}}  ,                     3                  f                      0                                {\displaystyle 3f_{0}}  ,                     4                  f                      0                                {\displaystyle 4f_{0}}  , etc., sont également émises par l'instrument. La note correspondant à la fréquence                               f                      0                                {\displaystyle f_{0}}   est appelée fondamentale, et les autres sont ses harmoniques. L'ensemble de ces fréquences, associées à leur intensité (c'est-à-dire si on les entend fort ou non) est appelé spectre, ou plus communément timbre.Dans un son de fondamentale                               f                      0                                {\displaystyle f_{0}}  , les harmoniques les plus présentes sont l'octave (                    2                  f                      0                                {\displaystyle 2f_{0}}  ), la quinte (                    3                  f                      0                                {\displaystyle 3f_{0}}  ), l'octave (                    4                  f                      0                                {\displaystyle 4f_{0}}  ), la tierce majeure (                    5                  f                      0                                {\displaystyle 5f_{0}}  ), etc..Deux notes dont les fréquences fondamentales ont un rapport qui est une puissance de deux (c'est-à-dire la moitié, le double, le quadruple…) donnent deux sons très similaires et portent le même nom. Cette observation permet de regrouper toutes les notes qui ont cette propriété dans la même catégorie de hauteur.Dans la musique occidentale, douze fréquences fondamentales différentes portent un nom. Sept d'entre elles sont considérées comme les principales et ont pour noms : do, ré, mi, fa, sol, la et si. Elles correspondent aux harmoniques naturels lorsque la note do est jouée.Les cinq notes restantes sont dites « altérations » et sont des notes intermédiaires. Par exemple, la  est une note de fréquence intermédiaire entre le la et le si.Pour distinguer deux notes de même nom dans deux octaves différentes, on numérote les octaves et donne ce numéro aux notes correspondantes : par exemple, le la3 a une fréquence de 440 hertz définie dans la norme internationale ISO 16 (bien qu'en pratique, cela puisse parfois varier). Cette fréquence de référence est donnée par un diapason.Depuis le XVIIe siècle, on considère que les notes sont également réparties sur une octave, c'est-à-dire que le rapport de fréquences entre une note et la suivante est de                                           2                          12                                            {\displaystyle {\sqrt[{12}]{2}}}   (exemple : la3 = 440 Hz ; la 3 = 466,16 Hz).La définition de l'écart entre les notes est ce que l'on appelle le tempérament, et lorsque l'écart entre une note et la suivante (en termes de fréquences) est toujours identique, ce tempérament est dit « égal ».Dans la gamme tempérée, la formule permettant de mesurer la fréquence d'une note par rapport à une note de départ est :                               f                      n                          =                  f                      0                          ×                  2                      n                          /                        12                                {\displaystyle f_{n}=f_{0}\times 2^{n/12}}  . Avec                     n              {\displaystyle n}   le nombre de demi-tons au-dessus de la note de départ                               f                      0                                {\displaystyle f_{0}}  . On s'aperçoit que la fréquence croît de manière géométrique par rapport à la note.De ce fait, chaque demi-ton correspond à une augmentation / diminution de la fréquence suivant un rapport de 1,0594630943 : 1 de la note voisine.Néanmoins, le tempérament égal a ses limites : lorsqu'une note est jouée, les harmoniques 3 et 5 (la quinte et la tierce) sont audibles, et leur fréquence est de                     3                  f                      0                                {\displaystyle 3f_{0}}   et de                     5                  f                      0                                {\displaystyle 5f_{0}}  .Par exemple, pour un la3 (440 Hz), on entend également la quinte (1 320 Hz) et la tierce majeure (2 200 Hz). Cette quinte et cette tierce se retrouvent également à la moitié ou le quart de leur fréquence (une octave en dessous) : 660 Hz (mi4) et 550 Hz (do 4).Or, dans la gamme tempérée (à tempérament égal), le mi étant la quinte du la, elle est séparée du la d'un facteur                               2                      7                          /                        12                                {\displaystyle 2^{7/12}}  , c'est-à-dire que le mi4 a sa fondamentale à 659,25 Hz, et le do 4 a sa fondamentale à 554,37 Hz.On remarque alors que si on joue une tierce harmonique (la4 et do 4 simultanément par exemple) avec un tempérament égal, l'harmonique de tierce de la fondamentale et la fréquence fondamentale de la tierce ne sont pas à la même fréquence.Dans la théorie de la musique, on parle de « degré ». Celui-ci représente une hauteur relative appartenant à une échelle musicale donnée. En effet, il existe de nombreuses possibilités pour choisir les fréquences des notes dans une octave. Le choix des échelles dépend des époques, des instruments et des types de musique.Selon l'échelle, on obtiendra des gammes musicales différentes. La musique classique en utilise deux : l'échelle diatonique et l'échelle chromatique.Pour nommer les notes de musique, la musique occidentale utilise deux systèmes différents, selon le pays :le premier système, inspiré de l'Antiquité, utilise les premières lettres de l'alphabet. Il est en vigueur, dans deux variantes simplifiées (ne différant que par la désignation du si), dans les pays anglophones et germanophones ;le second système utilise les syllabes d'un chant latin. Il a été élaboré pendant la deuxième moitié du Moyen Âge et il est en usage en France, en Italie, etc.Depuis Guido d'Arezzo, les notes de musique peuvent être désignées par ut, ré, mi, fa, sol, la. Cette pratique a été standardisée par les recommandations du pape Jean XIX. Auparavant, en Occident, divers systèmes de notation existaient. La nouvelle méthode permettait d'apprendre en un jour ce qu'il fallait un an pour apprendre avec la méthode grecque utilisant des lettres pour noter tant les tons que les échelles.La série constituée des syllabes ut, ré, mi, fa, sol, la (le si a été ajouté plus tard), promue par le moine bénédictin italien Guido d'Arezzo au XIe siècle, a été mise en place pour la notation musicale dans les pays de rite catholique dit « latin ». Cette série est constituée des premières syllabes de chaque demi-vers de l’Hymne à saint Jean-Baptiste, un chant religieux latin attribué au moine et érudit Paul Diacre : L’ut, renommé do, est surtout utilisé dans le langage théorique : clés, tonalités…La septième note si fut ainsi dénommée à partir des initiales de Sancte Ioannes dans le dernier vers. C'est plus tard, en Italie, que le nom ut, seule note de la gamme sans consonne en son début pour marquer l'attaque de celle-ci, a été remplacé par la syllabe do, à la diction plus aisée. Son origine exacte reste inconnue, mais do pourrait être la première syllabe de Domine : Seigneur, donc Dieu, en latin. En outre, selon certains, cette syllabe ouverte contrairement au ut fermé, serait plus facile à émettre, pour les besoins du déchiffrage chanté de musiques sans paroles, dans l'apprentissage du solfège. Depuis lors, on parle de la note do (ou do bémol, do dièse, do bécarre, etc.), mais on évoque toujours la clé d'ut. Quant aux tonalités, les deux noms coexistent : ut majeur ou do majeur, ut mineur ou do mineur, etc.En général, la musique est souvent représentée par une ou des notes, que ce soit dans la bande dessinée, pour symboliser un chant, ou dans l'informatique, pour spécifier que le fichier est un fichier musical.La représentation des symboles musicaux en informatique existe dans différents jeux de caractère (Unicode, LaTeX, LilyPond…). Par exemple, l'encodage en Unicode, pour ♩ (une noire), ♪ (une croche), ♫ (deux croches) et ♬ (deux doubles-croches) :Il existe également des notes dans la table des emojis.Cependant, cet encodage ne permet pas le positionnement sur la portée, ni aucune autre manière de distinguer les hauteurs de son. Pour cela, on emploie d'autres normes. Par exemple, un des aspects de la norme MIDI est la numérotation des notes. On dit alors que le la 440 Hz est le numéro 69 et que toute différence de numéro par rapport à cette note est comptée en demi-tons. Par exemple, tous les multiples de 12 sont des do. À l'origine, seuls les numéros de 0 à 127 étaient permis, mais selon la variante de cette échelle, des notes plus graves ou plus aiguës peuvent aussi être permises. À partir du moment où on suppose un tempérament égal (gamme tempérée), on peut aussi représenter des notes intermédiaires en utilisant des fractions (voir la formule de fréquence mentionnée ci-haut).D'autres notations informatisées utilisent les lettres de notes anglaises de A à G et le symbole # tenant lieu de dièse, mais pas de symbole bémol car non-nécessaire (tout bémol a un équivalent dièse). Les notes non-dièse peuvent être suivies d'un trait d'union ou d'une espace. On termine ensuite avec le numéro d'octave, qui augmente de 1 à chaque do comme dans la table ci-haut, mais qui peuvent être décalés (le la 440 pourrait être écrit A-2, A-3 ou A-4 selon l'échelle choisie). C'est ce qui est utilisé visuellement dans les éditeurs de type tracker (à la Amiga), quoique à l'interne, leurs formats utilisent des périodes (inverses de fréquences) comme le format MOD, ou une combinaison note de la gamme et octave (par exemple, les deux parties de la division avec reste d'une note MIDI par 12) comme le format S3M.Adolphe Danhauser, Théorie de la musique : Édition revue et corrigée par Henri Rabaud, Paris, Éditions Henry Lemoine, 1929, 128 p. (ISMN 979-0-2309-2226-5).Claude Abromont et Eugène de Montalembert, Guide de la théorie de la musique, Librairie Arthème Fayard et Éditions Henry Lemoine, coll. « Les indispensables de la musique », 2001, 608 p. [détail des éditions] (ISBN 978-2-213-60977-5).Acoustique musicaleDésignation des notes de musique suivant la langueHistoire de la notation musicaleLes notes, les altérations, les tons, le B.-A. BA Portail de la musique classique   Portail de la musique   Portail de l’écriture"
musique;"Un orchestre symphonique, ou orchestre philharmonique, est un ensemble musical formé des trois familles d'instruments : cordes, instruments à vent (bois et cuivres) et percussions. La composition des orchestres a beaucoup varié jusqu'à la fin de la période baroque. Ils associaient les instruments les plus divers ainsi qu'en témoignent les multiples combinaisons des six Concertos brandebourgeois (1721) de Bach. Ce n'est pas avant le milieu du XVIIIe siècle que la forme de l'orchestre symphonique commence à se figer et à s'étoffer progressivement de hautbois, de bassons, parfois de cors, de trompettes, et de timbales.La période classique avec Gossec, Haydn ou Mozart voit souvent les vents associés par deux (2 flûtes, 2 hautbois, 2 clarinettes, 2 bassons, 2 cors, 2 trompettes). Les pupitres de la période romantique s'ordonnancent plutôt par trois avec l'ajout plus ou moins systématique d'instruments comme le piccolo, le cor anglais, la clarinette basse, les saxophones, le contrebasson, les trombones ou le tuba. C'est aussi la période qui connaît la grande évolution des percussions. Au début du XXe siècle, l'orchestre symphonique peut être de grande taille, généralement, plus de quatre-vingts musiciens, l'effectif dépassant parfois la centaine d'instrumentistes.Depuis la fin du XVIIe siècle, sa principale fonction est dédiée à l'exécution, dans les salles de concert, d'œuvres symphoniques ou concertantes, profanes ou sacrées. Cette formation est également utilisée pour l'accompagnement en fosse, dans les salles d'opéra, des représentations d'art lyrique ou chorégraphique. Les compositeurs de musiques de film ou encore de musiques de jeu vidéo aujourd'hui, héritières des musiques de scène, utilisent eux aussi toutes les ressources musicales et expressives de l'orchestre symphonique.L'orchestre symphonique est constitué de trois familles d'instruments : les cordes, les vents (comprenant les bois et les cuivres) et les percussions. La composition précise de l'orchestre dépend de l'œuvre exécutée.Chaque pupitre comprend un premier soliste (pouvant être secondé par un second ou un troisième soliste) dont le rôle, comme son nom l'indique, est de jouer les parties solo d'une partition orchestrale, mais aussi de diriger des répétitions partielles de son pupitre. Les autres musiciens sont appelés des tuttistes.Le « premier violon solo » a un rôle hiérarchique et représente souvent l'orchestre devant son chef (qui le salue lors des concerts) et devant le public (il commande les levers des musiciens et accueille le chef d'orchestre). Il est de tradition que ce soit lui qui demande le « la » au hautbois pour vérifier l'accord des instruments.La disposition des différents pupitres peut varier notamment si l'orchestre est caché comme dans le cas d'une fosse d'opéra par exemple.En règle générale, les cordes sont réparties en demi-cercle de gauche à droite du chef d'orchestre, et de l'aigu vers le grave (premiers violons, deuxièmes violons, altos, violoncelles et, derrière ces derniers, contrebasses).Les vents peuvent être répartis en ligne (de l'aigu vers le grave) :cors / trompettes / trombones / tubasflûtes / hautbois / clarinettes / bassonscordesou en carrés (favorisant l'écoute entre les divers pupitres) :clarinettes / bassons       //      trombones / tubasflûtes / hautbois           //      cors / trompettescordesLes cordes sont la partie la plus constante de l'orchestre symphonique. Elles sont divisées en cinq pupitres, habituellement répartis de la manière suivante, de gauche à droite (en regardant l'orchestre) : les premiers violons, au nombre de 16 environ ;les seconds violons, au nombre de 14 environ ;les altos, au nombre de 12 environ ;les violoncelles, au nombre de 10 environ ;les contrebasses, au nombre de 8 environ.Ces effectifs sont à diviser par deux pour un orchestre symphonique de type « Mozart ».Disposée le plus souvent en arrière des cordes, la famille des bois peut être d'un effectif très variable suivant le répertoire abordé. Les flûtes, hautbois, clarinettes et bassons (la « petite harmonie ») peuvent former une section oscillant de deux à plus de vingt musiciens, incluant notamment les saxophones, certains compositeurs, comme Igor Stravinsky dans Le Sacre du printemps, utilisant cinq instruments et leurs dérivés par pupitre, exploitant ainsi toutes leurs richesses sonores, individuelles ou collectives. Chaque type d'instrument forme un pupitre comportant jusqu'à cinq musiciens dont les partitions sont toutes distinctes : contrairement aux cordes, aucune partie n'est doublée.La puissance sonore de la famille des bois étant supérieure à celle des cordes, dans un rapport de 2 à 7 suivant les instruments et les registres, à l'exception des notes les plus graves de la flûte d'une intensité à peu près équivalente à celle du violon, ceci explique le nombre réduit des pupitres de bois par rapport à ceux de cordes. Ils jouent fréquemment un rôle de soliste, particulièrement les flûtes et les hautbois, un peu moins les bassons qui renforçaient souvent les violoncelles, au moins dans les compositions du XVIIIe siècle.Dès l'origine et tout au long de la deuxième moitié du XVIIIe siècle, les deux bassons sont les bois les plus permanents de l'orchestre, d'abord accompagnés de deux hautbois, parfois de deux flûtes, puis de deux clarinettes. Des parties de piccolo et de contrebasson apparaissent chez Ludwig van Beethoven dans le finale de la symphonie no 5 en 1808, ou de la symphonie no 9 en 1824. Le cor anglais se dévoile dans l'ouverture de l'opéra Guillaume Tell de Rossini en 1829 ou, l'année suivante, dans la Symphonie fantastique d'Hector Berlioz. Dans L'Arlésienne de Georges Bizet en 1872, c'est le saxophone alto qui prend place, mais son emploi ne sera, et n'est encore de nos jours, qu'épisodique dans l'histoire de l'orchestre symphonique.Composition des bois d'un grand orchestre symphonique moderne Les flûtes1 piccolo2 à 4 flûtes traversières, l'une d'entre elles pouvant jouer le deuxième piccoloN.B. : certaines œuvres utilisent une flûte en sol jouée par l'un des flûtistesLes hautbois2 à 4 hautbois le quatrième pouvant jouer le deuxième cor anglais1 cor anglaisN.B. : certaines œuvres, peu fréquentes, utilisent un hautbois d'amour joué par l'un des hautboïstesLes clarinettes1 clarinette en mi♭ (dite « petite clarinette »)2 à 4 clarinettes en si♭ ou en la (très rarement en ut), la quatrième pouvant jouer la deuxième clarinette basse1 clarinette basseLes bassons2 à 4 bassons, le quatrième pouvant jouer le deuxième contrebasson1 contrebassonLe saxophoneNé au sein de l'orchestre du XIXème, il a sa place dans certains chefs-d'œuvre et dans la musique symphonique à partir de 1845.Disposés en arrière des bois, les cuivres participent à l'orchestre selon les besoins liés au répertoire abordé. Ils se composent des instruments suivants :2 à 4 trompettes, l'une d'elles pouvant jouer la trompette piccolo2 à 8 cors d'harmonie, quatre d'entre eux pouvant jouer les tubas wagnériens2 à 4 trombones1 trombone basse1 ou 2 tubasLeur très grande puissance sonore, de l'ordre du double de celle des bois et 6 ou 7 fois de celle des cordes, moins écrasante dans le grave mais atteignant jusqu'à plus de 15 pour la trompette par rapport au violon dans le registre aigu, impose leur place à l'arrière et leur nombre relativement limité.Dans un orchestre symphonique, les instruments de percussion, généralement placées derrière les cuivres, sont le plus souvent classés en trois groupes :Les claviersbois : xylophone, marimbamétal : vibraphone, glockenspiel, célesta, jeu de cloches ou carillon tubulaireLes peauxaccordables : timbales (le plus souvent par jeu de cinq)grandes : grosse caissemoyennes : tambour d'orchestre, caisse claire, timbale(è)spetites : tambourin et tambour de basque, bongos, congas, tumbasLes accessoires :bois : wood-block, temple block, castagnettes, fouet, güiro, maracas...métal : cymbales (frappées, suspendues, charlestons…), triangle, grelots, gong, tam-tam...divers : flexatone, sifflet, klaxon, sirène...Bernard Lehmann a relevé une origine sociale plus élevée des musiciens jouant d'instruments à cordes par rapport à ceux des instruments à vent, et des bois par rapport aux cuivres au sein des orchestres parisiens. Les cuivres sont également issus plus fréquemment d'harmonies municipales (cf. étude sociologique sur les orchestres d'harmonie en Alsace). Cette gradation ne se retrouve toutefois pas à un niveau des rémunérations. Ainsi, les bois ont une proportion de solistes nettement plus importante que les cordes, essentiellement composées de tuttistes. Les rémunérations diffèrent en conséquence.« Un orchestre symphonique est la plus belle métaphore de la société que je connaisse. Chacun est indispensable, mais doit savoir s'effacer pour faire vivre une réalité supérieure » — Riccardo Muti.En France la plupart des grands orchestres se sont constitués au XIXe siècle et au XXe siècle soit à partir des conservatoires de musique (les professeurs formant le noyau de l'orchestre comme à Paris ou à Strasbourg) soit par la Radio publique, à partir des années 1930 (en 1964, l'ORTF comptait 8 orchestres radio-symphoniques dont deux à Paris (où résidaient aussi l'Orchestre lyrique de l'ORTF et l'Orchestre de chambre ainsi qu'un orchestre de musique légère).Le cadre de la gestion des orchestres est très variable : certains sont en régie directe, d'autres sont des établissements publics ou en relèvent (comme celui de l'Opéra de Paris) ; certains sont des syndicats mixtes. Avec la création d'orchestres régionaux à partir de 1969 sous l'égide de Marcel Landowski au ministère de la Culture, on assiste à la création de grands orchestres gérés sous forme associative qui vont pour la première fois bénéficier d'une subvention importante de l'État. C'est le cas de la Société des concerts du Conservatoire de Paris, qui était une association symphonique (comme les Orchestres Pasdeloup ou Colonne) et qui deviendra l'Orchestre de Paris avec un cadre de gestion rénové et une structure de financement stabilisée. Mais certains orchestres resteront dans le giron municipal comme ceux de Strasbourg ou Bordeaux. Les orchestres régionaux avaient pour mission de diffuser la musique sur le territoire de toute une région. Ils devinrent aussi les représentants de la vitalité de la musique en France (commandes passées aux compositeurs, répertoire de la musique française, tournées à l'étranger, etc.).Les orchestres symphoniques sont par conséquent de véritables institutions culturelles en ce sens qu'ils ont une place reconnue par les autorités publiques dans la vie culturelle, soit localement, soit régionalement, soit nationalement. Ils ont acquis une légitimité à travers des missions de service public et d'intérêt général qui leur sont confiées et qui leur donnent stabilité et permanence comme l'explique Mario d'Angelo. Le rôle d'institution musicale confère aussi aux orchestres symphoniques un rôle dans la transmission d'un savoir artistique et de la construction des valeurs artistiques. Se pose pour tout orchestre, au-delà de son modèle économique, la question de la gouvernance, mais aussi celle de la gestion de sa ressource humaine et artistique. En outre l'environnement concurrentiel plus aigu (forte médiatisation), requiert de la part des dirigeants de l'orchestre qu'ils trouvent des moyens de communication de plus en plus importants pour la notoriété de l'orchestre, régionalement, nationalement ou internationalement. Le choix du directeur artistique est une autre décision cruciale. De même pour la résidence de l'orchestre : dispose-t-il d'un lieu dédié et acoustiquement bon, pour ses répétitions et ses concerts, avec idéalement la possibilité de répéter et jouer dans le même lieu et de pouvoir y développer des activités éducatives ? On note sur ces deux derniers aspects des différences sensibles entre les orchestres, que ce soit en France ou en Europe. Partout en Europe, la situation des orchestres symphoniques présente les mêmes enjeux de gestion qu'en France pour assurer leur maintien et leur développement : assurer la convergence de soutiens publics et privés en rénovant leur mode de gouvernance. Les orchestres doivent aussi développer leur public globalement vieillissant. À l'instar de l'Orchestre national de Lille, ils mettent en œuvre la démocratisation de la musique classique en jouant dans les lieux et pour les publics les plus divers (usines, prisons, hôpitaux...). L'Allemagne compte 122 orchestres stables aux statuts divers allant de la régie à la fondation, en passant par la société coopérative (comme l'Orchestre Philharmonique de Berlin) ou l'établissement régional de radio (au total 10 orchestres radio-symphoniques). Au Royaume-Uni, tous les orchestres sont en gestion privée à but non lucratif ; les quatre grandes formations londoniennes sont des coopératives (self governing orchestra) comme l'Orchestre symphonique de Londres (LSO). Le LSO dans une indépendance affichée à l'égard des pouvoirs publics n'en demeure pas moins une des grandes institutions musicales ayant su diversifier ses activités dès les années 1930 (enregistrements au disque et musiques de films). Depuis les années 2000, le LSO a mené un projet qui lui a permis d'avoir un lieu spécifique (LSO Luke's, une ancienne église restaurée et aménagées) pour y développer de nouvelles activités : studios d'enregistrements et activités éducatives « LSO Discovery ». La stratégie de diversification du LSO est également territoriale ; il est en résidence à New York (où il a mis en place une fondation pour financer cette résidence) et à la Salle Pleyel à Paris. L'orchestre-entreprise qu'est devenu le LSO n'est ainsi plus seulement limité à une activité principale de concert à Londres (à l'auditorium du Barbican Centre) et de tournées internationales.Liste d'orchestres symphoniquesPhilharmonie Musique symphoniqueMusique de chambreMusique instrumentaleOrganologieHector Berlioz et Joël-Marie Fauquet, De l'instrumentation, Paris, Le Castor astral, coll. « Les inattendus », 1994, 169 p. (ISBN 2-85920-227-7)Hector Berlioz, Traité d'instrumentation et d'orchestration, Paris, Henry Lemoine, 1843, réed.1993, 312 p. (ISMN 979-0-2309-4518-9). François-Auguste Gevaert, Nouveau traité d'instrumentation, Paris-Bruxelles, Lemoine & Fils, 1885, 340 p.  (Texte disponible sur www.imslp.org)Charles Koechlin, Traité de l'orchestration, vol. 1, Paris, Éditions Max Eschig, 1954, 322 p. (en) Paul Mathews, Orchestration : an anthology of writings, New York, Routledge, 2006, 230 p. (ISBN 0-415-97683-9)(en) (ru) Nikolaï Rimski-Korsakov et Maximilian Steinberg (trad. Edward Agate), Principles of Orchestration (Основы оркестровки), vol. 1, Berlin, Editions Russes de Musique,‎ 1913, trad.1922, 152 p. Charles-Marie Widor, Technique de l'orchestre moderne, Paris, Henry Lemoine, 1925, 200 p. Hyacinthe Ravet, L’orchestre au travail. Interactions, négociations, coopérations, Paris, Vrin, 2015, 379 pages  (ISBN 978-2-7116-2615-1)Christian Merlin, Au cœur de l'orchestre, Paris, Fayard, 2012, 520 pages  (ISBN 978-2-21366315-9)Pauline Adenot, Les musiciens d'orchestre symphonique, de la vocation au désenchantement, Paris, L'Harmattan, 2008, 381 p. (ISBN 978-2-296-05747-0, lire en ligne)Bernard Lehmann, L'orchestre dans tous ses éclats. Ethnographie des formations symphoniques, Paris, La Découverte, 2005, 262 p. (ISBN 978-2-7071-4610-6, LCCN 2002424262)Vincent Dubois, Jean-Matthieu Méon, & Emmanuel Pierru, Les mondes de l'harmonie. Enquête sur une pratique musicale amateur, Paris, La Dispute, 2005, 312 p. (ISBN 978-2-84303-149-6, LCCN 2009515324)Brigitte Ouvry-Vial, Georges Zeisel, L'Orchestre : Des rites et des dieux, Paris, Editions Autrement, 2000, 240 p. (ISBN 978-2-86260-218-9) Portail de la musique classique   Portail de la musique"
musique;"En musique, la polyphonie est la combinaison de plusieurs mélodies, ou de parties musicales, chantées ou jouées en même temps.Dans la musique occidentale, la polyphonie désigne le système de composition musicale, créé à l'église à partir du IXe siècle environ et qui connut un brillant développement, depuis un premier apogée aux XIIe et XIIIe siècles, jusqu'à la fin de la Renaissance (fin du XVIe siècle) et au-delà. À partir de la deuxième moitié du XVIe siècle, la pensée et le sentiment harmonique naissants prirent une place de plus en plus importante. Un désir de simplification joint au développement de l'individualité et du chant soliste feront qu'on passera progressivement du contrepoint linéaire à l'enchaînement vertical des accords (un texte musical contrapuntique se déroule horizontalement, chaque voix ayant sa propre vie à l'intérieur de l'ensemble, alors qu'un texte harmonique enchaîne des accords). Le nouveau système, toutefois, n'a pas remplacé le précédent : ces deux types d'écriture ont pu subsister parallèlement et aussi se mêler, pendant les siècles qui suivirent jusqu'à aujourd'hui.Par extension, c'est la capacité de jouer plusieurs notes à la fois ; on parle alors d'instruments polyphoniques.En Occident, la monodie, en usage au Moyen Âge et au-delà, recouvre des genres très différents comme le chant grégorien, la poésie aristocratique chantée des troubadours et des trouvères et la chanson de tradition orale appelée aussi chanson folklorique. L'accompagnement (s'il en existe un) n'est pas donné et n'est pas de nature mélodique. Dans l'opéra, l'expression un peu paradoxale de « monodie accompagnée » a une signification bien différente. Dans ce style vocal né avec l'opéra baroque au début du XVIIe siècle, le chant soliste est accompagné par une basse continue, aussi bien que par un orchestre. Ce type de monodie relève donc d'une écriture harmonique.Pour la notion de polyphonie en théorie de la littérature ou en linguistique voir les travaux de Mikhaïl Bakhtine et Oswald Ducrot. Dans ce cas, le sens du mot « polyphonie » n'a pas de lien avec celui qui a été développé plus haut.Durant l'Antiquité, l'art musical (et donc celui de l'Église primitive) n'avait apparemment connu que la monodie. La grande invention du Moyen Âge fut celle de la polyphonie. C'est au IXe siècle que cet art commence à apparaître et à se développer, de manière encore discrète mais manifeste, à l'église tout d'abord. Le philosophe Scot Érigène fait déjà allusion, dans son ouvrage De Divisione Naturæ, à la pratique d'une musique à plusieurs parties.L'avènement de la polyphonie occidentale constitue un des plus grands bouleversements de l'histoire de la musique. Voici ce qui paraît se dégager de plus net, sur ses débuts, tout d'abord (et tout naturellement) assez obscurs. À l'origine, à l'époque de l'Empire carolingien ou de son partage, elle était improvisée au lutrin, au cours d'offices religieux importants, à partir du chant grégorien, selon la technique du « chant sur le livre » (technique qui se perpétua jusqu'à la fin du XVIIIe siècle, en France). Ce qu'on avait à chanter était en grande partie à la charge des chantres professionnels qui travaillaient quotidiennement dans les nombreuses églises cathédrales et collégiales. Nous verrons que le chant polyphonique s'était primitivement répandu dans les abbayes, spécialement bénédictines, dont la vocation était de chanter la gloire de Dieu. Mais pendant quelques siècles, cette pratique cultuelle et culturelle est restée non-écrite, et donc invisible à nos yeux.Il s'agissait initialement d'amplifier la monodie grégorienne, en lui adjoignant une seconde voix, entendue en même temps qu'elle (c'est ce que Hucbald appelle, en 895, la « diaphonie », au départ en mouvements parallèles). Cela donna naissance à différents procédés polyphoniques, dont le déchant (discantus), et, rapidement, à des formes musicales élaborées dont la plus connue est appelée organum. Les œuvres entrant dans ce cadre formel font intervenir de 2  à   4 voix, entendues ensemble.La voix de déchant se définit par le mouvement contraire qu'elle adopte par rapport à la voix principale (appelée teneur). Quand la teneur monte, le discantus descend, et inversement. Ce principe remplaça rapidement le développement par mouvements parallèles, aux possibilités bien plus réduites. Le mouvement contraire reste aujourd'hui à la base de toute progression harmonique (certains parallélismes, comme les quintes ou quartes parallèles sont toujours considérés comme strictement interdits par les traités d'harmonie).La notion de déchant prit un sens plus large par la suite. C'est ainsi qu'elle désigna plus généralement l'écriture polyphonique. Un exemple de l'utilisation de ce mot apparaît dans le titre complet des Vêpres de 1610, de Claudio Monteverdi (Vespro della Beata Vergine). On trouve en effet le verbe decantare, dans le sous-titre latin de l'œuvre : Vesperæ pluribus [vocibus] decantandæ (« Vêpres chantées à plusieurs voix », c'est-à-dire jusqu'à dix voix, traitées en polyphonie).Il avait tout d'abord semblé indispensable, à l'oreille des premiers polyphonistes, d'éviter les superpositions de tierces et de sixtes, perçues comme dissonances, et de n'admettre comme consonances que les octaves, quartes et quintes (qui sont du reste les premières harmoniques nées de la vibration d'un son fondamental). Depuis la fin du XIe siècle environ, ce qui était considéré comme dissonances était néanmoins présent, mais, devant aboutir à une résolution, menait à une consonance de quarte ou de quinte (de même, dans le système tonal actuel, la note perçue par l'oreille comme « note sensible » doit son nom au fait qu'elle tend vers la note tonique, cf. par ex. le mouvement si → do lorsqu'on est dans le ton de do majeur).Cette notion de consonance imparfaite est présente dans une formule cadentielle (formule de fin de phrase) typique du XIVe siècle : la cadence à double sensible (appelée également cadence de Machaut). Celle-ci apparaît couramment dans les polyphonies à 3 voix. À la fin d'une phrase musicale, si la pièce contient, par ex., les trois notes superposées sol-si-mi, le mouvement de cadence qui peut leur succéder mène alors à la résolution fa-do-fa : l'intervalle de tierce sol-si aboutit à la quinte fa-do, amenant ainsi un sentiment auditif de stabilité.Une forme assez différente de polyphonie s'est développée dans un pays qui, par son isolement insulaire et par son éloignement des régions méditerranéennes, échappait plus que d'autres à l'influence des traditions gréco-latines : l'Angleterre. Ce nouveau procédé se répandit ensuite sur le continent. Ainsi, l'Angleterre a évolué différemment du monde latin, si bien que la polyphonie savante des Anglais se distingua, un temps, de celle des autres peuples par l'emploi de deux procédés qui lui sont particuliers : le gymel (du latin cantus gemellus : « chant jumeau ») et le faux-bourdon, qui se caractérisent essentiellement par l'usage de successions de tierces et de sixtes, considérées, à partir du XIIe siècle environ, comme consonances imparfaites, et non plus comme dissonances.Le gymel, comme son nom l'indique, est un chant à deux voix dont la seconde accompagne à la tierce inférieure ou supérieure le thème donné par la première (appelée « teneur », car c'est elle qui « tient » le chant). Les deux voix doivent conclure en se rejoignant à l'unisson par mouvement contraire.Le faux-bourdon est un chant à trois voix qui consiste à faire entendre en même temps que la mélodie principale deux autres mélodies parallèles, à la tierce et à la quinte inférieure, - à la quinte inférieure en apparence, en écriture, pour les yeux seulement, car cette troisième partie, qui a l'air d'une basse, ou, comme on disait autrefois, d'un « bourdon », n'est en réalité qu'une fausse basse, qu'un faux-bourdon (mot employé par analogie avec des instruments comme la vielle à roue ou les différentes sortes de cornemuses). En effet, cette troisième voix doit se chanter une octave plus haut qu'elle n'est écrite, c'est-à-dire qu'elle sonne à la quarte supérieure et non à la quinte inférieure du thème donné. La polyphonie d'un faux-bourdon est donc théoriquement constituée d'un enchaînement de quartes, de sixtes et de tierces parallèles superposées. L'évolution de cette forme musicale, au cours du temps, l'a amené à s'écarter de ce schéma de base. Vers 1771-1772, le bénédictin de Saint-Maur Dom Robert-Florimond Racine évoquera avec plaisir « cette forme ravissante que l'on n’entend qu’avec étonnement ». Le XIXe siècle continuera à en faire usage.Une polyphonie populaire naquit de la polyphonie savante. Elle est donc née à l'église. Actuellement, la polyphonie populaire subsiste dans les polyphonies corses, par exemple.Aux XIe et XIIe siècles, l'abbaye bénédictine Saint-Martial de Limoges et d'autres lieux (abbayes et églises) jouent un rôle important dans le développement de la polyphonie, en l'absence de ville-phare.Mais précisément, la polyphonie connaîtra un premier rayonnement national et international à Paris (alors qualifiée de « nouvelle Athènes »), aux XIIe et XIIIe siècles, grâce aux chantres-compositeurs de l'École de Notre-Dame de Paris, cathédrale nouvellement construite et centre culturel de premier plan (tout comme le sera l'université, créée à Paris au milieu du XIIIe siècle, par le théologien Robert de Sorbon). À Notre-Dame, les chantres les plus connus ont été Léonin et Pérotin. Ils représentent les premiers grands créateurs de ce que les musiciens du siècle suivant appelleront l’Ars antiqua, par opposition à l’Ars nova. Ce dernier, né peu avant 1300, se voudra résolument nouveau (Jacques Chailley le qualifie d’avant-gardiste). L'art musical du XIVe siècle s'éloignera donc très clairement de ce que nous appelons le classicisme du grand siècle médiéval, le XIIIe siècle.Dès cette époque apparaît la technique du hoquet, qui sera plus particulièrement développée au XIVe siècle, pendant la période de l’Ars nova. Cette technique consiste en une alternance entre les voix, qui se succèdent et se répondent rapidement, chacune d'elles pouvant ne faire entendre qu'une seule syllabe à la fois, la ligne mélodique se déroulant ainsi en alternance très rapprochée. Cette manière de faire sera reprise dans la deuxième moitié du XXe siècle. Ce mode de déroulement spécifique, à l'origine proche du tuilage primitif, n'est pas exclusivement pratiqué en Europe.Les créateurs de l’Ars nova se lancèrent ainsi dans de nouvelles recherches, parfois très abstraites (comme chez Philippe de Vitry), pour aboutir à l’Ars subtilior, à la fin du XIVe siècle. L'amiénois Pierre de la Croix fut le réformateur de la notation franconienne, la notation mesurée, née au XIIIe siècle. Le grand musicien né avec le XIVe siècle est le Rémois Guillaume de Machaut (v. 1300-1377), auteur, en particulier, de la première messe polyphonique entière (la Messe Nostre Dame). Comme les autres musiciens, il écrivit aussi nombre de pièces profanes, tant l'art de la polyphonie s'était répandu dans bien des domaines musicaux.L'art profane d'Adam de la Halle, à la fin du XIIIe siècle, était à la charnière de la monodie et de la polyphonie, si bien qu'on le considère souvent comme le dernier trouvère, à la fois poète et musicien. Machaut, purement polyphoniste, a eu lui aussi une activité aussi bien musicale que poétique, suivant une très longue tradition remontant à l'Antiquité. Jusqu'à la fin du XVe siècle, avec Eloy d'Amerval par exemple, elle continuera à se perpétuer.À partir des années 1420 se développe entre le Nord de la France et les Flandres une nouvelle école musicale, grâce à plusieurs générations de musiciens et de compositeurs formés dans les maîtrises du nord. Ils seront désignés sous le nom d’École franco-flamande. Ils répandront par la suite leur art dans les grands centres européens, surtout en Italie, alors en pleine Renaissance (le Quattrocento).En raison des troubles qui règnent en France pendant la guerre de Cent Ans, la culture musicale se déplace dans les régions du nord de la France, en Flandre, et en territoire bourguignon. La cour de Bourgogne est le centre de ce renouveau artistique, qui rassemble musiciens français, flamands, bourguignons et anglais, contribuant aux échanges et à la diffusion de musiques nouvelles. Rome (ville où la papauté s'est réinstallée, après l'épisode avignonnais du XIVe siècle) le devient également.Au début de cette période (vers la fin du XIVe siècle et au début du suivant), « la fameuse contenance angloise [...] déferle sur le continent ». Cette expression désignait alors le « style musical anglais, à la fois brillant et simple, qui influence tous les compositeurs du continent au début du XVe siècle, lesquels rompent ainsi avec le style complexe et cérébral de l’Ars Subtilior de la fin du XIVe siècle ». Le principal auteur anglais représentant ce style est John Dunstable.La polyphonie se développe de deux façons :dans la musique religieuse enseignée et pratiquée dans les maîtrises et les chœurs (les « psallettes ») des cathédrales (et des collégiales) du nord. Par exemple : Cambrai, Tournai, Bruges, Anvers, etc. Leurs usages sont imités dans les psallettes des autres régions de France et d'Europe ;dans les chansons polyphoniques de divertissement, essentiellement à la cour du duc de Bourgogne à Dijon.Les principaux représentants de ce mouvement sont les grands compositeurs de l'époque :Guillaume Dufaÿ (1397-1474), Gilles Binchois (v. 1400-1460) - qui furent influencés par l'anglais John Dunstable (1390-1453) -, Johannes Ockeghem (v. 1420-1497), qui exerça à Paris et à Tours, Antoine Busnois (1433-1492), bourguignon, Josquin des Prés (v. 1440-1521), Alexandre Agricola (1446-1506), Jean Mouton (1459-1522), Jacob Obrecht, (1457-1505), Pierre de La Rue (1460-1518), Antoine Brumel (1460-1520), Clément Janequin (v. 1485-1558), connu pour ses chansons polyphoniques, Clemens non Papa (v. 1510-1555), Jacobus de Kerle (1531-1591), Roland de Lassus (1532-1594), musicien européen, Claude Le Jeune (v. 1530-1600), partisan de la Réforme calviniste, Eustache Du Caurroy (1559-1609), etc.En Italie : Adrien Willaert (1490-1562), Cyprien de Rore (1515/16-1565), l'un et l'autre d'origine franco-flamande mais installés en Italie, Giovanni Pierluigi da Palestrina (v. 1525-1594) musicien de la Contre-Réforme, Costanzo Porta (1528/29-1601) maître « virtuose » de la polyphonie, longtemps apprécié de son vivant, Marc'Antonio Ingegneri (1535-1592) qui forma Claudio Monteverdi, Carlo Gesualdo (1566-1613) musicien d'une grande originalité et audace harmonique (qui nous surprennent encore aujourd'hui), etc.En Espagne : Cristobal de Morales, (v. 1500-1553), Diego Ortiz (1510-570), Francisco Guerrero (1528-1599), Tomás Luis de Victoria (v. 1548-1611), etc.En Angleterre : John Taverner (1490-1545), Christopher Tye (1500-1572/3), Thomas Tallis (1505-1585), William Byrd (1543-1623), John Bull (1562/3-1628), Orlando Gibbons (1583-1625), etc.En Allemagne : Heinrich Isaac (1450-1517), Ludwig Senfl (1486-1543), Leonhard Lechner (1553-1606), Hans Leo Hassler (1564-1612), Gregor Aichinger (1564-1628), Michael Praetorius (1571-1621), etc.À la Réforme religieuse luthérienne, qui se manifeste de manière assez forte dans les années 1530 en Allemagne, on voit apparaître un art liturgique d'une toute autre conception, avec la naissance du choral luthérien (le Choralgesang). Ces chorals, nouvelle forme musicale, vont être composés (texte et musique) par Luther lui-même ou par d'autres auteurs, dont Martin Agricola (1486-1556). D'un côté, Luther encourage le chant communautaire. Les fidèles chantent ensemble à l'unisson des mélodies faciles et rapidement connues de tous. Il rassemble une série de mélodies de diverses origines, les structure sous la forme de chorals et les propose comme mélodies qui auront une valeur liturgique (cf. par ex. Ein Kinderlied für Weihnachten, parmi bien d'autres choses). De l'autre côté, il est un admirateur de la musique polyphonique, donc il ne l'exclut pas. Il laisse ouvert la possibilité que les chants communautaires soient soumis à diverses formes d'élaboration polyphonique. Les mélodies pour les chants communautaires simples mais de grande qualité restent au cœur du répertoire (un peu comme le chant grégorien dans le culte catholique), mais c'est un matériau sur lequel les compositeurs peuvent élaborer des versions plus complexes et développées. C'est la base pour un développement plus tardif, qui s'épanouira tout au long des XVIe et XVIIe siècles, puis notamment dans la musique de J.S. Bach (XVIIIe siècle).Michael Praetorius (1571-1621) a été compositeur pour l'Église luthérienne. Il a également repris beaucoup de mélodies, comme matériau de base pour ses œuvres polyphoniques. C'est le cas notamment pour Vom Himmel hoch, da komm ich her. Ces timbres sont des mélodies préexistantes, sacrées ou profanes. Certaines sont même des mélodies grégoriennes. Dans les décennies suivantes principalement, on trouve aussi un bon nombre de parodies (procédé, bien entendu dénué de toute idée de caricature, qui consiste à prendre une mélodie profane et à la chanter sur un texte de la liturgie). Ainsi, l'échange à double sens entre les univers profane et sacré nourrit l'inspiration musicale (depuis le XVe siècle au moins, d'assez nombreuses chansons de tradition populaire reprennent des motifs grégoriens, en les adaptant).Ces notions (d'échange à double sens entre musique religieuse et musique profane) sont valables aussi bien chez les luthériens que chez les catholiques, les musiciens luthériens ayant repris les traditions de l'Église catholique et du monde profane.En France, Jean Calvin, personnage central de la Réforme religieuse protestante, estime que la musique doit être extrêmement simple. Apparaissent alors les psaumes monodiques. Les 150 psaumes de la bible sont harmonisés par Claude Goudimel (v. 1505-1572). On peut citer aussi ceux que Richard Crassot harmonisa et publia à Lyon en 1565. Pour Calvin, il ne doit pas y avoir d'instrument. S'il y a un orgue on le détruit. Malgré cela, il n'était pas fermé à l'idée de musique polyphonique, mais elle devait se faire toujours sans instrument. La polyphonie savante de Claude Le Jeune (cf. ses Octonaires de la vanité et inconstance du monde) en est un des principaux témoins, même si elle est strictement profane. En effet, il est important de préciser que ces Octonaires ont été composés sur une poésie d'inspiration calviniste, mais sans du tout être liturgique : écrits pour la vie civile et la réflexion, ils n'étaient pas destinés à être chantés au cours d'un office et développent simplement leur philosophie de vie.En Angleterre, la musique polyphonique demeure au cœur de la liturgie. Thomas Tallis, resté catholique, va développer, pour la liturgie anglicane, le style de l'anthem, motet spécifiquement anglais (et chanté dans cette langue). Gibbons créera le Full Anthem (en chant polyphonique) et le Verse Anthem (dont les versets font alterner chantre soliste et chœur à plusieurs voix : Gibbons adapte de cette manière la très ancienne pratique des « versets alternés », déjà présente dans la tradition grégorienne monodique).Plus généralement, depuis le début du XVIe siècle environ, chez Tallis et chez bien d'autres auteurs, l'écriture polyphonique se développe dans des directions nouvelles : la musique devient assez fréquemment polychorale. Dans ce cas, ce n'est plus seulement un chœur, mais plusieurs qui vont faire vivre des polyphonies réparties dans l'espace. Le Spem in alium de Tallis (à 40 voix réelles) est bien connu (8 chœurs à cinq voix). Mais c'est là une exception. On était beaucoup plus fréquemment répartis en deux chœurs à 4 voix, ou selon d'autres formations (qui pouvaient être impaires) : tout cela se développa en particulier à la basilique Saint-Marc de Venise, où les Gabrieli (Andrea et son neveu Giovanni) écriront à 8, ainsi qu'à 16 voix réelles, par exemple. Le fameux motet Osculetur me, à 8 voix (1582), du franco-flamand européen Roland de Lassus, est écrit dans ce style, pour deux chœurs à 4 voix, qui se répondent et/ou se mêlent. Au début de la période, Josquin avait déjà écrit un Qui habitat (Ps. 90), à 24 voix. On doit citer aussi le canon à 36 voix (Deo gratias), de Johannes Ockeghem (une des principales figures de la seconde moitié du XVe siècle). Alessandro Striggio (v. 1535-1592), qui vivait à la même époque qu'Andrea Gabrieli, est l'auteur d'un motet Ecce beatam lucem (« Voici la lumière bienheureuse »), à 40 voix. L’Agnus Dei de sa Messe sur « Ecco sì beato giorno », à 40 voix, fait même intervenir 60 voix...Vers la fin du XVIe siècle, le madrigal italien contribuera grandement au développement d'une nouvelle technique, qui, petit à petit, deviendra radicalement différente de la polyphonie traditionnelle et finira par créer une nouvelle conception de l'art musical : la monodie accompagnée.La polyphonie ne mourut pas pour autant. Témoins, par exemple, l'œuvre de Heinrich Schütz (au XVIIe siècle), presque toute l'œuvre de J. S. Bach ou encore, parmi bien d'autres choses, le Stabat Mater à 10 voix réelles, d'un de ses contemporains, le claveciniste Domenico Scarlatti. Ordinairement connu pour ses sonates de clavecin, il montre ici sa grande habileté dans l'art du contrepoint. Le compositeur napolitain écrivit ce motet pendant sa période romaine (1715-1719), pour la Cappella Giulia (la chapelle papale), si bien que (contrairement à ce qu'on pourrait imaginer) la partition ne doit rien au style polychoral qui s'était spécialement développé à Venise au début du XVIIe siècle : les choristes ne sont pas répartis en chœurs différents (même si les 10 voix ne sont pas constamment employées en une masse indivise).En France, Marc-Antoine Charpentier est un des représentants les plus importants pour l'usage et le développement de ce style d'écriture musicale. Parmi les principaux auteurs du XVIIe siècle, relevons également les noms d'Henry Du Mont et de beaucoup d'autres (comme Charles d'Helfer par ex.), au cours de ce siècle et à l'époque suivante (André Campra, Henry Desmarest sous Louis XIV et Louis XV, Henry Madin dans la première moitié du XVIIIe siècle, etc.).En Angleterre, l'écriture linéaire est présente dans certains épisodes de l'opéra de chambre d'Henry Purcell, Didon et Énée (1689). On la rencontre aussi dans sa Music for the Funeral of Queen Mary (1695), là encore, essentiellement dans les chœurs, mais pas uniquement : la fameuse « Mort de Didon » est bâtie sur un ground (un ostinato) présentant un chromatisme descendant, à la basse continue, sur lequel se développe la partie de solo, dont la dynamique et le chromatisme différent de la phrase d'introduction instrumentale (c'est la basse de cette phrase introductive qui sera reproduite obstinément jusqu'à la fin de l'air).L'écriture polyphonique ne se limite donc pas aux œuvres chantées : les 15 Sonates du Rosaire (Rosenkranzsonaten, pour violon et basse continue), composées vers 1678 par l'autrichien Heinrich Biber, en sont un exemple célèbre, parmi beaucoup d'autres.En Angleterre, l'écriture en contrepoint dans les consorts of viols (ensembles de violes) de la fin du XVIe et du XVIIe siècle est caractéristique de ce type d'ensembles, où l'unité, due à la texture très dense du tissu contrapuntique, ne laisse aucune place à l'individualité : difficile de suivre l'évolution d'une voix ou d'une autre à l'intérieur de la polyphonie.À l'époque, et jusqu'au XIXe siècle, l'habitude s'est prise de traiter certains chœurs de manière fuguée. C'est particulièrement vrai et développé pour les chœurs de fin (mais bien sûr, pas systématiquement et pas uniquement). On trouve cela, parmi de nombreux autres exemples, dans plusieurs œuvres célèbres de : Vivaldi (en particulier dans le chœur final de son Magnificat, vers 1725), Haendel (Le Messie, 1741, Solomon, 1749, etc.), W. A. Mozart (Requiem, 1791), ou encore dans le Requiem allemand de Johannes Brahms (1868), etc. Le Requiem de Mozart comporte plusieurs fugues. Celui de Brahms en présente deux (dans les numéros 3 et 6). L'écriture polyphonique et contrapuntique (qui ne rompt pas avec l'héritage d'Heinrich Schütz ou de Bach par exemple) est, d'une manière générale, bien présente dans l'œuvre de Brahms, romantique mais aussi digne continuateur des classiques ou de l'école germanique d'inspiration luthérienne.La musique profane n'est pas en reste. On peut citer, entre autres exemples :L'ouverture de La Flûte enchantée, opéra féerique et initiatique de Mozart (1791), qui présente un fugato orchestral développé. Ce Singspiel présente d'autres épisodes d'écriture linéaire ;Dans son célèbre quintette à cordes en sol mineur (1787, Koechel 516) sa maîtrise du contrepoint apparaît clairement, en particulier dans les développements ; il en est de même dans son non moins célèbre quatuor à cordes connu sous le nom de « quatuor des dissonances » (Koechel 465).Le Minuetto et le Finale de sa Symphonie « Jupiter » (1788) mettent en œuvre un contrepoint très complexe mais limpide, clairement hérité de J. S. Bach. Ceci est préfiguré sans sa symphonie n° 38 : Prague (1786).La Grande Fugue en si bémol majeur pour quatuor à cordes, opus 133, de Ludwig van Beethoven, a d'abord été composée comme dernier mouvement de son Quatuor op. 130 (1825). Il l'en détacha en 1827 pour la publier séparément ;Ses deux sonates pour violoncelle et piano (n° 4 et 5 : op. 102, n° 1 et 2) présentent chacune un mouvement en style de contrepoint élaboré (la seconde se termine par une fugue). Mais l'écriture polyphonique apparaît aussi bien dans d'autres circonstances, comme le dernier mouvement de sa Cinquième symphonie, par exemple : cela constitue alors un élément de langage, parmi d'autres. Beethoven avait été formé au contrepoint, essentiellement par le maître de chapelle et compositeur Johann Georg Albrechtsberger, à Vienne ;En 1846, dans sa Damnation de Faust, Berlioz ironisera sur la notion de fugue, forme musicale née de la polyphonie et du contrepoint vocal (il la reprendra à sa manière dans la scène de la taverne d'Auerbach à Leipzig). Selon Antoine Livio, dans son ouvrage intitulé Maurice Béjart, « la bestialité [des convives] atteint son paroxysme lorsque Brander réclame : ""pour l’Amen, une fugue ! une fugue, un choral ! Improvisons un morceau magistral"" ». Berlioz, auteur d'un célèbre Requiem (1837) et d'un Te Deum non moins célèbre (1849) ne comprenait plus guère ce langage musical que sous l'angle d'une théâtralité très romantique. Par ailleurs, la politique anticléricale du roi Louis-Philippe (qui régna de 1830 à 1848) est bien connue. Cela ne l'empêcha pas d'utiliser le mode d'écriture polyphonique dans ces œuvres, la fugue en particulier, dans le Sanctus de la messe de Requiem par exemple.Chez Richard Wagner, « le style des Maîtres chanteurs de Nuremberg [1868] se caractérise par un contrepoint qui suit les règles de Bach. Wagner qualifia un jour de « Bach appliqué » le prologue, dont les trois thèmes principaux sont réunis, à la fin, par des liens contrapuntiques. C'est la maîtrise de ces éléments de style qui permit à Wagner de reprendre, après douze ans d'interruption, la composition de l’Anneau du Nibelung (le 3e acte de Siegfried et le Crépuscule des dieux [1876]) avec une force d'expression musicale accrue. »Gustav Mahler utilise également ce mode d'écriture, par exemple dans sa symphonie nº 2, sous-titrée Résurrection (en allemand : Auferstehung).Au début du XXe siècle, Arnold Schoenberg, âgé de 33 ans (1907), utilise une écriture polyphonique savante, en imitations, dans Friede auf Erden (« Paix sur la terre », pour chœur à 8 voix et petit ensemble instrumental facultatif). Il en est de même dans ses Gurre-Lieder, lieder en forme de cantate, d'esprit post-romantique, pour 5 soli, récitant, grand chœur et grand orchestre (1900-1911). L'école sérielle, issue du Schoenberg dodécaphonique, reprendra elle aussi les principes de l'écriture polyphonique et contrapuntique, à partir des années 1920 jusqu'à la remise en cause globale du sérialisme dans les années 1980 environ.En 1942 (orchestration en 1946), le musicien d'origine alsacienne Charles Koechlin composa L'Offrande musicale sur le nom de Bach. Cette œuvre monumentale (dont l'effectif va du piano seul au très grand orchestre) attend toujours d'être donnée en France (elle n'avait été créée qu'en 1973, par l'orchestre de la Radio de Francfort). C'est cependant une des pièces maîtresses de son auteur.Dans West Side Story, drame lyrique (comédie musicale) de Leonard Bernstein (1957, 1961), le n° 13 de l'acte 1, Cool, est une fugue.Le musicien hongrois György Ligeti (1923-2006), une des principales figures de la seconde moitié du XXe siècle, actualise à sa manière l'écriture polyphonique évoluant sur un mode linéaire. En 2017 le Centre international de création musicale écrit : « En 1961, la pièce pour grand orchestre Atmosphères poursuit la voie inaugurée dans Glissandi en introduisant la technique de « micro-tonalité », où [la micropolyphonie résultant d']un contrepoint extrêmement serré avec de petits intervalles et un grand nombre de voix n’est plus perçu[e] en tant que tel[le], dans son détail, mais en tant que masse sonore mouvante. Lontano (1967) pour orchestre et Lux Æterna (1966) pour chœur explorent des voies similaires. Ligeti, par cette esthétique de l’ambivalence harmonie-timbre, influera beaucoup sur la génération des compositeurs de l’école spectrale. / Ligeti affina cette technique - où la répétition d’un même son dans plusieurs voix à des vitesses presque identiques crée des déphasages évoluant lentement dans le temps - dans diverses œuvres, notamment dans les scherzos du Deuxième quatuor à cordes (1968) et du Concerto de chambre (1970), ainsi que dans les Trois pièces pour deux pianos (1976). En plus de cette technique purement rythmique, Ramifications (1969) pour double orchestre à cordes brouille les lignes en accordant un des deux orchestres à un diapason légèrement différent de celui de l’autre. ».Des pièces comme Atmosphères, Lontano et Lux Æterna ont été reprises au cinéma par le réalisateur Stanley Kubrick (2001, l'Odyssée de l'espace) et The Shining (Lontano seulement).Dans l'Église catholique, l'usage de la polyphonie était considéré au deuxième rang après le chant grégorien, par le motu proprio Inter pastoralis officii sollicitudines du pape Pie X en 1903. Puis, en 1928, l'exécution de la polyphonie, surtout celle de Giovanni Pierluigi da Palestrina, était fortement recommandée dans la constitution apostolique Divini cultus sanctitatem, et par le pape Pie XI. Même si le concile Vatican II garde cette recommandation, la pratique subit un gros déclin à la suite de sa réforme liturgique, avec les chants sacrés vraiment simples.Le plus connu des compositeurs anglais du XXe siècle, Benjamin Britten, utilisa l'écriture polyphonique dans ses œuvres. Certaines devinrent très célèbres, comme la Simple Symphony (terminée en 1934), A Ceremony of Carols (1942) et sa Young Person's Guide to the Orchestra (« Présentation de l'orchestre à une jeune personne »). Cette dernière œuvre prend la forme d'une série de variations pour orchestre sous-titrées Variations et Fugue sur un thème de Purcell (1946). Les Variations sur un thème de Frank Bridge (1937) comportent également une fugue, enchaînée avec le Finale.Au cours du premier tiers du XXe siècle, Charles Tournemire par exemple, porté par le mouvement de renouveau du chant grégorien à l'intérieur de l'Église catholique, faisait entendre des mélodies grégoriennes utilisées en cantus firmus, dans certaines de ses pièces d'orgue (cf. dans L'Orgue mystique de 1927-1932 : le Cycle de Noël, op. 55 — le Cycle de Pâques, op. 56 — le Cycle Après La Pentecôte, op. 57). Jusqu'à nos jours, bien d'autres auteurs ont repris les principes de l'écriture contrapuntique.À la fin du XXe siècle, Jean-Louis Florentz pratiquera le même type d'écriture, en utilisant cette fois des mélodies d'origine non-européenne, comme sa « Harpe de Marie », n° 3 de ses Laudes pour orgue (1983-1985). On y entend des thèmes liturgiques en usage dans le christianisme éthiopien, ou encore (dans le n° 4 de ces Laudes : « Chant des fleurs »), d'autres thèmes venus cette fois du Burundi. Dans certaines de ces pièces, il arrive à Florentz d'associer cela à sa perception personnelle des sons et des harmoniques entendus dans le ronflement d'un moteur d'avion de ligne. En cela il est influencé par l'ingénieur et compositeur Pierre Schaeffer.Comme bien d'autres, le compositeur espagnol Hèctor Parra utilise l'écriture en style de contrepoint dans ses trois pièces pour orgue d'inspiration profane intitulées Tres Miradas (2016. Commande de Radio France, création l"
musique;"Un sopraniste est un chanteur adulte de sexe masculin dont la tessiture est proche de la soprano féminine. Sopraniste signifie ""qui joue la partie de dessus"" et pourrait être logiquement étendu à tous les musiciens jouant une partie dans la tessiture soprano. L'usage actuel veut qu'il ne serve qu'à désigner les contre-ténors (encore appelés falsettistes) soprani, et les castrats soprani (aujourd'hui disparus). Il est difficile d'en arrêter une définition fournie tant le nombre de sopranistes est aujourd'hui minuscule.Du XVIIe  au  XIXe siècle, lorsque les castrations de garçons prépubères étaient autorisées en Europe (à l'exception de la France), il existait des castrats contraltistes et sopranistes, mais ils utilisaient uniquement la voix de tête que l'absence de mue avait préservée. On peut donc qualifier certains castrats de sopranistes, bien que leur technique vocale ne soit pas assimilable à celle des sopranistes actuels. L’usage de l'Église du XVIIe – XVIIIe siècle a favorisé l’emploi du terme de « soprano naturel » pour qualifier les castrats sopranistes, au mépris des femmes soprani et des falsettistes sopranistes, deux tessitures ou pratiques vocales ayant parfaitement cours à l'époque. Farinelli, Bernacchi, Caffarelli, Carestini ou Gizziello sont les exemples les plus célèbres de castrats sopranistes.Un contre-ténor sopraniste est un chanteur dont la tessiture se situe au-dessus de celle du contre-ténor altiste. Ce dernier est souvent nommé contraltiste, et plus souvent encore alto par référence à la voix qu'il chante dans un chœur, mais également pour le différencier du joueur de violon alto.La voix d'un sopraniste peut être plus ou moins étendue, généralement assez proche de celle de la mezzo-soprano, environ du la2 au la4 (au-dessous et au-dessus d'une portée en clef de sol). Pourtant, elle est souvent plus pure et moins profonde, d'où sa qualification d'angélique.Tout comme le contre-ténor altiste, le sopraniste utilise sa voix de fausset pour chanter, ainsi que ponctuellement sa voix de poitrine pour des rôles particuliers, pour amuser, ou pour certaines notes graves où la voix mixte ne suffit plus.Il existe très peu de sopranistes à ce jour, mais citons Nicolas Hay, Hugo Mangon, Lionel Stoffel, Fabrice di Falco, Valer Barna-Sabadus, Jörg Waschinski, Jacek Laszczkowski, Mathieu Salama, David Hansen, Franco Fagioli, Arno Raunig, Adriano D'Alchimio, Aris Christofellis, Oleg Riabets, Gary Boyce, Edson Cordeiro, Yves Le Pech, Radu Marian, Angelo Manzotti, Paul Laumont, Michael Maniaci, Bagdasar Khachikyan, Chris Colfer, Francesco Divito, Oswald Musielski, Barabasi Zsolt, Marcel Lucien Arpots, Ludovic Baas, Nicolas Achten, Javier Fuentes, Paulo Abel do Nascimento, Étienne Cousineau, Alain Vu, Patrick Husson, Grégoire Solle, Philippe-Emmanuel Toussaint ou David Albert-Brunet... À noter que Max Emanuel Cenčić ainsi que Philippe Jaroussky, aujourd'hui tous deux mezzo-soprano (voire alto), commencèrent leur carrière comme sopranistes (Cenčić chanta par exemple Frühlingsstimmen de Johann Strauss II, air pour soprano léger ou soprano colorature). À noter que, hors de la sphère du chant classique, on trouve aussi des sopranistes tels que Klaus Nomi, Ugo Farell ou encore Thomas Otten (à la voix plus sombre de mezzo-soprano), notamment.Il existe, également, quelques très rares chanteurs masculins capables d'atteindre la colorature (le soprano colorature), à la tonalité encore plus aiguë (par exemple le hongrois André Vasary, mais aussi F. Divito ou M. Maniaci).MusiqueMusique classiqueVoix (instrument)Voix (musique classique)Chant(fr) Le site des contre-ténors et sopranistes Portail de la musique classique   Portail de l’opéra"
musique;"En musique, le système tonal est un ensemble de relations entre des notes et des accords structurées autour d'une tonique.Le langage tonal se construit sur les échelles diatoniques majeures ou mineures et en appliquant les lois de l'harmonie tonale.Ce système musical occidental s'est progressivement mis en place à la Renaissance ; il est utilisé dans la musique savante — de manière presque exclusive — depuis le XVIIe siècle jusqu'à la fin du XIXe siècle. À partir du  XVIIIe siècle, il s'est inséré puis circonscrit dans la gamme tempérée (partage d'une octave en douze demi-tons égaux) qui a permis son essor et son extension. Il est aussi la base de musiques contemporaines comme la chanson, le rock, la pop et le jazz, dont certaines développeront leur règles spécifiques.Le système tonal est le résultat d'une lente évolution qui s'est opérée sur quelque six siècles — de l'époque carolingienne à la fin du Moyen Âge. Apparu à partir de la Renaissance, le système tonal a succédé au système modal dont il est à la fois un appauvrissement et un enrichissement. Enrichissement harmonique, il correspond aussi à un net appauvrissement mélodique, puisqu'on passe de huit modes (et plus) à deux seulement, qui ont tendance à uniformiser les lignes mélodiques. Au cours du XVIe siècle, les deux systèmes ont plus ou moins coexisté, à l'intérieur d'une échelle non encore tempérée : simplement, la ""note sensible"" tendait à envahir les modes et préludait à l'introduction de la gamme ""tonale"" . Par ailleurs, quoique abandonné par l'école strictement classique de la seconde moitié du XVIIIe siècle, le système modal est encore largement utilisé jusqu'au XXIe siècle : depuis la fin du XIXe siècle (essor des écoles nationales en Europe), ce sont les systèmes modaux (issus des musiques traditionnelles) qui ont permis de ré-enrichir la tonalité.Le système tonal repose sur les sept degrés hiérarchisés de l'échelle diatonique, organisés autour du degré fondamental qu'est la tonique — « pôle d'attraction » des autres notes — ainsi que sur une note dominante, cinquième degré de la gamme : la tonique et la dominante permettent d'établir un système dynamique, la dominante créant une tension, que résout la tonique, les autres degrés se rattachant à l'une ou à l'autre.Parmi les sept degrés d'une gamme, trois sont considérés comme les « très bons degrés », le premier, le quatrième et le cinquième, parce qu'ils remplissent les trois « fonctions tonales » : fonction de tonique (Ier degré), fonction de dominante (Ve degré) et fonction de sous-dominante (IVe degré). La sous-dominante élargit la tonique (cadence plagale) et amène une cadence (cadence parfaite). Le deuxième degré est employé comme la sous-dominante (il a l'avantage d'être plus dynamique et d'évoluer plus facilement vers une dissonance) et le sixième degré permet une ambigüité conclusive, en même temps qu'une ouverture d'un mode majeur vers ton relatif (dans le même souci d'ambigüité). Seul le septième degré, ""note sensible"", reste fondamental mélodiquement mais inusité harmoniquement. Ce degré est plutôt employé comme un cinquième degré, la dominante, auquel est ajouté une septième mineure et soustrait de sa fondamentale.Le mot mode définit la structuration d'une échelle musicale (gamme de fréquences), qui, dans les systèmes modal et tonal, est hiérarchisée. Dans un sens plus restreint, il se réfère au schéma d'intervalles constitutif de chacun des deux « modes », majeur et mineur, de la musique tonale. On n'emploie pas le mot, pour les structures non hiérarchisées (musique atonale en général). Il désigne plus largement toute organisation définie à partir de n'importe quel paramètre sonore, dans la musique moderne (modes rythmiques, modes d'intensités, de timbres…).Au cours du XVIe siècle, les modes anciens médiévaux disparaissent progressivement : seuls subsisteront les modes majeur et mineur qui sont les héritiers des précédents. Cette suprématie ne sera remise en cause qu'à la fin du XIXe siècle, avec la réutilisation des modes anciens et l'introduction de nouvelles échelles issues des musiques traditionnelles européennes.Le point commun entre ces deux modes, majeur et mineur, est l'apparition d'une ""note sensible"" (septième degré à distance d'un demi-ton de la tonique), qui permettra l'élaboration de la dynamique tonale (Cette dynamique repose, en effet, sur l'alternance ""tonique-dominante-tonique"", qui permet la résolution de la tension engendrée par la note sensible sur le degré de la tonique, mélodiquement; harmoniquement, cette résolution est construite sur  l'enchaînement ""dominante-tonique"").La différence entre le mode majeur et le mode mineur repose précisément sur la position des tons et des demi-tons de l'échelle diatonique par rapport à la tonique. Plusieurs schémas d'intervalles s'imposent : Le mode majeur est formé de deux tétracordes contenant chacun un ton, un ton, un demi-ton ;Dans le mode mineur, un ton, un demi-ton, un ton forment le premier tétracorde ; le second tétracorde varie en fonction des gammes :un demi-ton, un ton, un ton pour la gamme mineure naturelle,un demi-ton, un ton et demi, un demi-ton pour la gamme mineure harmonique,un ton, un ton, un demi-ton (tétracorde majeur) pour la gamme mineure mélodique ascendante.Dans tous les cas, les deux tétracordes sont séparés par un ton.On transpose ces schémas pour obtenir les différentes tonalités, à partir des modèles initiaux de do majeur (notes naturelles) et la mineur (seule la sensible est altérée). On distinguera tout de même les modes mélodiques (deux possibilités pour le mode mineur) et les stricts modes harmoniques. À l'écoute, le mode majeur apparaît franc et lumineux, le mode mineur restant beaucoup plus en demi-teinte, avec une pointe de nostalgie. Cependant, depuis l'époque romantique, certains compositeurs en font une utilisation inverse.En musicologie, le mot mode désigne uniquement la composition d'une échelle, quelle qu'elle soit. Du fait de la structure hiérarchique des échelles modales et tonales, le mot mode tend à se référer à une échelle hiérarchisée. En ce sens, les échelles atonales ne correspondent pas à des modes.Dans un sens plus restreint, le mot mode se réfère à la constitution des deux échelles tonales possibles, majeure et mineure. Ce cadre utilise le terme de tonalité qui se caractérise par deux paramètres :Le ton, c’est-à-dire la note sur laquelle s'établit la tonique de l'échelle considérée ;Le mode, c’est-à-dire le système de séquence d’intervalles, majeur ou mineur, appliqué à ce ton.Les deux échelles de référence du système majeur/mineur sont le Do majeur et La mineur, elles ne demandent d’appliquer aucune altération sur les notes usuelles pour obtenir la séquence d’intervalles souhaitée.Enfin certains compositeurs, comme Olivier Messiaen, ont élargi la notion de mode aux rythmes (éléments constitutifs des échelles rythmiques), aux intensités (gammes d'intensités), aux timbres, et à tous les paramètres du son musical.La quinte juste est à la racine des musiques traditionnelles du monde entier, même de transmission orale ; en Europe aussi, la quinte juste accompagne les chants populaires (par exemple, en ""bourdon"" tenu). C'est donc cet élément ancestral et populaire que l'on retrouve, à la racine du système savant.Parmi les mouvements mélodiques les plus utilisés dans la musique tonale, l'intervalle de quinte juste occupe une place privilégiée — cf. cycle des quintes. C'est un intervalle à la fois structurel (ordre des gammes), harmonique (présent par exemple dans le mouvement de basse de la tonique à la dominante, ou dans l'accord parfait) et mélodique (de nombreux chants ou phrasés musicaux vocaux tournent autour de la quinte ou s'élaborent en ambitus de quinte).Harmoniquement, ce mouvement régit fréquemment la basse. L'enchaînement des trois bons degrés (V - I - IV) produit deux fois cet intervalle. Par ailleurs, l'enchaînement de l'accord de septième de dominante et de l'accord parfait de tonique — archétype du système tonal, appelé cadence parfaite — est également construit sur un mouvement harmonique de quinte juste à la basse (et un mouvement mélodique de seconde mineure ascendante, la ""note sensible"" se résolvant sur la tonique) à la partie supérieure (mélodique)  .Presque toute la musique tonale est construite sur une alternance de moments de tension (autour de la région de la dominante) et de moments de détente (retour à la tonique).D'autre part, la dissonance (intervalles de seconde ou de septième, par exemple), réalisée selon certaines règles, est ressentie comme une tension nécessitant une détente — cette dernière consistera en une résolution sur la consonance de base la plus proche, à la suite d'un mouvement mélodique d'attraction. Pour plus d'informations sur les enchaînements d'intervalles harmoniques, sur les dissonances, les consonances, etc., consulter l'article Mouvement harmonique.La sensible, VIIe degré des modes majeur et mineur, par son attraction vers la tonique située au demi-ton diatonique supérieur, participe également de ce mécanisme, et constitue donc l'un des deux éléments de la relation d'attraction fondamentale du système tonal .Enfin, concernant l'intervalle de quinte employé à la basse harmonique, à partir de la tonique, on peut dire que la quinte juste ascendante — ou son renversement, la quarte juste descendante — est du côté de la tension, tandis que la quinte juste descendante — ou son renversement, la quarte juste ascendante — est du côté de la détente (ces intervalles représentent le trajet de la dominante à la tonique, et son inverse).Jusqu'à la Renaissance, le procédé d'écriture — appelé contrepoint — consiste en une superposition de mélodies. À une mélodie (""teneur"" en valeurs longues, placée à la basse, à l'époque), on adjoint des ""contrechants"" (mélodies complémentaires) qui se rejoignent en fin de strophe ou de phrase; ensuite, l'évolution majeure sera le passage de la mélodie au soprano, les contrechants étant alors écrits au-dessous. Cela permettra l'émergence progressive du procédé d'écriture tonale — appelé faussement ""monodie"", c'est-à-dire ""mélodie accompagnée"", dès l'opéra baroque, ou mélodie accompagnée d'accords à  partir de la période classique  — qui remplace par des enchaînements harmoniques la polymélodie de la Renaissance. Dans une ""harmonisation"" tonale, le compositeur  privilégie une mélodie particulière, généralement confiée à la partie supérieure, soutenue par la partie basse — chargée de jouer les fondamentales et de remplir  les fonctions tonales — tandis que les parties intermédiaires remplissent la mission, plus modeste, de complément harmonique (ce qui n'empêche pas les compositeurs de toutes les périodes, ""Classique"" incluse - Mozart, Haydn, d'écrire de sublimes contrechants dans les parties intermédiaires, restant aussi sensibles (et formés) au contrepoint qu'à l'harmonie.Dans la dimension polyphonique d'une œuvre, on distinguera la polymélodie, qui relève du contrepoint, et les accords, que régissent les règles d'enchaînement de l'harmonie. De fait, tous les compositeurs ont une double conscience, à la fois horizontale (jeu des contrechants) et plus strictement verticale (rapports de simultanéité).Le mot polymélodie — que l'on abrège malencontreusement en polyphonie, est habituellement associé au système modal. On parle alors, plus précisément, de « contrechants » issus d'un son procédé de composition, le contrepoint — du XIIe siècle au XVIe siècle.C'est au contraire le mot harmonie qui est généralement associé au système tonal. On parle alors, plus précisément, d'« harmonie tonale » ou  encore, d'« harmonie classique », technique d'écriture qui complète le contrepoint au cours du XVIe siècle, et qui se perpétue jusqu'au début du  XXe siècle. La principale caractéristique de l'harmonie classique par rapport à la polyphonie, est de faire de l'accord une entité autonome avec une fonctionnalité spécifique.Au XVIIIe siècle, influencés par les découvertes dans le domaine de la physique, les musiciens cherchent à unifier les échelles musicales. Cette unification, rendue possible par le tempérament des instruments, et produisant une échelle unique — la gamme tempérée — est indissociable du système tonal. En 1722, Rameau, dans son Traité de l’harmonie réduite à ses principes naturels, et Bach, dans Le clavier bien tempéré, donnent, l’un la méthode, l’autre la mise en pratique, du système tempéré qui installe la tonalité, nouvelle logique dans la construction musicale.Parvenus aux confins de l’exploration harmonique et stylistique de la musique romantique, les compositeurs du début du XXe siècle essayent de se délier du système tonal, et de purifier l’écoute de la musique de ses éternels couplages entre tensions et détentes, que la tonalité lui a inculquées. L’« extra-tonalité » s’emploie à explorer, parfois en les combinant, tantôt la modalité — Debussy, Moussorgsky, etc. —, tantôt l’espace harmonique dans son entier — dodécaphonisme —, tantôt l’espace rythmique — Stravinsky… La musique dodécaphonique — œuvres d'Arnold Schönberg, Alban Berg, ou Anton Webern —, utilise des échelles de douze sons, dont aucun degré n'a plus d'importance que les autres. Elle ouvre la voie à l'atonalité.Depuis la fin du XIXe siècle, le compositeur cherche, crée, bouleverse, tonalités, modes, harmonie, formes, instruments… Il essaie tout, utilise tout, même les éléments d'un lointain passé venus jusqu'à lui. Le XXe siècle est l'époque des combinaisons les plus inattendues, le règne de l'audace, de la nouveauté, du paradoxe.Dans un tel contexte, on assiste à une contestation générale du système et des règles d'école : l'échelle diatonique et le système tonal, le principe de la résolution de la dissonance, la mesure et la régularité métrique, etc.Le compositeur dispose de l'apport classique — le système tonal avec ses modes majeur et mineur —, mais il exhume le mode mineur naturel, ainsi que les modes anciens, utilise certaines échelles appartenant à d'autres civilisations — gamme tzigane, gamme arabe, etc. —, ou encore, les modes défectifs, c'est-à-dire, possédant moins de sept degrés — telle que la gamme pentatonique, parfois appelée gamme chinoise.Ces diverses échelles — ou gammes — peuvent être transposées dans n'importe quelle échelle diatonique grâce à l'armure, ou armature, terme moins usité.La gamme habituelle, la gamme classique, est à la fois heptatonique et diatonique, ce qui signifie que, d'une part, cette gamme est composée de sept degrés et que, d'autre part, elle est toujours constituée, quel que soit le mode, de tons et de demi-tons diatoniques.Au début du XXe siècle, les compositeurs ont imaginé deux nouvelles gammes dérivées de l'échelle diatonique classique, mais exclusivement formées, l'une, de demi-tons — la gamme chromatique, que nous avons déjà rencontrée —, l'autre, de tons — la « gamme par tons ». Ces deux gammes sont donc utilisées de façon atonale, c'est-à-dire sans référence au système tonal. Gamme chromatique La gamme chromatique était connue des compositeurs classiques — Monteverdi, Bach, Mozart, etc. —, mais seulement dans le contexte du système tonal.Or, dès le début du XXe siècle, certains compositeurs modernes utilisent les 12 sons de l'échelle chromatique hors du système tonal : ces 12 sons, traités de façon équivalente, peuvent aussi être employés dans un ordre préétabli — appelé série —, sans tonique, sans dominante, sans aucune fonction tonale, etc. Ce système créé de toutes pièces par des compositeurs tels que Schönberg, Berg et Webern, s'appelle le dodécaphonisme et, dans le second cas, musique sérielle. Dans le strict système dodécaphonique, les douze demi-tons ne sont pas hiérarchisés ; dans une œuvre sérielle, c'est la série qui recrée une fonctionnalité dans l'œuvre.Le dodécaphonisme impliquant le tempérament égal, est par là tributaire de l'écriture traditionnelle — appellation des sept notes, appellation des intervalles, pour ne rien dire de la mesure et des instruments… C'est, d'ailleurs, malgré quelques recherches en quarts de tons, ce qui le limite. Gamme par tons La gamme par tons — parfois improprement appelée gamme chinoise —, est une gamme hexatonique dont les six degrés sont séparés par des tons : elle a été très utilisée par Debussy.Tout au long du XXe siècle, de nombreux compositeurs démontent méthodiquement et progressivement les différentes pièces de la musique traditionnelle et du système tonal, et s'acheminent vers de nouveaux systèmes.Voici les principales étapes de ce mouvement : polytonalité : procédé associant harmoniquement et mélodiquement des éléments appartenant à des tonalités différentes : tout d'abord deux — bi-tonalité —, puis davantage, jusqu'à aboutir à la musique atonale, dont la musique sérielle est un exemple notoire ;utilisation d'échelles formées d'intervalles inférieurs au ton : nouvelles échelles, désormais totalement indépendantes de l'échelle diatonique traditionnelle : échelles au tiers de ton, au quart de ton (ou autres micro-intervalles), etc. ;musique aléatoire : système privilégiant l'imprévu au détriment du prévu — musique à faire, depuis une partition —, en prenant en compte le paramètre du hasard ;musique concrète : système basé sur le collage et le traitement de sons enregistrés sur bande ;abandon des instruments classiques — jugés trop dépendants des conceptions traditionnelles — et utilisation directe des sons artificiels produits par des instruments électroniques ou électro-acoustiques.Le système de notation traditionnel — le solfège — qui depuis le XIIe siècle environ, était le seul et unique moyen de conserver une trace, même imparfaite, du geste musical, va sérieusement être concurrencé par l'invention — dès la fin du XIXe siècle — et surtout la banalisation, au cours de la deuxième moitié du XXe siècle, des divers procédés d'enregistrement et de diffusion sonore : disques, magnétophones, ordinateurs et synthétiseurs.Par ailleurs, le système de notation traditionnel est inadapté à la plupart des musiques nouvelles. Chaque musique nouvelle suppose son propre système de notation, ses propres règles de codification, « son propre solfège ».Cependant, et paradoxalement, un siècle après les premières œuvres sérielles, la très grande majorité de la musique consommée dans le monde occidental reste tributaire du système tonal — musique populaire, musique industrielle, et même, tout un pan de la musique savante. « De gré ou de force, nous baignons tous dans la tonalité. »Analyse harmoniqueGammes et tempéraments dans la musique occidentaleHarmonie tonaleTonalitéHeiner Ruland : ""Évolution de la musique et de la conscience - Approche pratique des systèmes musicaux"", ÉAR, Genève, 2006  (ISBN 2-88189-173-X).Philippe Gouttenoire et Jean-Philippe Guye, Vocabulaire pratique d'analyse musicale, DELATOUR FRANCE, 2006, 128 p. (ISBN 978-2-7521-0020-7) Portail de la musique classique   Portail de la musique"
musique;"En musique, le timbre est l'ensemble de caractéristiques du son qui permet de reconnaître un instrument ou une voix — par opposition à la note ou aux mots. En musique comme dans les autres domaines, le « timbre » d'une voix humaine est l'ensemble de ses caractères distinctifs.La notion de timbre apparaît en Europe au XVIIIe siècle dans la musique polyphonique. Au XXe siècle, la synthèse électronique des sons a permis le développement de timbres entièrement nouveaux. La musique pop  se caractérise par son invention de timbres, notamment de guitare électrique ; la musique électronique les a diversifiés et enrichis. L'étude de la perception des timbres, commencée au XIXe siècle avec les expériences psychoacoustiques de Helmoltz sur les sons continus, s'est poursuivie avec ces nouveaux instruments en incluant la dynamique sonore, puis en s'inscrivant dans les recherches sur la cognition, considérant que la reconnaissance des timbres est une instance de celle des formes.Par ailleurs, un timbre est une cloche, une clochette ou, anciennement, un petit tambour, fixes, que l'on frappe avec un marteau ; c'est aussi un boyau, un ressort ou un balai, effleurant une partie vibrante d'un instrument de musique pour en colorer le timbre comme celui d'un mirliton — par exemple sur une caisse claire, un balafon, une kora ; dans la chanson, un timbre était au XIXe siècle un air connu de tous sur lequel on mettait des paroles au goût du jour.Hector Berlioz présente « l'étude fort négligée jusqu'à présent, de la nature du timbre, du caractère particulier et des facultés expressives de chacun d'eux » comme un des thèmes principaux de son Traité d'instrumentation et d'orchestration , dont l'objet est justement l'organisation des timbres des instruments de l'orchestre.L'identification des timbres est considérée, en Occident, comme une compétence musicale de base. Elle permet de séparer, à l'écoute d'un ensemble, les lignes mélodiques des parties que jouent des instruments différents. Prokofiev a composé Pierre et le Loup pour en faciliter l'acquisition aux enfants. Il comprend des parties de violons, de flûte, de hautbois, de clarinette, de cors, de basson, de trompette. Cependant, ce qu'on appelle théorie de la musique ne le mentionne pas. Les partitions indiquent les noms des instruments : il faut que cela vaille comme description du timbre — avec des indications comme pédale, sourdine, col legno , etc., alors que la hauteur, la durée et la puissance sonore figurent explicitement. Contrairement à ces caractéristiques, le timbre ne correspond pas à une échelle de valeurs quantifiables, c'est une qualité, et non un degré ; on parle parfois de « couleur de son », comme en allemand où Klangfarbe équivaut au français timbre.La notion de timbre « n'est pas facile à cerner ». Comme la puissance mécanique de la vibration gouverne le volume sonore, et la fréquence dominante ou la fondamentale — dans le cas de sons musicaux — détermine la hauteur, Helmholtz a cherché à relier le timbre à la répartition des harmoniques. Cette analyse est insuffisante, au moins pour la voix humaine : les voyelles modifient cette répartition, mais n'empêchent pas de distinguer les locuteurs ou chanteurs. La définition courante du timbre est négative et psychologique : « caractère de la sensation auditive qui différencie deux sons de même hauteur et de même intensité… »Contrairement aux sons de la langue ou aux notes de musique, on ne peut pas décrire ou produire vocalement des timbres ; on peut seulement donner des analogies — dire qu'un timbre est rauque ou perçant, par exemple —. La mémoire des timbres est, de ce fait, capitale pour l'étude de la cognition auditive. Les recherches ont montré que la mémoire auditive n'a pas de rapport avec la mémoire motrice, et que la mémoire du timbre est particulièrement saillante chez les non-musiciens, tandis que les musiciens retiennent plus la mélodie et l'harmonie.La création et le perfectionnement des timbres est longtemps resté du domaine de la pratique des facteurs d'instruments. L'organologie décrit les instruments de musique, mais pas leur son. L'esprit scientifique s'est d'abord attaché à des notions reliées à des grandeurs physiques mesurables isolément : la hauteur, dont on sait depuis l'Antiquité qu'elle est en rapport avec la longueur de l'élément vibrant, et l'intensité, en rapport avec la puissance. Le timbre serait ce qui permet à l'être humain de distinguer finement des sources quand ces deux grandeurs sont égales. Il n'a pas de définition physique. On a cherché, depuis le XIXe siècle, à relier cette perception à des mesures physiques, sans succès. Certains théoriciens comme Schoenberg en ont conclu qu'il faut revenir sur la distinction entre hauteur. « c’est par son timbre — dont une dimension est la hauteur — que le son se signale ». Comme la hauteur se décompose en hauteur spectrale et en hauteur tonale, le timbre s'analyse en plusieurs composantes.Les premiers travaux de Joseph Fourier sur la décomposition d'une fonction périodique en une somme de fonctions sinusoïdales simples avaient laissé penser que la solution du problème devait se trouver dans l'analyse harmonique du son.Les appareils d'analyse de son de plus en plus perfectionnés sont venus infirmer ces hypothèses, qui ne sont correctes que pour un son périodique. Le sonagraphe, disponible à partir des années 1950, a permis d'explorer plus avant la décomposition du son en partiels et en harmoniques. La seule courbe d’enveloppe, qui exprime l’amplitude globale en fonction du temps, est apparue comme inadéquate à la description des caractéristiques du timbre. C'est la combinaison de la variation de l'amplitude de chaque composante harmonique (potentiellement une infinité) qui est nécessaire.Seul le sonagraphe, appareil de représentation graphique de la totalité des dimensions du phénomène (temps - fréquence - amplitude) a permis de suivre un spectre évolutif, dont chaque composante a une intensité relative qui évolue avec le temps.Les performances auditives humaines sont limitées en fréquence : au-delà de 5 kHz, on ne distingue plus les hauteurs que très approximativement, et on ne peut percevoir l'harmonicité des partiels. Au-delà de 15 kHz, les sons sont inaudibles, quoique des conjonctions de sons inaudibles puissent engendrer, par intermodulation, des perceptions sonores. Cela explique que pour les notes très aiguës, dont les harmoniques sont vite repoussées au-delà de cette limite, il devient difficile de distinguer la nature de l'instrument, sinon par ses caractéristiques dynamiques. Le sonagraphe limite en général son exploration des fréquences à 8 kHz, donnant les principaux formants discernables.D’autres éléments, physiquement simples à décrire, ne sont qu’intuitivement perçus comme influençant notre perception du timbre, et l’importance de leur rôle au sein du champ de cette reconnaissance est difficile à appréhender. La brillance, les formants, par exemple, mais aussi le vibrato, la texture sonore.Helmholtz dans sa théorie physiologique de la musique, a présenté une théorie s’appuyant sur la mise en évidence des harmoniques d’un son périodique et le calcul de leur intensité au moyen de résonateurs.Il découvrait parallèlement les fréquences de partiels inharmoniques et observait leur importance dans la nature du son. À sa suite, Carl Stumpf, philosophe et psychophysiologiste allemand, notait dans les années 1930, l'importance des transitoires (portion infime de l’attaque du son), du vibrato, des composantes spectrales (régions formantiques), de la chute dans la dimension du timbre. La portion d’attaque est essentielle à l’identification de l’instrument, ce que l'on sait également grâce aux travaux de Pierre Schaeffer. Ces transitoires d’attaque sont des phénomènes qui peuvent durer de 20 ms jusqu’à 200, voire 300 ms, selon les instruments, et qui affectent toute modification de la perception du timbre. Ces travaux révélèrent que les caractères proprement musicaux des sons sont inscrits dans la partie stationnaire.Mais on doit surtout aux travaux du Laboratoire d'acoustique musicale de l’université Paris VI (LAM), dirigé par Émile Leipp dans les années 70, d'avoir montré que bien des composantes du son, discrètes et continues; ne sont que des composantes psychologiques, psychoacoustiques, qui ne prennent place qu’au niveau cérébral, neuronal de la reconnaissance du timbre. L’étude des modes de jeu de certains interprètes révèle par exemple que la phase stationnaire est continuellement différenciée et varie perpétuellement au cours de l’exécution d’une œuvre.Beaucoup de composantes timbrales sont donc des éléments vivants, dynamiques : même si notre oreille ne peut les reconnaître intuitivement, elle sait le faire inconsciemment.À partir de ces analyses, la synthèse sonore a procédé par décomposition-recomposition pour élaborer ses modèles, et a ainsi permis de franchir un pas supplémentaire dans la compréhension des mécanismes de la reconnaissance du timbre.Commencée pour l’analyse-synthèse des sons cuivrés par Jean-Claude Risset (entre 1964 et 1969), l'étude des composantes spectrales du timbre des instruments fut reprise par James Andrew Moorer et John Michael Grey qui mirent en exergue un spectre à trois dimensions (fréquence, intensité, temps), ainsi que par Dexter Morrill dans une remarquable étude de la trompette : ces analyses ont permis de mettre en valeur l’évolution temporelle du spectre, et révélé l’importance de l’attaque et de l’enveloppe dynamique.Elles démontrent l’émergence progressive de certains harmoniques (de rang élevé) plus forts dans la partie stationnaire que dans l’attaque et la décroissance. Le concept d’espace de timbres introduit (en 1975) par J. M. Grey a ouvert la voie à la notion controversée de matériau musical, en le situant dans une représentation multidimensionnelle. Ainsi, le passage à des représentations à n (>2) dimensions est particulièrement significatif de la prise de possession des paradigmes mathématiques pour la représentation du timbre.Nous avons donc tout à fait besoin d’une vision neuve pour évaluer ce champ conceptuel de timbre. Juger un timbre revient à comparer deux matrices d’information. Du coup, les recherches qui visent à faire du timbre un élément de construction révèlent une difficulté essentielle et provoquent une réflexion sur la fonction formelle d’un paramètre mal connu.Petit à petit, l’intégration de certaines données scientifiques permet à de nouveaux paramètres de la vibration sonore de prendre place dans notre connaissance de la formation du timbre et dans celle, plus créatrice, du domaine d’influence de la fonction timbrale. Il en est ainsi, comme nous l’avons dit, des transitoires, de l’attaque, du vibrato, des composantes spectrales (régions formantiques), de la chute, etc. Mais si un classement par formes (formes d’ondes, d’attaques, d’enveloppe, de spectre) suffirait à l’établissement d’une typologie du sonore, il n’est pas sûr qu’un tel classement puisse s’effectuer au niveau musical, tant ce niveau se situe dans une autre hiérarchie. En fait, bien des composantes du son (discrètes et continues) ne sont que des composantes psychologiques, qui ne prennent place qu’au niveau cérébral, neuronal, et les données auxquelles elles se rattachent dans la musique sont fondées sur une base plus esthétique que rationnelle.On définit le timbre de la voix humaine comme l'ensemble des caractéristiques qui permettent de l'identifier.Enfant puis adulte, le timbre de la voix change lorsque l'individu grandit. Il peut devenir plus grave ou plus aigu, selon les personnes. En travaillant souvent sa voix, le timbre peut se modifier.Un timbre est à l'origine une cloche fixe ou un petit tambour, frappés avec une baguette ou un maillet.C'est aussi un boyau, un ressort ou un balai, effleurant une partie vibrante d'un instrument de musique pour en colorer le timbre comme celui d'un mirliton — par exemple sur une caisse claire, un balafon, une kora.Autre sens du mot timbre en musique : un timbre ou pont-neuf désignait au XIXe siècle un motif ou un air connu sur lequel on mettait des paroles au goût du jour pour en faire des chansons, comme c'était l'usage dans les sociétés chantantes et goguettes.Michèle Castellengo (préf. Jean-Sylvain Liénard et Georges Bloch), Écoute musicale et acoustique, Paris, Eyrolles, 2015 (ISBN 9782212138726, présentation en ligne), p. 287-385 Ch. 7 « La question du timbre ».Pierre Schaeffer, Traité des objets musicaux : Essai interdisciplines, Paris, Seuil, 1966, 2e éd., 713 p., notamment Livre III, « Corrélations entre le signal physique et l'objet musical », pp.159-260Robert G. Crowder, « La mémoire auditive », dans McAdams & alii, Penser les sons, Paris, PUF, 1994, p. 123-156, section 6 « La mémoire et les images liées au timbre musical », p. 149-152Helmholtz (Hermann Ludwig von), Die Lehre der Tonempfindungen als physiologische Grundlage für die Theorie der Musik (1863), Théorie physiologique de la musique, Paris, Masson, 1868, reprint Sceaux, J. Gabay, 1990, 544 p.Jean-Claude Risset, Hauteur et timbre des sons : Rapport IRCAM, Paris, IRCAM Centre Georges Pompidou, novembre 1978, 19 p..Jean-Claude Risset, « Quelques aspects du timbre dans la musique contemporaine », dans Arlette Zenatti, Psychologie de la musique, Paris, Presses universitaires de France, coll. « Psychologie d'aujourd'hui », 1994, p. 87-114.Barrière (Jean-Baptiste) (coord.), Le timbre, métaphore pour la composition, IRCAM, Christian Bourgois éditeur, Paris, 1991, 594 p.Son musicalAcoustique musicaleMélodie de spectre constantRecueil de timbresLAM (laboratoire d'acoustique musicale de l’université Paris VI) Portail de la musique   Portail de la musique classique   Portail de l’opéra"
musique;Une chanson, ou un chant, est une œuvre musicale composée d'un texte et d'une mélodie destinée à être interprétée par la voix humaine. Cette interprétation peut se faire sans accompagnement instrumental, c'est-à-dire a cappella, ou au contraire être accompagnée d'un ou plusieurs instruments (guitare, piano, groupe, voire un big band ou un grand orchestre symphonique). Elle peut être à une voix (monodie) ou à plusieurs (polyphonie) comme dans une chorale.Simple comptine enfantine de quelques mots ou longue chanson de geste (voir les 4 002 vers de La Chanson de Roland du XIIe siècle), cette expression littéraire et musicale peut revêtir des formes et des structures diverses (couplet/refrain, strophe ou laisse, canon, mélodie accompagnée ou lied allemand…) et couvrir des genres bien différents comme la musique traditionnelle ou folklorique, la musique classique ou ethnique, le rock 'n' roll ou le  jazz, le rap ou le slam.La création d'une chanson nécessite généralement la participation de deux artistes : l'auteur des paroles (parolier) et le compositeur de la musique. Leur travail se fait à leur gré, la mélodie naissant parfois du texte ou le texte de la mélodie, ou même les deux simultanément comme pour certains auteurs-compositeurs.L'interprète (le chanteur ou la chanteuse) donne vie à la création. Certains artistes comme Georges Brassens, Jacques Brel, Charles Aznavour ou Jean-Jacques Goldman, (parmi bien d'autres), réunissent les trois fonctions et sont alors nommés auteurs-compositeurs-interprètes ou chansonniers dans la tradition jusqu'au début du XXe siècle.Parfois, un quatrième musicien intervient : l'arrangeur musical, celui qui harmonise et donne la couleur particulière à la chanson par son orchestration (organisation des instruments d'accompagnement, notamment lors d'un enregistrement).Une chanson est composée le plus souvent d’une introduction, d’un couplet, d’un refrain, d'un pont et d’une fin. La longueur de ses éléments varie en fonction des choix opérés par les auteurs-compositeurs et aussi en lien avec les médias qui diffusent les chansons (notamment la radio, qui impose des critères plus ou moins stricts).  Le couplet est l'une des deux structures mélodiques constitutives se déroulant en alternance avec le refrain et dont la principale caractéristique est de présenter des paroles différentes à chaque nouvelle exposition, ce qui permet de faire évoluer le contenu du récit. La musique, c’est-à-dire les accords du couplet souvent ne changent pas au cours de la chanson.Le refrain est la répétition régulière de paroles d’une même chanson. C’est la partie de la chanson dont les gens se souviennent le plus souvent. Généralement, les différents refrains d'une même chanson possèdent non seulement les mêmes paroles, mais aussi la même mélodie.Il peut y avoir un pré-refrain qui est une courte partie qui se trouve directement avant un refrain mais qui se distingue du couplet par les paroles et sa musique. Le pont désigne une partie dont les accords se différencient des accords principaux. C’est une partie distincte de la chanson qui n’est pas du tout pareil au couplet ou au refrain. Entre chaque section il est aussi possible d’avoir un ou plusieurs interludes qui ne contiennent pas de paroles.Boris Vian, En avant la zizique… et par ici les gros sous, Le Livre de Poche, 1997, 192 pages  (ISBN 9782253140887). Portail de la musique   Portail des arts
musique;La trompette est un instrument de musique à vent de la famille des cuivres clairs. Elle est fabriquée dans un tube de 1,50 m de long comme le cornet. Le métal utilisé pour fabriquer la trompette est surtout le laiton (en moyenne 70 % de la trompette est fabriquée avec du laiton). Pour en jouer, on utilise souvent 3 pistons (parfois 4 dans la piccolo) ainsi que de l'air (colonne d'air).Deux trompettes ont été retrouvées dans le tombeau de Toutânkhamon (une en or et une en argent), ce qui semble indiquer l'origine très ancienne et peut-être égyptienne de cet instrument. En Grèce, la trompette alors appelée salpinx était considérée comme un instrument de guerre. On y trouvait trois épreuves : le son le plus fort (avec le plus de décibels), le son portant le plus loin, et le son le plus aigu. À Rome, on utilisait le cornu, le buccin (buccina) et le lituus. Les Celtes utilisaient le carnyx. Les Hébreux avaient également trois types de trompettes ou cors, le hazozerah, le chofar et le keren, ou du moins trois substantifs pour désigner cet instrument,.La trompette à la Renaissance ne comporte pas de piston. À cette époque, les européens ont empruntés aux Arabes les mots de la « buisine » (« buysine », « buzine », « busine »...) ainsi que le nom de « trumpa ».La crise de la trompette a duré soixante-cinq ans (1750-1815). D’une part, l’art du clarino avait atteint un sommet difficile à dépasser et, d’autre part, l’apparition de l’idéal bourgeois faisait incarner à la trompette un aspect héroïque démodé.L’époque classique montre un brusque changement dans la fonction des trompettes. Après avoir rempli une fonction héroïque qui donne le ton sous forme mélodique, la trompette se fond maintenant dans les tutti. Pour continuer à jouer son rôle héroïque, elle ne fait que couronner brièvement les crescendo. Elle doit s’adapter à la variété des tonalités, on voit donc apparaître des trompettes en fa, sol, si                    ♭              {\displaystyle \flat }   ou la. Dans la musique classique, le registre du clarino ne monte plus aussi haut que dans le baroque : on monte rarement au-dessus du sol (« juste au-dessus de la portée »), parfois on rencontre un la ou un do mais très rarement.Dès la fin du baroque, on a essayé de rendre la trompette chromatique car la plupart des notes à jouer se trouvent maintenant dans la troisième octave des partiels, les notes sont plus écartées donc les possibilités sont plus restreintes. Différentes techniques vont essayer de trouver une solution à ce problème.Une des plus anciennes de ces techniques est le bouchage qui fut inventé en 1775, puis inutilisé à partir de 1840. L’idée vient du corniste Anton Joseph Hampel (de), qui en 1750 avait remarqué qu’en introduisant la main dans le pavillon, on pouvait faire baisser la note émise d’un demi, voire un ton complet. La technique n’a pas été mise en œuvre tout de suite sur les trompettes car leur forme ne permettait pas à l’instrumentiste de mettre sa main au niveau du pavillon. C’est en 1777 qu’un facteur « enroula » plus la trompette pour lui donner une forme de demi-lune. On bouchait le pavillon avec trois doigts de la main droite. Le bouchage influence cependant le timbre de la trompette. En France, David Buhl fut le plus éminent des trompettistes jouant avec ce procédé. Dans sa méthode, il distinguait la trompette d’ordonnance (instrument de cavalerie en mi                    ♭              {\displaystyle \flat }  ) et la trompette d’harmonie (instrument d’orchestre en sol). On pouvait mettre cette dernière dans des tons plus graves à l’aide de coulisses de rechange et on obtenait les demi-tons au-dessous d’une note donnée en bouchant le pavillon. Le gros défaut de cette technique est l’inégalité sonore entre les notes ouvertes et bouchées.La deuxième technique est la trompette à clefs. Elle a les mêmes dates d’apparition et de disparition que le bouchage. L’idée commença, encore une fois, à être expérimentée sur le cor. La première trompette à clefs a été construite en 1777 mais ne connut aucun succès car le timbre caractéristique de la trompette disparaissait presque entièrement et était à mi-chemin entre la trompette et le hautbois. Indépendamment les uns des autres, plusieurs inventeurs firent différents essais dans le même sens. C’est en 1793 qu’un amateur nommé Nessman a mis au point une trompette à clefs qui gardait le timbre de la trompette et avec laquelle il pouvait monter une gamme chromatique. L’expérimentateur le plus heureux et en même temps le plus grand virtuose de la trompette à clefs fut A. Weidinger. D’ailleurs pour lui et sa trompette à clefs, Haydn, un de ses amis, composa son fameux concerto en mi bémol majeur, qui fait usage du registre du clarino et peut être joué avec seulement trois clefs, alors que celui de Hummel a un plus grand choix de notes graves et en nécessite une quatrième. Le gros défaut de cet instrument est le même que pour la trompette à boucher : l’inégalité entre les notes où certaines clefs sont ouvertes et ces mêmes notes lorsqu’elles sont toutes fermées.La troisième technique a été utilisée surtout en Angleterre entre 1790 et 1885 : c’est la trompette à coulisse. Comme son nom l’indique, le moyen utilisé ici pour rendre la trompette chromatique est la coulisse. Cette coulisse, qui est en forme de U comme sur un trombone mais moins longue que sur celui-ci, est plus proche de l’instrument et comporte un mécanisme permettant de revenir à la position initiale. Elle était appréciée grâce à sa sonorité noble et naturelle mais c’était plus un instrument d’orchestre que de solo à cause notamment de sa raideur mécanique. La trompette à coulisse se construisait en fa mais comportait des coulisses pour l‘accorder dans des tons inférieurs. Elle a subsisté plus longtemps que la trompette à boucher et celle à clefs grâce à la forte personnalité des personnes qui la défendaient. Piston La grande invention du XIXe siècle pour la trompette est le piston. C’est un des deux grands évènements de l’histoire de la trompette avec l’admission de la trompette dans la musique de concert vers 1600. Le piston a été inventé vers 1815 (mais des ébauches existaient dès 1788), il fut une réponse au vœu de faire devenir la trompette chromatique, dans le registre grave vers 1750. Le système de pistons avait tous les avantages des systèmes antérieurs de « chromatisation » sans aucun des inconvénients.Le piston est un élément de l'instrument très fragile et une simple chute peut le dérégler. Les avantages sur les autres systèmes sont : l’instrument est entièrement chromatique et toutes les notes présentent le même timbre (peut-être pas au début, mais des perfectionnements le permirent très vite). Alors que la trompette à clefs raccourcissait le tube en provoquant des pertes de charge — des « fuites » —, la trompette à pistons comme la trompette à coulisse l’allonge.Cependant on ne fait plus intervenir la physique en tirant sur une coulisse mais on agit mécaniquement sur un, deux, voire trois pistons, ce qui permet d'améliorer la dextérité. À la technique traditionnelle, vient maintenant s’ajouter un élément nouveau : l’habileté digitale. Alors que les trompettistes du baroque n’avaient que trois éléments à coordonner (lèvres, souffle et langue), ceux qui utilisèrent une trompette à pistons en avaient quatre : souffle, lèvres, langue et doigts.La trompette à pistons s’imposa rapidement dans la musique militaire mais se heurta à des oppositions (surtout par conservatisme) dans le milieu symphonique. L'inconvénient majeur du piston, hormis sa fragilité résolue par l'usage de monel, un alliage résistant, est qu'une trompette à perce parfaitement cylindrique et à pistons ne peut pas être juste, pour des raisons physiques. La modulation de la perce de la branche d'embouchure et l'adaptation de la colonne d'air et — un peu — des lèvres permettent de corriger la justesse. Chronologie sur les pistons La trompette à pistons (pistons de type Périnet) en si                    ♭              {\displaystyle \flat }   — plus communément appelée trompette en si bémol — est celle qui est la plus utilisée aujourd’hui dans la plupart des pays. Mais la trompette à valves rotatives (appelée aussi « trompette à palettes ») est largement présente en Allemagne et en Europe de l'Est.La trompette en ut est aussi beaucoup utilisée, en particulier en France, dans les orchestres symphoniques et pour certains concerti pour trompette. Elle existe aussi en version à pistons ou à valves rotatives.À cause d’une attaque trop aléatoire avec une trompette normale en si                    ♭              {\displaystyle \flat }  , certains instrumentistes utilisent la trompette piccolo pour jouer surtout des œuvres baroques dans lesquelles le registre aigu est souvent très utilisé (anciennement appelé clarino). La trompette piccolo ne monte pas plus haut que la trompette normale en si                    ♭              {\displaystyle \flat }  , elle n'est pas plus facile à jouer dans le registre aigu, cependant les traits aigus sont plus stables. Elle existe en version à pistons ou à valves rotatives. La plupart du temps, elle est aussi en si                    ♭              {\displaystyle \flat }   (qui peut être mise en la avec une coulisse additionnelle), parfois en ré.La trompette de poche est surtout utilisée par les jeunes trompettistes débutants. Elle est choisie car sa petite taille est adaptée à celle des enfants et son poids est mieux réparti donc elle n’est pas déséquilibrée vers l’avant. Mais certains trompettistes professionnels l’utilisent.Contrairement aux idées reçues, la trompette de poche a la même longueur de tube que la trompette normale, car celui-ci est simplement plus enroulé.Il existe d’autres types de trompettes qui sont dans des accords différents, mais qui sont beaucoup moins utilisées que celles citées ci-dessus (trompette en sol, ré, mi                    ♭              {\displaystyle \flat }  , fa ou trompette basse).La note et le volume de la trompette peuvent aussi être modifiés à l'aide d'une sourdine. Il en existe de nombreux types.Les plus connues sont les sourdines sèche, bol, wah-wah, plunger ou harmon.La trompette naturelle est fabriquée dans un tuyau de 1,50 m de long et constituée par l'embouchure, le tube (ou perce) et le pavillon. La perce est cylindrique, ce qui lui donne un son brillant, par comparaison au son plus doux de la famille des saxhorns. Cet instrument est encore employé dans la musique baroque sur des instruments anciens, et dans la musique militaire.Dans la trompette à pistons, un mécanisme est ajouté qui permet d'accroître la longueur du tube, ce qui permet de jouer des notes plus graves et de combler ainsi les notes faisant défaut dans la série harmonique.Le doigté est celui des instruments à pistons.Le registre courant s'étend sur deux octaves et demie, du fa# grave au do au-dessus de la portée (contre-ut). Certaines pièces du répertoire classique dépassent cette tessiture (par exemple, le 2e concerto brandebourgeois de Jean-Sébastien Bach). Pour ces pièces les instrumentistes utilisent généralement la trompette piccolo. Il est à noter qu'en jazz, il n'est pas rare d'entendre des musiciens monter jusqu'au bi-contre-ut voire plus haut.Les trompettes — sauf la trompette en ut — sont en général des instruments transpositeurs qui jouent des sons réels différents des notes écrites. Ainsi par exemple une trompette en si                    ♭              {\displaystyle \flat }   joue un son réel qui est un ton plus bas que la note écrite.La trompette actuelle la plus courante est un instrument soprano, en si                    ♭              {\displaystyle \flat }  .Il existe aussi des trompettes en ut (encore très utilisées par les musiciens classiques, car le son est souvent un peu plus fin, et pour l'enseignement dans les conservatoires, à partir d'un certain niveau), en ré et en mi                    ♭              {\displaystyle \flat }   et la trompette piccolo en si                    ♭              {\displaystyle \flat }   (souvent à quatre pistons) pour un registre plus élevé, largement utilisée dans la musique baroque. Le 4e piston de certaines trompettes piccolos sert à atteindre les notes graves de la tessiture de la trompette en descendant généralement d'une quarte.Il existe une multitude de trompettes moins usitées : celles en sol et en fa qui sont assez proches de la trompette piccolo en si                    ♭              {\displaystyle \flat }  .La trompette basse est rarement utilisée en France. Son registre est sensiblement le même que celui du trombone à pistons ou de l'euphonium.Certains orchestres spécialisés utilisent encore des trompettes baroques pour jouer des pièces de cette époque (par exemple le Messie ou la Musique pour les feux d'artifice royaux, de Haendel). Ces très longues trompettes permettant un certain chromatisme dans le registre de jeu (la première fréquence de résonance de l'instrument étant très basse, les harmoniques correspondant au registre de jeu sont assez rapprochées pour donner un quasi-chromatisme). Cependant, leur usage est assez anecdotique, ne serait-ce qu'à cause de la difficulté de jeu de ces instruments, parfois facilitée quelque peu par la perce sur des répliques d'instruments anciens d'un trou harmonique bouché ou libéré par l'instrumentiste.Enfin, il existe quelques trompettes atypiques : trompettes à quatre pistons permettant de jouer des quarts de tons (cf. Don Ellis, Ibrahim Maalouf…), ou trompettes combinant pistons et coulisse (la Firebird de la marque Holton, etc.)… Il existe également des trompettes qui, bien qu'acoustiquement très proches d'une trompette standard, ne sont pas enroulées : la trompette héraldique appelée aussi « trompette thébaine », utilisées pour des effets de mise en scène par exemple dans la marche triomphale d'Aida, opéra de Giuseppe Verdi.Les trompettes disposent d'une ou plusieurs clés d'eau pour évacuer le condensat d'eau produit par le souffle humide du musicien au contact du métal. Certaines trompettes sont munies d'un ou plusieurs barillets.Un barillet fonctionne à l'instar d'une valve, et permet de dévier la colonne d'air d'une coulisse vers une autre, le plus souvent à l'aide d'un bouton circulaire à cran que l'on tourne sur une position ou l'autre. L'instrumentiste peut ainsi changer l'accord de sa trompette. Sur la photo ci-contre, le barillet permet de changer l'accord de l'instrument d'ut en si bémol.S'il permet théoriquement de disposer d'une trompette à la fois en Ut et en Si bémol dans le même instrument, ce système a néanmoins une limite de justesse. En effet, la longueur des coulisses des pistons est calculée comme une fraction de la longueur totale du tube de l'instrument. Or, une trompette en Si bémol est plus longue qu'une trompette Ut, les coulisses des pistons de ces instruments doivent être différentes pour garantir des notes justes sans effort excessif du musicien sur ses lèvres. Certaines marques fabriquent, ou ont fabriqué, des trompettes transpositrices livrées avec deux jeux de coulisses de pistons pour pallier ce problème.Gabriele Cassone, The Trumpet Book, Zecchini Editore, 2009  (ISBN 88-87203-80-6).Charles Koechlin, Les instruments à vent, Paris, PUF, coll. « Que sais-je ? » (no 267), 1948, 128 p. (OCLC 843516730).Marc Honegger, Dictionnaire de la musique : technique, formes, instruments, Éditions Bordas, coll. « Science de la Musique », 1976, 1109 p. [détail des éditions] (ISBN 2-04-005140-6).Paul Archibald et Myriam De Visscher, La trompette et les cuivres.Edward Tarr, La trompette : son histoire de l'Antiquité à nos jours, Payot, 1977.Ressources relatives à la musique : MusicBrainz (en) Musical Instruments Museums Online Tout sur la trompette : une approche scientifique et pratique de l'instrument et de l'émission du son.The International Trumpet Guild. Portail de la musique   Portail de la musique classique   Portail du reggae   Portail du jazz
musique;"La musique traditionnelle, parfois abrégé en « musique trad », désigne l'ensemble des musiques associées à une culture régionale ou à une zone géographique. Musiques orales et populaires, elles se transmettent à l'oreille, bien que certains groupes et musiciens actuels préfèrent les transcrire sur partition afin de les interpréter ou de les répertorier.Elle se différencie de la musique dite folklorique car elle ne vise pas à montrer le passé d'une musique (avec costumes, etc.), mais à faire vivre les musiques appartenant à un patrimoine de culture populaire dans l'actualité : chaque groupe ou musicien peut s'approprier la musique à sa manière, en cela influencé par son environnement culturel et social, et la faire vivre.Les trois concepts essentiels dans la définition de la musique traditionnelle sont donc l'ancrage socio-culturel géographique, la transmission et la re-création.Dans une vision folklorique, le Conseil International des Organisations de Festivals de Folklore et d'Arts Traditionnels (CIOFF) qui organise chaque année plus de 300 festivals de folklore, un programme est considéré comme de culture traditionnelle si son contenu correspond à la définition de la Convention sur la sauvegarde du patrimoine culturel immatérielde l'UNESCO, c'est-à-dire qu'il doit :être transmis de génération en génération ;être recréé en permanence par les communautés et les groupes en fonction de leur milieu et de leur interaction avec la nature et de leur histoire ;procurer aux communautés et aux groupes un sentiment d'identité et de prospérité ;contribuer ainsi à promouvoir le respect de la diversité culturelle et la créativité humaine.Le CIOFF considère par ailleurs un programme comme d'« expression authentique » si :le contenu est régional ;le costume est authentique ou fidèlement reconstruit ;la musique et la danse sont présentées sans aucun arrangement.Les musiques actuelles sont souvent opposées à la musique traditionnelle, sur le champ de l'historicité et d'une connotation socio-culturelle empreinte de modernisme alors que le Ministère de la Culture classe les musiques traditionnelles au sein des musiques actuelles. Cette confusion est largement due à l'assimilation entre folklore et tradition, dans l'esprit du grand public mais aussi dans le réinvestissement du mouvement trad, et au fait que la musique folklorique est elle plutôt sujette à une fixation picturale.Les musiques traditionnelles ont largement subi et bénéficié des innovations successives. Ainsi, le mouvement folk revivaliste des années 1970 a souvent apporté l'électrification et le rajeunissement des groupes de musiciens (comme Alan Stivell ou Malicorne). La scène bretonne a joué un rôle important dans la popularité du répertoire traditionnel (Tri Yann, Matmatah, Gwerz …). Plusieurs festivals, comme le festival de Ris-orangis, le festival de Cornouaille à Quimper ou les Rencontres musicales de Nedde, ou structures associatives et groupes participent de nos jours à la réactivation et au renouvellement de la musique traditionnelle en France.Il se trouve que dans l'autre sens, la musique traditionnelle est parfois source d'inspiration dans les musiques populaires dites actuelles (la présence de la vielle à roue chez Olivia Ruiz, le répertoire breton revisité par Nolwenn Leroy ou auparavant la chanson La belle sardane de Charles Trenet en sont des exemples). Preuve que musiques dites actuelles et musiques traditionnelles sont toutes deux d'assise populaire, que la distinction est avant tout commerciale et parfois porteuse de préjugés, et que les musiques traditionnelles, par définition mouvantes et perpétuellement réinventées, pour peu qu'on n'érige pas les métissages et la re-création en dogmes, sont tout autant actuelles.Les musiques traditionnelles actuelles, outre leur caractère régional, sont la continuité du mouvement folk qui a eu lieu sur tout le territoire français au cours des années 1960-80. Une vaste entreprise de collectage a débuté durant cette période et a grandement contribué au corpus des musiques (et de danses) dites traditionnelles en France. La professionnalisation et leur intégration aux conservatoires des enseignants en musique traditionnelle à partir de la fin des années 1980 a contribué à l'essor de ces musiques.On distingue plusieurs grands ensembles de musiques traditionnelles, proches par leur bases culturelles. Ces musiques sont plus ou moins convergentes avec la langue (langue d'oc, langue d'oïl, langue celtique) parlée :la musique traditionnelle la plus connue du grand public est la musique bretonne qui est l'expression musicale de Bretagne ;la musique auvergnate, du Morvan, du pays limousin avec des compositeurs comme Didier Mario qui fait honneur du bon vivre en Limousin avec La valse limousine, du Berry, de la Corrèze, du Bourbonnais et même du Poitou. Proches par leur répertoire et le type de danses (à l'exception peut-être de la musique auvergnate relativement particulière et ayant essaimé partout en pays de langue d'oïl) ;on la retrouve aussi en Vendée, Haute Bretagne (Bretagne Gallicante, ou pays Gallo), Flandre Gallicante et Wallonie, Picardie, musique flamande ;la musique de langue d'oc, musique occitane, musique aquitaine, musique landaise, musique provençale, etc.la musique catalane ;la musique basque ;la musique alsacienne ;la musique corse ;la musique flamande ;la musique savoyarde.Cette musique vivante a largement été influencée par des migrations populaires aux XIXe et XXe siècles ; on retrouve des répertoires communs dans diverses régions. Restent quelques bassins culturels avec une musique caractéristique : Flandre française, ou dans les Alpes.Chacune de ces cultures inclut dans les instruments utilisés une cornemuse, et/ou un accordéon (arrivé d'Italie à fin du XIXe siècle). On peut y associer suivant les régions une flûte, violon, vielle à roue, bombarde, percussion et autres instruments spécifiques à une aire géographique.On peut entendre aujourd'hui des musiques en France dans les festivals, notamment au Festival interceltique de Lorient, aux Rencontres internationales de luthiers et maîtres sonneurs de Saint-Chartier, Le Son continu à Ars, aux Nuits Basaltiques du Puy-en-Velay, au Festival international de folklore de Romans-sur-Isère et dans les bals folk, bals trad ou fest-noz, le plus souvent dans des salles des fêtes, aussi en extérieur.Les musiques celtiques du nord de l'Europe (Irlande, Écosse, Pays de Galles, Île de Man)La musique folk britanniqueLes musiques scandinaves (Norvège, Danemark, Suède, Îles Féroé)Les musiques de la péninsule ibérique (Espagne, Catalogne, Asturies, Galice, Portugal, la musique flamenco)Les musiques d'Europe centrale ( Autriche, Allemagne, Pologne, Tchéquie, Slovaquie, Hongrie…)Les musiques des pays baltes (Estonie, Lettonie, Lituanie)Les musiques des Balkans (Bulgarie, Roumanie, Macédoine, Grèce…)La musique klezmerLes musiques de l'Italie (Tarantelle et autres...)Les musiques tsiganesCes musiques traditionnelles ont donné un essor décisif d'une part au genre du poème symphonique, et d'autre part à l'ethnomusicologie.[réf. nécessaire] Ainsi, Constantin Brăiloiu, l'ami du hongrois Béla Bartók et du roumain Georges Enesco - eux-mêmes compositeurs d'une musique savante fortement inspirée par le folklore de leurs pays, au même titre que d'autres musiciens de la seconde moitié du XIXe siècle (Liszt, Dvořák, Grieg, Sibelius…) et de la première moitié du XXe siècle (Janáček, Szymanowski, Harsányi, De Falla, Ropartz…) - a parcouru, durant l'entre-deux guerres, les villages les plus reculés de l'Europe centrale et orientale pour recueillir, magnétophone en main, des dizaines de milliers de chansons, danses, thèmes mélodiques, rythmes, etc. Tandis que la musique savante se voyait ressourcée au contact direct de la musique folklorique, ces recherches, menées aussi par d'autres ailleurs en Europe, ont abouti à une connaissance renouvelée du phénomène musical.[réf. souhaitée]On rencontre des influences irlandaises et françaises dans la musique traditionnelle québécoise. La jigue irlandaise mélangée à la chanson à répondre française en est globalement le résultat. Cette culture propre au Québec s'est diffusée grâce à la tradition orale. Autrefois, les veillées amenaient les musiciens des différentes paroisses à se rencontrer et à échanger leurs versions de chanson à répondre.Aujourd'hui, la musique traditionnelle québécoise mélange cette tradition orale, des textes retrouvés dans les archives ou encore des mélodies qui se transmettent dans les familles. Plusieurs groupes professionnels tels que Le Vent du Nord, De Temps Antan, Les Charbonniers de l'enfer, Galant, tu perds ton temps, Les Tireux d'Roches, La Volée d'Castors, Bon Débarras, Genticorum et la populaire La Bottine souriante proposent différentes façons de faire de la musique traditionnelle contemporaine. Que ce soit par les différentes sonorités ou des sujets actuels adaptés à la façon traditionnelle, il en existe aujourd'hui pour tous les goûts.Les instruments utilisés sont le violon, l'accordéon diatonique, la podorythmie (tenir un rythme avec ses pieds), les cuillères, la guimbarde, la flûte traversière, la mandoline, la guitare, la vielle à roue et l'harmonica (aussi appelé ruine-babine).Des organisations et des festivals mettent de l'avant ce type de musique et le folklore québécois comme EspaceTrad, à Montréal, le festival La Virée Trad, à Carleton-sur-Mer,  le Festival de musique Ripon Trad, en Outaouais, ou le Festival de musique trad Val-d'Or. Plusieurs musiques traditionnelles sont pratiquées :assiko ;hongo.Chanson traditionnelleMusique folkRépartition géographique des genres et styles musicauxHistoire de la musiquePatrimoine immatériel, Tradition orale, Livres et disques., Centre de Musiques Traditionnelles Rhône-Alpes, 2007 (lire en ligne [PDF])« Catalogue collectif d'archives sonores et audiovisuelles sur les traditions orales », sur Portail du patrimoine oral, 2011 (consulté le 1er mars 2021).Ressource relative à la musique : (en) MusicBrainz Tradzone, le principal forum en France sur les danses et musiques traditionnellesTout sur les musiques et danses traditionnelles du mondeMusictrad, le serveur des musiques traditionnelles en FranceTradmag: le magazine des musiques traditionnellesPartitions de musiques traditionnelles de France et du mondeLe site de référence de la musique traditionnelle du sud de l'ItalieTrad en Poche, musiques traditionnelles à l'harmonica Portail des musiques du monde   Portail des arts"
musique;"La musique populaire désigne les genres de musique tirant leur origine et trouvant leur public dans les milieux populaires. Elle se développe dans un milieu urbain et industrialisé et est souvent associée à l'histoire de la révolution industrielle et technologique ayant amené la technique phonographique, ainsi qu'à l'histoire de la mondialisation.Le terme est souvent utilisé comme un comparatif par certains défenseurs de la musique savante, qui perçoivent la musique populaire comme un produit commercial et pointent ses faiblesses esthétiques, qu'ils jugent en comparaison de la musique classique européenne. Si la musique populaire est souvent associée à la musique commerciale ou de masse, elle s'en distingue néanmoins par des critères qualitatifs et par sa capacité à former des communautés de mélomanes en se nourrissant de formes musicales inscrites dans diverses traditions historiques.Il ne faut pas confondre la musique populaire avec la musique pop, qui est un genre spécifique de musique populaire.Le terme de musique populaire est l'objet de débats. Le sociologue Simon Frith estime que le terme de culture populaire « n'a de sens qu'en tant que comparatif » et que ses plus fréquents objets de comparaison sont la haute culture, la culture folklorique et la culture de masse. La musique populaire est en effet souvent comparée à la musique savante, la musique traditionnelle et la musique commerciale.Depuis les années 1980, le milieu universitaire retient la « définition anglo-saxonne » du terme, comme le rappelle le musicologue français Olivier Julien : « sont populaires non pas les musiques qui ne sont pas savantes, mais les musiques qui ne sont ni savantes, ni folkloriques ». Citant Philip Tagg, pionnier des études sur la musique populaire, le même auteur précise que « les musiques populaires […] partagent avec les musiques folkloriques l'absence de cadre institutionnel, mais ont en commun avec la musique savante d'être jouées et composées par des musiciens professionnels ». La musique populaire se distingue aussi de la musique savante (transmise par la partition) et de la musique traditionnelle (transmise par la tradition orale) par son rapport avec la technique phonographique, qui lui fait traverser l'histoire. Pour Simon Frith, elle se distingue aussi de la musique de masse, car si la musique populaire est « consommée d'une manière particulière clairement différenciée de celle des élites culturelles », l'adjectif « populaire » ne se confond pas avec celui de « masse » : « de nombreuses musiques populaires […] ont de plus faibles ventes […] que des enregistrements de musique classique à succès ».La musique populaire européenne a hérité de certains des usages de la musique modale, du système tonal et des instruments de la musique classique, mais la musique populaire de manière générale peut aussi se référer à d'autres genres musicaux et à d'autres traditions musicales. Par exemple la musique populaire japonaise se nourrit à la fois du jazz et de sa musique traditionnelle, et certains morceaux des Beatles empruntent aussi bien à la musique classique européenne qu'à la pop américaine ou à la musique traditionnelle indienne. La variété des genres et l'éclatement des frontières musicales qui caractérise la musique populaire la lient intimement à l'histoire de la mondialisation et de la révolution industrielle.Pour ses détracteurs, la musique populaire est assimilée à la culture de masse ou à la musique « commerciale ». C'est le cas de Theodor W. Adorno, qui a rendu célèbre le concept d'industrie culturelle et voyait dans les genres de musique populaire comme le jazz des simples modes ou produits commerciaux.Il a des racines très anciennes dans le chant traditionnel dit folklorique ou de folklore vivant, en France pour partie chanté en Breton, basque, provençal, corse, flamand, alsacien, etc. puis en français surtout à partir du XIXe siècle. Les thèmes des saisons, des amours, des âges de la vie, du mariage, des guerres et de la mort sont récurrents. Il accompagnait la vie de tous les jours, les travaux des champs et la garde des troupeaux par les enfants, les danses, les fêtes, etc.C'est une personne (homme ou femme), souvent anonyme, qui chante sur la voie publique, parfois associée au camelot. Ce chanteur vit de l'argent que ses auditeurs lui donnent. Des styles et modes particuliers existent selon les époques et les pays (ex : les prosopopées dites lamenti italiens composés et imprimés durant la Renaissance, de 1453 aux années 1630-1650 ; lamenti storici, parodiques, satiriques et musicaux). Parfois sans instruments, parfois muni d'un porte-voix, il cherche à attirer et captiver un maximum d'auditoire en un temps très court et s'appuie pour cela sur une musique mélodieuse, un air déjà connu et/ou un texte accrocheur, parfois politique et satirique, devant alors parfois se jouer de la police.Très populaires avant l'invention des médias modernes (radio, télévision, enregistrement sonore), ils ont largement contribué à la diffusion d'idées ou d'informations au même titre que les journaux. En effet, en dehors de quelques grands standards de la musique populaire, leur répertoire s'inspirait souvent de faits majeurs ou de faits divers remarquables, assurant une publicité à ces événements. Au XIXe siècle avec la révolution industrielle, l'apparition d'une classe ouvrière urbaine et son exode rural, elle contribue à porter et diffuser la chanson ouvrière et « sociale ».Habitués à se mettre en public dans des conditions difficiles, les chanteurs de rue avaient souvent une personnalité originale et extravertie. Au nombre de ceux-ci le célèbre Aubert (né vers 1769, attesté en vie en 1848), doyen des chanteurs des rues de Paris fut nommé par ses confrères « Syndic des chanteurs des rues » de Paris. En 1848, il parle au nom de la délégation de 800 chanteurs, musiciens et mendiants des rues de Paris venus rendre hommage à l'Élysée au chansonnier Béranger membre de la commission des secours.Le chanteur des rues a toujours fait partie des « Cris (et bruits) de la rue », mais le développement de la voiture et l'augmentation du volume sonore lié à la vie moderne, la difficulté d'occuper la voie publique, l'accusation de mendicité et surtout la banalisation des enregistrements sonores ont réduit la présence des chanteurs de rue. Il en reste malgré tout, y compris officiellement,.Le marchand de musique ou de chanson est une profession aujourd'hui disparue en Europe mais qui était encore active dans l'entre deux-guerres, avant la large diffusion de la radio puis de la télévision. C'est un métier connexe à la chanson populaire depuis plusieurs siècles (chanson autrefois spécifiquement éditée et diffusée sur feuilles volantes). Les marchands de musique étaient itinérants et vendaient des partitions de chant en entonnant eux-mêmes la musique. Ils parcouraient les villes et se déplaçaient de foire en foire. Ils proposaient leurs chansons sous forme de feuille volante, souvent grossièrement imprimée, à des personnes qui ne savaient globalement pas lire la musique, mais qui étaient intéressées par la mélodie ou par le texte de la chanson. Ces feuilles volantes étaient parfois également illustrées par des gravures, œuvres d'illustrateurs connus, intéressantes du point de vue artistique et iconographique. Certains marchands de musique ne déchiffraient pas les partitions, mais avaient une bonne mémoire des airs. Leurs feuilles volantes restent une mine d'information sur les idées, les coutumes et les centres d'intérêt des Européens au XIXe et au début du XXe siècle.Dès le début du XIXe siècle, les orphéons fédèrent les masses. Il s'agit d'abord de chorales d'enfants puis d'ouvriers. Quelques noms : Wilhem, pédagogue et fondateur du premier orphéon en 1833. Delaporte qui contribuera dans la seconde partie du XIXe siècle à donner une ampleur nationale au mouvement. À partir des années 1850, le terme « Orphéon » désigne les chorales, les fanfares et les orchestres d'harmonie qui ont connu un essor dû au développement de l'industrie des instruments de musique. Les héritiers actuels du mouvement sont le mouvement À Cœur Joie (chorales) l'UFF (fanfares) la CFBF (batterie-fanfare), la CMF (orchestres d'harmonie, orchestres à plectre).Au XIXe siècle, des centaines de goguettes rassemblent à Paris, dans sa banlieue et aux alentours des dizaines de milliers d'ouvriers ou journaliers, hommes ou femmes. Il en existe encore au siècle suivant. La goguette de la Muse rouge disparaît seulement en 1939.La musique populaire s'appuie sur quelques standards musicaux et commerciaux. Elle est aussi à l'origine d'un certain vocabulaire.Il s'agit essentiellement de chansons (des paroles soutenues par une musique instrumentale ou un petit chœur). Une chanson dure la plupart du temps entre 3 et 5 minutes (durée initiale de la face d'un disque 78 tours ou d'un vinyle 45 tours). Les textes utilisent le vocabulaire courant, voire familier. La musique est essentiellement tonale, écrite dans le mode majeur ou en mode mineur. Sa structure repose souvent sur une alternance entre un refrain et quelques couplets (en général, moins de cinq).L'ensemble, musique et paroles, est facile à mémoriser par écoute répétée. Elle s'efforce ainsi d'être facilement compréhensible et donc diffusable internationalement. À cet effet, on note une nette prédominance de l'anglais dans les paroles, au moins en ce qui concerne celle qui s'exporte massivement. La musique s'efforce de pouvoir être diffusée le plus largement possible : utilisation d'instruments courants (guitares, claviers, cuivres, cordes, percussions), arrangements musicaux standards, quasi-monopole de la langue anglaise pour les paroles de la version dite « internationale » sans pour autant éliminer toute forme de production nationale.Avant l'invention des médias audios modernes (radio, télévision, disques), la diffusion était assurée par des chanteurs de rue qui vendaient les partitions sur les marchés en entonnant eux-mêmes les chansons. La généralisation de la radio a favorisé l'émergence d'une diffusion sur les ondes par des chanteurs qui initialement interprétaient en direct puis se sont enregistrés. Actuellement, la diffusion est massive et se fait par ondes radio, par CD (on parle alors d'EPK), et par diffusion de clips vidéo au cours d'émissions de télévision, mais surtout sur YouTube, ou par des applications de musique numérique, comme Spotify, Deezer ou Apple Music.Un tube, ou « hit », est une chanson qui a particulièrement « bien marché », c'est-à-dire qu'elle a atteint des sommets de vente.Un disque d'or ou de platine récompense l'auteur d'une musique qui s'est bien vendue.Un hit-parade (en anglais : chart) est une compétition permanente de musique populaire organisée par des chaînes de radio ou de télévision. L'objectif est d'être no 1 (être « au top »), ce qui est théoriquement déterminé par le nombre de disques vendus ou par le vote des auditeurs. Plus longtemps une chanson est en tête du hit-parade, plus elle s'assure une large diffusion, favorisant les retombées commerciales.Il est notable que l'aspect commercial et promotionnel soit une caractéristique dominante de la musique populaire depuis la deuxième moitié du XXe siècle : première en termes de part de marché dans le monde de la musique, la musique populaire est l'objet d'enjeux commerciaux énormes pour les producteurs de musique, ce qui justifie l'emploi de méthodes commerciales poussées, identiques à celles utilisées pour les produits de consommation courante : méthodes dites des « grands lessiviers » : Procter & Gamble, Henkel, etc. C'est ainsi qu'une musique fait l'objet d'une « politique de lancement » pour toucher une « cible privilégiée », qu'on « fait la promotion » d'un nouveau chanteur en espérant que ses ventes « décollent », ou qu'on résilie le contrat d'un chanteur qui ne « se vend plus assez » ou dont le genre « arrive en fin de vie », quitte à le rappeler s'il « rebondit ». Les droits d'exploitation des musiques les plus populaires représentent une source importante de revenus que l'on ne cède pas facilement.La principale production de musique populaire est donc le résultat d'une politique visant à générer des profits. Ces enjeux commerciaux sont surtout le fait des grandes majors du disque (Universal, EMI, Sony, BMG). Les maisons de disques indépendantes (comme Tôt ou Tard, Naïve Records) à la diffusion plus limitée semblent être moins à la recherche de profits. Certains musiciens ne trouvant pas de maisons de disques « s'autoproduisent », mais ils bénéficient alors d'une distribution « classique » (vente de CD en magasins) et d'une visibilité réduite, bien que le développement d'Internet ait changé la donne au cours des dernières années ; on voit notamment des sites permettant de participer à la production d'artistes inconnus du grand public et des outils de diffusion comme MySpace ou autres,.Si l'enregistrement de musique en studio fait toujours appel à des professionnels, la musique populaire est la musique la plus jouée par des amateurs. De nombreux « groupes de garage » se créent dans le but de reprendre leurs musiques préférées à partir des enregistrements de leurs vedettes. Les plus talentueux et les plus constants pourront même arriver à jouer en public (soirées privées, clubs d'étudiants, bals…). Ce type de réinterprétation à partir des disques a remplacé le modèle de la musique traditionnelle fondé en grande partie sur la transmission par le jeu. De nombreux groupes de rock, de pop ou de jazz ont commencé par faire de la musique sous cette forme. Parmi les plus célèbres on peut citer les Beatles et les Rolling Stones.Le karaoké est également une forme de réinterprétation devenue courante : à partir d'un enregistrement de l'arrangement musical « réputé exact », un soliste au micro chante la mélodie. Très utilisé dans les soirées conviviales et exclusivement fondé sur des chansons à succès, le karaoké laisse une part d'interprétation au soliste. Actuellement, les musiciens amateurs peuvent profiter de la vulgarisation des outils d'enregistrement et de reproduction (stations de mixage, samplers, logiciels de mixage, graveurs de CD, etc.) pour autoproduire leur musique et n'hésitent plus à la diffuser, par Internet notamment.(en) Frans A. J. Birrer, « Definitions and research orientation: do we need a definition of popular music? » (Définitions et axe de recherche : avons-nous besoin d'une définition de la musique populaire ?), 1985, in D. Horn, ed., Popular Music Perspectives, 2 (Gothenburge, Exeter, Ottawa et Reggio Emilia), p. 99-106.Hugh Dauncey & Philippe Le Guern, Stéréo, Sociologie comparée des musiques populaires France-Angleterre, Bordeaux, IRMA / Éditions Mélanie Seteun, 2008.Marcello Sorce Keller, Contextes socioéconomiques et pratiques musicales dans les cultures traditionnelles, in Jean-Jacques Nattiez (general ed.), Musiques, Une encyclopédie du XXIe siècle, Volume 3 : Éd. Actes Sud / Cité de la musique, p. 559–592.(en) Marcello Sorce Keller, The Problem of Classification in Folksong Research: A Short History, Folklore, XCV(1984), no 1, 100- 104.Vassal, Jacques. Folksong [soi-disant]: une histoire de la musique populaire [en majeure partie] aux États-Unis. Nouv. éd. Paris : Éditions Albin-Michel, 1972, cop. 1971. 354 p.Volume! La revue des musiques populaires. La seule revue universitaire française exclusivement consacrée à l'analyse pluridisciplinaire des musiques populaires.Philippe Darriulat, La Muse du peuple : chansons politiques et sociales en France 1815-1871, Presses universitaires de Rennes, 2010R. Thérien, I d'Amours (1992), Dictionnaire de la musique populaire au Québec, 1955-1992, Lavoisier.B. Bartók (1948), Pourquoi et comment recueille-t-on la musique populaire ?, Impr. A. KundigC. D. Pessin (2004), Chanson sociale et chanson réaliste - Cités, avec cairn.infoMarie-Dominique Amaouche-Antoine, « Le cahier de chansons du conscrit » ; Revue d'histoire moderne et contemporaine (1954-) T. 34e, No 4 (oct. - déc., 1987), p. 679–685, Éd. : Société d'Histoire Moderne et Contemporaine ([URL : https://www.jstor.org/stable/20529337 1re page])Top 50Billboard magazineCatégorie:Classement musical Portail de la musique"
musique;"La musique pop (ou simplement la pop) est un genre musical apparu dans les années 1960 au Royaume-Uni et aux États-Unis. Les chansons pop parlent en général de l'amour ou des relations amoureuses. Ce genre se concentre souvent sur une idée d'accessibilité, en mettant l'accent sur des mélodies accrocheuses, entraînantes, et en prenant souvent la forme de morceaux courts avec des rythmes associés à la danse. La musique pop fut beaucoup influencée par les technologies, comme l'enregistrement à pistes multiples (vers la fin des années 1960), et le synthétiseur (durant les années 1970 et 1980).L'expansion de la musique pop a été interprétée de diverses manières, notamment comme un processus d'américanisation et d'impérialisme culturel américain ou, plus globalement, de mondialisation. Par ailleurs, de nombreux débats chez les théoriciens de la pop posent la question du statut de cette musique : art majeur ou simple objet de consommation ?, La musique pop saura pourtant au fil des décennies profondément se diversifier et évoluer, autant dans des scènes grand public qu'indépendantes ou artistiques.Le terme « chanson pop » (pop song en anglais) est apparu pour la première fois en Angleterre en 1926 pour indiquer qu'une pièce de musique avait un certain aspect attirant. Le terme « musique pop » (« pop music ») est développé en Angleterre vers 1955 pour décrire le rock 'n' roll et les nouveaux styles musicaux des jeunes qui ont été influencés par celui-ci.Par la suite, le terme « pop » (parfois « pop rock ») désigne un sous-genre apparu dans les années 1950-1960. Le rock 'n' roll évolue alors pour se subdiviser en deux branches principales, le rock plus fidèle aux racines blues dont il est issu et la pop qui met plus l'accent sur les mélodies et les harmonies vocales. On peut de ce point de vue considérer que la pop connaît sa maturité avec l'avènement des Beatles. Les représentants les plus emblématiques de la branche rhythm and blues étaient les Rolling Stones (qui sur le tard reprirent cependant l'étiquette rock 'n roll).La pop, expression issue de l'anglais « popular music » (« musique populaire »), s'est donc petit à petit distinguée comme un sous-genre du rock, dans les années 1960. Si on considère que les Beatles ont créé ou au moins amené la musique pop, alors il s'agit d'une transformation adoucie et plus pétillante du rock 'n' roll. Le premier album sera Rubber Soul, toutefois précédé de quelques chansons de l'album Help!, où figure notamment Yesterday.En France, Serge Gainsbourg était considéré à ses débuts comme un chanteur de variétés et non de musique pop à l'époque de La Javanaise ou du Poinçonneur des Lilas. Ses chansons commencèrent à être cataloguées dans la pop à partir de créations comme Qui est « in » qui est « out » en 1966, année qui voit émerger les artistes de la pop française des années 1960.Après 1967, le terme « musique pop » est de plus en plus utilisé pour désigner un rock « soft », par opposition au rock traditionnellement plus énergique. La musique rock était perçue comme plus grave et avec des buts plus élevés, alors que la musique pop était perçue comme plus éphémère, axée sur les sentiments et plus accessible. Selon le sociologue de musique anglaise, Simon Frith, la musique pop est souvent produite à des fins commerciales, non à des fins artistiques ; elle est aussi créée pour être accessible à tout le monde. La musique pop est créée par les grandes compagnies, par les musiciens eux-mêmes. Ce point de vue est néanmoins discutable en regard des innombrables groupes pop qui n'ont jamais percé, et des nombreux autres, signés sur de petits labels indépendants (Mojave 3, Big Star, Yo La Tengo, etc.) restés confidentiels, loin des « majors » (grandes maisons de disques) et des artistes formatés et dynamisés par des campagnes commerciales.Le terme « musique pop » en français, ne correspond pas à l'anglais « pop music », ce dernier recouvrant tout ce qui est commercial et populaire. Les interprètes pop les plus connus sont Michael Jackson appelé « King of Pop » par les anglophones ou encore Madonna, appelée « Queen of Pop » et Britney Spears surnommée « Princess of Pop ». Pourtant, leur style musical est très différent de la musique pop des Beatles, The Beach Boys, Blur, Elton John, Oasis, Love, Coldplay, Radiohead, Red Hot Chili Peppers, Breeders ou Arctic Monkeys. Ces derniers rejoignent le genre pop rock, une version plus douce et mélancolique du rock.La musique pop s'inclut dans un mouvement de masse, dans lequel, et dès les années 1950, les industriels prennent en compte le pouvoir d'achat des adolescents, en leur fournissant, dans l'acception mercantile du concept d'identité, clubs où danser, tee-shirts et blousons de cuir pour se vêtir, motos pour se déplacer, plus tard tableaux à accrocher au mur ou sculptures à contempler, films à visionner, et musique pour se retrouver. À l'origine (i.e. dans les années 1950), la pop désigne en Amérique l'ensemble des musiques populaires (l'étiquette est alors en France de musique dite « de variété »). Mais la particularité essentielle de ce style musical reste en fait l'absorption de tous les autres genres : la pop music se nourrit de diverses racines, telles le rhythm and blues, le jazz, le folk, le blues, la soul, le funk, la musique légère (comme l’opérette, ou le vaudeville), la musique ancienne et classique, la techno, ou les musiques extra européennes (et en particulier la musique latine).La pop a également intégré une instrumentation extraordinairement variée : à partir de la configuration basique du rock 'n' roll (guitare, contrebasse ou basse, batterie, voire, piano et saxophone), elle y adjoint en premier lieu à peu près tous les pupitres de l’orchestre symphonique (vents, cuivres, percussions et cordes). Puis, après avoir enrichi les sonorités de la guitare de diverses pédales d’effet (sustain, wah wah…), et les progrès technologiques aidant, elle se pare de claviers aux sonorités très diverses (mellotron, synthétiseur, piano électrique, vocoder, sequencer, etc.). Enfin, la pop, à l’instar de la musique contemporaine, a approché les studios d’enregistrement comme des instruments de musique supplémentaires à part entière, agrémentant sa palette sonore par l’usage de filtres, effets de distorsion ou de saturation, écho, réverbération, etc.La musique pop a par la suite apporté une rythmique dansante et légère qui se base beaucoup sur les instruments électroniques (les synthétiseurs et autres boites à rythmes) et numériques (des drum machines Roland) et les effets électroniques surtout marqués à partir de la fin des années 1970 et au début dans les années 1980 avec l'influence anglo-américaine de groupes et artistes issus du courant new wave.Mais l’originalité majeure de la pop est d'être la première expression musicale définie en amont par un média : la nature de la pop est en effet d’être accessible au public le plus large possible, donc diffusée en radio, ce qui affecte à ses débuts son tempo (souvent proche du battement du cœur humain), sa durée (en moyenne trois minutes, avec quelques notables exceptions, tel le Hey Jude des Beatles, de plus de sept minutes), son débit et sa richesse harmonique (lent, et réduite), sa texture (soyeuse et policée), l’immédiateté de ses refrains (une chanson pop est généralement construite sur le principe couplet-refrain-couplet-refrain-montée à la tierce (modulation médiantique)-refrain ad lib), et le caractère apparemment inoffensif de ses textes,,.Au milieu des années 1990, les groupes anglais Oasis, Pulp et Blur ont réussi à ré-insérer la guitare électrique dans la musique pop mainstream, formant le style britpop. À la même période, une nouvelle tendance se développe dans le milieu de la pop : les boys bands. Véritable phénomène de mode, ses représentants les plus célèbres sont Take That au Royaume-Uni et les Backstreet Boys aux États-Unis. Leur succès colossal permet à des groupes féminins d'émerger comme les Spice Girls et All Saints. Un des forts courants pop des années 1990 a été les chanteuses « à voix ». Des artistes comme Céline Dion, Mariah Carey, Whitney Houston, Toni Braxton sont aux sommets des palmarès et vendent des dizaines de millions d'albums. Ensuite, quelques-unes d'entre elles ont allié la pop plus dance à leur voix. Christina Aguilera, Kelly Clarkson, Alicia Keys et Leona Lewis sont maintenant les représentantes de cette musique pop.À l'aube du XXIe siècle, ces groupes, chanteurs et chanteuses à voix sont peu à peu supplantés par des artistes solo comme Michael Jackson, Beyoncé, Lady Gaga, Madonna, Britney Spears, puis dans les années 2010, Bruno Mars, Ariana Grande, Rihanna, Katy Perry, Shakira, Taylor Swift, Kesha, Adele, P!nk, Miley Cyrus, Selena Gomez, Dua Lipa, Lorde, Charli XCX, Ava Max, Bebe Rexha.Philippe Daufouy et Jean-Pierre Sarton, Pop music/rock, éditions Champ Libre, 1972.Agès Guéraud, Dialectique de la pop, Paris, La Découverte, 2018, 528 p. (ISBN 978-2-7071-9958-4)Volume ! La revue des musiques populaires, revue de recherche consacrée à l'analyse des musiques populaires.Ressources relatives à la musique : Last.fm SoundCloud  Portail de la musique   Portail de la culture   Portail des arts"
musique;"La musique électronique est un type de musique conçu dans les années 1950 avec des générateurs de signaux et de sons synthétiques. Avant de pouvoir être utilisée en temps réel, elle a été enregistrée sur bande magnétique, ce qui permettait aux compositeurs de manier aisément les sons, par exemple dans l'utilisation de boucles répétitives superposées. Ses précurseurs ont pu bénéficier de studios spécialement équipés ou faisaient partie d'institutions musicales préexistantes. La musique pour bande de Pierre Schaeffer, également appelée musique concrète, se distingue de ce type de musique dans la mesure où sa conception n'est pas basée sur l'utilisation d'un matériau précis mais l'inversion de la démarche de composition qui part des sons et non de la structure. La particularité de la musique électronique de l'époque est de n'utiliser que des sons générés par des appareils électroniques.Le terme de « musique électronique », trop large pour désigner un genre de musique spécifique, renvoie dans le contexte francophone à la branche populaire de la musique faisant usage d'instruments électroniques, par opposition à la musique électroacoustique, généralement créée par des compositeurs ayant reçu une formation académique en conservatoires, universités ou centre de recherches. De plus, depuis quelques années, le terme « électro » est souvent utilisé de façon abusive pour désigner la musique électronique populaire ; l'electro étant un genre de musique électronique, et non la « musique électronique » dans son ensemble. Journalistes spécialisés et amateurs de musique électronique sont généralement prudents sur l'utilisation du terme « électro », souhaitant ainsi éviter l'amalgame avec les autres genres de musique électronique. Possédant une diversité sonore sans égal, la multiplication des genres et sous-genres dans la musique électronique était inévitable.Le désir des compositeurs de construire des instruments électriques, puis électroniques, date de la fin du XIXe siècle. Les premiers instruments ont été le fruit de recherches souvent longues. Ces recherches visaient au départ à élargir l’instrumentarium orchestral et à permettre de nouvelles recherches de timbre. Citons pour mémoire : l'electromusical piano et la harpe électrique d’Elisha Gray et Alexander Graham Bell (1876), le singing arc de William Duddell (1899), le telharmonium (ou dynamophone) de Thaddeus Cahill (1900), l’ætherophone ou thérémine de Lev Theremin (1920) et l’électrophon ou sphärophon de Jörg Mager (de) (1921). Ses instruments tiraient tous parti des tubes électroniques et dont la diversité des sonorités était, malheureusement pour leur développement commercial, proportionnelle à leur encombrement.La compositrice germano-américaine Johanna Magdalena Beyer est une pionnière de la musique électronique et synthétique, avec Music of the Spheres, une des premières compositions pour instruments acoustiques et électroniques en 1938.En 1939, Hammond crée le Novachord, considéré comme le premier synthétiseur polyphonique au monde.Les premières recherches musicales expérimentales se sont servies du matériel des divers laboratoires de musique et des techniques d’enregistrement radiophoniques qu'ils ont détournés de leur fonction première. C'est à cette époque que se sont constituées dans les studios d'enregistrement et dans les institutions musicales (notamment les radios) des entités spécialisées dirigées par des musiciens et consacrées à la musique électronique. En 1951, Herbert Eimert prend ainsi en charge le studio de musique électronique de la WDR (Westdeutscher Rundfunk) à Cologne ; Pierre Schaeffer transporte son Club d’essai (devenu GRMC, Groupe de Recherche de Musique Concrète) et s’installera en 1958 à la R.T.F. (Radiodiffusion-télévision française) à Paris ; Luciano Berio et Bruno Maderna fondent ce qui, en 1955, deviendra le studio de phonologie de la RAI (Radiotelevisione Italiana) à Milan. Dans les radios européennes, à Stockholm, à Helsinki, à Copenhague et à la B.B.C (British Broadcasting Corporation) à Londres, se mettent aussi sur pied des studios consacrés à la musique électronique.Le BBC Radiophonic Workshop est l’atelier de création sonore de la B.B.C (British Broadcasting Corporation), où travaillent les compositrices Delia Derbyshire (également directrice adjointe de la B.B.C) et Daphne Oram. Daphne Oram quittera l’atelier pour créer son propre studio en 1959, Oramics Studios for Electronic Composition, où elle développe une machine, Oramics, permettant de générer des sons électroniques, retravaillés par la suite sur une bande magnétique.Aux États-Unis, Vladimir Ussachevsky et Otto Luening débutent également en 1951 les travaux de leur centre rattaché en 1955 à l’Université de Colombia, puis inauguré en 1959 sous le nom de Columbia Princeton Electronic Music Center (C.P.E.M.C.). Les subsides de l’université leur permettront d’acquérir des synthétiseurs RCA. En 1956, après avoir ouvert un studio à New York d'enregistrement sur bande magnétique très couru par les musiciens d'avant-garde, Louis et Bebe Barron produisent la première bande originale de film entièrement composée électroniquement : il s'agit de Planète interdite produit par la MGM. Des recherches sont également entreprises au studio de sonologie d’Utrecht à partir de 1961, et dans les années 1970 le studio de Stockholm (E.M.S.) réalise des recherches d’interfaces pour musicien (appelées « synthèse hybride »).Le matériau musical récupéré par ces chercheurs est de plus en plus diversifié et sa maniabilité permettra aux compositeurs de se libérer progressivement de son inertie propre. En conséquence, leurs exigences se sont faites de plus en plus drastiques. Dès les premiers balbutiements de cette expression musicale, les compositeurs se prennent au jeu d’une écriture en conformité avec cette nouvelle technique, qui marierait le plus agréablement possible les critères physiques et les critères esthétiques du matériau sonore devenu musical. Libérée de la production instrumentale, la représentation peut s’attacher à l’effet plus que la source, pour composer en fonction de la phénoménologie du son. C’est pourquoi les compositeurs recherchent la possibilité d’extraire de la technologie une nouvelle liberté d’écriture, une nouvelle liberté de choix dans les éléments constitutifs de l’expression et une prise en compte des problèmes de composition et de leur résolution formelle.La génération des années 1960 a tenté de se dégager des tendances de l’écriture musicale d’après-guerre et de recréer une nouvelle forme attachée à ces nouveaux instruments pour permettre l’émergence d’un nouveau type de musique. Puis la synthèse sonore sort des laboratoires et entre dans un nombre de plus en plus grand d’institutions publiques et privées consacrées à l’expérimentation musicale. Les compositeurs de la génération des années 1970 seront aidés par le temps réel et la miniaturisation des composantes des instruments électroniques.En 1952, Karlheinz Stockhausen compose Konkrete Étude au Club d’Essai de Pierre Schaefer. Mais c’est en 1953 et 1954 qu’il réalise les premières œuvres de musique de synthèse avec Elektronische Studie I et II. À la fin des années 1950, la musique électronique évolue vers un traitement conjoint de sons concrets (musique concrète) et de sons électroniques (musique électronique) pour donner ce qui se nomme dès lors la musique électroacoustique. C'est dans ce contexte que sera créée l'œuvre Gesang der Jünglinge, par Karlheinz Stockhausen, à Cologne le 30 mai 1956. Cette œuvre mêle des voix d’enfants démultipliées et des sons électroniques dispersés dans l’espace. Elle est conçue pour cinq groupes de haut-parleurs répartis géographiquement et permettant de construire une polyphonie spatialisée. Karlheinz Stockhausen peut-être considéré comme le premier grand compositeur de musique électronique. Il a exercé une grande influence sur les compositeurs des générations suivantes, et beaucoup de musiciens de pop music se réclament de lui.Pierre Henry est un compositeur français de musique électroacoustique né le 9 décembre 1927 à Paris. Il est connu du grand public pour le morceau Psyché Rock de la suite de danses Messe pour le temps présent. Ce morceau, plus accessible au grand public de par sa partie instrumentale rock, n'est toutefois pas forcément représentatif de son œuvre musicale, ou de son approche musicale en général.Des recherches amènent trois équipes indépendantes à développer le premier synthétiseur électronique facile à jouer. Le premier de ces synthétiseurs à apparaître est le Buchla. Apparu en 1963, il était le produit des efforts conduits par le compositeur de musique électronique Morton Subotnick. En 1962, grâce à une bourse obtenue à la Fondation Rockefeller, Subotnick et son associé Ramon Sender emploient l'ingénieur électrique Don Buchla afin de construire une « boîte noire » à composition.Subotnick décrit son idée de la façon suivante : « Notre idée était de construire une boîte noire qui serait la palette du compositeur à la maison. Cela serait leur studio. L'idée était de la concevoir de telle sorte que ce soit comme un ordinateur analogique. Ce n'était pas un instrument de musique mais cela permettrait des modulations… Ce serait une batterie de modules de générateur d'enveloppes à tension asservie et cela aurait des séquenceurs directement inclus… Ce serait une batterie de modules que tu pourrais assembler. Il n'y avait pas de machines qui lui étaient comparables jusqu'à ce que CBS l'achète… Notre but était que ça soit moins de 400 $ pour le tout et nous sommes arrivé assez près de cela. C'est pourquoi l'instrument d'origine pour lequel j'ai récolté des fonds valait moins de 500 $[réf. nécessaire]. »Un autre synthétiseur facile à jouer, le premier à utiliser un clavier comme celui du piano, fut le fruit du travail de Robert Moog. En 1964, celui-ci invite le compositeur Herbert Deutsch (en) à passer le voir à son studio de Trumansburg. Moog avait rencontré Deutsch l'année précédente, avait écouté sa musique et décidé de suivre la suggestion du compositeur de concevoir des modules de musique électronique. Lorsque Deutsch lui rend visite en 1964, Moog vient de créer les prototypes de deux oscillateurs à tension asservie. Deutsch joue avec les appareils pendant quelques jours et Moog trouve ses expérimentations tellement intéressantes musicalement qu'il construit un filtre à tension asservie. Plus tard, en septembre, alors que Moog est invité à la convention AES (Audio Engineering Society, société d'ingénierie sonore) où il présente une conférence sur « Les modules de la musique électronique », il vend ses premiers modules de synthétiseur au chorégraphe Alwin Nikolais. Avant la fin de cette convention, Moog était entré de plain-pied dans le marché du synthétiseur.Aussi en 1964, Paul Ketoff, un ingénieur du son pour la RCA Italiana de Rome contacte William O. Smith, directeur du studio de musique électronique de l'Académie américaine de la ville, en lui proposant de concevoir pour le studio de l'Académie un petit synthétiseur qui serait facile à jouer. Après consultation avec Otto Luening, John Eaton et d'autres compositeurs résidant à l'Académie à l'époque, Smith accepte la proposition et Ketoff a pu livrer son synthétiseur Synket (pour Synthesizer Ketoff) au début de 1965.L'une des contributions les plus importantes du développement de la musique électronique sera celle de Max Mathews qui réalise, en 1957 le premier son numérique. C'est le début d'une grande aventure qui scellera les noces de la musique et de l'informatique. Dans les laboratoires de l'université de Stanford, les travaux de Mathews seront vite suivis par ceux John Chowning (qui invente le procédé de synthèse par modulation de fréquence) et du compositeur français Jean-Claude Risset. Un autre pas décisif est ensuite franchi par le physicien italien Giuseppe di Giugno (en), qui réalise au début des années 1980 à l'Ircam la première machine numérique en temps réel : la 4X. L'idée en est qu'une machine peut être différente suivant les types de programmes qu'elle exécute. La musique électronique en temps réel va dès lors s'imposer et même irriguer le monde des musiques populaires et commerciales. À la fin des années 1980, Miller Puckette écrit le logiciel Max (en hommage à Max Mathews) devenu depuis Max-Msp qui va vite s'imposer comme le standard pour les compositeurs de musique électronique en temps réel. Il va collaborer avec le compositeur français Philippe Manoury sur un cycle d’œuvres (Jupiter, Pluton, Neptune, La Partition du Ciel et de l’Enfer, En écho, etc.) qui développent un suivi automatique de partitions ainsi que différents modèles d’interactions entre instruments acoustique et systèmes électroniques en temps réel.Même si la musique électronique a vu le jour dans le monde de la musique « savante », elle est entrée par la suite dans la culture populaire avec des degrés d'enthousiasme différents. Une des premières signatures électroniques est à la télévision britannique pour le thème de l'émission Doctor Who en 1963. Elle est créée par Ron Grainer et Delia Derbyshire, à la BBC Radiophonic Workshop (en) (les ateliers radiophoniques de la BBC).À la fin des années 1960, Wendy Carlos popularise la musique réalisée avec les premiers synthétiseurs avec deux albums : Switched-On Bach et The Well-Tempered Synthesizer, qui reproduisaient des pièces de musique baroque à l'aide d'un synthétiseur Moog. En 1969, George Harrison l'a introduit dans la musique rock en l'utilisant d'abord dans son album Electronic Sound réalisé en 1969, puis avec Abbey Road des Beatles. Puis dès 1972, le musicien japonais Isao Tomita produit son premier album Switched-On Rock - (Electric Samurai), inspiré par le travail de Walter Carlos. Mais c'est son album suivant, Snowflakes are Dancing, sur lequel il reproduit des pièces de Claude Debussy au synthétiseur Modulaire Moog, qui le fera connaitre. Puis le groupe allemand Tangerine Dream produit, depuis 1967, avec le Moog, une musique particulièrement inspirante, il publie une vaste discographie qui inclut des bandes sonores. Le Moog génère à l'époque une seule note à la fois, de telle sorte que, pour produire des pièces multi-couches comme ceux de Carlos, plusieurs heures de studio étaient requises. Les premières machines étaient connues pour être instables et se désaccordaient facilement. Certains musiciens tels que Keith Emerson d'Emerson, Lake & Palmer et Tangerine Dream les utilisaient néanmoins en tournée. Le thérémine, un instrument difficile à jouer, était utilisé dans certaines musiques populaires. Un thérémine est utilisé dans la musique du générique du feuilleton britannique Inspecteur Barnaby. Le tannerin ou electro-theremin, un instrument proche du thérémine inventé par le tromboniste Paul Tanner, est utilisé dans la chanson Good Vibrations des Beach Boys. Le Mellotron est utilisé dans la chanson Strawberry Fields Forever des Beatles, et une pédale à volume tonal a été utilisée comme instrument d'arrière-plan dans Yes It Is. Mais la pièce musicale exécutée avec le thérémine sans aucun doute la plus célèbre reste celle de Jimmy Page, le guitariste et fondateur de Led Zeppelin dans leur chanson phare, Whole Lotta Love.En 1970, le compositeur et jazzman Gil Mellé, pour le film Le Mystère Andromède de Robert Wise, fabrique une bande originale à partir d'appareils électroniques qu'il a lui-même modifiés et arrangés. Au fur et à mesure que la technologie se développait et que les synthétiseurs devenaient moins chers, plus robustes et plus portables, ils étaient adoptés par de plus en plus de groupes rock. Des exemples de ces utilisateurs précurseurs dans la musique rock sont des groupes tels que United States of America, Silver Apples et Pink Floyd. Si toute leur musique n'était pas électronique (à l'exception de Silver Apples), une grande partie des sons dépendait du synthétiseur, remplaçant fréquemment le rôle joué par l'orgue. Dans les années 1970, le style électronique est révolutionné par le groupe de Dusseldorf Kraftwerk, qui utilise l'électronique et la robotique pour symboliser et célébrer l'aliénation du monde moderne à la technologie. En Allemagne, des sons électroniques ont été incorporés à la musique populaire par des groupes comme Cluster, Neu!, Tangerine Dream, Can, Popol Vuh, Deutsch-Amerikanische Freundschaft (D.A.F.) et d'autres. Le courant dit de « musique planante », popularisé puis délaissé par Tangerine Dream, est repris par Klaus Schulze, ainsi que Vangelis et Jean Michel Jarre (fils du compositeur Maurice Jarre) qui popularise la musique électronique en France par ce biais avec ses albums Oxygène (1976) et Équinoxe (1978).Plusieurs pianistes de jazz importants, notamment Herbie Hancock, Chick Corea, Joe Zawinul de (Weather Report) et Jan Hammer (avec Mahavishnu Orchestra), commencent à utiliser les synthétiseurs dans leurs enregistrements de jazz fusion dans les années 1972-1974. Les pièces I Sing the Body Electric de Weather Report et Crossings d'Herbie Hancock utilisent des synthétiseurs, même si c'est davantage à des fins d'effets sonores qu'en tant qu'instruments mélodiques. Mais à partir de 1973, les synthétiseurs utilisés en tant qu'instrument solo commencent à faire partie intégrante du son jazz fusion, tel qu'entendu dans l'album Sweetnighter de Weather Report et l'album bien connu Head Hunters de Herbie Hancock. Chick Corea et Jan Hammer ont rapidement suivi le pas en développant chacun une façon unique de jouer du synthétiseur, utilisant les effets slide, vibrato, ring modulators, distorsion et wah-wah. Plus tard dans les années 1980, Herbie Hancock sortira l'album Future Shock, en collaboration avec le producteur Bill Laswell, album qui connaîtra un grand succès avec la pièce Rockit en 1983.À cette époque, il y a eu beaucoup d'innovation dans le développement des instruments de musique électronique. Les synthétiseurs analogiques ont fait place aux synthétiseurs numériques et aux sampleurs. Les premiers échantillonneurs, comme les premiers synthétiseurs, étaient du matériel cher et encombrant. Des sociétés privées telles que Fairlight et New England Digital vendaient des instruments pour plus de 75 000€ (100 000 dollars). Dans le milieu des années 1980 cependant, l'introduction de sampleurs numériques à prix modique rend la technologie accessible à plus de musiciens.À partir de la fin des années 1970, beaucoup de musiques populaires sont développés sur ces machines numériques. Des groupes et des musiciens tels que Laurie Anderson, Kraftwerk, Ultravox, Gary Numan, The Human League, Landscape, Visage, Daniel Miller, Heaven 17, Eurythmics, John Foxx, Thomas Dolby, Orchestral Manoeuvres in the Dark, Yazoo, Erasure, Klaus Nomi, Alphaville, Art of Noise, Yello, Pet Shop Boys, Depeche Mode et New Order ont développé de nouvelles manières de faire de la musique par des moyens électroniques. Fad Gadget (Frank Tovey) est cité comme le père de la new wave, bien qu'Ultravox, The Normal (Daniel Miller), The Human League et Cabaret Voltaire ont tous produit des singles de ce genre avant Fad Gadget.Les nouveaux bruits électroniques que permettaient les synthétiseurs contribuent à la formation du genre de la musique industrielle, dont les pionniers sont Throbbing Gristle en 1975, Wavestar, Esplendor Geométrico et Cabaret Voltaire. Des musiciens comme Nine Inch Nails en 1989, KMFDM et Severed Heads prennent pour modèle les innovations de la musique concrète et de l'art acousmatique, et les appliquent à la musique dance et rock. D'autres groupes, tels que Test Department et Einstürzende Neubauten, ont pris ces nouveaux sons pour en créer des compositions électroniques bruitistes. D'autres groupes encore, tels que Robert Rich, Zoviet France et Rapoon créent des environnements sonores en utilisant les bruits synthétisés. Enfin, d'autres encore, tels que Front 242 et Skinny Puppy combinent cette aridité sonore à la musique pop et dance, créant ainsi l'electronic body music (EBM). Pendant ce temps, des musiciens de dub, tels que le groupe de funk industriel Tackhead, le chanteur Mark Stewart et d'autres musiciens du label On-U d'Adrian Sherwood intègrent l'esthétique industrielle et de la musique bruitiste à la musique sur bande et les samples. Cela ouvre la voie pour une large part de l'intérêt qui a été porté à la musique dub dans les années 1990, dans un premier temps avec des groupes tels que Meat Beat Manifesto et plus tard les producteurs de downtempo et de trip-hop Kruder & Dorfmeister.Le développement de la musique house à Chicago, des sons techno et electro à Détroit dans les années 1980 et, plus tard, le mouvement acid house de Chicago et de la scène anglaise de la fin des années 1980 et du début des années 1990 ont tous contribué au développement et à la diffusion de la musique électronique. Parmi les artistes House qui ont influencé le genre, il convient de citer Frankie Knuckles, Marshall Jefferson, Jesse Saunders, Larry Heard, Kerri Chandler ou encore les Masters At Work.Pour l'electro et la techno, Aphex Twin, Juan Atkins, Derrick May, Kevin Saunderson, Carl Craig, Richie Hawtin, les Daft Punk ou encore le collectif Underground Resistance à l'origine formé de Mad Mike, Jeff Mills et Robert Hood. Sorti en 2000, l'album Kid A de Radiohead marque les esprits par sa nature très électronique pour un groupe ayant jusque-là bâti son succès sur une musique rock[réf. nécessaire].Au début des années 1990, la techno hardcore, un genre musical inspiré de la techno, du breakbeat, de l'EBM et de la new beat, émerge aux Pays-Bas et en Allemagne. Le genre recense plusieurs autres genres et sous-genres comme le gabber, la makina, le happy hardcore et le speedcore, notamment.Dans les années 2000, un fond sonore souvent électronique tourne en boucle pendant le morceau puis viennent s'ajouter toutes sortes d'instruments et de samples électroniques, avec les progrès techniques et les prix abordables des échantillonneurs ainsi que l'apparition du numérique et la démocratisation des home-studios.La musique électronique, en particulier au cours des années 1990, donne naissance à tellement de genres et de styles et de sous-styles qu'ils sont trop nombreux pour être cités ici. Même si on ne peut parler de frontières rigides ou clairement définies, on peut sommairement identifier de manière non-extensive :des genres ou styles contemporains : électroacoustique ou acousmatique, musique pour bande, concrète et improvisée ;des genres ou styles expérimentaux : krautrock, musique planante, nu jazz, rock progressif, new wave, cold wave, post-rock, industriel, electro, electronica, intelligent dance music et turntablism, witch house ;des genres ou styles consacrés à la danse : nu-disco, house, deep house, Chicago house, acid house, techno, techno de Détroit, techno minimale, house progressive, acid techno, EBM, breakbeat, drum and bass, jungle, makina, hardstyle, hardcore, Frenchcore, Acidcore, hardtechno, tribe, terrorcore, trance, trance psychédélique, garage house, ghetto house, guetto techno, freestyle, new beat, dance, dream house, fidget house, dutch house, jumpstyle, future bass ;des genres ou styles dit de chill-out : ambient, downtempo, dub, illbient, trip hop, chillstep.En 1954-1955 eut lieu une des premières étapes importantes vers un nouveau procédé de production musicale électronique : la commande directe en temps réel d’un équipement de synthèse sonore. Sont alors apparus les premiers synthétiseurs (les electronic music synthesizer) : le Mark 1 suivi en 1958-1959 par le Mark II. Ces appareils sont imaginés et construits par Harry F. Olson et Herbert Belar pour la RCA. Ils équiperont par la suite le studio de l'C.P.E.M.C. où Milton Babbitt, qui avait obtenu des crédits élevés par l'Université, les adoptera pour créer et développer sa technique d’écriture complexe et mathématisée (Composition for Synthesizer et Vision and Prayer pour soprano et sons de synthèse composés en 1961 et Songs of Philomel en 1964).Depuis, des projets entre chorégraphe et compositeur sont créés tels qu'en 1942 le ballet de Merce Cunningham sur la musique de John Cage. En 1960 à Stony Point, John Cage compose Cartridge Music, une des premières œuvres de musique électronique jouée en direct. Des têtes de lecture de phonographes étaient utilisées comme transducteurs pendant la production de l’œuvre et non plus par l’intermédiaire de l’enregistrement sur bande.Museum of Modern Electronic MusicDavid Blot et Mathias Cousin, Le Chant de la Machine, 2 vol., Paris, Éditions Allia 2000-2002, rééd. en 2016, préfacé par Daft Punk.Nicolas Dambre, Mix, Paris, Éditions Alternatives, 2001.Ulf Poschardt, DJ Culture, traduit de l’allemand par Jean-Philippe Henquel et Emmanuel Smouts, Éditions Kargo, Paris, 2002 (1re éd. : Hambourg, 1995).Ariel Kyrou, Techno Rebelle : un siècle de musiques électroniques, Paris, Denoël, 2002.Laurent Garnier, David Brun-Lambert, Electrochoc, Paris, Flammarion, coll. Documents, 2003.Peter Shapiro, Rob Young, Simon Reynolds, Kodwo Eshun, Modulations : Une histoire de la musique électronique, traduit de l’anglais par Pauline Bruchet et Benjamin Fau, Éditions Allia, Paris, 2004 (1re éd. : Londres, 2000).David Toop, Ocean of Sound, ambient music, mondes imaginaires et voix de l'éther, traduit par Arnaud Réveillon, Paris, L'Éclat, coll. « Kargo », 2004.Jean-Marc Mandosio, D'or et de sable (cf. le chapitre VI : Je veux être une machine : genèse de la musique industrielle), Paris, éditions de l'Encyclopédie des Nuisances, 2008.Techno, anatomie des cultures électroniques, hors-série Art press, no 19, septembre 1998.Emmanuel Grynszpan, Bruyante Techno. Réflexion sur le son de la free party, Bordeaux, Éd. Mélanie Seteun, 1999.Sophie Gosselin et Julien Ottavi, L’électronique dans la musique, retour sur une histoire, in Volume !, no 1-2, Éd. Mélanie Seteun, Bordeaux, 2003.Volume !, Musiques électroniques : enjeux culturels et technologiques, no 3-1, automne 2006.Ouvrage Collectif, Modulations, traduit de l'anglais par Pauline Bruchet et Benjamin Fau, Paris, Éditions Allia, 2021, 352p. Portail de la musique électronique   Portail des arts"
sport;"L'activité physique regroupe à la fois l'exercice physique de la vie quotidienne (à la maison, lors du jardinage, des courses et autres ravitaillements, lors du travail, de la marche, de l'usage des escaliers, des déplacements et des modes de transport), l'activité physique de loisirs, et la pratique sportive. Selon l'OMS, le sport est un « sous-ensemble de l'activité physique, spécialisé et organisé ». Outre la régularité et la fréquence de l'exercice, trois paramètres semblent importants lors de l'exercice : la quantité d'énergie dépensée en mode aérobie, le pic d'intensité de l'effort et la durée de l'effort. 30 minutes d'exercices par jour durant 5 jours apportent autant de bénéfice que 3 séances de 10 minutes espacées de 4 heures dans chaque journée, 5 jours par semaine.Des preuves paléontologiques montrent que l'homme préhistorique en dépit d'une activité physique intense connaissait la maladie et vivait bien moins longtemps que nous.Depuis plusieurs millénaires, avec la sédentarisation et les progrès de la santé, l'espérance de vie a régulièrement progressé, mais l'espérance de vie en bonne santé semble donner des signes de recul (notamment attribués à une dégradation de l’environnement, de l'alimentation et à une sédentarité excessive).L'activité physique est pratiquée par les armées pour entraîner leurs soldats et par les sportifs des jeux olympiques depuis l'antiquité grecque au moins.Hippocrate (460 av. J.-C. - 377 av. J.-C.) affirmait déjà à cette époque que le manque d'activité physique était préjudiciable pour la santé.Au XVIIIe siècle, le médecin italien Bernardino Ramazzini note que les messagers, se déplaçant alors en courant, ne présentent pas les mêmes problèmes de santé que les travailleurs sédentaires tels que les tailleurs ou les cordonniers.Les premières études scientifiques quantitatives portant sur l'intérêt de l'exercice physique pour la santé ne datent cependant que du début des années 1950, avec le travail de Morris & al (1953) basé sur une population d'employés du transport londonien (travail conduit durant les pires périodes de smogs londoniens) et mettant en évidence un excès de décès par maladie coronarienne chez les travailleurs sédentaires comparativement aux travailleurs physiquement plus actifs.Elles ont ensuite notamment porté sur les effets du sport ou de l'activité sur les muscles, le système circulatoire, pulmonaire, cardiovasculaire et endocrinien ou encore sur le sommeil et l'humeur ou le risque de dépression à différents âges de la vie.Depuis les années 1990 la sédentarité physique est considérée par la médecine comportementale comme un facteur de risque cardiovasculaire.Les premières recommandations médicales (à visées thérapeutiques) ne datent aux États-Unis que de 1995 notamment produites par l'American College of Sports Medicine (ACSM).S.N Blair (médecin du sport) estime en 2009 que le manque d'activité physique pourrait devenir le premier problème de santé publique au XXIe siècle.En 2008, moins de 50 % des adultes américains avaient un niveau d'activité physique tel que recommandé.L'acronyme HEPA (pour health enhancing physical activity) désigne l'activité physique bénéfique pour la santé (APBS).L'Organisation mondiale de la santé recommande un minimum de 2 h 30 d'activité physique aérobie d'intensité modérée par semaine chez l'adulte en bonne santé.Cependant, une activité plus courte (15 min par jour) diminue déjà significativement le risque de survenue de maladies cardiovasculaires quels que soient l'âge et le sexe, par rapport à l'absence totale d'activité.La pratique d'activité physique est désormais importante, voire essentielle pour être en bonne santé physique et psychique. que ce soit pour les femmes ou pour les hommes. chez les femmes, les changements corporels et hormonaux demandent des adaptations particulières. En revanche ce n'est plus une affaire d'hommes, l'activité sportive féminines est de plus en plus présente dans les différents milieux sportifs.L’activité physique chez les enfants favorise une croissance et un développement sains.En étant actif et en bougeant, l’enfant maîtrise des d’habiletés pour faire travailler ses muscles.Il développe ainsi sa force, sa puissance et son endurance.Aussi, plusieurs avantages à court et à long terme seraient identifiés grâce à celle-ci.Parmi ceux-ci se retrouve le développement des os.Effectivement, l’activité physique lors de l’enfance et de l’adolescence permet la maximisation de l’accumulation de minéraux dans les os. Cette accumulation déterminera par la suite le pic de masse osseuse que celui-ci atteindra à l'âge adulte. Par la suite, l’activité physique va favoriser le maintien d’un poids santé.En effet, la pratique de celle-ci régulièrement permet de lutter contre l’obésité.Par ailleurs, cette maladie présente une grande hausse de cas chez les enfants et les adolescents.Pratiquer une activité physique maximiserait les chances de contrer l’obésité grâce à la dépense calorique[source insuffisante].De plus, un niveau de vie actif chez les adolescents permettrait de réduire les chances de développer certaines maladies chroniques telles que le diabète de type 2.Cela s’expliquerait grâce à une grande sensibilité à l’insuline grâce à la pratique d’activité physique régulière.Ensuite, la pratique d’activité physique chez les adolescents apporterait plusieurs bienfaits et avantages sur leur santé mentale et émotionnelle.Effectivement, la pratique de celle-ci sur une base régulière permettrait une certaine réduction de l’anxiété et de symptômes de la dépression.Aussi, elle permettrait d’augmenter l’estime de soi des jeunes et offrirais une meilleure capacité à lutter contre le stress au quotidien.Par la suite, pratiquer l’activité physique, notamment les sports d’équipes, lors de l’adolescence, pourrait avoir une grande influence sur leurs résultats scolaires.Ceci leur donnerait un plus grand sentiment d’appartenance, ce qui aurait tendance à amener plusieurs bienfaits académique[source insuffisante].Les précisions manquent.Les consensus sont en faveur de 60 minutes par jour, intensité modérée à élevée lors de sports, jeux, ou d'activités de la vie quotidienne.L'activité physique est considérée à ces âges comme bénéfique, mais doit être prudente et adaptée à la condition physique de la personne.Des recommandations ont été publiées en 2007 par l'American College of Sports Medicine Association et l'American Heart Association . Outre un suivi médical, il est souvent recommandé de diversifier les activités et de les préparer par un renforcement musculaire et des assouplissements ; des exercices spécifiques d'équilibre sont utiles.Elle dépend du type de l'activité, de son intensité et de sa durée.Elle peut être quantifiée en énergie dépensée (sous forme de calories), en MET (« metabolic equivalent »). un MET à 1 correspond à une consommation énergétique au repos. Un MET à 2 correspond par conséquent à une consommation énergétique doublée par rapport à celle au repos. Cette quantification peut être indexé sur le poids de la personne.Selon une étude récente du département Santé de l'Université d'York publiée par la revue PLoS ONE et financée par une Fondation contre les maladies cardiaques (Heart and Stroke Foundation), la plupart des Canadiens (quels que soient leur âge, leur origine ethnique et leur classification IMC) sous-estiment l'effort à faire pour que l'exercice physique ait un bénéfice pour la santé, et ils sous-estiment l'effort nécessaire pour atteindre ce que l'OMS considère être un exercice modéré, et plus encore pour un exercice intense ; ceci même après qu'ils ont reçu des descripteurs d'intensité d'exercice couramment utilisés (rythme cardiaque par exemple).Les lignes directrices données pour les recommandations d'activité physique dans le monde utilisent des termes généraux pour décrire l'intensité de l'exercice (déterminé par un pourcentage donné de la fréquence cardiaque maximale d'un individu). Il semble que ces termes ne soient pas compris par le public.La pratique d'une activité physique est l'un des facteurs souvent cités d'une bonne santé et de l'allongement de l'espérance de vie.Elle a notamment des effets positifs sur le système cardiovasculaire, le maintien des muscles, la consommation en excès des lipides et glucides dans l'alimentation des pays développés, la lutte contre le surpoids et l'obésité, etc. Le sigle APBS (ou HEPA en anglais) désigne l'activité physique bénéfique pour la santé,.La pratique d'une activité physique modérée, par son influence sur le système hormonal, permet également de réduire le risque de certaines maladies, tel le cancer du sein.L'exercice physique est un des facteurs d'allongement de la durée de la vie (ou son absence est un facteur de diminution de cette durée). Une enquête sur 20 individus a été menée de 1993-2007, dont deux sont décédés en cours d'enquête, afin de déterminer l'impact du mode de vie sur l'espérance de vie. L'étude conclut que le « mode de vie idéal » - absence de tabac, consommation d'alcool égale ou inférieure à un demi verre par jour, consommation de cinq fruits et légumes par jour, exercice physique d'une demi-heure par jour - majore l'espérance de vie de 14 ans par rapport au cumul de quatre facteurs de risque. Le cumul des quatre facteurs de risque (tabac, alcool, manque de fruits et légumes et d'exercice physique) multiplie le risque de décès par 4,4, trois facteurs, de 2,5, deux facteurs de près de 2 et 1 facteur de 1,4. C'est la première fois que l'on analyse l'effet cumulé des facteurs de risque sur la mortalité,.L'exercice physique est particulièrement important pour les enfants et adolescents afin de réduire les risques d’obésité et de surpoids (qui touchent notamment la France, avec en 2013 près de 20 % des enfants de 3 à 17 ans obèses ou en surpoids.Outre la prévention des maladies cardiovasculaires, l'exercice physique semble diminuer le nombre de certains types de cancers (essentiellement cancer du sein et cancer colorectal). Santé cardiovasculaire En 1968, une étude publiée par Saltin et ses collègues fait date : c'est la première à établir un lien direct entre l'activité physique et le fonctionnement cardiaque. Les auteurs de cette étude mettent en évidence que vingt jours d'inactivité complète, avec des sujets alités, diminuent à la fois la consommation maximale d’oxygène (VO2max) et le débit cardiaque maximal d'environ 26 %. Ils montrent, inversement, que huit semaines d'activité aérobie augmentent ces mêmes paramètres de 18 et 14 % respectivement.L'un des effets les plus importants de l'activité physique sur la physiologie cardiaque est une augmentation de la VO2max. La VO2max correspond au produit du débit cardiaque maximal et de la différence artérioveineuse en oxygène (en) ; le débit cardiaque maximal étant le paramètre clé de la VO2max. L'effet d'une activité physique le plus universellement observé est une augmentation du débit cardiaque maximal.La morphologie cardiaque est également profondément modifiée par la pratique d'une activité physique sur le long terme.Il a ainsi été mis en évidence que le cœur des athlètes pratiquant des sports d'endurance était significativement plus gros.Ces modifications concernent notamment le ventricule gauche dont les parois ainsi que le volume augmentent. En conséquence, le volume télédiastolique (volume expulsé dans la circulation systémique vers les organes) augmente alors que le volume télésystolique (volume résiduel après la contraction) diminue conduisant à une augmentation du volume d'éjection systolique, le cœur est plus efficace. Le débit cardiaque correspondant au produit de la fréquence cardiaque par le volume d'éjection systolique, l'augmentation de VO2max observée à la suite d'une activité physique est majoritairement due à l'augmentation du VES.Les propriétés électrophysiologiques cardiaques sont également modifiées par l'activité physique, parmi celles-ci une bradycardie au repos est la plus connue. Ce phénomène résulte notamment d'une augmentation du tonus parasympathique et d'une diminution de la réponse à la stimulation adrénergique, neurotransmetteur du système nerveux sympathique.L'activité physique augmente également la variabilité de la fréquence cardiaque, c'est-à-dire la durée séparant deux contractions cardiaques consécutives.Enfin, la durée du potentiel d'action cardiaque est diminuée.Une forte capacité du système cardiorespiratoire — marqueur objectif du niveau d'activité physique — est associée à une diminution de 60 à 70 % du risque de maladie cardiovasculaire.Un mode de vie associé à une activité physique aérobie régulière augmente l'espérance de vie de trois à sept ans.L'exercice physique a presque toujours été présenté comme utile au maintien et à l'amélioration de la santé mentale.De nombreuses études ont confirmé des améliorations de l'humeur et de l'estime de soi cependant, dans les années 1990, le lien de causalité n'était pas clairement établi.Des chercheurs ont montré que la pratique de 20 à 40 minutes d'activité aérobique assez intense par jour peut diminuer l'anxiété et améliorer l'humeur pour plusieurs heures, mais ces changements sont transitoires, surviennent surtout chez les individus ayant un niveau normal ou élevé d'anxiété, et sont limités aux formes aérobies d'exercice,,.Les études ayant porté sur des programmes d'exercices à long terme, n'ont pas constaté d'améliorations de la santé mentale ou n'ont constaté que des effets modestes chez les individus « normaux ». Par contre des améliorations sont démontrées pour les personnes plus anxieuses et/ou dépressives et une pratiques a priori opposée (relaxation) peut avoir des bénéfices au moins comparables pour diminuer l'anxiété. Selon les données cliniques disponibles les avantages psychologiques apportés par l'exercice sont comparables aux gains obtenus avec les formes standard de psychothérapie.Inversement l'exercice physique uniquement pratiqué pour lui-même et de manière très intense ou compulsive, peut aussi nuire à la santé mentale, par exemple quand il entraine une dépendance excessive à l'activité physique ou sportive, alors souvent associée à des troubles de l'humeur et de la socialisation, une alimentation déséquilibrée, une anorexie ou boulimie, des comportements de dopage, voire à une détérioration de la santé physique. Chez l'athlète, le culturiste ou le joggeur un entraînement trop intense (surentraînement) peut induire une perturbation importante de l'humeur voire le syndrome de staleness (détérioration des performances physiques et intellectuelles et à des troubles du comportement, du sommeil, pouvant conduire à la dépression clinique), peut être en lien avec un dysfonctionnement du système hypothalamique. Une pratique de plus de dix heures de sport par semaine pourrait conduire à un surmenage de l'activité cérébrale.L'exercice pratiqué sans excès et dans un contexte positif a des effets bénéfiques, ou sinon peut nuire à la santé mentale. Pour un individu en bonne santé, le bénéfice principal pourrait être la prévention, alors que pour les personnes souffrant de troubles émotionnels légers à modérés l'exercice physique a une certaine valeur thérapeutique. Quand la pratique est collective, il peut contribuer à une resocialisation ou maintenir du lien social.Sa programmation (mode, durée, fréquence, intensité, variété, heures de pratiques, cadre…) influe sur la qualité des changements attendus en matière de santé mentale, autant d'aspects qui n'ont pas ou peu été étudiés, de même que les mécanismes biologiques et psychologiques reliant l'exercice à l'humeur et à la santé mentale.Le manque d'activité physique serait responsable d'un décès sur dix à travers le monde, soit presque autant que le tabac et l'obésité,.Une étude de l'ANSES en 2020 révèle que « 95% de la population française adulte est exposée à un risque de détérioration de la santé par manque d’activité physique ou un temps trop long passé assis ». Toujours selon cette enquête, 5% des adultes en France ont une activité physique suffisante pour protéger leur santé : les femmes sont plus exposées que les hommes à un manque d’activité physique. Plus d’un tiers des adultes français cumule un haut niveau de sédentarité et une activité physique insuffisante : en conséquence, ils sont plus exposés au risque d’hypertension ou d’obésité et ont un taux de mortalité et de morbidité plus élevés causés par des maladies cardiovasculaires et certains cancers.Certains handicaps et contextes psychologiques et socioculturels sont un frein à l'exercice physique.Les nuisances sonores le sont également : une étude récente a conclu qu'une exposition chronique au bruit des transports (bruit d'avions et de véhicules au sol) peut défavoriser la pratique de l’exercice régulier et ainsi nuire à leur santé,. La baisse d’activité physique est en Suisse de 3,2 % pour chaque point sur l'échelle de gêne due au bruit. Même de faibles niveaux de nuisances sonores ont été associés à une baisse d’activité physique.Chez la femme, une activité physique excessive peut avoir des conséquences sur le système hormonal (aménorrhée, ostéoporose et troubles de l'alimentation).Les myokines sont des cytokines, des substances solubles de signalisation cellulaire synthétisées par les myocytes, les cellules constitutives des muscles. Elles sont produites lors de l'activité physique.Inserm, Activité physique : Prévention et traitement des maladies chroniques, Montrouge, EDP Sciences, coll. « Expertise collective », 2019, 824 p. (ISBN 978-2-7598-2328-4, lire en ligne)Arnaud, P. et Terret, T., Histoire du sport féminin, tome 1 et 2, Paris, L’Harmattan, 1996.PNAPS prévention par activité physiqueRéseau suisse Santé et activité physique HEPA (it) (fr) (de)Manger Bouger PNNS INPESfemmes et sport, l'histoire d'un long combat Portail du sport   Portail de la médecine   Portail de la physiologie   Portail de l’éducation                  Cet article est partiellement ou en totalité issu de l'article intitulé « Exercice physique » (voir la liste des auteurs)."
sport;L'attaquant (ou avant) est un joueur de football dont la tâche principale est de concrétiser le jeu offensif de son équipe. Ce rôle décisif en fait l'un des postes les plus médiatisés dans ce sport.L'attaquant est placé à proximité des buts adverses et a de fait plus de possibilités pour marquer des buts que ses coéquipiers plus défensifs. Pour autant le rôle des attaquants peut varier en fonction de leurs caractéristiques, de l'organisation tactique et du jeu de leur équipe. Il s'agit en effet d'une catégorie qui regroupe plusieurs postes et, en second lieu, plusieurs profils.L'avant-centre (parfois appelé attaquant de pointe ou numéro 9), se positionne plutôt dans l'axe et a pour objectif premier de marquer des buts. Pour cette raison, l'avant-centre est parfois appelé « buteur » par abus de langage, alors que tous les joueurs sont susceptibles de marquer. Cette fonction est historiquement représentée par les joueurs arborant le numéro 9. Depuis les années 2000, l'avant-centre remplaçant en club préfère parfois porter le numéro 18, car la somme de 1 et 8 fait 9, plutôt qu'un numéro de remplaçant entre 12 et 15.Il existe plusieurs profils d'avant-centres, souvent en relation avec leur physique, leurs capacités techniques et leur style de jeu ainsi que leur jeu de tête.Certains sont des joueurs qui partent de loin, qui misent sur leur vitesse et leur conduite de balle pour transpercer la défense et inscrire un but. Ce profil de joueur profite souvent des balles qui lui sont données en profondeur et se révèle particulièrement redoutable face aux défenses placées très haut sur le terrain. En général, ils ont plus de difficultés à s'exprimer face aux défenses basses et regroupées. On peut citer comme exemples Ronaldo, Ruud Van Nistelrooy, Andreï Shevchenko ou encore Sergio Agüero.D'autres avant-centres passent la majorité de leur temps à rôder dans la surface de réparation, pour offrir des solutions à leurs milieux ou récupérer les ballons qui traînent. Ils font partie de cette catégorie d'attaquants nommés « renards des surfaces ». Ce type de joueur participe peu au jeu collectif de l'équipe et ne touche en général que très peu de ballons, se contentant surtout de son rôle de finisseur. Pour cela, il doit faire preuve d'un grand opportunisme et d'une grande efficacité devant les buts, les occasions étant rares. Les « renards des surfaces » misent surtout sur leur vivacité, leur rapidité gestuelle et leur capacité à se débarrasser du marquage adverse. Ce sont souvent des joueurs costauds capables de rivaliser physiquement avec les défenseurs, aussi bien sur les balles hautes que sur les balles au sol. Très dangereux dans les petits espaces et les défenses basses, ils ont plus de difficultés dans les défenses hautes car ce ne sont pas toujours des joueurs rapides ou ayant une bonne conduite de balle. Certains renards des surfaces, notamment en fin de carrière, peuvent parfois marquer moins de buts que les avant-centres rapides et techniques à l'échelle d'une saison mais ils sont les spécialistes pour marquer des buts décisifs dans les matches qui comptent le plus. Parmi les renards des surfaces les plus célèbres, les noms de Falcão, Trezeguet ou encore Pippo Inzaghi reviennent généralement. La saison 2006-2007 d'Inzaghi avec le Milan AC incarne à merveille le principe de renard des surfaces : remplaçant au profit d'Alberto Gilardino la majeure partie de la saison, il inscrit deux buts en finale de la Ligue des Champions, ce qui permet au Milan AC de remporter le trophée.Il existe un autre type d’attaquant de pointe : le « pivot ». Ce rôle est souvent dévolu aux joueurs particulièrement physiques, ne bénéficiant pas d'atouts techniques importants, et de préférence de grande taille. Ils ont pour fonction de servir de point d'appui en attaque pour les milieux, soit grâce à leur jeu de tête, soit par leur capacité à garder le ballon entre les pieds. À la réception des centres et des passes, ils servent à écarter les ballons vers des joueurs mieux placés qu'eux et donc plus susceptibles de marquer. Jouant dos au but, ils ont souvent moins d'opportunités pour marquer, mais doivent faire preuve de certaines qualités physiques et techniques pour contrôler les ballons et servir leurs coéquipiers. Ils sont souvent très surveillés par les défenseurs notamment à cause de leur jeu de tête, et permettent ainsi d'offrir des espaces à leurs coéquipiers. Olivier Giroud est souvent considéré comme l'incarnation de ce profil de joueur, notamment avec l'Équipe de France à la Coupe du Monde 2018 où il permet aux ailiers Antoine Griezmann et Kylian Mbappé (4 buts chacun) de s'illustrer tandis que lui ne marque aucun but.Idéalement, l'avant-centre est vif, opportuniste et bon en un-contre-un. S'il parvient à être altruiste quand c'est nécessaire, il s'agit d'un joueur très complet, pouvant devenir une arme dangereuse dans de nombreuses situations de jeu. Une certaine proportion d'échecs lui est pardonnée à la condition qu'il concrétise un nombre correct d'occasions.Il existe cependant d'autres attaquants qui ne jouent pas spécifiquement en pointe. Ces derniers sont utilisés dans les formations comportant deux attaquants et plus. En effet, l'avant-centre étant généralement un joueur à tendance individualiste, associer deux joueurs évoluant à ce poste peut nuire au jeu collectif.Au numéro 9 peut être associé un attaquant « de soutien » ou second attaquant, également appelé « neuf et demi », ou encore selon l'expression italienne consacrée « Trequartista ». Son rôle est d'aspirer les défenseurs pour démarquer l'avant-centre. C'est souvent un joueur très mobile, n'hésitant pas à s'excentrer, ou à revenir en arrière pour chercher les ballons. Il est en général bon dribbleur et plus passeur que buteur mais, du fait de son placement, doit tout de même savoir concrétiser les occasions. Il s'agit parfois d'un milieu offensif placé plus haut. Ce type de joueur est devenu très courant dans le football moderne car il permet de constituer une véritable alternative entre l'attaque et le milieu de terrain. Aujourd'hui, on peut citer comme exemple Joao Felix.Certains attaquants peuvent avoir une position désaxée sur le terrain, c'est par exemple le cas des ailiers dans les attaques à trois ou quatre joueurs. Leur rôle est alors de contourner par les côtés la défense adverse, afin d'adresser des centres devant le but aux attaquants axiaux. Ces derniers se chargent ensuite de concrétiser les passes et de les transformer en buts. Les meilleurs joueurs de l'Histoire ont souvent été placés dans cette position.Un autre profil d'ailier est celui de l'« attaquant intérieur », à l'instar de Cristiano Ronaldo ou encore Lionel Messi. Ces joueurs évoluent sur les côtés, mais leur objectif est généralement de repiquer dans l'axe afin de frapper dans le but. En effet, le couloir étant l'endroit sur le terrain où se trouvent le moins de joueurs, il est plus simple pour un attaquant d'éliminer un adversaire et de repiquer dans l'axe pour frapper au but. On ne peut pas pour autant parler d'un poste à part entière pour les attaquants intérieurs d'aujourd'hui, mais plutôt d'un type de joueur particulier, bien que jusque dans les années 1950, il existait un poste d’attaquant intérieur. Généralement appelé inter en français, et inside forward en anglais, il évoluait entre l'ailier et l'avant-centre dans les systèmes W-M et 2-3-5 pyramide.Dans cette perspective, une des appellations les plus ambiguës est celle de « faux numéro 9 ». Très souvent confondue avec un poste réel, cette dénomination doit en fait être employée pour qualifier un attaquant (souvent un avant-centre, mais pas obligatoirement) qui descend au milieu de terrain pour y récupérer le ballon et créer. Il s'agit généralement d'un joueur très bon techniquement (passes, dribbles, contrôle de balle) dont le but n'est plus uniquement de marquer, mais de créer des occasions pour lui et ses coéquipiers. Pour cela, donc, il ne reste plus cantonné à son poste d'attaquant et sa zone, mais peut jouer entre les lignes, sur les côtés, participer au jeu au milieu (et ainsi créer le surnombre)... L'intérêt principal réside dans le fait que les défenseurs adverses sont en face d'un dilemme : soit ils laissent le joueur quitter leur zone, s'exposant ainsi à un dribble ou une frappe lointaine s'il revient, soit ils le suivent, quitte à laisser derrière lui de la place exploitable par les coéquipiers de l'attaquant. Les attaquants Dries Mertens, Karim Benzema, Wayne Rooney et Lionel Messi illustrent parfaitement la réussite de ce style de joueur dans le football moderne. Néanmoins, par extension, cette appellation est également employée dans le cadre d'un dispositif tactique particulier : celui d'une équipe évoluant sans attaquant de formation. Le terme de « faux 9 » sert alors à désigner le ou les milieux de terrain qui deviennent attaquants lors des phases offensives. Ce système de jeu a notamment été mis en place par le sélectionneur espagnol Vicente del Bosque lors de l'Euro 2012, faisant évoluer le milieu de terrain Cesc Fàbregas dans cette configuration particulière du faux 9ou encore Pep Guardiola avec Manchester City lors de la saison 2020-2021.Jean-Michel Larqué et Henri Cettour, Football: Ses règles, son langage, son organisation, 1988.  (ISBN 978-2851823588).(it) Fulvio Damele, Calcio da manuale, 1998.  (ISBN 978-8844006709).Claude Doucet, Football : perfectionnement tactique, 2005.  (ISBN 978-2851806765).Benoît Meyer, Football : le ballon rond dans tous les sens, 2012.  (ISBN 978-2745323965).Dispositifs tactiques en footballPostes du football moderne Portail du football
sport;Une balle de cricket est de forme sphérique, et est faite de liège dur recouvert de cuir. Les règles qui précisent ses caractéristiques et son utilisation sont définies par la quatrième loi du cricket.Les dimensions et la masse de la balle de cricket dépendent de la catégorie des joueurs. Pour un match impliquant des équipes sénior masculines, la balle doit peser, neuve, entre 155,9 et 163 grammes (entre 51⁄2 et 53⁄4 onces), et sa circonférence doit être comprise entre 22,4 et 22,9 centimètres (entre 813⁄16 et 9 pouces), soit un diamètre compris entre 7,13 et 7,29 centimètres. Pour une rencontre entre équipes féminines, elle doit peser de 140 à 151 grammes (de 415⁄16 à 55⁄16 d'onces) et avoir une circonférence de 21 à 22,5 centimètres (de 81⁄4 à 87⁄8 de pouces). Pour un match entre des juniors (joueurs de moins de 13 ans), elle doit peser de 133 à 144 grammes (de 411⁄16 à 51⁄16 d'onces) et avoir une circonférence comprise entre 20,5 et 22,0 centimètres (de 81⁄16 à 811⁄16 de pouces).La balle de cricket est faite de liège dur recouvert de cuir. Le cuir est séparé en deux hémisphères liés entre eux par une couture. La balle est traditionnellement de couleur rouge, avec une couture blanche. Des balles de couleur blanche sont utilisées pour les matchs de limited overs cricket, qui se déroulent, pour certains, en partie en soirée. C'est un héritage de la World Series Cricket, une compétition rebelle organisée entre 1977 et 1979, qui a introduit ces matchs joués en partie en soirée et une couleur de balle bien visible à la lumière des projecteurs. Une brève expérimentation avait été menée au XIXe siècle pour introduire des balles bleues pour les matchs de cricket féminin, le rouge ayant été supposé trop choquant pour les femmes.Une nouvelle balle peut être réclamée par chacun des capitaines au début de chaque manche. Pour un match d'une durée de plus d'un jour, le capitaine de l'équipe au fielding peut réclamer une nouvelle balle lorsqu'un minimum d'overs a été joué avec la balle précédente. Ce nombre d'overs minimum, fixé par l'instance dirigeante du pays où se déroule le match, ne doit pas être inférieur à 80, soit 480 lancers.Depuis le 2011, en format One-day International, deux balles neuves sont utilisées durant chaque manche, chacune attachée à une extrémité du pitch. Chaque balle est donc utilisée durant 25 overs.La 41e loi du cricket précise des actions licites ou illicites qu'un joueur pourrait effectuer sur la balle en cours de match.N'importe quel fielder a le droit de polir la balle à condition qu'aucune substance artificielle ne soit utilisée. Il peut aussi enlever la boue présente sur la balle, et la sécher avec une serviette. Tout autre action visant à modifier les propriétés de la balle, par exemple en la frottant sur le sol, est interdite.Batte de cricketLexique du cricketLanceur Portail du cricket
sport;Un but, au hockey sur glace, donne un point à l'équipe qui le marque. Le but est marqué lorsque la rondelle (ou palet) franchit complètement la ligne de but entre les poteaux.La finalité d'un match de hockey est de marquer plus de buts que l'équipe adverse. Le gardien de but et les défenseurs ont pour principal rôle d'empêcher l'équipe adverse de marquer alors que les attaquants essaient prioritairement de marquer un but à cette même équipe. Bien évidemment, les attaquants ont aussi un rôle défensif à jouer et les défenseurs, un rôle offensif.Pour qu'un but soit marqué, la rondelle doit franchir entièrement - et d'une seule pièce - la ligne située entre les poteaux et sous la barre transversale.Un but ne compte pas si le tir est fait avec la crosse (ou bâton) levée au-dessus de la barre transversale, ou si le but est marqué sur une action volontaire du pied ou de la main.Un but dévié accidentellement par un joueur est validé, en revanche s'il est dévié par un arbitre, il est refusé.Aucun but ne peut être accordé si la cage n'est plus dans son socle (ou en place).Si le gardien est empêché de défendre son filet par un attaquant adverse, le but est refusé, et une pénalité d'obstruction est appelée. Il peut aussi être refusé s'il est marqué par une équipe qui a trop de joueurs sur la glace ou s'il est marqué avec un bâton cassé.Si un joueur touche la rondelle avant qu'elle entre dans son propre filet, ce qui au football association (soccer) est appelé but contre son camp, le but est accordé au dernier joueur ayant touché la rondelle de l'équipe qui a marqué, ou à défaut au joueur le plus proche du but.Dans de nombreuses ligues ainsi que dans le règlement international (IIHF), un but n'est pas validé si un joueur a un patin ou son bâton dans le territoire de but avant que la rondelle ne rentre.La Ligue nationale de hockey a supprimé cette règle après le but en triple prolongation au cours de la finale de la Coupe Stanley de 1999. Brett Hull des Stars de Dallas marqua le but qui mit fin à la série face aux Sabres de Buffalo. À la vidéo, il est clair que le patin du joueur est entré dans l'enclave avant la rondelle. La LNH justifia le but en disant que Hull avait marqué sur son propre rebond, gardant la possession et le contrôle de la rondelle en tout temps, lui donnant le droit de pénétrer dans l'enclave.Le juge de but est un officiel qui est placé derrière chaque filet et dont la tâche spécifique est d'indiquer quand la rondelle a franchi la ligne de but. Dans les patinoires qui en sont équipées, le juge allume une lumière rouge lorsqu'il voit la rondelle franchir la ligne. Dans tous les cas, l'arbitre principal reste le décideur final et peut annuler la décision du juge de but.Le nombre de buts marqués est l'une des statistiques les plus regardées.Chaque année, le trophée Maurice-Richard est donné au joueur de la LNH ayant marqué le plus de buts. Ce trophée est nommé ainsi en l'honneur de Maurice Richard, le premier joueur de l'histoire à avoir inscrit 50 buts en une saison au temps où la saison régulière de la LNH ne comptait que 50 matchs (comparativement à 82 aujourd'hui).Le joueur ayant marqué le plus de buts en une saison de LNH est Wayne Gretzky. Gretzky est aussi le joueur ayant marqué 50 buts le plus rapidement, ce qu'il fit au cours de la saison 1981-1982 qu'il termina avec 92 buts, il marqua son 50e but au cours du 39e match de la saison des Oilers d'Edmonton.Le nombre total de buts marqué dans la saison est aussi attentivement suivi. Au cours des dernières saisons, ce nombre a baissé. Beaucoup y ont vu une baisse d'intérêt et ont mis en cause l’accroissement de la taille de l'équipement des gardiens et l'apparition de systèmes de jeu défensifs tels que la « neutral zone trap ». Les fans du hockey défensifs, quant à eux, trouvaient que le nombre de buts marqués lors des années 1980 était anormal et que ce n'était qu'un retour à la normale.Lors de la saison 2004-05 de la Ligue américaine de hockey, quatre règles majeures furent changées afin d'augmenter le nombre de buts dans les matchs et de rendre ce sport plus attirant aux yeux des amateurs occasionnels:Augmentation des zones offensives en réduisant la zone neutre de 2 pieds (30,96 cm) de chaque côté et en reculant la ligne de but de 2 pieds vers l'arrière.Restrictions de jeu avec la rondelle pour les gardiens.Permission pour les joueurs hors jeu d'annuler la pénalité en revenant dans la zone neutre.Permission de faire franchir à la rondelle une ligne bleue et la ligne centrale en une foisCes règles furent ensuite adoptées dans l'ECHL et la LNH lors de la saison 2005-2006.Les buts sont considérés différemment pour les statistiques mais comptent de la même manière au cours des matchs:But à égalité numérique : lorsque les deux équipes ont le même nombre de joueurs sur la glace.But en supériorité numérique : lorsqu'une équipe joue avec plus de joueurs à cause d'une ou plusieurs pénalités que l'équipe adverse a subie(s).But en désavantage numérique : lorsqu'une équipe ayant moins de joueurs sur la glace à cause d'une ou plusieurs pénalités réussit à inscrire un but.But dans un filet désert : lorsqu'une équipe qui a fait entrer un attaquant supplémentaire à la place de son gardien prend un but.But de pénalité : lorsqu'un but est inscrit lors d'un tir de pénalité. C'est une confrontation en un contre un entre le gardien et le joueur en raison d'une pénalité.But automatique : cas particulier du tir de pénalité. Si un joueur est victime d'une faute en situation d'échappée face aux buts rendus vides par la sortie du gardien, l'arbitre accorde directement le but à l'équipe attaquante.But en prolongation : lorsque le but est inscrit en temps supplémentaire et met fin au match.Go-ahead goal : but qui donne l'avantage à une équipe alors que le score était à égalité.But égalisateur : but qui permet à une équipe d'égaliser le score.But gagnant : but qui donne la victoire à une équipe.Autres termes employés:« garbage goal » ou « but rebut »: but marqué plus par chance ou opportunisme que par talentBut en échappée : but marqué par un joueur qui s'est glissé derrière les défenseurs pour affronter le gardien seul.Un « quinella » survient lorsqu'un joueur, au cours de la même partie marque un but en égalité numérique, un en supériorité numérique, un en infériorité numérique, un but de pénalité et un but dans un filet désert.Mario Lemieux est le seul joueur de l'histoire de la LNH à avoir réussi cet exploit contre les Devils du New Jersey. On y fait souvent référence comme « 5 buts, 5 différentes façons ».Une rondelle lancée qui atteint le gardien est compté comme un tir. Chaque tir stoppé par le gardien est compté comme un arrêt.Les deux coéquipiers (un seul en Extraliga) qui ont touché la rondelle avant le buteur sont crédités d'une aide.Quand un joueur marque 3 buts dans le même match on parle de tour du chapeau ou coup du chapeau. S'ils sont marqués consécutivement, il s'agit d'un tour du chapeau naturel.Le hockey sur glace est un des rares sports dans lequel une sirène retentit lors d'un but. C'est particulièrement vrai en LNH où la sirène retentit après chaque but de l'équipe hôte. Les sirènes sont différentes suivant les équipes, certaines ont même des effets sonores telles une alarme de police ou une corne de brume de bateau, ou même les deux combinés, comme les Capitals de Washington.NHL Rulebook, Rule #57 - Goals and AssistsJoueurs de la LNH avec 500 buts50 buts en 50 matchs Portail du hockey sur glace
sport;La Charte olympique est un ensemble de règles et de lignes directrices qui définit les principes fondamentaux pour l'organisation des Jeux olympiques et pour diriger le mouvement olympique.La Charte est publiée dès 1908 sous le nom Comité International Olympique - Annuaire, et reprend des règles écrites par Pierre de Coubertin en 1899. La Charte olympique est connue sous son nom actuel depuis 1978. La version actuelle date du 26 juin 2019.La Charte olympique est composée de six chapitres et de soixante et un articles.Le chapitre 1 définit le Mouvement olympique, mission et rôle du CIO y compris la non-discrimination, égalité entre les hommes et les femmes, le développement durable. Ses trois principales parties constitutives sont le CIO, les fédérations internationales et les Comités nationaux olympiques (art. 2).Le chapitre 2 décrit le Comité international olympique comme une « organisation internationale non gouvernementale à but non lucratif dotée de la personnalité juridique » dont le siège est à Lausanne, en Suisse et ayant pour but de « remplir la mission, le rôle et les responsabilités que lui assigne la Charte olympique » (art. 15).Le chapitre 3 décrit la mission et le rôle des fédérations internationales. Ce sont des « organisations internationales non gouvernementales qui administrent un ou plusieurs sports à l'échelle mondiale et qui comprennent des organisations administrant ces sports au niveau national » (art. 25).Le chapitre 4 définit les Comités nationaux olympiques. Leur mission est de « développer, protéger et promouvoir le Mouvement olympique dans leurs pays respectifs, conformément à la Charte olympique » (art. 27).Le chapitre 5 décrit les règles des Jeux olympiques. Cela inclut la célébration des Jeux, l'élection de la ville hôte, le comité d'organisation, le code d'admission aux Jeux olympiques, le programme des Jeux et le protocole olympique (symboles et cérémonies).Le dernier chapitre décrit les mesures et sanctions en cas de violation de la Charte olympique.Comité international olympiqueJeux olympiquesCharte olympique - Comité international olympique, 26 juin 2019 [PDF].Site officiel du mouvement olympique Portail des Jeux olympiques
sport;"Le Comité international olympique ou CIO (en anglais, International Olympic Committee ou IOC) est une organisation créée par Pierre de Coubertin en 1894, pour réinstaurer les Jeux olympiques antiques puis organiser cet événement sportif tous les quatre ans, puis en alternant tous les deux ans à partir de 1994, Jeux olympiques d'été et Jeux olympiques d'hiver.Depuis 1981, c'est une organisation internationale non gouvernementale à but non lucratif dont le siège est à Lausanne selon la Charte olympique. Cette association est dotée de la personnalité juridique à durée illimitée et son statut est reconnu par la Confédération suisse par arrêté du Conseil fédéral du 17 septembre 1981. Depuis le 10 septembre 2013, le neuvième président du CIO est Thomas Bach. Il est réélu le 10 mars 2021 pour un second et dernier mandat de quatre ans. 127 ans après la fondation de l'institution et l'adoption de la devise proposée par Henri Didon, celle-ci est modifiée lors de la 138e session du CIO : elle devient « Plus vite, Plus haut, Plus fort - Ensemble ».À Paris, le 23 juin 1894, en clôture du premier congrès olympique, le baron Pierre de Coubertin fonde le Comité international olympique afin de faire revivre les anciens Jeux olympiques après une absence de plus de 1500 ans. Il veut ainsi contribuer à bâtir un monde pacifique au moyen du sport en promouvant la communication, le fair-play et l'entente entre les peuples. Le CIO est une organisation dont le but est de localiser l'administration et l'autorité pour les jeux, ainsi que de fournir une seule entité légale qui détient tous les droits et les marques. Par exemple le logo olympique, le drapeau, la devise et l'hymne olympique sont tous administrés et possédés par le CIO. Le président du Comité olympique représente le CIO dans son ensemble, et les membres du CIO le représentent dans leurs pays respectifs.Ayant pour principe immuable une totale neutralité politique, le CIO prend une série de décisions historiques en février 2022 à la suite de l'invasion de l'Ukraine par la Russie. Il s'élève en effet contre la violation de la trêve olympique et demande aux fédérations sportives internationales d'exclure les athlètes et officiels russes et biélorusses de la totalité de leurs compétitions. Alors que jusqu'alors et compte tenu de la suspension de la Russie pour cause de dopage institutionnel, les sportifs de ce pays pouvaient concourir aux Jeux d'été et d'hiver sous bannière neutre, l'institution lausannoise prend position pour l'Ukraine et sa « communauté olympique », une démarche qu'elle n'avait encore jamais empruntée dans son histoire.Le manuscrit original du manifeste, texte fondateur et ayant servi au discours de Pierre de Coubertin du 25 novembre 1892, avait été vendu aux enchères chez Sotheby's à New York, le 18 décembre 2019. Il avait été acquis par le milliardaire russe Alicher Ousmanov pour un montant record de 8,8 millions de dollars. Le 10 février 2020, le Comité international olympique a annoncé que l'acquéreur lui a offert le manuscrit, tout en souhaitant que celui-ci soit exposé au musée olympique de Lausanne. Le CIO est composé de 115 membres qui se réunissent au moins une fois par an, et élisent un président pour une durée de 8 ans. Les membres sont tous des personnes physiques. Le CIO comprend notamment parmi ses membres des athlètes actifs, d'anciens athlètes ainsi que des présidents ou dirigeants au plus haut niveau de fédérations internationales de sport, d'organisations internationales reconnues par le CIO. Le CIO recrute et élit ses membres parmi les personnalités qu'il juge qualifiées. Les moyens financiers proviennent d'une part des droits de retransmission télévisée et d'autre part des partenariats avec des sociétés multinationales. La présidence de Juan Antonio Samaranch (1980 à 2001) a vu une explosion des droits télévisés et de parrainage des Jeux. En décembre 1998, éclate le scandale de corruption qui a entouré la désignation de Salt Lake City comme ville organisatrice des Jeux olympiques d'hiver de 2002.Le CIO, autorité suprême du mouvement olympique, désigne lors de sa session annuelle les villes hôtes pour les Jeux olympiques d'été comme d'hiver. L'élection de la ville hôte requiert la majorité absolue des suffrages exprimés. Si, à un tour donné, aucune ville n'obtient la majorité absolue des suffrages exprimés, un nouveau tour est effectué en éliminant la ville ayant reçu le moins de voix. Lors du dernier tour s'il y a lieu, les deux finalistes sont convoqués pour assister au résultat final du vote. Le vote est secret et n'ont pas le droit de vote les membres du CIO ayant la même nationalité qu'une ville encore en lice. Le CIO se réunit également en congrès exceptionnels.Chargé d'organiser les Jeux olympiques, il en délègue l'organisation matérielle à un comité local d'organisation des jeux olympiques (COJO) et l'organisation technique des épreuves retenues au programme de chaque olympiade aux fédérations internationales compétentes. Il gère les Jeux olympiques d'été depuis 1896, ceux d'hiver depuis 1924 et depuis 2010 les Jeux olympiques de la jeunesse organisés pour la première fois à Singapour. Ses membres sont également impliqués dans l'organisation des Jeux régionaux ou continentaux, reconnus par le CIO et gérés par des comités olympiques ad hoc : jeux asiatiques, jeux africains, jeux panaméricains, jeux méditerranéens, jeux du Pacifique.Depuis 2001, le CIO dispose du Service olympique de radiotélévision (Olympic Broadcasting Service), une agence capable de retransmettre les épreuves sans forcément passer par un diffuseur partenaire. Son siège est situé à Madrid. Pour les Jeux olympiques de 2020, le CIO souhaite prolonger le mouvement en créant une chaîne olympique (qui sera plus un catalogue de contenus à la demande).Depuis sa création en 1894 jusqu'au 10 avril 1915, le siège du CIO est à Paris. À cette date, Coubertin décide de le mettre à l'abri des hostilités en le localisant en Suisse à Lausanne. Le CIO s'installe d'abord au casino de Montbenon de 1915 à 1922 puis à la villa Mon-Repos de 1922 à 1968 et depuis 1968, son siège principal est le château de Vidy sur les rives du Léman. En 1986, le CIO inaugure la Maison olympique. Pour la première fois de son histoire, celui-ci possède un bâtiment qui centralise l'essentiel de ses activités dans le monde. Le musée olympique, fondé sous l'impulsion de Juan Antonio Samaranch, est inauguré le 23 juin 1993. Il est situé au bord du Léman sur le quai d'Ouchy. C'est le deuxième musée le plus visité de Suisse et il reçoit le prix du musée européen de l'année en 1995.Coubertin veut faire de l'Olympisme une véritable religion laïque. Aussi celui-ci n'échappe-t-il pas à une véritable liturgie marquée par des symboles forts qui se sont établis au fil des Jeux et qui sont actuellement des marques protégées contre tout usage illicite. Sont successivement apparus : la devise olympique, le credo olympique, les anneaux olympiques, le serment olympique, la flamme olympique, le relais olympique et l'hymne olympique.Citius-Altius-Fortius : expression latine signifiant plus vite, plus haut, plus fort. Coubertin l'emprunte au père Henri Didon qui utilise la formule Citius-Fortius-Altius — exprimée pour la première fois le 7 mars 1891 — pour décrire le parcours éducatif du collège Albert-le-Grand, à Arcueil, dont il est recteur : plus vite (athlétiquement), plus fort (intellectuellement et mentalement), plus haut (spirituellement). Cette expression latine est gravée dans la pierre au-dessus de l'entrée principale de l'établissement et reproduite sous cette forme au frontispice des premières Revue olympique avant de prendre sa forme actuelle.« Le plus important aux Jeux olympiques n'est pas de gagner mais de participer, car l'important dans la vie ce n'est point le triomphe mais le combat ; l'essentiel, ce n'est pas d'avoir vaincu mais de s'être bien battu ». Il s'agit de la forme actuelle du credo tel qu'il apparaît sur le panneau d'affichage à la cérémonie d'ouverture des Jeux olympiques. Pierre de Coubertin a repris puis adopté ce credo après avoir entendu le sermon de l'évêque de Pennsylvanie, Ethelbert Talbot (en), prononcé à la cathédrale Saint-Paul le 19 juillet 1908, au cours des Jeux de la IVe Olympiade à Londres. Les paroles exactes de Talbot sont : « L'important dans ces Olympiades n'est pas tant d'y gagner que d'y prendre part ».Conçu par Pierre de Coubertin lui-même en 1913, le drapeau olympique est présenté officiellement en juin 1914 au congrès de Paris. Mais du fait de la Grande Guerre, il ne flotte pour la première fois qu'aux Jeux d’Anvers en 1920. Les 5 anneaux entrelacés représentent les cinq continents réunis par l’olympisme et les six couleurs (en comptant le blanc du fond) rappellent les drapeaux de toutes les nations car au moins une d'elles se retrouve dans celui de celles présentes à la création des Jeux en 1896. Ce symbole est donc celui de l’universalité de l’esprit olympique. Depuis, une opinion courante mais démentie par le CIO, associe un continent à chaque couleur des anneaux (le bleu représentant l'Europe, le noir l'Afrique, le jaune l'Asie, le vert l'Océanie et le rouge l'Amérique).« Au nom de tous les concurrents, je promets que nous prendrons part à ces Jeux olympiques en respectant et suivant les règles qui les régissent, dans un esprit de sportivité, pour la gloire du sport et l'honneur de nos équipes ». Écrit par Coubertin, ce serment est prononcé par un athlète du pays hôte tenant le pan du drapeau olympique de sa main gauche. C'est en 1920 à Anvers que l'escrimeur belge Victor Boin prononce le serment olympique pour la première fois. Depuis, un juge et un entraîneur du pays hôte prononcent également chacun un serment dont l'énoncé est légèrement différent.La flamme olympique est un symbole qui nous vient des Jeux olympiques de l'antiquité au cours desquels une flamme sacrée brûle en permanence sur l'autel de Zeus. La flamme est allumée pour la première fois aux Jeux de la IXe Olympiade en 1928 à Amsterdam puis à nouveau pendant les Jeux de la Xe Olympiade en 1932 à Los Angeles. La flamme est allumée — par des femmes vêtues de tuniques similaires à celles portées par les Grecs de l'antiquité — au cours d'une cérémonie dans l'antique stade olympique d'Olympie dans la région grecque du Péloponnèse. La flamme est allumée naturellement par les rayons du soleil d'Olympie, réfléchis à l'aide d'un miroir parabolique. La grande prêtresse remet ensuite le flambeau au premier relayeur.En 1936, Carl Diem, président du comité d'organisation des Jeux de la XIe Olympiade à Berlin, propose d'allumer la flamme en ancienne Olympie et de la transporter jusqu'à Berlin via un relais du flambeau. Son idée est adoptée et la tradition se perpétue depuis lors.Cantate de Kostís Palamás mise en musique par Spýros Samáras en 1896, l'hymne olympique est joué pour la première fois à l'occasion de la première olympiade. Cependant, il n'est adopté comme hymne olympique officiel par le CIO qu'en 1957.Le président représente le CIO et préside toutes ses activités. Il est élu par la session au scrutin secret. Autrefois illimitée, la durée du mandat présidentiel est fixée à huit ans depuis le 12 décembre 1999, renouvelable une fois pour quatre ans. L’article 20 de la Charte olympique définit le rôle du président, notamment sa fonction de représentation.En septembre 2013, l'Allemand Thomas Bach devient le neuvième président du CIO. Après un premier mandat de huit ans, il est réélu pour un deuxième mandat de quatre ans le 10 mars 2021,,. Liste des présidents 1894-1896 :  Dimítrios Vikélas ;1896-1925 :  Baron Pierre de Coubertin ;1925-1942 :  Comte Henri de Baillet-Latour ;1946-1952 :  Sigfrid Edström ;1952-1972 :  Avery Brundage ;1972-1980 :  Michael Morris, lord Killanin ;1980-2001 :  Juan Antonio Samaranch ;2001-2013 :  Comte Jacques Rogge ;Depuis 2013 :  Thomas Bach.Ils sont au nombre de quatre, élus pour un mandat de quatre ans. Ils ne peuvent exercer que deux mandats consécutifs et doivent attendre ensuite deux ans pour être à nouveau éligibles. Yu Zaiqing Juan Antonio Samaranch i Salisachs Uğur Erdener Anita DeFrantzLe CIO étant une organisation non gouvernementale ses membres, choisis par cooptation par le reste du comité, ne représentent aucun des pays dont ils sont ressortissants. Ce fonctionnement atypique, souvent mal compris, est assimilé à un manque de transparence dans le fonctionnement. Sont le plus souvent visés :le choix des membres, ressenti comme le fait du prince ou de réseaux plus ou moins occultes. En outre le passé de quelques-uns d'entre eux n'est pas toujours des plus clairs. Certains — dont l'ancien président Avery Brundage — restent suspects de sympathies avec le régime nazi avant la guerre alors que l'ancien président Juan Antonio Samaranch est assurément secrétaire des sports du régime de Franco en 1967 ;la désignation des villes olympiques par vote à bulletins secrets. Pour exemple à propos du choix du lieu des Jeux olympiques d'hiver de 2014, le chancelier autrichien, Alfred Gusenbauer, a déclaré : « Si c'est une question de pouvoir politique et de gros sous, alors Salzbourg n'avait aucune chance. Je suis persuadé que le concept que nous présentions était absolument le meilleur. » ;la gestion des fonds. Des critiques sont émises tant sur d'éventuelles compromissions avec les sponsors qui semblent parfois dicter le programme même des Jeux que sur l'usage qui est fait de l'argent récolté. Pour la période 2001-2004 le mouvement olympique a généré un revenu de plus de quatre milliards de dollars.Depuis Albertville en 1992, la participation des athlètes des pays tropicaux aux Jeux d'hiver semble impactée par l'instauration de minima drastiques. L'argument du CIO « les Jeux olympiques d'hiver sont une manifestation quelque peu particulière, étant donné que, pour des raisons tout simplement climatiques et géographiques, ils ne conviennent pas à certains pays du monde, que ce soit au niveau de l'organisation ou de la participation » ne semble pas satisfaire tous les détracteurs.Le choix de Pékin pour les Jeux olympiques de 2008 qui a entraîné des expulsions massives de populations (1,5 million selon l'ONG COHRE) a été vigoureusement critiqué par les associations de défense des droits de l'homme, et entraîné dans certains pays des manifestations spectaculaires lors de leur traversée par le relais de la flamme.En 2004, un reportage de la chaîne britannique BBC montre que certains membres du CIO, dont Ivan Slavkov, président du comité olympique bulgare, sont prêts à monnayer leur soutien lors de divers votes. D'autres soupçons de corruption ont été établis, notamment lors des Jeux de Salt Lake City et de Sotchi. Le CIO a pris des mesures disciplinaires dans les cas avérés.En juillet 2019, l’ex-gouverneur de Rio, Sérgio Cabral Filho, reconnaît avoir payé des pots-de-vin à des délégués du Comité international olympique pour décrocher l’organisation de la compétition en 2016.Début 2021, le CIO annonce vouloir diminuer de 30 % ses émissions carbone d'ici à 2024 et de 45 % d'ici à l'horizon 2030. Il indique ainsi que chaque futur comité d'organisation des JO devra intégrer la dimension écologique à ses programmes et donc minimiser l'impact écologique. Thomas Bach justifie cet engagement : « Cet objectif ambitieux permet au CIO de se conformer à l’accord de Paris ».Jean Durry, Le Vrai Pierre de Coubertin, Paris, Comité français Pierre de Coubertin, 1997.Luis Fernandez, La Faillite du sport français : face aux 7 faillites du sport français, le bon sens !, Communauté européenne, Rue du sport, 2011, 123 p. (ISBN 978-2-84653-045-3 et 2-84653-045-9).Stefan Huebner, Pan-Asian Sports and the Emergence of Modern Asia, 1913-1974. Singapour: NUS Press, 2016 (le CIO et le sport dans l'Asie).Résultat des élections des villes hôtes des Jeux olympiques d'étéRésultat des élections des villes hôtes des Jeux olympiques d'hiverListe des membres du Comité international olympiqueComité national olympiqueListe des codes pays du CIOProcédure de sélection de la ville hôte des Jeux olympiquesAcadémie internationale des sciences et techniques du sportMusée olympiqueAndrew Jennings (journaliste d'investigation - enquête sur la corruption au sein du CIO)Ressource relative aux organisations : Registre de transparence de l'UE Ressource relative à la recherche : CrossRef Site du Comité international olympique(en) Archives complètes des rapports du CIO de 1896 à 2002 Portail des Jeux olympiques   Portail de Paris   Portail de Lausanne"
sport;
sport;La crosse, le bâton de hockey ou la canne de hockey désigne le bâton recourbé utilisé pour diriger le palet — également appelé puck, rondelle ou disque — ou la balle que ce soit au hockey sur glace, au roller hockey ou encore au hockey sur gazon. La crosse désigne aussi le jeu d'origine amérindienne.D'une longueur variable de 163 cm maximum, elle est prolongée par une palette incurvée d'une longueur maximum de 32 cm. La hauteur de la palette est comprise entre 5 et 7,5 cm. La crosse est composée de bois ou de différents matériaux composites comme le kevlar ou la fibre de verre.La crosse du gardien est plus coudée et la palette plus large (dessin de droite).Chaque région francophone possède un vocabulaire propre : la « crosse » est le terme utilisé en France, en Suisse romande on lui préfère le terme « canne de hockey ». Au Canada francophone, on parle soit de « bâton de hockey », soit tout simplement de « bâton » ou « hockey », parce que le terme crosse désigne l'équipement utilisé dans un autre sport collectif, la crosse, surtout populaire en Amérique du Nord.Il n’est pas rare de nos jours de voir des bâtons en graphite, d’autres en fibre de carbone, en plastique ou même en titane. Mais, à l'origine, la crosse de hockey était constituée d’un seul morceau de bois. Au XVIIe siècle, les Amérindiens jouaient avec des bâtons courbés, taillés dans un arbre, et avec une balle à un jeu semblable au curling sur gazon. En 1855, le hockey nait à Kingston, Ontario, importé par les soldats britanniques. À cette période, les joueurs confectionnaient eux-mêmes leur crosse en hêtre blanc. L'industrialisation de la fabrication des crosses date seulement du début du XXe siècle. Jacob Hespeler, entrepreneur Ontarien, a créé plusieurs moulins à scie afin de produire, en série, des crosses de hockey. À cette époque, les crosses étaient vendues 45¢ la douzaine, aujourd’hui, une crosse coûte entre 20 $ et 350 $ l’unité.En 2015, un musée canadien a acquis la plus vieille crosse du monde, datée des années 1830, pour la somme de 300 000 dollars.Aujourd’hui, avec la production en série, la crosse de hockey est fabriquée en plusieurs étapes. Le manche est fait d’un morceau de tremble sur lequel deux lames très minces de bouleau sont collées sous pression. Ensuite, le manche est scié en trois parties identiques, chaque partie permettant de fabriquer une crosse.Les trois manches sont décapés à la sableuse une première fois, puis renforcés avec de la fibre de verre et de la fibre de carbone.Les manches sont ensuite déposés dans un moule afin d’être cuits à 80 °C sous pression.La septième étape consiste à détailler les manches un par un à l’aide d’une moulurière qui arrondi les coins pour ensuite repasser dans une sableuse afin d’ôter les aspérités du bois.Un bloc est collé au bout du manche afin de recevoir la palette. La colle utilisée doit être résistante à l'eau.L’angle de la palette est très important. Les différents angles possibles sont définis par une norme. Les angles possibles sont numérotés de 1 à 6. Plus le chiffre est grand, plus l’angle entre la palette et le bout de la crosse est grand.Les étapes suivantes consistent à affiner la palette. Tout d’abord, l’ensemble de la crosse est à nouveau sablé afin d'éliminer les dernières aspérités. La crosse passe ensuite sur une CNC qui façonnera la palette en fonction du modèle choisi.Il existe plus de 6 000 modèles de palette. La courbure est obtenue après passage dans une machine à vapeur qui la rend malléable.Ensuite, la palette est renforcée avec une toile de fibre de verre et de la résine d’époxy. Après 24 heures de séchage à une température de 32 °C et une finition à la sableuse circulaire, la crosse est à nouveau plongée dans un bain de résine d’époxy.L'étape finale consiste en l'application de la marque du fabricant sur le manche par sérigraphie.En moyenne 40 000 crosses de hockey sont fabriquées chaque semaine. Portail du hockey sur glace
sport;"Le cyclisme recouvre plusieurs notions concernant la bicyclette : il est d'abord une activité quotidienne pour beaucoup, un loisir pour d'autres (cyclotourisme), enfin un sport proposant des courses selon plusieurs disciplines : l'école de cyclisme, le cyclisme sur route, le cyclisme sur piste, le cyclo-cross, le vélo tout terrain (abrégé couramment VTT), le BMX, le cyclisme en salle et le polo-vélo. Le sport cycliste est réglementé au niveau mondial par l'Union cycliste internationale (UCI).Le baron allemand Karl Von Drais invente en 1816 la draisienne, considérée comme l'ancêtre de la bicyclette. C'est un véhicule à deux roues alignées que le cycliste fait avancer en poussant sur le sol avec ses pieds. Il présente son invention à Paris, le 5 avril 1818. En 1861, le carrossier Pierre Michaux et son fils Ernest commencent la fabrication des premiers vélocipèdes à pédale. Pierre Michaux appelle cette pédale « pédivelle » et en généralise la fabrication en créant son entreprise en 1865, la « Maison Michaux » qui devient « La Compagnie parisienne » en 1869. Il est question, à partir de 1867, de succès populaire. Des engins similaires au vélocipède Michaux ont beaucoup de succès aux États-Unis à partir de 1866, lorsque Pierre Lallement, ancien associé de Pierre Michaux, obtient un brevet américain pour une machine qu'il appelle « bicycle ». Vers la fin des années 1870 apparaît le grand-bi. Il a une roue avant d'un très grand diamètre et une roue arrière plus petite. L'intérêt de la grande roue avant est d'augmenter la distance parcourue pour un tour de pédale. La roue avant étant plus haute et plus grande que la roue arrière, la conduite de l'engin est dangereuse et difficile.Deux ans plus tard, les premiers clubs cyclistes sont fondés : les Véloces Clubs de Paris, Toulouse, Rouen. En 1880, John K. Starley de la société The Coventry Sewing Machine Company (« société des machines à coudre de Coventry »), qui deviendra Rover, invente la « bicyclette de sécurité » avec des roues de taille raisonnable et une transmission par chaîne. Le cycliste y est installé à l'arrière, ce qui rend presque impossible la chute de type « soleil » où le cycliste est catapulté par-dessus la roue avant. Le 6 février 1881, l'Union vélocipédique de France (UVF) est créée à Paris. L'Écossais John Boyd Dunlop invente le pneumatique en 1888, ce qui contribue à améliorer encore le confort du cycliste. Trois ans plus tard, Michelin invente le pneu démontable. En 1896, le cyclisme devient sport olympique. En 1900, l'Union cycliste internationale est créée.En 1910, la roue libre fait son apparition sur le Tour de France (compétition créée en 1903), puis le dérailleur en 1937. En 1941, sous le régime de Vichy, Jean Borotra interdit aux femmes de participer à des compétitions cyclistes, jugeant que ce sport est nocif pour elles. En 1945, l'UVF est remplacée par la Fédération française de cyclisme (FFC). La Fédération Internationale de cyclisme amateur, fondée en 1900 par les membres des fédérations américaine, belge, française, italienne et suisse, fusionne en 1993 avec la Fédération Internationale de cyclisme professionnel pour devenir l'Union cycliste internationale (UCI). Plus de 170 fédérations nationales sont affiliées à l'UCI.L'équipement diffère sensiblement selon le type de pratiquant.En randonnée amateur ou en simple circulation, le port du casque par le cycliste n'est pas obligatoire, comme le phare, lorsque son utilisation n'est pas nécessaire (hors d'un tunnel, par beau temps, par exemple), bien que tous deux conseillés. On peut également installer des pare-boue (surtout en temps pluvieux ou dans une zone forestière). Mais il est obligatoire de porter et d'utiliser un phare diffusant un rayon blanc ou jaune lorsque le temps ou le lieu l’exige (dans un tunnel, lorsque le temps est pluvieux), de même qu'une bicyclette doit porter tout le temps un catadioptre blanc à l’avant et rouge à l'arrière du véhicule, et que le conducteur doit porter sur lui un gilet (recommandé hors agglomération, obligatoire de nuit ou quand la visibilité est nulle ou mauvaise (brouillard)).En course, lors de compétitions professionnelles, le port du casque est obligatoire dans toutes les disciplines afin d'éviter les accidents, tout comme pour le triathlon. De plus, depuis le 1er janvier 2000 le poids minimum des bicyclettes est fixé à 6,8 kg.Le cyclisme urbain constitue la branche du cyclisme dévolue au transport urbain. Il s'agit de tout ce qui est relatif aux déplacements à vélo sur de petites et moyennes distances (quelques kilomètres) en milieu quasi exclusivement urbain (dans la ville et sa proche banlieue), c'est-à-dire en partageant la voirie avec les autres modes de déplacement motorisés ou non. Sur une distance entre un et sept kilomètres le vélo est le mode de transport le plus rapide. Au-delà de cette praticité, le cyclisme urbain vise à limiter la pollution et à diminuer l'engorgement croissant des villes par l'usage massif de l'automobile. De plus, ses bienfaits multiples pour la santé individuelle sont reconnus. Il fait partie de l'écomobilité.Des aménagements spécifiques peuvent être construits pour tenter d'améliorer les conditions de circulation des cyclistes urbains. Ces aménagements peuvent être des bandes ou pistes cyclables, zone de rencontre, Double-sens cyclable et parkings dédiés aux vélos (ou vélos et cyclomoteurs). Des feux cyclistes spécialement dédiés sont également mis au point.Le cyclotourisme est la pratique de la randonnée à bicyclette, réalisée sans esprit de compétition. Le cyclotourisme se présente sous de multiples formes, de la promenade et de la pratique familiale à allure libre jusqu'aux rallyes et brevets sur tous les parcours et toutes les distances : rallyes libres, brevets-randonneurs, flèches, diagonales, cyclo-découvertes, cyclo-montagnardes, Audax. Créé en 1890, le Touring Club de France est la seule association représentative du cyclotourisme en France de la fin du XIXe siècle au début du XXe siècle. Par la suite, le club se tourne vers le tourisme automobile, ce qui amène la création le 8 décembre 1923 de la Fédération Française des Sociétés de Cyclotourisme, rebaptisée en 1945 Fédération française de cyclotourisme (FFCT). À partir des années 1970, le développement de la Fédération est devenu régulier et le nombre de clubs ne cesse d'augmenter en France.Le principe du cyclisme dans sa version sportive est de parcourir une distance donnée à bicyclette le plus rapidement possible. Les pratiquants sont répartis dans des catégories en fonction de leur âge, de leur sexe et de leur niveau.Le cyclisme sur route comprend plusieurs types d'épreuves incluant la « course à étapes » (Tour de France, Tour d'Espagne, Tour d'Italie, Tour de Suisse, Tour de Belgique, Tour de Pologne) ; la « course classique », course en ligne d'un jour (Paris-Roubaix, Tour de Lombardie, Liège-Bastogne-Liège, par exemple) ; la course « contre-la-montre », (Grand Prix des Nations, par exemple) et la course « contre-la-montre par équipe » (Eindhoven Team Time Trial, par exemple). Il est dit du cyclisme sur route que c'est un sport d'équipe, mais à classement individuel[réf. nécessaire].Les épreuves de cyclisme sur piste se déroulent sur des vélodromes et prennent des formes plus diversifiées : il existe des épreuves individuelles et des épreuves par équipes, des épreuves de sprint, de demi-fond et de fond. Les plus connues sont la vitesse, la poursuite et le kilomètre. Les principales compétitions sont les Jeux olympiques et les Championnats du monde de cyclisme sur piste. Le cyclocross est une spécialité hivernale, qui se court en circuit. La course dure une heure environ, sur un revêtement souvent mixte (asphalte et terre). Le BMX est essentiellement caractérisé par sa très petite taille et ses roues de 20"", ce qui lui confère une maniabilité sans égale et a permis la création de plusieurs disciplines : race, dirt, flat, rampe, street.Le vélo tout terrain, également appelée VTT, est la plus récente du cyclisme sportif. Le vélo tout terrain (ou Mountain Bike) a été inventé aux États-Unis en 1970. Il a été importé en France quelques années plus tard. Ce terme désigne également le vélo avec lequel cette discipline est pratiquée : compte tenu de sa polyvalence, ce type de vélo est, avec le VTC, le plus vendu en France actuellement.Les épreuves cyclosportives (ou cyclosport) sont à mi-chemin entre le cyclotourisme des randonneurs et la compétition cycliste, mais il s'agit d'épreuves de masse proposées à tous les cyclistes : les organisateurs proposent des dossards, un classement et un chronométrage comme dans les marathons de course à pied. En France, plus de 150 sont dénombrées, la plus célèbre et la plus ancienne étant « La Marmotte », créée en 1982 dans les Alpes, qui attire plus de 8 000 participants.Le cyclisme handisport est un sport dérivé du cyclisme et des compétitions existent depuis les années 1960. Championnats  Jeux olympiques Le sport cycliste est inscrit de longue date au calendrier des Jeux olympiques d'été. Les épreuves sont ouvertes tant aux hommes qu'aux femmes. Les épreuves sur route incluent la course en ligne et l'épreuve du contre-la-montre. Le cyclisme sur piste est également présent avec différentes disciplines et, depuis quelques années, le mountain-bike et le BMX ont été inclus au programme.Le cyclisme handisport ou paracyclisme est devenu un sport officiel aux Jeux paralympiques depuis les Jeux paralympiques d'été de 1988 à Séoul, avec une version sur route aux Jeux paralympiques d'été de 1996 à Atlanta. Nationaux et continentaux Chaque pays organise ses propres championnats pour déterminer les meilleurs athlètes par discipline. Les Championnats de France de cyclisme sont organisés tous les ans par la Fédération française de cyclisme. Ils rassemblent des athlètes par équipes et par régions et se déclinent par catégories d'âge (cadets, juniors, espoirs et élite). En Europe, il existe des championnats d'Europe dans chaque discipline. Personnalités La rédaction de L'Équipe magazine a établi un classement des 100 sportifs du siècle. Eddy Merckx, classé neuvième, est le premier d'entre eux. Quatre autres cyclistes ont été retenus : Fausto Coppi, Jacques Anquetil, Alfredo Binda et Bernard Hinault. Tous sont issus de la route et aucune femme ne figure dans ce classement. Par ailleurs, Eddy Merckx a été élu « Coureur du siècle » par l'UCI. Chaque année, plusieurs titres de meilleurs coureurs sont décernés. Le plus prestigieux de ces honneurs reste le Vélo d'or. Citons également le Mendrisio d'or et le coureur UCI de l'année. Performances et records La piste est également utilisée pour établir des records de temps sur une distance déterminée, ou de distance parcourue pendant une durée déterminée. Le plus célèbre est le record de l'heure, qui consiste à parcourir seul la plus grande distance possible pendant une heure. L'histoire du record de l'heure a été jalonnée par les exploits des plus grands coureurs spécialistes du contre-la-montre sur route ou de la poursuite sur piste (parmi lesquels Fausto Coppi, Jacques Anquetil, Eddy Merckx, Francesco Moser, Miguel Indurain peuvent être cités).La plus grande vitesse jamais atteinte sur du plat, l'a été par le Néerlandais Sebastiaan Bowier en septembre 2013, portant le record du monde à 133,78 km/h, sur son vélo couché hautement aérodynamique, grâce à un carénage aérodynamique. C'est le record toutes catégories pour les véhicules à propulsion humaine. Ultracyclisme L'ultracyclisme est une discipline regroupant les compétitions disputées sur de très longues distances. L'épreuve la plus connue de l'ultracyclisme avec assistance véhiculée est la Race Across AMerica (RAAM)[réf. nécessaire]. Cyclisme féminin Resté anecdotique pendant de nombreuses années, le cyclisme féminin est reconnu par l'Union cycliste internationale mais aussi par la Fédération française de cyclisme. Ce ne fut pas toujours le cas : en France, en 1941, sous le régime de Vichy, Jean Borotra interdit aux femmes de participer à des compétitions cyclistes, jugeant que le ce sport est nocif pour elles. Cette pratique prend un essor important avec l'organisation, en 1984, du Tour de France féminin. En effet, la Grande Boucle féminine internationale (anciennement Tour de France féminin) est une course cycliste sur route par étapes, qui s'est disputée chaque année entre 1984 et 2009 en France, exclusivement par des femmes. La course était considérée comme l'un des trois grands tours féminins avec le Tour d'Italie féminin et le tour de l'Aude. De 1984 à 1989, un Tour de France féminin est couru en lever de rideau de l'épreuve masculine. Cette épreuve est organisée par la Société du Tour de France organisatrice du Tour de France masculin. En 1990, cette épreuve change de nom et de format : elle devient le Tour de la CEE féminin qui s'arrête en 1993.Les jeux olympiques accueillent une épreuve féminine sur route depuis 1984, des compétitions féminines sur piste depuis 1988. En 2012, pour rétablir la parité aux jeux olympiques de Londres, le programme du cyclisme féminin est similaire au programme du cyclisme masculin : il comprend une course sur route (médaille d'or pour Marianne Vos, néerlandaise), une course contre la montre (médaille d'or pour Kristin Armstrong, américaine), une épreuve de vitesse (médaille d'or pour Anna Meares, australienne), une compétition de keirin féminin (médaille d'or pour Victoria Pendleton, britannique), l'omnium (médaille d'or pour Laura Kenny, britannique), la vitesse par équipes, la poursuite par équipes, ainsi qu'un cross-country en V.T.T. (médaille d'or pour Julie Bresset, française) et une compétition de B.M.X. (médaille d'or pour Mariana Pajón, colombienne).La Coupe du monde de cyclisme sur route féminine a été créée en 1998.Alors que les championnats du monde ont été créés en 1895 pour les hommes, le championnat du monde de cyclisme sur route féminin n'est apparu qu'en en 1958. Certaines épreuves, comme le 500 m femmes, sont toutes récentes. Réservées aux femmes, elles se disputent, seule contre la montre sur 500 mètres, départ arrêté avec un classement au temps. La cycliste accélère le plus rapidement possible jusqu'à sa vitesse maximale, puis tente de la maintenir jusqu'à la ligne d'arrivée. Les meilleurs temps pour le 500 mètres départ arrêté tournent autour de 34 secondes. Les tactiques ne sont pas prédominantes dans cette compétition. Il s'agit purement d'un test de puissance et de techniques précises.La pratique du cyclisme peut avoir des conséquences à long terme sur la santé de ses pratiquants.Les exercices physiques du cyclisme sont liés à l'amélioration de la santé et au bien-être. Elles assurent, selon la plupart des études de santé,, la perte de poids, améliorent l’endurance, diminuent les temps de récupération, permettent d'éviter les maladies cardiovasculaires liées à l'âge et amplifient les voies respiratoires. Tous ces effets sont bons à long terme. Néanmoins, les risques sont prépondérants en cas d'efforts trop fréquents[source insuffisante]. Bien qu'exposé à la pollution automobile, les bénéfices semblent[réf. nécessaire] l'emporter sur les effets de cette pollution, par rapport aux autres modes de déplacements.La pratique du cyclisme ne se fait pas sans risque de blessure. De telles contre-indications seraient d'abord liées au manque de prévention : vitesse trop élevée pour la route selon son état ou son inclinaison, non-respect du Code de la route (les grillages des feux urbains, le passage d'un Stop ou d'un Cédez-le-passage sans considération, la conduite à gauche ou en zigzag), intolérance des automobilistes. Ensuite, l'état de la chaussée et de la piste cyclable peut également jouer un rôle dans le déséquilibre du cycliste et la chute de celui-ci, si elles sont déplorables. Enfin, des conditions météorologiques, telles que la neige, la grêle ou même la foudre peuvent contribuer à blesser, voire tuer des cyclistes.Les traumatismes crâniens représentent, selon une étude de 2012, environ une personne sur vingt des tués et blessés graves en agglomération ; la moitié des accidents sont d'ailleurs liés[pas clair] à ce type de traumatisme. Les deux tiers des cyclistes accidentés dont le pronostic vital a été engagé présentaient effectivement un traumatisme crânien. Le cycliste peut également endurer des crampes, des fractures osseuses en cas de chute, un malaise, etc. Des douleurs à plus long terme peuvent également être la cause des blessures ressenties à vélo comme les hernie discale ou une tendinite… La vulve de la cycliste, présente chez une cycliste professionnelle sur cinq environ, est une déformation des grandes lèvres en raison du poids et du frottement des parties génitales contre la selle.Les auteurs de la précédente étude indiquent qu'en 2010, en France, 59 cyclistes sont décédés et 963 blessés.Depuis les origines du cyclisme, une presse spécialisée a accompagné son essor, tant celui du cyclisme de compétition que celui du cyclisme de loisir et du cyclotourisme. Pour la France, des ouvrages ont fait le recensement de cette littérature. De 1946 jusqu'aux débuts des années 1970, la presse sportive généraliste quotidienne (L'Équipe), hebdomadaire (Miroir Sprint, Miroir des sports), mensuelle (Sport digest, Sport collection, Sport mondial, Sport & vie) a dominé le marché en ouvrant largement ses colonnes au vélo. À partir de 1960, la presse magazine cycliste à périodicité mensuelle, prend le relais avec principalement :La France cycliste, organe de la Fédération française de cyclisme, paraît de 1946 à 2014 ;Miroir du cyclisme, bimestriel la première année puis mensuel, de janvier 1960 à avril/mai 1994 ;l'Équipe cyclisme magazine, en parution mensuelle à partir de décembre 1968, suivi de Cyclisme magazine en 1975, transformé en Vélo en 1978, puis en Vélo magazine en 1984. Un temps appelées Vélo-sprint 2000, ces parutions relevaient toutes du groupe L'Équipe ;Sprint international, de mars 1981 à 1986, puis Sprint 2000, d'août 1986 à décembre 1988 ;Cyclisme international, de avril 1986 à février 2004 ;Vélo Un , de 1993 à 1999 ;Le Cycle, de création plus ancienne, ouvert aux constructeurs de cycles et aux techniques du vélo, modernisé au début des années 1980.Actuellement, la presse cycliste magazine est représentée par de nombreux titres, couvrant l'ensemble des activités cyclistes. Pour la France seule, par ordre d'ancienneté de parution :Cyclotourisme est édité par la Fédération française de cyclotourisme depuis 1953.Vélo Magazine, édité par le groupe industriel auquel appartient le quotidien L'Équipe, poursuit sa parution avec constance depuis les années 1970.Le Cycle, « magazine des pratiquants du cyclisme ».VTT Magazine parait depuis 1988. Avec le titre suivant il accompagne la montée en France de la pratique du Vélo tout terrain.Vélo vert, créé en 1989, s'adresse à la même clientèle.Cyclo passion, « officiel du pratiquant », créé en 1994 par l'équipe éditoriale de Vélo Un.Top vélo paraît depuis 1997.Cyclo sport.Vélo tout terrain.Planète cyclisme, créé en 2005, de parution bimestrielle régulière, prend la suite des magazines cyclistes indépendants des organisateurs de courses, avec modernisme.Bike magazine, créé en 2007.Only bike, magazine créé en 2008 se concentre sur des annuaires très complets, créés dès 2003.Mountain bike action magazine.Le monde du vélo.Pédale !, parution irrégulière de numéros « hors-série » au moment du Tour de France.Le Sport Vélo, créé au début de l'année 2011 (après 1 premier essai en 2004) cessait sa parution en août 2013 après 27 numéros.Les bienfaits de ce sport pour la santé sont mis en avant par la Fédération française de cyclisme et celle-ci demandera lors de la pandémie du Covid-19 d’autoriser à nouveau la pratique de ce sport, dès le 11 mai 2020,.Les représentants de la Fédération française de cyclisme fustigent en parlant d'une ""véritable discrimination vis-à-vis de (leur) sport"".Ouvrages à vocation encyclopédiqueSport vélocipédique : les champions français, par E. Gendry (G. de Moncontour), 1891, éd. G. Meynieu (Angers) (en ligne sur Gallica - BNF).Le Cyclisme théorique et pratique, par L. Baudry de Saunier, éd. La Librairie Illustré, 1893 (en ligne sur Gallica - BNF).Le cyclisme, Marcel Viollette, Lucien Petit-Breton, Thornwald Ellegaard, Louis Darragon, Gaston Rivierre, Paul Meyan, Ernest Mousset, préface d'Henri Desgrange, 1912, éd. Pierre Laffitte et Cie - Paris (en ligne sur Gallica - BNF).Pierre Chany : La fabuleuse histoire du cyclisme, tome 1, Paris, éditions ODIL 1975. 1074 pages. Préface d'Antoine Blondin.Pierre Chany : La fabuleuse histoire des grandes classiques et des championnats du monde, tome 2, Paris, éditions ODIL, 1979. 984 pages.Gérard De Smaele, Le cyclisme dans les livres et les revues, L'Harmattan, 2015, 292 p. Préface de Keyzo Kobayashi et Isabelle Lesens. Postface de Jean Durry.Jean Durry « et ses amis » : l'en CYCLE opédie, Lausanne, éditions Edita, 1982. 424 pages. Préface de Pierre Chany, contributions de Jacques Seray, Daniel Rebour, Ami Guichard, Philippe Marte, Pierre Roques, Serge Laget, etc.Pascal Sergent, Encyclopédie illustrée des coureurs français depuis 1869, Eeklo, Editions de Eecloonaar, 1998, 768 p. (ISBN 90-74128-15-7).Guy Crasset, Hervé Dauchy, Pascal Sergent, Encyclopédie mondiale cyclisme, éditions De Eecloonaar (Belgique), 2134 pages en 3 volumes, parue en 2000 avec le patronage de l'Union cycliste internationale.Claude Sudres : Dictionnaire international du cyclisme, plusieurs éditions entre 1983 et 2004.Ouvrages généralistesJacques Borgé & Nicolas Viasnoff : Archives du vélo, Monaco, éditions Michèle Trinckvel, 1998. 204 pages.Françoise & Serge Laget : l'univers du vélo, Paris, éditions Solar, 2001. 144 pages.Françoise & Serge Laget : Le cyclisme (La Belle époque du sport), Courlay, éditions Jadault, 1978. 98 pages. Préface de Raymond Poulidor.Les cahiers de médiologie : La bicyclette, numéro 5-premier semestre 1998, Paris, Gallimard.Louise Roussel, À vos cycles ! Le guide du vélo au féminin, Tana, 2021, 208 p. (ISBN 979-1030103892)Ouvrages annuelsVelo, annuaire paraissant en Belgique depuis 1956, publie les résultats des compétitions cyclistes de l'année antérieure à son millésime, avec des éditions récapitulatives (la plus récente en 2009 sous l'appellation Velo plus 1869-2009, signée Joel Godaert) donnant les résultats chronologiques des courses. Ses premiers concepteurs étaient René Jacobs et Hervé Mahau, d'où son appellation courante : annuaire Jacobs.Le guide international du cyclisme. Il paraît depuis 2003 sous la signature de Benoît Gauthier et est édité par les éditions Onlybike. On y trouve les résultats des courses cyclistes de l'année antérieure au millésime de parution et les grandes lignes des palmarès de la plupart des coureurs du peloton professionnel, amateur et junior.L'année du cyclisme, créé en 1974 par le journaliste Pierre Chany aux éditions Calmann-Levy en était en 2013 à son quarantième millésime de parution. Sa vocation est plus restreinte que les deux annuaires cités précédemment mais les ouvrages sont agrémentés de nombreuses photographies.Ouvrages relatifs au cyclisme fémininLa Femme et la bicyclette, par le Dr J. Alvin, 1895Rémy Pigois, Cyclisme féminin, Editions Amphora, 1996 (ISBN 978-2-85180-307-8)(en) Selene Yeager, The Bicycling Big Book of Cycling for Women : Everything You Need to Know for Whatever, Whenever, and Wherever You Ride, Rodale Press Inc., 2015, 320 p. (ISBN 978-1-62336-486-1, lire en ligne) Ouvrages illustrés Collectif (trad. de l'anglais), Le cyclisme en infographie, Paris, L'imprévu, 2016, 128 p. (ISBN 979-10-295-0405-1)Championnat du monde de cyclismeCyclisme aux jeux olympiques d'étéCyclisme (handisport)Cyclisme artistiqueCyclisme urbainUltracyclismeGlossaire du cyclismeVélo couchéVélomobileRessource relative à la santé : (en) Medical Subject Headings Site de l'Union cycliste internationaleSite de la Fédération française de cyclisme« SPORT (Disciplines) Le cyclisme : Le cyclisme féminin », sur le site de l'encyclopédie Universalis.fr (consulté le 24 juillet 2016). Portail du sport   Portail du cyclisme   Portail de la bicyclette"
sport;"Les défenseurs (ou arrières) sont des joueurs de football dont la tâche principale consiste à perturber, ou idéalement empêcher, le jeu d'attaque de l'équipe adverse.Le ballon peut être récupéré à la suite d'un duel gagné par un défenseur (action individuelle) ou par une déstabilisation des adversaires par une stratégie collective (interception, provocation d'un hors-jeu, etc.). Outre sa solidité et sa rigueur physique, les qualités requises pour un bon défenseur sont donc le sang-froid, la concentration et l'intelligence de jeu, notamment dans le placement. Pendant longtemps, on a pu estimer que les défenseurs étant des « destructeurs » de jeu, ils n'avaient pas à montrer de capacités techniques particulières. Ce n'est plus le cas actuellement car ils sont amenés à participer à des tâches offensives.Une ligne de défense est habituellement constituée de quatre joueurs, plus rarement trois ou cinq. La défense à quatre « typique » comprend deux arrières latéraux, qui évoluent chacun sur un côté, et deux arrières centraux, qualifiés selon leur rôle de stoppeur ou de libéro (poste de plus en plus rare dans le football moderne).Lorsque le football est introduit en France à la fin du XIXe siècle, les postes des joueurs sont directement traduits des noms anglais. Ainsi, les joueurs chargés de défendre, c'est-à-dire d'empêcher les joueurs de l'autre équipe de marquer, prennent le nom d'arrières, de l'anglais back. Dans le schéma classique en 2-3-5, adopté des années 1880 aux années 1920, les équipes jouent avec deux arrières : un arrière droit, placé à droite du terrain, et un arrière gauche, placé à gauche du terrain. À la fin des années 1920, à la suite d'une modification de la règle du hors-jeu en 1925, de nombreuses équipes adoptent une formation en 3-2-5, dite WM, avec une ligne de trois arrières contenant un arrière droit, un arrière centre et un arrière gauche.À partir des années 1950, un nouveau dispositif est majoritairement adopté par les équipes, le 4-2-4. Un joueur supplémentaire est ajouté dans la ligne d'arrières, qui se compose alors d'une ligne de quatre joueurs : deux arrières latéraux (droit et gauche) et deux arrières centre. Leur rôle est toujours de défendre, et dans la terminologie, le nom du rôle des arrières centre va prendre le pas sur le nom de leur poste. Ainsi, dans le langage courant, l'arrière centre devient progressivement un défenseur central tandis que les arrières droit et gauche, eux, gardent leur nom, bien que leur poste n'ait plus rien à voir avec les arrières droit et gauche initiaux. Le terme défenseur remplace alors le terme d'arrière.Dans les années 1960, un système plus défensif appelé catenaccio, de l'italien cadenas, est proposé par plusieurs équipes, avec un cinquième défenseur ajouté derrière la ligne des quatre défenseurs. Il prend le nom de libéro, de l'italien libre. Ce système défensif tombe en désuétude pendant les années 1990, mais le terme libéro est resté et peut désigner ensuite le rôle d'un défenseur central qui dirige la défense et effectue des montées sur le terrain, au contraire du stoppeur, rôle de défenseur central davantage axé sur le marquage des attaquants adverses.Depuis les années 1970, les équipes jouent la plupart du temps avec quatre défenseurs (deux arrières latéraux et deux défenseurs centraux), mais peuvent aussi adopter une tactique plus défensive avec cinq défenseurs (deux arrières latéraux et trois défenseurs centraux), voire plus rarement une tactique à trois défenseurs (trois défenseurs centraux). Position Les défenseurs centraux occupent l'axe de la défense. Ils sont au nombre de deux ou de trois suivant l'organisation de l'équipe, soit alignés ou positionnés de manière que l'un d'entre eux occupe une position plus basse sur le terrain, le libéro. Dans ce dernier cas, on distingue deux types de défenseurs centraux : le « stoppeur » et le « libéro ».Les défenseurs modernes ne sont désormais plus caractérisés par les appellations de ""libéro"" ou ""stoppeur"", ou alors par erreur. Les quatre défenseurs jouent alignés, quand leur équipe n'a pas le ballon.L'expression ""charnière centrale"" désigne l'association des joueurs positionnés sur le terrain comme défenseurs centraux. Rôles Le « stoppeur » est un joueur caractérisé par ses capacités à empêcher un avant de pointe adverse à approcher des buts de son gardien. Il use du tacle pour prendre le ballon dans les pieds de son adversaire, de son jeu de tête pour empêcher les centres et les passes longues des milieux vers les attaquants et plus généralement de son physique pour stopper son adversaire. Le stoppeur est le joueur spécifiquement chargé de neutraliser l'avant-centre.Pendant une grande partie du XXe siècle, la défense par marquage individuel était prépondérante : chaque défenseur, à l'exception du libéro, se voit attribuer un attaquant qu'il suivra partout afin de gêner son jeu. Bien qu'efficace et simple à appliquer, cette option tactique montre cependant des faiblesses. Si un défenseur est battu, ce qui a de très fortes chances d'arriver pendant un match, l'attaquant qu'il marquait se retrouve seul et bénéficie donc d'une grande liberté d'action. De plus, les défenseurs « au marquage » ne peuvent pas participer au reste du jeu, au risque de s'éloigner trop de leur adversaire attitré.Dorénavant, la grande majorité des équipes pratique une défense dite « en zone », beaucoup plus flexible que le marquage individuel. Chaque défenseur couvre une partie du terrain, et il défendra contre l'adversaire qui s'y trouve. La défense en zone des premiers temps était rigide, les joueurs traçant des lignes mentales pour délimiter des endroits du terrain où ils n'allaient pas. Peu à peu, les entraîneurs ont enseigné une défense en zone intégrant les avantages de la défense individuelle. Les défenseurs sont alors amenés à se déplacer en bloc en fonction de la position du ballon et des adversaires. L'arrière droit se retrouve au marquage du joueur le plus à droite sur une action particulière, le défenseur central droit sur le deuxième le plus à droite, et ainsi de suite. Bien que très efficace, la défense en zone est particulièrement difficile à pratiquer, car elle nécessite plus de coordination, de concentration, de lucidité et d'intuition du jeu que le marquage individuel. L'entraînement collectif d'une défense consiste donc à cultiver les automatismes. C'est pourquoi beaucoup d'entraîneurs, une fois qu'ils ont trouvé leurs quatre (ou trois ou cinq) défenseurs, rechignent à en changer.Dans le jeu offensif, les défenseurs ont en général un rôle limité. Même s'ils peuvent se poster très haut sur le terrain, ils ne montent que rarement à l'attaque. Le risque d'un contre de l'adversaire est toujours possible, et un défenseur ne peut se permettre de monter pour ne pas risquer de perturber et d'affaiblir l'organisation défensive de son équipe. Néanmoins, tactiquement, il arrive que l'on laisse le droit à certains défenseurs centraux de monter. C'était très souvent le cas avec les libéros qui étaient des milieux reconvertis et qui donc possédaient des qualités pour jouer haut sur le terrain. Physionomie Physiquement, les défenseurs centraux sont en général plus grands et costauds que les autres joueurs, puisque ce sont souvent leurs capacités physiques qui leur permettent de prendre le dessus sur leurs adversaires. Dans le football moderne, ils mesurent souvent plus de 1,85 m. La taille est souvent avantageuse pour intercepter les ballons dans le jeu aérien ou couper les trajectoires des centres venus des ailes ou celles des passes longues en profondeur. Néanmoins, ce sont souvent leurs capacités à se placer et à travailler avec leurs coéquipiers de défense qui priment. Derniers remparts avant le gardien, leurs erreurs sont souvent lourdes de conséquences. Position Les deux arrières sont typiquement positionnés respectivement à droite et à gauche de la défense centrale (qu'elle soit composée d'un défenseur, comme cela était le cas auparavant, ou de deux comme le veut la norme au XXIe siècle). Ce positionnement nécessite une grande polyvalence en raison des tâches défensives et offensives que doit remplir un latéral, pouvant l'amener, selon les phases de jeu, à jouer successivement dans des positions de défenseur, de milieu ou d'ailier. En phase défensive, le latéral est souvent positionné le long de la ligne de touche, au contact de l'attaquant adverse. Toujours en phase défensive, lorsque le jeu est à l'opposé, il peut également se positionner en libéro de la défense car il dispose, à ce moment-là et en principe, de la meilleure vision d'ensemble. En phase d'attaque, il peut enfin se muer en contre-attaquant. Cette grande variété des positions qui peuvent être occupées par un latéral au cours d'un match suppose une grande versatilité des joueurs, souvent capables d'occuper d'autres postes. Rôles Les arrières latéraux ont pour rôle de protéger les côtés du terrain. Ce sont en général des joueurs rapides dont le rôle principal est d'empêcher l'adversaire (bien souvent l'ailier adverse) de déborder, de centrer ou de rentrer vers l'intérieur du terrain. Leur rôle n'est pas tant de récupérer le ballon dans les pieds de l'adversaire que de le gêner et de perturber ses transmissions de balle. À l'inverse des défenseurs centraux, les latéraux doivent souvent défendre en mouvement, sur des appels en profondeur par exemple, et se retrouvent rarement en position de dernier défenseur.Les arrières latéraux peuvent aussi avoir un rôle offensif même si cela n'est pas systématique. Pendant de longues années, les arrières latéraux ne se contentaient que de défendre et de fermer leurs couloirs. Depuis l'arrivée du système 4-2-4 et de ses successeurs au début des années 1960, le rôle des latéraux s'est accru d'une dimension offensive, à un degré variable selon la philosophie de jeu de l'entraîneur : ils doivent soutenir leur ailier afin de déborder ou d'apporter le surnombre. Dans ce rôle, ils évoluent souvent comme un second ailier et doivent faire preuve de qualités de dribbles et de centres. Giacinto Facchetti, le latéral gauche de l'Inter de Milan double champion d'Europe en 1964 et 1965, a été le premier grand exemple de ce nouveau type. On peut aussi citer l'Allemand (de l'Ouest) Manfred Kaltz dans les années 1980 et surtout le Brésilien Roberto Carlos dans les années 1990 comme prototypes du latéral moderne, dont l'entente avec son ailier est devenue une des clés de l'animation offensive d'une équipe. Le profil typique de l'arrière latéral moderne est un joueur rapide, bon centreur, capable de déborder ou de se replier très rapidement. Leur contribution aux phases aussi bien offensives que défensives font que ces joueurs parcourent souvent la plus grande distance lors d'un match. Ils sont aussi souvent ceux qui touchent le plus de ballons lors d'un match grâce à leurs nombreuses interventions, qu'elles soient dans leur moitié de terrain ou dans la moitié adverse. Néanmoins, le juste équilibre entre rôle défensif et rôle offensif n'est jamais parfait ; il existe des latéraux qui sont très offensifs et d'autres qui sont très défensifs. Physionomie Certains arrières latéraux peuvent être petits ou grands sans que cela soit préjudiciable à leur jeu. En effet, un latéral plutôt petit sera avantagé s'il doit monter pour participer à l'attaque quand un plus grand aura une meilleure présence physique et sera plus à même de gêner un attaquant de l'équipe adverse. Position Le piston est un rôle très spécifique, proche de celui d'arrière latéral, et souvent occupé par des défenseurs latéraux. Sa position est entre celle d'un milieu latéral et d'un arrière latéral dans une formation avec 3 défenseurs centraux tel qu'un 3-5-2. Cette formation, revenue en vogue dans les années 2010, voit notamment plusieurs joueurs classés à ce poste, comme Raphaël Guerreiro (football, 1993) avec le Borussia Dortmund ou Achraf Hakimi positionné à droite avec qui il était associé avec Raphaël Guerreiro (football, 1993) positionné à gauche lors de la saison 2019-2020 avec le Borussia Dortmund Rôles Le piston est souvent assimilé à un arrière latéral plus offensif que la moyenne, avec une grosse pointe de vitesse, une bonne qualité de centre et une constante envie d'aller vers l'avant sans pour autant délaisser ses tâches défensives dans le couloir. Physionomie Proche de l'arrière latéral, il possède des qualités semblables, même si la densité physique y est moins privilégiée par rapport à la finesse et la rapidité. Position Poste d'arrière central auparavant très populaire, il est tombé en désuétude depuis la fin des années 1990, la défense centrale à 2 étant devenue beaucoup plus utilisée entre-temps. Ce poste ressemble aussi à celui d'arrière central à l'époque où il n'y avait que trois défenseurs dans les lignes arrières, même si ce terme est désormais synonyme de défenseur central.Le libéro est le joueur de champ qui joue le plus bas sur le terrain, et est déchargé de tout marquage individuel. Parce qu'il a une position plus reculée et une meilleure lecture du jeu adverse, il est considéré comme le « patron » de l'animation défensive, qui peut transmettre les consignes tactiques : son placement était en couverture du stoppeur dans les systèmes homme à homme des arrières sur les avants. Rôles Il peut ""remonter"", c'est-à-dire s'avancer vers la cible adverse, en vue d'opposer une pression sur les avants de l'autre équipe pour les pousser au risque d'être en position de hors-jeu. Il est aussi le dernier recours parmi les joueurs de champ, venant suppléer un de ses partenaires débordés, ou anticipant une trajectoire pour intercepter le ballon. C'est un poste qui requiert une grande intelligence de jeu afin de savoir quelle est la bonne action à réaliser à un moment donné. Dans certains dispositifs tactiques, notamment en Allemagne, dans les années 1970 et 80, le libéro par sa liberté sur le terrain, avait souvent un rôle offensif et participait au jeu d'attaque, en montant aux avant-postes. Franz Beckenbauer fut même à ce poste et à son époque, le véritable meneur de jeu de l'équipe d'Allemagne. Physionomie  Franz Beckenbauer (1972 et 1976) Matthias Sammer (1996) Fabio Cannavaro (2006) Billy Wright (1957) Giacinto Facchetti (1965) Bobby Moore (1970) Franz Beckenbauer (1974 et 1975) Franco Baresi (1989) Roberto Carlos (2002) Virgil van Dijk  (2019) Karl-Heinz Schnellinger (1962) Franz Beckenbauer (1966) Ruud Krol (1979) Andreas Brehme (1990) Paolo Maldini (1994 et 2003)Dispositifs tactiques en football Portail du football"
sport;La flamme olympique (en grec : Ολυμπιακή Φλόγα / Olympiakí Flóga), appelée aussi torche olympique ou flambeau olympique bien que le Comité international olympique fasse une distinction entre ces termes, est un symbole olympique. Elle fait partie du cérémonial des Jeux olympiques : allumage puis relais de la flamme olympique, le dernier relayeur faisant le tour du stade avant de rejoindre une vasque (ou « chaudron olympique ») qu'il embrase grâce à sa torche.La chorégraphie et les costumes de la cérémonie actuelle existent depuis les Jeux olympiques d'été de 1936. Ils s'inspirent de l'Antiquité : dans la Grèce antique, le feu sacré brûlait en permanence dans les sanctuaires, son allumage étant réalisé par un miroir parabolique, le skaphia, qui concentrait les rayons du soleil. Au sanctuaire d'Olympie qui accueillait les Jeux olympiques antiques, une flamme brûlait en permanence sur l'autel de l'Héraion, temple d'Héra. De même une flamme était placée au milieu des sites sportifs et du banquet offert dans le Prytanée aux vainqueurs des Jeux.La flamme olympique des Jeux olympiques modernes est allumée au cours d'une cérémonie par des femmes, jouant le rôle de prêtresses d'Héra, vêtues de tuniques similaires à celles portées par les Grecs de l'Antiquité. La cérémonie se déroule, plusieurs mois avant le début des Jeux, sur les ruines du temple d'Héra à Olympie, en Grèce, à l'aide de rayons du soleil concentrés par un miroir parabolique (par précaution, s'il n'y a pas de soleil le jour de cette cérémonie officielle, la flamme est allumée selon le procédé antique du miroir, plusieurs jours avant, un jour de soleil). Les prêtresses, autour de l'autel, invoquent Apollon. La flamme sacrée est alors placée dans une urne en céramique qui est transportée dans l'ancien stade d'Olympie au cours d'une procession qui passe devant un olivier sauvage dont un rameau, symbole de paix et récompense du vainqueur des Jeux, est coupé. La grande prêtresse allume la torche et la remet au premier relayeur. Plusieurs autres relayeurs la transportent jusqu'au Stade panathénaïque qui a accueilli les Jeux olympiques d'été de 1896. Le Comité olympique hellénique qui avait la responsabilité des relais jusqu'à ce stade passe lui-même le relais au Comité d'organisation des Jeux Olympiques (COJO) du pays hôte.Chaque participant (sélectionné en raison de son « accomplissement personnel » ou de sa contribution à la vie locale) porte ensuite le plus souvent à pied la torche ou le flambeau olympiques (ou leurs répliques) sur une courte distance et la remet à un autre porteur. Le relais de la flamme olympique prend fin lors de la cérémonie d'ouverture des jeux au cours de laquelle traditionnellement le dernier porteur, généralement un champion ou un jeune sportif du pays organisateur des jeux, allume de façon spectaculaire et originale avec sa torche une vasque ou un chaudron monumental, lequel brûle pendant toute la durée des jeux. Le choix de ce dernier porteur est en principe gardé secret jusqu'à la dernière minute.La flamme est finalement éteinte lors de la cérémonie de clôture finale.La flamme olympique a brûlé pour la première fois le 28 juillet 1928 lors des Jeux olympiques d'été de 1928, à Amsterdam. Il n'y avait pas encore de relais pour porter la torche.Sur une idée attribuée à Carl Diem et retenue par Adolf Hitler, inspirée des lampadédromies antiques, le premier relais avec la torche a eu lieu lors des Jeux olympiques d'été de 1936 à Berlin, dans le but de glorifier le Troisième Reich. Depuis, le relais et l'allumage de la flamme ont eu lieu à chaque olympiade.Le long passage de la flamme olympique est parfois l'occasion de manifestations politiques ou sociales dirigées contre le pays organisateur. Ainsi, le passage de la flamme en 2008 à Istanbul, Londres, Paris, San Francisco, etc. fut le prétexte de manifestations pour les droits de la personne concernant la controverse tibétaine. Similairement, le passage de la flamme olympique des jeux de 2010 à Vancouver fut le prétexte de manifestations pour les droits de la personne concernant la situation des peuples autochtones du Canada.La flamme des Jeux olympiques d'hiver a été allumée pour la première fois pour les Jeux olympiques d'hiver de 1952 à Oslo. À cette occasion, la flamme a été allumée dans la maison de Sondre Norheim, pionnier norvégien des sports d'hiver.Ainsi, depuis 1952, tous les 4 ans, puis tous les 2 ans, la flamme est allumée à Olympie grâce à l'énergie solaire puis transportée de ville en ville jusqu'à la cérémonie d'ouverture.Parfois les torches sont fabriquées pour chacun des relayeurs qui peuvent ensuite les racheter et les revendre.En raison de la pandémie de Covid-19, la cérémonie d'allumage de la flamme olympique est exceptionnellement tenue à huis-clos pour les Jeux olympiques d'été de 2020 prévus à Tokyo.À quelques rares occasions, la flamme olympique s'est éteinte de façon fortuite ou provoquée. Elle fut à chaque fois rallumée par une des lanternes contenant la « flamme-mère », une flamme « de secours » règlementaire issue d'Olympie :En 1976, à Montréal, un orage violent éteignit la flamme pendant le déroulement des Jeux, quelques jours après l'ouverture. La flamme fut d'abord rallumée par un organisateur présent, à l'aide d'un simple briquet. Elle fut ensuite éteinte volontairement afin d'être correctement rallumée par la flamme de secours règlementaire.En 2004, au Stade panathénaïque, un vent violent éteignit la flamme alors que Yánna Angelopoúlou-Daskaláki, membre du Comité d'organisation, tentait de l'allumer pour le départ nocturne d'un grand relais de 78 000 kilomètres.En 2008, à Paris au cours du parcours des jeux olympiques de Pékin, la torche fut volontairement éteinte à trois ou cinq reprises par les organisateurs chinois des Jeux olympiques, à cause de manifestations de protestation pour les droits de l'Homme en Chine. Par contre, la flamme est restée allumée à l'abri dans sa cage transportée par bus. Le relais de la flamme dut être écourté, plusieurs relayeurs délaissés et son transport vers le stade Charléty s'acheva en autobus, avant son départ pour San Francisco, l'étape suivante.Aux Jeux olympiques d'été de 2012, elle s'est éteinte accidentellement pendant l'un des relais qui la menaient à Londres. Lors de ces mêmes Jeux, elle a été éteinte de façon volontaire le lendemain de la cérémonie d'ouverture afin de transporter la vasque du centre du stade jusqu'à son emplacement définitif, au pied de l'un des virages où elle a été rallumée de façon réglementaire.Depuis l'origine du parcours de la flamme, certains pays organisateurs et participants ont innové en matière de moyen de transport :En 1976, la flamme olympique a été transformée en signal radio. Le signal a été transmis depuis Athènes jusqu'au Canada où il a servi à rallumer une autre flamme au moyen d'un rayon laser.En 1996, la flamme olympique a été embarquée à bord de la navette Columbia lors de la mission STS-78 pour son 1e vol dans l'espace.En 2000, la torche olympique a été transportée sous l'eau par des plongeurs, au voisinage de la Grande barrière de corail.La même année, d'autres étapes de transport originales ont été réalisées à l'aide d'un canoë amérindien, d'un chameau et d'un avion Concorde.En 2004, une course par relais de 78 jours fut organisée. La torche a parcouru 78 000 kilomètres, en passant entre les mains de 11 300 porteurs de torche.En 2008, la flamme olympique a été emmenée jusqu'au sommet de l'Everest, à 8 849 mètres d'altitude, protégée du manque d'oxygène par une lampe de mineur spéciale.En 2013, la flamme a fait un voyage à bord d'un vaisseau Soyouz jusqu'à la Station spatiale internationale (ISS).                                                                                                                                                                                                           La torche de 2012 créée par Edward Barber et Jay Osgerby, designers britanniques, présente « des lignes moyenâgeuses sous un alliance d'aluminium doré criblé de 8 000 cercles découpés au laser, symbolisant à la fois les anneaux olympiques et les 8 000 relayeurs qui se [succédèrent] feu au poing jusqu'à l'ouverture officielle des Jeux le 27 juillet ». La géométrie trilatérale anguleuse rappelle que la ville de Londres a organisé trois fois les Jeux (1908, 1948 et 2012). Elle a été testée dans la soufflerie du constructeur automobile BMW pour s'assurer qu'elle ne s'éteindrait pas, et cela à des conditions extrêmes : « douches d'eau et trombes de neige, amplitudes insolentes de températures (entre -5 et +30 °C), tourbillons de vents à 8 kilomètres à l'heure… La flamme a tenu bon, mais néanmoins montré quelques faiblesses en début de parcours, dans le Devon, en raison d'un brûleur défectueux ». Quoi qu'il en soit, il existe une « flamme mère » conservée à Athènes, et qui est transportée dans des lanternes désignées au cas où il faudrait rallumer la torche.                                                                                                            Serment olympiqueRelais de la flamme olympique 2008 Portail des Jeux olympiques
sport;""
sport;"Au football, le gardien de but, aussi familièrement appelé goal, est le joueur chargé de protéger le but de son équipe, de manière que le ballon n'en franchisse pas la ligne. Il a le privilège – dans la surface de réparation – de pouvoir utiliser toutes les parties du corps, contrairement aux joueurs de champ. Il peut aussi évoluer sur tout le terrain, alors avec les mêmes restrictions que les autres joueurs. S'il touche le ballon intentionnellement de la main ou du bras hors de sa surface de réparation, il est, comme le seraient tous les autres joueurs, coupable d'une faute d'anti-jeu et, passible d'une sanction (carton jaune ou rouge).Dernier rempart entre le ballon et le but de son équipe, le gardien occupe donc un poste essentiel. Historiquement, il joue de plus en plus en position de libéro (dernier défenseur, donc vers la fin de sa surface de réparation) quand tout danger est écarté.Il doit se distinguer de ses coéquipiers par un maillot différent.Outre ses qualités physiques (détente, réflexes), son caractère et son savoir-faire ont un rôle important : il doit pouvoir faire preuve d'autorité (crier pour réorganiser sa défense lors d'une offensive adverse, pour placer son mur avant un coup franc, etc.) et s'imposer (sorties aériennes au cours d'un corner). On a ainsi souvent l'image d'un gardien « bouillonnant », avec par exemple l'Allemand Oliver Kahn.Son rôle a évolué ces dernières années, pour accompagner un football de plus en plus rapide. Il doit savoir relancer proprement et précisément, des deux pieds si possible, en jeu court ou long, et être le premier contre-attaquant si possible. Son rôle a surtout changé au niveau de la technique balle au pied et de la précision de ses relances.Outre les chaussures à crampon et les protège-tibias communs à tous les joueurs, le gardien dispose de quelques accessoires spécifiques destinés à lui faciliter la tâche :les gants : indispensables, ils servent à la fois de protection aux mains contre les ballons de forte puissance (ou pour les dégagements aux poings) et de support permettant une meilleure prise de balle qu'à mains nues et par différents temps (souvent grâce à un revêtement en latex) ;les maillots et/ou pantalons/shorts rembourrés : ces vêtements sont rembourrés aux endroits les plus susceptibles de faire mal à la retombée d'un plongeon, à savoir les coudes, les genoux, le haut des cuisses, etc. ;des genouillères/coudières (matelassées ou double peau)  : pour atténuer les traumatismes des articulations. Elles sont notamment utilisées sur les terrains en mauvais état (cailloux ou mauvaises herbes). Les genouillères sont fortement recommandées pour les entraînements et en match chez les jeunes ;le casque : assez rare, il est néanmoins utilisé par le gardien tchèque Petr Čech depuis le 20 janvier 2007 en raison d'une fracture du crâne causée par un choc lors d'un match le 14 octobre 2006. Avec cet accident (qui touche également Carlo Cudicini qui avait remplacé Čech lors du match), des gardiens professionnels se sont déclarés non opposés au port systématique ou recommandé du casque (entre autres, l'Allemand Jens Lehmann). Ce casque en textile et mousse est homologué par la FIFA ;une casquette : fort utile quand le soleil est de face voire rasant et quand des projecteurs sont dirigés vers les buts. La casquette protège aussi d'une pluie forte, offrant un confort visuel et évitant d'avoir à s'essuyer le visage avec les gants matelassés peu commodes.Avoir de bons réflexes et de la détente ne garantissent pas l'inviolabilité du but (balle renvoyée en plein milieu d'adversaires déchaînés devant un but vide...). Au cours d'un match, le gardien peut être amené à réaliser certains gestes techniques, qu'ils lui soient ou non exclusivement réservés. Voici un aperçu des principaux gestes presque toujours exécutés par le gardien :pour saisir au mieux la balle, il est conseillé de donner une forme en W ou en triangle aux doigts autour du ballon (les pouces jouent un rôle prépondérant). Cela évite par exemple que le ballon, glissant ou trop puissant, ne passe à travers des mains écartées. Dans la mesure du possible, placer le corps sur la trajectoire du ballon, en plus des mains, est une assurance supplémentaire ;on peut aussi former une « niche » avec les bras et une partie du ventre pour y réceptionner sûrement la balle ;le gardien peut bien sûr recevoir une passe de ses coéquipiers. Cependant, si la passe est effectuée avec le pied, il ne peut pas se saisir du ballon avec les mains ;le gardien réalise aussi la traditionnelle « claquette » qui consiste à claquer le ballon en dehors de la cage à l'aide d'une main.Comme indiqué plus haut, le gardien de but est parfois amené à prendre le risque de sortir, que ce soit pour chercher une balle aérienne lors d'un corner ou pour chercher le ballon dans les pieds d'un attaquant isolé et trop avancé. Le risque de cette action, en plus de la possibilité du contournement du gardien par l'attaquant, est notamment physique. Voici quelques actions qui ont plus ou moins marqué le football.Le 5 septembre 1931, le jeune gardien du Celtic Glasgow, John Thomson, âgé de 22 ans seulement, percuta très violemment, lors d'un Old Firm (le derby opposant le Celtic aux Rangers), le genou de Sam English, attaquant des Rangers. Transporté à l'hôpital, il décéda quelques heures plus tard. Ce choc a définitivement marqué l'histoire du football écossais et des Old Firm.Le 8 juillet 1982 à Séville, au cours de la demi-finale de la Coupe du monde opposant la France à l'Allemagne de l'Ouest, Patrick Battiston se présente seul devant les buts de l'allemand Harald Schumacher. Ce dernier sort à la rencontre du milieu français et, après le tir de Battiston (un lob qui manque de peu le cadre), vient le percuter violemment au visage avec sa hanche. Battiston s'écroule, perd trois dents lors du choc et a une vertèbre cérébrale fracturée. Il reste sonné pendant plusieurs minutes. Le gardien n'est pas averti par l'arbitre néerlandais Charles Corver.René Higuita est devenu célèbre lors du match amical entre l'équipe d'Angleterre et la Colombie (0-0) à Wembley le 6 septembre 1995. Sur un centre-tir de l'anglais Jamie Redknapp et alors que le ballon se dirige vers son but, Higuita plonge vers l'avant et propulse le ballon hors de la surface à l'aide de ses talons. Cet arrêt spectaculaire fut baptisé le coup du scorpion.Le 14 octobre 2006, lors d'un match Chelsea contre Reading, Petr Čech, gardien tchèque, est blessé à la tempe lors d'une collision avec Stephen Hunt à la première minute de jeu. Sorti sur son côté droit, il se couche et saisit la balle mais sa tempe est violemment heurtée par le genou de son adversaire. Il ne rejouera qu'à partir du 20 janvier 2007, avec un casque destiné à empêcher une autre blessure qui pourrait être fatale. Au cours du même match que celui qui vit l'accident arrivé à Cech, c'est son remplaçant, Carlo Cudicini, qui subit un choc (avec Ibrahima Sonko) lors d'un corner, alors qu'il avait repoussé la balle du poing. Inconscient, il est évacué peu avant la fin du match. Ce sera finalement John Terry qui terminera le match en tant que gardien.Pierre Chayriguès est le premier gardien de but français de renom, il a inventé les dégagements aux poings, les sorties dans les pieds des adversaires et surtout le plongeon. Il est le premier gardien français à quitter sa ligne pour anticiper sur les adversaires.Le renommé gardien allemand Sepp Maier fut la première victime du Tchèque Antonín Panenka lors des tirs au but éliminatoires lors du Championnat d'Europe de football 1976. Ce joueur, dont le nom est resté dans l'histoire du football, choisit de tirer une balle en « feuille morte » dans le centre des buts de Maier, qui était déjà parti sur son côté gauche.Lors de la finale de la Ligue des Champions 2006 opposant Arsenal et le FC Barcelone, l'Allemand Jens Lehmann est le premier gardien de l'histoire de cette compétition à être sanctionné d'un carton rouge (dès la 19e minute). Il était sorti sur le joueur Samuel Eto'o et l'avait déséquilibré un peu avant l'entrée de la surface de réparation.Le gardien italien Dino Zoff est le plus vieux vainqueur de la Coupe du monde de football (à 40 ans en 1982). Il ne prendra sa retraite qu'à l'âge de 41 ans pour enchaîner avec une carrière (moins couronnée toutefois) d'entraîneur.Le gardien soviétique Lev Yachine, considéré comme le meilleur gardien du XXe siècle, est le seul gardien de but à avoir remporté le Ballon d'or (en 1963). Aujourd'hui, le trophée récompensant le meilleur gardien de la Coupe du monde de football porte son nom.L'Espagnol Ricardo Zamora, considéré comme le plus grand gardien de l'entre-deux-guerres. Il est le premier footballeur à devenir un phénomène médiatique, participant à des publicités, à des films. Sa renommée est telle que lorsque Staline apprit que Niceto Alcalá-Zamora avait été nommé à la tête de la Seconde République espagnole, il s'exclama : « Ah ! Le gardien de football ».Le Roumain Helmuth Duckadam a réussi l'exploit d'arrêter quatre tirs au but en finale de la Coupe d'Europe des Clubs Champions 1986. Il a permis au Steaua Bucarest de devenir la première équipe de l'Est à gagner ce trophée, aux dépens du FC Barcelone. Personne n'a jamais fait aussi bien que Duckadham dans une finale majeure.Le gardien brésilien Rogério Ceni, auteur de plus de 1200 matchs sous le maillot de São Paulo, fut connu pour tirer les coups francs et les penalties de son équipe. Étant très performant dans ce domaine, il a ainsi pu inscrire 129 buts au cours de sa carrière faisant de lui le gardien le plus prolifique de l'histoire du football.Le gardien espagnol Iker Casillas , est le premier gardien à avoir remporté le triplé historique Euro 2008-Mondial 2010-Euro 2012. De plus, il possède le deuxième meilleur ratio de clean sheet (match sans encaisser de but) derrière Lev Yachine. Il est également le recordman du nombre de trophées du Meilleur gardien de football de l'année (IFFHS) remportés avec 5 titres soit un de plus que Gianluigi Buffon, autre grand gardien de l'histoire du football. Par ailleurs, il est aussi le gardien type de l'équipe type de la FIFA, l'UEFA et des journaux L'Équipe et The Guardian de 2007 à 2012.Le gardien portugais Vítor Baía est le portier le plus titré de l'histoire du football avec 33 trophées et le second joueur le plus titré tout poste confondu (le premier étant le gallois Ryan Giggs avec 35 titres). Baía remporta avec le FC Porto et le FC Barcelone 11 championnats, 7 coupes nationales, 10 supercoupes, 1 Ligue des champions, 1 Coupe UEFA, 1 Coupe des Coupes, 1 Supercoupe de l'UEFA et 1 Coupe intercontinentale et fait partie des six joueurs ayant remporté la C1, la C2, la C3, la Coupe intercontinentale et la Supercoupe de l'UEFA au moins une fois. En Europe, il devance le néerlandais Edwin van der Sar (25 titres), le danois Peter Schmeichel (24 titres) et l'Allemand Oliver Kahn (23 titres).Le gardien allemand Oliver Kahn est le seul gardien à avoir remporté le prix de meilleur joueur de la coupe du monde. C'était en 2002.Le gardien costaricain Keylor Navas est le premier portier de la CONCACAF à avoir remporté la Ligue des Champions et est également le premier à l'avoir remporté 3 fois de suite.Le gardien égyptien Essam El-Hadari devient le plus vieux gardien à avoir joué un match de la Coupe du monde de football (45 ans en 2018).Le gardien allemand Lutz Pfannenstiel est le seul joueur de football à avoir joué sur tous les continents durant sa carrière riche de 31 clubs.En France, les carrières professionnelles de Dominique Baratelli, d'Albert Rust, de Jean-Luc Ettori et de Mickaël Landreau se sont étalées sur 19 saisons (Jean-Paul Bertrand-Demanes 18).Le classement établi par l'IFFHS recense les vingt meilleurs gardiens du XXe siècle. L'IFFHS détermine également chaque année le « meilleur gardien de football de l'année ». À ce jour, le meilleur gardien de l'année est le Belge Thibaut Courtois. Lev Yachine Gordon Banks Dino Zoff Sepp Maier Ricardo Zamora José Luis Chilavert Peter Schmeichel Peter Shilton František Plánička Amadeo Carrizo Gylmar dos Santos Neves Ladislao Mazurkiewicz Pat Jennings Ubaldo Fillol Christian Piot Rinat Dasaev Antonio Carbajal Silviu Lung Ray Clemence Walter Zenga Oliver Kahn Gordon Banks Jean-Marie Pfaff Gianluigi Buffon Dino Zoff Rinat Dasaev Peter Schmeichel Rüştü Reçber Fabien Barthez Lev Yachine, Ballon d'or en 1963 Ivo Viktor, 3e en 1976 Oliver Kahn, 3e en 2001 et 2002 Gianluigi Buffon, 2e en 2006 Manuel Neuer, 3e en 2014 Lev Yachine (1960, URSS) José Ángel Iribar (1964, Espagne) Dino Zoff (1968, Italie) Sepp Maier (1972, RFA) Ivo Viktor (1976, Tchécoslovaquie) Harald Schumacher (1980, RFA) Joël Bats (1984, France) Hans van Breukelen (1988, Pays-Bas) Peter Schmeichel (1992, Danemark) Andreas Köpke (1996, Allemagne) Fabien Barthez (2000, France) Antónios Nikopolídis (2004, Grèce) Iker Casillas (2008 et 2012, Espagne) Rui Patrício (2016, Portugal) Gianluigi Donnarumma (2020, Italie) Cayetano Saporiti (1916 et 1917, Uruguay) Marcos (1919, Brésil) Juan Legnazzi (es) (1920, Uruguay) Américo Tesoriere (1921 et 1925, Argentine) Júlio Kuntz Filho (1922, Brésil) Pedro Casella (1923, Uruguay) Andrés Mazali (1924, Uruguay) Fausto Batignani (1926, Uruguay) Octavio Díaz (1927, Argentine) Ángel Bossio (1929, Argentine) Enrique Ballestero (1935, Uruguay) Juan Honores (1939, Pérou) Juan Alberto Estrada (en) (1941, Argentine) Aníbal Paz (1942, Uruguay) Claudio Vacca (en) (1946, Argentine) Julio Cozzi (1947, Argentine) Barbosa (1949, Brésil) Adolfo Riquelme (es) (1953, Paraguay) Julio Musimessi (1955, Argentine) Julio Maceiras (1956, Uruguay) Rogelio Domínguez (1957, Argentine) Osvaldo Negri (1959, Argentine) Roberto Sosa (1959, Uruguay) Arturo López (1963, Bolivie) Ottorino Sartor (1975, Pérou) Roberto Fernández (1979, Paraguay) Rodolfo Rodríguez (1983, Uruguay) Eduardo Pereira (1987, Uruguay) Cláudio Taffarel (1989, Brésil) Sergio Goycochea (1991 et 1993, Argentine) Fernando Álvez (1995, Uruguay) Cláudio Taffarel (1997, Brésil) Dida (1999, Brésil) Óscar Córdoba (2001, Colombie) Júlio César (2004, Brésil) Doni (2007, Brésil) Fernando Muslera (2011, Uruguay) Claudio Bravo (2015 et 2016, Chili) Alisson (2019, Brésil) Emiliano Martinez (2021, Argentine) Ham Heung-chul (1956 et 1960, Corée du Sud) Yitzhak Vissoker (1964, Israël) Aziz Asli (1968, Iran) Nasser Hejazi (1972, Iran) Mansour Rashidi (1976, Iran) Jasem Bahman (1980, Koweït) Abdullah Al-Deayea (en) (1984 et 1988, Arabie Saoudite) Shigetatsu Matsunaga (1992, Japon) Mohamed Al-Deayea (1996, Arabie Saoudite) Yoshikatsu Kawaguchi (2000 et 2004, Japon) Noor Sabri (2007, Irak) Eiji Kawashima (2011, Japon) Mathew Ryan (2015, Australie) Sergio Goycochea : 2 matchs (1992, Argentine) Mogens Krogh : 2 matchs (1995, Danemark) Lars Høgh : 2 matchs (1995, Danemark) Dida : 5 matchs (1997, Brésil) et 4 matchs (2005, Brésil) Jorge Campos : 5 matchs (1999, Mexique) Ulrich Ramé : 3 matchs (2001, France) Mickaël Landreau : 1 match (2001, France) et 1 match (2003, France) Grégory Coupet : 1 match (2001, France) et 2 matchs (2003, France) Fabien Barthez : 2 matchs (2003, France) Marcos : 1 match (2005, Brésil) Julio César : 5 matchs (2009 et 2013, Brésil) Bernd Leno : 1 match (Allemagne, 2017) Marc-André ter Stegen : 4 matchs (Allemagne, 2017) Enrique Ballestero (1930, Uruguay) Gianpiero Combi (1934, Italie) Aldo Olivieri (1938, Italie) Roque Máspoli (1950, Uruguay) Anton Turek (1954, RFA) Gilmar (1958 et 1962, Brésil) Gordon Banks (1966, Angleterre) Félix (1970, Brésil)Sepp Maier (1974, RFA) Ubaldo Fillol (1978, Argentine) Dino Zoff (1982, Italie) Nery Pumpido (1986, Argentine) Bodo Illgner (1990, RFA) Cláudio Taffarel (1994, Brésil) Fabien Barthez (1998, France) Marcos (2002, Brésil) Gianluigi Buffon (2006, Italie) Iker Casillas (2010, Espagne) Manuel Neuer (2014, Allemagne) Hugo Lloris (2018, France) Mary Harvey (1991, États-Unis) Bente Nordby (1995, Norvège) Briana Scurry (1999, États-Unis) Silke Rottenberg (2003, Allemagne) Nadine Angerer (2007, Allemagne) Ayumi Kaihori (2011, Japon) Hope Solo (2015, États-Unis) Alyssa Naeher (2019, États-Unis)Dès la création de la Coupe du monde, un gardien est nommé meilleur gardien du tournoi, en étant incorporé dans l'équipe du tournoi :  Enrique Ballestero en 1930 Ricardo Zamora en 1934 František Plánička en 1938 Roque Máspoli en 1950 Gyula Grosics en 1954 Harry Gregg en 1958 Viliam Schrojf en 1962 Gordon Banks en 1966 Ladislao Mazurkiewicz en 1970 Jan Tomaszewski en 1974 Ubaldo Fillol en 1978 Dino Zoff en 1982 Harald Schumacher en 1986 Sergio Goycochea en 1990 Michel Preud'homme en 1994 Fabien Barthez en 1998 Oliver Kahn en 2002 Gianluigi Buffon en 2006 Iker Casillas en 2010 Manuel Neuer en 2014 Thibaut Courtois en 2018 Sepp Maier (RFA, le premier) Dino Zoff (Italie) Fabien Barthez (France, ainsi que des Confédérations) Iker Casillas (Espagne) Sepp Maier (RFA) Bodo Illgner (Allemagne) Fabien Barthez (France) Iker Casillas (Espagne) Manuel Neuer (Allemagne)Trophée Lev YachineLe trophée du meilleur gardien de but de la Coupe du monde de football a été créé en 1994. Il a été nommé en mémoire de l'ancien gardien soviétique (mort en 1990). Il est décerné par un jury constitué d'anciens joueurs et d'entraîneurs, membres du groupe d'étude technique de la FIFA. Il a été attribué à :  Michel Preud'homme en 1994 Fabien Barthez en 1998 Oliver Kahn en 2002 Gianluigi Buffon en 2006 Iker Casillas en 2010 Manuel Neuer en 2014 Thibaut Courtois en 2018Le football a fait l'objet de très nombreuses études scientifiques ou documentaires. Parmi celles-ci, certaines ont porté sur le gardien de but (plus rarement sur les gardiennes), son rôle dans l'équipe, ses capacités physiques (taille, poids, indice de masse corporelle, pourcentage de graisse corporelle, souplesse, réflexes, vitesse, agilité, puissance musculaire et cardiaque, capacité aérobie et anaérobie... ), capacités proprioceptives et psychologiques, son entraînement, son état mental dans certaines circonstances, face à un penalty par exemple, quand il doit tenter de deviner ce que son ""adversaire""  va faire ou tenter de le distraire,, la manière dont il prend ses décisions ou dont on peut évaluer ses décisions.  On a ainsi montré à partir d'un échantillon d'une cinquantaine de gardiens que chez les jeunes (14-20 ans), le poids et la taille du gardien sont plus élevés en moyenne que ceux des attaquants, et que leur taux de graisse corporelle est également plus élevé que chez tous les autres postes, alors que leur souplesse, agilité, puissance anaérobie, capacité anaérobie et valeurs de récupération sont comparables avec celles des autres joueurs. À partir de 17 ans leur temps de sprint sur 30 m était aussi le plus lent parmi les joueurs.Des études spécifiques ont porté sur ses gants. Récemment, des études ont montré que des maillots de gardien de couleurs fluo avaient tendance à grandir la taille des gardiens aux yeux des attaquants et d'attirer leur regard lors de la frappe, donc d'attirer par extension la balle sur eux.Dispositifs tactiques en football Portail du football"
sport;""
sport;"Le hockey est un sport dans lequel deux équipes jouent l'une contre l'autre en essayant de manœuvrer une balle ou un palet (rondelle) dans le but de l'adversaire à l'aide d'une crosse. Il existe de nombreux types de hockey comme le bandy, le hockey sur gazon, le hockey sur glace ou le rink hockey.Dans la plupart des pays du monde, le terme hockey désigne en soi le hockey sur gazon, tandis qu'au Canada, aux États-Unis, en Russie et dans la plupart des pays d'Europe de l'Est et du Nord, le terme désigne généralement le hockey sur glace.La première utilisation enregistrée du mot hockey se trouve dans un livre de 1773, Juvenile Sports and Pastimes, to Which Are Prefixed, Memoirs of the Author: Including a New Mode of Infant Education par Richard Johnson, dont le chapitre XI est intitulé Nouvelles améliorations sur le jeu du hockey. La croyance que le hockey est mentionné dans une proclamation de 1363 par le roi Édouard III d'Angleterre est basée sur des traductions modernes de la proclamation, qui est à l'origine en latin et interdit explicitement les jeux Pilam Manualem, Pedivam, & Bacularem: & ad Canibucam & Gallorum Pugnam,. L'historien et biographe anglais John Strype n'utilise pas le mot « hockey » lorsqu'il traduit la proclamation en 1720, traduisant plutôt Canibucam par Cambuck. Cela peut faire référence à une forme précoce de hockey ou à un jeu plus semblable au golf ou au croquet.Le mot hockey lui-même est d'origine inconnue. Une supposition est qu'il s'agit d'un dérivé de hoquet, un mot du moyen français pour une houlette. Les extrémités incurvées ou «crochues» des bâtons utilisés pour le hockey auraient en effet ressemblé à ces bâtons. Une autre supposition découle de l'utilisation connue de bouchons de liège à la place de boules en bois pour jouer au jeu. Les bouchons provenaient de barriques contenant de la bière ""hock"", également appelée ""hocky"".Les jeux joués avec des bâtons courbes et une balle peuvent trouver dans l'histoire de nombreuses cultures. En Égypte ancienne, des sculptures vieilles de 4000 ans présentent des équipes avec des bâtons et un projectile, le hurling précède 1272 av. J.-C. en Irlande, et il y a une représentation d'environ 600 av. J.-C. dans la Grèce antique, où le jeu peut avoir été appelé kerētízein ou (ηερητίζειν) parce qu'il était joué avec une corne ou un bâton en forme de corne (kéras, κέρας). En Mongolie-Intérieure, le peuple Daur joue au beikou, un jeu similaire au hockey sur gazon moderne, depuis environ 1000 ans.La plupart des preuves de jeux de type hockey au Moyen Âge se trouvent dans la législation concernant les sports et les jeux. Le Statut de Galway promulgué en Irlande en 1527 interdit certains types de jeux de balle, y compris les jeux utilisant des bâtons «à crochets» (écrits « hockie », semblables à «hooky»).Au XIXe siècle, les diverses formes et divisions des jeux historiques commencent à se fondre dans les sports individuels définis aujourd'hui. Des organisations dédiées à la codification des règles et règlements se forment, et des organismes nationaux et internationaux voient le jour pour gérer la concurrence nationale et internationale.Le bandy se joue avec un ballon sur une patinoire de la taille d'un terrain de football, généralement à l'extérieur, et avec de nombreuses règles similaires au football. Il est joué professionnellement en Russie et en Suède. Le sport est reconnu par le CIO ; son organe directeur international est la Fédération internationale de bandy.Le bandy prend ses racines en Angleterre au XIXe siècle, appelé à l'origine ""hockey sur glace"", avant de se répandre à d'autres pays européens vers 1900. Un sport russe similaire peut également être considéré comme un prédécesseur et en Russie, le bandy est parfois appelé «hockey russe». Des championnats du monde de Bandy ont lieu depuis 1957 et les championnats du monde féminin de bandy depuis 2004. Il existe des championnats nationaux de clubs dans de nombreux pays.Le hockey sur gazon se joue sur du gravier, du gazon naturel ou du gazon artificiel avec une petite balle dure d'environ 73 mm de diamètre. Le jeu est populaire parmi les hommes et les femmes dans de nombreuses régions du monde, en particulier en Europe, en Asie, en Australie, en Nouvelle-Zélande, en Afrique du Sud et en Argentine. Dans la plupart des pays, le jeu se joue entre des équipes non mixtes, bien qu'elles puissent l'être.L'organe directeur est la Fédération internationale de hockey sur gazon (FIH), qui compte 126 membres. Le hockey sur gazon masculin est pratiqué à chaque Jeux olympiques d'été depuis 1908, sauf en 1912 et 1924, tandis que le hockey sur gazon féminin est joué aux Jeux olympiques d'été depuis 1980.Les bâtons de hockey sur gazon modernes sont fabriqués à partir d'un composite de bois, de fibre de verre ou de fibre de carbone (parfois les deux) et sont en forme de ""J"", avec un crochet incurvé à l'extrémité de jeu, une surface plane du côté du jeu et une surface incurvée à l'arrière côté. Tous les bâtons sont droitiers - les bâtons gauchers ne sont pas autorisés.Alors que le hockey sur gazon dans sa forme actuelle est apparu au milieu du XVIIIe siècle en Angleterre, principalement dans les écoles, ce n'est que dans la première moitié du XIXe siècle qu'il s'est fermement établi. Le premier club est créé en 1849 à Blackheath dans le sud-est de Londres. Le hockey sur gazon est le sport national du Pakistan. C'était le sport national de l'Inde jusqu'à ce que le ministère de la Jeunesse et des Sports déclare en août 2012 que l'Inde n'a pas de sport national.Le hockey sur glace se joue entre deux équipes de patineurs sur une grande surface plane de glace, en utilisant un palet de 76,2 mm diamètre aussi appelé rondelle. Cette dernière est souvent gelée avant les parties de haut niveau pour réduire la quantité de rebond et de friction sur la glace. Le jeu est joué partout en Amérique du Nord, en Europe et à des degrés divers dans de nombreux autres pays du monde. C'est le sport le plus populaire au Canada, en Finlande, en Lettonie, en République tchèque et en Slovaquie. Le hockey sur glace est le sport national de la Lettonie et le sport national d'hiver du Canada.L'organe directeur du jeu international est la Fédération internationale de hockey sur glace (IIHF), composée de 77 membres. Le hockey sur glace masculin est joué aux Jeux olympiques d'hiver depuis 1924 et l'a été aux Jeux olympiques d'été de 1920. Le hockey sur glace féminin est ajouté aux Jeux olympiques d'hiver de 1998. La Ligue nationale de hockey (LNH) d'Amérique du Nord est la ligue professionnelle de hockey sur glace la plus célèbre, attirant les meilleurs joueurs de hockey sur glace du monde entier. Les règles de la LNH sont légèrement différentes de celles utilisées dans le hockey sur glace olympique dans de nombreuses catégories. Les règles internationales du hockey sur glace sont adoptées à partir des règles canadiennes au début des années 1900.Le sport contemporain s'est développé au Canada à partir d'influences européennes et indigènes. Celles-ci comprennent divers jeux de bâton et de balle similaires au hockey sur gazon, au bandy et à d'autres jeux où deux équipes poussent une balle ou un objet d'avant en arrière avec des bâtons. Celles-ci ont été jouées à l'extérieur sur glace sous le nom de «hockey» en Angleterre tout au long du XIXe siècle, et même plus tôt sous divers autres noms. Au Canada, il y a 24 rapports  de jeux de type hockey au XIXe siècle avant 1875 (cinq d'entre eux utilisant le nom «hockey»). Le premier match de hockey sur glace organisé et enregistré est joué à Montréal, le 3 mars 1875 avec plusieurs étudiants de l'Université McGill.Les crosses sont de longs bâtons en forme de ""L"" faits de bois, de graphite ou de composites avec une lame au bas qui peut reposer à plat sur la surface de jeu lorsque le bâton est tenu droit et peut légalement se courber dans les deux sens, que le joueur soit gaucher ou droitier.Le hockey en fauteuil roulant électrique, aussi appelé power hockey, est une forme de hockey pour les personnes nécessitant l'utilisation d'un fauteuil roulant électrique. Il est pratiqué en Europe et est régi par l'ICEWH (International Wheelchair and Amputies Sport).Le hockey sur luge ou para-hockey sur glace est une forme de hockey sur glace conçue pour les joueurs ayant un handicap physique affectant le bas du corps. Les joueurs s'assoient sur des traîneaux à double lame et utilisent deux bâtons. Chaque bâton a une lame à une extrémité et de petits pics à l'autre. Les joueurs utilisent les bâtons pour passer, manier et tirer le palet ainsi que propulser leurs traîneaux. Les règles sont très similaires aux règles de hockey sur glace de l'IIHF.Le roller in line hockey est une variante du roller hockey très similaire au hockey sur glace, dont il est dérivé. Il est joué par deux équipes, composées de quatre patineurs et d'un gardien de but, sur une patinoire sèche divisée en deux moitiés par une ligne médiane, avec un filet à chaque extrémité de la patinoire. Le match se joue en trois périodes de 15 minutes avec une variante de la règle du hors-jeu du hockey sur glace. L'organe directeur est l'IIHF, comme pour le hockey sur glace, mais certaines ligues et compétitions ne suivent pas les règlements de l'IIHF, en particulier USA Inline et Canada Inline.Le street-hockey est une variante de hockey sur glace et de roller hockey jouée toute l'année sur une surface dure (généralement de l'asphalte). Une balle est généralement utilisée à la place d'une rondelle et aucun équipement de protection n'est généralement porté.Les joueurs de street-hockey portent généralement des patins à roulettes ou des chaussures. Puisque le street-hockey est généralement joué de façon récréative (et non compétitive), les joueurs peuvent utiliser un ou deux filets en fonction de l'équipement et l'espace disponible.Le rink hockey, ou hockey sur patins, est un sport de hockey se jouant sur des patins à roulettes. Ce sport est pratiqué dans plus de soixante pays et a un public mondial. Le roller hockey est un sport de démonstration aux Jeux olympiques d'été de Barcelone de 1992.D'autres jeux dérivés du hockey ou de ses prédécesseurs existent :Air Hockey : se joue à l'intérieur avec une rondelle sur une table à coussin d'air.Le hockey de plage, une variante du hockey de rue, est un sport courant sur les plages du sud de la Californie.Le ballon-balai se joue sur une patinoire de hockey sur glace, mais avec une balle au lieu d'une rondelle et un «balai» (en fait un bâton avec un petit outil en plastique à l'extrémité) à la place du bâton de hockey sur glace. Au lieu des patins, des chaussures spéciales sont utilisées avec des semelles en caoutchouc très souples pour maximiser l'adhérence lors de la course.Le floorball est une forme de hockey jouée dans un gymnase ou dans une salle de sport. Une balle en plastique est utilisée et les bâtons ne mesurent qu'un mètre de long et sont fabriqués à partir de matériaux composites.Gena, ou Ganna, un sport de hockey sur gazon joué en Ethiopie.Le hockey en salle est une forme de hockey sur glace pratiquée dans un gymnase. Il utilise des bâtons avec des extrémités en mousse et une balle en mousse ou une rondelle en plastique.Le hurling et le camogie sont des jeux irlandais qui ont des ressemblances avec le hockey.Le hockey en salle est une variante du hockey sur gazon en salle.Le hockey d'antan est une forme simplifiée de hockey sur glace qui se joue sur de la glace naturellement gelée.La ringuette est une variante de hockey sur glace conçue pour les joueuses; il utilise un bâton droit et un anneau en caoutchouc à la place d'une rondelle. Les règles diffèrent de celles du hockey et ressemblent à un mélange de crosse et de basketball.Shinty est un jeu écossais qui se joue maintenant principalement dans les HighlandsLe hockey sur table se joue à l'intérieur sur une table.Le hockey subaquatique se joue avec une rondelle lestée au fond d'une piscine.Le hockey sous glace est similaire au hockey sous-marin, mais il se joue avec une rondelle flottante sur le dessous d'une piscine gelée.Bowlsby, Craig. 1913: The Year They Invented The Future of Hockey (2013)Ellison, Jenny.  and Jennifer Anderson, eds. Hockey: Challenging Canada’s Game (2018)Carl Gidén, Patrick Houda et Jean-Patrice Martel, On the Origin of Hockey, Createspace, 2014 (ISBN 9780993799808)Gruneau, Richard.  and David Whitson. Hockey Night in Canada: Sport, Identities, and Cultural Politics (1993),Hardy, Stephen and Andrew C. Holman. Hockey: A Global History (U of Illinois Press, 2018).  online review 600 ppHolzman, Morey,  and Joseph Nieforth. Deceptions and Doublecross: How The NHL Conquered Hockey (2002),McKinley, Michael. Putting A Roof on Winter: Hockey’s Rise from Sport Spectacle (2000), on Canada and U.S.Andrew Podnieks et Szymon Szemberg, World of hockey : celebrating a century of the IIHF, Fenn Publishing, 2007 (ISBN 9781551683072) Portail du sport"
sport;"Le volley-ball, ou volleyball, est un sport collectif opposant deux équipes de six joueurs séparées par un filet de hauteur variable selon le niveau, qui s'affrontent avec un ballon sur un terrain rectangulaire de 18 mètres de long sur 9 mètres de large. Le volley-ball se joue soit à l'intérieur d'un gymnase, soit sur un terrain sablé de dimensions différentes; on parlera alors de volley-ball de plage. Avec 260 millions de pratiquants licenciés au niveau mondial en 2018, il s'agit de l'un des sports les plus pratiqués dans le monde,.  Le volley-ball a été inventé le 9 février 1895 aux États-Unis par un professeur d’éducation physique des UCJG (YMCA), à Holyoke dans le Massachusetts, William G. Morgan (1870-1942), afin d'occuper les athlètes pendant l'hiver. C'est en s'inspirant à la fois du basket-ball et également du tennis, mais surtout du badminton (qui fournit le premier modèle de filet), qu'est née la « mintonette », le 2 décembre 1895. Un autre sport de salle, le basket-ball, a été inventé seulement dix miles (seize kilomètres) plus loin dans la ville de Springfield (Massachusetts), seulement quatre années auparavant. La mintonette se devait d'être un sport de salle moins violent que le basket-ball, pour les membres les plus âgés du Young Men's Christian Association, tout en exigeant toujours un minimum d'effort physique.Les premières règles, écrites par William G. Morgan, instauraient un filet de 1,98 m de hauteur, un terrain de 15,2 × 7,6 m, et un nombre de joueurs illimité. Un match était composé de neuf tours avec trois services pour chaque équipe dans chaque tour, avec un nombre de contacts avec la balle illimité pour chaque équipe avant son renvoi à l'adversaire. En cas d'une erreur de service, un deuxième essai était permis. Le fait de frapper la balle dans le filet était considéré comme une faute (avec la perte du point ou d'un temps-mort) sauf si cela se passait à la première tentative de service[réf. nécessaire].Après avoir observé ce sport, Alfred Halstead remarqua la nature de « volée » dans le jeu à son premier match d'exhibition en 1896. Joué à l'International YMCA Training School (aujourd'hui appelé Springfield College), le jeu fut rapidement connu sous le nom de volley-ball (il a été à l'origine orthographié en deux mots: « volley ball »). Les règles du volley-ball furent légèrement modifiées par l'International YMCA Training School et la propagation du jeu dans les UCJG (YMCA) différents,.La nature du premier ballon officiel utilisé au volley-ball est contestée ; plusieurs sources disent que Spalding a créé le premier ballon officiel en 1896. Les règles ont évolué au cours du temps. En 1916, l'attaque est introduite, et trois années plus tard, la règle des trois touches est instaurée. En 1917, le set passe de 21 à 25 points. En 1919, environ 16 000 ballons de volley-ball sont distribués par l'American Expeditionary Forces à leurs troupes et leurs alliés, ce qui suscite la croissance de volley-ball dans de nouveaux pays,.Le premier pays, en dehors des États-Unis, à adopter le volley-ball est le Canada en 1900. Une fédération internationale, la Fédération internationale de volley-ball (FIVB), est fondée à Paris en 1947, et le premier championnat du monde se tient en 1949 pour les hommes et en 1952 pour les femmes. Le sport est aujourd'hui populaire au Brésil, en Europe (les équipes d'Italie, des Pays-Bas et des pays de l'Europe de l'Est sont des équipes de premier plan depuis la fin des années 1980), en Russie, et dans d'autres pays incluant la Chine et le reste de l'Asie, aussi bien qu'aux États-Unis,.Le beach-volley, une variante du jeu joué sur le sable avec seulement deux joueurs par équipe, intègre la FIVB en 1987 et devient un sport olympique aux jeux de 1996,.L'histoire du volley-ball aux Jeux olympiques remonte aux Jeux olympiques d'été de 1924 à Paris, où il est pratiqué dans le cadre d'un événement de démonstration sportif américain. Après la fondation de la FIVB et de quelques confédérations continentales, on commence à envisager son inclusion officielle. En 1957, un tournoi spécial se tient à la 53e session du Comité international olympique à Sofia (Bulgarie), pour soutenir une telle demande. La compétition est un succès et le sport est officiellement inclus dans le programme pour les Jeux olympiques d'été de 1964.Le tournoi de volley-ball Olympique était à l'origine une compétition simple, dont le format est semblable à celui toujours employé dans la Coupe du Monde : toutes les équipes jouent l'une contre l'autre et sont ensuite classées par les victoires, la moyenne de set, et la moyenne de point. Un inconvénient de ce système de round-robin est que les vainqueurs de médaille pourraient être déterminés avant la fin des jeux, entraînant une perte d'audience pour le résultat des matches restants. Pour changer cette situation, la compétition fut composée de deux phases avec l'addition d'une « final round », un tournoi d'élimination se composant des quarts de finale, des demi-finales et des finales en 1972. Le nombre d'équipes impliquées au tournoi Olympique a grandi progressivement depuis 1964. Depuis 1996, les événements masculins et féminins comptent douze nations participantes. Chacune des cinq confédérations de volley-ball continentales a au moins une fédération nationale affiliée impliquée dans les Jeux olympiques.L'URSS remporte deux médailles d'or dans la compétition masculine en 1964 et 1968. Après avoir remporté le bronze en 1964 et l'argent en 1968, le Japon remporte finalement l'or chez les garçons en 1972. Chez les femmes, l'or est revenu aux Japonaises en 1964, qui ont reproduit la même performance en 1976. La même année, l'introduction d'une nouvelle adresse offensive a permis à la Pologne de gagner la compétition masculine sur les Soviétiques dans un match très serré de cinq sets. Depuis que les plus fortes équipes masculines de volley-ball appartenaient aux pays de l'Est, le boycott américain n'avait pas autant d'effet sur ces événements que sur la compétition féminine. L'URSS remporte son troisième titre olympique chez les hommes en battant en finale la Bulgarie trois sets à un (même résultat pour l'équipe féminine qui remporte son troisième titre). Avec le boycott de l'URSS aux Jeux olympiques de 1984 à Los Angeles, les États-Unis ont balayé le Brésil dans les finales pour la médaille d'or des hommes. L'Italie remporte sa première médaille (le bronze chez les garçons) en 1984.Aux jeux de 1988, Karch Kiraly et Steve Timmons conduisent les États-Unis à un deuxième titre olympique de suite après celui de 1984. En 1992, le Brésil contrarie les Pays-Bas et l'Italie dans la compétition masculine pour remporter sa première médaille d'or. Second, les Pays-Bas, médaillés d'argent chez les hommes en 1992, reviennent emmenés par leurs leaders, Ron Zwerver et Olof van der Meulen, aux jeux de 1996 pour un match de cinq sets en battant l'Italie et remportent le tournoi. Médaillée de bronze masculin en 1996, la Serbie-et-Monténégro (jouant en 1996 et 2000 en tant que République fédérale de Yougoslavie) bat la Russie en finale en 2000, remportant sa première médaille d'or. En 2004, le Brésil remporte un deuxième titre olympique masculin en battant l'Italie en finale. En 2008, après vingt années de disette, la sélection américaine remporte son troisième titre olympique aux dépens du Brésil. C'est son premier succès mondial depuis sa domination des années 1980.Le matériel nécessaire se compose des éléments suivants :2 poteaux2 mires (rouge/blanc) posées sur le filet aux 2 extrémités du terrain. On peut aussi dire dans le langage courant les mires mais le terme exact est antenne.1 filet1 ballon1 terrainLe volley-ball se pratique le plus souvent en salle. Le terrain a une forme rectangulaire de 18 mètres de longueur sur 9 mètres de largeur. Les lignes de délimitation sont à l'intérieur du terrain. Une ligne centrale s'étend sous le filet sur toute la largeur du terrain et sépare les deux camps. Une ligne d'attaque est peinte au sol dans chaque moitié de terrain, à trois mètres du filet ; elle est communément appelée « ligne des 3 mètres ».Les dimensions du terrain de volley ne varient jamais.2 camps de 9 × 9 m chacun que ce soit pour une équipe minime ou internationale.Voir la photo du terrain à droite.La taille du terrain est plus petite chez les poussin(ne)s (9-10 ans) ou les benjamin(ne)s (11-12 ans)Pour le public scolaire, la dimension du terrain et le nombre de joueurs sur le terrain diminuent.Chaque équipe occupe une moitié du terrain séparée de l'autre par un filet d'1 mètre de large et 9m de long, dont la bordure supérieure est placée à une hauteur variable en fonction de la catégorie d'âge des joueuses ou joueurs : Deux antennes (barres verticales également appelées « mires ») sont accrochées au filet à hauteur des limites du terrain (le filet est généralement plus large que ce dernier). Le ballon doit passer entre ces antennes lors des échanges (sauf exceptions) entre équipes lors des phases de jeu. Deux bandes blanches, accolées aux antennes, sont situées sur le filet à la verticale des lignes de côté.Le ballon de volley-ball est plus souple et plus léger que celui de football. Il doit avoir une circonférence comprise entre 65 et 67 cm, une masse comprise entre 260 et 280 g et une pression comprise entre 0,300 et 0,325 7 bar. Depuis 1978, pour diminuer le temps de jeu, trois ballons sont utilisés lors des rencontres internationales et nationales. Finies les pertes de temps pour le récupérer au service, puisque quatre ramasseurs de balles sont placés autour du terrain. Ce système a permis une diminution d'environ 20 % du temps de jeu.Les points sont marqués soit en faisant tomber le ballon sur le terrain de l'équipe adverse, soit quand l'adversaire commet une faute. La première équipe à atteindre 25 points (avec deux points d'écart minimum) gagne le set et la première équipe qui gagne trois sets gagne le match. Dans le cas d'un score à deux sets partout, les équipes jouent un 5e set décisif. La première équipe ayant marqué 15 points (avec deux points d'écart minimum) remporte le 5e set et le match.Chaque équipe peut toucher le ballon jusqu'à trois fois (en plus d'un éventuel contre) avant que le ballon ne retraverse le filet, et les contacts consécutifs doivent être faits par des joueurs différents. Le ballon est d'habitude joué avec les mains, bras ou les poings mais les joueurs ont le droit de toucher le ballon avec les pieds.Le but du volley-ball est d'éviter de faire tomber la balle dans son propre camp. Pour éviter cela, quatre problèmes majeurs se posent; il faut récupérer la balle, l'envoyer dans les trois mètres, la garder dans les trois mètres, et l'envoyer chez l'adversaire. De plus les deux équipes sont séparées par un filet, il s'agit donc de communiquer avec son équipe et perturber verbalement l'autre équipe. Enfin, pour agresser l'adversaire il est préférable de frapper la balle, c'est pourquoi la gestion de son corps et la gestion de la pression temporelle sont importantes.Une équipe de volley-ball se compose de six joueurs sur le terrain : trois avants et trois arrières. La position des joueurs est généralement désignée par un numéro de 1 à 6 : 1 étant le joueur arrière droit (défenseur droit ou serveur), 2 l'avant droit (attaquant), 3 l'avant centre (attaquant central), 4 l'avant gauche (attaquant), 5 l'arrière gauche (défenseur gauche), et 6 l'arrière centre (défenseur central), ce qui donne cette configuration :Cette numérotation correspond à l'ordre de service lors du début de set. Le poste 1 est occupé par le premier joueur à servir. Le poste 2 correspond au joueur qui servira en deuxième, et ainsi de suite.Chaque joueur est tenu d'être à sa position lors de la mise en jeu de la balle. En revanche, dès la balle mise en jeu, les joueurs sont libres de se déplacer sur le terrain à leur guise, mais les joueurs arrière ne peuvent attaquer qu'en dehors des trois mètres et ne peuvent contrer ; en fait, dès qu'ils sont dans la zone d'attaque, ils ne peuvent pas renvoyer une balle de l'autre côté si le contact a lieu avec la balle entièrement au-dessus du filet.Les joueurs de l'équipe qui récupèrent le service font une rotation dans le sens des aiguilles d'une montre. Le joueur P2 devient P1, etc.Les phases de jeu sont les suivantes :le service est effectué par le joueur en position 1. Il s'effectue depuis l'arrière du terrain. Le joueur se place derrière la ligne de fond du terrain et frappe la balle à une main afin de la faire retomber à l'intérieur du terrain adverse (la balle peut toucher le filet). Le joueur est autorisé à lancer la balle, sauter, et smasher la balle : on appelle cela un service smashé, de plus en plus répandu à haut niveau ; dans tous les cas, ses pieds doivent rester à l'extérieur du terrain lors de l'appel (ne pas mordre la ligne de fond). La réception du saut peut se faire à l'intérieur du terrain après la frappe de la balle ;L'équipe adverse reçoit la balle, elle a le droit de la toucher trois fois (le contre ne comptant pas comme touche) avant de la renvoyer à son tour de l'autre côté du terrain. Un joueur ne peut pas toucher deux fois consécutivement la balle (mais après un contre, la première touche peut être faite par le contreur).L'échange continue alors jusqu'à ce qu'une des deux équipes commette une faute. Les fautes les plus courantes sont les suivantes :'faute de position' : ne pas respecter le placement correct des joueurs à l'instant où le serveur frappe la balle. Le joueur 1 doit être derrière le 2, le 3 entre le 2 et le 4, le 6 entre le 5 et le 1 et derrière le 3, et le 5 derrière le 4.faute de rotation : lorsque le serveur qui a effectué le service n'est pas le bon. En effet, après récupération du point, les joueurs tournent une fois dans le sens des aiguilles d'une montre.balle in : laisser la balle toucher le sol à l'intérieur des limites de son terrain ;faute des quatre touches : effectuer une quatrième touche avant de retourner la balle dans le camp adverse ;balle out : envoyer la balle de telle sorte qu'elle touche le sol en dehors des limites du terrain, ou un joueur envoie une balle qui ensuite touche l'antenne (communément appelée mire) ;faute au filet : toucher la bande blanche qui marque la partie supérieure du filet entre les mires avec une quelconque partie du corps ou des vêtements (nouvelle règle applicable depuis 2009) (Nouveau changement depuis 2015, tout filet touché est faute). Il faut que la faute soit volontaire, toucher le fil en se retournant lorsque le joueur n'est pas dans une action n'est plus considéré comme une faute. De même, les nouvelles règles ne sifflent plus faute de fil lorsque les cheveux touchent le fil.faute de pénétration : mettre le pied dans le camp adverse ou pénétrer dans l'espace adverse en gênant l'adversaire (toucher le camp adverse avec la main n'est plus en soi une faute, nouvelle règle applicable depuis 2009). On peut faire pénétrer n'importe quel partie de son corps de l'autre côté à condition que le pied n'ait pas entièrement passé et que cela ne dérange pas le jeu adverse.faute des deux touches ou double touche : un même joueur touche la balle deux fois successivement (hors contre et hors première touche), néanmoins si la balle touche le filet après la premiere touche le joueur peut retoucher la balle ;faute de frappe d'attaque : un joueur arrière (position 1, 5 ou 6) attaque une balle plus haute que le filet en étant à l'intérieur de la zone d'attaque délimitée par la ligne des trois mètres (sauf s'il prend son appel derrière la ligne des 3 mètres), ou le Libéro effectue une passe à dix doigts à l'intérieur de la zone des trois mètres qui est attaquée par un joueur au-dessus du filet ;ballon tenu ou transport ou portée: la balle est touchée de manière inadéquate par un joueur (toutes les frappes doivent être franches, il est interdit d'attraper le ballon puis de le relancer) ;faute de service trop long : aucun service ne s'est fait de la part d'une équipe 8 secondes après que l'arbitre en a donné le signal (coup de sifflet ainsi que signe de la main).faute de contre : le joueur contre la balle directement au-dessus du filet sur le service adverse.faute de ligne : lorsqu'un joueur arrière saute à l'intérieur de la ligne des 3 mètres lorsqu'il attaque, ou lorsque le serveur touche à la ligne extérieure du terrain au moment du service.Un point est alors marqué et l'équipe ayant marqué ce point gagne (ou conserve) le service. Si cette équipe n'avait pas le service, les joueurs de cette équipe effectuent alors une rotation sur le terrain dans le sens des aiguilles d'une montre (le 1 prend la place du 6, qui prend la place du 5, etc.). Le service est effectué alors par le joueur passant du poste 2 au poste 1.Les règles du volley-ball ont été largement remaniées entre 1998 et 2000 et autorisent désormais de toucher la balle avec toutes les parties du corps : autrefois, seules les parties au-dessus de la ceinture étaient autorisées. Le principe de comptage des points a été modifié : les sets se jouaient auparavant en 15 points et une équipe ne marquait de point que si elle avait le service. Lors du service, le ballon ne devait pas toucher le filet. Enfin, un joueur particulier a été introduit : le libéro, spécialiste en défense, qui ne peut ni attaquer, ni contrer ni servir.Lors de certaines compétitions comme la ligue mondiale, il est possible de faire appel à l'arbitrage vidéo, appelé challenge (utilisé pour la première fois en 2012 lors de la Ligue mondiale). Les équipes ne peuvent demander le ""challenge vidéo"" qu'à la fin d'un échange et sur la dernière action. Les équipes gardent leur droit de demander un autre ""challenge vidéo"" si leur demande est justifiée, avec une limite de deux ""challenge vidéo"" non validés par set.Pour jouer au volley-ball, le joueur peut avoir des tennis qui tiennent très bien la cheville. Les chaussures sont à semelle plates pour mieux adhérer au terrain. La plupart des joueurs plus avancés se procurent des chevillères car les blessures à la cheville sont fréquentes. Certains portent du « strap » pour soutenir une blessure ou pour se durcir les doigts et renforcer la frappe. Les joueurs ont besoin de genouillères pour bien protéger leurs genoux au sol et d'un maillot d'équipe portant un numéro pouvant aller de 1 à 99. La grande majorité du temps tous bijoux doivent être retirés de sur le joueur pour des questions de sécurité. Si celui-ci ne peut pas les enlever, il devra alors les recouvrir d'un ruban adhésif.Ces deux joueurs attaquent à l'aile (en position 2 ou 4) et aux trois mètres en position 6. À l'arrière, le complet est, avec le libéro, prioritaire pour faire la réception.NB : les joueurs appelés « complets » sont parfois aussi ceux qui sont choisis pour être en opposition au passeur, dans les équipes sans pointu. À ce moment-là, ce joueur doit être capable de réceptionner, d'attaquer, de bloquer, de servir. Lorsqu'il reste en position arrière, il peut aussi attaquer derrière les trois mètres. Cette attaque est couramment appelée ""pipe"". Elle se distingue de l'attaque de l'opposite en étant une balle plus rapide et souvent moins haute que l'attaque classique des trois mètres.Aussi appelé « joueur à la technique » ou opposite, c'est le joueur placé à l'opposé du passeur. Il attaque généralement en poste 2 sauf en phase de réception lorsque lui-même est sur la position 4 afin d'éviter des rotations inutiles. Il attaque alors en 4. Lorsqu'il est arrière, le pointu est déchargé de la réception afin de pouvoir attaquer aux trois mètres (le plus souvent en poste 1). Pointus et complets peuvent être regroupés sous l'appellation « ailiers ».À un niveau moins élevé, le pointu devient un « relanceur », et, son rôle consiste à prendre énormément la réception, et, le cas échéant, à remplacer le passeur, puisqu'il lui reste opposé. Lorsque le passeur est en réception *, c'est au pointu de prendre la passe, c'est en quelque sort un second passeur. Les joueurs au centre sont ceux qui se placent en position 3 et 5. En poste 3, les centraux ont pour principale fonction d'attaquer en fixe (passe courte et rapide du passeur) et en décalée (passe rapide où le central se trouve à 2 m du passeur) ou, dans le cas d'une feinte d'attaque, de « fixer » (attirer) la défense adverse (le contre ou mur) pour l'empêcher d'aller contrer un attaquant ailier. Le central sort souvent sur les postes arrière pour laisser sa place au libéro (car c'est un poste très épuisant). Il sort après avoir servi en position 1 et rentre en 4.  Les centraux servent aussi a bloquer la balle de l'adversaire qui attaque.Selon le système tactique mis en place, il y a un passeur (système 5-1) ou deux passeurs (système 4-2). Dans un système 5-1 (5 attaquants et 1 passeur), le passeur se place après le service généralement :en 2 quand il est sur les positions avant (2, 3 ou 4),en 1 quand il est sur les positions arrière (1, 5 ou 6).C'est le système le plus utilisé à haut-niveau, il y a un seul passeur et les autres postes sont relativement spécialisés. Cela permet une grande incertitude sur les possibilités à l'attaque, et les permutations entre les joueurs permettent de les placer là où ils sont les plus performants. En revanche, ce système nécessite des déplacements importants du passeur. Dans un système 4-2 (4 attaquants et 2 passeurs), les deux passeurs sont sur des positions opposées (quand un passeur est devant, l'autre est derrière), ainsi il y a toujours un passeur devant, c'est lui qui fait la passe. La position de chaque passeur après le service fonctionne sur le même principe qu'avec un seul passeur.Quand les deux passeurs attaquent également (le passeur sur les positions avant attaque sur des passes faites par le passeur sur les positions arrière) on parle d'un système en « faux 4-2 », en « 4-2 amélioré », ou en 6-2 qui permet d'avoir toujours 3 attaquants sur les postes avant. Dans ce système de jeu, c'est le passeur arrière qui fait toujours la passe (sauf s'il défend). Les passeurs peuvent aussi réaliser des deuxièmes mains : lorsque la réception est très bien effectuée et qu'il est près du filet, il peut sauter et placer la balle directement de l'autre côté. À haut niveau, le passeur peut sauter sur chaque balle afin d'attirer lui aussi le bloc adverse, et donc pouvoir alléger le travail des ailiers qui n'attaqueront que face à un seul bloc.Le rôle du passeur est de mener le jeu et de distribuer les balles aux attaquants en fonction du jeu adverse; c'est lui qui est responsable en grande partie de l'efficacité du système offensif de son équipe. Les points fort d'un passeur sont la précision, la rapidité, et la bonne lecture de jeu. Le joueur libéro est différent des autres joueurs. Il a pour fonction de faire des réceptions de service, des défenses et des relances vers le passeur. Il se doit donc d'exceller dans la première touche de balle de l'équipe. L'échange de joueurs entre libéro et central par exemple dans ce cas particulier n'a pas besoin d'être noté par l'arbitre. Il ne peut rentrer que sur les trois positions de la ligne arrière et il lui est interdit de servir et d'attaquer le ballon lorsque celui-ci est entièrement au-dessus du plan haut du filet et même de participer au contre. De plus, s'il transmet le ballon à un attaquant en effectuant une touche haute (ou à 10  doigts) et qu'il se trouve à l'intérieur de la zone des 3 mètres, l'attaquant ne peut attaquer ou passer le ballon chez l'adversaire que si le ballon est redescendu sous le plan haut du filet matérialisé par une bande blanche. Il peut par contre transmettre le ballon en manchette sans restriction : celui-ci peut être attaqué normalement. Il est le point fort du secteur réception-défense. Pour un schéma tactique traditionnel, il rentre sur chacun des centraux, après leur position de service, et tourne sur les trois positions arrière. Sur le terrain, il porte un maillot différent des autres joueurs de son équipe. L'équipe peut nommer deux libéros mais il ne peut en avoir qu'un sur le terrain.C'est la touche de base. Le geste consiste à toucher la balle devant soi, au-dessus du front, avec la pulpe des doigts répartis de part et d'autre du ballon. Contrairement à ce qui est visible, le mouvement des bras sert essentiellement à amortir le contact avec la balle, mieux la maîtriser et assurer la direction de la passe. La puissance et la portée de la passe vient des appuis, de la poussée des jambes au moment du contact.Rappelons qu'un contact prolongé avec le ballon est interdit au volley. C'est le seul sport collectif ayant cette caractéristique.La passe étant plus précise que la manchette, elle est de plus en plus utilisée lors de la réception des services flottants (smashés ou non). On différencie  trois types de passes; premièrement la ""passe avant"" qui a été décrite précédemment, où le but est d'envoyer la balle vers l'avant (ou devant soi). Le deuxième type de passe est la passe arrière; le passeur est situé sous le ballon, on a une extension du tronc vers l'arrière. Les bras s'étendent dans le prolongement du tronc et les poignets sont tirés vers l'arrière. Ce type de passe permet d'envoyer la balle derrière soi. Enfin, il existe la passe en suspension, c'est-à-dire au moment du contact avec la balle le joueur se trouve en extension et en parfait équilibre. C'est la même technique que la passe avant mais elle permet d’accélérer le temps de jeu.C'est le mouvement utilisé lorsque la balle est basse ou rapide (réception de service, défense sur un smash). Le plan de contact s'effectue au niveau de l'intérieur des avant-bras, les bras étant tendus et étant plus bas que les épaules, formant un angle avec le buste, cet angle étant variable selon la distance par rapport au passeur. La poussée se fait au niveau des jambes, tout en gardant l'angle entre le buste et les bras. La manchette sert à amener la balle au passeur qui lui va faire une passe.Il est important de bien placer droit les bras, pour obtenir une manchette efficace (qui ne passera pas en ligne droite de l'autre côté du filet) et de les garder parallèles au sol[pas clair].Souvent spectaculaire, la Corse est le geste défensif utilisé en ultime recours, lorsque le défenseur se trouve en « crise de temps ». Elle consiste à plonger vers l'avant pour glisser sa main, paume plaquée au sol, sous le ballon au moment du rebond, en sorte que ce dernier ne touche pas le sol. L'origine de cette appellation pourrait venir du fait que la position du corps (allongé bras tendu) ressemble à la Corse et au Cap Corse; l'expression pourrait aussi être « une allusion au goût supposé des insulaires pour la sieste ».En Belgique francophone, le terme utilisé pour ce geste est « sprawl ». Ce mot vient de l'anglais sprawl qui signifie « s'étendre », « s'étaler ».Les anglophones utilisent eux le terme de pancake.C'est la touche d'attaque. Il s'agit d'un geste très technique, la balle étant frappée par le joueur alors qu'il est en suspension. Il existe différents types d'attaques : l'attaque puissante, qui cherche à forcer le contre, c'est-à-dire à empêcher toute récupération après celui-ci. Elle peut aussi provoquer le block-out, c'est-à-dire frapper les mains du contreur de manière à être déviée vers l'extérieur du terrain.l'attaque feintée, aussi appelée ""roulette"", qui consiste à simuler un smash puissant et à amortir son geste au dernier moment, afin de lober le bloc et de surprendre la défense ;l'attaque placée, en un point où la défense sera incapable de la reprendre ;Les attaques se font sur les postes avant, c'est-à-dire les postes 4, 2 et 3 (central) et sur les postes arrière, généralement sur les postes 1 et 6 (attaques aux trois mètres). Dans ce cas le joueur arrière prend son appel derrière la ligne des trois mètres sans la toucher pour attaquer le ballon au-dessus du filet.Le libéro ne peut attaquer ni dans la zone avant ni dans la zone arrière. Il est le seul joueur qui ne peut pas attaquer au-dessus du filet.Pour surprendre l'adversaire, le smash peut être remplacé par une feinte. Le ballon n'est pas frappé, mais poussé avec trois doigts. L'élan est exactement le même que celui du smash, et le geste permet de placer la balle précisément, en visant un trou. Le but est de prendre de vitesse la défense, qui s'attend à une attaque franche.Sur les balles difficiles, il est possible d'attaquer pieds au sol. Ce type d'attaque est très prévisible. Une alternative est de faire un ""hammershot"", en frappant la balle avec les deux poings joints au-dessus de la tête. Ce geste, puissant et précis, est de plus en plus utilisé dans les équipes masculines de haut niveau.C'est un mouvement défensif (en salle, il n'est pas compté parmi les trois touches autorisées. A contrario, il est compté comme une touche de balle en beach-volley). Son objectif est d'empêcher le ballon de passer dans son camp tout en faisant tomber le ballon dans le terrain adverse. Dans certaines options tactiques, il peut être « défensif » afin de conserver le ballon dans son camp en facilitant le jeu des défenseurs pour enchaîner sur une phase d'attaque. L'objectif ici est de couvrir une zone et de ralentir une attaque puissante pour faciliter la reconstruction. Le bloc peut aussi être offensif, c'est-à-dire essayer d’empêcher la balle de franchir le filet.  Lorsqu'un joueur fait un contre et que le ballon retombe moins d'un mètre derrière le filet, chez l'adversaire, ce contre est alors appelé une « équerre », c'est la plus belle façon de contrer. Le contre est le travail principal du centre. Le libéro ne peut ni contrer, ni effectuer une tentative de contre (c’est-à-dire sauter lors d'un contre seul ou à plusieurs sans intention réelle de contrer). Un joueur arrière ne peut pas contrer ou participer à un contre effectif. Enfin, un service ne peut être contré.C'est la touche d'engagement. C'est toujours le joueur en poste 1 qui sert. Le serveur doit se placer derrière la ligne de fond de son terrain (sans marcher dessus), où il le souhaite en profondeur, mais rester dans les limites du terrain en largeur. Le serveur dispose de 8 secondes après le coup de sifflet de l'arbitre pour effectuer son service. Il doit frapper la balle à une main. Avant la frappe, le ballon doit être lancé ou lâché (il ne peut être tenu). Si le ballon touche le filet, mais passe dans le camp adverse, le service est validé. Au moment du service, les joueurs doivent respecter leur position pour la rotation en cours sous peine de faute. Il n'y a pas d'erreur quant à la position du serveur par rapport aux autres joueurs lors du service. À l'exception du serveur, tous les joueurs doivent être entièrement à l'intérieur du terrain lors du service : ils ne doivent pas toucher le terrain à l'extérieur des lignes. Une fois la balle frappée, les joueurs peuvent sortir du terrain et changer de poste (les joueurs aux postes 1, 5 et les joueurs aux postes 2, 3 et 4 entre eux).Il existe plusieurs types de services : Le « service cuillère » C'est le service utilisé habituellement par les débutants. Il consiste à prendre le ballon de la main gauche (pour un droitier), de tendre ce bras en avant à hauteur du bassin, lancer légèrement le ballon en hauteur et de le frapper par-dessous avec la main droite pour le faire « voler » en avant. Le service flottant Ce type de service ne nécessite pas l'utilisation des jambes. Le joueur lance sa balle en hauteur et la frappe en utilisant le geste caractéristique de l'attaque, sans toutefois rabattre sa main totalement. Le geste est arrêté au moment précis de l'impact entre ballon et main ferme. Le joueur peut également effectuer un contre poids avec son corps pour augmenter la puissance de la frappe. La balle suivra une trajectoire flottante qui rend incertain l'endroit précis où la balle est censée toucher le sol. Ce flottement met donc le réceptionneur en difficulté. Le service smashé Le service smashé est le type de service pratiqué le plus fréquemment par les professionnels. Ce service nécessite l'utilisation des membres inférieurs. Pour ce faire, le joueur doit se placer un peu après la limite du terrain, lancer très haut son ballon et effectuer une petite course d'élan (même course que celle de l'attaque) pour frapper sa balle lors de la suspension. Ce type de service très puissant nécessite une position de réception parfaite, néanmoins il est assez simple à réceptionner car la balle tournante et très rapide rebondit sur les bras du réceptionneur, qui n'a pas besoin de faire d'efforts pour la ramener en l'air. Le service smashé flottant (ou service « sauté » flottant) Ce service est très utilisé par les professionnelles féminines, mais également de plus en plus par les masculins. Il consiste à prendre une course d'élan (moins "
sport;""
sport;"Les Jeux olympiques d'été de 1936, Jeux de la XIe olympiade de l'ère moderne, sont célébrés à Berlin, en Allemagne du 1er au 16 août 1936. La capitale allemande est désignée pour la seconde fois comme pays organisateur, mais les Jeux olympiques de 1916 ont été annulés en raison de la Première Guerre mondiale.Dans le contexte du moment, les JO de Berlin prennent vite une signification très politique, même si personne ne peut encore prévoir les changements politiques qui vont survenir en Allemagne quand, en 1931, le CIO confie à Berlin et à la République de Weimar l'organisation des jeux. Après l'instauration du régime nazi en 1933, plusieurs pays demandent le boycott de ces Jeux olympiques et organisent des jeux alternatifs, les Olympiades populaires, à Barcelone, dont le déclenchement de la guerre d'Espagne la veille empêchent l'inauguration. Les Jeux de Berlin se déroulent dans une atmosphère de xénophobie et d'antisémitisme, Adolf Hitler voulant se servir de cet événement pour faire la propagande du nazisme et la promotion de l'idéologie de la supériorité de la race aryenne, notamment à travers le documentaire Les Dieux du stade de Leni Riefenstahl. Ces jeux sont souvent cités comme exemple de « blanchiment par le sport » organisé par un gouvernement autoritaire et belliqueux.Sur les 49 nations et 3 967 athlètes (dont 335 femmes) qui prennent part à 129 épreuves dans 19 sports, l'Allemagne est le pays le plus médaillé.Dans le contexte particulier des « Jeux nazis », les quatre médailles d'or remportées par l'athlète noir américain  Jesse Owens en sprint et saut en longueur représentent un important symbole dans l'histoire des Jeux olympiques modernes. Mais l'athlète le plus médaillé est le gymnaste allemand Konrad Frey (six médailles dont trois d'or). Au tableau des médailles, les athlètes allemands imposeront leur large domination tout au long des Jeux, remportant 89 médailles dont 33 d'or, devant les États-Unis, avec 56 médailles dont 24 d'or.Les Jeux olympiques étaient déjà attribués à l’Allemagne en 1916, mais ont été annulés à cause de la Première Guerre mondiale. Incriminée et tenue responsable pour le déclenchement du conflit mondial, l’Allemagne est suspendue des Jeux de 1920 et de 1924. Cependant, après un long processus de négociation, les autorités allemandes ont réussi à faire réintégrer leur pays pour participer aux Jeux olympiques d'été de 1928 et postulent pour accueillir les Jeux d’été de 1936. Leur argument est que les Jeux ont déjà été attribués à l’Allemagne dans le passé — en 1916 — donc, les infrastructures sont déjà prêtes, et la candidature est présentée comme un moyen de redorer son blason.Malgré les nombreuses confusions, l’événement sportif mondial est attribué au régime de Weimar, donc avant l'arrivée au pouvoir des nazis. En 1933, avec l’accession au pouvoir d’Adolf Hitler, la capacité d’organiser un tel événement est sérieusement remise en question, notamment en raison de l'idéologie raciste et discriminatoire du parti nazi. En fait, le régime nazi a aggravé la situation lorsqu’il suggéra l’exclusion des Juifs des Jeux de Berlin.À la surprise générale, malgré ses propos houleux et haineux envers les Juifs, Hitler approuve la réception des Jeux et promet de tout faire pour la réussite de l’événement. Le dictateur allemand clame publiquement la promotion des relations entre les Nations et le développement du sport chez les jeunes ; cependant, son but ultime est la prospérité acquise des atouts politiques non négligeables de l’organisation des Jeux olympiques[source insuffisante].Le Comité international olympique confie l'organisation des Jeux olympiques d'été de 1936 à la ville de Berlin, au cours de la 29e session du 26 avril 1931, à Barcelone. La capitale allemande l'emporte face à la candidature de Barcelone par 43 voix à 16. Alexandrie (Égypte), Budapest (Hongrie), Buenos Aires (Argentine), Cologne, Francfort et Nuremberg (Allemagne), Dublin (Irlande), Helsinki (Finlande), Lausanne (Suisse), Rio de Janeiro (Brésil) et Rome (Italie) sont candidates.Le contexte socio-politique de l’Allemagne a drastiquement changé en 1933 avec la montée de l’extrême droite et l’accès au pouvoir d’Adolf Hitler. Arrivé au pouvoir dans un contexte de crise économique et politique, Hitler tente d’exclure tous les peuples distincts qui ne cadrent pas dans son idéal de race aryenne. Pour y arriver, Hitler décide de lancer un programme de réarmement qui mène à une politique d’agression. Dès 1934, il établit un régime totalitaire et élimine tous les autres partis politiques, le parti nazi est le seul accepté. La dictature totalitaire d’Hitler module l’Allemagne en État autoritaire et centralisé autour du parti nazi. Il abolit le commerce étranger dans le but de restreindre l’Allemagne à l’autarcie et à l’autosuffisance. Pour atteindre ses objectifs, Hitler crée la police militaire (SS), une police nazie (SA) et une police secrète d'État (Gestapo). La politique allemande devient très vite raciste et antisémite. Selon Hitler, c’est l’idée de race qui domine l’Histoire, et les Jeux olympiques de 1936 représentaient une opportunité incroyable pour montrer la domination et la suprématie de la race aryenne[source insuffisante].À côté de l'aspect sportif, les JO de Berlin eurent une signification politique très importante dans le cadre de la montée des tensions au sein de l'Europe. Le souvenir de ces jeux reste lui aussi en très large partie politique : il reste un cas d'école exemplaire de la confusion du sport et de la politique et de la propagande par le sport.Alors que le choix de la ville de Berlin date de 1931, l'arrivée au pouvoir du parti nazi en 1933 et la montée consécutive des tensions internationales va donner à ces Jeux une dimension fortement politique.« En 1936, les organisations juives, le mouvement ouvrier international et plusieurs associations démocratiques et humanitaires appelèrent à boycotter les Jeux du Reich. » Les États-Unis menacent l'Allemagne de boycott, mais ne mettent pas leur menace à exécution.Les arguments des partisans du boycott sont les suivants :l’Allemagne nazie discrimine les Juifs, principal motif du boycott ou de la relocalisation des Jeux ;la discrimination n’est guère compatible avec l’esprit sportif ;les Jeux demeurent une plateforme pour le régime nazi afin de promouvoir la supériorité de la race aryenne ;une participation aux Jeux sous-entendrait une adhésion aux persécutions et au racisme.Les adhérents de la participation aux Jeux olympiques, dont le Comité International Olympique, défendent les arguments suivants :les aspects sportif et politique doivent être dissociés ;il n’y a pas de discrimination, donc il n’y a pas besoin de boycott ;il n’y a pas de discrimination seulement en Allemagne, il ne faut pas associer le fléau social à l’Allemagne uniquement. Les États-Unis, opposants au régime nazi et favorable au boycott, menaient eux-mêmes des ségrégations raciales contre leur propre population ;les Jeux olympiques sont porteurs de paix, de tolérance, d’égalité et de fraternité.Les pays qui décident le boycott organisent des « contre-Jeux populaires » parallèles à Barcelone. Les Jeux populaires de Barcelone abandonnent rapidement ses préoccupations initiales et deviennent une alternative aux Jeux de Berlin de 1936 et le slogan de la protestation contre l’organisation de l’événement sportif par l’Allemagne fasciste. Initialement, sa raison d’existence était d’enrayer la distinction entre la classe bourgeoise et la classe ouvrière dans le milieu du sport. Toutefois, les Jeux populaires de Barcelone avaient leurs propres caractéristiques définies et en contradiction avec certaines règles des Jeux olympiques :il n’y a pas de place pour la commercialisation et la militarisationon prône la participation des athlètes des nations non souveraines et des athlètes italiens et allemands exiléstous les athlètes ont la chance de concourir : des athlètes de haut niveau, des athlètes intermédiaires et des amateurson sacralise la participation des femmesil n’y a pas que des tournois sportifs, on assiste à des compétitions de peinture, de sculpture, de photographie, de littérature, de design ; on met aussi une emphase sur les activités folkloriques et intellectuelles.En tout, vingt-trois nations sont représentées : la Suède, la Suisse, la Hongrie, la Palestine, le Maroc, la Norvège, la Grande Bretagne, la Belgique, le Canada, les États-Unis, la France, la Grèce, le Portugal, les Pays-Bas, l’Algérie, le Danemark, la Tchécoslovaquie, les Émigrés juifs, l’Alsace, l’Espagne, les pays Basques, la Galice et la Catalogne. Les pays les mieux représentés sont la France (1 500 sportifs), la Suisse (deux cents), les Pays-Bas, la Belgique et la Grande Bretagne (cinquante représentants).Cependant, en plus des difficultés d'organisation, le déclenchement de la Guerre d'Espagne compromet définitivement le projet.Pour le régime du IIIe Reich, ces jeux devaient être l'occasion de prouver sa puissance et la « suprématie de la race aryenne », selon la terminologie nazie.Sur le plan intérieur, les Jeux furent utilisés par le régime nazi pour renforcer l'adhésion populaire envers lui. Ils servirent de support de propagande, dont l'expression la plus connue est le film Les Dieux du stade de Leni Riefenstahl. Ce film en soi est cependant plus un documentaire, sorte d'ancêtre des retransmissions télévisées actuelles (à côté de séquences esthétisantes comme les introductions ou celles dévolues à la gymnastique, à l'escrime et aux plongeons) : Riefenstahl montre en détail les exploits d'Owens, mais aussi, de manière plus étonnante, des défaites allemandes. Tout aussi étonnant est le fait que l'hymne le plus entendu à l'écran est l'hymne des États-Unis et non celui de l'Allemagne. Seule concession réelle à l'idéologie : les athlètes français, britanniques ou du Commonwealth sont peu représentés (malgré la victoire française en cyclisme montrée en détail, la participation française ne fut pas à la hauteur des espérances, en dépit de 7 médailles. Elle revint les mains vides sur les disciplines majeures comme l'athlétisme, la gymnastique et la natation).Au niveau de la politique extérieure, les Jeux olympiques contribuèrent à faire passer momentanément Hitler pour un pacifiste et de rassurer l'Europe quant à ses intentions belliqueuses.Hitler a le soutien de Pierre de Coubertin qui bien qu'ayant démissionné du CIO en 1925, participa activement à l'organisation de ces jeux. Il en fit le discours de clôture en prononçant ces mots : « Que le peuple allemand et son chef soient remerciés pour ce qu’ils viennent d’accomplir... ». Coubertin admirait « intensément » Hitler, et à la question qu'on lui posait de ce soutien, il répondait : « Comment voudriez-vous que je répudie la célébration de la XIe Olympiade ? Puisque aussi bien cette glorification du régime nazi a été le choc émotionnel qui a permis le développement qu’ils ont connu ». Selon Coubertin, Hitler a ainsi beaucoup fait pour le retentissement des Jeux olympiques.Les Jeux olympiques d'été de 1936 furent organisés par le Deutscher Reichsbund für Leibesübungen (DRL), le Bureau de Sports du Reich. Hans von Tschammer und Osten, le Reichssportführer ou chef du DRL, a nommé Theodor Lewald président et Carl Diem secrétaire général du Comité Organisateur des Jeux Olympiques à Berlin. Diem et Lewald introduisent des innovations originales, comme la cérémonie de la flamme olympique. Pour cacher les traces de l'antisémitisme nazi les panneaux antisémites furent provisoirement enlevés et les journaux mirent un bémol à leurs attaques. De cette façon, le régime exploita les Jeux olympiques pour fournir aux spectateurs et aux journalistes étrangers une fausse image d’une Allemagne pacifique et tolérante.Les Jeux olympiques de 1936 à Berlin étaient le parfait moyen de propagande pour Adolf Hitler. En effet, il a utilisé ces JO comme une vitrine pour mettre en avant ses idéologies. L’objectif des Jeux de Berlin était de refléter l’image de l’Allemagne nazie à travers le monde entier. L’organisation de cet événement avait pour but de célébrer la gloire d’Hitler et du nazisme en Allemagne. Dans le cadre des JO, Hitler a utilisé différentes formes de propagande : notamment grâce au cinéma, avec les réalisations cinématographiques de Leni Riefenstahl. Toute la propagande des JO, et plus généralement du nazisme, était organisée par Joseph Goebbels. Il organise toute une mise en scène afin de montrer la supériorité de la race aryenne. Toute la publicité autour des JO a permis d’attirer près de 3 millions de spectateurs : c’est une véritable réussite pour les nazis. Leurs idées et leur autorité sont propagées à travers la manifestation sportive. Les nazis ont essayé de faire oublier leur programme antisémite à travers les Jeux Olympiques afin de diffuser une fausse image de l’Allemagne nazie. Le sport a donné l’opportunité aux nazis d’afficher tous les moyens de propagande, et les Jeux de 1936 ont permis aux nazis de montrer la supériorité de la « race aryenne » et de mettre en avant leurs qualités physiques. Afin de ne pas être identifiées comme un régime nazi, les affiches de propagandes nazies ont été retirées durant la période des Jeux Olympiques. Les nazis ont donc présenté une fausse image d’une Allemagne pacifique. La propagande autour des JO a continué après les 1936 puisqu’en 1938, Leni Riefenstahl a sorti un documentaire pour mettre en avant le parti nazi lors des JO. Village olympique Les athlètes furent logés au village olympique de Dallgow-Döberitz, dont s'occupait l'officier allemand Wolfgang Fürstner. Ils eurent à disposition une salle de cinéma, de théâtre, de music-hall et une bibliothèque. Chaque chambre disposa d'une salle de bain et du chauffage central. Stade olympique Le monumental Stade olympique de Berlin d'une capacité de 100 000 places fut construit par l'architecte Werner March. Un virage entier est réservé aux SA. Le stade a accueilli les cérémonies d'ouverture et de clôture, les épreuves d'athlétisme, d'équitation et les finales de handball à onze et de football. Le baseball y fut en démonstration. Autres sites Stade nautique : natation, plongeon, water poloStade May Field : polo, équitationThéâtre de plein-air Dietrich Eckart : gymnastiqueStade de Hockey : hockey sur gazonCentre de tennis : basket-ball, escrimeSalle Cupola : escrimeCentre nautique de Grünau : aviron, canoë-kayakDeutschlandhalle : boxe, lutte, haltérophilieVélodrome : cyclisme sur pisteStand de tir olympique : tirChamp de manœuvres de Döberitz : pentathlon moderneLes régates de voile furent disputées dans la ville de KielLa cérémonie d'ouverture, orchestrée par Rudolf Laban, se déroula le 1er août 1936 devant les 100 000 spectateurs du Stade olympique de Berlin qui assistèrent dans un premier temps au défilé des brigades des Jeunesses hitlériennes. Alors que la Marche d’hommage du compositeur allemand Richard Wagner était entonné par l’orchestre, le chancelier Adolf Hitler pénétra dans le stade sous le salut nazi des spectateurs et rejoignit dans les tribunes le comte Henri de Baillet-Latour, président du Comité international olympique, ainsi que les membres du comité d’organisation.Un court enregistrement du baron Pierre de Coubertin fut diffusé dans l’enceinte :« L'important aux Jeux olympiques n'est pas d'y gagner, mais d'y prendre part ; car l'essentiel dans la vie n'est pas tant de conquérir que de bien lutter. »Peu après, Adolf Hitler déclara officiellement ouverts les Jeux olympiques de Berlin, sans autre discours. La flamme entra dans le stade après un relais de plus de 3 000 athlètes. Le dernier porteur du flambeau fut l’athlète allemand Fritz Schilgen, qui alluma la vasque olympique. Pour la première fois, la flamme olympique, à l’instigation du professeur Carl Diem, était introduite dans la cérémonie d'ouverture des Jeux. Ce fut aussi le premier grand événement retransmis en direct via la télévision.Les invitations sont lancées par le gouvernement présidé par Adolf Hitler par le biais du comité olympique allemand. L'Espagne, qui entame sa guerre civile, déclare forfait le matin même de la cérémonie d'ouverture. Finalement, 49 nations participent à ces jeux de Berlin. Cinq d'entre elles apparaissent pour la première fois : l'Afghanistan, les Bermudes, la Bolivie, le Costa Rica et le Liechtenstein.L'Allemagne et les États-Unis disposent du plus gros contingent d'athlètes avec respectivement 348 et 310 engagés. La France, la Hongrie et le Royaume-Uni se présentent à Berlin avec près de deux cents sportifs chacun.Dix-neuf sports et 129 épreuves composent le programme des Jeux olympiques de 1936. Trois nouvelles disciplines olympiques voient le jour : une forme de handball à onze, le canoë-kayak et le basket-ball. Des compétitions de vol à voile et de baseball sont disputées en démonstration.Le sprinteur noir-américain Jesse Owens fut le héros de ces jeux de Berlin en s'adjugeant quatre titres olympiques sur quatre épreuves auxquelles il participa. Le 3 août 1936 sur le 100 m, Owens est situé à la deuxième ligne. En quelques foulées, il dispose de tous ses adversaires, et en particulier de son compatriote Ralph Metcalfe pour réaliser le temps de 10 s 3. Le lendemain, Owens, âgé alors de 23 ans, décroche sa deuxième médaille d'or dans l'épreuve du saut en longueur sous les yeux d’Adolf Hitler. Dans son duel serré avec l'Allemand Luz Long, il prend l'avantage lors de son dernier essai qui est mesuré à 8,06 m, soit un nouveau record olympique. Le lendemain, l'Américain remporte sa victoire la plus nette sur le 200 m en battant de quatre dixièmes (4 m environ) Mark Robinson. Enfin, le triomphe de Jesse Owens s'achève le 9 août avec ses partenaires du 4 × 100 m américain. Au départ du premier relais, il creuse l'écart sur ses concurrents italiens et allemands. L'équipe des États-Unis remporte la course en établissant un nouveau record du monde en 39 s 8 qui tiendra vingt ans.Les exploits de cet athlète ont d'autant plus de retentissement qu'ils se situent à Berlin en 1936 dans le cadre d’une manifestation olympique servant de propagande aux thèses sur la supériorité de la race aryenne sur les Juifs ou les Noirs. Après la guerre, une légende a prétendu qu'Adolf Hitler avait quitté la tribune afin de ne pas saluer le vainqueur du 100 m, Jesse Owens, parce que celui-ci était Noir. La raison en est beaucoup plus simple. Le premier jour des jeux, Hitler avait félicité tous les athlètes allemands, ce qui avait eu pour conséquence que le Comité olympique avait demandé, par souci de neutralité olympique, qu'il félicite tous les athlètes ou aucun. Hitler choisit cette dernière option et ne serra plus la main à aucun athlète durant les jeux.À l'encontre de cette légende, Owens précise dans son autobiographie comment Hitler s'est levé et l'a salué :« Après avoir passé le chancelier, il surgit en me saluant de la main, je l'ai salué en retour. Je pense que des auteurs ont montré un mauvais goût en critiquant l'homme de l'heure en Allemagne. »Lors de l'inauguration du nouveau stade olympique de Berlin en 1984, la veuve de Jesse Owens déclara que son mari avait été plus respecté par les autorités nazies que par les dirigeants de sa propre équipe nationale.En athlétisme, les États-Unis remportent près de la moitié des épreuves. L'Américain Glenn Morris s'adjuge le titre alors qu'il participe à son troisième et ultime décathlon. Sa compatriote Helen Stephens décroche deux médailles d'or au total. Les cinq titres allemands reviennent à des lanceurs. Le marathon bénéficie de repères kilométriques, qui permettent aux concurrents de mesurer leur effort. Tous les trois kilomètres, des points de ravitaillement bien fournis ont été prévus, avec des points chronométriques qui leur donnent l'écart avec leurs prédécesseurs. Ces dispositions permettent de limiter le nombre d'abandons.L'équipe de France de cyclisme repart de ces jeux avec sept médailles en six épreuves au programme. Robert Charpentier, remporte la course sur route individuelle, le contre-la-montre par équipes et la poursuite par équipes (4 000 m).En gymnastique, les Allemands Alfred Schwarzmann et Konrad Frey remportent six titres olympiques au total. Dans l'épreuve du deux de couple d'aviron, l'équipe britannique (Leslie Southwood et Jack Beresford) remporte la victoire sur le fil. Âgé de douze ans et demi, le barreur français Noël Vandernotte, obtient deux podiums en deux et quatre barré et devient le plus jeune médaillé de l'histoire des Jeux olympiques.En natation, le Japon domine les compétitions (onze médailles au total). Au plongeon, l'Américaine Marjorie Gestring, âgée de treize ans et 267 jours, devient la plus jeune championne olympique de l'histoire. Les épreuves d'équitation sont toutes remportées par les cavaliers allemands. En sports collectifs, ces Jeux de Berlin voient le sacre des États-Unis en basket-ball, de l'Italie en football, de l'Allemagne en handball à onze et de l'Inde au hockey sur gazon.C'est la dernière année où le sport automobile a été inscrit aux Jeux (en démonstration à Berlin). Sur les 125 voitures inscrites, les nombreuses voitures allemandes partaient favorites, accompagnées d'une seule voiture britannique, une Singer Le Mans 1500 pilotée par la Britannique Betty Haig, petite nièce du maréchal Douglas Haig. Neuf jours plus tard, elle remportait l'épreuve, devenant ainsi la première femme de l'histoire à battre des hommes à une épreuve olympique.Parmi les quarante-neuf nations qui participent à ces Jeux, trente-deux repartent avec au moins une médaille, comme il est détaillé dans le tableau ci-dessous. Vingt et un de ces pays gagnent au moins une médaille d'or et vingt-neuf remportent plus d'une médaille. L'Allemagne, pays organisateur, arrive à la première place de ce tableau avec quatre-vingt-neuf médailles dont trente-trois en or, vingt-six en argent et trente en bronze. Les États-Unis et la Hongrie prennent les deuxième et troisième places avec respectivement cinquante-six et seize médailles.Une controverse nourrie surgit relative au salut olympique de quelques délégations devant la tribune officielle présidée par Adolf Hitler. Le salut olympique s'inspire du salut du Bataillon de Joinville bras tendu puis replié vers le torse ainsi que le justifia Pierre de Coubertin dont les jeux olympiques de 1924 furent les derniers qu'il organisa.Lors des Jeux olympiques de 1936, la Grèce qui est toujours le premier pays à faire son entrée sur le stade, fit le salut olympique, ainsi que le Canada, la France et l'Italie. Majoritairement, les autres nations choisirent de découvrir la tête, de saluer militairement ou de ne pas saluer.Les nazis assimilèrent le salut olympique au salut fasciste, et crurent à l'adhésion des délégations à leur idéologie, ce qui déclencha des applaudissements nourris et des levées de saluts fascistes en réponse.Il est à noter que le salut olympique dit « salut de Joinville » a été modifié 10 ans après les Jeux olympiques de 1936 mais n'a pas totalement disparu des cérémonies puisque, lors de la cérémonie d'ouverture des Jeux de Munich, la délégation de Bolivie le pratiquait encore.Ce fut aussi l'occasion pour Leni Riefenstahl de réaliser un film d'anthologie sur les Jeux : Les Dieux du stade, tout autant considéré comme un grand classique du cinéma de propagande que novateur par sa façon de filmer les compétitions sportives.Ce furent également les premiers Jeux olympiques de l'histoire retransmis à la télévision.La comédie L'as des as (Gérard Oury, 1982) se déroule à Berlin durant les Jeux olympiques. Le héros du film, interprété par Jean-Paul Belmondo, est entraîneur de l'équipe française de boxe et se trouve embarqué à sauver une famille juive, avant de croiser la route d'Hitler.Berlin 1936, réalisé par Edward Cotterill, 2016.1936, les Jeux de Berlin, réalisé par Bernd Wilting, 2016.Les Jeux d'Hitler, Berlin 1936, réalisé par Jérôme Prieur, Arte France-Roche productions, 90 min, 2016.Dans les secrets des JO de Berlin, réalisé par Laure Philippon, 2016.Berlin 1936 - Dans les coulisses des Jeux olympiques, réalisé par Mira Thiel et Florian Huber, 2016.Charlie Chan aux Jeux olympiques réalisé par H. Bruce Humberstone en 1937.L'Épreuve du temps d'Eduard von Borsody en 1940.Berlin 36 de Kaspar Heidelbach en 2009.La Couleur de la victoire réalisé par Stephen Hopkins en 2016.                               Jeux olympiquesJeux olympiques d'étéStade olympique de BerlinCarl Diem (inventeur de relais de la flamme olympique, instauré en 1936)Les Dieux du stade (film)Olympiades populaires (contre-jeux organisés en Espagne)Hôtel AdlonSportswashing Bibliographie Alexandre Najjar, Berlin 36 : roman, Paris, Plon, 2009, 284 p. (ISBN 978-2-259-21082-9, OCLC 466658576).Jean-Michel Blaizeau, Les Jeux défigurés Berlin 1936, Éditions les Indes savantes, 294 pages, 350 photos, 2012.Fabrice Abgrall & François Thomazeau, 1936 : La France à l’épreuve des Jeux olympiques de Berlin, Alvik, 2006.Jean-Marie Brohm, Jeux olympiques à Berlin, Bruxelles, Complexe (« La mémoire du siècle »), 1983.Daphné Bolz, Les arènes totalitaires, CNRS, 2007.Jérôme Prieur, Berlin, Les Jeux de 36, Éditions La Bibliothèque, Paris, 2017  (ISBN 9782909688862)(fr) La page des Jeux olympiques de Berlin sur le site officiel du CIO.Rapport olympique des jeux de 1936(en) United States Holocaust Memorial Museum, Online Exhibition: Nazi Olympics: Berlin 1936United States Holocaust Memorial Museum - Library Bibliography: 1936 OlympicsJeux Olympiques [1] : Le Comité International Olympique est l’organisation responsable de l’organisation des Jeux Olympiques et du choix des villes hôtes. Tous les communiqués, toute l’historique, toute l’actualité sportive ou politique concernant les Jeux Olympiques peuvent être retrouvés là-dessus. Portail des Jeux olympiques   Portail des années 1930   Portail de Berlin   Portail du nazisme"
sport;Les Jeux olympiques d'été de 2000, jeux de la XXVIIe olympiade de l'ère moderne, se sont déroulés à Sydney (Nouvelle-Galles du Sud, Australie) du 15 septembre au 1er octobre 2000. Ces derniers jeux du millénaire sont les deuxièmes à se tenir en Australie quarante-quatre ans après Melbourne en 1956.Le comité d'organisation (SOCOG) est composé de 2 500 personnes et est assisté par 50 000 volontaires. Les compétitions se répartissent sur 36 sites. La plus grande partie des épreuves, ainsi que le village des athlètes sont concentrés dans le parc olympique de Homebush Bay près du centre-ville de Sydney. 199 nations et 10 651 athlètes (dont 4 069 femmes) prennent part à 300 épreuves dans 28 sports, dont le taekwondo et le triathlon qui font leur première apparition officielle au programme olympique.Avec ses cinq médailles obtenues, la sprinteuse américaine Marion Jones fut considérée pendant longtemps comme l'héroïne de ces jeux, avant que le scandale de dopage lié au laboratoire Balco éclate. Après avoir avoué fin 2007 la prise de substances interdites, elle fut contrainte de restituer au CIO ses médailles obtenues à Sydney.Comme tous les Jeux olympiques organisés dans l'hémisphère sud (à l'exception de ceux de Melbourne), l'appellation ne correspond pas à la saison en cours. Ainsi, les Jeux olympiques d'été de Sydney se sont en fait déroulés tout à la fin de l'hiver austral et au début du printemps.L’Australie n’avait pas accueilli cet évènement depuis les Jeux olympiques d'été de 1956 à Melbourne, les villes de Brisbane et Melbourne furent battues aux élections des jeux d’été de 1992 et 1996. En 1989, la ville de Sydney et le gouvernement de la Nouvelle-Galles du Sud obtiennent le soutien du comité olympique australien à la candidature des jeux de l’an 2000. Immédiatement, Sydney obtient un large soutien de la population, des entreprises et des médias locaux.Le 23 septembre 1993, au cours de sa 101e session tenue à Monaco, le Comité international olympique décide de confier l'organisation les derniers Jeux olympiques du millénaire à Sydney. Après une présentation de 30 minutes de chaque candidat, les membres du CIO désignent la ville australienne à l’issue du 4e tour de scrutin. Sydney devance la ville de Pékin de deux voix seulement. Les autres finalistes Manchester, Berlin et Istanbul sont éliminés lors des tours précédents. Deux autres villes retirèrent leur candidature au cours du processus d'appel d'offres : Milan et Brasilia.Le logo des jeux de Sydney représente un athlète aux symboles et aux couleurs associés aux paysages de l'Australie. Les jambes, formés par un boomerang, sont rouges comme la terre de l'intérieur du pays. Les bras et la tête sont de couleur jaune à l'image du soleil. La silhouette porte une torche olympique laissant échapper un éclair bleu évoquant les plages et la baie de Sydney. La torche olympique est inspirée de l'Opéra de Sydney ainsi que des eaux bleues de l'Océan Pacifique. Elle est constituée de trois couches symbolisant la terre, le feu et l'eau. Comme pour les jeux précédents, la médaille olympique représente la déesse de la victoire brandissant une couronne de vainqueur. Sur le revers, figurent l'Opéra de Sydney, la torche et les anneaux olympiques. Les trois mascottes officielles reprennent les symboles de l'olympisme et du pays organisateur. Olly (diminutif d'olympique) est un kookaburra représente la générosité et l'universalité, symboles de l'olympisme. Syd (diminutif de Sydney) l'ornithorynque met en avant l'environnement ainsi que l'énergie du peuple australien. Millie (diminutif de millénaire) est un échidné représentant la technologie de l'an 2000.Le Parc olympiqueLe Sydney Olympic Park ou Parc Olympique de Sydney, situé à Homebush Bay à proximité du centre-ville, constitue le cœur des Jeux olympiques de Sydney. Il comprend le village olympique, ainsi qu'une dizaine d'installations sportives. Le Stade Olympique (Stadium Australia) fut construit spécialement pour les jeux de l’an 2000. D’une capacité de 110 000 places, cette enceinte sportive accueille les épreuves d’athlétisme, la finale de football ainsi que les cérémonies d’ouverture et de clôture. L'Aquatic Center (17 500 places) est le siège des compétitions de natation, de natation synchronisée, de plongeon, et de water polo. Ce bassin olympique fut considéré par Juan Antonio Samaranch, président du Comité international olympique, comme « la plus belle piscine qu’il ait vue de sa vie ». Les autres sites du parc olympique de Sydney sont le Centre de tennis NSW  (16 000 places), le State Hockey Centre (15 000 places), le Baseball Stadium et le Sydney International Archery Park. Par ailleurs, le Dome accueille les compétitions de badminton, de handball et de volleyball. Le Superdome celles de gymnastique et de basketball. Enfin, les épreuves de tennis de table et de taekwondo ont lieu au State Sports Centre.Les autres sites Il s'agit de la salle Convention and Exhibition Centre (lutte, boxe, judo et escrime), du vélodrome Dunc Gray, des Centres internationaux de tir et d'équitation, du Blacktown Olympic Centre (baseball et softball) et du Stade de football de Sydney. Les courses d'aviron et de canoë-Kayak sont disputés au Regatta Centre, les régates de voile dans la baie de Sydney. Les matchs de volley-ball ont lieu au Sydney Entertainment Centre ainsi que sur la plage de Bondi Beach. Des matchs du tournoi olympique de football sont disputés à l'extérieur de Sydney : Canberra (Bruce Stadium), Adelaide (Hindmarsh Stadium), Melbourne (Cricket Ground) et Brisbane (Cricket Ground).Le village olympiqueSitué au sein du parc de Homebush Bay, le village olympique de Sydney est le premier à héberger l'ensemble des compétiteurs dans un seul endroit. 10 000 athlètes et 5 000 officiels sont logés dans 800 maisons et 350 appartements dont l'architecture rappelle les immeubles de la ville de Sydney. Construit sur 94 hectares et mesurant 1,5 km de long, le village comprend un restaurant, un centre commercial, une banque, un hôpital et même une discothèque.Ce lieu fut conçu en respect de l'environnement. Fonctionnant avec l'énergie solaire, il fut construit en matériaux recyclables et produits verts. Sydney fut la première ville à inclure le volet écologique dans son dossier de candidature.La cérémonie d'ouverture des Jeux olympiques d'été 2000 se déroule le 15 septembre 2000 devant les 110 000 spectateurs du stade olympique de Sydney. Le spectacle débute avec la star international australienne Olivia Newton-john et le chanteur populaire australien John Farham avec la chanson Dare to dream. Le spectacle est conçu par Ric Birch et David Atkins, est un hommage à l'histoire de l'Australie. Il débute par l’entrée de près de 120 cavaliers avant d’évoquer les symboles de l’Australie que sont l’océan, le désert et les animaux. La culture aborigène est à de nombreuses reprises mise en avant par l’intermédiaire de chants et de danses traditionnelles. Les 199 nations pénètrent ensuite dans le stade. Les deux Corée défilent ensemble, dans les mêmes tenues, et derrière le même drapeau : le profil bleu ciel de la péninsule sur fond blanc, et porté par deux athlètes, un de chaque pays. Le serment olympique est prêté par la joueuse de Hockey Rechelle Hawkes. Cathy Freeman, Australienne aux origines aborigènes, allume la flamme olympique en tant que dernière relayeuse de la torche. Elle fut choisie comme symbole de la volonté de réconciliation entre les Aborigènes et les descendants des migrants européens. Elle remporta lors de ces jeux la médaille d'or de la course du 400 mètres. Pour conclure cette cérémonie, William Deane, gouverneur général d’Australie, déclare officiellement l’ouverture des jeux de la XXVIIe olympiade.La cérémonie de clôture met en scène de manière festive la culture populaire australienne avec le rappel de films locaux (Crocodile Dundee, Priscilla, folle du désert, ...). Kylie Minogue, la chanteuse australienne a participé à cette cérémonie en chantant les titres Dancing Queen et On A Night Like This. L'événement se conclut par un concert du groupe INXS.Quatre pays ont fait leur entrée dans les Jeux olympiques : l'Érythrée, les États fédérés de Micronésie, les Palaos et le Timor oriental. Ce pays, nouvellement indépendant et victime de combats contre l'armée indonésienne, fut invité au dernier moment par le Comité international olympique. Quatre athlètes timorais participèrent à titre individuel en tant qu'« athlètes internationaux olympiques ».199 nations ont participé aux Jeux olympiques de Sydney, soit deux de plus que les jeux précédents d'Atlanta de 1996. Au total, 10 651 athlètes (4 069 femmes et 6 582 hommes) ont participé aux épreuvesDeux nouveaux sports font leur entrée lors de ces jeux, le taekwondo et le triathlon. Par ailleurs, de nouvelles disciplines apparaissent (le duo et le plongeon synchronisés en natation, le trampoline en gymnastique, le keirin et la course américaine en cyclisme, le fortyniner en voile).Enfin, des disciplines réservées jusqu'alors aux hommes s'ouvrent aux femmes tels le marteau et le saut à la perche (athlétisme), la fosse et le skeet olympique (tir), le water polo, l'haltérophilie et le pentathlon moderne.Au total, ce sont 28 sports et 300 épreuves qui figurent au programme de ces jeux de 2000.AthlétismeRésultats détaillésEn sprint, Maurice Greene devient l'homme le plus rapide du monde sur 100 m en 9 s 87, alors que Konstadínos Kedéris remporte le 200 m, le premier titre de la Grèce depuis 1896. Sur le tour de piste, Michael Johnson conserve son titre d’Atlanta, Cathy Freeman réussit quant à elle le pari de s’imposer sur son sol. L’Australienne d’origine aborigène devient ainsi la première athlète ayant allumé la flamme olympique à remporter une médaille d'or dans les mêmes jeux. Angelo Taylor permet aux États-Unis de remporter leur cinquième titre consécutif sur le 400 m haies. Avec la médaille d’argent de la Jamaïque sur le relais 4 × 100 m, Merlene Ottey devient l’athlète féminine la plus décorée aux Jeux olympiques. Sur le 800 m, le favori Wilson Kipketer subit la loi de l’Allemand Nils Schumann, alors que le Kényan Noah Ngeny surprend le favori marocain Hicham El Guerrouj sur 1 500 m. En fond, l’Éthiopien Haile Gebrselassie est pour la seconde fois victorieux du 10 000 m. Sa compatriote Derartu Tulu remporte également cette épreuve après son titre en 1992 et une grossesse. Le Polonais Robert Korzeniowski devient le premier homme à s'adjuger les deux épreuves de marche lors de la même olympiade. Jan Železný conserve son titre du lancer du javelot et signe une troisième victoire de rang. Au disque, la Biélorusse Ellina Zvereva devient, à près de 40 ans, l’athlète la plus âgée à remporter un titre olympique. Nick Hysong s’impose dans l’épreuve du saut à la perche, une première américaine depuis Mexico en 1968.AvironRésultats détaillésLes nations européennes s’adjugent treize des quatorze titres disputés. À 38 ans, le rameur britannique Steve Redgrave réalise l’exploit de remporter son cinquième titre olympique consécutif, record sans précédent. Il décroche avec l'équipe du Royaume-Uni la médaille d’or dans l'épreuve du quatre sans barreur en devançant l’équipage italien de près de quatre dixièmes. Steve Redgrave conclut ainsi une série débutée aux Jeux olympiques de 1984 à Los Angeles. Sir Steve Redgrave est considéré comme l'un des plus grands sportifs du Royaume-Uni.BadmintonRésultats détaillésL’équipe de Chine poursuit sa domination mondiale dans ce sport en remportant quatre des cinq titres mis en jeu. En double messieurs, la victoire revient à la paire indonésienne.BaseballRésultats détaillésL’équipe masculine des États-Unis bat les champions olympiques en titre cubains en finale (4-0). Cuba avait remporté les deux premiers titres de baseball à Barcelone et à Atlanta.Basket-ballRésultats détaillésLes États-Unis d'Amérique remportent leur troisième titre olympique consécutif, leur douzième en quatorze participations. Elle bat l’équipe de France en finale de 10 points seulement, après avoir frôlé la correctionnelle en demi-finale face à la Lituanie. Côté féminin, les États-Unis conservent leur titre obtenu quatre ans plus tôt en disposant de l’Australie (76-54).BoxeRésultats détaillésLes boxeurs cubains sont une nouvelle fois dominateurs en s’adjugeant six médailles dont quatre titres olympiques. Le Russe Oleg Saitov conserve son titre des welters alors que le Cubain Felix Savon remporte en catégorie poids-lourds sa troisième médaille d’or consécutive. Le Royaume-Uni décroche son premier titre en boxe depuis 1968. Brahim Asloum offre à la France une médaille d’or 64 ans après sa dernière victoire olympique.Canoë-kayakRésultats détaillésL’Allemande Birgit Fischer devient l’athlète la plus titrée du canoë-kayak en remportant sa sixième et septième médaille d’or (K2 et K4). L’Allemagne et la Hongrie s’adjugent la moitié des épreuves.CyclismeRésultats détaillésLa cycliste Leontien van Moorsel s'adjuge quatre médailles dont trois titres lors de ces jeux de Sydney. Sur route, la Néerlandaise remporte les deux épreuves. Elle s'impose au sprint final dans la course en ligne et devance ses rivales Mari Holden et Jeannie Longo-Ciprelli dans le contre-la-montre. Sur piste, Van Moorsel remporte sa troisième médaille d'or de ces jeux dans l'épreuve de la poursuite individuelle où elle bat en finale la Française Marion Clignet de près de cinq secondes. La cycliste de 30 ans parachève ses exploits par une médaille d'argent obtenue dans la course aux points. Félicia Ballanger est l'autre triomphatrice de ces Jeux avec deux médailles d'or, dont le titre du sprint. Chez les hommes, le cycliste américain Marty Nothstein offre à son pays la première médaille olympique sur piste depuis les Jeux de Los Angeles en 1984. Le titre de la course en ligne revient à l’Allemand Jan Ullrich et celui du contre-la-montre au Russe Viatcheslav Ekimov. La France, grâce à notamment à Florian Rousseau et à Miguel Martinez (en VTT), récolte huit médailles au total.ÉquitationRésultats détaillésL’Allemagne et les Pays-Bas remportent quatre des six épreuves d’équitation. L’Australie signe une troisième victoire d’affilée au concours complet par équipe, les Allemands une cinquième au dressage.EscrimeRésultats détaillésL’italienne Valentina Vezzali ajoute deux médailles d’or supplémentaires à son palmarès, le Russe Pavel Kolobkov obtient son sixième podium olympique douze ans après sa première médaille à Séoul. La Hongroise Tímea Nagy conserve sa couronne olympique à l’épée individuel. L’Italie et la Russie remportent six des dix titres mis en jeu.FootballRésultats détaillésL’équipe du Cameroun crée la surprise en battant l’Espagne en finale après les tirs au but. Il s’agit de la première médaille d’or olympique de l’histoire pour ce pays. Le tournoi féminin revient à la Norvège qui bat les États-Unis en finale (avec un score de 3 buts à 2 après prolongations).GymnastiqueRésultats détaillésLa Chine remporte le concours général masculin par équipe après quatre décennies de frustration. Ils mettent fin au règne russe débuté en 1980. Le titre féminin revient à l’équipe de Roumanie. En individuel, Alexei Nemov est le gymnaste le plus en vue de ces Jeux avec six médailles dont deux titres au concours général et à la barre fixe. Sa compatriote Svetlana Khorkina remporte l’or aux barres asymétriques. La Russie s’impose également en gymnastique rythmique et au trampoline, portant son total à neuf médailles d’or sur dix-huit épreuves.HaltérophilieRésultats détaillésLe Turc Naim Suleymanoglu échoue dans sa tentative de conquête d’un quatrième titre olympique consécutif. Les Grecs Pyrros Dimas et Kakhi Kakhiashvili remportent leur troisièmes médailles d’or d’affilée. Les Chinois s’adjugent cinq des quinze titres olympiques.HandballRésultats détaillésLa Russie remporte pour la quatrième fois le tournoi olympique de handball masculin en battant la Suède 28 à 26 en finale. Les Suédois laisse échapper la victoire pour la troisième fois d'affilée. Le russe Andreï Lavrov devient à 38 ans le premier homme à remporter trois médailles d'or olympique en handball. Chez les dames, le Danemark bat la Hongrie en finale (31-26). La Corée du Sud ne figure pas sur le podium pour la première fois en 20 ans.Hockey sur gazonRésultats détaillésL’équipe des Pays-Bas dispose de la Corée du Sud en finale après l’épreuve des tirs au but, et devient la première nation à conserver son titre depuis l’Inde en 1956. L’Australie s’impose face à l’Argentine dans le tournoi féminin et décroche à l’occasion sa troisième médaille d’or dans cette discipline.JudoRésultats détaillésLe Français David Douillet conserve son titre des poids-lourds décroché quatre ans plus tôt à Atlanta, le japonais Tadahiro Nomura en fait de même en super-légers. Ces deux nations dominent par ailleurs la compétition avec 14 médailles en autant d’épreuves.LutteRésultats détaillésDans le choc de ces Jeux olympiques en moins de 130 kg, l’Américain Rulon Gardner bat en finale le Russe Alexandre Kareline après treize ans d’invicibilité. « L’homme le plus fort du monde » n’avait pas perdu un combat international depuis 1987 et fut champion olympique en 1988, 1992 et 1996.NatationRésultats détaillésAprès un début de saison marqué par de multiples records du monde, la nageuse Inge de Bruijn confirme son état de forme en remportant quatre médailles dont trois d'or. Ses trois victoires sont toutes gagnées dans des épreuves individuelles. La Néerlandaise s'impose tout d'abord sur le 50 m nage libre où elle devance la Suédoise Therese Alshammar de 19 centièmes avant de remporter le 100 m nage libre devant la même concurrente pour 50 centièmes. Dans le 100 m papillon, Inge de Bruijn établit un nouveau record du monde de la distance (56 s 61) et décroche sa troisième médaille d'or de ces jeux. Elle obtient en supplément une médaille d'argent dans le relais 4 × 100 m avec l'équipe des Pays-Bas. Sur un plan général, les États-Unis dominent les épreuves avec 14 couronnes sur 32 épreuves. Ian Thorpe remporte quatre médailles, dont le titre du 400 m nage libre et deux relais et Pieter van den Hoogenband s'impose sur le 100 m et sur 200 m où il établit un nouveau record du monde. L'Américaine Jenny Thompson ajoute quatre médailles olympiques supplémentaires à son palmarès (10 médailles dont 8 titres). Michael Klim décroche quatre médailles alors que sa compatriote Brooke Bennett réalise le doublé 400 m et 800 m nage libre. Les épreuves de plongeon sont dominés par les Chinois, celles de natation synchronisée par les Russes. L'équipe de Hongrie remporte le tournoi masculin de water polo et l'Australie le tournoi féminin. Le nageur équatoguinéen Éric Moussambani, surnommé « Éric le nageur » ou « Éric l'anguille » (Eric the Eel) par les médias, il a connu une célébrité internationale éphémère, lorsqu'il réalisa son 100 m nage libre en 1 min 52 s 72, soit plus de deux fois le temps mis par ses concurrents (le record du monde de la discipline était de 47 s 84 secondes lors de ces Jeux olympiques), et 10 secondes de plus que le record du monde du 200 m, qui était de 1 min 42 s 96. Les médias relevèrent la caractère incongru de sa performance, tout en applaudissant son courage, perçu comme un symbole de l'esprit olympique.Pentathlon moderneRésultats détaillésLe Russe Dmitry Svatkovsky remporte le titre masculin et la Britannique Stephanie Cook le titre féminin.SoftballRésultats détaillésÀ l’instar de leurs homologues masculins du baseball, l’équipe américaine de softball remporte le tournoi olympique en s’imposant lors des derniers matchs contre des adversaires les ayant battus lors du premier tour.TaekwondoRésultats détaillésSix nations se partagent les titres de cette nouvelle discipline olympique, la Corée du Sud en remporte trois.TennisRésultats détaillésL’Américaine Venus Williams décroche deux médailles d’or, le simple dames et le double avec sa sœur Serena. Le Russe Ievgueni Kafelnikov remporte le tournoi masculin.Tennis de tableRésultats détaillésComme à l’accoutumée, la Chine domine les épreuves du tennis de table avec quatre titres sur quatre et huit médailles sur douze. Wang Nan décroche deux médailles d’or.TirRésultats détaillésLes tireurs chinois montent huit fois sur le podium, dont trois sur la plus haute marche. Le Français Franck Dumoulin remporte le titre du pistolet à 10 mètres.Tir à l'arcRésultats détaillésL’Australien Simon Fairweather remporte l’épreuve individuelle masculine alors que les archers de Corée du Sud s’emparent des trois autres titres, dont les épreuves féminines.TriathlonRésultats détaillésInscrit pour la première fois au programme des Jeux olympiques, le triathlon consacre le Canadien Simon Whitfield et la Suissesse Brigitte McMahon.VoileRésultats détaillésL’Australie remporte, avec sa victoire en 470, sa première médaille d’or depuis 1972. Les Britanniques s’imposent à trois reprises (Europe, Finn et Laser). 10 000 spectateurs assistent à la dernière course des Jeux sur la baie de Sydney, soit la plus grande affluence enregistrée à l’occasion des régates olympiques.Volley-ballRésultats détaillésL’équipe de Yougoslavie remporte le tournoi masculin face à la Russie alors que l’équipe de Cuba remporte son troisième titre féminin consécutif.En beach volley, la paire américaine Dain Blanton et Eric Fonoimoana classée onzième mondiale bat les favoris brésiliens en finale. Chez les dames, la victoire revient à l’Australie.NB : L'athlète américaine Marion Jones qui avait obtenu cinq médailles lors de ces jeux (trois d'or et deux de bronze) a reconnu s'être dopée le 5 octobre 2007. Par conséquent, ses médailles ont dû être restituées au CIO.Quatre-vingt délégations repartent des jeux de Sydney avec au moins une médaille. Les États-Unis totalisent le plus grand nombre de podiums (94 dont 37 médailles d'or), devançant la Russie (88) et la Chine (59). L'Australie, pays hôte, réalise la plus belle moisson de récompenses olympiques de son histoire avec 58 médailles dont 16 titres.48 heures avant le début des épreuves du 400 m, l'athlète française Marie-José Pérec quitte brusquement les Jeux olympiques en raison du harcèlement dont elle s'estime victime depuis son arrivée dans la ville australienne. L'athlète évoquera notamment l'agression verbale d'un individu ayant forcé la porte de sa chambre. Elle dut faire face également à la pression médiatique australienne dans la mesure où sa principale concurrente, Cathy Freeman, est une des athlètes les plus populaires d'Australie. Lors de son départ de l'aéroport de Singapour, son compagnon Anthuan Maybank aura une altercation avec un cadreur australien.Marie-José Pérec avait comme objectif de remporter une troisième couronne consécutive sur le tour de piste mais cet épisode la poussa à mettre un terme définitif à sa carrière.Dix cas de dopage ont été enregistrés durant ces jeux de Sydney. Ils concernent en majorité des haltérophiles et des lutteurs. Les athlètes suivants furent disqualifiés et déchus de leur médaille éventuelle.En France, France Télévisions, Canal+ et Eurosport diffusèrent l’événement.La participation d'Éric Moussambani, qui a réalisé l'anti-record de 1 min 52 s 72 au 100 mètres nage libre (le record du monde et olympique était de 47 s 84) a été très remarquée. Le nageur équatoguinéen évoluait pour la première fois dans un bassin olympique. Le public australien lui réserva une ovation durant les cérémonies de médailles en scandant son prénom. Les médias relevèrent le caractère incongru de sa performance, tout en applaudissant son courage, perçu comme un symbole de l'esprit olympique,.Jeux olympiques - Jeux olympiques d'étéLes Jeux de Sydney sur le site du Comité International Olympique(en) Rapport officiel des Jeux olympiques de Sydney 2000 [PDF]Rétrospective de la XXVIIe olympiade, sur le site gamesinfo.com.au Portail des Jeux olympiques   Portail des années 2000   Portail de l’Australie   Portail de Sydney
sport;""
sport;Les Jeux paralympiques sont un événement sportif international regroupant les sports d’été ou d’hiver, auquel des milliers d’athlètes atteints de handicap participent à travers différentes compétitions tous les quatre ans à la suite des Jeux olympiques, pour chaque olympiade. Y participent des athlètes atteints par un handicap physique, visuel ou mental. Ils sont organisés par le Comité international paralympique (et non pas par le Comité international olympique).Les personnes atteintes de surdité peuvent prendre part aux Deaflympics. Les personnes atteintes d'un handicap mental pouvaient aussi participer aux Jeux olympiques spéciaux jusqu'à leur réintégration en 2012 aux Jeux paralympiques d'été de 2012 (après en avoir été exclus depuis 2000).Mr Ludwig Guttmann, médecin neurologue de l'hôpital de Stoke Mandeville dans le comté de Buckinghamshire près de Londres, eut l'idée d'organiser dès 1948 sur le terrain de l’hôpital les premiers « Jeux mondiaux des chaises-roulantes et des amputés » (« World Wheelchair and Amputee Gates »). Connus plus tard sous le nom de « Jeux de Stoke Mandeville », ils étaient destinés à réhabiliter par la pratique physique des victimes et anciens combattants de la Seconde Guerre mondiale devenus paraplégiques. Deux équipes d’anciens combattants ont alors participé à une unique épreuve, le tir à l’arc.Les 9e jeux de Stoke-Mandeville eurent lieu à Rome en 1960 une semaine après les Jeux olympiques d'été de 1960, et l'on considère qu'il s'agit des premiers Jeux paralympiques. Les premiers Jeux paralympiques d'hiver eurent lieu à Örnsköldsvik en Suède en 1976.Parallèlement, ont lieu à Saint-Étienne, à l'initiative d'Yves Nayme, plusieurs éditions de jeux internationaux pour les personnes handicapées physiques (jeux européens de 1966,  jeux mondiaux de 1970 et 1975 et championnats du monde de 1990),.Les personnes atteintes de paralysie cérébrale participent aux Jeux paralympiques depuis les Jeux d'Arnhem en 1980.Depuis les Jeux paralympiques d’été de Séoul en 1988, les Jeux olympiques et les Jeux paralympiques sont organisés dans la même ville et sont organisés après les jeux olympiques.Les premiers Jeux paralympiques africains auraient dû avoir lieu à Rabat au Maroc en janvier 2020. Ils ont cependant été reportés pour des raisons logistiques et matérielles.Les Jeux paralympiques d'été de 2020 à Tokyo ont été reportés à 2021, tout comme les Jeux olympiques d'été de Tokyo en 2020, en raison de la pandémie de Covid-19, ce qui est une première dans l'histoire des Jeux paralympiques.À l'origine, le nom « paralympique » était une combinaison de « paraplégique» et de « olympique ». Avec la participation d'athlètes avec différents handicaps, le terme « paralympique » est aujourd'hui défini comme la réunion de « para », préfixe d'origine grecque signifiant « à côté de » ou « parallèle » et de la terminaison « lympique » des Jeux olympiques. Les Jeux paralympiques sont ainsi considérés comme solidaires des Jeux olympiques.L'objectif du Mouvement paralympique est de donner l’occasion aux athlètes ayant un handicap physique ou mental de se dépasser et de réaliser des performances sportives comparables à celles des athlètes olympiques.Les Jeux paralympiques regroupent des athlètes en situation de handicap physique ou visuel appartenant aux catégories suivantes : tétraplégie et paraplégie, séquelles neurologiques assimilables, amputation et assimilé, infirmes moteurs cérébraux, grands handicaps (myopathie), non-voyants et malvoyants.Pour que la compétition soit équitable, les athlètes sont regroupés par catégories selon leur handicap. L'objectif est de faire concourir ensemble des athlètes ayant des aptitudes fonctionnelles comparables. Dans chaque handisport, on définit des catégories. Ainsi en athlétisme, il y a des épreuves de course pour les personnes atteintes de cécité (acuité visuelle inférieure à 3/60), de déficience visuelle (inférieure à 3/10 et supérieure à 1/20), pour les personnes amputés qui courent avec une prothèse et des courses en fauteuil roulant.Les sourds et malentendants n'ont toujours pas le droit de participer aux Jeux paralympiques. Ceci peut paraître logique, dans la mesure où les personnes faiblement sourdes ont des capacités physiques peu altérées. Par contre, il est difficile de comprendre pourquoi les sourds profonds qui peuvent avoir une altération de l'équilibre ne participent pas avec les autres handicapés. En réalité, la non-intégration des sourds et malentendants semble découler du fait qu'ils ont leur propre concours, les Deaflympics, qui sont historiquement la plus ancienne compétition handisport internationale. Il peut aussi y avoir des cas de tricherie comme cela s'est produit avec les handicapés mentaux.Entre 2004 et 2012, les personnes en situation de handicap mental ont été exclues des Jeux paralympiques auxquels ils prenaient part depuis 1996, pour des problèmes de classification de handicap et de fausse déficience intellectuelle. Les personnes en situation de handicap mental pouvaient cependant participer aux Jeux olympiques spéciaux qui n'avaient pas lieu la même année que les Jeux olympiques ordinaires et les Jeux paralympiques.Cependant, depuis 2012, les personnes en situation de handicap mental sont réintégrées dans les compétitions officielles et participent aux Jeux paralympiques de Londres en athlétisme, natation, et tennis de table.Après des années de travail « main dans la main » la Fédération française du sport adapté (FFSA), les différentes fédérations nationales et la Fédération internationale des sports pour personnes en situation de handicap mental ont pu « établir de nouveaux critères d'éligibilité incomparablement plus solides que par le passé ».La décision de réintégrer les personnes en situation de handicap mental a été prise lors de l'assemblée générale du Comité international paralympique à Kuala Lumpur. Gérard Masson, président de la Fédération française handisport, a soutenu cette décision même si parfois « l'intégration n'est pas aussi évidente dans le monde du handicap ». Pour lui, la réticence de certains pays à réintégrer les personnes en situation de handicap mental dans les jeux paralympiques « tenait plus aux critères de classification qu'à un rejet de la part des autres athlètes paralympiques ».Dans un communiqué, la ministre de la santé et des sports Roselyne Bachelot et la secrétaire d'État chargée des sports ont alors salué « le travail remarquable engagé depuis plusieurs années par la Fédération internationale de sport adapté, avec le soutien du Comité paralympique et sportif français et de la Fédération française de sport adapté, pour réintégrer les sportifs en situation de handicap mental dans le mouvement paralympique ».                   Les Jeux paralympiques reprennent la plupart des symboles olympiques : les cérémonies d'ouverture et de clôture, la flamme olympique, les mascottes.AthlétismeAvironBoccia (sport ressemblant aux boules, pratiqué avec des balles en cuir par des handicapés moteurs)Basket-ball en fauteuil roulantCyclismeÉquitationEscrimeFootball à 5 (ou cécifoot) (pratiqué par des athlètes malvoyants ou non-voyants)Football à 7 (pratiqué par des athlètes handicapés moteur)Goalball (sport de ballon pratiqué par des athlètes malvoyants ou non-voyants avec un ballon sonore)HaltérophilieJudo (pratiqué par des athlètes malvoyants ou non-voyants)NatationRugby en fauteuil roulantTennis en fauteuil roulantTennis de tableTir à l'arcTir sportifTriathlonSki alpinSki nordique (ski de fond et biathlon)Para-hockey sur glace (anciennement hockey sur luge)CurlingSnowboardLa classification Comité international paralympique (CIP) pour les sports d'hiver indique la classification des athlètes en fonction de leur handicap pour les disciplines de sport d'hiver et en particulier pour les Jeux paralympiques d'hiver.Les athlètes sont classés par catégorie de handicaps assimilables et selon les matériels techniques utilisés. Les classes B concernent les handicapés visuels (Blinds). Les classes LW (Locomotion Winter) les handicapés moteurs, debout : LW1 à LW9 ou assis : LW10 à LW12.Les skieurs assis peuvent effectuer des tests fonctionnels. L'objectif est de définir l'appartenance à l'une des trois classes LW10, LW11 ou LW12. Pour la classe LW12 également classer les athlètes dans les sous-classes LW12/1 et LW12/2.Catégorie:Tableau des médailles des Jeux paralympiques d'étéCatégorie:Tableau des médailles des Jeux paralympiques d'hiverListe d'athlètes olympiques ou paralympiques devenus parlementairesInternational Paralympic Committee« Toute l'actualité handisport et sport adapté »(Archive.org • Wikiwix • Archive.is • Google • Que faire ?)L'actualité paralympique francophone  Portail des Jeux olympiques   Portail du handisport
sport;"Au football, le libéro est un joueur défensif dont le rôle est de couvrir ses coéquipiers défenseurs. Il se trouve par conséquent derrière les autres joueurs de la défense mais devant le gardien de but, et n'est pas tenu au marquage individuel.Libero veut dire « libre » en italien. Le mot a été employé au sens actuel pour la première fois par le journaliste italien Gianni Brera ; il s'est ensuite répandu dans maintes langues. À l'origine, il caractérisait le joueur qui évoluait derrière ses défenseurs et qui verrouillait les brèches de sa défense, dans le championnat italien. Ce rôle nécessitait que son positionnement fût libre, derrière la défense, d'où l'appellation. Au début, il était dévolu aux joueurs plutôt rugueux et physiques, il était capable d'intervenir comme dernier défenseur. Le système du libéro prenait tout son sens dans le catenaccio italien mis en place par Helenio Herrera à l'Inter Milan au milieu des années 1960, qu'Alfredo Foni avait déjà testé quelques années auparavant. Ce système s'inspirait de l'organisation de jeu mise en place par l'Autrichien Karl Rappan dans l'équipe de Suisse dans les années 1930 et 1940. En plaçant un défenseur juste devant le gardien, Rappan avait inventé le poste de libéro.Cependant, le rôle du libéro va considérablement évoluer à la fin des années 1960, grâce à l'Allemand Franz Beckenbauer. Sur les conseils de son entraîneur Branko Zebec, Beckenbauer réinvente le poste et fait du libéro un atout offensif. En effet, par sa liberté sur le terrain, le libéro, exempt de tout marquage, peut dès lors se joindre à l'attaque et participer au jeu offensif. Dans cette organisation tactique, le libéro est alors souvent un milieu offensif reconverti, possédant une bonne vision du jeu et de grandes qualités techniques, et capable de se transformer en véritable meneur de jeu. Le système de jeu avec un libéro va être particulièrement prisé dans les années 1970 et 1980, en particulier en Allemagne, où le rôle de libéro était souvent réservé aux meilleurs joueurs de l'équipe.Néanmoins, le recours à un libéro va tomber en désuétude au cours des années 1990. La systématisation de la défense en ligne imposée par Arrigo Sacchi (où les quatre défenseurs jouent tous sur une même ligne afin de mettre l'adversaire hors-jeu) et le choix de privilégier le marquage dit en zone plutôt que le marquage individuel vont sonner le glas du libéro. De plus, l'obligation pour le gardien de systématiquement jouer les ballons donnés par ses coéquipiers au pied ou de la tête (et non plus de la main comme auparavant) va précipiter la disparition des libéros. C'est désormais le gardien qui assure les relances comme dernier défenseur.Le terme « libéro » est cependant encore usité dans le football, souvent à tort puisqu'il est difficile de considérer ce rôle dans une défense en ligne. Dans ce contexte de complémentarité des défenseurs centraux, on désigne comme « libéro » celui des deux qui est le plus technique et privilégie le placement et l'anticipation, par opposition au stoppeur, plutôt voué à faire valoir ses qualités physiques. En réalité, les meilleures équipes ont des défenseurs centraux jouant sur les deux registres, alors que les moins bonnes se contentent en général de joueur au profil de stoppeur.On peut aussi remarquer que cette disparition du libéro est quelquefois compensée, lors de balle en profondeur derrière la défense, par la sortie du gardien au-delà de sa surface de réparation — ceci étant facilité par la capacité des gardiens modernes à utiliser aussi bien leurs pieds que leurs mains.Tactique (football) Portail du football"
sport;Une mise en échec ou charge est une technique défensive au hockey sur glace qui consiste à bousculer l'adversaire pour le gêner ou lui faire perdre la rondelle (palet). Pour ce faire, les joueurs utilisent leur épaule, leur hanche ou leur bras.En France, l'utilisation de la mise en échec est autorisée depuis 1951. Elle l'est uniquement à partir de la catégorie Minimes U15, et interdite pour le hockey sur glace féminin.Au Canada, les mises en échec sont permises en Alberta au niveau peewee (11-12 ans) et au Québec au niveau bantam (13-14 ans).Un « bloc » consiste à faire perdre la rondelle par le biais d'une mise en échec lorsque celui-ci se trouve le long de la bande (balustrade). Lors de cette tentative, c'est la responsabilité d'un coéquipier de reprendre possession de la rondelle et non du joueur qui bloque l'adversaire. Cette technique est plutôt complexe car un joueur ne peut pas bloquer son adversaire plus que deux secondes sans risquer une pénalité pour obstruction.Un allègement consiste à soutirer la rondelle d'un adversaire le long de la bande en même temps de le sortir du jeu en utilisant une mise en échec. Cette tentative est plus pratique que le bloc car elle implique seulement un joueur comparativement à deux.Les joueurs doivent savoir appliquer correctement une mise en échec, car certaines charges sont sanctionnées (voir article sur les pénalités):cinglage (frapper ou tenter de frapper un joueur avec le bâton de hockey)faire trébucher (que ce soit avec la crosse, le pied, le bras)charge incorrecte (mauvaise position, prise d'élan, saut...)ou plus grave : charge avec le bâton, dans la région de la tête, dans le dos, en dessous du genou, crosse haute (au-dessus des épaules), charge contre la bande (si l'adversaire n'a pas le palet), coup de coude,Un joueur ne doit pas tenter de blesser un adversaire.La mise en échec n'est pas sans risques, elle fait plus que tripler le risque de blessures et de commotions cérébrales dans le hockey pee-wee, selon une étude réalisée par l'Université de Calgary.Les joueurs doivent se protéger pour réduire les risques de blessures. Les plus courantes sont les fractures, les commotions cérébrales et les coupures. Certaines peuvent occasionner des lésions permanentes comme celles infligées au joueur canadien de 17 ans Francis Gariépy, devenu paraplégique lors d’un match au niveau secondaire à la suite d’une mise en échec en 2009.Certains groupes s’opposent au Canada à cette pratique violente du hockey.Dans l'arrêt de principe Morin c. Blais de la Cour suprême du Canada, la Cour énonce que la violation d'une norme élémentaire de prudence fait présumer le lien de causalité entre la faute et le préjudice. Dans la décision Zaccardo c. Chartis Insurance Company of Canada, la Cour supérieure du Québec a appliqué cette règle dans le contexte d'une violation des règles d'une ligue de hockey par un joueur qui a commis une mise en échec par-derrière interdite par Hockey Canada et qui a rendu un autre joueur tétraplégique. Ce qu'il faut retenir de l'arrêt Zaccardo, c'est que la violation d'une règle élémentaire de prudence contenue dans la soft law (par ex. règle sportive, règle d'hygiène, règle de sécurité) va faire présumer le lien causal entre la faute et le dommage. Portail du hockey sur glace
sport;"La musique populaire désigne les genres de musique tirant leur origine et trouvant leur public dans les milieux populaires. Elle se développe dans un milieu urbain et industrialisé et est souvent associée à l'histoire de la révolution industrielle et technologique ayant amené la technique phonographique, ainsi qu'à l'histoire de la mondialisation.Le terme est souvent utilisé comme un comparatif par certains défenseurs de la musique savante, qui perçoivent la musique populaire comme un produit commercial et pointent ses faiblesses esthétiques, qu'ils jugent en comparaison de la musique classique européenne. Si la musique populaire est souvent associée à la musique commerciale ou de masse, elle s'en distingue néanmoins par des critères qualitatifs et par sa capacité à former des communautés de mélomanes en se nourrissant de formes musicales inscrites dans diverses traditions historiques.Il ne faut pas confondre la musique populaire avec la musique pop, qui est un genre spécifique de musique populaire.Le terme de musique populaire est l'objet de débats. Le sociologue Simon Frith estime que le terme de culture populaire « n'a de sens qu'en tant que comparatif » et que ses plus fréquents objets de comparaison sont la haute culture, la culture folklorique et la culture de masse. La musique populaire est en effet souvent comparée à la musique savante, la musique traditionnelle et la musique commerciale.Depuis les années 1980, le milieu universitaire retient la « définition anglo-saxonne » du terme, comme le rappelle le musicologue français Olivier Julien : « sont populaires non pas les musiques qui ne sont pas savantes, mais les musiques qui ne sont ni savantes, ni folkloriques ». Citant Philip Tagg, pionnier des études sur la musique populaire, le même auteur précise que « les musiques populaires […] partagent avec les musiques folkloriques l'absence de cadre institutionnel, mais ont en commun avec la musique savante d'être jouées et composées par des musiciens professionnels ». La musique populaire se distingue aussi de la musique savante (transmise par la partition) et de la musique traditionnelle (transmise par la tradition orale) par son rapport avec la technique phonographique, qui lui fait traverser l'histoire. Pour Simon Frith, elle se distingue aussi de la musique de masse, car si la musique populaire est « consommée d'une manière particulière clairement différenciée de celle des élites culturelles », l'adjectif « populaire » ne se confond pas avec celui de « masse » : « de nombreuses musiques populaires […] ont de plus faibles ventes […] que des enregistrements de musique classique à succès ».La musique populaire européenne a hérité de certains des usages de la musique modale, du système tonal et des instruments de la musique classique, mais la musique populaire de manière générale peut aussi se référer à d'autres genres musicaux et à d'autres traditions musicales. Par exemple la musique populaire japonaise se nourrit à la fois du jazz et de sa musique traditionnelle, et certains morceaux des Beatles empruntent aussi bien à la musique classique européenne qu'à la pop américaine ou à la musique traditionnelle indienne. La variété des genres et l'éclatement des frontières musicales qui caractérise la musique populaire la lient intimement à l'histoire de la mondialisation et de la révolution industrielle.Pour ses détracteurs, la musique populaire est assimilée à la culture de masse ou à la musique « commerciale ». C'est le cas de Theodor W. Adorno, qui a rendu célèbre le concept d'industrie culturelle et voyait dans les genres de musique populaire comme le jazz des simples modes ou produits commerciaux.Il a des racines très anciennes dans le chant traditionnel dit folklorique ou de folklore vivant, en France pour partie chanté en Breton, basque, provençal, corse, flamand, alsacien, etc. puis en français surtout à partir du XIXe siècle. Les thèmes des saisons, des amours, des âges de la vie, du mariage, des guerres et de la mort sont récurrents. Il accompagnait la vie de tous les jours, les travaux des champs et la garde des troupeaux par les enfants, les danses, les fêtes, etc.C'est une personne (homme ou femme), souvent anonyme, qui chante sur la voie publique, parfois associée au camelot. Ce chanteur vit de l'argent que ses auditeurs lui donnent. Des styles et modes particuliers existent selon les époques et les pays (ex : les prosopopées dites lamenti italiens composés et imprimés durant la Renaissance, de 1453 aux années 1630-1650 ; lamenti storici, parodiques, satiriques et musicaux). Parfois sans instruments, parfois muni d'un porte-voix, il cherche à attirer et captiver un maximum d'auditoire en un temps très court et s'appuie pour cela sur une musique mélodieuse, un air déjà connu et/ou un texte accrocheur, parfois politique et satirique, devant alors parfois se jouer de la police.Très populaires avant l'invention des médias modernes (radio, télévision, enregistrement sonore), ils ont largement contribué à la diffusion d'idées ou d'informations au même titre que les journaux. En effet, en dehors de quelques grands standards de la musique populaire, leur répertoire s'inspirait souvent de faits majeurs ou de faits divers remarquables, assurant une publicité à ces événements. Au XIXe siècle avec la révolution industrielle, l'apparition d'une classe ouvrière urbaine et son exode rural, elle contribue à porter et diffuser la chanson ouvrière et « sociale ».Habitués à se mettre en public dans des conditions difficiles, les chanteurs de rue avaient souvent une personnalité originale et extravertie. Au nombre de ceux-ci le célèbre Aubert (né vers 1769, attesté en vie en 1848), doyen des chanteurs des rues de Paris fut nommé par ses confrères « Syndic des chanteurs des rues » de Paris. En 1848, il parle au nom de la délégation de 800 chanteurs, musiciens et mendiants des rues de Paris venus rendre hommage à l'Élysée au chansonnier Béranger membre de la commission des secours.Le chanteur des rues a toujours fait partie des « Cris (et bruits) de la rue », mais le développement de la voiture et l'augmentation du volume sonore lié à la vie moderne, la difficulté d'occuper la voie publique, l'accusation de mendicité et surtout la banalisation des enregistrements sonores ont réduit la présence des chanteurs de rue. Il en reste malgré tout, y compris officiellement,.Le marchand de musique ou de chanson est une profession aujourd'hui disparue en Europe mais qui était encore active dans l'entre deux-guerres, avant la large diffusion de la radio puis de la télévision. C'est un métier connexe à la chanson populaire depuis plusieurs siècles (chanson autrefois spécifiquement éditée et diffusée sur feuilles volantes). Les marchands de musique étaient itinérants et vendaient des partitions de chant en entonnant eux-mêmes la musique. Ils parcouraient les villes et se déplaçaient de foire en foire. Ils proposaient leurs chansons sous forme de feuille volante, souvent grossièrement imprimée, à des personnes qui ne savaient globalement pas lire la musique, mais qui étaient intéressées par la mélodie ou par le texte de la chanson. Ces feuilles volantes étaient parfois également illustrées par des gravures, œuvres d'illustrateurs connus, intéressantes du point de vue artistique et iconographique. Certains marchands de musique ne déchiffraient pas les partitions, mais avaient une bonne mémoire des airs. Leurs feuilles volantes restent une mine d'information sur les idées, les coutumes et les centres d'intérêt des Européens au XIXe et au début du XXe siècle.Dès le début du XIXe siècle, les orphéons fédèrent les masses. Il s'agit d'abord de chorales d'enfants puis d'ouvriers. Quelques noms : Wilhem, pédagogue et fondateur du premier orphéon en 1833. Delaporte qui contribuera dans la seconde partie du XIXe siècle à donner une ampleur nationale au mouvement. À partir des années 1850, le terme « Orphéon » désigne les chorales, les fanfares et les orchestres d'harmonie qui ont connu un essor dû au développement de l'industrie des instruments de musique. Les héritiers actuels du mouvement sont le mouvement À Cœur Joie (chorales) l'UFF (fanfares) la CFBF (batterie-fanfare), la CMF (orchestres d'harmonie, orchestres à plectre).Au XIXe siècle, des centaines de goguettes rassemblent à Paris, dans sa banlieue et aux alentours des dizaines de milliers d'ouvriers ou journaliers, hommes ou femmes. Il en existe encore au siècle suivant. La goguette de la Muse rouge disparaît seulement en 1939.La musique populaire s'appuie sur quelques standards musicaux et commerciaux. Elle est aussi à l'origine d'un certain vocabulaire.Il s'agit essentiellement de chansons (des paroles soutenues par une musique instrumentale ou un petit chœur). Une chanson dure la plupart du temps entre 3 et 5 minutes (durée initiale de la face d'un disque 78 tours ou d'un vinyle 45 tours). Les textes utilisent le vocabulaire courant, voire familier. La musique est essentiellement tonale, écrite dans le mode majeur ou en mode mineur. Sa structure repose souvent sur une alternance entre un refrain et quelques couplets (en général, moins de cinq).L'ensemble, musique et paroles, est facile à mémoriser par écoute répétée. Elle s'efforce ainsi d'être facilement compréhensible et donc diffusable internationalement. À cet effet, on note une nette prédominance de l'anglais dans les paroles, au moins en ce qui concerne celle qui s'exporte massivement. La musique s'efforce de pouvoir être diffusée le plus largement possible : utilisation d'instruments courants (guitares, claviers, cuivres, cordes, percussions), arrangements musicaux standards, quasi-monopole de la langue anglaise pour les paroles de la version dite « internationale » sans pour autant éliminer toute forme de production nationale.Avant l'invention des médias audios modernes (radio, télévision, disques), la diffusion était assurée par des chanteurs de rue qui vendaient les partitions sur les marchés en entonnant eux-mêmes les chansons. La généralisation de la radio a favorisé l'émergence d'une diffusion sur les ondes par des chanteurs qui initialement interprétaient en direct puis se sont enregistrés. Actuellement, la diffusion est massive et se fait par ondes radio, par CD (on parle alors d'EPK), et par diffusion de clips vidéo au cours d'émissions de télévision, mais surtout sur YouTube, ou par des applications de musique numérique, comme Spotify, Deezer ou Apple Music.Un tube, ou « hit », est une chanson qui a particulièrement « bien marché », c'est-à-dire qu'elle a atteint des sommets de vente.Un disque d'or ou de platine récompense l'auteur d'une musique qui s'est bien vendue.Un hit-parade (en anglais : chart) est une compétition permanente de musique populaire organisée par des chaînes de radio ou de télévision. L'objectif est d'être no 1 (être « au top »), ce qui est théoriquement déterminé par le nombre de disques vendus ou par le vote des auditeurs. Plus longtemps une chanson est en tête du hit-parade, plus elle s'assure une large diffusion, favorisant les retombées commerciales.Il est notable que l'aspect commercial et promotionnel soit une caractéristique dominante de la musique populaire depuis la deuxième moitié du XXe siècle : première en termes de part de marché dans le monde de la musique, la musique populaire est l'objet d'enjeux commerciaux énormes pour les producteurs de musique, ce qui justifie l'emploi de méthodes commerciales poussées, identiques à celles utilisées pour les produits de consommation courante : méthodes dites des « grands lessiviers » : Procter & Gamble, Henkel, etc. C'est ainsi qu'une musique fait l'objet d'une « politique de lancement » pour toucher une « cible privilégiée », qu'on « fait la promotion » d'un nouveau chanteur en espérant que ses ventes « décollent », ou qu'on résilie le contrat d'un chanteur qui ne « se vend plus assez » ou dont le genre « arrive en fin de vie », quitte à le rappeler s'il « rebondit ». Les droits d'exploitation des musiques les plus populaires représentent une source importante de revenus que l'on ne cède pas facilement.La principale production de musique populaire est donc le résultat d'une politique visant à générer des profits. Ces enjeux commerciaux sont surtout le fait des grandes majors du disque (Universal, EMI, Sony, BMG). Les maisons de disques indépendantes (comme Tôt ou Tard, Naïve Records) à la diffusion plus limitée semblent être moins à la recherche de profits. Certains musiciens ne trouvant pas de maisons de disques « s'autoproduisent », mais ils bénéficient alors d'une distribution « classique » (vente de CD en magasins) et d'une visibilité réduite, bien que le développement d'Internet ait changé la donne au cours des dernières années ; on voit notamment des sites permettant de participer à la production d'artistes inconnus du grand public et des outils de diffusion comme MySpace ou autres,.Si l'enregistrement de musique en studio fait toujours appel à des professionnels, la musique populaire est la musique la plus jouée par des amateurs. De nombreux « groupes de garage » se créent dans le but de reprendre leurs musiques préférées à partir des enregistrements de leurs vedettes. Les plus talentueux et les plus constants pourront même arriver à jouer en public (soirées privées, clubs d'étudiants, bals…). Ce type de réinterprétation à partir des disques a remplacé le modèle de la musique traditionnelle fondé en grande partie sur la transmission par le jeu. De nombreux groupes de rock, de pop ou de jazz ont commencé par faire de la musique sous cette forme. Parmi les plus célèbres on peut citer les Beatles et les Rolling Stones.Le karaoké est également une forme de réinterprétation devenue courante : à partir d'un enregistrement de l'arrangement musical « réputé exact », un soliste au micro chante la mélodie. Très utilisé dans les soirées conviviales et exclusivement fondé sur des chansons à succès, le karaoké laisse une part d'interprétation au soliste. Actuellement, les musiciens amateurs peuvent profiter de la vulgarisation des outils d'enregistrement et de reproduction (stations de mixage, samplers, logiciels de mixage, graveurs de CD, etc.) pour autoproduire leur musique et n'hésitent plus à la diffuser, par Internet notamment.(en) Frans A. J. Birrer, « Definitions and research orientation: do we need a definition of popular music? » (Définitions et axe de recherche : avons-nous besoin d'une définition de la musique populaire ?), 1985, in D. Horn, ed., Popular Music Perspectives, 2 (Gothenburge, Exeter, Ottawa et Reggio Emilia), p. 99-106.Hugh Dauncey & Philippe Le Guern, Stéréo, Sociologie comparée des musiques populaires France-Angleterre, Bordeaux, IRMA / Éditions Mélanie Seteun, 2008.Marcello Sorce Keller, Contextes socioéconomiques et pratiques musicales dans les cultures traditionnelles, in Jean-Jacques Nattiez (general ed.), Musiques, Une encyclopédie du XXIe siècle, Volume 3 : Éd. Actes Sud / Cité de la musique, p. 559–592.(en) Marcello Sorce Keller, The Problem of Classification in Folksong Research: A Short History, Folklore, XCV(1984), no 1, 100- 104.Vassal, Jacques. Folksong [soi-disant]: une histoire de la musique populaire [en majeure partie] aux États-Unis. Nouv. éd. Paris : Éditions Albin-Michel, 1972, cop. 1971. 354 p.Volume! La revue des musiques populaires. La seule revue universitaire française exclusivement consacrée à l'analyse pluridisciplinaire des musiques populaires.Philippe Darriulat, La Muse du peuple : chansons politiques et sociales en France 1815-1871, Presses universitaires de Rennes, 2010R. Thérien, I d'Amours (1992), Dictionnaire de la musique populaire au Québec, 1955-1992, Lavoisier.B. Bartók (1948), Pourquoi et comment recueille-t-on la musique populaire ?, Impr. A. KundigC. D. Pessin (2004), Chanson sociale et chanson réaliste - Cités, avec cairn.infoMarie-Dominique Amaouche-Antoine, « Le cahier de chansons du conscrit » ; Revue d'histoire moderne et contemporaine (1954-) T. 34e, No 4 (oct. - déc., 1987), p. 679–685, Éd. : Société d'Histoire Moderne et Contemporaine ([URL : https://www.jstor.org/stable/20529337 1re page])Top 50Billboard magazineCatégorie:Classement musical Portail de la musique"
sport;"Le palet, la rondelle, le disque, le ou la puck est un disque rond et épais notamment utilisé en hockey sur glace et en roller in line hockey. Le joueur lance le palet vers le but de l’adversaire à l'aide d'une  crosse ou bâton de hockey.Le palet est fabriqué en caoutchouc vulcanisé de 2,54 cm d'épaisseur (un pouce) et 7,62 cm de diamètre (trois pouces). Il pèse entre 156 et 170 grammes (5,5 à 6 onces).Le palet de hockey sur glace fut créé en 1877 par William F. Robertson, en coupant une balle en deux, pour éviter les rebonds incessants vers les spectateurs.La vitesse maximale enregistrée d'un tir a été de 183,67 km/h par Aleksandr Riazantsev lors du 4e Match des étoiles de la Ligue continentale de hockey, en 2012.La chaîne de télévision Fox, dans le but de rendre les matchs de la Ligue nationale de hockey plus faciles à suivre à la télévision, inventa le FoxTrax, rondelle munie de diodes électroluminescentes.Au roller in line hockey, le palet est fait en plastique. Il mesure environ 2,5 cm d'épaisseur et de 7,62 cm de diamètre, et est légèrement rebondi en son centre ou muni de pastilles de roulement. Les rondelles de bonne qualité sont fabriquées de manière à rester toujours couchées sur le sol et à ne pas rouler sur leur tranche.Généralement noir, il peut aussi être orange, jaune, rouge, rose ou vert.Il existe de nombreuses formes de jeux de palets en Angleterre, en Espagne, en, France, au Portugal et en Vallée d'Aoste (Palet valdôtain).On peut diviser le jeu en deux grandes pratiques : le jeu de palet proprement dit qui consiste à lancer des palets le plus près possible d'un autre palet plus petit, préalablement lancé sur une surface délimitée (planche de bois, plaque de plomb) ou directement sur le sol (terre, route).Le jeu de galoche consiste lui à faire tomber un cylindre sur lequel on place la mise ou ce qui tient lieu de ""bouchon"" le plus près possible de ses palets.En Belgique francophone, en France et en Vallée d'Aoste, on utilise le terme « palet ».En Suisse romande, c'est le terme puck (au masculin) qui est le plus courant, prononcé autrefois « pok », mais le terme « palet » est aussi utilisé, ainsi que le terme « rondelle ».Au Canada français, on emploie les termes « rondelle », « disque » et puck (féminin ou masculin ; dans le langage familier).Le mot anglais puck est utilisé dans de nombreuses langues, dont l'allemand, le néerlandais et l'italien. Son origine est incertaine et ne semble liée ni à la mythologie, ni à Shakespeare ; l'Oxford English Dictionary suggère que le mot vient du verbe to puck (toucher ou frapper en français).Brittanie Cecil, une spectatrice de 13 ans d'un match de hockey, est morte à Columbus le 16 mars 2002 en recevant un tir du joueur professionnel norvégien Espen Knutsen. Désormais, toutes les patinoires internationales ont, au-dessus du plexiglas, un filet pour empêcher le palet de sortir.En France, un garçon de 8 ans est mort après avoir été heurté par un palet lors d'un match entre Dunkerque et Reims le 1er novembre 2014, dans une patinoire non équipée de vitres de plexiglas autour de la glace.(en) Cet article est partiellement ou en totalité issu de l’article de Wikipédia en anglais intitulé « Hockey puck » (voir la liste des auteurs).Cet article est partiellement ou en totalité issu de l'article intitulé « Jeux de palets » (voir la liste des auteurs). Portail du hockey sur glace"
sport;"Un sport de raquette est un sport où les participants utilisent des raquettes, qui servent à renvoyer vers l'adversaire un objet, le plus souvent une balle.En règle générale, ces sports utilisent le principe du duel et les adversaires se renvoient un objet à l'intérieur des limites d'un terrain. Il existe des variations sur l'objet renvoyé, comme le badminton où un volant remplace la balle. D'autres sports se rapprochent des jeux de raquettes, comme la balle au tambourin où le tambourin remplace la raquette ; certains sports anciens se jouent à main nue, tel initialement le jeu de paume dont la plupart des sports de raquette actuels sont plus ou moins dérivés.Les sports de raquette ont comme ancêtre commun le jeu de paume, inventé en France au XIIIe siècle, qui se jouait à l'origine à mains nues, puis avec des gants. Vers la fin du XVe siècle les gants sont renforcés avec une sorte de cordage, puis apparaissent des battoirs en bois. La première mention d'une raquette date du début du XVIe siècle. Les premières raquettes avaient un long manche et un cordage en boyau de mouton, pesaient environ 400 grammes et mesuraient 65 centimètres. Au XVIe siècle le jeu de paume devient le « jeu des rois », et François Ier, Henri II, Charles IX et Henri IV le pratiquent souvent. Les Anglais jouaient au jeu de paume avec des raquettes, et vers 1850 ils inventent le jeu de rackets qui se joue à plusieurs contre un mur et avec des raquettes en 15 points, mais ce jeu passe de mode; il réapparaîtra en 1924 sous le nom de squash. Le badminton apparaît en 1873 et l'histoire du tennis commence en 1874 avec le major Walter Clopton Wingfield qui en codifie les règles. C'est vers 1890 qu’apparaît le tennis de table. Les autres sports de raquette sont codifiés par la suite à partir du début du XXe siècle.Les sports de raquette font partie des activités qui contribuent à améliorer l'endurance cardiovasculaire. Ils ont comme caractéristiques communes de demander une acuité visuelle dynamique, une bonne perception de la profondeur, une perception périphérique du mouvement et une bonne coordination générale ainsi que le sens de l'anticipation.BadmintonTennisTennis de table (ou ping-pong)Beach ballBeach tennisJeu de paumeJeu de raquettesPadelRacketlonRacquetballSpeed-ballSpeed Badminton ou Speedminton et son équivalent nocturne le BlackmintonSquashTennis légerTouchtennisBalle au tambourinCrosse au champ (Lacrosse au Canada et aux États-Unis, La crosse au champ est au programme des Jeux olympiques d'été de 1904 à Saint-Louis et 1908 à Londres )Crosse en enclos (Lacrosse au Canada et aux États-Unis)Pelote basqueJokariFrescobolUltimate pingUn filet et des sports, Approches sociologique, historique, prospective, comportementaliste, Stéphane Méry, Logiques sociales, janvier 2008,  (ISBN 978-2-296-04632-0) Portail du sport   Portail des sports de raquette"
sport;"Le roller in line hockey, souvent désigné par le terme plus générique roller hockey, qui englobe également la discipline du rink hockey, ou par son nom anglais inline hockey, parfois abrégé RILH, est une des disciplines de hockey en patins à roulettes. Il se pratique avec des patins aux roues alignées (rollers en ligne), généralement en intérieur. Dans certains pays comme la France, le roller in line hockey est parfois appelé « street hockey » lorsqu'il n'est pas pratiqué en salle, bien qu'en toute rigueur ce terme désigne une variante spécifique, jouée avec une balle.Chaque match oppose deux équipes, composées d'un gardien de but et de 4 joueurs de champ présents sur le terrains (deux défenseurs et deux attaquants). L'objectif est de marquer plus de buts que l'autre équipe, en envoyant à l'aide d'une crosse (ou bâton de hockey) un disque en plastique, appelé rondelle ou palet, dans le but des adversaires, situé à l'extrémité du terrain à l'opposée de son propre but.Le roller in line hockey s'est développé à partir des années 1990 avec la popularisation des rollers en ligne. Largement inspiré du hockey sur glace, il s'en démarque notamment par l'interdiction des mises en échec (également interdites au hockey sur glace féminin), ce qui rend sa pratique nettement moins dangereuse, et un joueur en moins par équipe. Les contacts entre joueurs sont toutefois autorisés, contrairement au rink hockey.Au niveau international, deux fédérations gèrent le roller hockey, la Fédération internationale de roller sports (FIRS) et la Fédération internationale de hockey sur glace (IIHF pour International Ice Hockey Federation, son nom anglais). Chacune organise ses propres compétitions indépendamment et avec des équipes qui diffèrent (notamment les championnats du monde masculin, le championnat du monde féminin n'étant quant à lui organisé que par la FIRS). Le RILH fait partie des sports officiels des jeux mondiaux depuis l'édition 2005.Les premières images connues de hockey avec des rollers en ligne, au lieu des « quads » habituellement utilisés au rink hockey, datent de 1938. Elles sont filmées à Vienne (Autriche) et diffusées dans le film d'actualité Giornale Luce (it) B1401 du 3 novembre 1938. La vidéo montre un match, joué en extérieur sur une surface rectangulaire, visiblement en asphalte. Les joueurs portent des patins avec cinq roulettes plates en métal alignées et un frein à l'avant, et utilisent des crosses de hockey sur glace et une balle. Chaque équipe est composée de quatre joueurs et d'un gardien de but.Bien que plus rapides que les « quads », les rollers en ligne, considérés comme moins maniables, restent peu répandus jusqu'en 1993, quand les Hosers de San Diego deviennent la première équipe à remporter le championnat national américain en étant équipés uniquement de ce type de patins. La fédération américaine de roller sports (en), sous l'égide de la fédération internationale de roller sports (FIRS), organise à Chicago le premier championnat du monde en 1995, puis le premier championnat du monde junior l'année suivante, toujours à Chicago. Le premier championnat du monde féminin a lui eu lieu en 2002 à Rochester, dans l'État de New York. La fédération internationale de hockey sur glace (IIHF) organise un championnat du monde masculin, distinct de celui de la FIRS, à partir de 1996.Depuis 2005, le roller in line hockey fait partie des disciplines des Jeux mondiaux et depuis 2017 des World Roller Games.La crosse est constituée d'un manche mesurant au maximum 1,57 m, présentant un coude à sa base, entre le manche à proprement parler et la palette, large, plate et incurvée vers l'intérieur.Pour les joueurs, la longueur maximum d’une palette est de 32 cm tout en conservant une largeur comprise entre 5 et 9 cm.Pour les gardiens, la longueur maximum est de 39 cm et sa largeur à 13 cm.Le bâton de hockey peut être fait de différents matériaux : traditionnellement en bois, on en trouve en fibre de carbone, voire en aluminium, ou en plastique type PVC pour ceux bon marché.Le palet est un cylindre en plastique dur, d'environ 2,5 cm d'épaisseur et de 7,62 cm de diamètre, légèrement rebondi en son centre ou muni de pastilles de roulement. Les rondelles de bonne qualité sont fabriquées de manière à rester toujours couchées sur le sol et à ne pas rouler sur leur tranche.Les rollers utilisés sont dépourvus de frein. La platine en métal est courte et accueille quatre roues. Contrairement aux rollers utilisés pour le fitness, de structure droite et à semelle parallèle au sol, les roues sont généralement de tailles différentes (les deux roues arrière ont un plus grand diamètre que les deux roues avant, avec deux ou trois diamètres de roues par patins), ou alors le patin est légèrement incliné (talon plus haut que la pointe) afin de faciliter les changements brusques de direction et certaines prises d'appui. La chaussure montante est dure afin de protéger des coups de crosse et des tirs, et présente un évasement laissant à la cheville une mobilité accrue. Les rollers de gardien peuvent avoir cinq roues, plus petites et plus dures, pour une meilleure stabilité debout et une mise au sol rapide pour les arrêts, particulièrement pour les gardiens de style papillon.Le roller hockey est un sport avec rythme rapide et collectif, les contacts physiques entre les joueurs sont assez courants au cours d'un match ou d'un entraînement. C'est contacts peuvent entraîner des blessures et pour parer à cela, les joueurs utilisent différents équipements de protection. L'équipement utilisé par les joueurs est différents selon le sexe du joueur et différent de l'équipement du gardien de but.Les joueurs sont équipés d'un casque qui doit être une protection faciale intégrale. Il porte également une gaine, des jambières, des coudières, des gants et des rollers. Il est obligatoire d'avoir une tenue qui recouvre l'équipement excepté les gants, le casques et les rollers. Les tenues des joueurs de champ sont constituées de maillots identiques à manches longues et de pantalons longs.Détail de l'équipement de joueur : des jambières (obligatoires) qui protègent du haut du pied jusqu'au genou inclus. Elles protègent des coups de crosse, des jets de palets et des chutes.une coquille (obligatoire) qui protège essentiellement du palet et des crosses ; pour les femmes, une protection pelvienne est requise.des gants (obligatoire) qui possèdent une large ouverture pour permettre de plier le poignet. Le pouce doit contenir une tige rigide pour protéger des chutes. Identiques aux gants de hockey sur glace, ils sont articulés suivant la structure des phalanges, et contiennent des blocs mobiles de mousse dense protégeant des coups.un casque (obligatoire) avec une grille ou une visière intégrale. Seuls les seniors (nés avant le 1er janvier 1989) sont autorisés à ne pas porter de protection faciale intégrale (source : règles du RILH février 2008).des coudières (obligatoire) qui protègent des chutes et des coups de crosse sur les coudes et les avant-bras.une culotte (facultatif) qui protège le coccyx, les cuisses et les hanches.un protège-dents (facultatif mais recommandé si le casque n'a ni visière intégrale ni grille).un plastron, obligatoire pour les femmes au-dessus de 15 ans ; les épaulières rigides sont désormais interdites car jugées dangereuses lors des chocs accidentels épaule contre tête.L'équipement du gardien est plus spécifique : un gants appelé bouclier, un casque comprenant un protège cou obligatoire depuis la blessure de Clint Malarchuck, une crosse plus large, des jambières aussi appelé ""bottes"", un plastron épais, une gaine et une mitaine.L'équipement du gardien comprend :des bottes,une culotte protégeant les cuisses, les fesses, le coccyx et les hanches,un plastron protégeant le ventre, le torse et les bras,un casque, un protège cou,une mitaine : gant en cuir possédant une « poche » pour attraper le palet en l'air ou le bloquer au sol,un bouclier : gant en cuir protégé par une plaque rectangulaire rigide au-dessus de l'avant-bras,une crosse, plus courte que celles des joueurs et dont la moitié inférieure est plus large.Les tenues sont composées d'un pantalon long et d'un maillot à manches longues. Le numéro du joueur doit y être inscrit et être compris entre 00 et 99. Toutes les protections doivent être placées sous la tenue, à l'exception du casque, des gants et des bottes du gardien.La largeur du terrain est comprise entre 20 et 30 m et sa longueur entre 40 et 60 m avec un ratio de un sur deux. La FIRS préconise l'usage de cages en acier, dont les dimensions internes sont de 105 par 170 cm. Historiquement, le RILH se jouait avec des cages de hockey sur glace, mais lorsque la FIRS en a pris en charge la gestion, elle a choisi d'aligner la taille des buts sur celle du rink hockey, sa discipline historique. L'IIHF ne reconnait pas ce changement, et continue d'utiliser des cages de 122 par 183 cm, comme pour le hockey sur glace. Même parmi les compétitions estampillées FIRS, nombreuses sont celles qui persistent à utiliser des cages de hockey sur glace, particulièrement en Amérique du Nord, où le rink hockey est une pratique très marginale.Le terrain est entouré par des balustrades hautes de 1,06 m et comporte des marquages. Le marquage au sol réglementaire est similaire au hockey sur glace. Ce marquage doit comporter les éléments suivants :Une ligne au centre du terrain partageant le terrain en deux camps distinctsDeux lignes de but situés à 3,80 m de chaque extrémité de la pisteDeux zones de but, une pour chaque camp, rectangulaireCinq points d'engagement. Un point situé au centre du terrain entouré d'un cercle de 3 m de rayon. Deux fois deux points situés à 6,10 m de chaque ligne de but.D'une zone pour les arbitres, représentée par un demi-cercle de 3 m de rayon, située au bord du terrain, traversée en son centre par la ligne central.Le sol peut être fait dans un revêtement spécialement étudié pour le roller in line hockey, qui optimise l'adhérence des roues en polyuréthane. Il doit être rigide et le plus lisse possible pour obtenir une bonne glisse du palet.En France, les terrains sont en moyenne de l'ordre de 40 × 20 m et sont souvent partagés au sein de salles omnisports. De fait on trouve peu de surfaces « parfaites », mais des terrains recouverts de résine ou encore du béton peint très lisse. Ces surfaces sont tout de même très adhérentes et la rondelle peut glisser rapidement.À chaque match, deux arbitres doivent être présents sur le terrain. Ils doivent assurer la protection des joueurs en faisant respecter les règles.En France, contrairement au hockey sur glace, il n'y a pas de hors-jeu. En revanche le hors-jeu existe dans certains pays comme la Belgique. Les charges corporelles sont interdites, contrairement au hockey sur glace. Un joueur ou la crosse d'un joueur adverse ne peut pas être présent dans la zone du gardien.Il s'agit d'un sport collectif de glisse, très rapide, se jouant avec un palet, opposant deux équipes de 5 joueurs présents en même temps sur le terrain (1 gardien de but et 4 joueurs de champs). Une équipe peut comprendre au total 16 joueurs (14 joueurs et 2 gardiens) pouvant se remplacer à tout moment. Le match comprend, en Senior (divisions nationales) 2 périodes de 25 minutes effectives, avec une mi-temps de 10 minutes.Le gardien de but possède un statut particulier. En effet, s'il commet une faute il ne peut être directement sanctionné par une exclusion. Un joueur de l'équipe sera exclu à sa place. Suivant la gravité de la faute, l'exclusion sera plus ou moins longue. Le joueur exclu devra alors se mettre à côté du terrain et pourra rentrer, une fois le temps écoulé, lors du prochain arrêt de jeu.Les exclusions se classent en différentes catégories :mineure (2 min de prison) : trois pénalités mineures d'un même joueur lors d'un même match entraînent une pénalité additionnelle de méconduite de 10 minutes.majeure (5 min de prison) : deux pénalités majeures d'un même joueur lors d'un même match entraînent une pénalité majeure de 5 minutes d'un autre joueur de l'équipe et une pénalité de méconduite pour le joueur concerné pour le match.de méconduite (10 min de prison)de méconduite pour le match : le joueur fautif est exclu du match et du suivant, mais est remplacé par un autre joueur.Deux pénalités de méconduite d'un même joueur lors d'un même tournoi entraînent une pénalité de match.pénalité de match : le joueur est exclu du match et du reste du tournoi. Un autre joueur de l'équipe fera 5 min de prison.Au niveau international, deux instances gèrent ce sport : l'IIHF, fédération internationale de hockey sur glace et la FIRS, fédération internationale de roller sports.Chacune de ces fédérations possède son propre règlement, organise ses propres championnats du monde et est reconnue par certains pays mais pas d'autres. En règle générale, les pays où le hockey sur glace est particulièrement développé (comme la Suède ou la Russie) reconnaissent l'IIHF comme instance internationale (et ne participent donc pas aux championnats organisés par la FIRS).En France, la Fédération française de roller sports est, elle, affiliée à la FIRS. Dans d'autres pays, par exemple en Belgique, les deux fédérations sont présentes, les clubs reconnaissants l'une ou l'autre fédération.(en) Cet article est partiellement ou en totalité issu de l’article de Wikipédia en anglais intitulé « Roller in-line hockey » (voir la liste des auteurs).Roller ou patin à roulettesClub de roller in line hockey | Compétition de roller in line hockey | Joueur de roller in line hockeyRoller in line hockey en FranceChampionnat de France de roller in line hockeyComité international de roller in line hockey, affilié à la Fédération internationale de roller sports (FIRS)Roller Hockey & Fun (RHAF), site d'information français, plus mis à jour depuis 2014World Inline Hockey Association (WILHA), association de promotion du roller-hockeyLigueElite.fr, site d'actualités sur la Ligue Elite de roller-hockey en France, affilié à la Fédération française roller sportsRollerhockeyfeminin.fr, site d'actualités sur le roller-hockey en France, indépendantLes Archives du roller-hockey, site d'archives des résultats et classement, indépendant Portail du sport   Portail du roller et du skateboard"
sport;"Le sport, d'usage récent (XIXe siècle) dans la langue française, est un ensemble d'exercices physiques se pratiquant sous forme de jeux individuels ou collectifs pouvant donner lieu à des compétitions.Le sport, qui ne peut être dénommé ainsi avant le XIXe siècle (gymnastique, exercice physique, voir Pociello, C. Sociologie du sport, 1983), est un phénomène presque universel dans le temps et dans l'espace humain. La Grèce antique, la Rome antique, Byzance, l'Occident médiéval puis moderne, mais aussi l'Amérique précolombienne ou l'Asie, sont tous marqués par l'importance de pratiques physiques. Certaines périodes sont surtout marquées par des interdits.Le terme de « sport » a pour racine le mot de vieux français desport qui signifie « divertissement, plaisir physique ou de l'esprit ». Antérieurement il peut être rattaché au latin portus, comme dans transport, export, import, déporter, déport, etc., qui désignait simplement un port, un lieu de passage marin. En traversant la Manche, desport se mue en « sport » et évacue de son champ la notion générale de loisirs pour se concentrer sur les seules activités physiques et mentales. La langue allemande admet aussi le terme « sport » et sa définition anglaise en 1831 ; la France en fait usage pour la première fois dès 1828, mais à ce moment-là il est essentiellement associé aux courses de chevaux et aux paris sur ces courses de chevaux (Cf. le journal Le Sport). La frontière entre jeux et sports n'est pourtant pas très claire. La Fédération française des échecs fondée en 1921 reçoit ainsi un agrément sportif du Ministère de la Jeunesse et des Sports en 2000, mais uniquement parce qu'elle était une fédération « associée » au CNOSF. Certaines pratiques traditionnelles posent également problème : sport ou jeu ? La question reste encore ouverte.Le sport moderne se définit par quatre éléments indispensables :la mise en œuvre d'une ou plusieurs qualités physiques : activités d'endurance, de résistance, de force, de coordination, d'adresse, de souplesse, etc ;une activité institutionnalisée, ses règles tendent à être identiques pour l'ensemble de la planète ;une pratique majoritairement orientée vers la compétition ;une pratique fédérée (sous la tutelle d'une fédération sportive).Ces piliers qui mettent surtout en avant l'organisation des différentes disciplines sportives n'excluent nullement les pratiques comme le sport-loisir, le sport-aventure, le sport-santé, le sport scolaire ou l'éducation physique et sportive. Si la compétition est prédominante, il existe toutefois d'autres formes de pratique mettant plutôt en avant le plaisir, la santé, l'éducation ou l'épanouissement.Le Conseil de l'Europe propose ainsi la définition suivante dans sa « Charte européenne du sport » (article 2.1) (2001) : « On entend par « sport » toutes formes d'activités physiques qui, à travers une participation organisée ou non, ont pour objectif l'expression ou l'amélioration de la condition physique et psychique, le développement des relations sociales ou l'obtention de résultats en compétition de tous niveaux ».La question de l'histoire du sport bute sur un débat qui oppose deux thèses.Pour un courant de pensée, le sport est un phénomène universel, qui a toujours existé et partout sous des formes très diverses. Ce serait un « invariant culturel » (selon les termes de Frédéric Baillette, enseignant et directeur de la revue Quasimodo). Cette thèse est notamment soutenue en 1991 par le médecin français Jean-Paul Escande (Les avatars du sport moderne, in Ardoino, Brohm, Anthropologie du sport, Perspectives critiques, 1991). Cette thèse est implicitement soutenue par ceux qui parlent de « sport antique », de « sport médiéval », etc. Le médiéviste américain Charles Homer Haskins est le premier historien à utiliser le terme de « sport » dans le cadre d'une étude portant sur le Moyen Âge dans son livre The Latin Litterature of Sport (1927). Au début du XXIe siècle, Wolfgang Decker (Institut d'Histoire du Sport de l'École Supérieure du Sport de Cologne) et Jean-Paul Thuillier (directeur du Département des Sciences de l'Antiquité à l'École normale supérieure) estiment que : « contrairement à ce que l'on estime souvent, le sport n'est pas né à Olympie, pas plus qu'il ne s'est éteint dans l'Attique ou le Péloponnèse. L'Égypte nous offre de nombreuses scènes sportives, entre autres de lutte, dès le IIIe millénaire avant notre ère, et les Romains, héritiers des Étrusques sur bien des points et en particulier dans ce domaine, ont peut-être créé le sport moderne, avec ses spectacles de masse, ses clubs puissants et ses enjeux financiers colossaux ».Pour un autre courant de pensée, le sport est un phénomène apparu à un moment précis de l'histoire et dans un contexte particulier : au sein de l'élite sociale de l'Angleterre industrielle du XIXe siècle. Cette thèse est notamment développée en 1921 par l'écrivain allemand Heinz Risse (Soziologie des Sports, Berlin, 1921 et Sociologie du sport, Presses universitaires de Rennes, 1991) qui estime qu'« il est erroné de regarder le passé avec nos modes de pensée actuels et d'imaginer que les pratiques qui ressemblent à celles que nous connaissons peuvent se rapporter à cette appellation ""sport"" ». Cette thèse est notamment soutenue par l'historien français Roger Chartier et par les sociologues Norbert Elias, et Pierre Bourdieu,. En 2000, l'historien du sport Philippe Lyotard (université de Montpellier) juge qu'« il y a une coupure très nette entre le sport moderne et le sport antique : c’est la notion de record (et donc de performance). Le record et la performance expriment une vision du monde qui est profondément différente entre les Grecs et les modernes. La culture du corps est différente. Pour les Grecs, cette culture est rituelle, culturelle, d’inspiration religieuse, pour les modernes, le corps est une machine de rendement ».À travers l'exemple des joutes au XVe siècle en France et en Espagne, Sébastien Nadot avance dans sa thèse intitulée Joutes emprises et pas d'armes en Castille, Bourgogne et France, 1428-1470 (soutenue à l'EHESS en 2009) que l'on peut effectivement parler de sport au Moyen Âge et que la plupart des historiens confondent la notion de naissance avec celle de démocratisation du sport quand ils évoquent son apparition seulement à partir du XVIIIe siècle. Mais une autre façon de résoudre la question est de forger la notion de « sport moderne » pour distinguer ce phénomène d'autres pratiques historiquement attestées. Dans une étude, une équipe de l'UFR-STAPS de l'université de Bourgogne estime ainsi en 2004 que « Le sport moderne [...] renvoie à l’idéologie de Coubertin, caractérisée par la compétition, la performance, l’entraînement dans des structures institutionnelles (fédérales et scolaires) afin de lutter contre l’oisiveté et les risques de dégénérescence psychologique et physiologique de l’homme ». Cette notion de « sport moderne » est exposée par l'historien américain Allen Guttmann dans From Ritual To Record, The Nature of Modern Sports (1978). Auteur notamment de Sports: The First Five Millennia, Guttmann ne renonce pas à l'emploi du mot « sport » de l'Antiquité à nos jours.Selon l'interprétation large de la notion, le sport est un phénomène universel dans le temps et dans l'espace humain, et, pour reprendre une maxime byzantine, « les peuples sans sport sont des peuples tristes ». Nombre de phénomènes qui paraissent récents, accompagnent en fait l'histoire du sport depuis l'origine : du professionnalisme au dopage, des supporters aux problèmes d'arbitrage.La Grèce, Rome, Byzance, l'Occident médiéval puis moderne, mais aussi l'Amérique précolombienne ou l'Asie, donnent tous une importance au sport. Certaines périodes sont surtout marquées par des interdits concernant le sport, comme c'est le cas en Grande-Bretagne du Moyen Âge à l'époque Moderne. Interrogée sur la question, la Justice anglaise tranche ainsi en 1748 que le cricket n’est pas un jeu illégal. Ce sport, comme tous les autres, figurait en effet sur des édits royaux d'interdiction régulièrement publiés par les monarques britanniques du XIe  au  XVe siècle. En 1477, la pratique d'un « jeu interdit » est ainsi passible de trois ans de prison. Malgré l'interdit, la pratique perdure, nécessitant un rappel quasi permanent à la règle.Le sport est l'une des pierres d'angle de l'éducation humaniste du XVIe siècle. Les Anciens mettaient déjà sur le même plan éducation physique et intellectuelle. Pythagore était un brillant philosophe qui fut également champion de lutte puis entraîneur du grand champion Milon de Crotone. La Renaissance redécouvre les vertus éducatives du sport et, de Montaigne à Rabelais en passant par Girolamo Mercuriale, tous les auteurs à la base du mouvement humaniste intègrent le sport dans l'éducation (relire par exemple Gargantua). Chaque époque a eu son « sport-roi ». L'Antiquité fut ainsi l'âge d'or de la course de chars. Pendant plus d'un millénaire, les auriges, cochers des chars de course, étaient des « stars » adulées par les foules dans tout l'Empire romain. Le tournoi, qui consiste à livrer une véritable bataille de chevaliers, mais « sans haine », fut l'activité à la mode en Occident entre le XIe et le XIIIe siècle (il ne faut pas confondre le tournoi et la joute équestre, version très allégée du tournoi). La violence du tournoi cause sa perte, d'autant que le jeu de paume s'impose dès le XIIIe siècle et jusqu'au XVIIe siècle comme le sport roi en Occident. Ce jeu de raquettes embrase Paris, la France puis le reste du monde occidental. Le XVIIIe siècle voit le déclin du jeu de paume et l'arrivée, ou plutôt le retour, des courses hippiques qui s'imposent comme le sport roi des XVIIIe et XIXe siècles. La succession des courses hippiques fut âprement disputée car le nombre des sports structurés augmente spectaculairement dès la fin du XIXe siècle. Le football devient ensuite et reste encore aujourd'hui (2018) l'incontestable sport « numéro un » sur la planète.Au-delà de ce tableau général coexistent des nuances régionales parfois très marquées. Ainsi, le football tient une place secondaire dans les pays de l'ancien empire britannique. En revanche, il cultive les autres sports que soutenait jadis la bonne société anglaise, du tennis au hockey sur gazon en passant par le rugby et le cricket. Le cricket a ainsi le statut national dans des pays comme l'Inde ou le Pakistan. De même, l'Amérique du Nord a donné naissance à plusieurs sports, le hockey sur glace et le basket-ball au Canada, le baseball et le football américain aux États-Unis, parvenant ainsi à échapper à la vague du football (appelé soccer en Amérique du Nord). En France, le sport roi de la fin du XIXe siècle est le cyclisme qui garde la palme jusqu'au triomphe du football, entre les deux guerres mondiales. Le rugby n'est pas parvenu à mettre fin à la domination de ces deux sports, freiné par une implantation trop régionale.La puissance du mouvement sportif est aujourd'hui considérable, il est une des composantes de la mondialisation. Une fédération internationale comme la FIFA a la capacité de modifier les règlements et d'exiger sa mise en application à la planète entière. Certains ont donc pu estimer que le sport proposerait ainsi un premier modèle de mondialisation réelle.À l'inverse de cette structure centralisée, notons l'existence d'un mouvement sportif plus indépendant, notamment aux États-Unis. La NBA a des règles particulières distinctes de celles de la Fédération internationale de basket-ball, sauf pour les Jeux olympiques pour lesquels c'est la FIBA qui est chargée des épreuves. Le baseball américain illustre encore plus fortement cette décentralisation : les deux ligues qui s'affrontent pour le trophée des World Series - Ligue américaine et Ligue nationale - ne suivent pas les mêmes règles du jeu.La liste suivante regroupe les sports les plus connus, classés par catégories usuelles. D'autres sports pourraient compléter cette liste. Certains sports peuvent appartenir à plusieurs catégories. La présence des catégories « sports mécaniques » et, plus récemment, « sports cérébraux » dans cette liste, longtemps contestée, se justifie par les qualités communes aux sports physiques qu'ils demandent, pratiqués à haut niveau de compétition, comme en particulier la concentration ou l'endurance.La plupart de ces sports ont leur équivalent pour les personnes handicapées : les handisports.Les Jeux olympiques sont une compétition internationale qui regroupe une sélection de disciplines sportives. Ainsi, il est possible de classer les sports entre ceux qui sont inscrits aux Jeux olympiques, dits « Sports olympiques » et ceux qui le sont pas.Un sport ne peut être olympique que s'il fait partie d'une fédération internationale reconnue par le Comité international olympique (c'est-à-dire qui répond à de multiples critères de sélection très stricts). Celles-ci sont alors divisées en trois groupes :les fédérations internationales parmi lesquelles au moins un des sports dont elles ont la gouvernance se trouve inscrit au programme des jeux olympiques d'été (ASOIF, Association of Summer Olympic International Federations) ;les fédérations internationales parmi lesquelles au moins un des sports dont elles ont la gouvernance se trouve inscrit au programme des jeux olympiques d'hiver (AIOWF, Association of International Olympic Winter Sports Federations) ;les fédérations internationales reconnues par le Comité international olympique n'ayant actuellement aucun des sports dont elles ont la gouvernance aux programme des jeux (ARISF, Association of IOC Recognised International Sports Federations).Les sports olympiques actuels furent tous inclus au programme des jeux à un moment spécifique de l’histoire, au cours d'une décision commune prise entre le CIO et les fédérations internationales. Une fois qu'un sport a été désigné comme sport olympique, il ne peut plus être retiré des programmes des jeux (mais le nombre d'épreuves qui composent ce sport peut être revu à chaque édition des jeux), sauf si la fédération internationale qui dirige ce sport est radiée de la liste ASOIF ou AIOWF auquel cas tous les sports qui dépendent d'elle sont radiés du programme (comme ce fut le cas après les jeux de Pékin en 2008 pour la fédération de baseball-softball, la WBSC).D'autres sports peuvent devenir olympiques à l'avenir sous trois conditions :soit le CIO décide d'augmenter le nombre d'épreuves pour l'une des éditions, et un nouveau sport est proposé par une fédération afin d'occuper un certain nombre de ses épreuves (rentrant ainsi en concurrence avec les sports olympiques dont les fédérations internationales souhaitent inscrire plus d'épreuves aux jeux) ;soit le CIO décide de radier une fédération des listes ASOIF ou AIOWF et propose à d'autres fédérations de proposer de nouveaux sports à la place ;à partir des jeux de Tokyo en 2020, le CIO propose pour toutes les futures éditions des jeux d'été que des sports additionnels soient rajoutés au programme juste pour le temps de l'olympiade à venir, le tout au choix de la ville organisatrice. Un sport peut ainsi devenir olympique le temps d'une édition des jeux (où plusieurs si la fédération gouvernant ce sport parvient à la placer dans différents futurs jeux).Si la candidature d'un nouveau sport d'une fédération ARISF est acceptée, la fédération internationale en question prend aussitôt le statut de ASOIF ou AIOWF, même si cette dernière n'a pas pu imposer aux jeux tous les sports dont elle a la gouvernance (dans le cas des fédérations contrôlant plusieurs sports différents). Une fédération déjà ASOIF ou AIOWF qui disposerait de sports non olympiques sous sa gouvernance peut également poser leur candidature pour de futurs jeux.Le tableau ci-dessous présente par ordre alphabétique les sports olympiques et les sports non olympiques par fédérations reconnues par le CIO.Nombre de fédérations ne sollicitent pas la reconnaissance du CIO (sport automobile, notamment) tandis que d'autres sont en phase de reconnaissance par le CIO.La pratique équilibrée d'un sport aide à se maintenir en bonne santé physique et mentale. En revanche, le surmenage sportif et l'absence totale d'exercice physique sont nocifs pour la santé.La pratique du sport régulier maintient notre organisme en bonne santé, réduit le stress et augmente la capacité de réflexion.La pratique d'un sport se décompose en trois types d'activités : l'entraînement sportif, la compétition et la récupération.L'entraînement a pour objectif de former et d'entraîner le pratiquant pour que ses performances augmentent. Pour être bénéfique, l'entraînement doit être réparti sur une succession de séances régulières, progressives et complémentaires les unes aux autres.La compétition a pour objectif de mesurer les sportifs entre eux et de récompenser les meilleurs. Pour de nombreux sportifs, la compétition est le moment le plus fort et le plus agréable de la pratique du sport.Enfin, la pratique d'un sport comprend des phases de récupération et de détente. L'objectif de ces séances est de laisser au corps de l'athlète le temps et le repos nécessaire pour qu'il se remette en état de produire les meilleurs efforts.Chaque discipline fait appel à des compétences sportives particulières.L'équilibre, la force, la motricité, la vitesse, l'endurance, la concentration, le réflexe, la dextérité sont les compétences les plus connues. Certaines disciplines font plutôt appel à une seule compétence alors que d'autres font appel à un éventail de plusieurs compétences. Hormis les compétences sportives, il existe des facteurs physiques déterminants de la performance sportive, ces facteurs sont la force, la vitesse, l'endurance, la souplesse et la coordination des unités motrices (intra et intermusculaire+proprioception).Le succès dans une discipline dépend de la capacité du sportif à exécuter un geste précis. Certaines disciplines consistent à exécuter le geste le plus précis possible en disposant de tout le temps nécessaire à la préparation du geste. Le tir à l'arc est un exemple de ce type de disciplines. D'autres disciplines laissent peu de temps de préparation et le sportif doit ici exécuter son geste de manière spontanée. Le karaté est exemple de ce type de disciplines.La pratique d'un sport fait travailler le système cardio-respiratoire et différents muscles. Elle permet de brûler des calories et donc de prévenir de l'obésité (prévention de l'obésité). Elle incite à avoir une alimentation correcte (alimentation du sportif). Elle facilite l'évacuation de la tension nerveuse accumulée dans la journée (ex : stress chez l'humain). Elle permet la découverte du corps[C'est-à-dire ?] et de ses limites. Elle facilite l'acquisition du sens de l'équilibre, soit dans des situations prévues (exercices de gymnastique), soit dans des situations imprévues (jeux de ballon, sports de combat). Il permet aussi au pratiquant de construire une méthodologie du travail, réutilisable pour d'autres disciplines.Il est recommandé de pratiquer un sport d’intensité moyenne ou, plus simplement, d’exercer une activité physique pendant un temps allant de 50 min à 1 h 30 min si l'on veut avoir un effet sur le maintien ou l'abaissement de son poids, au moins trois fois/semaine. Une étude de l'ANSES en 2020 révèle que « 95% de la population française adulte est exposée à un risque de détérioration de la santé par manque d’activité physique ou un temps trop long passé assis ». Toujours selon cette enquête, 5% des adultes en France ont une activité physique suffisante pour protéger leur santé : les femmes sont plus exposées que les hommes à un manque d’activité physique. Plus d’un tiers des adultes français cumule un haut niveau de sédentarité et une activité physique insuffisante : en conséquence, ils sont plus exposés au risque d’hypertension ou d’obésité et ont un taux de mortalité et de morbidité plus élevés causés par des maladies cardiovasculaires et certains cancers.La marche est l'activité physique la plus pratiquée par un très grand nombre d'adultes et de personnes âgées.Une grande étude taïwanaise publiée en 2011 dans le journal médical The Lancet, a montré qu'une activité physique modérée de quinze minutes par jour ou quatre-vingt-dix minutes par semaine pouvait diminuer la mortalité globale de 14 % contribuant ainsi à une augmentation de l'espérance de vie de trois ans.Le sport donne lieu à la manifestation d'émotions intenses, souvent surmédiatisées. Elles effacent un corps contraint, policé par les heures d'entraînement. Ces émotions se déclenchent lors des confrontations ou des résultats de l'action. Il est exigé du sportif qu'il évacue les émotions dites négatives, tandis qu'il doit vivre à l'excès, en communion avec le public, les émotions dites positives. Cependant cette vision binaire rend difficile la compréhension des émotions humaines. L'univers sportif dilate les enjeux et l'émotion devient si forte qu'elle sort du corps, par effraction en quelque sorte. Au sortir de l'exercice solitaire et fort, elle restaure le lien social, par elle le héros ou l'héroïne retrouve une place dans la société. La pratique intensive du sport induit une difficulté permanente à exprimer ses émotions, une alexithymie, et ces moments sont les seuls prévus pour se libérer et, les mots manquants, le corps surjoue les expressions. Mais bien vite, le star-système impose sa loi, et exige de revenir à des manifestations codifiées, alors qu'il serait plus bénéfique pour tout le monde d'aider la personne à déchiffrer ses émotions.La pratique du sport présente des risques. Le sportif peut se blesser en faisant un faux mouvement, en chutant (entorse, élongation musculaire, claquage, fracture osseuse, traumatisme crânien) ou en recevant un coup. Il peut être victime d'un accident cardiovasculaire (du type infarctus du myocarde).Certains sports présentent des risques réels d'accidents corporels graves, tels que le traumatisme crânien ou la noyade, et leur pratique n'est autorisée qu'avec un équipement adapté, tels que : gilet de sauvetage pour le canoë, casque pour la descente en VTT, harnachement complet pour le gardien de hockey sur glace. Certains sports dits « extrêmes » présentent même de tels risques d'accidents mortels que leur pratique en est interdite.L'activité sportive intensive est source de blessures graves qui peuvent contraindre le sportif à s'arrêter et qui peuvent laisser des séquelles. La pratique d'un sport doit être adaptée à l'âge du pratiquant et à son état de fatigue. Une personne peut être marquée à vie par une activité sportive trop intense dans son enfance. Un sportif peut être obligé d'arrêter la pratique de son sport à la suite de séances d'entraînement ou de compétitions trop dures et trop fréquentes. La gymnastique artistique est l'exemple d'une discipline où de jeunes sportifs sont soumis à des exercices dangereux pour leur santé.La meilleure prévention contre les accidents consiste à pratiquer un sport dans les règles de l'art qui lui sont applicables : apprentissage des gestes techniques, apprentissage des règles de bonne pratique et de sécurité, entraînement régulier, échauffement préalable aux exercices violents, port des protections recommandées, alimentation adaptée avant, pendant et après l'effort, récupération entre les séances d'entraînement et entre les compétitions, respect des interdictions liées aux conditions météorologiques, pratique en groupe, etc. Des pratiques sportives de compensation sont largement recommandées dans le concept d'ergomotricité initié sur les lieux de travail pour lutter contre les accidents du travail. La visite médicale annuelle en début de saison permet d'obtenir l'avis d'un spécialiste sur la capacité d'un individu à pratiquer un sport.Le refus de poursuivre un effort qui semble trop difficile à supporter est un geste de sauvegarde de sa santé. Tels sont les principaux moyens de prévention des accidents. Le dopage est un des risques pour la santé du sportif. Ce problème n'est toutefois pas spécifique au sportif.Le dopage consiste à utiliser des produits qui augmentent la performance par différents moyens tels que l'augmentation de la masse musculaire ou la résistance à la douleur. L'EPO est un exemple de produits dopants.Le dopage est une pratique de certains sportifs professionnels de haut niveau mais également de certains sportifs amateurs. Le dopage est efficace : il permet en général à ceux qui se dopent d'obtenir des performances supérieures à ce qu'elles seraient sans dopage. Le dopage est illicite : le sportif convaincu de dopage est sanctionné. Le dopage est dangereux pour la santé du sportif : certains décès de sportifs pourraient être la conséquence d'un dopage.La lutte et la prévention antidopage existent. Elles concernent tout le monde et, au tout premier plan, les sportifs, leur entourage professionnel et les organisateurs de compétitions. Les contrôles antidopage permettent de déterminer si le sportif a ou n'a pas été dopé pour obtenir son résultat dans la compétition. La déchéance d'un titre et l'exclusion à vie de toute compétition sont des exemples de sanctions.Les sports où les cas de dopages sont les plus connus du grand public sont le cyclisme, l'athlétisme, la natation et l'haltérophilie.Un numéro de la revue International Journal Of Sport Science and Physical Education fait le point sur le problème du dopage dans le sport. On y voit notamment le fait que les médecins du sport sont largement impliqués dans ce problème notamment dans les pays anglo-saxons. On voit également que la loi existante n'est pas adaptée au problème puisqu'en général les seuls punis sont les athlètes ou coureurs, alors que la plupart du temps c'est un système complexe et que tout l'entourage est impliqué voire dans certains cas (Tour de France cycliste), il s'agit pratiquement d'une tradition liée à l'activité qui donne lieu à un véritable rituel initiatique (lié aux pratiques dopantes) pour les participants. Des articles sont également parus sur le sujet dans la Éthique publique (Canada) et dans la Revista brasileira de ciencas do esporto (Brésil). Le dopage y est analysé notamment par Eric Perera comme associé à la pollution du corps, aux notions de pur et d'impur. Les travaux de l'anthropologue Mary Douglas (Purity and Danger. An analysis of the concept of pollution and taboo, 1965) servent de références pour mieux comprendre ce problème. Température Selon que la température est trop ou pas assez élevée, l'organisme peut être soumis respectivement à l'hyperthermie ou à l'hypothermie.En cas de température élevée, on portera des vêtements légers en textile adapté, mais continuant à protéger du soleil, surtout en altitude ou en cas de canicule. Le rendement sportif pourra être maintenu grâce à l'utilisation d'un gilet réfrigérant.En cas de basse température, on utilisera au contraire des vêtements chauds, en particulier pour les extrémités (doigts, orteils...) qui pourraient facilement subir de graves gelures: c'est le cas typique de l'alpinisme d'altitude ou hivernal. Les nageurs en eau froide se trouvent eux aussi soumis à l'hypothermie d'autant plus rapidement que la température de l'eau est basse; préventivement, ils utilisent une combinaison de plongée ou enduisent leur corps de graisse.Quelle que soit la température, le vent ou la vitesse du sportif augmentent considérablement les échanges thermiques par convection: aussi, les sportifs qui sont dans ce cas augmenteront particulièrement les précautions.De même, l'humidité accélère considérablement les échanges thermiques, rendant beaucoup plus difficiles à supporter les froids et chaleurs humides que secs. Pression Une pression trop basse ne permet pas de respirer convenablement, alors que tout sportif a besoin d'échanges respiratoires élevés pour être performant, ou simplement survivre. Ces limites s'observent pour l'alpinisme d'altitude, où la pression atmosphérique en haut de l'Everest n'est qu'environ 1/3 de la pression au niveau de la mer considérée comme pression normale pour vivre par la plupart de l'humanité (le rendement est fortement dégradé pour tous, et beaucoup d'alpinistes se voient obligés d'utiliser des bouteilles d'oxygène sur les sommets de plus de 8 000 m, et un caisson isobare en cas de malaise aigu ou d'accident). Il est bien connu que l'altitude a un impact sur les compétitions sportives en altitude, comme ce fut par exemple le cas aux Jeux olympiques de Mexico.Pour des raisons inverses, le plongeur ne peut descendre à trop grande profondeur sans équipement (scaphandre autonome avec pression régulée).Le sport se pratique durant le parcours scolaire, au travers de multiples APS, au sein d'un club soit hors de tout club. Les clubs sont affiliés à des fédérations. Les clubs organisent les entraînements et mettent leurs moyens à la disposition des compétitions. Les fédérations organisent les compétitions et édictent les règlements.La grande majorité des sportifs est composée de sportifs amateurs, c'est-à-dire d'hommes et de femmes qui pratiquent leur activité sans recevoir aucun salaire en retour. L'amateurisme possède son revers avec un accès limité aux classes populaires. pour certaines activités et l'amateurisme marron, c'est-à-dire la rémunération occulte ou la fourniture d'emplois de complaisance à des sportifs officiellement amateurs.Certains sportifs perçoivent un salaire en retour de leur activité. Ces sportifs sont dits « professionnels ». La plupart d'entre eux sont sous contrat avec un club. Le football en Europe et le basket-ball aux États-Unis sont deux exemples connus de sports pratiqués par des professionnels. Depuis le début des années 1990 et la professionnalisation des Jeux olympiques, longtemps bastion du sport amateur, le phénomène du professionnalisme sportif touche presque l'ensemble des disciplines.La puissance du mouvement sportif est aujourd'hui considérable. Une fédération internationale comme la FIFA a la capacité de modifier les règlements et d'exiger la mise en application à la planète entière à compter d'une date précise. Le sport propose ainsi un modèle de mondialisation[réf. souhaitée].À l'inverse de cette structure centralisée, notons l'existence d'un mouvement sportif plus indépendant, notamment aux États-Unis. La NBA a des règles particulières et il n'est pas question pour elle de se plier à la Fédération internationale de basket-ball. Aux Jeux olympiques, la FIBA est néanmoins chargée du règlement des épreuves, et les joueurs NBA doivent alors y jouer selon les règles communes au reste du monde. Le baseball américain est encore plus caricatural sur ce point, avec deux ligues qui s'affrontent pour le trophée des World Series : l'American et le National n'ont pas les mêmes règles du jeu.Voici une liste des principaux grands évènements sportifs. Cette liste n'est pas exhaustive.Événements internationauxChampionnat du monde de basket-ballChampionnat d'Europe de footballCoupe de l'AmericaCoupe du monde de cricketCoupe du monde de footballCoupe du monde de rugby à XVJeux olympiques d'étéJeux olympiques d'hiverJeux mondiauxLigue des champions de l'UEFARyder CupTournoi des Six NationsÉvénements nationauxLes compétitions sportives sont des formes de spectacles, mais leur scénario n'est pas écrit d'avance. Pendant l'Antiquité, la sculpture ou la poésie furent de bons vecteurs de médiatisation du sport. Avec l'arrivée des médias modernes avec dans l'ordre chronologique la presse écrite, la radio, la télévision puis internet, le sport dispose de puissants supports médiatiques. Ainsi, il existe depuis 1977 des chaînes de télévisions sportives dont l'objet sont la diffusion d'épreuves et d'informations sportives. Certaines sont généralistes et se consacrent à divers sports tandis que d'autres se spécialisent dans une discipline. Parmi les titres de la presse écrite sportive on citera L'Équipe en France, Sports Illustrated aux États-Unis ou La Gazzetta dello Sport en Italie, notamment.Dans certains sports, la médiatisation d'acte antisportif majeur et violent sont souvent interrompus pour ne pas inciter les téléspectateurs à la violence.Un club sportif (CS) est une infrastructure encadrant les sportifs.La recherche suggère que le sport a la capacité de connecter la jeunesse à des modèles de rôle adultes positifs et de fournir des possibilités de développement positif, tout en favorisant l’acquisition et l’application des compétences utiles dans la vie courante. Ces dernières années, le recours au sport pour lutter contre la délinquance, et prévenir l’extrémisme violent et la radicalisation, est devenu plus fréquent, notamment en tant qu’outil pour améliorer l’estime de soi, resserrer les liens sociaux et donner aux participants le sentiment d’être utile.Récemment, le sport a long"
sport;"Un sport individuel est un sport qui oppose des individus, par opposition à un sport collectif où ce sont des équipes qui s’affrontent. Lors des compétitions, seul un individu est récompensé sur sa seule performance.Cela ne signifie pas forcément que le sportif n'appartient pas à une équipe : par exemple le tennis est un sport individuel lorsque les joueurs jouent en simple mais dans le cadre de la Coupe Davis ou Fed Cup pour les féminins, c'est une nation qui gagne la compétition par ajout des résultats en simple. Le double en tennis est par contre une discipline qualifiée de collective. Exclusivement en simple Athlétisme (ne sont pas concernés les disciplines de relais)BoxeEquitationEscaladeGolfHaltérophilieJudo (parfois épreuve par équipe en compétition)LutteNatation (ne sont pas concernés les disciplines de relais)Pentathlon moderneSkeletonSki alpinSki acrobatiqueSnowboardTaekwondoTir sportifTriathlon (ne sont pas concernés les disciplines de relais) Comportant aussi des épreuves doubles Aviron : seul le skiff comporte un seul rameurBadminton : existe en format double et en format par équipeCanoë-kayak : dans les catégories K-1 ou C-1LugePatinage artistique : hors discipline couple et dansPlongeon : hors plongeon synchroniséTennis : en simpleTennis de table : existe en format double et en format par équipeVoile : seul le dériveur en solitaire et la planche à voile Comportant aussi des épreuves par équipe Badminton : existe en format double et en format par équipeBiathlonCyclisme : épreuve sur piste de vitesse individuelle, sur route individuel, VTT, BMXÉquitationEscrimeGymnastiquePatinage de vitesseShort-trackSaut à ski/Combiné nordiqueSki de fondTennis de table : existe en format double et en format par équipeTir à l'arcArt martial / Sport de combatCourse automobile : la Formule 1 est individuel mais pas le Rallye automobile où le copilote est associé à la victoire ; en Championnat du monde d'endurance, c'est par contre une équipe de relais qui est récompensé.Spéléologie Portail du sport"
sport;"La tactique en football décrit comment les joueurs d'une équipe de football se positionnent sur le terrain et opèrent entre eux. Ces dispositifs recouvrent la mise en place initiale d'un plan de jeu (on parle de « formation »), et une fois la partie commencée, le placement des joueurs les uns par rapport aux autres et leurs actions de déplacements, qui peuvent être « orchestrées » à partir du banc de touche par l'entraîneur.Le football étant un sport d'équipe, les questions de tactique et d'intelligence collective sont primordiales. Le résultat d'un match ne dépend pas seulement de l'habileté des joueurs à manier le ballon, mais aussi des choix tactiques des deux équipes qui, suivant le dispositif, peuvent s'avérer décisifs. Le positionnement des joueurs sur le terrain, la capacité à exécuter parfaitement des phases de jeu répétées à l'entraînement, et, d'une manière générale, l'aptitude des joueurs à pratiquer un football homogène et cohérent entrent pour une grande part dans les résultats.La tactique s'adapte aux trois moments principaux du jeu d'une équipe : la possession du ballon, la possession du ballon par l'adversaire, le changement de possession. La tactique implique l'alternative ; la longueur des séquences de jeu amène le joueur à rencontrer une grande variété de situations d'enchaînements offensifs et défensifs.Dès les débuts du football, il s'est avéré que le principe consistant à ce que tous les joueurs se dirigent vers le ballon était une stratégie perdante : un seul joueur pouvant avoir la maîtrise de la balle, la trop grande proximité de ses coéquipiers ne lui sert pas et peut même le gêner. Les partenaires du porteur de balle ont plutôt à se répartir sur le terrain de manière à lui offrir le maximum de possibilités de passes, tout en restant aptes à défendre leur camp en cas de perte de balle.Certains joueurs, de par leurs qualités physiques ou techniques, sont plus aptes à aller marquer des buts, ou au contraire montrent une grande efficacité de récupération de balle. Il a donc été naturel d'affecter les premiers aux tâches dites offensives, près du but adverse (ils sont appelés « avants », et plus communément aujourd'hui « attaquants »), et les seconds aux tâches dites défensives, à proximité de leur propre but, de manière à en empêcher l'accès aux attaquants adverses (les « arrières » ou « défenseurs »). La règle du hors-jeu, introduite en 1866 en même temps que l'autorisation des passes en avant,, permet d'éviter la formation de deux groupes de joueurs, chacun devant un but, en imposant aux uns et aux autres de se déplacer sur le terrain en fonction des actions de jeu. Un joueur ne pouvant pas multiplier les courses pendant 90 minutes, la liaison entre les lignes d'attaque et de défense est assurée par les joueurs se trouvant dans l'« entre-jeu », initialement appelés « demis », plus connus depuis les années 1970 comme des « milieux de terrain ».Défense, milieu de terrain et attaque sont des concepts constants au cours de l'évolution des dispositifs tactiques, qui sont généralement basés sur ce modèle en trois lignes.Dans le football moderne, et notamment depuis la révolution du football total apparu dans les années 1970, chaque joueur est généralement appelé à participer au jeu collectif de son équipe, quelle que soit la phase de jeu, qu'elle soit offensive ou défensive, avec ou sans la possession de la balle. Ainsi, un avant doit tenter de perturber le jeu adverse quand il n'a pas la balle, par un pressing qui est d'autant plus efficace qu'il est collectif. Inversement, un arrière peut venir apporter le surnombre au cours des phases offensives ou de montée de balle. Les milieux de terrain peuvent s'intégrer aux lignes avant ou arrière en fonction des circonstances, si bien que le dispositif tactique peut radicalement changer au cours de la partie. La capacité d'adaptation du dispositif tactique, selon les circonstances (but marqué ou encaissé, changement de joueur dans l'équipe adverse, expulsion ou blessure, état de la domination ou de l'initiative…) est aujourd'hui souvent déterminante. Par conséquent, la polyvalence d'un joueur est un atout appréciable qu'un entraîneur pourra utiliser quand il le jugera nécessaire. La polyvalence et l'adaptation priment dans le jeu moderne sur le jeu au poste. Les rôles d'attaquants ou de défenseurs qu'un entraîneur peut attribuer à certains de ses joueurs peuvent n'être que temporaires, selon la menace que leurs actions opèrent.« Quand vous disputez un match, c'est statistiquement prouvé qu'un joueur a le ballon trois minutes en moyenne. Donc, le plus important c'est ce que vous faites les 87 minutes pendant lesquelles vous n'avez pas le ballon. C'est ce qui détermine si vous êtes un bon joueur ou pas. »— Johan CruyffOn lit dans la presse des années 1950 : « Au début, le football se jouait à 10 devant, aujourd'hui, il se joue à 10 derrière ». Si le constat est exagéré, il traduit l'évolution tactique connue par le football depuis la fin du XIXe siècle.Le passage du « dribbling game » au « passing game » constitue une première révolution, entre 1860 et 1880. À son origine, le football est très individualiste : les joueurs, tous « attaquants », se ruent vers le but ballon au pied, en enchaînant les dribbles, selon le schéma de jeu du « kick and rush » (balancer le ballon devant et courir après). L'efficacité du geste, l'évolution des règles (l'autorisation des passes vers l'avant et l'apparition du hors-jeu) et l'amélioration continue de la qualité des ballons et des terrains va contribuer à transformer le football en jeu de passes, d'abord en Écosse puis dans toute l'Angleterre, notamment après la victoire en finale de la Coupe d'Angleterre de Blackburn Olympic en 1883. Cependant, avec l'obligation jusqu'en 1925 de compter trois joueurs adverses entre la ligne de but et le joueur à la réception d'une passe, un avant-centre devait toujours avoir de solides qualités de dribble pour espérer conclure une action.Le WM règne en maître dans le football européen jusqu'en 1953 et la défaite des Anglais à domicile face aux Hongrois. Après que de nombreux entraîneurs aient tenté de trouver une parade au WM, Gusztáv Sebes conçoit pour le Budapest Honvéd et la sélection de Hongrie une tactique basée sur les permutations pendant le jeu, une grande nouveauté. En reculant, l'avant-centre propose un point d'appui à ses deux milieux offensifs, qui peuvent monter à sa place et entraîner un surnombre. Les Hongrois sont les premiers à prôner le dépassement du rôle. Ce principe novateur favorise le passage au 4-2-4. Les Brésiliens adoptèrent cette formule du 4-2-4 et la firent évoluer progressivement en 4-3-3 durant les années 1960 ; ce positionnement restera majoritaire jusqu'aux années 1970.En parallèle de cette histoire des tactiques offensives, il existe également une école défensive. Le « Verrou suisse » mis en place dès les années 1930 est le modèle de tous les bétons (français) et autres Catenaccio (italien) qui prennent le relais après la Seconde Guerre mondiale. En France, certaines formations deviennent réputées pour leurs stratégies défensives comme Lyon, Strasbourg et surtout Bordeaux, « la forteresse imprenable »[réf. nécessaire]. L'émergence de milieux de terrain créatifs dans les années 1970 et 1980, à la manière de Cruyff, Platini et Maradona, exige de nouvelles adaptations défensives[réf. nécessaire].La culture tactique diffère dans les différents championnats, l'Angleterre par exemple étant longtemps restée considérée comme un parent pauvre dans ce domaine. Avec la rapidité des transferts de joueurs, les cultures tactiques deviennent cependant moins le fait des championnats et des clubs que des entraîneurs, dont les plus connus et les plus durables au haut niveau développent des préférences pour tel ou tel schéma. On note également un certain nivellement tactique, grâce au développement de la vidéo notamment[réf. nécessaire].L'évolution connue vers un jeu de plus en plus défensif semble avoir atteint ses limites dans les années 2000. Les défenses à cinq joueurs deviennent rares, et compter moins de trois ou quatre joueurs à vocation offensive parmi ses milieux et attaquants est considéré généralement comme contre-productif. L'accent est mis sur la polyvalence et le resserrement des lignes de joueurs, souvent résumés dans l'expression de « bloc-équipe », dans le but de réduire le temps et l'espace disponibles à l'adversaire. L'animation défensive s'uniformise en Europe autour de fondamentaux invariables : participation des dix joueurs de champ, replacement, pressing raisonné des attaquants, défense en zone et en ligne. L'animation offensive est le terrain de plus d'expérimentations, de créativité (de un à trois attaquants, avec ou sans ailiers excentrés, avec ou sans meneur de jeu axial).Deux des principaux schémas tactiques jusqu'aux années 1950           À l'origine, de nombreux dispositifs sont utilisés : 2-2-6, 1-1-8.À partir des années 1880 s'impose le 2-3-5, schéma tactique adapté à l'évolution aux blocs d'équipes aptes à alterner la défense massive et un football offensif. Il est composé de trois lignes : deux arrières, trois demis et cinq avants. C'est l'équipe de Cambridge qui est la première à placer trois milieux de terrain et c'est l'équipe de Preston North End Football Club qui popularise ce schéma tactique avec lequel elle réussit le premier doublé coupe-championnat d'Angleterre en 1889. Ce dispositif « en pyramide » est utilisé par la majorité des clubs pendant plusieurs décennies.Avec l'assouplissement de la règle du hors-jeu en 1925, les données du problème changent, et Herbert Chapman (entraîneur de football anglais) met au point une tactique révolutionnaire, dite du « WM », composée de trois défenseurs, deux demis, deux inters et trois attaquants,. Cette tactique s'impose à travers l'Europe et conforte l'Angleterre dans son rôle de pays « inventeur » du football. Elle offre à Chapman une collection de trophées gagnés avec Arsenal. L'AS Cannes est l'un des premiers clubs français à adopter cette tactique dès 1931[réf. nécessaire].Cette formation est en fait une variante du W-M pour la première fois utilisée par Boris Arkadiev, entraîneur du Dynamo Moscou dans les années 40 et qui sera sa marque de fabrique. Les 2 demis et 2 inters du W-M étaient disposé sur le papier comme formant un losange : un demi à vocation défensive, un demi et un inter qui joue plus ou moins au même niveau et un inter qui était à vocation offensive. Cependant au cours d'un match, les 2 demis, 2 inters ainsi que les 3 avants avaient pour but de constamment dézonner afin d'empêcher l'adversaire d'effectuer un marquage individuel. De plus, les joueurs avaient pour consignes d'effectuer beaucoup de passes courtes. Ce style de jeu est aux origines du football total et du tiki-taka. Les journalistes de l'époque ont appelé cette formation : le désordre organisé.Selon Aksel Vartanyan, historien du football russe, le demi défensif était quelquefois tellement bas qu'il pouvait être considéré comme un quatrième défenseur. Il serait donc le premier entraîneur à avoir utilisé une forme de défense à 4 dans l'histoire du football.Cette formation (quatre défenseurs, deux milieux de terrain et quatre attaquants) est assez peu répandue comme formation de base de par la faiblesse de son milieu de terrain.Elle est le plus souvent une version du 4-4-2 en phase d'attaque, ou formation utilisée notamment en fin de partie (par remplacement de milieux de terrains par des attaquants) par une équipe qui doit absolument marquer.La plupart du temps, elle se résume à un 4-4-2 offensif, avec deux attaquants prenant en charge les couloirs et épaulant les deux avants-centres.Cette formation fut popularisée à la suite de l'exploit de l'équipe nationale de Hongrie qui choisit ce 4-2-4 pour contrer le fameux WM des Anglais. Ce choix tactique leur permit de faire chuter l'Angleterre pour la première fois de son histoire à Wembley, puis par la suite utilisée par le Brésil et qui lui a permis de remporter ses deux premières Coupes du monde. Aujourd'hui elle est devenue complètement désuète, et a presque disparu du football professionnel !À noter, tout de même, qu'il s'agit plus d'une formation intermédiaire entre le 4-3-3 à deux récupérateur et le 4-4-2, qu'un pur 4-2-4 à deux pointes centrales et deux ailiers.Néanmoins le Milan AC, par le biais de son ancien entraîneur Leonardo, l'a remise au goût du jour et plus récemment, Louis Van Gaal, lors de son passage au Bayern de Munich, a réussi à atteindre la finale de la Ligue des champions (2009-2010, perdue contre l'Inter Milan) et a décroché un titre de champion d'Allemagne (2009-2010), s'appuyant sur la technique, la vivacité, et la capacité d'élimination en un contre un de ses ailiers Arjen Robben et Franck Ribéry, tandis que les deux attaquants de pointe se chargent de créer des espaces.L'ancien entraîneur de la Juventus de Turin, Antonio Conte, a utilisé cette tactique lorsqu'il a repris l'équipe en septembre 2011. Depuis, peu d'entraîneurs, comme Roger Schmidt ou encore Hein Vanhaezebrouck, continuent d'employer un 4-2-4, notamment en Ligue des champions[réf. nécessaire].Le 4-4-2 (quatre défenseurs, quatre milieux de terrain, deux attaquants) est l'un des schémas classiques du football actuel. Il existe sous deux formes : le 4-4-2 classique, à plat, ou carré (à droite), et le 4-4-2 losange ou diamant (à gauche). Au niveau de la défense et de l'attaque ces deux formes sont identiques (deux arrières centraux, deux arrières latéraux, et deux avants). C'est au milieu de terrain que la différence est notable.Dans le 4-4-2 classique Le milieu de terrain est composé de deux milieux défensifs : généralement un relayeur et un récupérateur. Il y a également deux milieux offensifs latéraux, un à gauche et un à droite, qui sont chargés de construire le jeu et de combiner avec les autres joueurs offensifs, sur leur aile comme dans l'axe. Les deux milieux offensifs peuvent tout à fait être des ailiers. Seulement, ils auront des consignes défensives (qu'ils respectent ou pas). Manchester United, qui effectua le triplé en 1999, jouait dans cette configuration. C'est l'une des tactiques les plus offensives du football moderne, avec 4 joueurs offensifs, contrairement aux autres configurations.Dans le 4-4-2 losange (ou 4-3-1-2 et 4-1-3-2 selon le profil de base des joueurs utilisés) Qui se joue soit avec un milieu défensif, deux milieux latéraux ou relayeurs, et un milieu offensif (10). Soit avec trois milieux défensifs : un milieu récupérateur dans une position axiale et deux milieux relayeurs occupant les couloirs. Mais ceux-ci ont un profil beaucoup plus défensif que des milieux latéraux. En effet, ils doivent épauler le récupérateur dans sa tâche défensive, sans quoi il se retrouverait bien seul. Devant ces trois milieux défensifs se tient un milieu offensif axial (un meneur de jeu, même si cette appellation indique un rôle, et non un placement sur le terrain). Il est le métronome de son équipe et doit se montrer particulièrement décisif dans ses passes pour les deux attaquants afin de compenser l'absence de véritables milieux de débordement. Cette formation est appelée en anglais diamond (losange). L'équipe du Milan AC de Carlo Ancelloti l'utilisait, tout comme Laurent Blanc aux Girondins de Bordeaux lors du titre de champion de France en 2009.Le 4-3-3 (quatre défenseurs, trois milieux de terrain et trois attaquants) possède une défense qui évolue souvent en ligne comme pour le 4-4-2, mais le milieu de terrain change de fonction. Il est souvent à vocation plus défensive, et doit faire parvenir la balle rapidement à l'attaque. Celle-ci est composée d'un attaquant de pointe et de deux ailiers. Le profil des joueurs la composant est le suivant : une défense classique avec de préférence des latéraux offensifs, trois milieux de terrains (deux relayeurs, et un récupérateur qui fonctionnent comme dans un 4-4-2 losange) avec un important volume de jeu (pour pallier l'absence du quatrième élément), deux ailiers rapides et bons frappeurs et un attaquant de pointe de préférence athlétique et doté d'un bon jeu de tête. C'est cette organisation qui permit à l'Angleterre d'être championne du monde en 1966, et au FC Barcelone de remporter la Ligue des Champions lors de la saison 2008-2009 ainsi que le Championnat d'Espagne de football et la Coupe du Roi cette année-là. Le 4-3-3 a permis à José Mourinho et Chelsea FC de devenir champions d'Angleterre deux fois de suite avec 91 et 95 points (record en Premier League). Ce dispositif, utilisé par le Belge Raymond Goethals dans les années 1990, a permis à Luis Fernandez de remporter la coupe d'Europe des vainqueurs de coupes avec le PSG en 1996, ainsi qu'à l'Olympique Lyonnais de remporter sept titres de champion de France consécutivement.À noter que le 4-1-4-1, comme l'intitulent certains entraîneurs, n'est autre qu'une version défensive du 4-3-3. Dans un 4-1-4-1, il y a toujours un récupérateur et deux relayeurs. Seulement, les deux ailiers sont des milieux offensifs à la charge défensive plus importante.Le 4-2-3-1 (4 défenseurs, 5 milieux, 1 attaquant) est un système qui vise, comme le 4-3-3, à étouffer son adversaire au milieu de terrain par l'utilisation de deux milieux défensifs, généralement un relayeur doté d'une bonne relance et un récupérateur (qui, par son activité, récupère un grand nombre de ballon). Le milieu est celui d'un 4-4-2 carré auquel on ajoute un milieu offensif axial, chargé d'animer le jeu et d'avoir la vista et la technique nécessaires pour créer à lui seul des opportunités de but. L'attaque n'est elle plus animée que par un seul buteur ou finisseur, ce qui permet stratégiquement de libérer un joueur supplémentaire dans la construction du jeu ; il sera néanmoins épaulé par trois milieux offensifs. Cette formation met en valeur les capacités d'un meneur de jeu.Ce fut la tactique principalement utilisée par Raymond Domenech lors de la coupe du monde 2006 (dans le schéma exact expliqué auparavant, Patrick Vieira étant un vrai milieu axial à l'anglaise, avec une grosse activité verticale, aux côtés d'un Claude Makelele plus limité au travail de récupération). C'est aussi l'un des dispositifs les plus utilisés par Roberto Mancini à Manchester City ou José Mourinho au Real Madrid et par la majorité des entraîneurs de Ligue 1. Roger Lemerre a opté pour cette configuration lors de l'Euro 2000 afin de mettre Zinédine Zidane dans les meilleures dispositions. Aimé Jacquet l'a employé à de nombreuses reprises, notamment lors des matches de poule de la Coupe du monde 1998, avant de finir la compétition dans un 4-3-2-1 particulièrement défensif. Jacques Santini, lors de l'Euro 2004, avait finalement opté pour un 4-4-2 losange avec Zidane en numéro 10 et Robert Pirès et Patrick Vieira en relayeurs. Il est également utilisé par Didier Deschamps lors de l'Euro 2016 et de la Coupe du monde 2018 .À noter que le système dit de 4-4-1-1 peut être clairement considéré comme un 4-2-3-1.Cette formation à vocation défensive se base normalement sur trois arrières centraux, dont l'un des joueurs peut prendre le rôle de libéro. Les arrières latéraux supplémentaires viennent soutenir le milieu de terrain. Cette formation est très comparable au 3-5-2 mais elle utilise des latéraux plus défensifs. Au contraire du 3-5-2 elle est en général utilisée par des équipes faibles techniquement qui refusent le jeu.Il existe néanmoins des variantes au sein même de ce système. Si le principe demeure le même (gagner la bataille du milieu et ainsi s'assurer la maîtrise du ballon), il en existe deux principales versions : la version offensive, avec deux milieux défensifs évoluant devant la défense et un trio d'animation en soutien des deux attaquants ; et la version défensive, constituée d'une ligne de quatre récupérateurs devant la défense et d'un seul meneur axial derrière les deux attaquants. Ces formations sont principalement utilisées par des équipes sud-américaines.Cette formation absolument défensive, parfois appelée Catenaccio, est généralement développée durant le cours du match par des équipes ayant déjà marqué suffisamment de buts ou voulant à tout prix éviter la défaite et opérant en contre-attaque. Dans le cas du 5-4-1, on retrouve souvent une défense à quatre avec un libéro, destiné à bloquer les espaces et intercepter d'éventuels ballons en profondeur. Le milieu est lui disposé comme celui d'un 4-4-2 carré. Il s'agit notamment de la tactique utilisée par José Mourinho avec l'Inter Milan lors du match retour au Nou Camp qui lui permet d'arracher la qualification pour la finale de la Ligue des Champions (même si défaite au match retour 1-0, victoire au match aller 3-1).[réf. nécessaire]Quasi identique au 5-4-1 dans le placement des joueurs, mais en plus offensif dans le comportement, cette formation comprend une ligne de trois défenseurs axiaux, une ligne au centre de deux milieux récupérateurs/relayeurs et deux arrières latéraux capables de dédoubler et lancer en profondeur les deux ailiers en attaque qui suppléent l'avant centre. Bien qu'elle puisse offrir une grande polyvalence et permette un surnombre lors des phases de possession, cette tactique mise avant tout sur les contres et les percées par les ailes. Elle peut aussi être utilisée en losange.Les arrières très offensifs envoient les ailiers dans la profondeur ou parfois quand les ailiers sont sur le côté du pied faible (un gaucher jouant ailier droit, comme Lionel Messi par exemple) dédoublent. Cette tactique est utilisée par de plus en plus de clubs, on peut citer comme équipes utilisant ce dispositif depuis plusieurs saisons le Genoa ou son ami historique le SSC Napoli qui maîtrise actuellement ce dispositif avec beaucoup de savoir-faire. Il a aussi été appliqué par Diego Maradona avec l'Argentine lors des éliminatoires de la Coupe du monde 2010, avant d'imposer un 4-2-3-1 plus classique lors de la compétition en Afrique du Sud. On a pu récemment voir le FC Barcelone adopter cette tactique.Compétence (sport)Textes officiels concernant l'éducation physique et sportive en FranceComposition d'une équipe de rugby à XVD. Herr, À propos des domaines d'action, Revue EP Strasbourg, no 2, CRDP Strasbourg, 1995Tactique en footballLexique tactique / 1 : postes et rôles, les Cahiers du football, septembre 2014.Lexique tactique / 2 : principes et philosophies, les Cahiers du football, septembre 2014.Benigni, A. et al. - Football. - Paris : Éditions De Vecchi, 1999. - 403 p. -  (ISBN 2-7328-6739-X) - Cote Dewey : 796.334 F687 1999. - (Les schémas de jeu depuis les origines, pages 271-278).Jean-Francis Gréhaigne, L'organisation du jeu en football, Actio, Paris, 1992  (ISBN 2906411086)Raphaël Cosmidis, Gilles Juan, Christophe Kuchly, Julien Momont, Comment regarder un match de foot ?, Solar, Paris, 2016  (ISBN 2263071687) Portail du football"
