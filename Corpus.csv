médecine;La thyroïdite de Hashimoto ou thyroïdite chronique lymphocytaire est une thyroïdite chronique auto-immune particulièrement fréquente caractérisée notamment par la présence d'anticorps anti-thyroperoxydase et par une infiltration lymphoïde de la glande thyroïde. Généralement évoqué à l'examen clinique devant un goitre et une hypothyroïdie, le diagnostic de la maladie nécessite la réalisation d'examens complémentaires biologiques et morphologiques. Le traitement de la maladie fait généralement appel à une hormonothérapie substitutive.C’est en examinant des pièces de thyroïdectomie obtenues chez quatre femmes d’âge moyen dans un contexte de goitre compressif que le médecin japonais Hakaru Hashimoto (1881?1934) découvre la maladie en 1912. Il publie sa découverte avec l’article K?j?sen rinpa-setsu shush?-teki henka ni kansuru kenky? h?koku ou Zur Kenntnis der lymphomatösen Veränderung der Schilddrüse (Struma lymphomatosa) dans « Archiv für klinische Chirurgie », y décrivant alors l’infiltration lymphocytaire de la glande thyroïde.En 1957, la thyroïdite de Hashimoto devient la première maladie auto-immune spécifique d’organe à être reconnue. Initialement sous-diagnostiquée et considérée comme une maladie rare,, la thyroïdite de Hashimoto est aujourd'hui reconnue comme une des pathologies thyroïdiennes les plus fréquentes.Les estimations de l’incidence et de la prévalence de la thyroïdite de Hashimoto sont variables. Une incidence de 1/1000 a été proposée ainsi qu’une prévalence de 8/1000. La maladie est ainsi la première cause d’hypothyroïdie dans les pays où les apports en iode sont satisfaisants,.Il existe une nette prédominance féminine de la maladie de Hashimoto avec un rapport estimé entre 8 et 20 femmes pour 1 homme. Cette thyroïdite survient généralement aux alentours de l’âge de 40 ans mais peut se voir à tout âge y compris en population pédiatrique. Elle serait également plus fréquente chez les populations caucasiennes et asiatiques.Une histoire familiale de maladies de la thyroïde est fréquente (20 % des cas), en faveur d'une prédisposition génétique. Celle-ci est le plus souvent associée à des haplotypes particuliers tels que HLA-DR3, HLA-DR4 et HLA-DR5. Par ailleurs, la thyroïdite de Hashimoto pourrait être liée, avec un niveau de risque plus faible, avec le polymorphisme de gènes impliqués dans la réponse immunitaire comme le gène CTLA-4 (Cytotoxic T-lymphocyte Associated-4) (en) qui entraîne une diminution du fonctionnement des produits du gène et une régulation négative de l'activité des lymphocytes T,. Ce mécanisme est également retrouvé dans le diabète de type 1.La carence iodée serait un facteur de protection contre le risque de thyroïdite de Hashimoto, et une correction excessive en iode ,, une carence en sélénium, des maladies infectieuses et quelques médicaments ont été impliqués comme facteurs de risque chez les personnes avec un risque génétique déjà prédéterminé.L'incidence est augmentée chez les patients avec des anomalies chromosomiques, comme dans le syndrome de Turner,, le syndrome de Down (ou trisomie 21) et le syndrome de Klinefelter.Des recherches récentes suggèrent un potentiel rôle du virus HHV-6 (possiblement variant A) dans le développement ou la stimulation de la thyroïdite de Hashimoto.Le rôle du tabac est discuté. Des études indiquent un risque plus élevé chez les fumeurs, d'autres tendent à montrer un effet protecteur du tabac avec une réduction des taux sériques en auto-anticorps ainsi qu'une évolution moins fréquente vers l'hypothyroïdie. Le mécanisme physiopathologique de cet effet protecteur n'est cependant pas compris.La thyroïdite de Hashimoto est associée à la survenue d'autres maladies auto-immunes : diabète de type 1,, maladie cœliaque,, vitiligo, maladie de Biermer, insuffisance surrénale (notamment dans le cadre d'un syndrome de Schmidt) et polyarthrite rhumatoïde.La maladie de Basedow peut être associée à la thyroïdite de Hashimoto et il existe des formes de passage entre ces deux maladies. On parle ainsi de « Hashitoxicose », entité décrite pour la première fois en 1971.Sur le plan physiopathologique, les anticorps dirigés contre la thyroperoxydase et/ou la thyroglobuline causent une destruction progressive des follicules thyroïdiens de la glande thyroïde.Macroscopiquement, le goitre est symétrique, non adhérent aux éléments péri-thyroïdiens et présente une surface capsulaire discrètement bosselée.En microscopie les lésions consistent en une association de fibrose interstitielle, d'infiltration lymphoïde et de destruction épithéliale,. Le degré de fibrose est très variable. L'infiltration lymphoïde présente une organisation en follicules avec des lymphocytes B au centre et des lymphocytes T dans le cortex. Les cellules épithéliales thyroïdiennes sont également modifiées, apparaissant élargies et acidophiles (cellules de Hürthle).Ces signes sont liés à la présence du goître induit par la thyroïdite. Celui-ci est diffus et sa surface est le plus souvent régulière. Sa consistance est très particulière : ferme, « suiffée » ou « caoutchoutée ». Parfois ces changements peuvent ne pas être palpables. Le goitre peut éventuellement être responsable de signes compressifs (dysphonie, dysphagie et dyspnée). La palpation cervicale ne retrouve généralement pas d'adénomégalie. Il n'est pas mis en évidence non plus de douleur ou de signes inflammatoires locaux.Ces derniers sont liés à la dysthyroïdie avec au premier plan l'hypothyroïdie. L'hyperthyroïdie peut également être présente, en particulier au début de l'évolution de la maladie. On soulignera que les signes de dysthyroïdie peuvent être absents initialement, la fonction thyroïdienne n'étant pas nécessairement perturbée. En revanche plus la maladie évolue et plus l'hypothyroïdie devient fréquente.Parmi les signes cliniques induits par l'hypothyroïdie on retrouve notamment : constipation, bradycardie, myxœdème, anémie, règles irrégulières, asthénie, troubles de la concentration et de la mémoire, dépression, peau sèche et épaissie, perte de cheveux.La positivité à un taux élevé des anticorps anti-TPO, retrouvée dans 95 % des cas, est le meilleur signe biologique pour diagnostiquer la thyroïdite de Hashimoto. Elle survient préférentiellement chez des sujets HLA B8-DR3 (en). Le titre en anticorps est de plus associé au degré d'infiltration lymphoïde de la glande. On notera que la positivité des anticorps anti-TPO est par ailleurs très rare chez les sujets sains.En cas de négativité des anticorps anti-TPO, on peut retrouver une augmentation des anticorps anti-thyroglobuline.Il n'y a pas nécessairement, au début, de trouble de la fonction hormonale, mais la maladie évoluera toujours vers une hypothyroïdie avec des taux de T4 anormalement bas et secondairement des taux de TSH élevés.Enfin, la thyroïdite de Hashimoto n'est pas associée à la présence de marqueurs sériques de l'inflammation.L'échographie de la thyroïde montre un goitre hypoéchogène,. Le parenchyme thyroïdien devient plus hétérogène au cours de l'évolution. On peut notamment mettre en évidence des pseudo-nodules et des nodules de régénérations hyperéchogènes (white knight). Des ganglions récurrentiels peuvent être visualisés. La vascularisation est hétérogène en Doppler couleur. L'étude en Doppler pulsé retrouve une élévation des vitesses systoliques, toutefois moindre que dans la maladie de Basedow.La scintigraphie est inutile dans ce contexte d'hypothyroïdie. Lorsqu'elle est réalisée elle montre des résultats très variables ne contribuant donc pas au diagnostic.La thyroïdite de Hashimoto peut être associée à toute autre maladie auto-immune : collagénose, insuffisance surrénalienne, à un cancer de la thyroïde ou entraîner des complications cardio-vasculaires. Elle peut aussi entraîner des symptômes laissant penser à tort à un virage maniaque (manie) caractéristique d'un trouble bipolaire.La thyroïdite de Hashimoto peut se compliquer d'une encéphalopathie (encéphalopathie de Hashimoto). Cette entité a été décrite pour la première fois en 1966 et seuls une centaine de cas ont été rapportés dans la littérature depuis.Le lymphome thyroïdien complique moins de 1 % des thyroïdites auto-immunes,. Toutefois celui-ci doit être évoqué devant toute augmentation de volume du goitre ou en cas de survenue d'adénopathie.Le traitement chirurgical n'a aujourd'hui que rarement sa place dans la thyroïdite de Hashimoto. On le réservera essentiellement aux goitres compressifs.La prise en charge est avant tout médicale, consistant en une hormonothérapie thyroïdienne substitutive. Les hormones thyroïdiennes permettraient une diminution du volume du goitre tout en corrigeant l'hypothyroïdie latente ou évidente.Endocrionologie, diabète et maladies métaboliques. Collège des enseignants d'endocrinologie, diabète et maladies métaboliques p361InfoThyroAFMT : site officiel de l'Association française des malades de la thyroïde Portail de la médecine
médecine;"La glande thyroïde ou thyroïde est une glande endocrine régulant, chez les vertébrés, de nombreux systèmes hormonaux par la sécrétion de triiodothyronine (T3), de thyroxine (T4) et de calcitonine. Dans l'espèce humaine, elle est située à la face antérieure du cou, superficiellement.Ses déformations (on parle de goitre quand le volume de la thyroïde est augmenté) sont visibles sous la peau. Elle peut être le siège de diverses affections : hyperthyroïdie, hypothyroïdie, tumeur maligne ou tumeur bénigne. On peut l'étudier grâce à l'échographie et à la scintigraphie.La thyroïde, moulée sur l'axe trachéo-laryngé, est de consistance ferme, de couleur rosée, et pèse de 25 à 30 grammes généralement mais en cas de goitre sa masse peut augmenter jusqu'à 100-150 grammes. Elle est entourée d'une capsule avasculaire (ou gaine viscérale péri-thyroïdienne) qui lui est propre et qui est différente de la loge thyroïdienne.La thyroïde se compose de deux lobes droit et gauche situés verticalement de part et d'autre du larynx. Une partie intermédiaire horizontale, l'isthme thyroïdien, forme un pont entre les deux lobes. Généralement, la glande thyroïde répond aux 2e et 3e anneaux trachéaux ; mais elle peut avoir une position haute : 1er et 2e anneaux trachéaux, ou une position basse : 3e et 4e anneaux trachéaux. Les deux lobes ont un sommet supérieur, ainsi qu'une grande base inférieure. On leur décrit trois faces : médiale, postérieure et antéro-latérale. Sa hauteur est d'environ 6 cm pour une longueur de 6  à   8 cm. On trouve souvent entre les deux lobes, donc au niveau de l'isthme, le lobe pyramidal de Lalouette, souvent déporté vers la gauche : c'est un reliquat du canal thyréoglosse.Il existe des variations morphologiques, s'expliquant par l'embryologie : en effet les deux lobes sont parfois éloignés l'un de l'autre sans qu'il n'y ait d'isthme, ou au contraire peuvent être soudés donnant une thyroïde en forme de V. Provenant d'un bourgeon de cellules endodermiques naissant près de la racine de la langue, différentes positions de la glande thyroïde peuvent cependant survenir durant l'ontogenèse : une mauvaise migration de cette ébauche conduit alors à la détection de cette glande (fonctionnelle ou non fonctionnelle) dans la région linguale, cervicale, voire endo-thoracique.La thyroïde présente les rapports anatomiques suivants :ventralement : muscles cervicaux superficielslatéralement : nerfs récurrents et axes vasculaires jugulo-carotidiensdorsalement : larynx au pôle supérieur et trachée cervicale au pôle inférieurLes quatre parathyroïdes ont des positions variables, mais se situent généralement aux quatre pôles thyroïdiens.La thyroïde est un organe richement vascularisé. En effet on retrouve :Deux artères principales :artère thyroïdienne supérieure, première branche de l'artère carotide externe ; elle se divise en 3 branches (latérale, médiale et postérieure) une fois la glande atteinte.artère thyroïdienne inférieure, naissant du tronc thyro-cervical, branche collatérale de l'artère subclavière. Se divise également en trois branches (mêmes situations) dans la thyroïde.Dans de très rares cas il est possible qu'une 3e artère vienne vasculariser la thyroïde dans sa portion basse appelée artère de Neubauer qui est une branche de la crosse de l'aorte.Les deux artères principales de la thyroïde sont anastomosées ; l'ATS droite avec l'ATS gauche et l'ATI droite, et l'ATI droite avec l'ATS droite et l'ATI gauche.Il existe néanmoins d'autres artères, moins volumineuses, inconstantes, naissant directement de l'arc aortique. Par exemple l'artère thyroïdea ima vascularisant la partie isthmique. Celle-ci est présente chez environ 5 à 10 pour cent des sujets et peut provoquer une hémorragie en cas de trachéotomie.Trois veines principales :veine thyroïdienne supérieure, résultant de la confluence de trois veines dans la glande, et formant avec les veines linguale et faciale le tronc thyro-lingo-facial qui se jette dans la veine jugulaire interne.veine thyroïdienne moyenne, réunion de plusieurs branches pas très volumineuses se jetant dans la veine jugulaire interne.veine thyroïdienne inférieure, formée par la confluence de trois veines dans la glande et se jetant dans le tronc veineux brachio-céphalique.De même que pour les artères, certaines veines accessoires vascularisant préférentiellement l'isthme vont rejoindre les troncs veineux brachio-céphaliques droit et gauche.Chez nos mammifères domestiques, on parle de 2 glandes thyroïdes (droite et gauche), car contrairement à l'homme, les 2 lobes thyroïdiens ne sont pas réunis par un isthme.  Chez les autres vertébrés, la glande thyroïde est diffuse, formée de groupes dispersés de follicules, et situés latéralement à des distances variables de l'œsophage.Chez les agnathes et la plupart des téléostéens (poissons), les groupes de follicules se distribuent sur toute la partie ventrale de la tête.Malgré cette diversité morphologique, la structure histologique folliculaire de la thyroïde est hautement conservée chez tous les vertébrés, ce qui témoigne d'un processus original et commun de production hormonale.L'unité morpho-fonctionnelle de la glande thyroïde est le follicule thyroïdien (ou vésicule thyroïdienne), composé d'un épithélium unistratifié de cellules folliculaires (les thyréocytes), produisant les hormones thyroïdiennes, disposées autour d'une lumière centrale contenant la colloïde : la colloïde est principalement constituée du précurseur des hormones thyroïdiennes, la thyroglobuline. Le follicule thyroïdien est un véritable piège à iode (ion iodure), élément rare à la surface de la terre, et indispensable au fonctionnement de l'organisme; l'iode sera ainsi capté et stocké dans la colloïde : la biosynthèse des hormones thyroïdiennes pourra alors  se dérouler, l'iode venant se coupler à la thyroglobuline ; la thyroglobuline iodée est ensuite réintégrée dans le follicule thyroïdien, et sécrétée dans le courant sanguin.Le follicule thyroïdien, en dehors d'une majorité de cellules folliculaires, contient 1 à 2 % de cellules dites parafolliculaires (ou cellules C, ou cellules claires), produisant la calcitonine : elles n'ont cependant jamais de contact avec la colloïde.On trouve aussi des amas de cellules (ilots de Woffler), cellules jointives pouvant se transformer en vésicule thyroïdien.La thyroïde est issue de trois ébauches :deux ébauches latérales issues du 4e sillon branchial interne et qui forme une partie des lobes latérauxet une ébauche centrale issue de l'évagination du pharynx buccal constituant ainsi le tractus thyréoglosse qui forme l'isthme ainsi  que la majeure partie des lobes latéraux. Rappelons que le tractus thyréoglosse est l'axe de migration de la thyroïde chez l'embryon.La thyroïde sécrète :la T3 ou triiodothyronine en très faible quantité ;la T4 ou thyroxine ;la calcitonine intervenant dans le métabolisme du calcium.La production de ces hormones est régie par la thyréostimuline (TSH, « thyroid-stimulating hormone »), produite par l'hypophyse et nécessite un apport en iode. La plus grande production de la T3 est obtenue par la conversion de la T4 au niveau du foie, pour la plus grosse quantité et les intestins pour le reste. La thyroïde ne produit, elle, de la T3 directement que pour à peine 10 à 20 %.De par sa position superficielle, la thyroïde est explorée en premier lieu par une échographie cervicale, qui recherchera des nodules ou un goitre. L'image ultrasonore permet d'évaluer le volume de la thyroïde ; à l'échelle du diagnostic individuel ou de population (suivi épidémiologique).La tomodensitométrie avec injection de produit de contraste iodé est peu utilisée, généralement dans le cadre du bilan pré-opératoire des goitres volumineux, notamment des goitres plongeants.La scintigraphie thyroïdienne à l'iode 123 est un examen fonctionnel. L'injection d'un traceur d'iode radioactif mettra en évidence des zones du parenchyme plus ou moins actives, et permettra la distinction entre un nodule hypersécrétant et un nodule « froid ».Un bilan thyroïdien standard comporte le dosage de la TSH et de la T3 ou de la T4. La calcitonine n'est pas systématiquement dosée.Les dysthyroïdies peuvent avoir des origines génétiques, être liées à des carences nutritionnelles en iode, mais aussi être induites par des toxiques (plomb, ou iode radioactif par exemple - on parle alors de « thyrotoxicoses »). Hypothyroïdie Situation d'imprégnation insuffisante de l'organisme en hormones thyroïdiennes, le plus souvent à cause d'un mauvais fonctionnement de la glande thyroïde.Les symptômes de l'hypothyroïdie découlent d'un ralentissement métabolique général : fatigue, difficultés de concentration, troubles de la mémoire, frilosité, myxœdème, prise de poids malgré un appétit stable voire diminué, diminution de la pilosité avec perte de cheveux ou cheveux devenant cassants, éclaircissement des sourcils, sécheresse ou épaississement cutané, pâleur, crampes musculaires, fourmillement ou engourdissement des extrémités, inappétence, tendance à la dépression, insomnies, tendance à la constipation.L'examen clinique recherche une augmentation de la taille de la thyroïde qui peut être importante (goitre), un ralentissement de la fréquence cardiaque, la bradycardie, et parfois, de la tachycardie ou des symptômes ressemblant à ceux de l'hyperthyroïdie.Le traitement est une substitution journalière à vie en hormones thyroïdiennes, par voie orale. Hyperthyroïdie Symptomatologie due à un excès de production d'hormones thyroïdiennes :cardio-vasculaire : tachycardie, éréthisme cardio-vasculaire (frémissement du choc de la pointe du cœur) ;digestif : syndrome polyuro-polydipsique (boit et urine en grande quantité), amaigrissement, diarrhée, flatulences ;neuro-psy : tremblement, agitation, trouble de l'humeur (irritabilité allant à la dépression), trouble du sommeil, trouble du comportement alimentaire (mange en quantité excessive, perte de poids) ;généraux : hypersudation (mains souvent moites, transpiration), hyperthermie, thermophobie (température élevée et n'apprécie pas les températures élevées) ;musculaire et articulaire : douleur et fatigue musculaire, ostéoporose, augmentations des glandes lactogènes.Le tabac multiplie par dix le risque de survenance de la maladie de Basedow, la forme la plus fréquente de l'hyperthyroïdie, et augmente les risques de complications. Thyroïdites La thyroïdite est une inflammation de la glande thyroïde.Il existe plusieurs types de thyroïdites: Maladie d'Hashimoto La cause la plus fréquente d’hypothyroïdie.Elle consiste en une destruction de la glande thyroïde causée par des taux d’anticorps antithyroïdiens anormalement élevés dans le sang et des globules blancs. La thyroïde ne sécrète alors plus suffisamment d’hormones thyroïdiennes.Cette maladie nécessite donc très souvent un supplément hormonal.Pour confirmer le diagnostic, il faut réaliser une prise de sang qui dosera les hormones thyroïdiennes et la capacité du corps à les gérer (T4, T3, TSH), ainsi que les auto anticorps thyroïdiens (AC anti TPO, et AC anti thyroglobuline).  Thyroïdite du post-partum La thyroïdite du post-partum peut survenir dans l'année qui suit un accouchement. Dans ce cas, la glande a tendance à récupérer, et le traitement de remplacement des hormones thyroïdiennes n’a besoin d’être administré que durant quelques semaines. Une évolution vers une hypothyroïdie permanente est possible. Thyroïdite silencieuse Elle porte ce nom, car elle n’entraîne aucun signe ni symptôme d’inflammation de la thyroïde. De prime abord, le patient présente une hyperthyroïdie pouvant donner lieu aux mêmes symptômes que ceux de la maladie de Basedow-Graves, qui laisse place à une phase d’hypothyroïdie aboutissant à une guérison complète. La présence d’anticorps antithyroïdiens comparables à ceux décelés au cours de la maladie de Hashimoto est un facteur de risque de persistance de l’hypothyroïdie. Thyroïdite subaiguë dite de Quervain Il s’agit d’une forme passagère de thyroïdite provoquant une hypothyroïdie. La thyroïdite subaiguë serait causée par une infection virale, car la majorité des patients atteints ont présenté une infection de la gorge dans les semaines précédant son apparition. Cette affection se manifeste sous forme d’épidémies de faible envergure et est généralement associée à des infections virales connues. Dépression et thyroïde L'hypothyroïdie peut parfois être confondue avec un état de dépression et l'hyperthyroïdie pour un état d'excitation. Le diagnostic thyroïdien permettra d'éliminer ces faux diagnostics.Cependant des travaux récents ont montré qu'une hypothyroïdie traitée uniquement avec de la thyroxine peut davantage encore se rapprocher d'un état dépressif. Plutôt que de prescrire un antidépresseur, le médecin peut parfois proposer une simple substitution d'une partie de la dose de thyroxine (T4) par de la tri-iodo-thyronine (T3) ou de flavinine (substitut générique du B52).Un goitre est une thyroïde globalement augmentée de volume. Il est dit toxique lorsqu'il sécrète des hormones thyroïdiennes de façon excessive, entraînant une hyperthyroïdie. Les goitres sont rarement homogènes et le plus souvent multinodulaires ; chaque lobe thyroïdien présente un nombre important de nodules bénins de volume variable.Les goitres peuvent être en situation cervicale normale ; ils sont dits « plongeants » lorsque le pôle inférieur d'au moins un lobe pénètre dans le médiastin à travers l'orifice supérieur du thorax. Plongeants ou non, les goitres peuvent être (rarement) compressifs, lorsque le volume trop important de la thyroïde comprime les organes de voisinage, principalement la trachée, mais aussi l'œsophage et parfois étirant les nerfs récurrents, entraînant alors une paralysie de la corde vocale homolatérale.Les tumeurs de la thyroïde se présentent sous la forme d'un nodule thyroïdien, qui peut être bénigne (adénome de la thyroïde) ou maligne (carcinome de la thyroïde). Tumeurs bénignes Adénome vésiculaire de la thyroïdeAdénome oncocytaire de la thyroïdeLipoadénome de la thyroïdeAdénome à cellules claires de la thyroïdeAdénome vésiculaire à cellules en bague à chaton de la thyroïdeAdénome vésiculaire mucosécrétant de la thyroïdeAdénome vésiculaire à noyaux bizarres de la thyroïdeAdénome vésiculaire atypique de la thyroïde Tumeurs malignes Il existe deux types histologiques principaux de cancers thyroïdiens :carcinomes différenciés folliculairescarcinomes différenciés papillaires (le plus fréquent, 80 % des cas)La thyroïde est généralement abordée par une cervicotomie médiane, qui peut être élargie latéralement en cervicotomie en U s'il existe une nécessité de curage ganglionnaire cervical. En cas de volumineux goitre plongeant, un refend cutané en Y en regard de l'extrémité crâniale du sternum sera souvent pratiqué. Au maximum, une simple manubriotomie (on parle alors de cervicomanubriotomie) ou une sternotomie médiane pourra être pratiquée.livret d'information ""Cancer de la thyroïde"" ; Institut Gustave Roussy.ARC, Monographie Volume 79 (2001) Some Thyrotropic AgentsFini J.B et Demeneix B (2019) Les perturbateurs thyroïdiens et leurs conséquences sur le développement cérébral. Biologie Aujourd’hui, 213(1-2), 17-26.Remaud S et Demeneix B (2019) Les hormones thyroïdiennes régulent le destin des cellules souches neurales. Biologie Aujourd’hui, 213(1-2), 7-16 (résumé). Portail de l’anatomie   Portail de la physiologie"
médecine;"L'appareil cardiovasculaire, appareil circulatoire ou système sanguin, est un système circulatoire en circuit fermé qui assure le transport du sang du cœur vers les extrémités et les divers organes et, en retour, de ceux-ci vers le cœur. Il est constitué du cœur et des vaisseaux sanguins qui forment le système vasculaire, les vaisseaux lymphatiques qui composent le système lymphatique lui étant parfois associé.La circulation du sang permet le transport et l'échange interne d'une grande variété de substances biochimiques. Elle permet d'acheminer des nutriments, du dioxygène et des hormones aux cellules de l'organisme. Ces éléments proviennent du tube digestif, des poumons et des glandes endocrines. Le système cardiovasculaire assure également la collecte des déchets métaboliques des cellules, comme le dioxyde de carbone ou l'urée, acheminés vers les poumons, le foie et les reins. Enfin, il participe à la régulation de nombreux facteurs, tels que le taux de sucre.La relation entre le saignement et la mort a sans doute été mise en évidence très tôt dans l'histoire de l'humanité.Les Égyptiens avaient identifié le sang comme source de vie et siège de l'âme.Les dissections pratiquées par les médecins grecs de Cos au Ve siècle av. J.-C., dans la lignée d'Hippocrate, sur des animaux égorgés induisent des erreurs de représentation : les artères sont retrouvées vides, on pense donc qu'elles transportent de l'air, tandis que le foie et la rate sont gorgés de sang, ces deux organes sont donc considérés comme des éléments importants du transport du sang. Hérophile, médecin d'Alexandrie du IVe siècle av. J.-C., décrivit le premier la palpation du pouls. C'est à Erasistrate de Keos (320–250 av. J.-C.) que l'on doit la première description des valves veineuses.Galien (131-201) fait une description précise du réseau de veines et d'artères à partir de dissection de porcs, mais interprète faussement le rôle des organes. Selon lui, le sang est créé dans le foie à partir des aliments, il circule par les veines et va d'une part vers les poumons pour se mélanger à de l'air, d'autre part passe du ventricule droit au ventricule gauche par la paroi poreuse où il prélève la chaleur qu'il redistribue dans le corps ; arrivé aux extrémités du corps, le sang est consommé et ressort sous forme de transpiration.Les médecins musulmans traduisent les traités de médecine égyptiens découverts lors de l'invasion de l'Égypte au VIIe siècle, dont le traité de Galien sur la circulation (traduit par Averroès). À partir du Xe siècle, ils décrivent de nombreuses maladies cardiovasculaires (thrombose et collapsus pour Avicenne, péricardite pour Avenzoar). Ibn Al-Nafis, le père de la physiologie fait partie des autres précurseurs de la dissection humaine. En 1242 il a été le premier à décrire la circulation pulmonaire, les artères coronaires et la circulation capillaire qui forment la base du système circulatoire. L'œuvre d'Ibn Al-Nafis restera ignorée jusqu'en 1924 lorsque le Dr Al-Tatawi, médecin égyptien résidant en Allemagne, retrouve la traduction d'Andrea Alpago dans la Bibliothèque nationale de Berlin.En 1543, Andreas Vesalius publie ses travaux De humani corporis fabrica, dans lesquels la théorie physiologique de Galien fut adaptée à ses nouvelles observations.Cette théorie de Galien sur la Physiologie du Système circulatoire est bouleversée en 1551 quand Amato Lusitano (João Rodrigues de Castelo Branco, 1511-1568), médecin marrane portugais fuyant l'Inquisition en Italie, décrit la circulation du sang dans son ouvrage de sept volumes Curationum Medicinalium Centuriæ Septem en 1551 (la 1re édition) et pour la première fois, constate que les veines ont des valvules qui obligent le sang à retourner vers le cœur,. Cette découverte renverse ce qui était admis depuis Galien qui disait que le sang sortait du cœur par les artères et les veines et n'y retournait pas.Au XVIe siècle, Michel Servet (espagnol) décrit la circulation pulmonaire, d'abord en 1546,, puis publié en 1553. Ce fait est resté ignoré, car il a été publié dans un traité de théologie considéré comme hérétique, et dont l'édition a été détruite. On ne connait que trois exemplaires, conservés respectivement à Édimbourg, Paris et Vienne.L'italien Realdo Colombo est un des premiers à décrire parfaitement la circulation pulmonaire.C'est Andrea Cesalpino (1519-1603) qui utilise le premier le terme de « circulation » et qui en attribue le rôle au cœur, alors que l'on pensait jusqu'ici que le mouvement du sang dépendait de la pulsation des artères. Finalement William Harvey (1578-1657), élève de Fabrice d'Acquapendente (1537-1619), fait la première description complète du système circulatoire, dans son ouvrage Exercitatio Anatomica de Motu Cordis et Sanguinis in Animalibus de 1628. Il décrit notamment le sens de circulation et le rôle exact des valvules veineuses, et établit que la circulation est importante (plusieurs litres par minute) alors qu'on la croyait au goutte-à-goutte. L'idée première en reviendrait à Walter Warner. Marcello Malpighi identifie pour la première fois les capillaires au microscope en 1661.L'idée du cœur comme pompe et moteur de la circulation sanguine a encore été mise en doute à l'époque contemporaine, notamment par le cardiologue Leon Manteuffel-Szoege.L'appareil cardiovasculaire a pour fonction d'apporter aux différentes cellules les nutriments et le dioxygène dont elles ont besoin et d'éliminer leurs déchets comme le dioxyde de carbone. Cette circulation continue fonctionne grâce à des vaisseaux sanguins et une pompe : le cœur. On appelle artères les vaisseaux apportant les du cœur vers les tissus tandis que les veines apportent le sang des tissus vers le cœur.On distingue la circulation systémique (grande circulation), dont le rôle est de recharger les muscles et organes en dioxygène et en nutriments et la circulation pulmonaire (petite circulation) dont le rôle est d'assurer la ré-oxygénation du sang par les poumons et l'élimination par ceux-ci du dioxyde de carbone (hématose).Dans la grande circulation, le ventricule gauche du cœur expulse le sang via l'artère aorte vers les capillaires des différents organes où s'effectuent divers échanges. L'aorte est une artère élastique et épaisse capable de résister aux hautes pressions lors de la contraction cardiaque. Son élasticité contribue à la restitution d'un débit continu alors que les contractions cardiaques sont discontinues. Le sang est ensuite ramené au cœur droit via les veines caves supérieure et inférieure.Dans la petite circulation, le ventricule droit du cœur propulse le sang via l'artère pulmonaire vers les poumons. Le ventricule droit est moins épais que le gauche car il doit seulement assurer la vascularisation d'une partie restreinte du corps.Ainsi, dans la circulation systémique, les artères apportent du sang oxygéné aux tissus et les veines ramènent du sang appauvri en oxygène vers le cœur ; dans la circulation pulmonaire, les artères pulmonaires transportent du sang pauvre en dioxygène et les veines pulmonaires du sang riche en dioxygène.Les veines profondes et superficielles sont équipées de valvules. Ces « clapets », disposés tous les quatre à cinq centimètres, imposent un sens unique de circulation du sang, et empêchent le reflux.L'aspiration du sang des pieds vers le cœur est le résultat de plusieurs mécanismes. Ainsi, la compression de la voûte plantaire, la contraction des muscles des mollets et des cuisses chassent le sang vers le haut. Les mouvements respiratoires facilitent également le travail en diminuant la pression au sein du thorax lors de chaque inspiration. C'est pourquoi la marche et l'exercice physique permettent de limiter les risques d'insuffisance veineuse.Les éponges, qui appartiennent à l'embranchement des spongiaires ou porifères, n'ont pas de véritables tissus ni d'organes, elles sont également dépourvues de système vasculaire. Le corps d'une éponge ressemble à un sac percé de pores, d'où le nom Porifera (« qui portent des pores », du latin pori, « pores », et -fera, « qui portent »). Grâce à ces pores inhalants, l'eau peut pénétrer à l'intérieur d'une cavité centrale nommé spongocèle. L'eau ressort ensuite par une ouverture plus grande appelée oscule. Les éponges sont caractérisées par la présence des choanocytes. Le choanocyte est une cellule qui est dotée d'une collerette et d'un flagelle qui assure la circulation de l'eau et l'absorption des particules nutritives dans la cavité centrale des spongiaires.Les cnidaires sont dépourvus de système vasculaire. Ils ont l'aspect d'un sac renfermant un compartiment digestif central appelé la cavité gastrovasculaire (qui a rapport aux vaisseaux et à l'estomac). La cavité gastrovasculaire est une innovation évolutive par rapport aux éponges. Cette cavité a une seule ouverture servant à la fois de bouche et d'anus. En fait les cnidaires dépendent principalement de la diffusion pour obtenir l'oxygène dont ils ont besoin, car leur corps est formé de deux couches de cellules, l'une étant à l'extérieur appelé épiderme et l'autre étant enveloppé sur la cavité gastrovasculaire appelé gastroderme. Le contenu de la cavité gastrovasculaire doit être renouvelé en gaz et en nutriments. Pour ce faire l'organisme va « aspirer » les éléments puis les « recracher » à travers la membrane.Les plathelminthes sont dépourvus de système circulatoire. Contrairement aux animaux radiaires, les vers plats et les autres bilatériens sont triploblastiques. L'un des feuillets embryonnaires, le mésoderme permet la formation d'organes plus complexes, de systèmes d'organes et de vrais muscles. Cependant ils n'ont pas de cavité générale interne, autrement dit de cœlome. Les échanges se font par diffusion. Cette diffusion se fait grâce aux déplacements de l'animal qui agite les fluides interstitiels. Par contre, il existe des cellules spécialisées qui forment l'appareil excréteur.Le corps cylindrique des nématodes n'est pas segmenté. Les vers ronds sont revêtus d'un exosquelette appelé cuticule. Ils possèdent un tube digestif complet mais pas de système cardiovasculaire. Le liquide qui circule dans leur pseudocélome (cavité corporelle recouverte de mésoderme) apporte des nutriments à toutes les cellules du corps. La respiration se fait par diffusion au travers de pores qui percent la cuticule imperméable.Les annélides tels le lombric possèdent un système circulatoire fermé ; la circulation sanguine est assurée par la contractilité de certains vaisseaux et par des « cœurs » comportant des valvules et une seule cavité.Les mollusques ont généralement un appareil circulatoire ouvert avec un cœur constitué de deux oreillettes et d'un ventricule. Le liquide permettant le transport des gaz respiratoires est appelé hémolymphe. Le pigment respiratoire qu'il contient, l'hémocyanine n'est pas contenu dans des cellules sanguines mais circule librement dans le liquide. Il porte deux cations cuivreux Cu+ permettant le transport de l'O2. L'hémocyanine libre est incolore tandis que la forme lié à l'O2 (Cu2+) est bleue.Parmi les mollusques, les céphalopodes ont acquis un système circulatoire clos. Poissons Le cœur des poissons est constitué d'une oreillette et d'un ventricule disposés en série. Le sang traverse 2 lits capillaires, un dans les branchies pour échanger les gaz (capillaires branchiaux) et l'autre dans le restant du corps (capillaires systémiques). Amphibiens La circulation sanguine des amphibiens est close, avec un cœur possédant deux oreillettes et un ventricule. De ce fait, il se produit un mélange entre le sang oxygéné et le sang désoxygéné. Reptiles La circulation sanguine de beaucoup de reptiles est double, composée d'un petit circuit capturant l'oxygène au niveau pulmonaire, puis d'un grand circuit le distribuant à l'ensemble des organes. Elle est incomplète ; le cœur, n'étant pas totalement cloisonné, permet un mélange entre le sang artériel oxygéné et le sang veineux riche en dioxyde de carbone (sauf chez les crocodiles qui possèdent une circulation complète). Les reptiles ont une circulation double, incomplète et fermée. Ils ont aussi deux oreillettes et un ventricule. Mammifères et oiseaux La circulation sanguine chez les oiseaux et les mammifères (dont l'humain) est assurée par un cœur ayant deux oreillettes et deux ventricules, ce qui permet alors une séparation entre le sang oxygéné et le sang désoxygéné. La circulation des mammifères est identique à celle de l'humain.VasculariteInsuffisance veineuseL'accident vasculaire cérébral, plus communément appelé AVC, est une affection due à un problème vasculaire qui est à l'origine d'une mauvaise alimentation en sang du cerveau. Il existe deux types d'AVC :l'AVC ischémique provoqué par un caillot qui obstrue une artère du cerveau stoppant ainsi son approvisionnement sanguin ;l'AVC hémorragique provoqué par une rupture de l’artère due à une hypertension artérielle.François Boustani, La Circulation du sang: entre Orient et Occident : l’histoire d’une découverte, Philippe Rey, 2007.Ressource relative à la santé : (en + es) MedlinePlus Modélisation de la circulation sanguine générale et lymphatique (animation flash).Visualisation de la circulation du sang et de sa teneur en O2 et CO2 (animation flash).« Circulation sanguine : un petit tour, et puis sang va », Eurêka ! , France Culture, 27 juillet 2022. Portail de l’anatomie   Portail de l’hématologie   Portail de la médecine   Portail de la physiologie"
médecine;"Le système digestif, appelé aussi appareil digestif, est l'ensemble des organes qui chez les animaux a pour rôle d'assurer l'ingestion et la digestion des aliments pour en extraire l'énergie et les nutriments nécessaires à la survie de l'organisme qui sont ensuite absorbés par l'organisme. Ce système est essentiel à la vie des animaux et se retrouve nécessairement pour toutes les espèces. Le rôle de ce système biologique est également d'assurer l'excrétion des matières alimentaires qui n'ont pu être absorbées par l'organisme.Le système digestif varie de manière plus ou moins importante d'un animal à un autre. Ainsi il peut être très simple pour les organismes unicellulaires et les éponges où il n'y a pas de système digestif différencié et où la digestion des aliments se fait au sein même de leurs cellules, ou bien être différencié mais sans spécialisation comme chez les cnidaires et les vers plats où la digestion est assurée au sein de la cavité gastrovasculaire et où la bouche et l'anus sont indifférenciés. La spécialisation commence lorsque la bouche et l'anus sont différenciés et qu'ainsi les aliments passent successivement dans une série d'organes plus ou moins nombreux selon les espèces. Les nématodes sont un exemple de système digestif spécialisé très simple ne disposant que d'une bouche, d'un pharynx, d'un intestin et d'un anus. Mais le nombre d'organes et leur complexité peut augmenter selon le régime alimentaire qui nécessite plus ou moins de traitements pour que la digestion soit assurée. C'est ainsi que les ruminants disposent de plusieurs organes supplémentaires et d'un estomac à plusieurs chambres.L'étude du système digestif d'un animal permet de déterminer de nombreuses choses sur son régime et comportement alimentaire. C'est ainsi qu'avec les seules dents, les anthropologues sont capables de déterminer si un hominidé était en majorité herbivore ou omnivore.Les plantes sont capables de produire des composés organiques (elles sont autotrophes), contrairement aux animaux qui en sont incapables et sont ainsi obligés de se nourrir d'êtres vivants (ils sont hétérotrophes). Les composés organiques ingérés servent à deux choses :comme source d'énergie ;comme éléments que l'organisme est incapable de synthétiser, ce sont les nutriments et minéraux essentiels.Les molécules organiques ingérées sont relativement complexes et nécessitent d'être digérées par l'organisme pour être réduites en plus petites molécules. Une fois réduites, elles sont absorbées, tous les autres éléments sont évacués du système digestif.Bien que les éponges soient des animaux, la digestion de leur nourriture est intracellulaire, comme chez la bactérie. Mais des animaux plus complexes, comme l'hydre, ont développé une cavité gastrovasculaire où la nourriture est acheminée pour y être digérée sous l'effet de diverses enzymes.Le système digestif se spécialise lorsqu'il y a une séparation de la bouche (par où pénètre la nourriture) et l'anus (par où sont expulsés les déchets). Dans le cas de l'hydre, la bouche et l'anus sont indifférenciés, ce qui n'est pas le cas de la nématode qui a développé un tractus digestif unidirectionnel où la nourriture passe successivement par la bouche, le pharynx, l'intestin et l'anus.Selon leurs régimes alimentaires, les animaux sont classés en trois groupes :les herbivores, qui se nourrissent exclusivement de plantes ;les carnivores, qui se nourrissent exclusivement d'autres animaux ;les omnivores, qui se nourrissent aussi bien de plantes que d'animaux.Ces régimes alimentaires ont nécessité la spécialisation de tous les organes composant le système digestif.La bouche est le point d'entrée de la nourriture. Chez certains animaux, comme l'Homme, des glandes salivaires produisent ce liquide qui sert à mouiller les aliments pour faciliter la déglutition, mais aussi pour éviter l'abrasion des tissus composants le pharynx et l'œsophage. La salive contient des enzymes servant à prédigérer certains composés organiques, comme c'est le cas de l'amidon. La digestion salivaire est généralement minimale chez les carnivores, car ils ingurgitent leur nourriture sans véritablement la mâcher, contrairement aux herbivores.La bouche a des formes très différentes selon les espèces. Un exemple de cette diversité est présent chez les insectes où les formes de leurs pièces buccales sont aussi différentes que leur régimes alimentaires.Selon le régime alimentaire, les dents ont différentes utilités. Chez les carnivores elles servent à arracher la nourriture et sont à cet effet pointues et sans surface plate. contrairement aux herbivores qui mâchent les plantes pour en extraire la cellulose, leurs dents sont donc plates et crénelés.Chez les omnivores, comme l'Homme, on retrouve ces deux types de dents. La partie antérieure correspond à un carnivore avec les canines, alors que la partie postérieure est de type herbivore avec les molaires.Chez les animaux dépourvus de dents (comme les oiseaux), la mastication est assurée par un organe spécialisé, le gésier.La langue assure plusieurs rôles et pas nécessairement liés à la nutrition comme c'est le cas de la phonation. Elle assure le mélange de la nourriture avec la salive et une fois les aliments correctement mâchés, la langue déplace ces derniers vers l'arrière de la bouche où la déglutition commence.Lorsque les aliments arrivent dans le pharynx, un réflexe déclenche plusieurs mouvements différents selon les espèces. Ainsi chez les vertébrés le larynx pousse la glotte contre l'épiglotte évitant ainsi aux aliments de se retrouver dans le tractus respiratoire.Le réflexe déclenché par la pression dans le pharynx déclenche également dans l'œsophage, qui est un tube musculeux, un péristaltisme, qui est une série de contractions musculaires permettant aux aliments mâchés de se déplacer vers l'estomac. L'entrée dans l'estomac est, selon les espèces, contrôlés par un sphincter et évitant ainsi que les aliments dans l'estomac, qui baignent dans des sécrétions acides, ne remontent dans l'œsophage et n'abîment ce dernier.L'estomac est un organe creux situé sous le diaphragme dont une poche qui sécrète de l'acide chlorhydrique concentré qui est essentiel dans la digestion. Selon le régime alimentaire des animaux, l'estomac peut avoir une structure très différente. Ainsi certains herbivores, tous les carnivores et omnivore ont une seule poche et sont à ce titre des animaux mono gastriques. Les ruminants par contre sont plurigastriques car il est nécessaire pour eux en raison de leur régime alimentaire riche en cellulose de ruminer leur nourriture pour la remastiquer.Le gésier est une partie musculaire (muscle fibreux qui forme une poche) de l'estomac des archosauriens, groupe qui comprend les oiseaux et les crocodiliens. Il a aussi des fonctions glandulaires.C'est aussi la partie située autour des cloaques, notamment chez la poule.Le gésier fait partie des structures anatomiques dites « pro-ventricule » (poches précédant l'estomac dans le tractus digestif), structure également présente chez certains poissons et invertébrés. Des évidences fossiles nous permettent de croire qu'il était présent chez certains dinosaures. Cet organe leur permet de broyer les aliments durs, en avalant des cailloux dits grit (diminutif de gastrolithes).Il n'y a pas d'équivalence au gésier chez les mammifères. La panse des ruminants est un autre type de structure.L'intestin est un tube digestif comptant 3 différentes parties principales (Duodénum-jéjunum-ileum) qui servent à absorber les nutriments comme les Acides Aminés (présents dans les protéines), les Acides Gras et les Oses (sucres). Son pH est plutôt basique à cause de la bile, qui est sécrétée par le foie et concentrée dans la vésicule biliaire. Elle est relarguée dans le duodénum (avec le HCl qui provient de l'estomac).Son rôle est d'absorber l'eau, récupérer quelques nutriments (composés organiques et minéraux) et de préparer les selles.Le rôle du côlon est principalement de stocker les déchets, de récupérer les liquides (H2O) et semi-liquide, de maintenir l'équilibre hydriqueC’est la dernière partie du rectum. La matière fécale y est éjectée lorsqu'elle s'est accumulée dans le côlon et passe le sphincter anal .L'appendiceLe foie s'occupe du stockage des glycogènes dans l'organisme au niveau des muscles et de la sécrétion de la bile pour la digestion.La vésicule biliaire est un organe piriforme, en forme de petite bourse, appendue à la face inférieure du foie. Ce réservoir membraneux (de 8 à 10 cm de long sur 3 à 4 cm de large) stocke la bile entre les phases de sécrétion. On distingue trois parties : le fond, le corps et le col qui se termine par le canal cystique. Ce canal cystique de 3 centimètres de long fait communiquer la vésicule biliaire au canal hépatique commun pour former le canal cholédoque qui s'abouche dans le duodénum.La vésicule peut être l'objet d'anomalie de position, de nombre sans conséquence médicale. Elle peut contenir des calculs biliaires: c'est la lithiase vésiculaire à l'origine de la cholécystite aiguë.Le pancréas est un organe abdominal, une glande annexée au tube digestif appartenant à la cavité péritonéale située derrière l'estomac, devant et au-dessus des reins. Ses fonctions dichotomiques de glandes à sécrétions exocrine comme les enzymes digestifs et endocrine comme le fameux Insuline font du pancréas une glande amphicrine. Chez l'Homme, le pancréas avoisine les 15 cm de long pour une masse allant de 70 à 100 g.Système digestif humainSystème digestif aviaireRessource relative à la santé : (en) Medical Subject Headings  Portail de la physiologie   Portail de l’anatomie"
médecine;"Une glande endocrine est une glande interne qui sécrète des hormones dans la circulation sanguine directement, plutôt que via un canal comme une glande exocrine. Ces hormones exercent alors leur action spécifique sur des organes cellules ou récepteur distants. On trouve des glandes endocrines chez la plupart des animaux, y compris chez les invertébrés.Les hormones agissent comme des sortes de messagers biochimiques, régulant de nombreuses fonctions de l'organisme telles que la croissance et le développement, la différenciation sexuelle, la reproduction, le métabolisme, la pression artérielle, la glycémie et assure l'homéostasie de nombreux paramètres corporelsLes animaux possèdent, en plus du système immunitaire, deux grands réseaux de communications internes : le système nerveux et le système endocrinien. L'appareil endocrinien transmet ses messages grâce à la sécrétion des hormones, généralement des peptides ou des protéines, tandis que le système nerveux utilise les neurones, qui libèrent des neurotransmetteurs dans les synapses pour transmettre l'influx nerveux à d'autres neurones. Mais ces deux systèmes ont des inter-relations profondes, puisque certains neurones synthétisent également des peptides, appelés neuropeptides, qui sont alors libérés dans la circulation sanguine : par exemple, chez les mammifères, les fibres nerveuses hypothalamiques à somatostatine ou à hormone thyréotrope (TRH) libèrent dans l'éminence médiane leurs produits de sécrétion, qui atteignent l'hypophyse antérieure par l'intermédiaire des vaisseaux du système porte hypothalamo-hypophysaire.Les épithéliums glandulaires endocrines peuvent être sous forme :de glandes endocrines bien individualisées (exemple : thyroïde, hypophyse, testicules, ovaires…) ;d'amas de cellules endocrines (exemple : îlots de Langerhans du pancréas, cellules de Leydig des testicules) ;dispersée, diffuse, au sein d'autres organes (exemple : cellules endocrines du tube digestif, comme les cellules à gastrine de l'estomac, les cellules à sécrétine du duodénum, ou les cellules à glicentine du côlon).Certaines glandes sont amphicrines, c'est-à-dire qu'elles sont à la fois endocrines et exocrines, comme les gonades (testicules et ovaires) ou le pancréas, sécrétant vers le milieu extérieur, généralement par l'intermédiaire de canaux excréteurs.Chez les mammifères, les glandes endocrines pures sont la thyroïde, les parathyroïdes, les surrénales, l'hypophyse, la glande pinéale (ou épiphyse). Chez les arthropodes, on peut citer la glande de mue.Enfin, d'autres organes peuvent également jouer un certain rôle endocrine : par exemple, les cellules de la graisse (ou adipocytes) sécrètent de la leptine ; les ovaires et les testicules produisent naturellement les gamètes, mais ils ont également une fonction endocrine.Pendant la grossesse, le placenta joue également le rôle d'une glande endocrine ; il devient le principal producteur d'hormones stéroïdes. Les hormones produites par ces glandes peuvent être des peptides (par exemple la TRH) ou des protéines (par exemple l'insuline) ; des stéroïdes comme l'œstrogène, la progestérone et la testostérone ; ou des dérivés d'acides aminés comme les hormones thyroïdiennes.HypothalamusHypophyseSurrénaleThyroïdeOvaire ou testiculeParathyroïdeThymusPancréasGlande pinéaleGlande de mueOvaire ou testiculeComplexe allato-cardiaqueEndocrinologie Portail de l’anatomie   Portail de la médecine   Portail de la physiologie"
médecine;"Une hormone est une substance produite de façon naturelle par un organe du corps, qui est transportée par le sang et agit sur d’autres organes. C'est une substance chimique biologiquement active, synthétisée par une cellule glandulaire et sécrétée dans le milieu intérieur où elle circule, agissant à distance et par voie sanguine sur des récepteurs spécifiques d'une cellule cible. Elle transmet un message sous forme chimique et joue donc un rôle de messager dans l'organisme. Le terme « hormone » (du grec ?????, mettre en mouvement) a été adopté par Starling en 1905 pour désigner les substances qui assurent la liaison entre les divers organes.Une hormone est une molécule messagère produite par le système endocrinien (une glande endocrine ou un tissu endocrinien) en réponse à une stimulation et capable d'agir à très faible dose.Elle est ensuite diffusée dans l'ensemble de l'organisme. Les hormones animales sont sécrétées par des glandes spécialisées et diffusées par le sang ou la lymphe. Les hormones, comme les autres molécules circulantes, sont excrétées dans les excréments et l'urine, parfois après conjugaison et/ou dégradation. Des molécules apparentées, les phéromones, sont produites par des glandes externes, qui servent par exemple chez l'animal à marquer le territoire, la dominance dans le groupe ou les dispositions sexuelles. Chez les végétaux, les hormones sont soit véhiculées par la sève, soit transportées activement par les cellules, soit diffusées entre les cellules dans la paroi ou vers l'extérieur, avec émission éventuelle dans l'atmosphère sous forme gazeuse (éthylène par exemple) ou dans la rhizosphère dans le sol.L'organe émetteur agit ainsi à distance sur l'ensemble des organes cibles de l'organisme ou d'organismes voisins de la même espèce, voire d'organismes symbiotes dont les récepteurs sont activés au contact des hormones spécifiques (interactions durables).Les hormones ont une fonction de communication qui, en comparaison avec celle du système nerveux, peut être qualifiée de lente, continue et diffuse. Les concentrations hormonales, étudiées en endocrinologie, contiennent donc des informations représentatives de différents états. Elles régulent ainsi l'activité d'un ou plusieurs organes ou organismes dont elles modifient le comportement et les interactions. Exemple : la mélatonine a une concentration dans le sang qui est régulée par les variations de la lumière (alternances jour/nuit) : comme elle est produite pendant la nuit, sa concentration circulante est plus élevée en hiver (nuits plus longues) qu'en été. C'est elle qui est responsable des variations saisonnières de la reproduction chez les petits ruminants (ovins, caprins), les chevaux et de très nombreuses espèces sauvages.La régulation de la sécrétion hormonale se fait par l'intermédiaire de rétrocontrôle, dit « positif » en cas d'augmentation de sécrétion de l'hormone, et « négatif » s'il induit une diminution de la sécrétion hormonale. Cette régulation est également influencée par de nombreux cycles hormonaux ou systèmes en cascade où la concentration en une première hormone commande la libération de la (ou des) suivante(s), ou au contraire l'inhibition de leur sécrétion. Exemple : la GnRH contrôle la libération de FSH et LH. (Elle agit donc sur leurs propres cycles de rétrocontrôle pour influer sur leurs concentrations). FSH et LH jouent un rôle majeur dans la libération d'hormones sexuelles dans le sang. C'est en fonction de cette concentration en hormones sexuelles qu'est libérée la GnRH. Les hormones interviennent dans de nombreux processus, dont la reproduction, la différenciation cellulaire, l'homéostasie, ou encore la régulation des rythmes chronobiologiques…Le rôle des hormones sexuelles externes est encore très discuté chez l'Homme qui a, par rapport aux autres mammifères, un odorat faible et une sexualité plus complexe, mais certaines études laissent penser qu'il existe. Les poils des aisselles et de la zone pubienne, du scrotum et du périnée pourraient ainsi jouer un rôle de « diffuseur hormonal », par exemple d'androstadienone (dérivé de la testostérone présent dans la sueur et d'autres sécrétions masculines, qui influe sur l'humeur des femmes et affecte la sécrétion de l'hormone lutéinisante stimulant l'ovulation). Il a été montré que des extraits de sueur féminine placés sur la lèvre supérieure, sous les narines d'autres femmes pouvaient modifier leurs taux d'hormones et synchroniser leurs cycles menstruels avec le cycle de la femme ayant fourni l'échantillon de sueur. On a aussi montré que des extraits de sueur masculine, déposé sur la lèvre supérieure d'une femme élèvent le taux de cortisol de cette femme dans les 15 minutes qui suivent, avec des effets persistants une heure (on ignore encore si c'est le taux de cortisol qui affecte l'humeur des femmes ou l'inverse).Les endocrinologues sont amenés à travailler sur de nouvelles questions telles que :relations entre hormones ou perturbateurs endocriniens et cancers ;conséquences du traitement substitutif de la ménopause ;effets de la contraception hormonale au long terme sur la santé, et aussi sur l'environnement (hormones diffusées via les urines et non traitées par les stations d'épuration) ;influence des hormones sur le développement de l’obésité et du diabète de type 2 ;impacts de l'usage d'hormones dans l'alimentation animale.Chez les vertébrés, on distingue les classes chimiques suivantes :Les hormones dérivées d'amines, qui sont constituées d'un seul acide aminé (la tyrosine ou le tryptophane) mais sous une forme dérivée. Exemples : les catécholamines et la thyroxine ;Les hormones peptidiques ; qui sont des chaînes d'acides aminés, donc des protéines, appelées peptides pour les plus courtes. Exemples d'hormones à base d'oligopeptides : le TRH et la vasopressine. Exemples d'hormones de type protéines : l'insuline et l'hormone de croissance. Ou encore l'ocytocine, découverte en 1954, fabriquée dans l’hypothalamus et la post-hypophyse, qui stimule les contractions de l'utérus chez la femme enceinte et accélère le travail de l'accouchement ;Les hormones stéroïdes, qui sont des stéroïdes dérivés du cholestérol. Les principales sources sont la cortico-surrénale et les gonades. Exemples d'hormones stéroïdes : les œstrogènes, la testostérone et le cortisol. Les hormones du type stérol tel le calcitriol sont un système homologue ;Les hormones à base de lipides et de phospholipides sont dérivées de lipides comme l'acide linoléique et de phospholipides comme l'acide arachidonique. Les eicosanoïdes forment la classe principale, parmi laquelle les plus étudiées sont les prostaglandines.Les hormones végétales sont plus rigoureusement appelées phytohormones ou facteurs de croissance car ce ne sont pas à proprement parler des hormones. Elles ont souvent comme fonction d'assurer la croissance de la plante ou sa morphogenèse.C'est le cas notamment de l'auxine qui contribue à la formation des organes de la plante (les racines par exemple) et à sa croissance mais intervient aussi dans les phénomènes de tropisme.Elles se distinguent des hormones animales en plusieurs points :leur sécrétion n'est pas assurée par des organes spécifiques de la plante (tout juste existe-t-il des zones de synthèse privilégiées) ;leur effet varie en fonction de leur concentration (exemple : à faible concentration 10?10 g/mL, l'auxine a un effet discret positif sur la croissance racinaire. À de plus fortes concentrations, 10?8 g/mL, elle inhibe l'élongation et induit la rhizogenèse) ;elles agissent rarement seules : leurs effets résultent bien souvent d'une action coordonnée de plusieurs hormones (exemple : stimulation de la division cellulaire grâce à l'action conjuguée de l'auxine et des cytokinines).Sur le même mode d'action chimique :dans le cas de diffusions limitées à une zone restreinte, on parle d'hormone paracrine ou substance paracrine. Il existe aussi un cas particulier où la substance agit sur la cellule productrice, on parle alors d'hormone autocrine ;les hormones libérées par des neurones sont appelées des neurohormones (à ne pas confondre avec les neuromédiateurs). Elles sont sécrétées de la même manière que les neurotransmetteurs dont le mode d'action est identique, mais dans le sang et non dans la synapse. Il arrive d'ailleurs qu'une même molécule soit appelée neurotransmetteur ou hormone suivant son utilisation ou le contexte dans lequel elle est étudiée.Certaines substances ont des effets chez les êtres humains et les animaux qui sont similaires à ceux des hormones naturelles. Parmi les androgènes naturellement présents dans certaines huiles, on trouve le campestérol et le stigmastérol. Ces substances sont appelées phyto-androgènes. Les phyto-androgènes sont beaucoup plus communs. On les retrouve dans des aliments de base comme le soja. Plusieurs substances chimiques entraînent également des effets hormonaux chez les mammifères et perturbent l'action des hormones naturelles. Ces substances sont classées dans la catégorie des perturbateurs endocriniens. Le terme perturbateur endocrinien est souvent utilisé comme synonyme de xénohormone (xéno-androgène s'il entraîne des effets androgéniques et xénoestrogène pour les effets œstrogéniques) même si ce dernier terme peut désigner tout composé naturel ou de synthèse présentant des propriétés similaires à celles des hormones (se liant généralement à certains récepteurs hormonaux).HormonothérapieHormone sexuelleSystème endocrinienPerturbateur endocrinienListe d'hormonesPhéromonesHormone gastro-intestinaleLes Hormones, PUF, coll. Que sais-je ?, n°63 : [1] Portail de la biochimie   Portail de la médecine   Portail de la physiologie"
médecine;"Un organe est un groupe de tissus collaborant à une même fonction physiologique. Certains organes assurent simultanément ou alternativement plusieurs fonctions, mais dans ce cas, chaque fonction est généralement assurée par un sous-ensemble de cellules.Le niveau d'organisation supérieur à l'organe sont les appareils et les systèmes, qui remplissent un ensemble de fonctions complémentaires. Le niveau d'organisation inférieur à l'organe est le tissu.L'étude des organes relève de l'anatomie, qui fait partie du domaine de la biologie.Les organes peuvent être décrits par des planches anatomiques, des préparations anatomiques, des représentations en cire ou des modèles informatiques.Le mot organe a une étymologie latine d'origine grecque :du latin « organum », « instrument, outil » ;du grec « ??????? » (organon) de même sens, « instrument, outil ».Dans le domaine de la recherche médicale et de la chirurgie reconstructrice, on parle aussi :d'organes artificiels (exemple : cœur artificiel) ;d'organes bio-artificiels, qui jouent le rôle de microcosme, par exemple pour l'évaluation toxicologiques de produits, en alternative à l'expérimentation animale (alternatives demandées en Europe par la directive REACH). Ils évoluent parfois vers des systèmes de tests miniaturisés, avec par exemple des cellules fixées sur une puce électronique qui réagissent à des toxiques ou médicaments, éventuellement dilués dans un fluide circulant. En France, l'INERIS, avec l'UTC produit des modèles mathématiques pour évaluer la cinétique et les effets des contaminants in vivo à partir de ce type de puces et d'organes bio-artificiels. À titre d'exemple, cette approche doit aider à étudier de manière non invasive et moins coûteuse les effets toxicologiques de l'exposition chroniques à de faible dose de résidus chimiques présent dans les aliments (dont néoformés via la cuisson), pour mieux comprendre et traiter les maladies inflammatoires chroniques intestinales (MICI) ;sous-ensemble d'un appareillage, composé de plusieurs pièces assemblées, destiné à effectuer une opération particulière ou un travail spécifique :mécanique (« transmission », organe de mécanique automobile),électrique (organe de commande) interrupteur, régulateur.La communauté des microbes (« flore intestinale ») qui habitent symbiotiquement l'intestin de l'humain (environ 100 000 milliards de bactéries par être humain) ou des animaux est parfois considérée comme une sorte d'organe virtuel. Le mot microbiote désigne cet organe virtuel.Par exemple, certaines vitamines indispensables ne peuvent être fabriquées par le microbiote. Le « Metagenomics of the Human Intestinal Tract », un programme initié en 2008 pour identifier le métagénome de ces microbes a montré qu'il existe chez l'humain trois groupes de composition bactérienne intestinale spécifiques (dit entérotypes) que l'on conserve toute sa vie, et qui est caractéristique à l'individu (comme le groupe sanguin).Par extension, on parlera d'organe dans le cadre des organisations humaines (exemple : organe de presse).Une pièce mécanique peut être désigné par le mot organe (exemple : organe de transmission dans un véhicule).On classe les organes selon leur fonctions, leur disposition leur nombre (un organe peut appartenir à plusieurs classes).Classement partiel : « organes vitaux », qui sont ceux dont l'organisme ne peut se passer ou qui le mettent en danger de mort en cas de blessure ou amputation. Ce sont généralement aussi des organes internes tels que le cerveau ou des viscères ;organes creux (cœur ou vessie) ;organes multiples (ex. : ganglions lymphatiques), doubles (ex. : poumons, reins), uniques (ex. : foie, cœur, cerveau) ;organe lymphoïde ;organes génitaux (de la reproduction) ;organes préhensiles (mains, pieds, bec, tentacules, queue chez certains vertébrés) ;organes embryonnaires (par exemple, chez l'humain : les branchies ou la queue qui apparaissent puis disparaissent au cours de l’embryogenèse) et organes-reliques (comme chez l'humain le coccyx, reste d'une queue, ou l'appendice, relique de cæcum, bien que ce cas soit discuté, l'appendice pouvant avoir un rôle immunitaire).Selon Ginet et Roux, les premiers organes seraient apparus chez les Plathelminthes ou vers plats. « C'est en effet avec les plathelminthes que le niveau correspondant aux organes est atteint, les précédents embranchements ne dépassaient pas le stade cellulaire ou le stade tissulaire ».Crâne, contenant l’encéphale (cerveau, cervelet et tronc cérébral)Œil, oreillesLangue, dentsCuir cheveluLarynx, pharynxGlandes salivairesGlande thyroïde et glandes parathyroïdesVertèbresMoelle spinale Organes féminins du bassin OvairesTrompe de FallopeUtérusVaginVulveClitoris Organes masculins du bassin TesticulesPénisProstateVésicules séminalesLes fonctions physiologiques sont :PoumonsCoeurCerveauMusclesFoie et intestinsReins et vessiePeauPoumonsRacinesRhizomeTroncTigeBrancheLa structure des organismes biologiques qui constituent la biosphère peut être décomposée en plusieurs niveaux d'organisation : atomique, moléculaire, cellulaire, tissulaire, des organes, des systèmes, et enfin celui de l'organisme dans sa totalité fonctionnelle, et éventuellement de supers-organismes (essaim d'abeilles, récif corallien, etc.). Pour la description de cette structure en niveaux emboîtés, Georges Chapouthier a proposé l'utilisation du concept de mosaïque. Comme dans une mosaïque au sens artistique du terme, qui laisse à ses tesselles l'autonomie de leur couleur ou de leur forme, chaque niveau de complexité du vivant intègre les niveaux inférieurs comme des parties, en leur laissant une autonomie de fonctionnement.L'étude scientifique du vivant se fait par des recherches sur les éléments de chacun de ces niveaux, puis par la compréhension des interactions entre ces différents niveaux (voir l'article « Méthode scientifique »).L'étude du niveau des organes permet de comprendre la structure, la fonction et le fonctionnement des organes, qui constituent les différents systèmes fonctionnels de l'organisme (système nerveux, système digestif, système immunitaire…).AnatomieOrgane ou structure vestigialeClassement thématique des neurosciencesGreffe d'organeDon et vente d'organeOrganismeOrganicisme Portail de la biologie   Portail de la physiologie   Portail de l’anatomie"
médecine;"L’appareil urinaire du système excréteur est l’appareil permettant l’évacuation des produits du catabolisme d'un vertébré sous une forme liquide, l'urine, et assure par conséquent l'épuration du sang ainsi que le maintien de l'homéostasie au sein de l'organisme. Aussi, il maintient l'équilibre sanguin, soit le volume et la composition chimique du sang. Pour ce faire il élimine entre autres les surplus de certains minéraux, nommés électrolytes, et renvoie dans le sang les substances utiles au bon fonctionnement de l'organisme. Chaque jour, un être humain produit 800 à 2000 millilitres d'urine. L'appareil urinaire fait partie du système excréteur.Le système excréteur est constitué des organes excréteurs (néphridies, tubes de Malpighi chez les invertébrés, reins chez les vertébrés, cellules à chlorures des branchies des téléostéens, glandes à sel des oiseaux, et les canaux excréteurs associés).Le système urinaire ou appareil urinaire est un ensemble d'organes dont le rôle est de filtrer puis d'évacuer les déchets de l'organisme sous forme liquide. Il comprend chez les vertébrés une succession d'organes rétro- puis sous-péritonéaux : les reins, qui fabriquent l'urine, et les voies excrétrices urinaires extrarénales constituées par les deux uretères qui transportent l'urine, la vessie qui la stocke et l'urètre qui permet de l'évacuer (phénomène de miction qui aboutit à l'évacuation périodique, complète et contrôlée des urines)L'appareil urinaire se compose des reins, des uretères, de la vessie, de l'urètre et du méat urinaire. Il se forme et commence à fonctionner avant la naissance. Quand la vessie contient 250 mL d'urine, l'envie d'uriner se fait sentir.Le rôle de cet appareil est de former l'urine qui sera évacuée. L'urée est excrétée par les reins qui fabriquent l'urine ; cette urine est acheminée par l'uretère jusqu'à la vessie, une poche retenant l'urine, ensuite rejetée à l'extérieur de l'organisme lors de la miction par l'urètre s'abouchant au méat urinaire.Le corps humain possède deux reins. Toutefois, un seul rein peut suffire à l'accomplissement des fonctions d'épuration et d'élimination.Ils ont la taille d'un poing, la forme d'un haricot et sont de couleur bordeaux. Les reins sont fixés sous les côtes de part et d'autre de la colonne vertébrale, ils sont en liaison avec l'artère rénale, par laquelle arrive le sang à filtrer.Le rein possède une fonction sécrétoire (filtration du sang au niveau des glomérules) puis excrétoire à partir du pyelon (triangle à base issue du hile rénal) origine de l'uretère. On parle de jonction pyelo-urétérale. Chaque rein contient environ 1 million de néphrons. Sur chaque rein, on retrouve des glandes surrénales. Elles sécrètent des hormones qui modifient la quantité des urines produites. Le sang est épuré au niveau du néphron, dans lequel certains éléments sont réabsorbés (ions minéraux, glucose, eau, acides aminés) et retourneront à la circulation sanguine par la veine rénale.Les déchets récupérés constituent une urine primitive qui sera déversée dans le bassinet, puis dans l'uretère attenant au rein dont elle est issue.Ils sont le prolongement des reins. Leur rôle est de collecter l'urine au niveau du bassinet. Ils se présentent comme des tubes dont l'extrémité supérieure prend une forme d'entonnoir, composée de fibres musculaires lisses évitant les reflux d'urine. L'uretère se dirige vers le bas, en avant et  dedans pour rejoindre la partie postéro-supérieure de la vessie. On distingue ainsi à l'uretère  quatre parties :l'uretère lombaire (12 cm) ;l'uretère iliaque (3 cm) ;l'uretère pelvien (12 cm) ;l’uretère mural ou vésical (correspond à la traversée de la paroi vésicale par l’uretère).La vessie se présente sous la forme d'une poche dont les parois sont faites de muscles lisses (le détrusor) et de tissu épithélial et voit s'aboucher à sa partie inférieure l'urètre : on parle de col vésico-urétral.Elle recueille l'urine qui lui parvient par les uretères. Sa capacité est d'environ 300 à 600 ml. L'urine est évacuée au niveau de l'urètre lors de la miction.Le contrôle de la miction est réalisé par un sphincter lisse à commande involontaire et par un sphincter strié volontaire utilisé en cas de retenue forcée (ou en période post-opératoire).Constituée en majeure partie d'eau 95 %, de sels minéraux 2 % (chlorures, phosphates, sulfates, sels ammoniacaux) et des matières organiques 3 % (urée, créatine, acide urique, acide hippurique).Son nom vient d'une molécule issue de la dégradation des protéines : l'urée. Celle-ci est en partie responsable de la couleur jaunâtre de l'urine.En moyenne, les reins produisent 800 à 2000 mL d'urine chaque jour.La couleur de l'urine provient de deux pigments : l'urochrome et l'urobiline. La couleur de l'urine peut beaucoup varier sur 24 heures, car le taux d'urobiline varie énormément en fonction de la sécrétion biliaire.La cystite, la néphrite, la pyélonéphrite ainsi que l'urétrite sont des inflammations des organes de l'appareil urinaire.La glycosurie représente le taux de glucose dans l'urine. Sa valeur normale est nulle.Le glucose est en principe filtré et réabsorbé par le rein. Au-delà d'une certaine glycémie (taux de glucose sanguin), les capacités de réabsorption du rein, qui sont d'environ 9mmol/L, sont saturées : l'excédant est donc évacué par l'urine.Une glycosurie peut être signe de diabète.La rétention aiguë d'urine est l'incapacité d'uriner malgré le remplissage de la vessie. Elle peut avoir plusieurs origines.MictionReinUretèreUrologieVessieRessources relatives à la santé : FMA TA2 Uberon Xenopus Anatomy Ontology (en) Medical Subject Headings (en) NCI Thesaurus (no + nn + nb) Store medisinske leksikon (la + en) TA98 (cs + sk) WikiSkripta (en) Excretory products and their elimination Portail de l’anatomie   Portail de la médecine   Portail de la physiologie"
médecine;
"ire, ma""";
médecine;"En biologie, la notion de fonction renvoie à plusieurs significations. En biologie moléculaire et en biochimie, la notion de fonction décrit le rôle biologique d'un composant de la cellule : protéine, gène, organite cellulaire, etc. En physiologie, une fonction est un processus ou ensemble d'actions coordonnées concourant au fonctionnement d'un organisme, accomplies par un ou plusieurs organes : la fonction respiratoire, la fonction circulatoire, etc.Une fonction physiologique peut être définie comme ""l'ensemble des actes accomplis par une structure organique défini en vue d'un résultat déterminé"".ÉvolutionPhylogénie Portail de la biologie cellulaire et moléculaire   Portail de la biochimie   Portail de la physiologie"
médecine;"La maladie de Basedow ou Graves-Basedow est une hyperthyroïdie auto-immune (maladie de la thyroïde). La personne atteinte produit des anticorps anormaux (stimulant le recepteur de la TSH) dirigés contre les cellules folliculaires de la thyroïde. Plutôt que de détruire ces cellules, comme le ferait tout anticorps normal, ces anticorps reproduisent les effets de la TSH et stimulent continuellement la libération d'hormones thyroïdiennes, provoquant une hyperthyroïdie accompagnée de signes cliniques spécifiques. La maladie de Basedow ou de Graves, plus fréquente chez la femme que chez l'homme, se manifeste le plus souvent par une accélération du métabolisme basal, une diaphorèse, des pulsations cardiaques rapides et irrégulières, une augmentation de la nervosité et une perte pondérale. Il s'agit de sa forme la plus fréquente.Elle doit son nom à Carl von Basedow.La maladie de Basedow peut toucher tout le monde, mais essentiellement les individus entre 40 et 60 ans et plus rarement à l'adolescence. Elle est cinq à dix fois plus fréquente chez les femmes. Elle est la cause de près des trois quarts des hyperthyroïdies.Il existe un facteur génétique comme l'atteste une atteinte concomitante chez de vrais jumeaux ainsi que la présence d'antécédents familiaux.Comparativement au tabagisme actif persistant, la cessation de consommation de tabac diminue le risque de développer une maladie de Basedow, notamment dans sa forme oculaire, et plus particulièrement chez les femmes.Décrit à de nombreuses reprises, ce syndrome doit son nom à Carl von Basedow mais peut avoir plusieurs dénominations :goitre exophtalmique ;goitre toxique diffus ;hyperthyroïdie auto-immune ;maladie de Graves ;maladie de Graves-Basedow.La maladie est probablement de cause multifactorielle. Il existe une participation génétique et plusieurs gènes sont impliquées : CD40, CTLA4, PTPN22, FCRL3, gènes de la thyroglobuline et au récepteur à la TSH.Le stress peut avoir un rôle provocateur. Le tabagisme est un facteur de risque.La maladie est plus fréquente chez la femme, faisant suspecter une participation génétique et/ou hormonale. D'autres facteurs ont été identifiés : infection à Yersinia enterocolitica, déficit en sélénium ou en vitamine D.L'auto-immunité se développe à partir des anticorps anti-récepteurs de la TSH, dans lequel le corps fabrique des anticorps pour le récepteur de la thyroïde-stimulant hormone (TSH-R). Ces anticorps se lient aux récepteurs TSH qui se trouvent sur les cellules qui produisent des hormones thyroïdiennes, ce qui entraîne une production anormalement élevée de T3 et T4. C'est une hypersensibilité de type V.Ils comprennent l'association de signes d'hyperthyroïdie et de signes plus spécifiques de la maladie.Elle se caractérise par une asthénie, un amaigrissement contrastant avec un appétit conservé voire augmenté, une hypersudation, des attitudes d'évitement de la chaleur. Il peut exister des troubles psychologiques, une agitation, une nervosité, un tremblement, une soif permanente avec augmentation des mictions (polyurie-polydipsie)L'examen clinique montre une fréquence cardiaque accélérée (tachycardie), voire un rythme irrégulier.Les signes et symptômes sont les suivants :goitre (augmentation de volume de la thyroïde) ;exophtalmie (déplacement de l'œil hors de son orbite) ;myxœdème (infiltration cutanée) au niveau des tibias ;gynécomastie, (développement excessif des glandes mammaires chez l'homme) ;augmentation du rythme cardiaque ;augmentation de l'activité métabolique ;plus rarement, hippocratisme digital (déformation du doigt et des ongles).L'hyperthyroïdie est démontrée biologiquement par l'effondrement du taux de TSH dans le sang, couplé à un taux élevé de triiodothyronine (T3) et de thyroxine (T4). En cas de maladie de Basedow, il existe une élévation du taux d'anticorps anti-récepteur de la TSH (« TRAK »), hautement spécifique et sensible, et des TSI (Thyroid stimulating immunoglobulins).La scintigraphie thyroïdienne consiste en la visualisation du captage (fixation) par cet organe d'un composé radioactif. Elle montre typiquement une thyroïde augmentée de volume et hyperfixante. Elle permet de différencier la maladie de Basedow d'autres causes d'hyperthyroïdie, comme un nodule, dit « toxique », par exemple.La vascularisation du goitre est augmentée et peut être démontrée par un doppler de la thyroïde. L'échographie de la glande, couplée au doppler, a des résultats comparables à ceux de la scintigraphie. L'échographie permet par ailleurs de détecter les nodules et distinguer une thyroïdite d'un Basedow (baisse du débit dans le premier cas).L'imagerie peut ainsi mettre en évidence dans près de 35 % des cas de maladie de Basedow des nodules thyroïdiens associés, généralement non fonctionnels. Dans environ 1% des cas, il est retrouvé un ou des nodules fonctionnels en plus de la maladie de Basedow (syndrome de Marine-Lenhart).Insuffisance surrénalienneMyasthénieThyroïdite d'HashimotoDiabète de type 1Polyendocrinopathie auto-immuneSyndrome de MeansUne rémission spontanée se fait dans près d'un tiers des cas. Le risque de rechute à court terme après arrêt des antithyroïdiens de synthèse peut être prédit par la persistance d'un taux élevé d'anticorps anti-récepteur de la TSH. Il reste toutefois élevé, dépassant 50 %.Les trois options sont la prescription de médicaments antithyroïdiens de synthèse (méthimazole et propylthiouracile), l'ablation chirurgicale de la thyroïde et l'emploi d'iode radioactif (iode 131 : irathérapie), détruisant ainsi sélectivement la glande. Ces trois traitements ont une efficacité comparable mais le taux de rechute est plus élevé avec les médicaments (près de 40 %) qu'avec les deux autres méthodes. Le choix de l'une ou l'autre des options dépend d'un certain nombre de paramètres, dont font partie les habitudes locales (les États-Unis recourant de manière beaucoup plus fréquente que l'Europe à l'iode radioactif en première intention).Pour les antithyroïdiens de synthèse, deux techniques de prescription peuvent être utilisés : la titration (ou recherche de la dose minimale efficace) ou la prescription à doses importantes conduisant à une hypothyroïdie et à une substitution hormonale associée. dans tous les cas, un dosage des hormones thyroïdiennes doit être fait au premier mois, puis tous les trois mois si l'hyperthyroïdie est équilibrée. Le traitement est prolongé jusqu'à 18 mois, voire plus.La prise en charge de l'atteinte oculaire a fait l'objet de la publication de recommandations par l'European Group on Graves' Orbitopathy en 2008. L'exophtalmie nécessite une prévention des lésions oculaires, secondaire à une couverture insuffisante des paupières avec un risque de lésion de la cornée. Plus rarement, elle doit être traitée pour elle-même.Louis-Ferdinand Céline en 1933, met au point un produit, La Basedowine, enregistré au Laboratoire National de contrôle des médicaments sous le no 343-4 et commercialisé par les Laboratoires Gallier jusqu'en 1971.Glafira Ziegelmann, première femme interne de Montpellier et la première femme admissible à l'agrégation de médecine, consacre en 1898 sa thèse au traitement de la maladie de Basedow. Portail de la médecine"
médecine;"Le rein est un organe de l'appareil urinaire des vertébrés. Il a de multiples fonctions : hormonales, de régulation de la pression sanguine et d'élimination des toxines. Il assure ainsi, par filtration et excrétion d'urine, l'équilibre hydroélectrolytique (homéostasie) du sang et de l'organisme en général. Ses fonctions hormonales comprennent la synthèse de l'érythropoïétine, du calcitriol (forme active de la vitamine D) et de la rénine.Chez les amniotes, il est le plus souvent pair et situé dans l'abdomen, dans le rétropéritoine, suivant une symétrie plus ou moins bilatérale. Il est de taille et de conformation très variable en fonction des espèces : lisses chez les humains, lobulés chez les ruminants, diffus chez les oiseaux...Par abus de langage, le langage courant nomme souvent « reins » la zone des vertèbres lombaires (ex.: « tour de reins » pour parler de lombalgie). Les reins de certains animaux de production sont consommés par les humains, sous le nom de rognons.La fonction complexe de cet organe vital a peut-être suscité l'ambivalence de sa réputation : objet de culte (il est cité 25 fois dans la Bible où il est considéré, selon l'art divinatoire tiré de l'examen hiéroscopique des reins de l'animal sacrifié, comme le siège de la sagesse, de l'intelligence et des émotions, notamment dans le Livre des Psaumes), il a également longtemps été méprisé car jugé peu noble (assimilé à une passoire qui filtre les déchets pour constituer les urines).Chez l'être humain, les reins sont des organes aplatis, ovoïdes, dits « en haricot ». La face externe est convexe ; la face interne est concave, et accueille le hile qui se projette au niveau de la 1re vertèbre lombaire : il constitue la zone de transit des éléments vasculo-nerveux et des voies excrétrices urinaires. La surface des reins est lisse chez l'adulte, de couleur rouge-brun. En moyenne, ils ont pour hauteur 12 cm, largeur 6 cm, épaisseur 3 cm et chacun pèse environ 150 g. Ces mensurations sont très variables d'un individu à l'autre.Les reins se situent dans l'espace rétropéritonéal, où ils se projettent par leur face postérieure dans la région lombaire. Celle-ci constitue d'ailleurs la principale voie d'abord chirurgical du rein.Le rein gauche est placé entre la 11e vertèbre thoracique et la 3e lombaire. Le rein droit quant à lui se projette entre la 12e vertèbre thoracique et l'espace entre la 3e et la 4e lombaire. Ce décalage est dû à la pression du foie sus-jacent sur le rein droit. Ils s'orientent :dans le plan frontal, selon un angle d'environ 18° avec l'axe médian (orientés vers le bas et en dehors) ;dans le plan transversal, selon un angle de 40 à 60° avec l'axe sagittal (orientés vers l'avant et en dedans)Un seul rein suffit pour vivre ; 5 % des individus n'ont qu'un rein, mais dans ce cas il s'agit le plus souvent du rein droit, mieux vascularisé et grâce à la présence du quadrilatère de Rogie qui favorise la stase veineuse et a des répercussions au niveau génital gauche.Le rein est vascularisé par les artères et veines rénales et c'est par une échancrure dans la face concave que ces vaisseaux pénètrent dans le rein (hile du rein).Les artères rénales sont deux artères droite et gauche qui naissent de l’aorte abdominale au niveau de L1. L’artère rénale gauche est plus courte que la droite. Chaque artère rénale donne deux branches terminales : une branche antérieure et une branche postérieure. Les artères et les veines présentent les subdivisions suivantes, jusqu'au glomérule :  Les veines rénales croisent en avant les artères rénales et se jettent dans la veine cave inférieure au niveau de L2. La veine rénale gauche est plus longue et de gros calibre. Le parenchyme rénal est entouré d'une capsule dure, très résistante qui le protège. La partie périphérique du parenchyme est le cortex alors que la partie centrale est la médulla. Cette médulla n'est pas continue : elle est interrompue par des prolongements du cortex qui vont jusqu'au sinus rénal.Le rein est innervé par le plexus rénal qui accompagne et entoure l'artère rénale. Il est innervé par le système nerveux sympathique et parasympathique. L'innervation parasympathique est assurée par le nerf vague (X).L'innervation sympathique émerge des segments de la moelle spinale T10 à L1. Les fibres pré-synaptiques vont se réunir pour former les nerfs splanchniques, ceux-ci font synapse principalement dans le ganglion aortico-rénal. De là partent les fibres post-synaptiques qui vont innerver le rein. Accessoirement on peut retrouver une innervation du 1er nerf splanchnique lombaire et l'implication des ganglions mésentérique supérieur et rénal.Pour plus de détails sur l'innervation orthosympathique des viscères de l'abdomen, consulter l'article concernant les plexus prévertébraux.Le rein a aussi une fonction endocrine (érythropoïétine, système rénine-angiotensine-aldostérone, calcitriol). En raison de caractéristiques génétiques ou liées aux traits de vie, la capacité des reins varie significativement selon les individus et selon l'âge. Elle est médiocre chez le nouveau-né et décline chez l'adulte avec l'âge. Les capacités fonctionnelles du rein peuvent être dégradées par diverses maladies et par l'exposition à certains toxiques (fluor, plomb, cadmium, autres métaux lourds, alcool ou excès de sodium…). En cas de déficience grave, les derniers recours sont la filtration externe du sang dans un rein artificiel (dialyse), ou la greffe de rein.De l'extérieur vers l'intérieur :Elle comporte les glomérules, les tubes contournés proximaux et distaux et les tubes collecteurs.Les colonnes de Bertin, dans les espaces entre les pyramides de Malpighi.Pyramides rénales ou de Malpighi ; dont la base est sous-corticale et la pointe tournée vers l'intérieur, forment les papilles sur lesquelles viennent se ventouser les petits calices. Elles comportent les tubes droits proximaux et distaux ainsi que l'anse de Henle et les canaux de Bellini.Une pyramide et ses colonnes forment un lobe du rein.Les néphrons qui se déversent dans le même canal collecteur forment collectivement un lobule du rein.Les petits calices recueillent l'urine émise par les pyramides de Malpighi. L'union des petits calices forment les grands calices, il y a trois ou quatre grands calices par rein. Tube abouché à la pointe de la pyramide rénale, et qui en se rejoignant forment le bassinet.Tube en forme d'entonnoir qui se jette dans l'uretère. Il est également appelé pyélon.C'est l'endroit où va passer l'urine à sa sortie du néphron via le tube collecteur. Les bassinets tout comme les calices possèdent un tissu musculaire lisse qui se contracte et propulse l'urine par péristaltisme.Le rein est issu de la métamérisation (segmentation puis formation de tubules) du mésoblaste intermédiaire (tissu du disque embryonnaire) en cordon néphrogène au cours de la 3e semaine de développement. Ce cordon se divise en trois régions distinctes dans le temps et l'espace (selon un axe céphalo-caudal) qui vont évoluer successivement:- le cordon pronéphrogène (le plus céphalique) qui se métamérise en pronéphros. Ce premier rein ne fonctionne pas et dégénère à la fin de la 4e semaine de développement ;- le cordon mésonéphrogène qui se métamérise après la dégénérescence du pronéphros en mésonéphros. Ce rein fonctionne dès la fin de la 4e semaine de développement, mais il dégénérera également ;- le cordon métanéphrogène (le plus caudal) qui donne le métanéphros, rein fonctionnel, qui est le rein définitif. Il a la particularité de ne pas se métamériser.Voir aussi le paragraphe sur l'embryologie du néphron.Le néphron est l'unité structurelle et fonctionnelle de base du rein. C'est un tubule mince consistant en un amas de capillaires appelés glomérules, entourés d'un bulbe creux, la capsule glomérulaire. La capsule glomérulaire amène à un long tubule entortillé en deux sections : le tubule contourné proximal, l'anse du néphron, le tubule contourné distal, et le tubule rénal collecteur. Les tubules collecteurs se déversent dans les calices via les papilles, les calices se jettent dans le pelvis rénal (appelé également pyélon ou bassin), qui est connecté à l'uretère. Chaque rein humain compte environ un million de néphrons. Le nombre de néphrons, fixé à la naissance, est d'une grande variabilité. Il dépend de multiples facteurs dont l'âge gestationnel, le retard de croissance intra-utérin, l'état nutritionnel maternel.Le rôle essentiel et le plus connu des reins est la formation de l'urine. Ils éliminent du sang les déchets provenant de la destruction des cellules de l'organisme et de la disgestion des aliments .La formation de l'urine et le rejet de celle-ci, comprennent quelques étapes:L'artère rénale apporte le sang au rein - Les artères rénales droite et gauche nées de l'aorte apportent une grande quantité de sang aux reins, environ 1700 litres par jour,, soit un cinquième du débit cardiaque. Elles se divisent en de nombreuses branches pour aboutir à des artérioles microscopiques qui vont alimenter les néphrons;Le néphron filtre le sang et produit l'urine - Chaque rein est constitué d'un million de minuscules canaux juxtaposés appelés néphrons. Chaque néphron comprend un glomérule et un tubule. Le glomérule est un filtre très fin qui retient les globules rouges et les grosses molécules (protéines) mais laisse passer l'eau, les électrolyses (sodium, potassium, calcium...) et les petites molécules (glucose, urée, acide urique, créatinine...). Il en résulte une urine primitive qui va subir des transformations à l'intérieur du tubule. Certaines substances y sont évacuées, d'autres sont réabsorbées, aboutissant à l'urine définitive qui va s'écouler dans les tubes collecteurs;L'urine atteint le bassinet, sorte d'entonnoir - Les tubes collecteurs déversent l'urine dans 8 à 10 calices qui se vident dans le bassinet, sorte d'entonnoir dans lequel s'abouche l'uretère;L'urine est déversée dans deux conduits: les uretères - Les uretères sont des tuyaux de 2,5 mm de diamètre et de 30 cm de long qui, partant du bassinet, vont amener l'urine à la vessie;La vessie stocke puis évacue l'urine par l'urètre - La vessie est un réservoir qui peut contenir jusqu'à 800 ml d'urine. Elle se remplit progressivement et se vide, par un mécanisme déclenché volontairement, laissant échapper l'urine par l'urètre: c'est la miction.Hormis sa fonction principale de filtration et d'épuration du sang, le rein intervient à bien des niveaux, notamment dans la régulation de la pression artérielle. Par sa fonction de synthèse de substances spécifiques régulatrices, notamment :la rénine synthétisée par le rein et qui va provoquer, via l'angiotensine II (ATII), une stimulation de la sécrétion d'aldostérone, qui est une hormone qui va en cas de baisse de pression artérielle, stimuler la réabsorption de sodium ; or les mouvements d'eau suivent les mouvements de sodium, donc cela va entrainer une réabsorption accrue d'eau qui va faire augmenter la volémie au niveau plasmatique et ainsi faire augmenter la pression dans le sang. Ce n'est qu'un aspect schématique et non exhaustif de la rénine car elle a comme effet également de stimuler la sécrétion de noradrénaline, toujours via l'angiotensine II et ainsi provoquer une vasoconstriction ;En cas d'hypertension artérielle (HTA), le rein va synthétiser de la kallikréine pour donner en fin de réaction de la bradykinine (qui est une kinine) qui a des effets vasodilatateurs, donc de réduire la pression au niveau des vaisseaux.Ceci explique pourquoi l'apport excessif de sel fait augmenter la pression artérielle : les mouvements de sodium dans le rein se font également passivement, donc si on augmente notre apport en sodium (sel), cela entrainera une réabsorption accrue d'eau également provoquant une augmentation de volémie donc de pression car : PA = DC × Rp DC = FC × VES PA = pression artérielleDC = débit cardiaqueRp = résistance périphérique FC = fréquence cardiaqueVES = volume d'éjection systolique (volume éjecté par le ventricule cardiaque gauche à chaque contraction).On sait que les entrées et sorties en sodium sont équivalentes, la quantité absorbée est éliminée au début mais si l'apport en sel n'est plus ponctuel mais continu, alors une autre limite est fixée et l'élimination se fait moins bien ; ceci augmente avec l'âge.Un rein adulte reçoit normalement le quart du débit cardiaque à chaque minute. Son rein est irrigué en moyenne chaque jour par plus de 1 700 litres de sang (toutes les quatre minutes, la totalité du sang de l'organisme, soit près de 6 litres, est filtrée en traversant cet organe), soit environ 900 litres de plasma sanguin. Sur ces 900 litres de plasma, 20 % sont filtrés au niveau des glomérules rénaux pour former 180 litres d'urine primitive qui subit par les différents segments du tubule rénal des modifications, essentiellement des phénomènes de réabsorption (plus de 99 % de l'eau et des sels filtrés sont réabsorbés), aboutissant à la production d'1 à 2 litres d'urines définitives. La diurèse quotidienne normale est de 1 à 1,5 L dépendant des apports hydriques.[pas clair]Lors du sommeil, le taux d'ADH sécrété par l'hypophyse augmente, ce qui a pour effet d'augmenter la réabsorption d'eau par le rein, donc de diminuer la quantité d'urine excrétée.Le débit de filtration glomérulaire normal est de 120 à 130 mL/min (un débit anormal sert à diagnostiquer l'insuffisance rénale chronique) soit les 180 litres d'urine primitive quotidienne.L'insuffisance rénale chronique (IRC) semble en augmentation dans les pays riches, probablement secondairement à l'augmentation des cas de diabète (diabète sucré) ;l'augmentation des cas d'hypertension artérielle ;des néphropathies vasculaires liées au vieillissement de la population ;un régime trop riche en sel conduisant à une hypertension artérielle qui peut être fatale au rein ;l'alcoolisme ;l'obésité ;une exposition excessive à certains produits néphrotoxiques (toxiques rénaux) : plomb, cadmium en particulier ou médicaments tels que phénacétine ou ciclosporine… éventuellement exacerbée par des produits chélateurs très présents dans notre environnement tels que le glyphosate.Elles peuvent être dues à des anomalies génétiques ou à une malformation survenue lors du développement (polykystose rénale le plus souvent, ou reflux vésico-urétéral chez l'enfant), des infections (fréquemment à la suite d'angines, d'infection urinaire, de tuberculose dans les pays en développement) ou à des intoxications ou séquelles d'intoxications. Certains cancers du rein pourraient être précocement induits par des perturbateurs endocriniens. La défaillance du rein peut apparaître brutalement 10 à 40 ans après le début de l'affection.Malformation congénitale : l'exposition prénatale à l'alcool peut provoquer une diminution de la quantité de néphrons, des reins en fer à cheval. On classe généralement les maladies rénales en :maladies glomérulaires (glomérulonéphrites primitives dont la cause initiale n'est pas comprise, ou maladies glomérulaires connues telles que le diabète sucré ou lupus érythémateux, ou couramment dans les pays pauvres les amyloses) ;néphropathies interstitielles (infection urinaire à pyélonéphrite, intoxication par des toxiques tels que cadmium ou plomb). Chez la femme, une cystite aggravée est la cause première. Chez l'homme, un cancer ou un grossissement de la prostate freinant l'écoulement de l'urine peut faciliter une infection conduisant à une néphropathie interstitielle ;néphropathies vasculaires ; le cas le plus fréquent étant une néphroangiosclérose liée à une hypertension artérielle (près de 10 % de la population dans les pays riches).NéphrectomieSonde urinaire en double JUn rein artificiel (ou générateur de dialyse) est un dispositif médical permettant d'épurer le sang des patients dont les reins ne fonctionnent plus.La première réalisée en France eut lieu à Paris sur le jeune Marius Renard en 1952, par l'équipe du docteur Louis Michon à l'Hôpital Necker ; les suites néphrologiques ont été assurées par le professeur Jean Hamburger et Gabriel Richet, mais le jeune homme est rapidement décédé. La méthode aujourd'hui[Quand ?] utilisée est la méthode « de Kuss » (1913-2006).Le 23 décembre 1954, le chirurgien américain Joseph Murray réalise la première transplantation rénale réussie au monde, en la pratiquant sur des jumeaux monozygotes, les frères Ronald et Richard Herrick (en) au Peter Bent Brigham Hospital (en).Il se réalise environ 3 000 greffes de reins par an en France.Le bicarbonate de sodium s'avère efficace pour ralentir la progression de maladie rénale chronique - études ayant exclu les personnes souffrant d'obésité morbide associée, de troubles cognitifs, de septicémie chronique, d'insuffisance cardiaque manifeste ou d'hypertension non contrôlée,,.Le système excréteur chez les autres animaux est constitué d'organes excréteurs et des canaux excréteurs associés :organes excréteurs non spécialisés (élimination de différents types de solutés sous forme d'urine) : organes néphridiens chez les invertébrés, tubes de Malpighi chez les arthropodes, organe de Bojanus (en) des mollusques, rein des vertébrés ;organes excréteurs spécialisés (élimination d'un type de soluté), par exemple les cellules à chlorures des branchies des téléostéens, les glandes à sel des oiseaux.Reins de plusieurs animaux obtenus par la technique d'injection de vinyle et corrosion :                        Ressources relatives à la santé : FMA TA2 Uberon Xenopus Anatomy Ontology (en) Medical Subject Headings (en) NCI Thesaurus (no + nn + nb) Store medisinske leksikon (la + en) TA98 Renaloo, communauté de patients et de proches concernés par l'insuffisance rénale, la dialyse, la greffe. Portail de l’anatomie   Portail de la médecine   Portail de la physiologie"
médecine;"L'hyperthyroïdie (appelée aussi dans des cas très prononcés — graves et rares — thyréotoxicose ou thyrotoxicose) est le syndrome clinique causé par un excès de thyroxine libre circulante (FT4) ou de triïodothyronine libre (FT3), ou les deux. Chez les humains, les causes principales sont la maladie de Basedow (cause la plus fréquente : 70-80 % des cas), l'adénome toxique de la thyroïde, le goitre multinodulaire toxique, et la thyroïdite sub-aiguë.La glande thyroïde, stimulée par la TSH (thyroid-stimulating hormone), secrète deux hormones, la thyroxine (= tétraiodothyronine) ou T4 et la triiodothyronine (T3). La première est une prohormone, transformée en la seconde qui constitue la forme active.L'hyperthyroïdie consiste en l'augmentation des taux de T3 et de T4 dans le sang. Si cette hypersecrétion est secondaire à une maladie de la thyroïde (ce qui est vrai dans la quasi-totalité des cas), la TSH est effondrée (par rétrocontrôle)L'incidence annuelle est de 0,6 pour 1 000 femmes. Elle est quatre fois moindre chez les hommes. La prévalence aux États-Unis est de 1,3 %.La cause la plus fréquente chez le sujet jeune est la maladie de Basedow et chez le sujet âgé, le nodule toxique ou le goitre multinodulaire, surtout si l'apport iodé de la nourriture est pauvre. Les thyroïdites, entraînant le relargage d'hormones thyroïdiennes à la suite de la destruction cellulaire, comptent pour 10 % des hyperthyroïdies. Les autres causes sont rares.C'est la première cause d'hyperthyroïdie en termes de fréquence. Elle est plus fréquente chez la femme jeune. On retrouve de manière non constante un souffle à l'auscultation de la glande thyroïde qui est augmenté de volume, un discret gonflement des parties molles de la jambe (myxœdème prétibial) ou des globes oculaires légèrement proéminents (exophtalmie). Le diagnostic est fait en présence de TSI (Thyroid stimulating immunoglobulins) dans le sang des patients. La structure de cette TSI est proche de celle de la TSH et stimule ainsi la production d'hormones thyroïdiennes par la glande.Le nodule toxique de Plummer est évoqué devant le nodule isolé de la glande thyroïde qui peut parfois être palpé et surtout, par la fixation d'iode radioactif de ce dernier de manière exclusive à la scintigraphie thyroïdienne, le reste de la glande n'étant plus visualisé. Il devient une cause importante d'hyperthyroïdie chez la personne âgée. Son traitement demande l'éradication du nodule, que cela soit par chirurgie ou par iode radioactif.Elle peut être :infectieuse (thyroïdite de De Quervain dans un contexte grippal) ou post opératoire ;auto-immune comme lors de la thyroïdite de Hashimoto avec la présence d'anticorps anti-TPO ;survenir après un accouchement (assez fréquente puisqu'elle concerne jusqu'à 10 % des parturientes, le plus souvent très discrète et guérissant sans séquelle).Elle évolue parfois vers une hypothyroïdie (diminution des hormones thyroïdiennes) régressive.La scintigraphie montre alors l'absence totale de fixation de l'iode radioactif (scintigraphie blanche).Parmi les autres causes possible, on distingue :le goitre multinodulaire : le goitre est révélé à l'examen clinique de la glande, il peut être suffisamment important pour causer des compressions des structures adjacentes. La fonctionnalité des nodules est affirmée par la scintigraphie thyroïdienne. Le traitement est essentiellement chirurgical : l'utilisation d'iode radioactif peut faire disparaître l'hyperthyroïdie clinique mais ne parvient pas, en règle générale, à faire diminuer le goitre ;l'association d'une maladie de Basedow et de nodules fonctionnels (syndrome de Marine-Lenhart) ;le cancer de la thyroïde évolué ;l'adénome hypophysaire à TSH ;la prise d'hormone thyroïdienne en quantité trop élevée ;effet secondaire de la prise de certains médicaments, surtout du fait de leur richesse en iode dans le principe actif ou les excipients : antiseptiques contenant de l'iode (polyvidone), produits de contraste de radiologie, etc. L'amiodarone peut donner également des hypothyroïdies. L'hyperthyroïdie de l'amiodarone est plus fréquente dans les régions avec apports iodés insuffisants. Elle impose l'arrêt de ce médicament lorsque c'est possible, en sachant que sa demi-vie prolongée (plus de 100 jours) fait que l'imprégnation en médicament va persister très  longtemps.La plupart des signes restent non spécifiques ou peuvent être discrets. La sévérité des signes est corrélée avec les taux hormonaux. Ils sont toutefois plus frustes chez la personne âgée.L'hyperthyroïdie peut se manifester par tout ou partie des signes ci-dessous.Une perte de poids malgré un appétit conservé ou accru (polyphagie).Une prise de poids dans environ 10 % des cas.Une chaleur ressentie comme insupportable (thermophobie).Une polydipsie, soif excessive.Une asthénie, fatigue, à l'instar de l'hypothyroïdie, pouvant avoir comme conséquence des troubles de l'érection dans la moitié des cas, chez l'homme, réversible sous traitement.Une fréquence cardiaque élevée (tachycardie) avec des palpitations ou des extrasystoles auriculaires.Un essoufflement (dyspnée) ;Un pouls irrégulier pouvant correspondre à une fibrillation auriculaire, cette dernière pouvant être présente même en cas d'hyperthyroïdie dite sub-clinique.Des tremblements fins des extrémités, conséquence de l'excès de circulation sanguine rapide du sang (Attention, ce tremblement n'est pas d'origine neurologique !).Le tout peut se compliquer soit :d'une insuffisance cardiaque typiquement à haut débit, régressive le plus souvent après normalisation des hormones thyroïdiennes mais pouvant aboutir à des séquelles dans un tiers des cas ;de douleurs thoraciques pouvant évoquer une angine de poitrine.Diarrhée chronique.Nausées ou vomissements.Il existe une diminution de la force musculaire (myopathie endocrinienne) avec parfois diminution de la taille des muscles (atrophie musculaire).La maladie peut se présenter sous forme de dépression ou irritabilité.Dans les formes graves, l'hyperthyroïdie peut entraîner un coma, des mouvements anormaux sous forme de chorée, des troubles du comportement pouvant ressembler à une psychose.Peau luisante, chaude et humide.Démangeaison isolée.Plusieurs symptômes sont décrits :impuissance ;augmentation de la taille des seins (gynécomastie) ;infertilité ;absence totale ou partielle de menstruations.L'hyperthyroïdie, même modérée (dite sub-clinique) peut se compliquer d'une décalcification osseuse (ostéoporose secondaire).Certaines thyrotoxicoses peuvent ainsi faciliter un saturnisme inattendu, via une contamination de l'organisme par relargage du plomb antérieurement stocké dans les os. Dans ce dernier cas, l'augmentation de la plombémie est accompagnée d'une augmentation du taux sérique d'ostéocalcine qui reflète l'augmentation du remodelage osseux qui accompagne souvent l'hyperthyroïdie.Inversement ou en retour le plomb pourrait affecter la thyroïde en inhibant la captation d'iode, phénomène d'abord observé chez l'animal puis confirmé chez l'Homme dans les années 1960,, y compris dans un cas d'intoxication saturnine liée à la présence d'une balle en plomb non extraite de l'organisme.On observe une élévation de l'hormone TSH ou une chute de la thyroxine sérique et libre, en cas d'exposition chronique et plutôt quand la plombémie dépasse 60 µg/100 mL.Diminution de la concentration de cholestérol sanguin (hypocholestérolémie).Anémie (diminution de la concentration d'hémoglobine dans le sang).Neutropénie (diminution du nombre de polynucléaire neutrophiles sanguin).Selon la cause de l'hyperthyroïdie on observe un goitre, un nodule thyroïdien, une hypertrophie thyroïdienne...Le diagnostic est établi par un examen sanguin : mesure du taux de TSH dans le sang. Un taux effondré de TSH est spécifique d'une hyperthyroïdie périphérique (l'immense majorité des hyperthyroidies, secondaire à une atteinte de la thyroïde). Le diagnostic est confirmé par une mesure du taux de T3 libre et T4 libre sanguin que l'on retrouve augmenté. L'augmentation de ces deux hormones peut cependant être dissociée avec des cas rares d'hyperthyroïdie à T3, la T4 étant normale. Si la TSH est basse et la T4 et T3 sont normales sur des dosages répétées, on parle d'« hyperthyroïdie infraclinique ».Une fois le diagnostic fait, il reste à rechercher la cause. Il est indispensable de doser les anticorps spécifiques (Anticorps anti-récepteur de la TSH, anti-thyroglobuline, anti-thyropéroxydase « anti-TPO ») et de réaliser un examen d'imagerie de la thyroïde : échographie (par ultrasons) ou  scintigraphie (par injection d'un isotope radioactif qui se fixe sur la glande thyroïde et dont le rayonnement est détecté par une caméra à scintillations). Ces examens précisent l'aspect de la glande et la répartition géographique de son activité (fixation à la scintigraphie).La prise en charge de l'hyperthyroïdie a fait l'objet de la publication de recommandations par l'American Thyroid Association en 2011.Le choix du traitement dépend de la cause, de la sévérité et du terrain.Un traitement d'urgence est l'ingestion de solution saturée d'iodure de potassium (SSKI), un fort taux d'ion iodure permettant de stopper temporairement la sécrétion de thyroxine par la thyroïde.Elle consiste en l'ablation de la totalité ou d'une grande partie de la glande thyroïdienne. La chirurgie se doit de respecter les glandes parathyroïdes de petite taille et situées en arrière de la thyroïde. Elle doit également passer en dehors du nerf récurrent qui remonte en arrière de la glande. La section de ce nerf peut entraîner un changement de la voix (dysphonie) du fait qu'il innerve les cordes vocales.Dans le cas de l'hyperthyroïdie, une radiothérapie métabolique peut être prescrite. Il s'agit de l'ingestion d'iode 131 radioactif qui va se fixer sur la glande thyroïde et la détruire. Ce traitement n'est proposé qu'à certaines formes de maladie de Basedow et est naturellement inefficace en cas de non fixation de l'iode sur la glande (scintigraphie blanche). Elle expose à un risque d'hypothyroïdie (comme la chirurgie par ailleurs) qui est facilement traitée par la prise d'hormones thyroïdiennes.Il s'agit de  médicament inhibant la production d'hormones thyroïdiennes, comme le méthimazole (alias thiamazole), le carbimazole ou le propylthiouracile. Le délai d'efficacité peut être long. Deux techniques de prescription peuvent être utilisés : la titration (ou recherche de la dose minimale efficace) ou la prescription à doses importantes conduisant à une hypothyroïdie et à une substitution hormonale associée.En cas de douleurs, il est possible de donner un antalgique et un antipyrétique en cas de fièvre.Les bêta-bloquants ralentissent le cœur et diminuent les palpitations ainsi que les tremblements.Elle est définie par un taux bas de TSH et un taux normal de T4 libre et de T3 totale. Plus de la moitié des hyperthyroïdies sont sub cliniques et leur prévalence serait de 0,7 % aux États-Unis.Ce syndrome est associé avec un risque majoré d'ostéoporose chez la femme âgée mais aussi de maladies cardiovasculaires, de mortalité cardiaque et de fibrillation auriculaire.Un traitement systématique d'emblée n'est pas recommandé : une surveillance régulière du taux des hormones doit être faite et le traitement débuté à l'élévation de ces dernières.C'est l'une des maladies hormonales les plus fréquentes chez le chat, souvent provoquée par une tumeur bénigne (non cancéreuse) de la thyroïde. Cette maladie a été décrite pour la première fois dans les années 1970. Portail de la médecine"
médecine;Une injection intraveineuse (IV) est une injection d'un liquide dans une veine en général à l'aide d'une seringue et d'une aiguille. Elle est notamment utilisée lorsque l'effet de la substance administrée doit être rapide : il peut s'agir d'un médicament, d'un analgésique, d'un marqueur en imagerie médicale ou d'une drogue.  Portail de la médecine   Portail des soins infirmiers
médecine;
"escript""";
médecine;"La pharmacologie est une discipline scientifique du vivant, subdivision de la biologie, qui étudie les mécanismes d'interaction entre une substance active et l'organisme dans lequel elle évolue, de façon à pouvoir ensuite utiliser ces résultats à des fins thérapeutiques, comme l'élaboration d'un médicament (principalement) ou son amélioration.Pour ce faire, la pharmacologie intègre des concepts et données issus de la physiologie, physio-pathologie, biochimie, génétique et biologie moléculaire.Le champ de la pharmacologie peut être étendu puisqu'elle étudie également les moyens d'administration des médicaments, les interactions médicamenteuses et les effets néfastes de ces médicaments (effets latéraux, effets secondaires).Cette discipline pharmaceutique est fortement liée à la recherche fondamentale, à la recherche clinique et à la santé publique (pharmaco-épidémiologie), mais aussi à la toxicologie et la chronopharmacologie.Durant l'Antiquité, Hippocrate de Cos fut le premier médecin à rejeter les superstitions et les croyances qui attribuaient la cause d'une maladie à des forces surnaturelles ou divines. Dans son livre Sur la maladie sacrée, il évoque sa « théorie des humeurs » qui prouva qu'une maladie n'est pas une punition infligée par les dieux, mais plutôt le résultat de facteurs environnementaux, de l'alimentation et des habitudes de vie. Ce constat permit à la médecine de se dissocier de la religion et d'exister en tant que discipline à part entière, mais aussi de concevoir l'interaction de l'organisme, entité matérielle, avec son environnement qui est lui aussi matériel. Héraclide de Cumes, contemporain d’Asclépiade de Bithynie, il fut un des fondateurs de la pharmacologie et de la toxicologie. Galien, médecin grec considéré comme l'un des fondateurs de la pharmacie, reprit et précisa la théorie d'Hippocrate. Selon lui, la maladie résulte d'un déséquilibre entre les humeurs et la thérapie consiste donc à en rétablir l'équilibre, souvent à l'aide de remèdes ayant l'effet inverse aux symptômes identifiés. Toutefois, la dissection de cadavres étant interdite par le droit romain, il pratiqua la dissection sur des d'animaux ce qui l'amena à développer un grand nombre d'idées erronées sur l'anatomie humaine. Ainsi, Galien proposa la saignée comme remède à presque tous les maux. L'utilisation abusive de purgatifs, de sudorifiques et d'émétiques (vomitifs), visant à purifier le corps pour en chasser l'excès maladif de l'une des humeurs, fut l'un des premiers balbutiements de la pharmacie et de la pharmacologie occidentale.Au Moyen Âge, on ne constate pas d'avancement significatif : la théorie galénique est conservée jusqu'au XVIe siècle et on fait une distinction entre la pharmacie et la médecine. Néanmoins, les concoctions ayant des propriétés miraculeuses sur l'organisme se multipliaient. Des épiciers connaissant les propriétés médicamenteuses de certaines épices se spécialisèrent en apothicairerie. En effet, la profession d'apothicaire devint popularisée aux XIIIe et XIVe siècles. Le terme « apothicaire » désignait alors les boutiquiers qui vendaient des drogues et des médicaments pour les malades. Évidemment, la distinction entre l'apothicaire et le charlatan était presque imperceptible. Des rapports houleux s'établirent alors entre les apothicaires et les médecins du XIIe siècle. Ces derniers considéraient l'acte médical des autres comme moins noble. En 1241, l'édit de Salerne édicté par Frédéric II sépara juridiquement la médecine et l'apothicairerie, ce qui marqua l'origine officielle de la profession d'apothicaire.Au XVIe siècle, Paracelse, alchimiste et médecin parfois considéré comme le père de la toxicologie et de la pharmacologie, écrit un livre sur le corps humain qui réfute Galien. À l'époque, on se purgeait encore dans l'intention de se « nettoyer des humeurs putrides ou malsaines » qui attaquaient le corps et provoquaient un déséquilibre physiologique. Paracelse énonça la théorie selon laquelle le fonctionnement de l'organisme s'explique par un ensemble de réactions chimiques. Selon lui, les maladies sont provoquées par des désordres chimiques, provenant d'organes spécifiques, à l'intérieur du corps et elles ne peuvent donc être soignées que par des moyens chimiques. Le remède est donc formé par l'extraction d'un élément particulier pour chaque maladie, la « quintessence », puis donné au malade. À ce titre, il introduit notamment l'utilisation du mercure pour le traitement de la syphilis, ce qui provoqua un évident intérêt pour les gens de l'époque. L'une des causes de ce succès est dû au fait qu'il reconnaissait la relation dose-effet d'un médicament, l'un des principes fondamentaux de la pharmacologie. Dans ses mots, Paracelse écrit :« Toutes les choses sont poison, et rien n’est poison ; seule la dose détermine ce qui n’est pas un poison. »L'apparition des premières pharmacopées, recueils officiels de médicaments, vit le jour aux XVe et XVIe siècles. On parle entre autres du Codex Medicamentarius Parisiensis paru en 1638.La pharmacie galénique.La pharmacocinétique, qui étudie le devenir d'une molécule bioactive dans l'organisme, de son absorption à son excrétion en passant par son métabolisme. En résumé, elle étudie les effets de l’organisme sur la molécule.La pharmacodynamique, qui étudie comment une molécule produit un effet sur un organisme.La pharmacogénétique, qui étudie l'influence des gènes sur l'activité des médicaments sur l'organisme.La pharmacogénomique.La toxicologie s'intéresse spécifiquement aux molécules ayant un effet nocif sur un organisme.La pharmaco-épidémiologie.La pharmacovigilance, qui étudie les effets indésirables des médicaments.Les étudiants en pharmacologie doivent acquérir un large éventail de connaissances, notamment en physiologie, biochimie, chimie, génétique, ainsi qu'en pharmacologie moléculaire et clinique.À Sherbrooke, Québec au Canada, l'Université de Sherbrooke offre son baccalauréat en pharmacologie depuis 2001.Michael Neal, Pharmacologie Médicale , Deboeck, 2017  (ISBN 978-2-807-306110)PharmacologueRépertoire de la pharmacologieEthnopharmacologieInstitut de pharmacologie de SherbrookePharmaciePharmacothérapieToxicologieSociété Française de Pharmacologie et de ThérapeutiquePharmacomedicale.org, site officiel du collège national (France) de pharmacologie médicale (CNPM), et site d'information sur le médicament Portail de la pharmacie   Portail de la chimie   Portail de la biochimie   Portail de la médecine"
médecine;La thyroxine ou T4 est une hormone thyroïdienne agissant comme une prohormone devant être désiodée en triiodothyronine, ou T3, par la thyroxine 5'-désiodase pour être pleinement active. Elle est biosynthétisée chez les mammifères dans la thyroïde par iodation de la thyroglobuline sous l'action de l'iode introduit dans les cellules par la pendrine et oxydé en iode atomique par la thyroperoxydase, une enzyme dont l'expression est accrue par la thyréostimuline (TSH). Elle est inactivée par la thyroxine 5-désiodase, qui la convertit en 3,3',5'-triiodothyronine ou « T3 inverse », isomère inactif de la T3.Les hormones thyroïdiennes jouent un rôle important dans le métabolisme énergétique et agissent en relation avec d'autres hormones, telles que l'insuline, le glucagon, l'adrénaline ou encore l'hormone de croissance.La L-thyroxine (énantiomère S-(–), ou lévothyroxine) est synthétisée en laboratoire comme médicament contre l'hypothyroïdie ou comme traitement à vie en cas de thyroïdectomie. Elle est prise sous forme de comprimés à raison de 12,5 ?g à 200 ?g par jour, habituellement une demi-heure avant le petit déjeuner afin d'en maximiser l'absorption dans la mesure où elle est mal absorbée par l'intestin. Elle peut également être administrée par intraveineuse dans les cas d'hypothyroïdie sévère.Liste d'hormones Portail de la chimie   Portail de la biochimie   Portail de la médecine
médecine;
"es médi""";
médecine;Le corps humain est la  structure culturelle et physique d'un être humain. Le corps humain est constitué de plusieurs systèmes (nerveux, digestif, etc.), ainsi que de 206 os et 639 muscles dont 570 sont des muscles squelettiques. La science et la pratique visant à décrire l'organisation et le fonctionnement du corps humain est l'anatomie humaine, qui est une spécialité de la médecine. La médecine vise plus généralement à préserver la santé, c'est-à-dire le fonctionnement normal du corps humain.L'anatomie est la discipline consacrée à la description du corps humain et de ses différentes parties. L'anatomie macroscopique s'intéresse à sa surface et aux éléments visibles à l’œil nu, tandis que l'anatomie microscopique, ou histologie, suppose l'utilisation d'outils grossissants comme le microscope. L'anatomopathologie se focalise sur la morphologie du corps malade. La physiologie se consacre elle aux fonctions des différents éléments corporels. La position anatomique de référence utilisée afin d'uniformiser les descriptions est celle d'un corps en position debout, la tête droit, le regard fixe, les bras sur les côtés avec les paumes tournées vers l'avant et les pieds joints. La terminologie comprend des termes directionnels, servant à localiser les parties du corps les unes entre les autres, ainsi que des termes régionaux, utiles pour désigner ces parties. Le corps peut être divisé selon un plan médian, coronal ou transverse.Le corps se compose de différents niveaux de stratification et de complexité structurales. Le niveau fondamental est chimique, donc atomique et moléculaire. La plupart des biomolécules se composent de quatre éléments (le carbone, l’hydrogène, l'oxygène et l'azote). 4 % du poids corporel correspond à d'autres éléments, dont le potassium, le sodium, le calcium et le phosphore. Le corps d'un homme adulte contient 60% d'eau, ce qui représente environ 42 litres. Les cellules sont les plus petites unités indépendantes de matière vivante. La plupart sont spécialisés afin de répondre à une fonction précise. Il en existe environ 30 000 milliards de 300 types.Les tissus forment le niveau d'organisation intermédiaire entre la cellule et l'organe. Ils se composent d'un ensemble de cellules semblables et de même origine, sont regroupés en amas, réseau ou faisceau (fibre), se régénèrent et assurent la même fonction. Les tissus conjonctifs, épithéliaux, musculaires et nerveux sont les quatre types de tissus principaux, avec chacun leurs subdivisions. Constitués de plusieurs tissus, les organes, au nombre d'environ 80, réalisent une fonction physiologique unique et sont associés à un ou plusieurs systèmes. Par différence avec les appareils, qui remplissent un ensemble de fonctions complémentaires, les systèmes sont répartis dans l'ensemble de l'organisme. Ils sont constitués d'organes et de tissus assurant une même fonction vitale, comme la digestion ou la respiration.Anatomie humaineDroit à l'intégrité physiquePlanète corps, de Pierre-François Gaudry, Collection : Odyssée des sciences, 88 minutes, Mona Lisa, Universcience, l’Inserm, Smith&Nasht(en) Cet article est partiellement ou en totalité issu de l’article de Wikipédia en anglais intitulé « Composition of the human body » (voir la liste des auteurs).Janet S. Ross et Kathleen Jean Wilson Wallace (trad. de l'anglais, ill. Richard Tibbitts, édité par Anne Waugh & Allison Grant, coordination scientifique de l’édition française par Julie Cosserat), Anatomie et physiologie normales et pathologiques, Issy-les-Moulineaux, Elsevier Masson, 2019, 13e éd., XV-575 p. (ISBN 978-2-294-76408-0, BNF 45760534).Elaine N. Marieb et Katja Hoehn (trad. de l'anglais par Sophie Dubé), Anatomie et physiologie humaines, Montreuil, Pearsons, 2019 (ISBN 978-2-7661-0122-1, BNF 45798350), p. XXVI-1310.(en) Yoram Yom-Tov et Eli Geffen, « Geographic variation in body size : the effects of ambient temperature and precipitation », Oecologia, vol. 148, no 2,? juin 2006, p. 213-218 (PMID 16525785, DOI 10.1007/s00442-006-0364-9, lire en ligne, consulté le 30 mars 2021).Exploration d'un corps humain virtuel (ikonet)Portail de l'explorateur du corps humain de Google (Body Browse) Portail de l’anatomie   Portail de la biologie
médecine;"L'embryologie est une discipline scientifique qui englobe la description morphologique des transformations de l'œuf fécondé, le zygote, en organisme (embryologie morphologique) et l'étude de leur déterminisme (embryologie causale). « L'embryologie causale » est plus couramment désignée, depuis les années 1990 et l'avènement de la génétique moléculaire, par le terme de « biologie du développement » qui inclut aussi l'étude du développement post-embryonnaire.On peut diviser la formation embryonnaire (ou embryogénèse) en trois stades :le clivage, ou segmentation ;la gastrulation ;l'organogénèse.L'organogénèse, commune à tous les organismes, comporte plusieurs sous-étapes comme la neurulation ou la métamérisation, qui se déroulent parfois en même temps.La tératologie est l'étude des anomalies de l'embryon et du fœtus.Le modèle actuel du développement de l'embryon repose sur l'épigenèse qui stipule que celui-ci se développe de manière de plus en plus complexe en rapport direct avec son environnement. Au XVIIIe siècle, elle a été opposée à la théorie de la préformation qui voit l'embryon comme un être vivant « miniature » où tous les organes sont déjà présents.Il faut attendre le début du XIXe siècle et Karl Ernst von Baer (1792-1876) pour entrer véritablement dans l’embryologie moderne. Alors professeur à l’université de Königsberg, von Baer élabore entre 1819 et 1834 les travaux d'embryologie qui établiront sa notoriété à partir de l'étude de mammifères.Sa découverte la plus célèbre est celle de l'ovule, jusque-là confondu avec le follicule ovarien, chez les mammifères en 1827, venant après la mise en évidence, en 1824, du rôle fécondant des spermatozoïdes par Prevost (1790-1850) et Dumas (1800-1884).Le passage d'une embryologie descriptive à une véritable embryologie comparée est effectuée en 1828 par Von Baer. L’embryon est formé de trois feuillets à partir desquels se forment ultérieurement les organes ; les premiers stades sont semblables chez tous les animaux.Les feuillets embryonnaires sont l'objet de nouvelles découvertes que Von Baer décrit dans son ouvrage Über Entwickelungsgeschichte der Thiere (1828-1837).En 1866, Ernst Haeckel introduit la théorie de la récapitulation, aujourd'hui réfutée en grande partie, qui fait le parallèle entre la croissance d'un embryon et l'évolution de son espèce : selon lui, l'ontogenèse suit la phylogenèse. Par exemple, à un certain stade de leur développement, les organes qui formeront les nageoires de l'embryon d'un mammifère marin comme le dauphin présentent une conformation qui rappelle les pattes des animaux terrestres. Cette observation coïncide avec l'existence d'un ancêtre terrestre.Lorsque le spermatozoïde féconde l'ovocyte, le développement de l'œuf en animal commence.Ce développement se déroule en cinq grandes étapes : la segmentation, la gastrulation, l'organogenèse (comportant les phénomènes de délimitation, neurulation et métamérisation) et l'histogenèse.La segmentation : première phase du développement embryonnaire caractérisée par une suite de divisions rapides et rapprochées, à interphases très courtes. Ces divisions, qui sont en fait des mitoses singulières, fragmentent l'œuf en un ensemble de cellules nommées blastocytes - ou blastomères, dont la taille diminue à mesure des clivages. La segmentation n'engendre pas l'accroissement du diamètre de l'œuf, qui garde ainsi le même volume, et s'achève au stade morula.La gastrulation : mise en place dans l'embryon du disque embryologique tridermique équivalant aux trois feuillets fondamentaux (ectoderme, mésoderme et endoderme qui chez l'homme apparaissent dès la 3e semaine de développement), et de la chorde (structure médiale de l'embryon, inductrice primaire de beaucoup d'éléments, comme le tube neural, ou le gril costal. Elle est transitoire et donnera un vestige chez l'adulte : le nucleus pulposus). Ils dérivent des deux feuillets éphémères (épiblaste et hypoblaste) dont vont dériver les futurs organes.La neurulation : mise en place des ébauches neurales. L'embryon qui en est le siège est la neurula. C'est une phase caractéristique des chordés qui correspond à l'enroulement et à la soudure des bords externes de la gouttière neurale. La neurulation primaire aboutit à la formation du tube neural et de l'ampoule neurale, premières ébauches du système nerveux central. La neurulation secondaire aboutit à la formation du reste du système nerveux (filum terminale, etc.)La métamérisation : fragmentation ou bourgeonnement du mésoblaste. Dans la jeune neurula, les lames mésodermiques gauche et droite s'étendent de manière continue d'un bout à l'autre de l'embryon. Ces lames vont se découper en une série de segments successifs.ex. : chez les vertébrés la métamérisation engendre les somites et les vertèbres.L'histogenèse : formation des tissus par différenciation des cellules embryonnaires.Il y a donc formation de tissus par transformation des ébauches embryonnaires.Les grands types de tissus sont : les épithéliums (avec tissus glandulaires), les tissus musculaires, les tissus nerveux, les tissus conjonctifs (dont les tissus osseux, le tissu sanguin, les tissus conjonctifs communs, etc.).L’œuf est libéré par la femelle et fécondé au stade de deuxième division de méiose.La fécondation est le stade qui précède l’ontogenèse qui elle-même est le phénomène étudié en biologie du développement. La fécondation est par définition la fusion des gamètes qui sont chez cette espèce libérés dans le milieu aquatique. Suit l'amphimixie qui est l'accolement des pronoyaux sans fusion sous le contrôle des centrioles distal et proximal (originaires du spermatozoïde). Dès lors commence la segmentation de l'animal.Le clivage est qualifiée de pseudoholoblastique car la division de l'embryon se fait de façon proportionnée durant les premières phases de segmentation de l'animal puis de façon inégale à partir du stade 32 blastocystes. Dès lors on peut différencier les différents blastocystes par leurs tailles; ils sont nommés macromères au pôle végétatif et micromères, de plus petite taille, au pôle animal. L'embryon passe ensuite par le stade de morula (qui est ici une cœloblastula).Cette phase de l'ontogenèse aboutit à la formation des trois feuillets embryonnaires (chez les organismes triploblastiques comme l'échinoderme). Elle aboutit à la formation d'une gastrula qui possède par définition un archentéron permettant le commencement de la phase suivante de développement embryonnaire.L'organogenèse est le processus de formation des organes à partir des trois feuillets embryonnaires fondamentaux (ectoderme, endoderme et mésoderme). Elle se fait chez l’échinoderme par invagination de la plaque primaire afin de former l'archentéron. Étant un être deutérostomienCe premier orifice formé deviendra l'anus chez l'adulte. Lors de cette embolie est formé le squelette de l'animal (formé de spicules) à partir de mésenchyme primaire, ainsi que du mésenchyme secondaire. Puis le fond de l'archentéron va fusionner avec la plaque stomodéale du côté du pôle buccal de l'animal ce qui formera plus tard la bouche. Ainsi le tube digestif (qui s'étend de la bouche à l'anus) est formé. L'Organisme obtenu est une larve appelée pluteus qui subira la métamorphose, processus de transformation post-embryonnaire, pour former un organisme épithélioneurien comme l'étoile de mer ou encore l'oursin.Le terme d'Addendum Mammifères (ou Annexes embryonnaires) désigne les organes utilisés lors de l'embryogenèse et ne perdurant pas à l'âge adulte. Parmi ces annexes embryonnaires on peut citer le placenta (qui reste sans doute la plus communément connue) ou encore le diverticule allantoïdien. Elles ont un rôle prépondérant dans la distribution ou le stockage de réserves nutritives ou encore dans le recyclage des urines comme chez Gallus domesticus.On divise les placentas en deux ordres qui sont les décidués et les indécidués. Ces deux ordres sont divisés en deux types de placentation. On peut les lister ainsi : Indécidués  Epithélio-chorial Dans ce cas le placenta est qualifié de placenta diffus. Comme son nom l'indique les barrières histologiques entre mère et embryon sont au nombre de trois :Épithélium ;Conjonctif ;Endothélium.Les pachydermes, cétacés, équidés et suidés possèdent ce type de placenta. Conjonctivo-chorial Dans ce cas-ci le placenta traverse l'épithélium et se retrouve au sein du conjonctif maternel. On qualifie la placentation de cotylédonaire du fait de sa morphologie. Les ruminants possèdent ce type de placentation. Décidués  Endothélio-chorial Le capillaire embryonnaire est en contact avec l'endothélium maternel (qui constitue les vaisseaux sanguins situés à la périphérie du placenta). On qualifie ce placenta de zonaire. Il est présent chez tous les carnivores. Hémo-chorial Les contacts entre l'embryon et la mère est fait par des lacs sanguins qui permettent l'alimentation de l'embryon. Le placenta est alors qualifié de discoïdal. Les insectivores, chiroptères, rongeurs et primates (et donc humains) possèdent ce type de placentation.Raphaël Franquinet et Jean Foucrier, Embryologie descriptive, 2e édition.VitellusEmbryogenèse humaineCours d'embryologie en ligne à l'usage des étudiants et étudiantes en médecine (développé par les universités de Fribourg, Lausanne et Berne - Suisse)Site sur l'embryologie humaine (université Paris 5)Court-métrage avec prises de vue accélérées de la division et de la différenciation cellulaire chez un amphibien (Jan Van Ikjen) Portail de la biologie"
médecine;"La maladie est une altération des fonctions ou de la santé d'un organisme vivant.On parle aussi bien de la maladie, se référant à l'ensemble des altérations de santé, que d'une maladie, qui désigne alors une entité particulière caractérisée par des causes, des symptômes, une évolution et des possibilités thérapeutiques propres.Un ou une malade est une personne souffrant d'une maladie, qu'elle soit déterminée ou non. Lorsqu'elle fait l’objet d'une prise en charge médicale, on parle alors de patient(e).La santé et la maladie sont liées aux processus biologiques et aux interactions avec le milieu social et environnemental. Généralement, la maladie se définit comme une entité opposée à la santé, dont l'effet négatif est dû à une altération ou à une désharmonisation d'un système à un niveau quelconque (moléculaire, corporel, mental, émotionnel…) de l'état physiologique ou morphologique considérés comme normal, équilibré ou harmonieux. On peut parler de mise en défaut de l'homéostasie.Les termes maladie et malade proviennent du latin male habitus signifiant qui est en mauvais état.Ce terme est unique en français, italien et espagnol, alors que l'anglais et l'allemand disposent de doublons tels que illness et disease, Erkrankung et Krankheit qui expriment des distinctions particulières de sens.Il n'existe pas de terme commun désignant la maladie dans le groupe des langues indo-européennes, on note l'existence de nombreux synonymes dont la signification étymologique appartient à quatre champs sémantiques :la faiblesse, la perte de force, l'incapacité à travailler ;la difformité et la laideur ;la gêne, le trouble, le malaise ;la souffrance et la douleur.Le concept initial d'état morbide ou de maladie s'appuie sur un critère objectif (incapacité de fournir un travail pour soi ou pour la société), et un critère subjectif (de la gêne ou indisposition à la douleur aiguë).Ce concept n'est pas socialement neutre, car il implique un jugement moral et esthétique : il y a la maladie, mais aussi le mal, le mauvais, et le laid. Disease, illness, sickness En français, les termes « maladie » et « malade » sont utilisés de façon indistincte pour signifier « avoir une maladie » (reconnue par un médecin), « être malade » (se sentir mal), « être un malade » (être reconnu comme tel par l'entourage ou la société).L'anglais utilise trois termes, plus ou moins interchangeables, mais en principe utilisés le plus souvent dans un contexte spécifique. Disease se rapporte à une perturbation biomédicale, objectivée par une maladie reconnue par un médecin, dans le cadre d'une pathologie référencée (nosologie).Illness se rapporte à l'expérience vécue, personnelle et intime, de la maladie : « je me sens, ou je suis, malade ».Sickness se rapporte à la perception de la maladie dans le cadre de l'entourage non-médical (social ou culturel) : «je suis un malade» (reconnu comme tel). Limites et extensions Il a été montré en 1989 que plus les étudiants en médecine étaient avancés dans leur cursus plus ils avaient tendance à qualifier de maladie les conditions parmi 38 qui leur étaient présentées, sans que cette qualification n'ait de lien fort avec les propriétés de gravité, curabilité, responsabilité du patient ou causalité externe. L'idée de maladie, plutôt qu'être parfaitement définie, évolue donc chez l'étudiant en fonction de son avancement dans le cursus.Classifier un certain état comme une maladie est aussi un fait social d'évaluation. Ainsi, certains états ne sont reconnus comme des maladies que dans certaines cultures, ou à certaines époques, et pas dans d'autres. On parle alors de syndromes culturels. Parfois la catégorisation d'un état comme une maladie est controversé au sein d'une même société. L'hyperactivité et l'obésité sont par exemple des états de plus en plus considérés comme des maladies par l'opinion publique dans les pays occidentaux mais n'étaient pas ainsi considérés il y a encore quelques décennies, et ne le sont toujours pas dans certains pays.La maladie est à différencier des blessures, handicaps, syndromes et affections.Une blessure est une lésion, physique ou psychique.Un handicap est une déficience qui peut aussi bien être due à une maladie qu'à une blessure.Un syndrome est un ensemble de signes ou de symptômes qui apparaissent simultanément. Ainsi l'usage médical distingue une maladie, qui a une cause spécifique connue, d'un syndrome, qui ne se préoccupe pas des causes.Une affection désigne une altération de fonctions qui est rattachée à un organe spécifique et qui ne prend en compte ni les causes, ni les symptômes, ni le traitement. Tout comme les syndromes, elle est parfois distinguée d'une maladie.Par extension, on peut associer la maladie à des entités non biologiques pour signifier qu'elles sont altérées ou que leur fonctionnement n'est plus considéré comme bon. Il est ainsi habituel d'entendre les termes « société malade » ou « entreprise malade » par exemple.La maladie humaine est le noyau fondateur de la médecine, une grande partie de la connaissance médicale étant orientée vers la maladie et ses solutions.La science vétérinaire concerne les maladies qui affectent les animaux, dont les zoonoses.La phytopathologie est la science qui concerne les maladies qui affectent les plantes et autres sujets botaniques.La pathologie est la branche de la médecine traitant des causes et des symptômes des maladies dans leur ensemble. Le terme est souvent utilisé fautivement pour désigner la maladie elle-même, ou ses manifestations, y compris par des médecins.La pathogénie est l'étude des mécanismes responsables du déclenchement et du développement d'une maladie.L'ontologie est l'étude de la genèse des entités médicales telles que les maladies, les signes cliniques et les syndromes.L'étiologie est l'étude spécifique des causes et des facteurs d'une maladie.La séméiologie, ou sémiologie médicale, est la branche de la médecine qui traite des signes cliniques et des symptômes des maladies et de la façon de les présenter.Le diagnostic est la réflexion menant à l'identification de la nature d'une maladie à partir des symptômes relevés par les observations.Le pronostic est la prévision de la progression de la maladie et des chances éventuelles de guérison.La prophylaxie désigne le processus ou l'ensemble de mesures visant à prévenir la propagation ou l'apparition d'une maladie.La thérapeutique est la section de la médecine s'occupant de l'étude des traitements.La nosologie est la branche de la médecine qui étudie les critères de classification systématique des maladies.Les facteurs des maladies sont le domaine d'étude de l'étiologie et physiologie. Catégorisation des facteurs  Facteurs intrinsèques et extrinsèques Il existe de nombreux facteurs différents pouvant entraîner l'apparition d'une maladie.Ces facteurs peuvent être aussi bien intrinsèques qu'extrinsèques à l'organisme concerné par la maladie.La présence d'un facteur intrinsèque n'exclut pas celle d'un facteur extrinsèque, et inversement. Ainsi, de nombreuses maladies résultent d'une combinaison de facteurs intrinsèques et extrinsèques. Liste Les facteurs peuvent être répartis dans les catégories suivantes :Facteurs chimiquesFacteurs économiquesFacteurs sociauxFacteurs psychologiquesFacteurs biologiquesFacteurs environnementauxLes facteurs environnementaux incluent les produits chimiques toxiques (par exemple les acétaldéhydes dans la fumée de cigarette et les dioxines relâchées lors de l'utilisation d'Agent orange) et les agents infectieux (par exemple les virus de la varicelle ou de la polio).Certains facteurs peuvent faire partie de plus d'une catégorie. Facteurs biochimiques C'est le cas des causes biochimiques de maladies qui peuvent être considérées comme un spectre où à l'une des extrémités la maladie est causée exclusivement par des facteurs génétiques (par exemple les répétitions CAG dans le gène HD (ou gène huntingtine ou encore gène IT15) qui cause la maladie de Huntington) et à l'autre causée entièrement par des facteurs environnementaux.Entre ces deux extrêmes, gènes et facteurs environnementaux interagissent pour causer la maladie comme c'est le cas pour la maladie inflammatoire appelée maladie de Crohn où les gènes NOD2/CARD15 et la flore intestinale jouent chacun un rôle. L'absence de facteur génétique ou environnemental dans ce cas a pour résultat l'absence de manifestation de la maladie. Étude des facteurs environnementaux Les postulats de Koch peuvent être utilisés pour déterminer si une maladie est causée par un agent infectieux. L'émergence de nouvelles maladies infectieuses est liée aux activités humaines perturbant l'équilibre des écosystèmes.Par exemple, l'Institut de recherche pour le développement indique que « le déboisement des forêts primaires reste l'une des causes principales de l'apparition de nouveaux agents infectieux et de leur circulation épidémique dans les populations humaines ». En effet, les forêts jouent un rôle essentiel pour la biodiversité terrestre, élément stabilisateur des agents pathogènes. Étude des facteurs génétiques Pour déterminer si une maladie est causée par un facteur génétique, les chercheurs étudient la présence de la maladie dans l'arbre généalogique familial.Cela fournit des informations qualitatives à propos de la maladie, c'est-à-dire comment elle est héritée.Un exemple classique de cette méthode de recherche est l'héritage de l'hémophilie dans la famille royale britannique. Plus récemment cette méthode a été utilisée pour identifier le gène Apoliprotéine E (ApoE) comme un gène susceptible d'être lié à la maladie d'Alzheimer, bien que certaines formes de ce gène (ApoE2) en soient moins susceptibles.Pour déterminer jusqu'à quel point une maladie est causée par des facteurs génétiques, c'est-à-dire pour obtenir des informations quantitatives, des études sur des jumeaux sont effectuées. Les jumeaux monozygotes sont génétiquement identiques alors que les jumeaux dizygotes sont seulement génétiquement similaires. De plus des jumeaux, qu'ils soient monozygotes ou dizygotes, partagent souvent un environnement similaire. Ainsi en comparant l'incidence de la maladie (nommée taux de concordance) chez des jumeaux monozygotes avec l'incidence de la maladie chez des jumeaux dizygotes, la contribution de chaque gène à la maladie peut être déterminée.Les gènes suspects peuvent être identifiés grâce à plusieurs méthodes. L'une d'entre elles est la recherche de mutation d'un organisme modèle (par exemple les organismes Mus musculus, Drosophila melanogaster, Caenhorhabditis elegans, Brachydanio rerio et Xenopus tropicalis) qui possèdent un phénotype similaire à la maladie étudiée. Une autre approche est la recherche de ségrégation de gènes ou l'utilisation de marqueurs génétiques (par exemple les polymorphismes nucléotidiques et marqueurs de séquences exprimées). Maladies complexes Les maladies complexes sont dues à l'interaction entre un profil génétique particulier et un environnement particulier. Quelques exemples :ObésitéDiabète sucréHypertension artérielleAthérome et athéroscléroseAsthmeMaladies dysimmunitaires ou auto-immunesMaladies neurodégénératives (maladie d'Alzheimer, maladie de Parkinson, sclérose latérale amyotrophique)Un symptôme se distingue d'un signe. Le symptôme est l'expression subjective des effets ressentis par le malade alors que les signes en sont l'expression objective déduite par le médecin, ou plus généralement de la personne réalisant un diagnostic.Certaines maladies sont contagieuses ou infectieuses, comme c'est le cas par exemple de l'influenza (ou grippe). Les maladies infectieuses peuvent être transmises par un grand nombre de mécanismes, incluant l'expulsion de particules dans l'air lors d'un éternuement ou d'une toux, les fomites (objets contaminés par des pathogènes), les morsures et piqûres d'insectes ou autres animaux vecteurs porteurs de la maladie, et l'absorption d'eau ou de nourriture contaminée.Il existe également des infections ou maladies sexuellement transmissibles (MST ou IST). Ce sont des maladies infectieuses qui se transmettent au cours de rapports sexuels, ou de contacts sanguins. Au début du XXIe siècle, un des principaux représentants de ces maladies est le SIDA. Un représentant plus ancien est la syphilis.Certaines maladies sont dites non transmissibles, elle ne se transmettent pas directement. Il y a par exemple les maladies liées à l'environnement.Une des principales mesures permettant d'éviter la propagation d'une maladie parmi une population ou seulement le développement d'une maladie chez un individu est la prévention.Elle peut se décomposer en trois parties :La prévention, qui a pour but de réduire la probabilité d'apparition de la maladie (ex : vaccination).La prévision, qui doit prévoir des mesures pour combattre le sinistre si celui-ci survient.La protection, qui a pour but de limiter l’étendue et la gravité de la maladie ou de l'épidémie, lorsqu'elle est déjà présente (ex : amputation, quarantaine).En médecine, on parle plus particulièrement de prophylaxie, le processus qui vise à prévenir les épidémies et la propagation d'une maladie. La prophylaxie est, plutôt qu'un traitement médical, une promotion de la prise de conscience générale des bonnes conduites à adopter face à la maladie.Les principales mesures de prévention de la maladie sont l'amélioration de l'hygiène et la vaccination.La thérapeutique est la section de la médecine s'occupant de l'étude des traitements.Les traitements consistent souvent, suivant le niveau évolutif de la société humaine concernée, en la prise de médicaments à base de molécules de synthèse ou bien de remèdes produits à partir de l'environnement naturel. Il existe toutefois de nombreuses autres thérapies, telles la radiothérapie ou la kinésithérapie, n'ayant pas recours à l'ingestion et à l'injection de substances extérieures.L'identification d'un état comme une maladie, plutôt que comme une simple variation de la structure humaine ou de fonctions, peut avoir des implications sociales et économiques significatives et peut changer le statut social de l'être concerné.La maladie peut parfois entraîner l'exclusion sociale des personnes touchées. Un exemple est l'exclusion des lépreux, courante en Europe depuis le Moyen Âge, et leur regroupement dans des établissements appelés léproseries dans le but de limiter la propagation de la maladie par contagion.La peur de la maladie a été et est encore un phénomène social très répandu, bien que toutes les maladies, notamment les plus bénignes, n'aient pas ce genre de répercussions sociales.Dans certains pays, les maladies infectieuses les plus dangereuses, du point de vue du risque épidémique, sont des maladies à déclaration obligatoire, c'est-à-dire qu'elles doivent être déclarées aux autorités dès qu'elles sont diagnostiquées par le médecin ou le vétérinaire.Certains dispositifs ont également été mis en place dans de nombreux pays pour éviter ou compenser les effets néfastes de la maladie. C'est dans cette optique qu'est apparue l'assurance maladie, qui est un dispositif chargé d'apporter une compensation financière à un individu subissant ou ayant subi une maladie.Une dérive consiste à élargir les descriptions nosographiques des maladies tout en y sensibilisant le grand public afin d'augmenter le marché de certains fournisseurs de traitements contre ces mêmes maladie. Cette pratique est appelée le disease mongering.L'étude des différentes classifications de la maladie concerne la branche de la médecine appelée « nosologie ».Il existe différentes tentatives de classification des maladies. Toutefois, du fait de la constante évolution de la médecine, elles ne sont pas figées. Les maladies peuvent être catégorisées en fonction de leurs causes et facteurs, de leurs symptômes ou des fonctions et organes touchés. On parle alors respectivement de classification étiologique, nosographique et fonctionnelle. Classification étiologique Maladies par agents physiques (froid, chaleur, etc.)Maladies toxiques (produits chimiques, poisons, etc.)Maladies parasitaires (champignons, vers, etc.)Maladies infectieuses (virus, bactéries, etc.)Maladies traumatiques (chocs psychologiques ou physiques, brûlures, etc.)Maladies dyscrasiques (troubles des métabolismes, troubles génétiques, etc.)Maladies psychiques (facteurs psychiques, bien que ces maladies puissent aussi avoir les mêmes facteurs que les maladies précédentes) Classification fonctionnelle Dysfonctionnements moléculaires (au niveau de la molécule)Dysfonctionnements cellulaires (au niveau de la cellule)Dysfonctionnements organiques (au niveau de l'organe)Dysfonctionnements corporel (au niveau d'un système d'organes)Dysfonctionnements mental (au niveau psychologique)On peut également séparer les maladies en :maladies aiguës et maladies chroniques, suivant qu'elles aient un développement rapide ou étalé ;en maladies bénignes et maladies malignes, suivant leur gravité ;en maladies locales et maladies générales, suivant l'étendue de la zone touchée ;en maladies évitables et inévitables.L'Organisation mondiale de la santé publie et est responsable de l'évolution de la Classification internationale des maladies, poursuite des travaux de Jacques Bertillon. Cette classification permet le codage des maladies, des traumatismes et de l'ensemble des motifs de recours aux services de santé grâce aux codes CIM (ou ICD en anglais). Elle permet également l'analyse systématique et l'interprétation des causes de morbidité et de mortalité dans le monde entier. Son but est notamment l'organisation et le financement des services de santé.De nombreuses cultures ont tenté de donner une signification et une origine à la maladie.Dans la mythologie grecque, l'apparition de la maladie est expliquée par l'ouverture de la boîte de Pandore. Zeus, qui voulait se venger des hommes à la suite du vol du feu par Prométhée, ordonne la création de Pandore, femme qu'il envoie auprès du frère de ce dernier. Pandore apporte avec elle une boîte qu'il lui est interdit d'ouvrir. La curiosité la pousse à le faire tout de même et c'est ainsi qu'elle libère la maladie et les autres maux de l'humanité que la boîte contenait.Au Proche-Orient ancien, l'origine naturelle de la maladie est concevable, mais elle se rajoute à une origine surnaturelle, par exemple la colère des dieux, la première étant la conséquence de la seconde.À partir de 1860, la pensée tendait vers l'idée que les homosexuel(le)s souffraient plutôt d'une maladie. Cette position de la communauté médicale et scientifique a perduré jusque vers les années soixante, où plusieurs voix se sont manifestées pour remettre en question cette vision de l'homosexualité. En 1974, l'Association américaine de psychiatrie a éliminé l'homosexualité de sa liste des maladies mentales, le Manuel diagnostique et statistique des troubles mentaux. Le 17 mai 1990, c'était au tour de l'Organisation mondiale de la santé de prendre la même position et de retirer l'homosexualité de sa Classification internationale des maladies dans sa dixième version (CIM-10).La maladie a inspiré de nombreuses créations artistiques.Le personnage du malade tient par exemple la place centrale dans Le Malade imaginaire, la dernière comédie écrite par Molière.Mais aussiHôpital général, de Slaughter : Tous les aspects de la médecine y sont représentés : l’organisation hospitalière…L’Hôpital, d’Alfonse BoudardLe Pavillon des cancéreux de Soljenitsyne : le cancerLa Mort du pantin, de Pierre MoustiersUn cri, de Michèle LoriotLa Peste, de Camus : les épidémiesLe Hussard sur le toit de Giono : Le choléraOpération épidémie, de SlaughterLa Montagne magique de Thomas Mann : La tuberculoseUn grand patron, de Pierre Véry : la formation médicaleLe Destin de Robert Shanon, de CroninLe Médecin de campagne de Balzac : l’exercice de la médecineLe Docteur Pascal de ZolaVoyage au bout de la nuit de L.-F. CélineLes Hommes en blanc, d’André SoubiranLe Livre de San Michele, d’Axel MuntheSept morts sur ordonnance, de Georges Conchon : les problèmes morauxKnock, de Jules Romain : l’arrivisme, les tentations et dérives de la médecineLes Grandes Familles, de Maurice Druon : la tentation des honneurs avec le professeur LartoyLes Thibault, de Roger Martin du GardOscar et la Dame rose, d’Eric-Emmanuel Schmitt.Philippe Adam et Claudine Herzlich, Sociologie de la maladie et de la médecine (1994), Paris, Armand Colin, 2014.Marc Augé et Claudine Herzlich (dir.), Le Sens du mal. Anthropologie, histoire, sociologie de la maladie, Bruxelles, Éditions des archives contemporaines, coll. « Ordres sociaux », 1984.Philippe Batifoulier, Capital-Santé. Quand le patient devient client, Paris, La Découverte, 2014.Frédéric Bauduer, Histoires des maladies et de la médecine, Paris, Ellipses, coll. « Sciences humaines en médecine », 2017.Henri Bergeron et Patrick Castel, Sociologie politique de la santé, Paris, PUF, coll. « Quadrige », 2014.Max Blecher, Aventures dans l’irréalité immédiate, suivi de Cœurs cicatrisés, trad. d’Elena Guritanu, Paris, L’Ogre, 2015.Max Blecher, La Tanière éclairée, trad. par Georgeta Horodinca et Hélène Fleury, Paris, Maurice Nadeau, 1989.Norbert Elias, La Solitude des mourants (1982), trad. Sybille Muller, suivi de Vieillir et mourir : quelques problèmes sociologiques, trad. Claire Nancy, Paris, Christian Bourgois éditeur, 1987.Dr. Christophe Fauré, Vivre ensemble la maladie d'un proche, Albin Michel, 2002.Jean-Claude Fondras, Santé des philosophes, philosophes de la santé, Nantes, éditions nouvelles Cécile Defaut, 2014.Elodie Giroux et Maël Lemoine (dir.), Philosophie de la médecine. Santé, maladie, pathologie, Paris, Vrin, 2012.Xavier Guchet, La Médecine personnalisée. Un essai philosophique, Paris, Les Belles Lettres, 2016.Hervé Guibert, À l'ami qui ne m'a pas sauvé la vie, Paris, Gallimard, coll. « Folio », 1990.Céline Lefève, Lazare Benaroyo et Frédéric Worms (dir.), Les Classiques du soin, Paris, PUF, 2015.Thomas Mann, La Montagne magique (1924), trad. Maurice Betz, Paris, Le Livre de poche, 1991.Claire Marin, Violences de la maladie, violence de la vie, Paris, Armand Colin, 2008.Ruwen Ogien, Mes Mille et Une Nuits : la maladie comme drame et comme comédie, Paris, Albin Michel, 2017.Roselyne Rey, Histoire de la douleur, Paris, La Découverte, coll. « Histoire des sciences », 1993 ; nouvelle édition avec des postfaces de Jean Cambier et Jean-Louis Fischer, Paris, La Découverte, 2011.Susan Sontag, La Maladie comme métaphore (1977, 1978), trad. Marie-France de Paloméra, suivi de Le Sida et ses métaphores, trad. Bruce Matthieussent, Paris, Christian Bourgois éditeur, 1993.Virginia Woolf, De la maladie (1930), trad. Élise Argaud, Paris, Payot & Rivages, 2007.Ressources relatives à la santé : Orphanet (en) Classification internationale des soins primaires (en) Diseases Ontology (en) Medical Subject Headings (en) NCI Thesaurus (no + nn + nb) Store medisinske leksikon (cs + sk) WikiSkripta (fr) Site officiel de l'Organisation mondiale de la santé(fr) Site officiel du Ministère de la Santé, de la Jeunesse et des Sports français.(fr) (en) Site officiel de Santé Canada.(fr) Site officiel du Ministère de la Santé et des Services sociaux du Québec(fr) Site officiel du Service public fédéral de la santé belge.(fr) Site officiel du Ministère de la Santé du Congo-Kinshasa(fr) Site officiel du Ministère de la Santé du Luxembourg(fr) (en) (de) (it) Site officiel de l'Office fédéral de la santé publique suisse.(fr) Classification étionosographique des pathologies sur psychobiologie.ouvaton.org Portail de la médecine"
médecine;"La médecine (du latin : medicina, qui signifie « art de guérir, remède, potion »), au sens de pratique (art), est la science témoignant de l'organisation du corps (anatomie), son fonctionnement normal (physiologie), et cherchant à préserver la santé (physique comme mentale) par la prévention (prophylaxie) et le traitement (thérapie) des maladies. La médecine humaine est complémentaire et en synergie avec la médecine vétérinaire.La médecine contemporaine utilise l'examen clinique, les soins de santé, la recherche et les technologies biomédicales pour diagnostiquer et traiter les blessures et les maladies, habituellement à travers la prescription de médicaments, la chirurgie ou d'autres formes de thérapies.Il n'existe pas suffisamment de données fiables pour déterminer le début de l'usage des plantes à des fins médicinales (phytothérapie). Les données médicales contenues dans le Papyrus Edwin Smith peuvent être datées du XXXe siècle av. J.-C.. Les premiers exemples connus d’interventions chirurgicales ont été réalisés en Égypte aux alentours du XXVIIIe siècle av. J.-C. (voir chirurgie). Imhotep sous la troisième dynastie est parfois considéré comme le fondateur de la médecine en Égypte antique et comme l'auteur originel du papyrus d’Edwin Smith qui énumère des médicaments, des maladies et des observations anatomiques. Le papyrus gynécologique Kahun traite des maladies des femmes et des problèmes de conception. Nous sont parvenues trente-quatre observations détaillées avec le diagnostic et le traitement, certains d'entre eux étant fragmentaires. Datant de 1800 av. J.-C., il s’agit du plus ancien texte médical, toutes catégories confondues. On sait que des établissements médicaux, désignés par l’expression Maisons de vie ont été fondés dans l’Égypte antique dès la première dynastie.Les plus anciens textes babyloniens sur la médecine remontent à l’époque de l’ancien empire babylonien dans la première moitié du IIe millénaire av. J.-C. Cependant, le texte babylonien le plus complet dans le domaine de la médecine est le Manuel de diagnostic écrit par Esagil-kin-apli le médecin de Borsippa, sous le règne du roi babylonien Adad-ALPA-iddina (1069-1046 av. J.-C.).Hippocrate, est considéré comme le père fondateur de la médecine moderne et rationnelle,, et ses disciples ont été les premiers à décrire de nombreuses maladies. On lui attribue la première description des doigts en baguette de tambour, un signe important pour le diagnostic de la bronchopathie chronique obstructive, du cancer du poumon et des cardiopathies cyanogènes congénitales. Pour cette raison, le symptôme des doigts en baguette de tambour est parfois appelé hippocratisme digital . Hippocrate a également été le premier médecin à décrire la face hippocratique. Shakespeare fait une allusion célèbre à cette description dans sa relation de la mort de Falstaff dans Henry V, acte II, scène III,. Le Corpus hippocratique popularise la théorie des humeurs. La médecine rationnelle grecque et latine coexiste cependant pendant toute l'Antiquité avec les cultes des Dieux guérisseurs.Agnodice (Hagnodice) ou Hagnodikè (en grec ancien : ????????) fut, selon une légende grecque rapportée par Hygin (Caius Julius Hyginus) dans la 274e de ses Fabulae, l'une des premières femmes médecin et gynécologue. Issue de la haute société athénienne, elle se déguisa en homme pour suivre les cours de médecine du célèbre médecin Hérophile. Vers 350 av. J.-C., elle passa l'examen et devient gynécologue, mais sans révéler qu'elle était une femme.La médecine pratiquée et enseignée en occident a ses racines dans les connaissances acquises et protocolées de l'Antiquité au Ier millénaire av. J.-C. de l'Orient à l'Empire romain.Elles proviennent de la Torah, étonnement rationnelle en la matière, car tenant compte des conditions climatiques. En effet, les cinq livres de Moïse qui la constituent, contiennent diverses « lois » ayant des conséquences directes sur la santé à travers différents rituels, tels que l'isolement des personnes infectées (Lévitique 13:45-46), le lavage des mains après avoir manipulé un cadavre (Livre des Nombres 19:11-19) et l’enfouissement des excréments à l’extérieur du campement (Deutéronome 23:12-13).La traduction dans les années 830-870 de 129 œuvres du médecin grec Galien (1er siècle av J.C.) en arabe par Hunayn ibn Ishaq et ses élèves sert de modèle à la médecine des civilisations islamiques et se propage rapidement à travers l’Empire arabe, reprenant en particulier, l'insistance de Galien sur une approche rationnelle et systématique de la médecine. Qusta ibn Luqa joua aussi un rôle important dans la traduction et la transmission des textes grecs. Les médecins musulmans ont mis en place certains des premiers hôpitaux, institution qui importée en Europe à la suite des croisades.En Europe occidentale, l'effondrement de l'autorité de l’empire romain a conduit à l’interruption de toute pratique médicale organisée. La médecine était exercée localement, alors que le rôle de la médecine traditionnelle augmentait, avec ce qui restait des connaissances médicales de l'antiquité. Les connaissances médicales ont été préservées et mises en pratique dans de nombreuses institutions monastiques qui s’étaient souvent adjoint un hôpital et disposaient de carrés d'herbes médicinales. Une médecine professionnelle organisée est réapparue, avec la fondation de l’école de médecine de Salerne en Italie au XIe siècle qui, en coopération avec le monastère du Mont Cassin, a traduit de nombreux ouvrages byzantins et arabes.À partir du XIe siècle, l'Église veut dissocier la vocation de moine de la profession de médecin. La volonté d'encadrer le savoir aboutit à la formation d'universités aux mains des ecclésiastiques. Les médecins de l'université de médecine de Montpellier, dépositaires des doctrines des médecins juifs et arabes, privilégient les plantes, ceux de l'Ancienne université de Paris privilégient la purge et la saignée.Au XIXe siècle, Karl August Wunderlich publie Das Verhalten der Eigenwärme in Krankheiten, qui établit que la fièvre est seulement un symptôme et met fin au credo d'une maladie infectieuse jusqu'alors nommée « fièvre intermittente ». En 1881 Theodor Billroth réalise la première gastrectomie, il révolutionne la chirurgie du pharynx et de l'estomac. En utilisant l'analyse statistique, le médecin Pierre-Charles Alexandre Louis (1787-1872) montre que l'utilisation des saignées chez les malades atteints de pneumonie n'est pas bénéfique mais néfaste. Ceci esquisse la notion d'étude randomisée en double aveugle.Madeleine Brès (1842-1921) est la première femme de nationalité française à accéder aux études de médecine en 1868, mais sans avoir le droit d'accéder aux concours. Elle obtient son doctorat en médecine, en 1875 et devient gynécologue et pédiatre. Elle démontre dans sa thèse que le lait du nourrisson se modifie au cours de l'allaitement et crée une des premières crèches parisiennes. Elizabeth Garrett Anderson, britannique la devance de cinq ans en France dans l'obtention de son doctorat.En 1854, Florence Nightingale est la première à utiliser les statistiques pour réorganiser les soins aux blessés de la guerre de Crimée et faire baisser la mortalité des soldats,,.Le 25 novembre 1901, Aloïs Alzheimer décrit le tableau clinique de la maladie qui porte son nom, dont il n'existe toujours aucun traitement connu à ce jour. Les traitements médicaux font des progrès spectaculaires avec l'invention de nouvelles classes de médicaments. Felix Hoffmann dépose le brevet de l'aspirine le 6 mars 1899. En 1909, le Nobel de médecine Paul Ehrlich invente la première chimiothérapie en créant un traitement à base d'arsenic contre la syphilis. En 1921 Frederick Banting de l'université de Toronto isole l'insuline et invente un traitement du diabète sucré. Le premier antibiotique date de 1928 avec la découverte de la pénicilline par Alexander Fleming.Selon la psychanalyste argentine Raquel Capurro, la médecine a été le premier domaine influencé par le positivisme d'Auguste Comte, à partir du milieu du XIXe siècle, à travers des personnalités telles que le docteur Robinet parmi d'autres.La délimitation de ce qui est médecine et de ce qui ne l'est pas est source de débat.La plus grande partie de cet article traite de la médecine telle qu'elle s'est développée à partir de l'époque moderne, et pratiquée à partir du XIXe siècle. Les innovations majeures apportées par la médecine occidentale à partir du XIXe siècle (anesthésie et asepsie puis vaccination et antibiotiques au XXe siècle), ses succès, ainsi que sa diffusion à travers le monde par le biais notamment de la colonisation par l'Occident vont inciter à poser, dès la fin du XIXe siècle, la médecine scientifique occidentale comme modèle de médecine faisant autorité, lequel s'est diffusé au niveau mondial à travers son industrialisation au XXe siècle.Certains chercheurs réhabilitent de même certains aspects de la médecine médiévale occidentale. Ainsi l'historien de la médecine Roger Dachez qui met en valeur l'aspect préventif et la vision globale qu'avait de la médecine le Moyen Âge.De même, toujours à la fin du XXe siècle, notamment sous l'effet de la mondialisation, les médecines traditionnelles ou non occidentales ont vu leur place reconnue au sein de la médecine mondiale : en 2002, l'organisation mondiale de la santé a ainsi mis en place sa première stratégie globale en matière de médecine traditionnelle.On identifie ainsi, à côté de la médecine occidentale, d'autres types de médecines, dites « alternatives » incluant : médecine chinoise, médecine tibétaine traditionnelle, médecine ayurvédique, médecine traditionnelle, et médecine non conventionnelle.En Occident, l'usage de médecines alternatives et complémentaires est constaté dans certaines conditions où les traitements de biomédecine semblent inefficaces, notamment dans le cas de maladies chroniques.Les étapes de l'acte médical sont formées de :l'étiologie qui désigne l'étude des causes de la maladie ;la pathogénie ou pathogenèse qui désigne l'étude du mécanisme causal ;la physiopathologie qui désigne l'étude des modifications des grandes fonctions au cours des maladies ;la sémiologie qui désigne l'étude de l'ensemble des signes apparents. Elle est apparentée à ce qui est nommée la clinique, opposée à la para-clinique qui sont les résultats des examens complémentaires. Face à la complexité croissante des techniques d'imagerie, il s'est développé une sémiologie des examens complémentaires ;le diagnostic qui désigne l'identification de la maladie ;le diagnostic différentiel qui désigne la description des maladies comportant des signes proches et qui peuvent être confondues ;la thérapeutique qui désigne le traitement de la maladie ;le pronostic qui désigne l'anticipation de l'évolution de celle-ci ;la psychologie qui désigne la partie de la philosophie qui traite de l’âme, de ses facultés et de ses opérations. La psychologie du patient est un élément important de la réussite du processus médical. Comme le dit dès 1963 l'historien de la médecine Jean Starobinski, « une médecine vraiment complète ne se borne pas à cet aspect technique ; s'il accomplit pleinement son métier, le médecin établit avec son patient une relation qui satisfera les besoins affectifs de ce dernier. L'acte médical comporte donc un double aspect : d'une part les problèmes du corps et de la maladie font l'objet d'une connaissance qui n'est pas différente de celle que nous prenons du reste de la nature - et l'organisme du patient est alors considéré comme une « chose » vivante capable de réagir conformément à des lois générales ; d'autre part, le rapport thérapeutique s'établit entre deux personnes, dans le contexte d'une histoire personnelle - et la médecine devient alors cette fois un art du dialogue, où le patient s'offre comme un interlocuteur et comme une conscience alarmée ». Georges Canguilhem écrivait lui que « l’acte médicochirurgical n’est pas qu’un acte scientifique, car l’homme malade n’est pas seulement un problème physiologique à résoudre, il est surtout une détresse à secourir ». Une décision médicale doit tenir compte à la fois des données de la science, mais également des préférences des patients et de l’expérience du praticienEn travaillant ensemble comme une équipe interdisciplinaire, de nombreux professionnels de la santé hautement qualifiés sont impliqués dans la prestation des soins de santé modernes. Voici quelques exemples : les infirmiers, les techniciens médicaux d'urgence et les ambulanciers, les scientifiques de laboratoire, pharmaciens, podologues, physiothérapeutes, inhalothérapeutes, psychologues, orthophonistes, ergothérapeutes, radiologues, des diététiciens, des bioingénieurs, des chirurgiens et des vétérinaires.Un patient admis à l'hôpital est habituellement sous les soins d'une équipe spécifique en fonction de leur problème de présentation principale, par exemple, l'équipe de cardiologie, qui peut ensuite interagir avec d'autres spécialités, par exemple, la chirurgie, la radiologie, pour aider à diagnostiquer ou traiter le problème principal ou des complications ultérieures. Les médecins ont de nombreuses spécialisations et sous-spécialisations dans certaines branches de la médecine, qui sont énumérés ci-dessous. Il existe des variations d'un pays à l'autre en ce qui concerne les spécialités et les sous-spécialités.Les principales branches de la médecine sont :les sciences fondamentales ;les spécialités médicales ;les domaines interdisciplinaires, comme les humanités médicales.L'anatomie : étude de la structure physique des organismes. Contrairement à l'anatomie macroscopique ou brute, la cytologie et l'histologie sont concernés par des structures microscopiques.La biochimie : étude de la chimie qui se déroule dans les organismes vivants, en particulier la structure et la fonction de leurs composants chimiques.La biologie moléculaire : étude des mécanismes moléculaires des processus de réplication, de transcription et de traduction du matériel génétique.La biomécanique : étude de la structure et des mouvements des systèmes biologiques au moyen de la mécanique.La biophysique : science interdisciplinaire qui utilise les méthodes de la physique et de la chimie physique pour étudier les systèmes biologiques.La biostatistique : application des statistiques à des champs biologiques dans le sens le plus large. Une connaissance de la biostatistique est essentiel dans la planification, l'évaluation et l'interprétation de la recherche médicale. Il est également fondamental de l'épidémiologie et de la médecine fondée sur des preuves (EBM).La cytologie : étude des cellules.L'embryologie : étude du développement précoce des organismes.L'épidémiologie : étude de la démographie des processus de la maladie, et inclut, mais sans s'y limiter, l'étude des épidémies.La génétique : étude des gènes, et leur rôle dans l'héritage biologique.L'histologie : étude des structures des tissus biologiques par microscopie optique, la microscopie électronique et l'immunohistochimie.L'immunologie : étude du système immunitaire, qui comprend le système immunitaire inné et adaptatif.La microbiologie : étude des micro-organismes, y compris les protozoaires, les bactéries, les champignons, les virus et les prions.La neuroscience : étude du système nerveux.La nutrition (mise au point théorique) et la diététique (orientation pratique) : étude de la relation entre la nourriture et des boissons à la santé et à la maladie, en particulier dans la détermination d'une alimentation optimale. thérapie nutritionnelle médicale se fait par des diététistes et est prescrit pour le diabète, les maladies cardiovasculaires, le poids et les troubles alimentaires, les allergies, la malnutrition et les maladies néoplasiques.La pathologie en tant que science : étude des maladies, de leurs causes, progressions et traitements.La pharmacologie : étude des médicaments et de leurs actions.La physiologie : étude du fonctionnement normal de l'organisme et les mécanismes de régulation sous-jacents. La physiologie peut être subdivisée (physiologie cardiaque, endocrinienne…).La physique médicale : étude des applications des principes de physique en médecine.La toxicologie : étude des effets nocifs des médicaments et des poisons. Par pratique l'anatomopathologie : étude microscopique des tissus malades ;l'anesthésie-réanimation : l'anesthésie qui est la médecine péri-opératoire, la réanimation qui est la prise en charge des malades présentant au moins deux défaillances d'organe ou une nécessitant une technique de suppléance ;la biologie médicale ;la chirurgie : thérapeutique médicale qui comporte une intervention mécanique au sein même des tissus ;l'éducation de la santé ;la médecine esthétique : type de soins visant à améliorer l'aspect plastique du patient ;la médecine générale (médecine de famille) ;la médecine du travail : médecine préventive consistant à éviter toute altération de la santé des travailleurs du fait de leur travail, notamment en surveillant les conditions d'hygiène du travail, les risques de contagion et l'état de santé des travailleurs ;la médecine d'urgence : médecine hospitalière (service des urgences) et extrahospitalière (Samu), traitement des urgences vitales ;la nutrition : prise en charge du métabolisme et de l'alimentation ;la pharmacie : dispensation des médicaments et prise en charge pharmaco-thérapeutique ;la radiologie, spécialité de l'imagerie médicale. Par type de patient L'andrologie : médecine de l'homme, prise en charge des maladies spécifiques du sexe masculin ;la gynécologie : spécialité médicochirurgicale, dont l'activité variée inclut notamment la médecine de la femme, le suivi gynéco-obstétrical et les cancers des organes génitaux féminins ainsi que des seins ;l'obstétrique : médecine de la femme enceinte. À noter la pratique médicale à part entière des sages-femmes, qui se consacrent à la surveillance de la grossesse normale ;la médecine fœtale : médecine du fœtus grâce à l'apparition de méthodes d'explorations de la vie intra-utérine (échographie, Doppler, amniocentèse) ;la médecine légale : recherche des causes de la mort sur un cadavre (nécropsie) et rédaction d'un rapport pour la Justice ;la pédiatrie : médecine des enfants, domaine très large et englobant généralement la génétique clinique ;la néonatologie : médecine et réanimation des nouveau-nés et des prématurés ;la gériatrie : médecine des personnes âgées ;la médecine des gens de mer : médecine des marins et travailleurs de la mer.la médecine vétérinaire : médecine des animaux. Par organe L'angiologie : médecine des vaisseaux ;la cardiologie : médecine des maladies du cœur et du système vasculaire ;la dermatologie : médecine des maladies de la peau ;l'endocrinologie : médecine des maladies des glandes, des anomalies hormonales, des troubles de la nutrition et des métabolismes ;l'hématologie : médecine des maladies du sang ;l'hépato-gastro-entérologie : aussi appelée gastroentérologie, médecine des maladies de l'appareil digestif dans son ensemble, incluant celles du tube digestif et celles du foie, du pancréas, ainsi que de la paroi abdominale. La gastroentérologie comprend également les activités d'endoscopie digestives, soit haute (endoscopie œsogastroduodénale), soit basse (iléocoloscopie) ;l'immunologie : médecine des maladies ou des troubles du système immunitaire ;la néphrologie : médecine des maladies des reins ;la neurologie : médecine des maladies du système nerveux ;l'odontologie : soins des dents ;l'ophtalmologie : médecine des maladies des yeux, de l'orbite et des paupières ;l'orthopédie : discipline chirurgicale traitant les affections de l'appareil locomoteur ;l'oto-rhino-laryngologie (ORL) : médecine des maladies des oreilles, du nez et de la gorge ;la pneumologie : médecine des maladies de la plèvre, des bronches et des poumons ;la proctologie : médecine des maladies du rectum et de l'anus ;la rhumatologie : discipline médicale traitant les affections de l'appareil locomoteur ;la stomatologie : médecine des maladies de la bouche ;l'urologie : médecine de l'appareil urinaire. Par affection L'addictologie : médecine des dépendances, regroupant l'alcoolisme, le tabagisme et la toxicomanie (branche de la psychiatrie selon certains) ;l'alcoologie : médecine des troubles liés à l'alcool ;l'allergologie : médecine des allergies ;la cancérologie ou oncologie : médecine des cancers (comprenant la chimiothérapie des tumeurs) associée avec la radiothérapie : traitement des tumeurs par radiations ionisantes ;la diabétologie : médecine des diabètes ;l'infectiologie : médecine des maladies infectieuses ;la psychiatrie : médecine des troubles comportementaux, psychiques et des maladies mentales ;la toxicologie : traitement des empoisonnements et intoxications ;la traumatologie : traitement des patients ayant subi de graves blessures, généralement accidentelles ;la vénérologie : médecine faisant l'étude des maladies transmises par l'acte sexuel. Types de chirurgie Chirurgie cardiaqueChirurgie digestiveChirurgie de la face et du cou (cervico-faciale)Chirurgie généraleChirurgie pédiatriqueChirurgie orthopédiqueChirurgie dentaireChirurgie plastique, reconstructrice et esthétiqueChirurgie thoraciqueChirurgie urologique (Urologie)Chirurgie vasculaireChirurgie viscéraleNeurochirurgieTechniques chirurgicales Divers Anatomie et cytologie pathologiques (voir anatomopathologie)Anesthésie-réanimationBiologie médicaleGénétiqueGynécologie obstétriqueInformatique Médicale et Technologies de l'InformationMédecine généraleMédecine interneMédecine hyperbareMédecine nucléaireMédecine nutritionnelle (voir nutrition)Pathologie (pays anglophones)PédopsychiatrieMédecine physique et de réadaptationSanté publiqueLes académies de médecineles Centers for Disease Control and Prevention, soit « centres de contrôle et de prévention des maladies »les hôpitauxles organismes de recherche médicaleles organismes publicsles Conseils de l'Ordre de médecinsl'Agence européenne des médicamentsUne profession de la santé est une profession dans laquelle une personne exerce ses compétences ou son jugement ou fournit un service lié au maintien ou l'amélioration de la santé des individus, ou au traitement ou soins des individus blessés, malades, souffrant d'un handicap ou d'une infirmité. Des exemples de profession peuvent notamment inclure : médecin, pharmacien, chirurgien-dentiste, sage-femme, masseur-kinésithérapeute, physiothérapeute, ergothérapeute, psychomotricien, infirmier, podologue, aide-soignant, ambulancier, et attaché de recherche clinique.Chaque profession possède son propre cursus de formation. En plus des études permettant d'exercer la profession de médecin dont l'organisation varie selon les pays, on trouve donc notamment les études en soins infirmiers, et les études de pharmacie.L'étudiant en médecine s'appelle carabin.Les apports de la médecine, particulièrement de la médecine occidentale depuis le XIXe siècle, se mesure notamment par l'allongement de la durée de la vie, l'espérance de vie en bonne santé, la réduction de la mortalité infantile, et l'éradication ou la capacité technique d'éradication de très anciennes épidémies (tuberculose, peste, lèpre, etc.). Ces progrès se poursuivent comme avec les succès de nouvelles thérapies (ou actes chirurgicaux) sur des pathologies considérées encore incurables il y a une quinzaine d'années (comme certains cancers et maladies auto-immunes).La médecine n'est pas une science exacte, et l'acte médical peut parfois affecter la personne humaine de manière négative, par exemple via :des « effets secondaires » ou indésirables de médicaments ou traitements, qui devront pour certains (Distilbène par exemple) être supportés par plusieurs générations. La recherche de ces effets se fait par pharmacovigilance ;l'antibiorésistance est due à la sélection de souches bactériennes résistantes à divers antibiotiques à cause d'un usage non raisonné de ces derniers ;les maladies nosocomiales peuvent apparaître en hôpital à cause de la concentration de malades. La forte pression exercée par les traitements ainsi que par les désinfectants et antiseptiques sur ce « pot pourri » de germes amène à long terme à l’émergence d'agents infectieux résistants qui pourront infecter facilement les malades déjà affaiblis ;les résultats de maladresses, d'erreurs médicales, de défauts d'organisation, de prises excessives de médicaments ou de traitements inadaptés. Un trouble ou une maladie est dite iatrogène lorsqu'elle est provoquée par un acte médical ou par les médicaments, même en l’absence d’erreur du médecin, du soignant, du pharmacien ou tout autre personne intervenant dans le soin. En France, 4 % des hospitalisations sont consécutives à des soins, et 40 % de ces cas seraient évitables. Ces problèmes comprennent une partie des maladies nosocomiales dont les plus fréquentes sont les infections nosocomiales.De nombreux progrès sont annoncés ou espérés dans les années à venir, en matière de santé-environnement, d'épidémiologie, d'allongement de la durée de vie, si ce n'est de la durée de vie en bonne santé. La médecine prédictive, le clonage, les cellules-souches posent des questions nouvelles en termes de bioéthique.Des défauts d'anticipation font que, par exemple en France, en 2025, alors que la population aura augmenté (et la population âgée plus encore), le nombre de médecins aura diminué de 10 % et la densité médicale de 15 %, à la suite du non-remplacement des médecins baby-boomers induit par les quotas d’accès aux études de médecine dans les années 1970 à 1990. La médecine libérale devrait perdre 17 % de ses effectifs, et le secteur salarié 8 %, sauf en milieu hospitalier où le ministère envisage une hausse de 4 % ; 13 % des généralistes auront disparu, contre 7 % pour les spécialistes (ophtalmologistes, oto-rhino-laryngologistes et psychiatres surtout). La faible « densité médicale » augmentera aussi le coût des soins, l’impact des déplacements en termes de pollution (et secondairement de santé) et pourrait diminuer l'efficience médicale (une moindre densité médicale augmente la mortalité), d'autant plus que les patients sont plus pauvres.Cet article est partiellement ou en totalité issu de l'article intitulé « Histoire de la médecine » (voir la liste des auteurs).(en) Charles Singer et E. Ashworth Underwood, A Short History of Medicine, New York et Oxford, Oxford University Press, 1962(en) Roberto Margotta, The Story of Medicine, New York, Golden Press, 1968.(en) Roberta Bivins, Alternative Medicine? : A History, Oxford University Press, 5 octobre 2007, 264 p. (ISBN 978-0-19-156881-7, lire en ligne)(en) Robert A. Schwartz, Gregory M Richards et Supriya Goyal, « Clubbing of the Nails », Medscape Reference,? 28 février 2012 (lire en ligne, consulté le 11 juin 2012).Stanis Perez, Histoire des médecins. Artisans et artistes de la santé de l'Antiquité à nos jours, Perrin, 2015, 470 pages.Ressource relative à la littérature : (en) The Encyclopedia of Science Fiction Ressource relative à la santé : (en) Medical Subject Headings Haute Autorité de Santé : recommandations, conférences de consensus, etc. (France)Code de la santé publique (France)Service Public Fédéral (SPF) Santé publique, Sécurité de la Chaîne alimentaire et Environnement (Belgique)CISMeF : annuaire de sites médicaux Internet francophoneBase de données de publications médicales(en) Medline, base de données de publications médicales Portail de la médecine"
médecine;"La physiologie (du grec ?????, phusis, la nature, et ?????, logos, l'étude, la science) étudie le rôle, le fonctionnement et l'organisation mécanique, physique et biochimique des organismes vivants et de leurs composants (organes, tissus, cellules et organites cellulaires). La physiologie étudie également les interactions entre un organisme vivant et son environnement. Dans l'ensemble des disciplines biologiques, en définissant schématiquement des niveaux d'organisation, la physiologie est une discipline voisine de l'histologie, de la morphologie et de l'anatomie.La physiologie regroupe des processus qu'elle étudie en grandes fonctions qui sont :les fonctions de nutrition ;la fonction de reproduction ;les fonctions de relation : la locomotion et les fonctions sensorielles (voir les articles détaillés dans la liste ci-dessous).Le terme physiologie a aussi été utilisé au XIXe siècle par les écrivains réalistes pour qualifier de petites études de mœurs de personnage typiques comme les concierges, les curés de campagne, le bagnard ou la femme de trente ans dont certains sont regroupés dans l’ouvrage Les Français peints par eux-mêmes. Balzac a publié Physiologie du mariage en 1829.L’étude de la physiologie humaine remonte à au moins 420 av. J.-C. avec Hippocrate. La pensée critique d'Aristote et son accent sur la relation entre la structure et la fonction a marqué le début de la physiologie dans la Grèce antique, tandis que Claude Galien est le premier à réaliser des expériences pour étudier le fonctionnement de l'organisme, faisant de lui le fondateur de la physiologie expérimentale.Au XVIIe siècle naît la « première révolution biologique » : le cabinet d'études du physiologiste s'équipe de nombreux instruments de mesure (balance thermomètre, baromètre) qui permettent de mesurer les paramètres biologiques des animaux sacrifiés mais les résultats de ces études ne sont pas mis à profit par les médecins qui appliquent toujours le Primo saignare, deinde purgare, postea clysterium donare (« d'abord saigner, ensuite purger, postérieurement seringuer »). Lui succède au XIXe siècle une seconde révolution, la médecine expérimentale dont les bases ont été formulées et théorisées par le physiologiste français Pierre Rayer puis par son élève Claude Bernard.La physiologie comporte plusieurs subdivisions regroupées en divers articles :L'électrophysiologie est la partie de la physiologie qui mesure les courants électriques des cellules. Les phénomènes électriques sont nombreux et variés dans l'organisme, en particulier dans les tissus excitables (muscle, système nerveux central), le cœur, le rein ainsi que certaines glandes.Le système nerveux autonome est un système en réseau formé des organes des sens, des nerfs, du cerveau, de la moelle épinière, etc. Avec le système endocrinien (qui est l'ensemble des glandes sécrétant des hormones), il assure l'homéostasie de l'organisme en agissant par des impulsions électriques exerçant une action sur les muscles ou les organes. Neurophysiologie La neurophysiologie, physiologie du cerveau et des cellules nerveuses (neurone et cellule gliale), est la partie de la physiologie qui traite du système nerveux pouvant être séparé en deux parties :système nerveux central ;système nerveux périphérique. Physiologie sensorielle PerceptionGoûtOdoratOuïeAudition humaineOreilleVueŒilSomesthésieLe système reproducteur chez les humains est l'ensemble des organes qui concourent à la reproduction d'un organisme. Le développement du système reproducteur et son bon fonctionnement dépendent de glandes sécrétant des hormones endocrines.Appareil reproducteurReproduction (biologie)Physiologie de la reproductionMenstruationLe système circulatoire, dont l'organe moteur est le cœur, transporte les matières chimiques, les gaz respiratoires et la chaleur dont l'organisme a besoin. Il sert donc au maintien de l’homéostasie. Il est composé de deux sous-systèmes :l'appareil cardiovasculaire :cœursangcirculation sanguinele système lymphatique :lympheLe système circulatoire est essentiel au fonctionnement des autres systèmes, respiratoire, nutritif, immunitaire, endocrinien et thermorégulateur.Pour un organisme animal, le système respiratoire permet l’approvisionnement des cellules en oxygène et le rejet du CO2. Le système respiratoire assure ces échanges de gaz vitaux au niveau des poumons ; tandis que le système circulatoire les transporte des cellules aux poumons.PoumonBroncheLobe pulmonaireRespirationRespiration humaineVentilation pulmonaireRéflexe (réaction motrice)Activités posturalesMouvement volontaireMuscleSqueletteLe système digestif a pour fonction de transformer les aliments en des formes physiques et chimiques capables d'être absorbées et transportées dans le système circulatoire (sang et lymphe) pour répondre aux besoins en glucides, lipides, protéines, vitamines, sels minéraux et eau des cellules d'un organisme.NutritionDigestionRéserves énergétiquesExcrétionLa thermorégulation permet à un organisme de conserver une température constante. Elle est le résultat de productions et de déperditions de chaleur. On distingue les organismes homéothermes des poïkilothermes. Les poïkilothermes sont les animaux dont la température interne varie en fonction de la température externe.La thermorégulation comprend deux phénomènes :thermolyse (biologie) (perte de chaleur),thermogenèse (production de chaleur).La physiologie végétale, ou phytobiologie, est la science qui étudie le fonctionnement des organes et des tissus végétaux et cherche à préciser la nature des mécanismes grâce auxquels les organes remplissent leurs fonctions. Elle cherche en somme à percer les secrets de la vie chez les plantes.Les domaines d'étude de la physiologie végétale sont très diversifiés et concernent notamment :La nutrition, en particulier l'absorption des éléments minéraux et les fonctions de synthèse :Nutrition carbonée ;Nutrition azotée ;Nutrition minérale ;Photosynthèse.La respiration et les échanges gazeux chez les plantes.La transpiration est affectée par la chaleur et par une circulation d'air sec et chaud, donc perte de H2O chez les plantes.Les relations des végétaux avec leur environnement.La croissance et le développement.La reproduction, végétative ou sexuée.Cette discipline s'intéresse aux mécanismes de fonctionnement des diverses fonctions vitales des organismes vivants du règne animal, ainsi qu'à ses liens avec les structures organiques présentes à différents niveaux d'organisation : organes, tissus, cellules, molécules.La physiologie animale tente de brosser un panorama des adaptations des animaux à leur environnement, dans leur diversité.Sylvie Meyer, Catherine Reeb et Robin Bosdeveix, Botanique, biologie et physiologie végétales, Paris, éditions Maloine, collection « Sciences fondamentales », 2004  (ISBN 2-224-02767-2). XII, 461, L pages.Jack Baillet et Erik Nortier, préface de Roger Guillemin, Précis de Physiologie Humaine, 1998.A. Calas, J.-F. Perrin, C. Plas et P. Vanneste, Précis de physiologie, éditions Doin, 1997.Knut Schmidt-Nielsen, Physiologie animale ; adaptation et milieux de vie, éditions Dunod, 1998.(en) Gilbert Chauvet, Theoretical Systems in Biology : Hierarchical and Functional Integration, vol I, II et III, Elsevier, 1996.Bernard Calvino, Introduction à la physiologie, éditions Belin, 2003  (ISBN 2-7011-3079-4).Lauralee Sherwood, Hillar Klandorf et Paul Yancey, Physiologie animale, De Boeck Superieur, 2016 (lire en ligne)Pour une biologie et une physiologie intégrative sur le site de Gilbert ChauvetUne modélisation piagétienne du système nerveux humain Portail de la physiologie   Portail de la biologie   Portail de la médecine"
médecine;"La santé est « un état de complet bien-être physique, mental et social, et ne consiste pas seulement en une absence de maladie ou d'infirmité ». Dans cette définition par l'Organisation mondiale de la santé, OMS, depuis 1946, la santé représente « l’un des droits fondamentaux de tout être humain, quelles que soient sa race, sa religion, ses opinions politiques, sa condition économique ou sociale »,. Elle implique la satisfaction de tous les besoins fondamentaux de la personne, qu'ils soient affectifs, sanitaires, nutritionnels, sociaux ou culturels.. Mais cette définition confond les notions de santé et de bien-être.Par ailleurs, « la santé résulte d’une interaction constante entre l’individu et son milieu » et représente donc « cette « capacité physique, psychique et sociale des personnes d’agir dans leur milieu et d’accomplir les rôles qu’elles entendent assumer d’une manière acceptable pour elles-mêmes et pour les groupes dont elles font partie ». René Dubos présente en 1973 la santé comme « la situation dans laquelle l'organisme réagit par une adaptation tout en préservant son intégrité individuelle. C'est l'état physique et mental relativement exempt de gênes et de souffrances qui permet à l'individu de fonctionner aussi longtemps que possible dans le milieu où le hasard ou le choix l'ont placé. »,Pour René Leriche en 1936, « la santé c'est la vie dans le silence des organes. »,Dans les sociétés traditionnelles (« primitives »), la santé relève généralement autant de l'individu que du groupe. Elle est imbriquée avec les croyances animistes et religieuses, et le rôle des guérisseurs (chamans, sorciers, etc.) qui utilisent à la fois la pharmacopée locale, le toucher et des pratiques relevant de la magie, de la divination, ou de la psychologie.À partir du XVIIIe siècle, la maladie cesse progressivement d'être considérée comme une fatalité et le corps redevient un sujet de préoccupation. Ce mouvement concerne d'abord les élites, puis s'étend progressivement à l'ensemble de la société. La santé devient alors un droit que les États se doivent de garantir.L'état de santé se recherche à la fois pour chaque individu, avec la médecine clinique, ou pour une population, avec la santé publique.La santé d'une population est classiquement évaluée d'abord par les taux de mortalité et de morbidité, avec l’espérance de vie. La santé est une notion relative, « parfois non présentée comme corollaire de l'absence de maladie : des personnes porteuses d'affections diverses sont parfois jugées « en bonne santé » si leur maladie est contrôlée par un traitement. A contrario, certaines maladies peuvent être longtemps asymptomatiques, ce qui fait que des personnes qui se sentent en bonne santé peuvent ne pas l'être réellement. »« État de santé ressentie » : c'est l'un des indicateurs d'état de santé. Il est publié tous les deux ans depuis 2002, pour les pays de l'OCDE. Après une tendance à la hausse de 2002 à 2008, il a chuté de plusieurs points en 2010 « Quelles que soient les tranches d’âge, le pourcentage des femmes et des hommes s’estimant en bonne ou très bonne santé baisse en 2010. Et lorsque l’on considère l’ensemble des sexes, il en est de même pour le quintile de revenu le plus élevé ». En 2008, 74,9 % des hommes se jugeaient en bonne ou très bonne santé, contre 70,6 % en 2010. Pour les femmes ce taux est passé de 70,1 % à 66,5 %.La santé mentale peut être considérée comme un facteur très important de la santé physique pour les effets qu'elle produit sur les fonctions corporelles. Ce type de santé concerne le bien-être émotionnel et cognitif ou une absence de trouble mental. L'Organisation mondiale de la santé (OMS) définit la santé mentale en tant qu'« état de bien être dans lequel l'individu réalise ses propres capacités, peut faire face aux tensions ordinaires de la vie, et est capable de contribuer à sa communauté ». Il n'existe aucune définition officielle de la santé mentale. Il existe différents types de problèmes sur la santé mentale, dont certains sont communément partageables, comme la dépression et les troubles de l'anxiété, et d'autres non communs, comme la schizophrénie ou le trouble bipolaire.Pour l'Organisation mondiale de la santé (OMS), la santé reproductive est une composante du droit à la santé.Cette notion récente évoque la bonne transmission du patrimoine génétique d'une génération à l'autre. Elle passe par la qualité du génome, des spermatozoïdes et des ovules, mais aussi par une maternité sans risque, l'absence de violences sexuelles et sexistes, l'absence de maladies sexuellement transmissibles (MST), la planification familiale, l'éducation sexuelle, l'accès aux soins, la diminution de l'exposition aux perturbateurs endocriniens, etc.Un certain nombre de polluants (dioxines, pesticides, radiations, leurres hormonaux, etc.) sont suspectés d'être, éventuellement à faibles ou très faibles doses, responsables d'une délétion de la spermatogenèse ou d'altération des ovaires ou des processus de fécondation puis de développement de l'embryon. Certains sont également cancérigènes ou mutagènes (ils contribuent à l'augmentation du risque de malformation et d'avortement spontané).Les soins de santé reproductive recouvrent un ensemble de services, définis dans le Programme d’action de la Conférence internationale sur la population et le développement (CIPD) tenue au Caire (Égypte) en septembre 1994 : conseils, information, éducation, communication et services de planification familiale ; consultations pré et postnatales, accouchements en toute sécurité et soins prodigués à la mère et à l’enfant; prévention et traitement approprié de la stérilité ; prévention de l’avortement et prise en charge de ses conséquences ; traitement des infections génitales, maladies sexuellement transmissibles y compris le VIH/SIDA ; le cancer du sein et les cancers génitaux, ainsi que tout autre trouble de santé reproductive ; et dissuasion active de pratiques dangereuses telles que les mutilations sexuelles féminines.La santé au travail fait partie des principaux thèmes de santé identifiés par l'OMS.Un déterminant de santé est un facteur qui influence l’état de santé d'une population soit isolément, soit en association avec d’autres facteurs.L'hygiène est l'ensemble des comportements concourant à maintenir les individus en bonne santé. Ils demandent de pouvoir notamment faire la part entre les « bons microbes » et ceux qui sont pathogènes ou peuvent le devenir dans certaines circonstances. Ces circonstances l'hygiène cherche à les rendre moins probables, moins fréquentes ou supprimées. Après une phase hygiéniste, dont l'efficacité de court terme est indiscutable, sont apparus une augmentation des allergies, des maladies auto-immunes, des antibiorésistances et des maladies nosocomiales jugées préoccupantes. La recherche de juste équilibre entre exposition au risque et solution médicale usuelle est rendue difficile dans un contexte d'exposition accrue à des cocktails de polluants complexes (pesticides en particulier) et perturbateurs hormonaux, de modifications sociétales et climatiques planétaires (cf. maladies émergentes, risque pandémique, zoonoses, risque de bioterrorisme, etc.).La lutte contre les infections nosocomiales à l'hôpital, ou contre les toxi-infections alimentaires par exemple, est née après la découverte de l'asepsie sous l'influence par exemple de Ignace Semmelweis ou Louis Pasteur. Les comportements individuels et collectifs sont de toute première importance dans la lutte contre les épidémies ou les pandémies.Cette discipline de l'hygiène vise donc à maîtriser les facteurs environnementaux pouvant contribuer à une altération de la santé, comme la pollution par exemple, avec des problèmes paradoxaux à gérer : par exemple, l'amélioration des conditions d'hygiène semble avoir paradoxalement pu favoriser la réapparition de maladies comme la poliomyélite et diverses maladies auto-immunes et allergies.De nombreux facteurs de risque sont intrinsèquement liés au mode de vie. Les soins corporels, l'activité physique, l'alimentation, le travail, les problèmes de toxicomanie, notamment, ont un impact global sur la santé des individus.Nutrition : Aliments - Oligo-élément - AlicamentProduits d'hygiène : Crème solaire - Dentifrice - Préservatif - SavonToxicomanies & dépendances : Alcool - Cannabis - Cocaïne - Tabac - Jeu pathologiqueDe nombreux risques et dangers sont liés au domaine de la santé, l'évolution humaine et également les changements de son mode de vie ne sont pas sans conséquences. L'alimentation et les nouvelles technologies sont également des facteurs de risques en France et dans le reste du monde. Les rythmes, les cadences de travail ; les gestes inadaptés sont des facteurs très importants sur la santé. Ils entraînent des troubles psychosomatiques et parfois des handicaps pour la vie.Quatre facteurs permettraient d'allonger considérablement la durée de la vie : absence de tabac, consommation d'alcool égale ou inférieure à un demi verre par jour, consommation de 5 fruits et légumes par jour, exercice physique d'une demi-heure par jour. Le tout donnerait une majoration de l'espérance de vie de 14 ans par rapport au non-respect de ces facteurs.Du strict point de vue de l'alimentation, de nombreuses études concordantes concluent qu'une alimentation exclusivement végétarienne permet de limiter les risques de cancer et de maladies cardio-vasculaires, et donc d'avoir une espérance de vie en bonne santé plus longue,. Les études mettent à la fois en évidence les bénéfices d'une alimentation riche en légumes et fruits et les risques relatifs liés à la consommation de viande, poisson et produits laitiers,,,. Les compléments alimentaires synthétiques ne seraient absolument pas nécessaires,.D'autres pistes sont explorées pour allonger la durée de vie en bonne santé : le jeûne, le jeûne intermittent et la restriction calorique.Par ailleurs, l'« hygiénisme moral » trans-national débuté au XIXe siècle (à ne pas confondre avec la médecine alternative créée par Herbert Shelton) est une doctrine contre le « relâchement des mœurs », ce qui serait le meilleur moyen de garantir la santé. C'est ce courant qui a par exemple déclaré la lutte contre la syphilis ou l'alcoolisme comme priorité nationale. C'est également lui qui déclare que si les obèses sont gros, c'est qu'ils sont gourmands et paresseux, ou encore que les fumeurs n'ont pas de volonté; Il semble persister dans certaines politiques et campagnes d'information et d'éducation des citoyens à l'hygiène.C'est un domaine, parfois nommé « santé environnementale », qui se développe depuis la fin du XXe siècle, à la suite de la prise de conscience du fait que l'environnement, notamment lorsqu'il est pollué, est un déterminant majeur de la santé.La pollution aiguë ou chronique, qu'elle soit biologique, chimique, due aux radiations ionisantes, ou due aux sons ou la lumière (ces facteurs pouvant additionner ou multiplier leurs effets) est une source importante de maladies.Dans l'Union européenne, la Commission a adopté (11 juin 2003) une « stratégie communautaire en matière de santé et d'environnement », traduite le 9 juin 2004, en un « Plan d'action » (2004-2010), qui vise notamment les maladies dites « environnementales ». Cela concerne l'asthme et les allergies respiratoires, en cherchant plus généralement à « mieux prévenir les altérations de la santé dues aux risques environnementaux » (dont l'exposition aux pesticides et à leurs résidus). Des systèmes de veille sanitaire permanente doivent identifier les menaces émergentes (dont nanotechnologies, OGM, maladies émergentes, impacts des modifications climatiques, etc.) et en évaluer l'impact sanitaire selon des actions réalisées au niveau communautaire mais aussi national. Un « plan d'action environnement et santé » va être développé afin de mettre en œuvre cette stratégie ; de plus un processus de consultation a été lancé. Le plan d'action vise à faire le point sur les connaissances scientifiques existantes et à évaluer la cohérence et les progrès réalisés dans l'installation du cadre législatif communautaire en matière de santé et d'environnement. Un nouveau système d'information sur la santé est prévu « qui fonctionnera également dans le domaine de l'environnement » et veut devenir « la plus importante source de données fiables pour l'évaluation de l'impact des facteurs environnementaux sur la santé ». Ces aspects seront coordonnés avec les systèmes de réaction rapide et une approche intégrée « visant à juguler les déterminants environnementaux de la santé ».En ce qui concerne plus spécifiquement la France, un premier Plan national santé-environnement a été lancé en 2004 et un second en 2009, à la suite du Grenelle de l'environnement. Le bilan des actions menées devrait être fait en 2013.La santé publique désigne à la fois l'état sanitaire d'une population apprécié via des indicateurs de santé (quantitatifs et qualitatifs, dont l'accès aux soins) et l'ensemble des moyens collectifs susceptibles de soigner, promouvoir la santé et d'améliorer les conditions de vie.La notion de santé publique regroupe plusieurs champs :la santé au travail incluant la médecine du travail et parfois des démarches épidémiologiques ;la gestion des campagnes de prévention, qui doivent influencer les autres secteurs de la société pour y promouvoir la santé (économie, écoles, trafic, habitation, environnement, style de vie, etc.), la vaccination... ;l'organisation des réseaux de soins : premiers secours, hôpitaux, médecine libérale, médecine d'urgence... ;la formation initiale et continue des professions médicales et paramédicales ;la sécurité sociale et l'assurance maladie (Sécurité sociale en France) ;la recherche médicale et pharmacologique.Les règles en matière de santé font l'objet de textes internationaux édictés par l'OMS ou la FAO (Codex alimentarius pour l'alimentation).L'Union européenne a produit de nombreuses directives, règlements ou décisions pour protéger la santé des consommateurs ou d'animaux consommés.La promotion de la santé telle que définie par l'OMS est le « processus qui confère aux populations les moyens d'assurer un plus grand contrôle sur leur propre santé, et d'améliorer celle-ci ». Cette démarche relève d'un concept définissant la « santé » comme la mesure dans laquelle un groupe ou un individu peut d'une part réaliser ses ambitions et satisfaire ses besoins, et d'autre part évoluer avec le milieu ou s'adapter à celui-ci.La santé est prise en compte par le droit, y compris du point de vue des Conditions de travail.Les crises sanitaires sont des pandémies importantes, qui touchent entre une dizaine de personnes (cas des crises très médiatisées qui touchent les pays développés, comme certaines crises alimentaires) et des millions de personnes. Elles peuvent avoir des coûts économiques, sociaux et politiques considérables.L'OMS a d'ailleurs été créée pour qu'une pandémie telle que celle produite par la grippe espagnole ne se reproduise pas avec les mêmes effets (30 à 100 millions de morts selon les sources).Les sommes en jeu dans le domaine de la santé sont considérables, tant pour les coûts induits par les maladies, les pollutions et l'absentéisme, que par le marché des soins et des médicaments (en 2002, le marché mondial du médicament a été évalué à 430,3 milliards de dollars, contre 220 milliards en 1992). Le marché pharmaceutique a augmenté de 203 milliards d'euros. Et la consommation médicale progresse plus rapidement que le PIB dans les pays développés.Des crises sanitaires telles qu'une pandémie peuvent avoir des coûts économiques, sociaux et politiques considérables.La santé comme concept peut être un objet d’étude anthropologique. Tel que rapporté par Roy, elle est souvent conceptualisée comme une construction sociale par les anthropologues puisque le rapport que les sociétés ont avec elle est très variable d’une à l’autre, et selon les époques également. Le travail anthropologique cherchera donc à mieux comprendre l’expérience que font les groupes sociaux et culturels de la santé. Cet objet d’étude, pour faire preuve de rigueur méthodologique, doit être replacé dans son contexte global, notamment à travers les changements sociaux. On cherche alors à comprendre les phénomènes de relation santé/maladie, bien que de plus en plus le schéma santé/vie prend place. Pour dire autrement, selon Massé, l’anthropologie médicale s’intéresse à comment les acteurs sociaux définissent la bonne ou la mauvaise santé, et comment les maladies sont soignées dans ce contexte.Quelques approches théoriques sont nées en anthropologie médicale, rapportées par Roy. Parmi elles, celle de la théorie médico-écologique, celle de la phénoménologie et celle de la critique de la médecine et de la santé internationale.La théorie médico-écologique est formulée par Alexander Alland au début des années 1970, mais est reprise par d’autres quelques années après. Elle part du principe que les groupes humains adaptent leur culture à l’environnement. Cette théorie propose l'idée que l’adaptation culturelle est intimement liée à l’adaptation biologique en fonction de l'environnement et du milieu dans lequel le groupe se trouve. Ainsi, la santé est liée à ces transformations externes.L’approche phénoménologique se développe en parallèle à cette dernière. Des auteurs comme Kleinman et Good en sont un point d’origine, en cherchant à redonner une subjectivité à l’expérience humaine de la santé, s’éloignant de l’objectivité préconisée par la médecine. Pour ce faire, des perspectives expérientielles et sémantiques sont mobilisées.L’approche critique de la médecine et de la santé internationale se développe dans les années 1960. Elle a pour objet les conditions notamment politiques et économiques, donc globales, dans lesquelles sont vécues la santé et la maladie : les inégalités sociales façonnent l’accès à l’information, aux ressources de maintien de la santé et aux traitements. Un texte clé pour comprendre ce mouvement est notamment celui de Baer, Singer et Johnsen.De nombreux médias et émissions sont spécialisés dans les thèmes de la santé. En voici une sélection :Le Magazine de la santé, sur France 536.9, sur la Radio télévision suisseQuoi de neuf doc ?, sur TV5 MondeRadio Public SantéRadio France internationale, émission Priorité santéRadio Canada première chaîne, émission RDI SantéPlace à la santéSanté MagazineAlternative santéEnvironnement, Risques et SantéHealth On the Net Foundation est fondation qui indique aux internautes dans quels sites internet, ils peuvent obtenir des informations justes et sérieuses dans le domaine de la santé.PubMedSantepratiquePortail Santé-UEFasosante.netCarenity (réseau social santé sur internet destiné aux malades et à leurs proches).André Rauch, Histoire de la santé, PUF, Que-sais-je ?, 1995Georges Canguilhem, La santé, concept vulgaire et question philosophique, Sables, Pin-Balma, 1990Ressources relatives à la santé : (en) Medical Subject Headings (en) NCI Thesaurus Ressource relative à la recherche : Horizon 2020 Liste des thèmes de santé, site de l'OMS(en) Global Health, site Our World in Data Portail des soins infirmiers   Portail de la médecine   Portail de la société"
médecine;"En médecine, un symptôme (du grec ????????, « rencontrer ») ou signe fonctionnel est un signe qui représente une manifestation d'une maladie, tel qu'il est observé chez un patient. En général, pour une pathologie donnée, les symptômes sont multiples, et parfois il peut ne pas y avoir de symptôme (la maladie ou le malade est dit dans ce cas asymptomatique) ou peu de symptômes (maladie ou malade paucisymptomatique). Inversement, un même symptôme peut très souvent être attribué à différentes maladies : on ne peut donc en général pas conclure automatiquement qu'un symptôme (par exemple, le mal de gorge) est dû à une maladie donnée (par exemple, la grippe) ; ce serait commettre le sophisme de l'affirmation du conséquent.Le mot ????????, en grec, signifie « accident », « coïncidence » ; il est constitué du préfixe ???, « avec » et de ?????, « arriver », « survenir ». Le symptôme est donc, à l'origine, « ce qui survient ensemble », ce qui « concourt » ou « co-incide », au sens littéral du terme.Les symptômes sont les signes cliniques dont le malade se plaint (comme la douleur, la toux, le vertige, la tristesse). Les symptômes sont les éléments d'alerte d'un processus pathologique en cours, motivant ainsi le recours à une consultation médicale permettant d'objectiver la plainte en retrouvant des signes, qui, rassemblés en syndrome, puis en maladie en établissant un diagnostic, permettront de guider l'attitude thérapeutique.Les symptômes sont donc à différencier :des autres signes cliniques :les signes physiques, découverts en examinant le malade : contracture abdominale, souffle cardiaque,certains signes généraux : fièvre, hypotension artérielle ;des signes paracliniques obtenus à l'aide d'examens complémentaires :les signes radiologiques à la suite de radiographies,les signes biologiques à la suite de prélèvements.Par exemple, dans l'arthrose de hanche, le patient peut se plaindre de douleur à la marche (symptôme), et le praticien pourra objectiver à l'examen une limitation de mobilité de la hanche (signe physique), et sur une radiographie du bassin (signe radiologique).En créant la psychanalyse, Sigmund Freud va donner un sens au symptôme. À la suite des Études sur l'hystérie (1895), il n'a plus cesse de l'interroger dans les manuscrits à une époque « où la psychiatrie le réduisait à un phénomène hétérogène et opaque de la vie psychique ».Le symptôme peut être une manifestation somatique : une paralysie, des troubles du langage.Il peut être aussi une manifestation psychique : angoisse, hydrophobie.En étudiant le cas d'Anna O. (Bertha Pappenheim), une hystérique soignée par Josef Breuer grâce à la méthode cathartique, Freud a d'abord vu dans le symptôme un résidu mnésique d'expériences émotives (c'est-à-dire de traumatismes psychiques).Ensuite, en formulant sa nouvelle compréhension du système psychique, il a interprété différemment le symptôme.L'appareil psychique est composé de différentes instances en conflit : le moi, le ça et le surmoi.Quand une représentation (pulsionnelle) tombe sous le coup d'un interdit, elle est refoulée dans l'Inconscient par la censure opérée par le moi, mais jamais anéantie. Un processus alors de tentative de réapparition des éléments refoulés se met en place : c'est le retour du refoulé. Il y a plusieurs façons de déjouer la censure : le rêve, les lapsus, les oublis et les actes manqués ou bien les symptômes. Ces formations substitutives sont des formes de déguisement de la représentation, rendus acceptables pour la conscience pour pouvoir réinvestir son champ. Ainsi, ils permettent la satisfaction du désir sans éveiller la censure en formant un compromis entre les désirs et les interdits. Ce sont tous ces déguisements qui sont investigués, interprétés dans la cure psychanalytique.Remarque : il y a des liaisons associatives entre le symptôme et ce à quoi il se substitue.Le symptôme est le substitut de représentations tombées sous le coup d'un interdit et refoulées dans l'Inconscient. Il est le déguisement de ces représentations pour qu'elles puissent réinvestir le champ de la conscience, en étant acceptable. Et, il apporte une satisfaction de remplacement au désir inconscient, sans éveiller la censure et même en satisfaisant les exigences défensives. Cette double-satisfaction explique la capacité de résistance du symptôme car il est maintenu des deux côtés.Récapitulatif :il est formation de compromis en tant qu'il est le produit du conflit défensif ;il est formation substitutive dans la mesure où c'est le désir qui cherche à se satisfaire ;il est formation réactionnelle dans la mesure où c'est le processus défensif qui prévaut.Le symptôme est satisfaction, décharge pulsionnelle, il offre un bénéfice primaire. On ne saurait chercher à retirer au malade mental son symptôme, en ce qu'il en jouit, et que le psychologue doit reconnaitre comme jouissance.Ce bénéfice primaire correspond à la signification que porte le symptôme, signification qui seule permet l'expression d'un désir inconscient - le symptôme se rattache donc à la représentation, voire au discours. Pour Jacques Lacan, le symptôme est donc métaphore (Le symptôme est une métaphore que l'on veuille ou non se le dire).Le symptôme peut également engendrer un bénéfice secondaire, plaisir supplémentaire qui ne se relie donc pas directement au sens que veut énoncer ce signe de la maladie, mais qui provient plus d'un hasard relatif cette fois à la nature même du symptôme. Ainsi, le procédurier paranoïaque ralliant à lui un mouvement de soutien.Du point de vue psychosociologique, le symptôme est la façon particulière dont un individu trouve sa place dans le monde et règle son rapport à celui-ci, en fonction des contraintes et des stimulations psychosociales qui lui parviennent. Le symptôme est un prolongement de la personnalité, qui permet à cette dernière d'appréhender le monde mais aussi de s'en distancier, par un ensemble de protections constitutives dudit symptôme.Ainsi le symptôme est-il, du point de vue du sujet :stratégie d'individualisation ;matériau de la personnalité ;interprétation continue du monde ;modalité comportementale dynamique ;dispositif protecteur du Moi (ou ego) ;routine pathologique sitôt qu'il étouffe la créativité du sujet ou porte atteinte à l'intégrité d'autrui.Cet article est partiellement ou en totalité issu de l'article intitulé « Symptôme fonctionnel » (voir la liste des auteurs). Médecine et biologie  Psychanalyse Augustin Jeanneau, « symptôme », dans Alain de Mijolla (dir.), Dictionnaire international de la psychanalyse 2. M/Z, Paris, Hachette, 2005 (ISBN 2-0127-9145-X), p. 1769-1770.Augustin Jeanneau et Roger Perron, « symptôme (formation de -) », dans Alain de Mijolla (dir.), Dictionnaire international de la psychanalyse 2. M/Z, Paris, Hachette, 2005 (ISBN 2-0127-9145-X), p. 1770-1772.Yves Morhain, « Permanence du corps et variations du symptôme hystérique et/ou psychosomatique », Psychothérapies, 2011/2 (Vol. 31), p. 131-141. DOI : 10.3917/psys.112.0131. [[ lire en ligne]]Valentin Nusinovici, « symptôme,sinthome », dans Alain de Mijolla (dir.), Dictionnaire international de la psychanalyse 2. M/Z, Paris, Hachette, 2005 (ISBN 2-0127-9145-X), p. 1772-1774.Marcel Scheidhauer, « Le symptôme, le symbole et l'identification dans l'hystérie dans les premières théories de Freud », in: Enfance, tome 40, n°1-2, thématique : « Identités, Processus d'identification. Nominations », 1987, p. 151-162, sur le site de Persée, consulté le 30 mars 2021 [lire en ligne].Liste des symptômes en médecine humaineSémiologie médicaleTableau cliniqueSigne physiquePathomimieSymptôme (pathologie végétale)Inhibition, symptôme et angoisseSinthome Portail de la médecine   Portail de l’agriculture et l’agronomie   Portail de la psychologie"
médecine;"Un antiseptique est un désinfectant à usage corporel ; c'est une substance qui détruit ou prévient le développement des agents infectieux (microorganismes ou virus) sur la peau ou les muqueuses. Les antiseptiques sont à distinguer des antibiotiques, qui agissent seulement contre les bactéries et sont administrés par injection ou par voie orale, et des bactériophages qui sont des produits contenant des virus prédateurs des bactéries. Au plan règlementaire, les antiseptiques sont des médicaments nécessitant une autorisation de mise sur le marché.L'antisepsie fut étudiée expérimentalement au XVIIIe siècle par John Pringle. Joseph Lister, inspiré par les travaux de Pasteur sur les fermentations, fut un des pionniers et le plus efficace vulgarisateur de l'application de l'antisepsie à la chirurgie.Il existe plusieurs classes de produits antiseptiques, déterminées selon leur structure chimique et leur efficacité. Les antiseptiques majeurs regroupent les biguanides (chlorhexidine), les dérivés iodés (povidone iodée), les dérivés chlorés (hypochlorite de sodium), et les alcools (éthanol),. Les différentes classes d'antiseptiques ne doivent pas être mélangées ni combinées, sous peine d'inactivation, voire d'entraîner la formation de produits irritants. Certains antiseptiques existent sous forme de solution aqueuse ou de solution alcoolique. En dehors d'une contre-indication occasionnelle, la forme alcoolique doit être préférée. En effet, l'action est plus rapide, et l'indice de pénétration de l'antiseptique meilleur. La concentration et la pénétration du di-iode est ainsi augmentée.Les antiseptiques mineurs regroupent les colorants, comme l'éosine, la solution de Milian, la fluorescéine, ou le Bleu de méthylène.D'autres classes, moins utilisées, existent. Les composés organomercuriels ne sont plus fabriqués. Les oxydants comprennent l'eau oxygénée et le permanganate de potassium. Les ammoniums quaternaires sont, eux, dominés par le chlorure de benzalkonium.La rémanence est le temps durant lequel persiste l'action antiseptique en absence de nouvelle application. L'éthanol, par exemple, ne possède pas de rémanence, tandis que celle de la povidone iodée est de trois heures.Les antiseptiques sont utilisés de plusieurs manières. Ils peuvent servir à désinfecter une plaie souillée, sur peau lésée, mais aussi sur peau saine à préparer le champ opératoire. À cette fin, il existe différents types de produits adaptés à l'utilisation cutanée ou muqueuse, orale comme gynécologique.Les antiseptiques permettent l'élimination de la flore cutanée transitoire, et la réduction de la flore résidente.Le choix du protocole doit être adapté au niveau de risque. On distingue les procédures en un temps (passage unique d'une solution alcoolique) pour les gestes à faible risque, en deux temps (deux passages avec un antiseptique majeur) pour les gestes à risque intermédiaire, et enfin en quatre temps pour les gestes à risque élevé de contamination. Cette dernière procédure comporte en premier lieu une détersion par un savon antiseptique, qui est rincé, puis la peau est séchée. Enfin, le dernier temps est un badigeon par un antiseptique majeur de la même classe.La durée de conservation des antiseptiques dépend à la fois du produit et du conditionnement. Les conditionnements en uni-dose, de même que les solutions diluées, doivent être jetés après utilisation. Les conditionnements multidoses se conservent, eux, pendant un mois. À cette fin, la date d'ouverture doit être écrite sur le flacon, qui doit être rebouché après utilisation. L'antiseptique ne doit pas être transvasé, ni reconditionné.Les différentes classes d'antiseptiques ne possèdent pas le même spectre antimicrobien. Pour toutes, l'activité létale est maximale envers les bactéries. Les biguanides sont peu actifs sur les champignons, et inactifs sur les spores, au contraire des dérivés iodés et chlorés et de l'éthanol. Les virus sont difficilement inactivés par les biguanides et l'éthanol, alors que les dérivés iodés et chlorés possèdent une activité létale modérée.AntisepsieAntiseptiques et désinfectants Portail de la microbiologie   Portail de la médecine   Portail de la pharmacie"
médecine;"Une maladie infectieuse (ou infection) est une maladie provoquée par l'invasion d'un ou plusieurs micro-organismes ou agent infectieux (bactéries, champignons, parasites, protozoaires, virus) dans un tissu où ils se multiplient, et par une réaction générale des cellules et des tissus infectés pour éliminer ces agents pathogènes ou leurs toxines (processus impliquant notamment le système immunitaire des plantes et des animaux).L'étude des agents infectieux relève de la biologie, de la microbiologie médicale, de l'épidémiologie et de l'écoépidémiologie. Dans la nature, des maladies infectieuses se développent chez tous les organismes vivants (animaux, végétaux, fongiques, micro-organismes… il existe également des virus de virus). En tant qu'interactions durables, les maladies infectieuses font partie des boucles de rétroaction qui entretiennent la stabilité relative (équilibre dynamique) des écosystèmes, la plupart des pathogènes coévoluant avec leur hôte depuis des millions d'années. Leur mode de transmission est variable et dépend de leur réservoir (humain, animal, environnemental) et parfois de vecteurs (maladies vectorielles).Elles sont plus ou moins contagieuses. Par exemple, le tétanos est une toxi-infection causée par Clostridium tetani, une bactérie qui se trouve dans la terre. Il n’y a pas de transmission interhumaine, l’infection se produit lorsque la bactérie entre dans l’organisme par une plaie souillée. Un vaccin existe contre cette affection et est obligatoire en France pour tous les enfants d’âge scolaire. Autre exemple, le paludisme est dû à un parasite, le Plasmodium falciparum (il existe d’autres Plasmodii), transmis d’homme à homme par l’intermédiaire d’un moustique, l’anophèle. Le réservoir du parasite est humain mais il n’y a pas de transmission interhumaine. Il n’existe à l'heure actuelle pas de vaccin. La tuberculose se transmet d’homme à homme par mécanisme aéroporté : le réservoir est humain et c’est une maladie contagieuse. Les infections sexuellement transmissibles (ou encore MST pour maladies sexuellement transmissibles) se transmettent à l’occasion de rapports sexuels ou par le sang.De nombreux microbes vivent normalement et nécessairement dans notre tube digestif et sur notre peau, et ne deviennent infectieux qu'à certaines occasions. Le contact avec les microbes est nécessaire à l'entretien et au bon fonctionnement de la digestion et du système immunitaire.L'infection est le terme désignant soit une maladie infectieuse en général, soit la contamination par un germe. C'est la conséquence pathologique au niveau d'un tissu ou d'un organisme de la présence anormale et/ou de la réplication d’un germe bactérien, viral ou mycosique. La contamination est la pénétration du germe dans un organisme.L'infectiologie est la branche de la médecine concernant les maladies infectieuses. Le médecin spécialiste est un infectiologue. Suivant le type de germe, il est également question de bactériologie, de virologie, de parasitologie ou de mycologie.Un sepsis est une infection grave. L'adjectif septique se rapporte à un organisme ou un objet contaminé par un germe (fosse septique par exemple). Une septicémie est la contamination grave et durable (sans traitement) du sang par un germe. Une bactériémie est une contamination transitoire du sang par un germe. Lorsque les cas se multiplient dans un lieu et une période limitée, il est question d’épidémie. Si la diffusion est beaucoup plus généralisée, il est alors question de pandémie. Lorsque l'épidémie concerne le milieu animal, il est question d'épizootie. Lorsque le germe se transmet de l’animal à l’homme, il est question d'anthropozoonose ou plus simplement de zoonose.Le contage désigne la contamination par le germe.La période d’incubation est le délai entre le contage et la première manifestation de la maladie. Le malade peut être contagieux durant ce temps.La période de contagion est le temps pendant lequel le patient excrète le germe et peut le transmettre. Elle dépend de chaque maladie infectieuse.Les infections nosocomiales (ou iatrogènes) sont des infections attrapées à l’hôpital. Elles sont particulièrement complexes et dangereuses car elles surviennent chez des sujets affaiblis et concernent souvent des germes résistants aux antibiotiques. Il s’agit d’un problème de santé publique majeur.Comme le résumait en 1935 le bactériologiste français Charles Nicolle : « Malheureusement, les signes des maladies infectieuses sont presque tous les mêmes : fièvre, maux de tête, agitation ou stupeur, éruption. Seuls leur groupement, leur succession, une observation minutieuse ont pu, après de longs tâtonnements, permettre d'établir des tableaux symptomatiques particuliers et les distinguer entre eux. »Les maladies infectieuses sont responsables dans le monde de 17 millions de décès par an, soit un tiers de la mortalité et 43 % des décès dans les pays en voie de développement (contre 1 % dans les pays industrialisés). Les six maladies suivantes représentent 90 % des décès par maladies infectieuses dans le monde.Depuis les années 2000, de nombreuses urgences sanitaires reliées à l’émergence de nouveaux agents étiologiques responsables de maladies respiratoires sévères sont survenues : le syndrome respiratoire aigu sévère (SRAS), les infections d’influenza aviaire A (H5N1) chez les humains dans plusieurs pays de l’Asie, la pandémie de grippe A (H1N1) et, plus récemment, le virus influenza aviaire A (H7N9) en Chine, le coronavirus du syndrome respiratoire du Moyen-Orient (MERS-CoV) et la pandémie de Covid19. La pathogénicité et la létalité élevées de la plupart de ces virus génèrent des répercussions sociales et une pression importante sur les services de santé.La population mondiale infectée par le VIH continue de croître : rien qu’en 2000, 5,3 millions de nouveaux cas se sont déclarés dans le monde, dont la moitié parmi les jeunes de plus de 25 ans.Après une phase de forte régression (époque pastorienne et hygiéniste), les maladies infectieuses sont revenues ou sont devenues plus résistantes (antibiorésistance). Des maladies infectieuses émergentes ou réémergentes inquiètent périodiquement les épidémiologistes et les autorités sanitaires en raison de leurs impacts sanitaires, économiques et socio-politiques actuels ou potentiels. Le Haut Conseil de la santé publique (HCSP) a récemment fait 25 recommandations (sur la recherche et l'enseignement, la surveillance sanitaire et la gestion raisonnée des crises sanitaires notamment).Les progrès de l'hygiène et de la vaccination ont fourni un espoir de pouvoir les éradiquer, mais elles sont encore en France, la troisième cause de mortalité :Il est également noté que certaines infections sont aussi à l’origine de maladies inflammatoires chroniques (telles que l’asthme) et de cancers.Les maladies infectieuses entravent la santé de base des individus et ont une influence négative sur chaque indice du développement humain et plus particulièrement sur l'espérance de vie à la naissance, l'éducation et le PIB réel. Elles sont responsables d'une forte mortalité dans les régions où l'hygiène connaît un déficit et où l’accès aux soins est difficile. La malnutrition ainsi qu'un accès limité à l'eau potable sont autant de facteurs aggravants qui diminuent les chances de survie des malades mais aussi des enfants en bas âge de même que leurs conditions de développement. Ces deux facteurs désarment le système immunitaire et peuvent être vecteurs de maladies infectieuses.Ces maladies ont des conséquences négatives importantes sur le développement cognitif et les performances scolaires chez l’enfant. La malaria, entre autres, peut causer de graves séquelles, dont des troubles comportementaux, des problèmes moteurs et un manque d’autonomie. Une telle infection est donc un frein à l’éducation. Dans le cas des épidémies, il peut arriver que les enseignants soient eux aussi touchés par la maladie. Un manque de corps enseignant réduirait de façon directe la qualité de l’éducation en affaiblissant le système scolaire. Par ailleurs, si dans une famille, les responsables de l'éducation des enfants (souvent la mère) sont touchés par la maladie, c’est l’éducation dans son ensemble qui va être affectée. Le coût du traitement réduit le budget qui aurait pu être accordé à la scolarisation mais également les conditions de vie de l’enfant. Ce qui crée un cercle vicieux : les couches les plus éduquées de la population sont de moins en moins atteintes par des maladies infectieuses telles que le sida. En effet ces personnes qui sont les plus éduquées sont les mieux informées sur les modes de transmission et de prévention. Or, plus de 80 % des personnes atteintes par ces maladies vivent dans les pays en développement.D'un point de vue macroéconomique, les maladies infectieuses ont un impact sur la croissance économique et le PIB. Dans les pays en développement, la main d’œuvre est le facteur-clé de la production et donc du PIB. Néanmoins, le bon fonctionnement des entreprises et la possibilité d'être concurrent sur le marché international nécessitent avant tout une bonne santé et une éducation de base. Lorsque la santé de la personne génératrice de revenu pour la famille est affectée, toute la famille en souffre. Les maladies infectieuses aggravent donc la pauvreté, réduisent la croissance économique, le capital humain et contribuent à l’augmentation des inégalités entre les pays en voie de développement et les pays riches.La prévention des maladies infectieuses vise à limiter le risque infectieux (y compris professionnel, notamment pour les métiers de la santé, de contact avec les animaux, des déchets, des cadavres, des eaux usées, des échantillons à analyser en laboratoires de biologie, etc.).Elle s’articule en trois volets : éviter l’infection, renforcer les défenses immunitaires et prendre des traitements préventifs (prophylaxie) en cas de risque d’exposition.La maladie infectieuse est provoquée par la pénétration dans l’organisme d’une bactérie ou d’un virus. La première précaution consiste donc à « fermer les portes d’entrée », à savoir :les voies respiratoires : tousser ou éternuer dans un mouchoir, dans le coude, ou dans les mains (en se les lavant immédiatement après) pour éviter de contaminer l’entourage ; porter un masque facial lorsque des personnes vulnérables sont rencontrées (par exemple dans certaines zones des milieux hospitaliers, personnes immunodéprimées) ou porteuses de virus très contagieux (comme le sras) ; pour la ventilation artificielle, utiliser un filtre antibactérien ;les voies digestives : se laver les mains avant de manger ou de préparer un repas, ou après une exposition à des liquides biologiques (par exemple en sortant des toilettes), voire les désinfecter lorsqu’il s’agit de liquides d’une autre personne (par exemple accident d'exposition au sang) ; porter des gants fins (latex, ou pour les personnes allergiques en PVC ou nitrile) lorsqu’une telle exposition est probable ; en général laver les mains régulièrement pendant la journée ;effraction cutanée : toute plaie grave devra être montrée à un médecin qui prendra les mesures nécessaires ; toute plaie simple doit être nettoyée, ou mieux désinfectée (voir l’article bobologie) ; mais la première précaution est bien sûr d’éviter de se faire une plaie, en respectant les règles de sécurité de certaines activités et en portant des protections adaptées (gants de travail…) ;voie oculaire : éviter de se frotter les yeux et se laver les mains avant au cas où cela arriverait ; en cas de risque d’exposition à des liquides biologiques, porter des lunettes de protection ;sexualité : utiliser un préservatif pour réduire les risques de transmission des maladies sexuellement transmissibles.Le port d'équipements de protection individuelle dépend de l’évaluation des risques. Au travail outre des gants de protection, un appareil de protection respiratoire et des lunettes masques ou une visière sont parfois nécessaires, voire un vêtement de protection intégral.Les gants fins sont recommandés en cas de risque d’exposition à des liquides biologiques ou chimiques, mais déconseillé pour les activités courantes : en effet, la peau est alors dans une atmosphère chaude et humide propice au développement de germes, et par ailleurs, il vaut mieux des mains propres que des gants sales. À noter qu’au bout d’une vingtaine de minutes, certains gants fins deviennent poreux ou sont incompatibles avec certaines substances.Il faut aussi limiter le développement de germes pathogènes sur et dans le corps et dans l’habitation, par une hygiène suffisante :hygiène corporelle : se laver, se brosser les dents ;hygiène ménagère : avoir un réfrigérateur créant un froid suffisant, décongelé et nettoyé régulièrement, laver les couverts, assiettes et verres après utilisation, stocker les ordures dans des poubelles dédiées et ramassées régulièrement par les services municipaux, évacuation des eaux usagées vers une fosse septique vidangée régulièrement ou vers les égouts, rangement et nettoyage de l’habitation, aérer pour limiter la pollution intérieure (acariens, composés organiques volatils) et donc les allergies et les maladies respiratoires ;surveiller et traiter les parasitoses (certaines facilitent les maladies infectieuses, virales ou bactériennes). Par exemple, chez le porc, l'ascaris augmente le risque de bronchopneumonie, la trichocéphalose l'entérite hémoragique, l'oesophagostomum les salmonelloses, les strongyloides le rouget, les metastrongylus la grippe porcine, etc.Les collectivités territoriales jouent un rôle important en ce qui concerne l’hygiène collective, avec la gestion des eaux pour fournir de l’eau potable, l’organisation de la collecte et du traitement des ordures, l’équarrissage des cadavres d’animaux et la police des funérailles et des lieux de sépulture (condition de transport et de conservation des corps avant crémation ou inhumation, gestion des cimetières et crématoriums).La première mesure consiste à avoir une bonne hygiène de vie : alimentation saine, exercice physique régulier, sommeil suffisant, éviter les comportements à risque (tabagisme, excès d’alcool), ce qui permet d’avoir un meilleur état de santé général donc de mieux résister aux infections.Par ailleurs, il convient de respecter les vaccinations préventives obligatoires, ou recommandées comme la vaccination des personnes âgées contre la grippe.Il faut aussi prendre précautionneusement les médicaments prescrits par un médecin, en lisant systématiquement les notices accompagnatrices, riches en informations (effets secondaires, interactions avec d’autres médicaments, recommandations…) et ne pas hésiter à questionner le médecin ou le pharmacien en cas de doute. Les effets peuvent ne pas être immédiats, et il faut continuer le traitement jusqu’à la fin même en cas d’amélioration et disparition des symptômes, notamment dans le cas des antibiotiques : la disparition des symptômes signifie la diminution du nombre de germes, mais pas leur disparition, si le traitement est interrompu trop tôt, ceux-ci peuvent se redévelopper, et devenir résistants à l’antibiotique.Il ne faut pas que le médecin prescrive systématiquement d’antibiotique : ils ne sont pas efficaces contre les maladies virales.Les mesures d’hygiènes simples sont les meilleurs traitement préventifs : lavage des mains, pour éviter la transmission des infections alimentaires, éternuer dans ses coudes lors d'un Éternuement et non pas dans ses mains afin de ne pas les « contaminer » par d'éventuels microbes… Il est parfois nécessaire de prendre des médicaments à titre préventif, comme les médicaments contre le paludisme lors d’un voyage dans un pays impaludé.La détection précoce d’une maladie permet de démarrer son traitement plus tôt et donc de réduire la mortalité ; il est recommandé de faire au moins une visite médicale par an. En cas de doute sur une infection (par exemple plaie souillée, accident d’exposition au sang, rapport sexuel non protégé), le médecin pourra mettre en place un traitement préventif pour diminuer les risques de développement d’une maladie. Pour les maladies sexuellement transmissibles, il existe en France des centres anonymes et gratuits de dépistage.Certains patients doivent être isolés (voire mis en quarantaine) pour éviter la dissémination du germe : ainsi, lors d’une varicelle, l’enfant ne doit pas aller à l’école pendant 15 jours à partir de la première éruption. Il s’agit de l'éviction scolaire. La prévention hospitalière des infections nosocomiales est un sujet complexe. Elle repose essentiellement sur l’hygiène des soignants et des soignés (lavage des mains), sur l’isolement des patients porteurs de germes résistants aux antibiotiques, mais aussi sur une antibiothérapie ciblée et adaptée.Une nouvelle approche en phase d'étude est d'utiliser la phagothérapie à des fins préventives pour la santé humaine comme cela se fait déjà dans l'agriculture et l'industrie alimentaire.Leur étude relève de l'épidémiologie et pour les zoonoses ainsi que de l'écoépidémiologie.Certaines situations (crises sanitaires ou alimentaires…) ou lieux (ports, aéroports) sont des facteurs de risques.Le traitement par antibiotiques est le traitement qui a permis de vaincre les maladies infectieuses jusqu'à l'apparition des bactéries multi-résistantes.Il présente de nombreux avantages dont la possibilité d'une fabrication en masse, rapide et bon marché des médicaments antibiotiques.Il trouve ses limites avec l'apparition de bactéries de plus en plus résistantes.La phagothérapie est apparue au début du XXe siècle avec le développement par le Français Félix d'Hérelle de médicaments bactériophagiques réalisés à partir de virus bactériophages (simplement appelés bactériophages ou même phages) lytiques afin de traiter certaines maladies infectieuses d’origine bactérienne. D'Hérelle a ainsi traité des épidémies de peste et de choléra avec succès.La phagothérapie a été largement utilisée dans le monde avant la découverte des antibiotiques. Si elle a été progressivement abandonnée par les pays occidentaux séduits par les avantages de l’antibiothérapie, la phagothérapie traditionnelle est toujours employée et développée dans les pays de l'ancienne Union Soviétique. Dans les pays occidentaux, des patients victimes d'infection par bactéries multi-résistantes se regroupent pour faciliter l'accès aux traitements bactériophagiques étrangers,,.Elle connaît un regain d'intérêt en Occident avec l'émergence de l'antibiorésistance. Elle fait l'objet de recherches à l'Institut Pasteur mais son utilisation demeure soumise à ATUn par l'ANSM.Antoine van Leeuwenhoek (1632-1723) voit pour la première fois des agents bactériens en microscopie.Louis Pasteur permet le rapprochement entre maladie et agents infectieux. Première vaccination contre la rage.Robert Koch est célèbre pour sa découverte du bacille de la tuberculose qui porte son nom : le bacille de Koch.Jonas Salk et Albert Sabin assurent le développement de la vaccination anti-polio.Charles Nicolle, Destin des maladies infectieuses, PUF 1939Brown L. (2010), ""Le plan B pour un pacte écologique mondial"", Paris, Calmann-Lévy Souffle Court Editions, 509 pages.Contrepois A. L'invention des maladies infectieuses. Édition des Archives Contemporaines. 2001. Naissance et développement institutionnel de la bactériologie médicale en France et en Allemagne au XIXe siècle.Flahaut A. et Zylberman P. Des épidémies et des hommes. Édition de la Martinière. 2008. Une bonne vulgarisation par deux experts de la question, avec nombreuses photos et illustrations.INSTITUT PASTEUR , ""Le défi des maladies infectieuses"", http://www.pasteur.fr/ip/easysite/pasteur/fr/presse/dossiers-de-presse/sante-en-voyage/le-defi-des-maladies-infectieuses, dernière visite le 8 mars 2014.Nicolle C. Le destin des maladies infectieuses. Édition France Lafayette. 1993. Réédition d'un grand classique de 1933. Conférences au Collège de France par Charles Nicolle, Prix Nobel de Médecine 1928. Toujours d'actualité.Orth G. et Sansonetti P. (sous la direction de). La maitrise des maladies infectieuses. Académie des Sciences. EDP Sciences. 2006. État des lieux et recommandations adressées aux pouvoirs publics et à l'ensemble des acteurs de santé. Un ouvrage collectif à l'aspect sévère, mais une actualisation pointue de tous les aspects (médico-scientifiques, socio-culturels, etc.) du problème.Raoult D. (1999), ""Les nouvelles maladies infectieuses, que sais-je ?"", Presses universitaires de France, 128 pages.Dossier documentaire Société Française de Santé PubliqueRessources relatives à la santé : ICD-10 Version:2016 ICD9Data.com (en) Diseases Ontology (en) Medical Subject Headings (en) NCI Thesaurus (no + nn + nb) Store medisinske leksikon (cs + sk) WikiSkripta Infectiologie.com Portail de la médecine   Portail de la microbiologie   Portail des maladies infectieuses"
médecine;"En médecine, un patient est une personne physique recevant une attention médicale ou à qui est prodigué un soin.Le mot patient est dérivé du mot latin patiens, participe présent du verbe déponent pati, signifiant « celui qui endure » ou « celui qui souffre ».Il existe plusieurs dénominations communes au terme patient, dont personne soignée, bénéficiaire de soins , ""usager"" ou encore client employé notamment dans la culture anglophone[réf. souhaitée]. Dans la recherche médicale, le patient est parfois appelé sujet. On commence même à utiliser le terme d’actient (patient qui agit) du fait de l'évolution des patients à se renseigner par eux-mêmes et à poser de plus en plus de questions au praticien.[réf. souhaitée].En médecine, le patient bénéficie d'examens médicaux, de traitements prodigués par un médecin ou un professionnel de la santé pour faire face à une maladie ou à des blessures. Le patient peut également bénéficier d'actes de prévention.Knock ou le Triomphe de la médecine, pièce de théâtre de Jules Romains, ayant fait l'objet de plusieurs adaptations cinématographiques, qui illustre de manière humoristique les relations entre médecins et patients.Luc Perino, Patients zéro. Histoires inversées de la médecine, La Découverte, 2020Jean-Philippe Pierron, « Une nouvelle figure du patient ? Les transformations contemporaines de la relation de soins », Sciences sociales et santé, vol. 25, no 2,? 2007, p. 43-66 (DOI 10.3406/sosan.2007.1858)Relation médecin-patientCharte du patient hospitaliséÉducation thérapeutique du patientDossier médical du patientPatient zéro Portail de la médecine"
médecine;
médecine;"Les soins palliatifs sont des soins qui ne visent qu'au confort du malade, souvent en phase de fin de vie. L'objectif des soins palliatifs est de prévenir et de soulager les douleurs physiques, les symptômes inconfortables (nausées, constipation, anxiété...) ou encore la souffrance psychologique. Un soin palliatif est une mesure visant à endiguer les conséquences d'un grave problème médical, en ne se préoccupant plus de sa cause.En parallèle, une aide psychologique, morale, spirituelle peut être offerte aux proches du patient.— Cicely Saunders (1918-2005)   Les soins palliatifs ont pour mission d'améliorer la qualité de vie des patients atteints d'une maladie évolutive grave ou mettant en jeu le pronostic vital ou en phase avancée et terminale.  Les soins palliatifs ne sont pas le synonyme de « soins de fin de vie », bien que les soins terminaux soient des soins palliatifs. Ainsi, les patients bénéficiant de ces soins sont aussi ceux qui ont l’espérance de vivre encore plusieurs mois ou quelques années avec une qualité de vie acceptable malgré la présence d'une maladie inéluctablement évolutive.On dit souvent des soins palliatifs qu'ils sont « tout ce qu'il reste à faire, quand il n'y a plus rien à faire ». Autrement dit, pour un malade recevant des soins palliatifs, l'objectif n'est plus la guérison de sa maladie causale mais la lutte contre tous les symptômes inconfortables qui découlent de cette maladie, dont la douleur, la fatigue et l'anorexie. La démarche de soins palliatifs peut donc parfois envisager la prise d'un traitement médical ou la réalisation d'un acte chirurgical si ce traitement permet de soulager un symptôme inconfortable. Elle vise aussi à éviter les investigations et certains traitements déraisonnables s'ils ne peuvent faire espérer une amélioration de confort. Ce qui prime avant tout est le confort et la qualité de vie définie de manière personnalisée avec le patient. Les soins palliatifs cherchent à limiter les ruptures de prise en charge en veillant à la bonne coordination entre les différents acteurs du soin. Les aspects sociaux, et éventuellement religieux et spirituels, sont pris en compte.Les proches sont aussi accompagnés dans la compréhension de la maladie de leur proche et in fine dans leur cheminement de deuil.Le patient a le choix de recevoir ses soins où il souhaite à partir du moment où le médecin traitant lui assure une accessibilité complète aux soins palliatifs compte tenu de la gravité de sa maladie. Les soins sont effectués en hôpital ou à domicile, les services à domicile sont plus complexes à obtenir car il faut un accord supplémentaire du médecin traitant, ainsi qu’une équipe spécialisée disponible pour apporter les soins nécessairesIl s'agit de l'ensemble des valeurs portées par ce qu'on appelle « le mouvement des soins palliatifs » dont l'origine remonte aux pionnières anglo-saxonnes du « Saint Christopher Hospice » autour de Cicely Saunders.  Le docteur Maurice Abiven (1924-2007), spécialiste de médecine interne, fut l'un des pionniers de la pratique des soins palliatifs en France et Charles-Henri Rapin (1947-2008), médecin gériatre suisse, l'est dans le monde francophone de la gériatrie. Ce mouvement s'appuie sur des concepts éthiques faisant une large part à l'autonomie du malade, au refus de l'obstination déraisonnable ainsi qu'au refus de vouloir hâter la survenue de la mort. Michel Castra explique : « Cherchant à s'affranchir du cadre traditionnel d'une médecine techniciste et scientifique, les promoteurs des soins palliatifs sont parvenus à affirmer les principes d'une médecine privilégiant une logique de confort et ayant pour objectif de lutter contre les conséquences d'une maladie devenue incurable... (Il s'agit) d'une redéfinition des conceptions du « bien » pour le malade qui est ici à l'œuvre et qui marque un changement de légitimité de l'action médicale fondée non plus sur une rationalité strictement biomédicale mais sur de nouvelles croyances dans la finalité de l'intervention soignante auprès des patients terminaux : il s'agit notamment de privilégier la qualité de la vie qui reste à vivre sur la durée de cette vie... (Elle implique le) refus de l'euthanasie et de l'acharnement thérapeutique ». Certains partisans des soins palliatifs en tant que concept de prise en charge sont donc opposés à l'euthanasie définie comme l'administration de substances à doses mortelles dans le but de provoquer la mort dans un objectif compassionnel. Un des points importants défendu par le mouvement des soins palliatifs est la place à reconnaitre dans notre société à « celui qui meurt ». Pour le mouvement des soins palliatifs il est important de se rappeler que la mort est un phénomène naturel de la vie. Les soins palliatifs sont des soins actifs délivrés dans une approche globale de la personne atteinte d'une maladie grave évolutive ou terminale. Il faut également évoquer l'importance accordée à la prise en compte de la souffrance globale du patient : physique, sociale, psychologique, spirituelle et de son entourage. Dans une approche interdisciplinaire, une place particulière est accordée aux bénévoles d'accompagnement dans la démarche de soins dans le cadre des soins de support.L'Église catholique condamne l'euthanasie mais soutient les pratiques palliatives. La Congrégation pour la doctrine de la foi a rappelé l’obligation d’alimenter et d’hydrater les malades en état végétatif, dans un document rendu public le 14 septembre 2007. Le Vatican répond à deux questions posées par les évêques américains à la suite de l'affaire Schiavo en 2005 : il dit oui à l’administration de nourriture et d’eau, « moralement obligatoire » et non à la possibilité d’interrompre la nourriture et l’hydratation fournies par voies artificielles à un patient en état végétatif permanent.Pour ce qui est des sédatifs, il est « licite de supprimer la douleur au moyen de narcotiques, même avec pour effet d'amoindrir la conscience et d'abréger la vie » (affirmation de Pie XII rappelée dans l'encyclique Evangelium Vitae, 65).Il existe de nombreuses définitions « officielles » des soins palliatifs : définition de la loi française (juin 1999), définition de l'Organisation mondiale de la santé, autres (Ordre des médecins, ANAES). L'introduction des soins palliatifs en France a été plus tardive qu'en Grande-Bretagne et aux États-Unis, d'où est parti le « hospice movement » après la Seconde Guerre mondiale, avec Cicely Sanders, pionnière des soins palliatifs. Elle a été officiellement reconnue par la circulaire Laroque de 1986 « relative à l'organisation et à l'accompagnement des malades en phase terminale ». Depuis 1991, « ces soins font partie des missions de l'hôpital et leur accès est présenté comme un droit des malades » (Comité consultatif national d'éthique, avis no 63 ).  La loi du 9 juin 1999 et la circulaire DHOS/O2/DGS/SD5D du 19 février 2002 tracent le droit à l'accès aux soins palliatifs. Ces derniers sont  également inscrits dans le Code de déontologie médicale de 1995 (art. 37-38), qui rejette aussi l'acharnement thérapeutique. À la suite de la mission Jean Leonetti « sur l’accompagnement de la fin de vie » menée en 2002, la « loi relative aux droits des malades et à la fin de vie » de 2005 a été promulguée, tandis qu'un Observatoire national de la fin de vie était inauguré en février 2010 par la ministre Roselyne Bachelot.La circulaire de 2002 édicte les modalités d'organisation des soins palliatifs, souligne l'accès inégal de ceux-ci sur le territoire national. Les objectifs législatifs sont :le respect du choix du malade sur les conditions et le lieu de leur fin de vie ;l'adaptation et la diversification de l'offre territoriale de SP et l'articulation entre les différents dispositifs, structures et instances concernées ;la promotion du bénévolat et des soins de support.D'autre part :chaque département devra être doté d'un réseau de soins palliatifs.chaque établissement de santé se doit d'organiser les soins palliatifs dans son projet d'établissement, avec mise en place de formations de personnels, organisation de soutien des soignants, réflexion sur l'accueil et l'accompagnement des familles.La circulaire définit les notions d'unités de soins palliatifs, de lits « identifiés soins palliatifs », d'équipes mobiles de soins palliatifs.L'Agence régionale de l'hospitalisation (ARH) a été chargée de cette mise en œuvre. Depuis 2009, l'Agence régionale de santé (ARS) se substitue à l'ARH.Le 2 février 2016, le Code de la santé publique  intègre les modifications intervenues à la suite de la « petite loi » adoptée par l'Assemblée Nationale à partir de la proposition de loi créant de nouveaux droits en faveur des malades et des personnes en fin de vie déposée par les députés Leonetti et Claeys. Soins palliatifs et tarification à l'activité Le codage des soins palliatifs (SP) regroupe trois groupes homogènes de séjours (GHS) selon le lieu du séjour, respectivement dans un lit sans autorisation spéciale (GHS 7956), dans un lit « dédié » aux SP (GHS 7958), ou si le séjour a lieu dans une unité de SP (GHS 7957). La tarification est différente selon ces 3 cas. Le code DP (diagnostic principal) de Soins palliatifs est le code Z51.5.Les soins palliatifs peuvent, être pratiqués par toutes les équipes soignantes spécialisées dans l'accompagnement des malades en fin de vie, aussi bien au domicile qu'en milieu hospitalier. Cependant, il existe des situations complexes nécessitant l'intervention d'équipes de soins palliatifs (à caractère pluridisciplinaire). En France, on distingue habituellement : Les unités de soins palliatifs où se gèrent des situations de phases terminales complexes ne pouvant se dérouler au domicile ou en milieu hospitalier traditionnel en raison notamment de la survenue de syndromes réfractaires, c’est-à-dire résistants aux traitements habituels, altérant la qualité de vie restante du malade.Les équipes mobiles de soins palliatifs qui interviennent soit au sein des services d'un même hôpital, soit au sein de plusieurs établissements, soit à domicile, pour venir appuyer et conseiller les équipes référentes dans la prise en charge de patients atteints de maladies graves et potentiellement mortelles. Elles n'ont pas vocation à se substituer à l'équipe soignante.Les réseaux de maintien à domicile, sont chargés de coordonner l'action des soignants et des équipes mobiles prenant en charge un patient atteint d'une maladie grave et potentiellement mortelle.D'autres structures comme l' hospitalisation à domicile (HAD), les services d'hospitalisation à domicile, ou des lits identifiés pour la pratique des soins palliatifs au sein d'un service, complètent l'ensemble de ces structures spécialisées « en soins palliatifs ».Les services de médecine, de chirurgie ou de soins de suites et réadaptation (SSR), sans avoir le titre d'unités de soins palliatifs peuvent également assurer cette mission, d'autant que les besoins de la population sont bien supérieurs au nombre de lits disponibles ou d'unités de SP.La mise en place de soins palliatifs et d’accompagnement en maternité et en néonatalogie est en cours d’élaboration en France depuis les années 2000. Comme toute démarche de soins palliatifs, elle repose sur  un accompagnement pluridisciplinaire qui :accorde une grande place à l’écoute de la souffrance des parents face à la maladie de leur enfant à naître ou à celle de leur nouveau-né ;assure un suivi médical rapproché de la maman et du bébé pendant tout le temps de la grossesse ;permet l’élaboration d’un projet de vie pour le bébé. En accord avec les parents, le pédiatre définit les soins que pourra recevoir le bébé après sa naissance : soins de confort et soins proportionnés qui excluent tout acharnement thérapeutique et qui contribuent au bien-être du bébé ;privilégie la présence des parents auprès du bébé et de la fratrie pendant tout le temps de vie de l’enfant malade. Et si l’état du bébé le permet, son  retour au domicile familial  peut être envisagé en liaison avec le médecin traitant de la famille et en collaboration avec un réseau de soins palliatifs à domicile.Cette démarche de soins palliatifs et d’accompagnement est possible en cas de diagnostic prénatal d’une maladie létale du bébé à naître, dans le cadre d’une poursuite de la grossesse mais aussi dans les situations où le pronostic vital du  nouveau-né est engagé après sa naissance.La mise en place de soins palliatifs en service de pédiatrie est apparue dans les services d'oncologie pédiatrique au cours des années 1980-1990 avec le développement de la discipline de psycho-oncologie pédiatrique, puis s'est développée dans d'autres spécialités.Une étude de l'INSERM de février 2002 indique que les médecins français « restent peu formés » en matière de soins palliatifs et pointe les difficultés d'accès des patients à ce type de prise en charge. Ce sont ainsi 57 % des patients habitant en zone rurale, et 67 % des patients âgés de plus de 65 ans qui n'ont pas eu accès aux soins palliatifs, ces deux statistiques n'étant pas indépendantes. En octobre 2014, le Comité consultatif national d'éthique (CCNE) publie un rapport de synthèse soulignant le « non-respect du droit d'accéder à des soins palliatifs pour l'immense majorité des personnes en fin de vie ».Le nombre de structures dédiées bien qu'en progression constante reste encore insuffisant et  inégalement réparti sur le territoire.Les moyens employés par les médecins pour procurer des soins de fin de vie incluent la sédation profonde et continue.À cet égard, l’Académie nationale de médecine a déploré le glissement sémantique entre « fin de vie » et « arrêt de vie » autour de la question de la sédation, abordée par le texte publié par le Conseil national de l'Ordre des médecins et intitulé « Fin de vie, Assistance à mourir », le 14 février 2013.Les bénévoles font partie d'une association d'accompagnement de la maladie grave et de la fin de la vie. Ils sont recrutés, formés et encadrés par leur association. Une période de formation initiale leur permet de débuter des accompagnements avec l'appui d'un tuteur (bénévole expérimenté), soit dans un service clinique (oncologie, réanimation, médecine interne, gastro-entérologie, gériatrie), soit dans les soins palliatifs (unité de soins palliatifs, lits identifiés de soins palliatifs, équipe mobile de soins palliatifs, réseau de santé) ou à domicile (EHPAD, maison de retraite). Il est recommandé d'accomplir ce bénévolat au sein d'une équipe et sans exclusive d'accompagnement.Ce bénévolat a été inscrit dans la loi du 9 juin 1999 (loi no 9-477, article 10) visant à garantir le droit à l'accès aux soins palliatifs pour tout citoyen, puis dans le code de santé publique (article L1112-5). Le rôle du bénévole d'accompagnement se situe dans la présence (même silencieuse) et l'écoute empathique de la personne malade et de ses proches au nom d'une société dont il est toujours membre. Il n'interfère jamais dans les soins. Il connaît et applique la charte de la personne hospitalisée  notamment la confidentialité, le respect des convictions philosophiques et religieuses, l'intimité de la personne.Il participe chaque mois à un groupe de paroles de deux heures animé par une psychologue clinicienne. Cela lui permet de déposer ses ressentis, ses difficultés, ses limites et son impuissance, de visiter son système de défense, ses croyances, sans jugement d'autrui et tolérance des autres participants qui œuvrent à cette dynamique collective.  Cet exercice libère les nœuds, les énergies mais ne constitue pas une thérapie personnelle ou de groupe.Ce cheminement personnel du bénévole est sous tendu par une éthique de convictions et de responsabilité (Max Weber) dont les attributs sont : la dignité intrinsèque et la vulnérabilité de tout homme, sa dimension spirituelle, de solidarité enfin, ciment d'une société démocratique et fraternelle.Ressources relatives à la santé : (en) Medical Subject Headings (en) NCI Thesaurus Suzanne Philips-Nootens « La personne en fin de vie : le regard du droit civil au Québec » RDUS 2009-2010, vol. 40, p. 327[PDF].Michèle-H. Salamagne et Emmanuel Hirsch, Accompagner jusqu'au bout de la vie. Manifeste pour les soins palliatifs, Coll. Recherches morales, 2e édit., Les éditions du Cerf, Paris 1993, 145 p.Aline Cheynet de Beaupré, Vivre et laisser mourir. (D.2003.2980)Serge Paugam (sous la direction de) Repenser la solidarité. PUF, 2007Isabelle de Mézerac, Un enfant pour l'éternité, Éditions du Rocher, 2004 Au Canada Palli-Science : portail conçu pour le rehaussement des soins palliatifs En Belgique Soins palliatifs : portail wallon (Belgique francophone) En France HELEBORCentre national de ressources soin palliatifSociété française d'accompagnement et de soins palliatifsDossier documentaire de la Société Française de Santé Publique En Suisse Palliative.ch : société suisse de médecine et de soins palliatifs Portail de la médecine   Portail des soins infirmiers   Portail de la bioéthique   Portail de la psychologie"
médecine;"En médecine, un traitement, appelé aussi traitement médical, traitement thérapeutique, thérapie ou plus généralement thérapeutique, est un ensemble de mesures appliquées par un professionnel de la santé à une personne vis-à-vis d'une maladie, afin de l'aider à en guérir, de soulager ses symptômes, ou encore d'en prévenir l'apparition. Lorsque ce professionnel décide de ne pas médicaliser une situation qu'il juge ne pas relever de traitement, il s'agit d'abstention thérapeutique (non-initiation d'un traitement ou interruption du traitement appelée retrait ou congé thérapeutique). L'observance thérapeutique désigne la capacité d'un patient à suivre correctement le traitement qui lui a été prescrit.Concernant la santé animale, il s'agit des mesures appliquées par un vétérinaire.Le traitement peut être théoriquement classé selon le but global poursuivi pour l'individu :curatif, dont l'objectif est d'obtenir la guérison d'une personne malade (exemple : fracture d'un fémur d'origine traumatique chez un individu sain) ;palliatif, dont l'objectif est de soulager les manifestations d'une maladie (exemple : fracture d'un fémur d'origine pathologique chez un individu en fin de vie) ;préventif, dont l'objectif est de prévenir l'apparition d'une maladie (exemple : accident de la voie publique, ostéoporose et risque de fracture).En outre, on peut théoriquement classer un traitement selon son mode d'action principal par rapport à une maladie :étiologique, dont l'objet est la cause de la maladie (exemple : un antibiotique pour une angine bactérienne) ;symptomatique, dont l'objet est la manifestation d'une maladie (exemple : un antalgique pour la douleur liée à une angine).On peut classer un traitement, selon le type d'acte dispensé, en traitement médical, chirurgical ou médicotechnique.Le traitement médical fait intervenir un pharmacien, un médecin ou un infirmier, le plus souvent à l'aide de mesures hygiénodiététiques (conseil sur le mode de vie et l'alimentation, éducation thérapeutique) et de médicaments par voie injectable ou non.D'autres procédés font partie du traitement médical. La rééducation fait intervenir un kinésithérapeute, un orthophoniste ou un ergothérapeute. La psychothérapie fait intervenir un psychiatrie ou un psychologue. Le pansement fait intervenir un infirmier. Le massage et la physiothérapie font intervenir un kinésithérapeute. Il existe également l'hydrothérapie en rhumatologie, l'électroconvulsivothérapie et la luminothérapie en psychiatrie, et l'asticothérapie en dermatologie. Dans le cadre de soins d'urgence ou de réanimation, le traitement médical peut également concerner la pratique d'acte technique ""simple"" tel que le sondage des voies naturelles (urinaire, digestive, respiratoire) ou le massage cardiaque.Il faut également citer ici le traitement non conventionnel qui regroupe différentes pratiques ayant en commun le fait de ne pas avoir de base scientifique théorique ni de preuve scientifique d'efficacité.Le traitement chirurgical fait intervenir un chirurgien qui va pratiquer une incision.Plusieurs traitements sont à la frontière de la chirurgie. Le traitement radio-interventionnel fait intervenir un radiologue. Le traitement endoscopique fait intervenir un médecin endoscopiste. La radiothérapie fait intervenir un radiothérapeute. La photothérapie fait intervenir un médecin spécialiste.On peut classer le traitement selon la méthode employée :entretien oral : mesures hygiénodiététiques, psychothérapie ;molécule chimique : traitement médicamenteux, pharmacothérapie ;intervention mécanique : chirurgie, endoscopie, radiologie interventionnelle, oncologie physique, massage ;rayonnement ionisant : radiothérapie ;rayonnement électromagnétique non ionisant : photothérapie, luminothérapie ;onde ultrasonore : ultrasonothérapie ;modification de température : thermothérapie, cryothérapie ;électricité : électrothérapie, électroconvulsivothérapie ;eau : hydrothérapie ;animal : asticothérapie.De nombreuses autres méthodes sont utilisées en médecine non conventionnelle.Parfois, la surveillance, qui peut être clinique (exemple : pression artérielle), biologique (exemple : protéine C réactive) ou radiologique (exemple : radiographie du thorax), est considérée comme partie intégrante du traitement, en particulier pour la surveillance clinique.En orthopédie, on différencie le traitement chirurgical (mise en place de matériel rigide dans le corps) ; le traitement orthopédique (mise en place d'un plâtre) et le traitement fonctionnel (mise en place d'une immobilisation relative non plâtrée : attelle ou bandage).En psychologie, la psychothérapie se décline en différents types. On retrouve par exemple les thérapies cognitivo-comportementales, psychanalytiques ou familiales.ChimiothérapieImmunothérapieÉchec thérapeutiqueProphylaxie""Chapitre neuf du livre de médecine dédié à Mansur, accompagné des commentaires de Sillanus de Nigris"" est un livre latin par Rhazès, de 1483, qui est connu pour son chapitre 9, qui est d'environ thérapeutiques Portail de la médecine   Portail de la psychologie"
économie;"La production est l'action d'un sujet qui transforme une matière première  pour faire exister un nouvel objet. On rencontre ce phénomène de production dans la société, mais aussi bien dans la nature. C'est pourquoi on peut l'étudier soit sous l'angle économique et sociologique, soit sous l'angle biologique.Le terme « production » dérive du latin classique qui signifie « prolonger, mettre en avant ». Dans l'Antiquité, il désigne aussi bien les créations de la nature (l'arbre producteur de fruits) que celles de l'homme (l'artisan producteur d'objets utiles). Ce n'est qu'au début de l'ère industrielle qu'il entre dans le discours économique.Selon John Stuart Mill, « l'économie décrit les lois des phénomènes de société qui se produisent du fait des opérations conjointes de l'humanité pour la production de richesses ». L'économie est donc la discipline scientifique qui étudie la production comme élément fondamental, mais aussi l'échange, la distribution et la consommation des biens et des services. C'est ainsi qu'on étudie la production selon les méthodes, les lieux et les marchés. On compare la production d'un même produit à partir de modèles différents d'organisation. On calcule le volume de production par pays et par époques. On sépare l'analyse par secteurs économiques. On distingue la production marchande de la production non marchande. La production marchande est celle qui est réalisée et vendue essentiellement par les entreprises sur le marché des biens de consommation achetés par les ménages ou sur celui des biens de production achetés par les entreprises. La période de référence est généralement l'année. Elle est différente de la production annuelle. Grâce à la variation des stocks (stockage lorsque la production annuelle n'est pas totalement vendue ou déstockage dans le cas où celle-ci est insuffisante), la production permet de répondre au besoin annuel du marché national. Cette production est celle réalisée sur le territoire national (par des entreprises nationales ou étrangères) et ne tient donc pas compte de la production réalisée par des entreprises nationales dans le reste du monde. Par contre, la production est dite non marchande lorsque le prix payé par l'utilisateur est inférieur à la moitié de son coût de production. La production non marchande est réalisée essentiellement par l'État et accessoirement par les administrations privées (syndicats et partis politiques, par exemple) et les ménages. Les services concernés sont, essentiellement, de défense nationale, de sécurité, de justice, religieux et de spectacle public. La production non marchande sous forme d'autoconsommation des ménages n'est pas prise en compte par la comptabilité nationale car elle n'est pas justifiée par des documents (factures ou bulletins de paie, par exemple) justifiant son existence. Par contre, les travaux domestiques (services de jardinage ou d'éducation des enfants) sont comptabilisés lorsque le paiement des domestiques est prouvé par des pièces justificatives et non effectué uniquement gratuitement ou par remise d'une somme d'argent de la main à la main.La première approche économique de la production fut celle des physiocrates au XVIIIe siècle, qui considéraient que seule l'agriculture était vraiment productrice puisque le végétal apporte plus de graines qu'il n'en consomme, les autres activités ne faisant que transformer les produits de la terre. Au siècle suivant, David Ricardo va mettre l'accent sur la théorie de la valeur fondée sur le travail, approfondissant la distinction entre valeur d'usage et valeur d'échange. Henry Charles Carey est un célèbre économiste américain qui s'est opposé à Ricardo et au libre-échange en faisant l'éloge du capitalisme protectionniste et interventionniste américain,.Aujourd'hui, la production est l'activité socialement organisée exercée par une unité institutionnelle qui combine des facteurs de production (facteur travail et facteur capital) afin de transformer les consommations intermédiaires en biens ou en services s'échangeant sur le marché.Depuis les travaux de Colin Clark, on regroupe les activités économiques de production  selon trois grands secteurs :le secteur primaire : l'ensemble des activités qui exploitent les ressources naturelles : agriculture, mines, pêche...le secteur secondaire : toutes les activités de transformation d'une matière première : industries manufacturières, construction...le secteur tertiaire : principalement marchand : commerce, transports, hébergement-restauration... ; ou non-marchand : administration publique, enseignement...Selon une enquête de 2016, en France, le secteur primaire représente 2,8 % des 26 millions de personnes possédant un emploi (au sens du Bureau international du travail) ; le secteur secondaire 20,6 % et le secteur tertiaire 75,7 %. La France est le pays européen où le poids du tertiaire est le plus élevé.Selon l'INSEE, l'industrie regroupe « les activités économiques qui combinent des facteurs de production (installation, approvisionnement, travail, savoir) pour produire des biens matériels destinés au marché. » En France, l'industrie représente 12,4 % du PIB (20,3 % en Allemagne, 8,7 % au Royaume-Uni). La part de l'industrie manufacturière dans l'économie française a diminué de moitié depuis 1970 (5,7 millions de salariés contre 2,7 millions aujourd'hui).On distingue la production marchande de la production tout court.La production marchande peut se subdiviser en deux catégories :la production marchande simple où le producteur vend son produit sur le marché ou rend un service marchand à titre individuel ;la production marchande capitaliste où le produit ou le service créé par des salariés est propriété du capitaliste. Il est ensuite vendu en tant que marchandise dans le but de réaliser un bénéfice.La production non-marchande se définit comme la production de biens ou services proposés gratuitement ou à un prix inférieur au coût de production, par des organisations publiques, ou des associations.La production réelle d'une entreprise ne correspond pas normalement à sa production vendue. Celle-ci comprend en effet, en plus de la production propre de l'entreprise, celle issue d'autres entreprises, qui correspond aux matières premières et aux autres produits achetés (appelés consommations intermédiaires) pour fabriquer le produit vendu. La production réelle de l'entreprise, appelée « valeur ajoutée », est donc sa production vendue, de laquelle il faut retrancher les consommations intermédiaires.Lorsque la production n'est pas vendue sur le marché (l'essentiel de la production des administrations), sa valeur correspond, par définition, à son coût de production. Comme dans le cas de la production marchande, la valeur ajoutée des administrations est obtenue après avoir retranché les consommations intermédiaires de la production.Différentes organisations permettent de produire un bien ou un service. Certaines sont des espaces où sont concentrés les moyens de production et les ressources humaines pour produire à grande échelle, en grande quantité et d'une manière répétitive avec une division des tâches poussée. D'autres sont des structures plus éclatées et plus mobiles comme l'entreprise en réseau, (l'entreprise étendue) mise en place dans le cadre de l'économie post-industrielle.Trois grands modes d’organisation de la production peuvent être observés : organisation de type « série unitaire »,  les industries process, la production manufacturière.La sociologie économique considère que la production est une activité de création, de rencontre, d'échange et de partage de nombreux éléments tels que le temps, l'espace, les biens, les idées et les émotions.Les économistes ont modélisé la production en identifiant les éléments qui contribuent à sa réalisation, à savoir les facteurs de production. L'un des facteurs de production est constitué par le travail, ce qui représente la dimension sociale de la production du point de vue des théories économiques.Depuis les années 1970 environ, où sont apparus et se sont développés les mouvements écologistes, on se rend compte que la production, surtout industrielle, est grosse consommatrice de ressources naturelles, ce qui pose le problème de la rareté ou de l'épuisement de ces ressources, et qu'elle peut engendrer d'importantes pollutions. C'est pourquoi est apparue la notion de développement durable, qui combine deux aspects : ne pas abuser des ressources naturelles ; régler la production pour qu'elle ne détruise ni ne pollue l'environnement.Du point de vue biologique, tous les êtres vivants, végétaux comme animaux, sont des producteurs : ils produisent de la matière vivante en prélevant des éléments dans leur milieu de vie. L'animal comme le végétal produit sa propre matière à partir des aliments qu'il consomme. On distingue deux types de producteurs :les producteurs primaires : ce sont les végétaux verts qui contiennent de la chlorophylle grâce à laquelle en présence de lumière et uniquement à partir de matières minérales, ils fabriquent de la matière organique carbonée ;les producteurs secondaires : ce sont tous les autres êtres vivants qui fabriquent leurs substances organiques à partir de la matière d'un autre être vivant végétal ou animal.L'histoire de la production est marquée par deux grandes ruptures. La première est la Révolution  néolithique caractérisée par la transition de tribus de chasseurs-cueilleurs vers des communautés  d'agriculteurs. La première émergence eut lieu au Proche-Orient, il y a 5000 ans environ, où les hommes passèrent graduellement de la cueillette de céréales sauvages, à la production de plantes et d'animaux domestiqués. Les hommes ne se contentent plus de prendre ce que la nature leur offre, ils modifient radicalement leur environnement par des techniques agricoles nouvelles pour obtenir d'importants surplus de production. Une société sédentaire remplace progressivement les groupes nomades.La seconde rupture majeure, à partir du XVIIe siècle, est la Révolution industrielle qui transforme une société à dominante agraire et artisanale en une société commerciale et industrielle. Le caractère dominant de cette mutation est le passage de l'outil (prolongement de la force musculaire de l'ouvrier) à la Machine (dispositif autonome mû par une énergie naturelle), ce qui permet la mise en place de la production en série, c'est-à-dire d'une production de masse, production d'objets tous identiques à  grande échelle.Dans le cadre du capitalisme, la production est généralement conçue comme l'activité destinée à satisfaire non plus les besoins du producteur (autoconsommation), mais à être vendue sur le marché. Cette dernière est appelée « production marchande ». De plus, la vente n'est pas effectuée pour satisfaire les besoins jugés nécessaires ou urgents. Ceux-ci doivent être armés d'un pouvoir d'achat ; autrement dit, la production est destinée aux consommateurs qui sont capables de payer.Dans le second Discours, Jean-Jacques Rousseau cherche à cerner l'origine de la civilisation, qui est aussi selon lui l'origine du malheur de l'homme. Il affirme : « La métallurgie et l'agriculture furent les deux arts dont l'invention produisit cette grande révolution ». Au XIXe siècle les archéologues et les historiens ont parlé de ""révolution néolithique"" pour caractériser ""la période de la préhistoire marquée par l'émergence des premières sociétés agricoles sédentaires (...) qui ont éliminé, en quelques millénaires les sociétés de chasseurs-cueilleurs"", et qui ont installé ""une économie de la production"".C'est aussi au XIXe siècle que Karl Marx a élaboré une philosophie qui donne une grande importance à la production :d'une part, il en fait la base de la compréhension de l'homme : les hommes ""commencent à se distinguer des animaux dès qu'ils commencent à produire leurs moyens d'existence, pas en avant qui est la conséquence même de leur organisation corporelle"". La production n'est donc pas seulement une action économique; elle a un sens plus profond car elle est ""la façon dont les individus manifestent leur vie"". ""Ce qu'ils sont coïncide donc avec leur production"" (ibid.). À la question philosophique : ""qu'est-ce que l'homme ?"", Marx répond donc : ""l'homme c'est le monde de l'homme, l'État, la société"". Il faut donc dire que l'homme se produit lui-même dans l'histoire : ""par son activité historique, l'homme se donne une valeur humaine, il produit ses propriétés d'homme"", il se met en valeur. Marx écrivait : « Tout ce qu'on appelle l'histoire universelle n'est rien d'autre que l'engendrement de l'homme par le travail humain. »d'autre part, l'étude de la production fournit la base scientifique qui permet de comprendre la structure et l'évolution des sociétés humaines. C'est la théorie du matérialisme historique selon laquelle les rapports de production (relations entre les classes sociales) sont liés  aux forces productives (techniques, outillage et machines): ""les rapports sociaux sont intimement liés aux forces de production. En acquérant de nouvelles forces productives, les hommes changent leur mode de production et en changeant leur mode de production,ils changent la manière de gagner leur vie, ils changent tous leurs rapports sociaux"". Dans un fameux raccourci, il écrit : « Prenez le moulin à bras et vous aurez la société féodale avec le suzerain; prenez le moulin à vapeur et vous aurez la société avec le capitaliste industriel». Les rapports de production  sont d'abord en accord avec l'état de développement des forces productives, mais l'évolution de ces dernières finit par créer le besoin de nouveaux rapports de production, ""alors commence une ère de révolution sociale"" qui se conclut par l'apparition d'un nouveau mode de production et donc d'un nouveau type de société. L'histoire de l'humanité se définit par celle des modes de production. Il distingue les suivants : asiatique, antique, féodal et capitaliste auxquels devrait succéder le mode de production communiste débarrassé de la lutte entre les classes sociales qui a caractérisé les précédents.D'une manière plus générale, Michel Henry crédite Marx d'avoir pensé ""l'activité productive des hommes"" comme une praxis: ""C'est dans la pratique qu'il faut que l'homme prouve la vérité"". Selon Adolfo Sanchez-Vasquez, le concept de praxis signifie : ""activité orientée vers la transformation d'un objet (nature ou société) en tant que fin tracée par la subjectivité consciente et agissante des hommes et, par conséquent, activité objective et subjective à la fois"", et en ce sens, il s'oppose à toutes les philosophies précédentes, car comme le dit la XIe thèse des thèses sur Feuerbach : « Les philosophes n'ont fait qu'interpréter le monde de diverses manières, il s'agit maintenant de le transformer ».Biens et services marchandsCapital productifConsommationÉconomie post-industrielleEntrepriseEntreprise étendueFacteur de productionMatérialisme historiqueMode de productionProduction audiovisuelleProductivismeProductivité Portail de l’économie   Portail des entreprises   Portail de la philosophie   Portail de la sociologie   Portail de la production industrielle"
économie;"Un agent économique est, en économie, une personne physique ou morale prenant des décisions qui participent à l'activité économique. Il est l'actant économique principal des modèles économiques. Le périmètre pertinent de définition de l'agent économique dépend des conceptions de l'économie : les courants de pensée économiques les définissent de manière différentes, ainsi que la comptabilité nationale. La question de la définition de l'agent économique est au centre des controverses économiques du XVIIIe siècle. Au sein de l'école physiocrate, François Quesnay crée un système de pensée où il définit l'économie nationale comme peuplée de trois agents économiques : les fermiers, les propriétaires fonciers et les artisans. Il qualifie cette dernière de « classe stérile ». Cette conception est plus tard critiquée par Adam Smith, qui considère les travailleurs et les commerçants comme les agents économiques majeurs.Le marxisme se fonde lui aussi à partir d'une remise en question de la définition des agents économiques. Karl Marx propose une analyse de l'économie avec une bipartition sociale, entre les capitalistes d'un côté, et les travailleurs de l'autre, représentatifs de la bourgeoisie et du prolétariat. Néanmoins, au sein de la classe des capitalistes, Marx distingue deux catégories d'agents économiques : ceux qui produisent des biens de production et ceux qui produisent des biens de consommation. Pour l'école du circuit, les agents économiques doivent être définis de manière proche de la définition de la comptabilité nationale, c'est-à-dire en étant regroupés en pôles fonctionnels. Cette école considère ainsi que les grands agents économiques sont les institutions financières (et notamment les banques), les entreprises, les ménages et les administrations publiques. Chez les keynésiens, chaque agent économique est classé par référence à sa fonction principale dans l'économie. Un entrepreneur appartement à l'agent "" entreprises "" en raison de sa fonction de base bien qu'il peut effectuer des opérations de consommation courante, à titre secondaire, qui concerne l'agent "" ménages "". Une banque effectue, à titre principal des opérations financières est classée donc dans l'agent "" banques "" bien qu'elle peut effectuer des opérations, à titre secondaire, relevant des agents "" ménages "" ou "" entreprises "". On peut faire un raisonnement analogue pour démontrer l'appartenance des individus à l'agent "" ménages "".La macroéconomie considère que l'agent économique pertinent peut être, ou bien un agent représentatif (le ménage moyen ou médian), ou bien une agrégation d'agents économiques, ou bien, dans le cadre d'une étude sectorielle, un groupe homogène. Dans tous les cas, les agents économiques sont agents car ils agissent dans le cadre d'échanges économiques avec d'autres agents,.L'objectif de l'étude menée par l'économiste oriente son choix dans la définition des agents économiques qu'il souhaite étudier.La microéconomie s'intéresse aux décisions prises par les agents économiques. Chaque agent possède des caractéristiques particulières qui permettent aux économistes de prévoir ses décisions. Plutôt qu'être un simple représentant de sa classe ou de son groupe d'appartenance, l'agent microéconomique arbitre entre les choix possibles, pour maximiser son utilité. L'hypothèse qui sous-tend cette conception est celle de la rationalité des agents, qui sont censés effectuer des choix optimaux en s'appuyant sur un calcul coût-avantage.Les néoclassiques considèrent qu'il faut s'intéresser à deux types d'agents économiques : le consommateur et le producteur. Le consommateur offre son travail en échange d'un salaire, qu'il va consommer sur le marché des biens et services. Le producteur achète la force de travail et les capitaux nécessaire à la production, et l'écoule ensuite sur le marché des biens et services. Comme le note la Direction générale du Trésor dans une note longue de 2021, « les modèles traitent les ménages et les entreprises de manière quasi-symétrique malgré leur profonde différence de nature, en les rassemblant dans le concept d'agents économiques ».Les organes de comptabilité nationale regroupent les agents économiques selon leurs fonctions, à l'instar de l'école du circuit. Les catégories les plus simples sont les ménages, les entreprises et le gouvernement. La fonction principale des entreprises et du gouvernement est de produire des biens et services, celle des ménages est de consommer.En France, l'INSEE catégorise les agents économiques en six catégories, aussi appelées unités institutionnelles. Chaque catégorie inclut des sous-divisions.Les modèles économiques théoriques sont le plus souvent basés sur des hypothèses comportementales homogènes, afin de décrire les ensembles économiques de la manière la plus simple. Ainsi les ménages consomment, les entreprises produisent.La théorie économique gagnant en complexité, les économistes affinent leur segmentation des agents économiques pour les différencier selon certains critères comme le revenu, le patrimoine ou l'âge.Agent (fonction publique)Unité institutionnelleÉconomie des institutions[PDF] http://www.newschooljournal.com/files/NSER01/82-94.pdf Duncan Foley, The strange history of the economic agent, 2002Les agents économiques et leurs opérations Portail de l’économie"
économie;Un bien de consommation est un produit fabriqué destiné au consommateur final. En économie, on le distingue d’un bien de production. Un service ne peut pas être considéré comme un bien.  Biens et services qui se consomment en une seule fois (pain, électricité du logement…)Biens semi-durables : ils durent quelque temps mais s'usent assez facilementBiens durables au plein sens du terme que l'on peut utiliser durant de nombreuses années (réfrigérateur, automobile…)Équipements (machines, moyens de transport…)Produits semi-finisMatières premièresÉnergie (électricité, pétrole…)Services rendus par une entreprise à une autre entrepriseBiens et services marchandsBien intermédiaireConsommation« Industrie des biens de consommation / Biens de consommation », sur le site de l'Insee Portail de l’économie
économie;"Un bien durable est un bien qui ne s'use pas rapidement ou, plus spécifiquement, dont l'utilité se maintient dans le temps au lieu d'être complètement consommé en une utilisation. Un objet comme la brique pourrait être considéré comme parfaitement durable, car en pratique on suppose qu'elle ne s'use pas. Les biens hautement durables comme les réfrigérateurs, les voitures ou les téléphones mobiles continuent à être utiles pendant un usage de trois ans ou plus, c'est pourquoi les biens durables sont habituellement caractérisés par de longues périodes entre des achats successifs.Les biens de consommation durables incluent les voitures, l'équipement de la maison (ameublement, électroménager, électronique grand public...), l'équipement sportif et les jouets.Les biens non durables (consommables) sont l'opposé des biens durables. Ils peuvent être définis comme des biens qui sont immédiatement consommés en une utilisation ou qui ont une durée de vie de moins de 3 ans.Les exemples de biens non durables incluent les biens de grande consommation tels que les cosmétiques, les produits de nettoyage, la nourriture, le carburant, les boissons alcoolisées, les cigarettes, les médicaments et les fournitures de bureau.Définir ce qu'est un bien durable est difficile, car la notion de développement durable est complexe et encore relativement nouvelle. Tous les domaines liés à l'économie n'ont pas encore pleinement intégré ces notions parfois floues.Une définition de bon sens serait donc un bien dont la conception, la production, l'utilisation, et la fin de vie respectent les principes fondamentaux du développement durable.Les biens durables sont généralement des biens d’équipement du foyer, de la personne, mais également des biens telle qu’une voiture.Parmi les domaines qui n'ont pas encore intégré complètement les notions de développement durable, on peut citer :le droit, qui n'en est qu'au stade de la réflexion sur ce qu'est la responsabilité envers les générations futures, en raison de la difficulté à définir sur un plan juridique ce que sont les besoins des générations futures ;la comptabilité nationale, qui connaît, pour le calcul de la formation brute de capital fixe (l'investissement), la notion d'actifs fixes, utilisables pendant plus d'un an dans un processus de production, parfois confondus par erreur avec des biens durables.(en) Cet article est partiellement ou en totalité issu de l’article de Wikipédia en anglais intitulé « Durable good » (voir la liste des auteurs).Développement durableMicroéconomiePrincipe de destination universelle des biensUtilisation durable Portail de l’économie   Portail de l’environnement"
économie;
"startup""";
économie;"La gestion des ressources humaines ou GRH (anciennement gestion du personnel ; parfois appelée gestion du capital humain) est l'ensemble des pratiques mises en œuvre pour administrer, mobiliser et développer les ressources humaines impliquées dans l'activité d'une organisation.Ces ressources humaines sont l'ensemble des salariés de tous statuts (ouvriers, employés, cadres) faisant partie de l'organisation, mais aussi – et de plus en plus – liés à elle par des rapports de sujétion (ainsi, les prestataires extérieurs, ou sous-traitants, sont considérés comme faisant partie de fait du périmètre des ressources humaines de l'entreprise).Dans un premier temps, cette fonction est entendue dans une perspective opérationnelle. Il s'agit d'administrer un personnel qui peut être numériquement important et réparti en différents niveaux de hiérarchie ou de qualification : (gestion de la paie, droit du travail, contrat de travail, etc.).Dans un second temps, la fonction acquiert une dimension plus fonctionnelle[pas clair]. Il s'agit d'améliorer la communication transversale entre services et processus, et de mettre en œuvre un développement des salariés à l'intérieur de l'entreprise (gestion des carrières, gestion prévisionnelle des emplois et des compétences ou (GPEC), recrutement (sélection), formation, etc.).La gestion des ressources humaines intervient à tous les stades de la vie des salariés dans l'entreprise, dont leur entrée et leur départ. Elle se décline ainsi en de multiples tâches : définition des postes, recrutement, gestion des carrières, formation, gestion de la paie et des rémunérations, évaluation des performances, gestion des conflits, relations sociales et syndicales, motivation et l'implication du personnel, communication, les conditions de travail, sélection, et équité (justice distributive, interactive, etc.).Afin de valoriser les compétences, la motivation, l'information et l'organisation, il est possible de donner toute l'attention nécessaire à certains outils de gestion :le recrutement. En évaluant les compétences et la motivation lors du recrutement, on s'assure d'avoir un personnel adéquat en nombre et en qualification ;la formation et le coaching. Afin d'améliorer le niveau de compétence des salariés, mais aussi pour améliorer leur motivation ;la motivation positive (récompense : félicitation, prime, promotion, formation…) et négative (sanction : réprimandes, réduction ou suppression d'une prime, rétrogradation, voire licenciement).La motivation positive et la motivation négative ont chacune leur efficacité. La sanction peut être démotivante pour l'intéressé. Mais il faut relativiser cette crainte car elle fait appel au principe de responsabilité et d'exemplarité. Elle renvoie aussi l'individu au groupe. Ce dernier peut mal vivre des comportements non sanctionnés quand ils sont hors jeu. C’est peut être un facteur de démotivation quand une absence de sanction traduit de fait un déséquilibre entre celui qui se dévoue et celui qui ne fait rien. Le souci d'équité doit guider l'administrateur. De ce point de vue, la gestion des ressources humaines doit intégrer aussi dans sa pratique administrative, la notion de groupe ou d'équipe : par la communication et la transparence. Il est essentiel que le salarié ait les informations nécessaires à l'accomplissement de sa tâche, et ait une idée précise de l'évolution et des objectifs de l'entreprise elle-même, et de son environnement. De nos jours, l'abondance d'informations a rendu nécessaire la mise en place de systèmes de gestion de l'information, comme les systèmes de gestion des connaissances ; par la planification et le contrôle de l'avancement des tâches. L'optimisation de l'organisation, c'est-à-dire l'ordonnancement des tâches et leur affectation aux personnes les plus compétentes disponibles, permet d'améliorer l'efficacité d'exécution ; par l'administration du personnel. Il est coutumier de dire qu'une bonne gestion des ressources humaines se traduit en premier lieu par une administration fiable du personnel. En l'occurrence, il s'agit de sécuriser son effectif en assurant un paiement rigoureux des salaires et des primes, en suivant la gestion des présences et des absences, des heures supplémentaires, en planifiant les congés, en organisant les remplacements, etc.Ce point est essentiel, car il caractérise une part des obligations contractuelles (statutaires pour un fonctionnaire) d'une entreprise (d'un service public) envers son salarié. Lorsque l'entreprise traverse une crise, le rôle des ressources humaines est primordial. Une crise, même financière, naît souvent d'une erreur humaine[réf. nécessaire]. C'est le devoir des responsables des ressources humaines de mettre en place un projet de redressement et ceci passe par la nomination et le suivi d'une équipe d'intervention efficace. De l'identification à la sortie de crise, la gestion des ressources humaines est la véritable clé dont l'avenir de la structure peut dépendre.Un enjeu de la gestion des ressources humaines est la gestion des coûts, par exemple ceux liés à la rotation du personnel ou à l'absentéisme.La notion d'entreprise vue comme étant un « corps social » est intronisée et développée au début du XXe siècle, entre autres par des gestionnaires praticiens comme en France Henri Fayol. Dans cette perspective, la gestion des ressources humaines correspond à une véritable fonction de l'entreprise.Les directions des ressources humaines assurent leurs missions et fonctions en collaboration avec les autres directions et les responsables de terrain dans une logique d'objectifs fixés par l'entreprise, l'association ou l'administration. C'est ainsi que la gestion des ressources humaines est considérée - dans certaines organisations - comme coresponsable de domaines comme la production ou la gestion de la qualité.Il est possible d'identifier de nombreuses tâches pour cette fonction qui sont :l’administration du personnel (c’est sous cet aspect que la fonction commence à exister et à être perçue dans l’entreprise) :l’enregistrement, le suivi et le contrôle des données individuelles, et collectives du personnel de l’entreprise ;l’application des dispositions légales et réglementaires dans l’entreprise ;la préparation des commissions et des réunions ;le maintien de l’ordre et du contrôle et les travaux de pointage.la gestion au sens large (cette expression recouvre trois domaines) :l’acquisition des ressources humaines : par la gestion de l’emploi, programmes de recrutement, plans de carrières, mutations et promotion, analyse des postes et l’évaluation des personnes ;la gestion des rémunérations : par l’analyse et l’évolution des postes, grille de salaires, politique de rémunération, intéressement et participation ;la gestion de la formation : par la détection des besoins, l’élaboration des plans de formation, la mise en œuvre des actions de formation et l’évaluation des résultats.la communication, l’information : Les tâches de la direction des ressources humaines en cette matière sont :La définition des publications orientées vers l'extérieur et la conception des messages,La conception du bilan social de l’entreprise (s'avérant être une obligation annuelle pour les organisations ayant plus de 300 salariés),La gestion des moyens de communication : journal d’entreprise, affichage, audio-visuel, réunions systématiques ;l’amélioration des conditions de travail. En cette matière les principaux thèmes sont :l’hygiène et la sécurité au travail et dans les trajets,l'ergonomie des conditions de travail,la prévention des risques psychosociaux et des maladies professionnelles.la qualité de vie au travail.La GRH nécessite la mobilisation de connaissances et expertises variées : gestion, économie, droit, sociologie, psychologie…Il est possible de distinguer les approches théoriques suivantes :L'approche la plus fréquemment rencontrée réside dans l'approche de la gestion des ressources humaines au fur et à mesure des grandes phases du cycle de vie du contrat de travail. Ceci permet d'aborder la relation de l'organisation avec son salarié du recrutement à son départ de l'entreprise (retraite, licenciement, démission…). Elle doit nécessairement être complétée par une vision collective au travers de processus que sont les relations sociales et syndicales, les systèmes d'information, le contrôle de gestion sociale…Une autre approche reprise dans l'ouvrage Manager RH retient pour les ressources humaines quatre missions essentielles qui sont :Construire l’organisation : ce que l’on appelle le « marché du travail » sur lequel se déterminent les salaires ne ressemble pas à un marché boursier. Son fonctionnement est, en partie, « interne » à l’entreprise et dépend des procédures et de l’architecture (division verticale et horizontale du travail) construites par le responsable RH ;Mobiliser l’organisation : il ne suffit pas que les salariés possèdent les compétences requises. Encore faut-il qu’ils veuillent les utiliser. Cette volonté sera en fonction de ce que leur offrira l’entreprise : une rémunération (globale), des conditions de travail, des perspectives d’évolution, autant d’aspects qu’il appartient au responsable RH de mettre en forme ;Doter l’organisation des compétences requises : les compétences d’aujourd’hui seront ainsi obsolètes demain. Le recrutement, la formation, la gestion prévisionnelle des emplois et des compétences sont autant de moyens utilisables pour réaliser la transformation nécessaire des qualifications ;Réguler l’organisation : les dysfonctionnements constituent le mode normal de fonctionnement des organisations que le responsable RH doit cependant maîtriser pour éviter que leur expression ne menace la survie de l’entreprise. Il doit aussi en contrôler les effets externes sur le système social, c’est-à-dire assumer ce qu’on considère être la « responsabilité sociale » de l’entreprise.Les ressources humaines auraient quatre missions essentielles d'après l'ouvrage Human Resource Champions :être le partenaire de la stratégie de l'entreprise au quotidien (le DRH en tant que business partner) ;gérer et accompagner le changement (le DRH « maître d'œuvre » des politiques de formation, de développement des compétences) ;administrer le quotidien (le DRH « gestionnaire » : payer, administrer, répondre aux obligations légales, etc.) ;assister les salariés (le DRH « coach »).L’évaluation de la gestion des ressources humaines est un processus crucial dans l’évaluation du plan d’action d’une organisation. Elle peut se faire à partir de critères établis ou bien de résultats enregistrés après une mise en œuvre de stratégies de ressources humaines dans une organisation. L'évaluation de la gestion permet une révision complète des politiques du capital humain au sein d’une organisation et un ajustement de son plan d’action.Il est important et souvent très nécessaire d’évaluer méthodiquement les politiques ainsi que les pratiques de gestion des ressources humaines. Pour obtenir le succès prévu, il serait impératif de faire une bonne évaluation qui permet une amélioration constante. Cette étape est en quelque sorte une évaluation qui détermine la performance organisationnelle. Ainsi, elle peut soit être forte ou faible. Dans le cas où cette dernière se trouve à être faible, il faut ressortir tous les points négatifs de la fonction des ressources humaines et par la suite déterminer la source de ces problèmes. Ils peuvent être perçus dans la mise en œuvre des politiques de gestion ou dans le plan lui-même. Si le problème provient de l’application des politiques de ressources humaine, les gestionnaires auront tendance à s’opposer aux changements dans leurs plans stratégiques. De plus, les employés peuvent aussi avoir cette attitude envers les changements soudains, car ceci aura tendance à leur donner des nouvelles responsabilités. Pour éviter les conflits, il est important d’avoir des évaluations régulières afin d’appliquer les changements d’une façon constante, car une application soudaine des changements cause des conflits. « L’un des obstacles majeurs à franchir, autant pour les responsables que pour la Direction RH, est l’indifférence des systèmes actuels RH quant aux missions et projets de plus en plus transversaux. Ainsi, la participation des techniciens d’un laboratoire à une mission transversale va dégrader leur ratio de productivité au sein de leur unité de production. Et il en sera de même pour tous les autres membres des équipes transversales, qu’ils soient des services marketing, juridique, informatique, recherche… ».Des cas de pratiques de forced ranking, ou sous-notation forcée, dans certaines entreprises sont progressivement dévoilés. Il s'agit de sous-évaluer un salarié pour remplir des quotas de mauvais salariés et pouvoir les licencier pour insuffisance professionnelle, tels les cas rapportés chez Sanofi ou dans le secteur de l'automobile.Le but d’un entretien d'évaluation et de développement est d’identifier les écarts entre les compétences dont dispose un salarié et les exigences du poste qu’il occupe (telles que définies par son cahier des charges), afin de déterminer les objectifs de développement prioritaires. Cette évaluation peut avoir lieu dans le cadre de l’entretien annuel d’évaluation, ou faire l’objet d’un entretien spécifique.Afin de faire une évaluation adéquate de la gestion des ressources humaines, il suffit de faire la comparaison entre les objectifs fixés et les résultats finaux à l’aide des critères d’évaluation et de correction. Ces critères doivent refléter les résultats escomptés, dont il s’agit de mesurer la pertinence des actions entreprises pour atteindre les objectifs fixés en tenant compte des divers partenaires de l’organisation. Enfin le résultat des évaluations doit apporter des mesures correctives qui vont améliorer et repositionner les politiques de gestions des ressources humaines d’une organisation afin qu’elle soit performante dans son environnement interne et externe.L’évaluation de la performance des ressources humaines passe par un travail organisationnel de définition des indicateurs de performance individuelle et de coordination en vue de l’utilisation de ces indicateurs. Le recours à des solutions logicielles permet de faciliter l’accès à de nombreux indicateurs sur la gestion des talents et de mettre en lumière la performance des salariés clés de l’entreprise. On peut citer notamment le recours fréquent aux SIRH. Une étude indépendante ayant analysé la question a ainsi montré que les entreprises et administrations françaises ont recours à trois expertises différentes liées à l’utilisation de logiciels dans l’évaluation de la performance de la fonction ressources humaines : le conseil, l’externalisation et le décisionnel.Si les solutions de gestion des talents et d’évaluation de la performance RH sont historiquement apparues sur les marchés par l’intermédiaire de spécialistes d’un des trois domaines d’expertise, des solutions généralistes apparaissent également. Il existe de très nombreuses solutions informatiques, appelées SIRH, internalisables ou en SaaS qui permettent la gestion des ressources humaines :La gestion des compétencesLa gestion du planningLa gestion de la paieLa gestion de la formationLa gestion du recrutementLa gestion des risques professionnels La numérisation du processus des ressources humaines La fonction des Ressources Humaines n'échappe pas aux processus de numérisation. En quelques années, l'évolution des nouvelles technologies a poussé les entreprises à évoluer. Le domaine des ressources humaines est aujourd'hui particulièrement concerné par ce phénomène.Cette numérisation consiste en l'utilisation des nouvelles technologies et des NTIC (Nouvelles Technologies de l’Information et de la Communication) afin de rendre plus efficace l'ensemble des fonctions des ressources humaines. La numérisation est aujourd'hui un moyen d'optimiser la gestion du service RH via la réduction de tâches chronophages. On entend par là les nombreux documents (papiers) associés au service des ressources humaines. La dématérialisation de ces documents s'impose donc de manière logique dans les processus RH purement administratifs (paie, congés, absences…).Les fonctions des Ressources Humaines les plus impactées par la numérisation sont :Le recrutement : Le recrutement traditionnel fait place aujourd'hui à de nouveaux procédés. On parle aujourd'hui de recrutement en ligne, c'est-à-dire de recrutement qui utilise un ensemble d'outils informatique (Smartphones, visioconférence, salons virtuels…) mais également de recrutement 2.0, c'est-à-dire un recrutement qui utilise les outils internet (Candidature sur des sites, vivier numérique…).La gestion de la paie : L'utilisation des différents logiciels de paie permet aujourd'hui aux services RH de se concentrer sur des tâches plus « sociales », comme le management, le climat social ou le bien-être des salariés.La formation : Le domaine de la formation est une fonction qui est réellement impactée par le numérique. Même si la formation en présence reste une valeur sûre pour la formation des salariés (avec la formation continue notamment), de nouvelles méthodes prennent une place de plus en plus importante. En plus de la formation en ligne et des MOOC (Massive Open Online Courses ou cours en ligne ouverts et massifs), on trouve aujourd'hui de nouvelles formes de formation comme le Blended learning (on désigne par le terme de blended-learning, la formation dispensée selon plusieurs modalités d’apprentissage cumulatives : en présence, à distance asynchrone et à distance synchrone), les classes virtuelles, les « serious games », les plateformes d'engagement... Ces nouvelles formations ont un coût bien plus faible que les formations traditionnelles.Les professionnels des ressources humaines peuvent être membres d'associations.En France, on trouve par exemple l'ANDRH (Association Nationale des Directeurs de Ressources Humaines). Des associations de jeunes professionnelles permettent également de se tenir régulièrement informés Au niveau Européen, on trouve l'EAPM (European Association for People Management). Au Québec, le CRHA (Ordre des Conseillers en Ressources Humaines Agréés).Cahier de l'université d'hiver http://www.entreprise-personnel.com/#/entre-nous/activites/publications/etudesLa Gestion des ressources humaines, Tania Saba, Simon L. Dolan, Susan E. Jackson et Randall S. Schuler, Compagnon Web, édition 4, 2008, p. 71-72.De la lutte contre les discriminations à la promotion de la diversité au sein de l'entreprise, Paul Schiettecatte, Jean-françois Roquet, Catherine de Verdière, Didier Rapeaud, Bénédicte Michon (…), éditions EMD-Synthec (2007).Paroles d'experts RH, Yves Richez, Hélène Morel, Pierre-Eric Sutter, Frédéric Fougerat, Benjamin Chaminade, Didier Pitelet, Véronique Frogé, Isabelle Deprez, Editions Studyrama, collection Focus RH, 1er trimestre 2013, 284 p.Ressource relative à la santé : (en) Medical Subject Headings https://disruptifrh.com/2017/08/01/construire-sa-veille-rh/ Portail du management   Portail du travail et des métiers   Portail de la psychologie"
économie;"Selon le vocabulaire de la comptabilité nationale, l’investissement (mesuré par la formation brute de capital fixe, en abrégé FBCF), peut être le fait de différents agents économiques :pour les entreprises : c'est la valeur des biens durables acquis pour être utilisés pendant au moins un an dans leur processus de production. Il peut avoir trois formes : capacité, remplacement et productivité ;pour les ménages : la FBCF dans le cadre de leur activité domestique ne concerne que l'acquisition ou la production pour leur propre compte de logements ;pour les entrepreneurs individuels : la formation brute de capital fixe (FBCF) des ménages en tant qu'entrepreneurs est comptée dans la FBCF des entreprises.Les investissements financiers, les acquisitions de terrains et les investissements immatériels (publicité, etc.) ne sont pas comptabilisés dans la FBCF, bien que ces investissements aient pris depuis les années 1980 une grande importance dans la stratégies des sociétés.Le rendement d'un investissement fait l'objet d'un calcul prenant en compte sa durée de vie ou sa durée d'utilisation (avec dans ce cas la prise en compte de valeur finale résiduelle de l'investissement).L'investissement durable stratégique désigne un type d'investissement se voulant plus vertueux, dans la perspective de développement durable, ce qui implique de prendre en compte dans la prise de décision d'investissement de nouveaux paramètres comme la durabilité et la soutenabilité.L'impact d'un investissement sur une entreprise, qui produit et/ou met à disposition des biens et des services, peut être financier ou uniquement en propriété.Cet impact est financier si l'entreprise reçoit effectivement le montant de l'investissement. Cet investissement va donc augmenter son capital social.Seul un investissement sur le marché primaire (ex : lors de la fondation d'une société ou lors d'une émission d'actions d'une société existante) a un impact financier sur l'entreprise. Cet investissement sert en général à acquérir ou améliorer des moyens de production (machines, locaux, informatique, etc.).Un investissement sur le marché secondaire (ex : un produit financier d'une assurance vie composé d'un « panier » d'actions) est un échange financier (ex : entre un particulier et sa banque) dont pas un sous ne va aux entreprises dont les actions composent le panier.Dans tous les cas, que ce soit un investissement sur le marché primaire ou secondaire, il a un impact sur la propriété. Ainsi, les actions rachetées par d'autres lors d'une OPA peuvent provoquer des changements importants d'actionnaires qui, en vertu des pouvoirs que donne la propriété des actions, sont en mesure de modifier profondément les destinées de l'entreprise et de ses salariés, alors même que l'entreprise elle-même n'a pas reçu un sous : la transaction est uniquement entre investisseurs. Il en est de même lorsque des membres d'un C.A. de sociétés sont des gestionnaires de fonds d'investissement ou de pensions tenus de rentabiliser les placements de leurs petits ou gros épargnants.Ces deux marchés (primaire et secondaire) nous suggèrent une typologie des investissements :Investissements ayant une finalité d'accroissement du capital technique (ou capital fixe, ou capital productif) ;Investissement financier dont la finalité est de rechercher une contrepartie (placement) ou à plus ou moins long terme un gain financier (plus-value).Marx parle de cet investissement d'une manière que certains qualifieraient de manichéenne dans « Das Kapital, Band 2, Abschnitt 1, 1.4 Der Gesamtkreislauf » :« Geldmachen ist das treibende Motiv. Produktion erscheint nur als notwendiges Übel dazu. » soit « Gagner de l'argent est le motif moteur. Pour cela, la production n'apparaît que comme un mal nécessaire ». … à défaut de pouvoir s'en défaire ou d'en rêver comme il le précise dans la parenthèse ensuite : « Alle kapitalistischen Nationen ergreift periodisch ein Schwindel, den sie zur Geldmacherei frei von lästiger Produktion nutzen. » soit « Toutes les nations capitalistes ont périodiquement une chimère, celle de pouvoir faire du fric en se passant d'une production pesante ennuyeuse ».Ce rêve de légèreté et de vitesse des investissements se réalise justement dans la sphère financière, dans le marché secondaire, avec des « produits » financiers de toute sorte et les systèmes « électroniques » pour les transactions internationales. Il se réalise également dans la sphère de l'économie réelle, parfois au détriment de PdG trop adeptes d'une logique industrielle ou sociale et pas assez d'une logique « financière ».Ainsi, Pierre Suard, ancien PdG d'Alcatel, a été nommé par des investisseurs dont la finalité était productive. Il a créé un empire industriel à l'image de Siemens, son concurrent le plus semblable. Il a été débarqué et remplacé par Serge Tchuruk en 1995 après l'arrivée de nouveaux actionnaires dont les investissements étaient plutôt à finalité « financière ». Ce dernier a concentré Alcatel sur son « cœur de métier », escompté le plus rentable, et a vendu le reste. Le changement de slogan qui a suivi, même du « cœur de métier », est révélateur d'un changement de finalité des investissements, moins industriel et plus financier : le slogan « être un architecte d'un monde internet » est remplacé par « apporter de la valeur ajoutée aux actionnaires ».En mars 2021, la même mésaventure arrive à Emmanuel Faber, PdG de Danone débarqué sous l'impulsion d'actionnaires anglo-saxons insatisfaits des résultats financiers de leurs investissements, obérés, d'après eux, par la politique sociale de ce PdG.À la vue des réalités économiques actuelles, il semble que l'influence des investissements « financiers », y compris sur le marché primaire, soit de plus en plus grande.Cette domination des investissements « financiers » peut aussi s’apprécier en considérant les flux financiers : (1-) les flux financiers correspondant au marché primaire (à savoir investissements productifs) sont beaucoup moins importants que ceux correspondant au marché secondaire (à savoir investissements « financiers ») ; (2-) même au niveau du marché primaire, il semble que l'investisseur souhaite minimiser, rendre ""marginale"" son investissement et il a à sa disposition les outils juridiques pour le faire.En effet, le plus souvent, les entreprises investissent directement soit en recyclant une partie de leurs bénéfices, soit surtout en empruntant directement sur les marchés bancaires ou obligataires. La part d’investissement par le marché primaire (ex : par émission d'actions) est minime au regard de leur investissement direct : en 2016 investissement par émission d'actions : 22 M€ ; par emprunt des entreprises : 297 M€ (source : LaTribune et Insee). De plus, il faut déduire des investissements sur le marché primaire la part de plus en plus importante de « rachat d'actions » par l'entreprise sur ordre de ses « investisseurs », ce « rachat » consiste à reverser à ceux-ci les montants de la valorisation d'une partie de leurs actions pour les « annuler ». Souvent l'entreprise doit emprunter pour cela.Enfin, l'investissement, au regard des investissements directement faits par les entreprises, est à considérer en tenant compte du concept de « responsabilité limitée » conjugué avec la non réalité juridique de l'entreprise : les investisseurs d'une entreprise ont de fait la propriété et le contrôle de TOUS les moyens de production de celle-ci alors même qu'ils n'y ont que peu contribué par leur argent.Le concept de « responsabilité limitée » et sa mise en œuvre dans les lois au XIXe siècle (ex : en France, lois du 23 mai 1863 puis du 24 juillet 1867 ; en Angleterre lois de 1856 à 1862 sur les Joint-Stock Company limited) compte, d'après Y.N. Harari dans son célèbre ouvrage Sapiens, « parmi les inventions les plus ingénieuses de l’humanité » : « Peugeot est une création de notre imagination collective. Les juristes parlent de « fiction de droit ». Peugeot appartient à un genre particulier de fictions juridiques, celle des « sociétés anonymes à responsabilité limitée ». L’idée qui se trouve derrière ces compagnies compte parmi les inventions les plus ingénieuses de l’humanité ». Harari en explique les avantages : « Si une voiture tombait en panne, l’acheteur pouvait poursuivre Peugeot, mais pas Armand Peugeot. Si la société empruntait des millions avant de faire faillite, Armand Peugeot ne devait pas le moindre franc à ses créanciers. Après tout, le prêt avait été accordé à Peugeot, la société, non pas à Armand Peugeot, l’Homo sapiens » actionnaire !Cette explication montre que la « responsabilité limitée » est en fait non pas une limitation des risques mais un véritable transfert de responsabilité et des risques de l'investisseur-actionnaire à la société-entreprise, à son collectif de travail, responsabilité pénale et économique. Toutefois ce transfert ne s'accompagne pas en retour d'un transfert de propriété du fait de la non réalité juridique de l'entreprise : quel que soit le montant investi par l'investisseur-actionnaire il a toujours le pouvoir et est propriétaire de fait (de par sa possession des actions) de tous les moyens de production (locaux, machines, moyens informatiques, etc.), y compris de ceux acquis grâce aux « millions » empruntés : c'est l'entreprise, qui acquiert en empruntant, qui rembourse, et qui entretient à ses frais les moyens de production en plus, bien entendu, de payer les salaires, charges et taxes.Grâce à cette « responsabilité limitée » conjuguée avec la non-réalité juridique de l'entreprise, plusieurs procédés permettent aux investisseurs-actionnaires d'accroître les moyens de production qu'ils contrôlent en minimisant au maximum leur mise (le capital social) : investissement par effet de levier, achat à effet de levier, rachat d'actions. Il est donc très compréhensible que les investisseurs-actionnaires recourent à ces procédés plutôt que d’émettre des actions supplémentaires provoquant l'arrivée d'autres investisseurs-actionnaires avec qui certes les risques sont partagés mais également le pouvoir et la propriété. Si l'entreprise était, comme une association 1901, sujet de droit, la « responsabilité limitée » serait remplacée par les « responsabilités et propriétés partagées » entre actionnaires et le collectif de travail de l'entreprise, chacun selon sa contribution. Les procédés « à effet de levier » et autres au profit de certains ne seraient plus et beaucoup d'autres s'en réjouiraient.Sous la finalité générale d'accroissement du capital technique (ou capital fixe, ou capital productif) des objectifs plus précis peuvent être visés :l'investissement de remplacement ou de renouvellement, a pour but de maintenir l'activité à son niveau actuel ;l'investissement de modernisation ou de productivité, a pour but d'accroître la productivité en introduisant des équipements modernes et perfectionnés ;l'investissement de capacité ou d'expansion, a pour but d'augmenter la capacité de production de l'entreprise en ajoutant par exemple des unités de production que ce soit d'un produit déjà existant, il s'agit alors d'une expansion quantitative, ou d'un nouveau produit - on parle alors d'expansion qualitative ;l'investissement total.L'investissement peut être qualifié de :productif : attention double sens possible :soit renvoie à l'idée qu'il s'agit d'un investissement de nature directement productive,soit renvoie à l'idée de l'efficacité de son rendement : la valeur cumulée des biens et des satisfactions obtenues est supérieure voire très supérieure au coût investi ;non directement productif (voire improprement qualifié d'improductif): il concerne des biens et des services d'utilité publique (écoles, hôpitaux, etc) ;matériel : il se traduit par la création d'un bien ou actif réel (un bien de production, par exemple) ;immatériel : il concerne des services : formation, recherche-développement, innovation, marketing, technologies de l'information, publicité, etc., susceptibles d'apporter un développement futur ;financier : il doit être considéré à part compte tenu de ce que sa finalité est de rechercher une contrepartie (placement) ou à plus ou moins long terme un gain financier (plus-value) ;stratégique, lorsqu'il est jugé essentiel pour la survie ou l'avenir de l'investisseur ;réputationnel, lorsqu'il contribue ou est nécessaire à la réputation de l'entreprise ou à son maintien, avec par exemple la publicité, l'acquisition de certains labels et certifications, certaines formes de mécénat, le rappel de produit.L'investissement « productif » se décompose d'abord en bâtiments puis en équipements.La manière dont sont enregistrées et répertoriées les dépenses d'investissement peut conduire à des difficultés pratiques :Par exemple, les dépenses en technologies de l'information sont habituellement rattachées à des centres de coût dans les entreprises. Or dans ce type de dépenses, 50 % en moyenne[réf. nécessaire] concerne la maintenance d'applications existantes, (dépenses d'exploitation) les 50 % restant concernent les développements (dépenses d'investissement). Or la distinction est souvent perdue dans la comptabilité des entreprises (avec un impact fâcheux sur l'évaluation objective de l'effort d'investissement et/ou d'innovation).On parle d'investissement brut quand le flux d'investissement comprend l'investissement neuf et l'investissement de remplacement.Le calcul de l'Investissement net s'obtient par différence entre : Capital technique de fin de période - Capital technique en début de période. Il représente l'investissement brut moins l'amortissement. Selon la théorie économique L'investissement doit être fait jusqu'au point où son bénéfice marginal égale son coût marginal. Ceci suppose évidemment que les biens d'investissements nécessaires soient disponibles. Selon le critère de la rentabilité Investir revient à engager de l'argent dans un projet, en renonçant à une consommation immédiate ou à un autre investissement (coût d'opportunité) et en acceptant un certain risque, pour accroître ses revenus futurs.La rentabilité est mesurable selon différentes méthodes qui ne donnent pas toutes toujours exactement le même résultat, tout en restant globalement cohérentesle retour sur investissement, qui peut s'exprimer en taux ou en temps, mesure le ratio des sommes rapportées par l'investissement sur le montant investi ;la valeur actuelle nette : l'investissement rapporte la différence entre son coût et la VAN, qui dépend du taux d'actualisation retenu ; elle diffère du retour sur investissement en ce qu'elle tient compte du montant total investi : par exemple, le retour sur investissement peut être meilleur pour l'achat d'une bicyclette que d'une maison (rendement respectif de 100 % et 1 %) et la VAN en sens inverse (VAN respective de 200 € et 100 000 €) ;le taux de rentabilité interne (TRI) : l'investissement est d'autant plus rentable que ce taux est élevé (cependant, pour un taux d'actualisation donné et connu, la VAN est un indicateur plus significatif, alors qu'on peut trouver deux investissements A et B tels TRIA > TRIB et VANB > VANA).On peut également assimiler à la rentabilité des critères tels que le temps nécessaire pour atteindre le point mort (durée nécessaire pour que les flux générés soient égaux au montant de l’investissement initial).Le risque pris par l'investisseur est aussi un critère important, dont un indicateur est le ratio de la capacité d'autofinancement par rapport au montant investi ; il est souvent fait à titre prévisionnel pour déterminer si un investissement proposé est adapté, et dans quelle mesure il satisfera l'investisseur.Quelle que soit la méthode utilisée, les paramètres suivants doivent être convenablement appréciés et intégrés dans le calcul :le capital investi a une durée prévue d'utilisation à la fin de laquelle il peut encore présenter une valeur résiduelle ;le prix relatif du capital par rapport à celui du travail influe sur l'investissement. Lorsque le prix du capital baisse par rapport à celui du travail, il est intéressant d'engager des investissements de productivité, qui permettent de substituer du capital (moins cher) au travail (plus cher) ;les taux d'intérêt déterminent le coût des emprunts contractés pour effectuer un investissement et peuvent donc freiner l'investissement s'ils sont élevés ;le niveau d'endettement de l'entreprise joue aussi : une entreprise endettée devra consacrer ses profits à son désendettement au risque de disparaître ;les entreprises cherchent à anticiper la demande avant d'investir pour savoir s'il est nécessaire d'augmenter leurs capacités de production. Ainsi, des anticipations favorables où l'on prévoit une hausse de la demande, favorisent l'investissement tandis que les anticipations défavorables qui prévoient une stagnation ou une baisse de la demande, le freinent. C'est le principe de la demande anticipée ou effective évoquée par Keynes. C'est la demande anticipée des entrepreneurs qui va déterminer l'offre. Autres méthodes financières D'autres méthodes existent qui s'inscrivent dans la mouvance des théories financières intégrant davantage l'incertitude future liée aux valorisations découlant du marché :James Tobin a proposé un critère, appelé le Q de Tobin qui compare la valeur boursière de l'investissement avec son coût de remplacement ;Dixit et Pindyck(1994) proposent de faire l'analogie avec les options: pour la décision d'investissement, l'entrepreneur a le choix entre ne rien faire (attendre) ou investir tout de suite, choix dont l'irréversibilité joue un rôle important dans la productivité.Dans sa décision d'investir, l'entrepreneur compare le cout de l'investissement (I) et la somme des valeurs, actualisées et pondérées par les risques, des rentrées de trésorerie obtenues grâce à l'investissement (R). Le projet d'investissement sera réalisé si R > I. Dans l'analyse keynésienne, l'efficacité marginale du capital désigne le taux de rendement interne de l'investissement. Elle sert de taux d'actualisation des recettes tirées de l'investissement. À savoir, l'investissement est d'autant plus important que le taux d'intérêt est faible. Pour Keynes, l'investissement dépend de la comparaison entre l'efficacité marginale r de l'investissement et le taux d'intérêt pratiqué sur le marché des capitaux, i. Si r > i, la décision de réaliser l'investissement est justifiée. Il peut être financé soit à partir de fonds dont dispose l'entreprise, soit à partir d'emprunt dont le coût est inférieur au taux de rendement de l'investissement. La formule de Keynes n’est valable que pour un investissement financé uniquement par la dette. Si une partie du financement est apportée en fonds propres, il est nécessaire d’en calculer le coût puis de calculer le coût moyen pondéré du capital, qui sera substitué à i. Par ailleurs, l’entrepreneur prendra une marge de sécurité car en pratique, le rendement de l’investissement ne sera pas égal à celui anticipé.Dans l'analyse macro-économique, le terme d'investissement est réservé à la seule création de biens capitaux nouveau (machines, immeubles...). Pour Keynes, l'investissement dépend de l'efficacité marginale du capital et du taux d'intérêt. En fait, les dépenses en biens d'investissement dépendent principalement de deux variables :le rendement attendu de l'investissement, dit ""efficacité marginale du capital"",le taux d'intérêt i ou coût d'emprunt contracté pour financer l'acquisition de biens d'investissement.Pour une efficacité marginale donnée, l'investissement apparaît comme une fonction décroissante du taux d'intérêt. Le niveau du taux d'intérêt est donc la variable incitatrice ou désincitatrice privilégiée du processus d'investissement. Dans l'analyse Keynésienne, l'investissement est considéré comme autonome, c'est-à-dire indépendant du revenu.Avant toute chose, le dirigeant doit faire tout d'abord son métier en restituant l'investissement dans la stratégie d'entreprise et l'organisation d'entreprise. À défaut, il risque de prendre des décisions hâtives en matière de moyens mais sans chemin pertinent et/ou dans une facilité trompeuse qui juge inutile la nécessité de cette réflexion.Avant d'engager ses ressources propres à l'investissement, l'entreprise doit en effet examiner toutes les solutions possibles pour financer son besoin de financement : autofinancement, recours à l'emprunt, leasing, aides publiques (pour la R&D), augmentation de capital ou financement par prélèvement sur fonds propres. Ces sources de financement peuvent être combinées.Il faut aussi noter que les investissements peuvent aussi être financés par cession d'actifs , (dans l'hypothèse où l'entreprise désinvestit dans le cadre d'une stratégie de réorientation ou de recentrage de ses activités).Le législateur offre des possibilités de réduction d'impôt sur le revenu et impôt de solidarité sur la fortune (ISF) pour les particuliers qui investissent dans les PME. La PME doit répondre à des critères quantitatifs (CA < 50M€, emploie de moins de 250 salariés) et à la définition de PME communautaire.Pour l'ISF la réduction fiscale ne concerne que la souscription au capital initial de la société ou la souscription à une augmentation de capital de celle-ci. La possibilité de réduire son ISF risque donc de ne profiter principalement qu'à un cercle réduit de contribuables, sollicités par leur entourage pour participer à ce type d'opérations souvent réalisée en cercle restreint.L'autofinancement est le financement des investissements par des moyens internes à l'entreprise. L'autofinancement se mesure de deux manières : le taux de marge qui donne une indication sur les ressources de l'entreprise (excédent brut d'exploitation / valeur ajoutée) et le taux d'autofinancement : EB/FBCF (Formation Brute de Capital Fixe) qui mesure la part de l'investissement qui est financée par l'épargne brute (partie de l'EBE, hors dividendes, intérêts et impôts, servant à financer la FBCF).Cela consiste à lever des capitaux sous forme de prêt auprès de tiers. La durée de l'emprunt doit être en accord avec la durée d'amortissement du bien acheté (en général l'emprunt est un peu plus court que celle-ci). L'emprunt peut être de 2 types : bancaire ou obligataire.Il s'agit d'augmenter les capitaux propres de l'entreprise en faisant souscrire de nouvelles parts (SARL) ou actions (SA). Il est demandé, via une opération d'augmentation de capital en numéraire,aux actionnaires de mettre la main à la poche pour financer les investissements ;et/ou à de nouveaux actionnaires d'entrer dans le capital de l'entreprise.Cette méthode a l'avantage de renforcer la solvabilité de l'entreprise, laquelle de toute façon ne peut dépasser un certain montant de recours à l'emprunt sans perdre la confiance de ses banques et fournisseurs. Cela dit, cette opération est assez souvent mal vue par les actionnaires, car l'émission de nouvelles actions va « diluer » la valeur de leurs actions actuelles.Cette méthode n'est donc utilisable que si les actionnaires acceptent de remettre de l'argent dans la société. Cela dépendra en grande partie :de la rentabilité des fonds propres affichée ou visée par l'entreprise. Cette rentabilité et le risque qui lui est associé doit être comparée aux autres couples rentabilités/risques disponibles par ailleurs ;et, pour les sociétés cotées, du cours de bourse, qui doit être supérieur au prix d'émission des nouvelles actions pour qu'il y ait intérêt à souscrire celles-ci ;ou encore, facteur plus négatif mais qui entraîne une pression forte sur les actionnaires, d'une situation d'endettement critique risquant de faire sombrer l'entreprise si elle ne trouve pas de l'argent frais pour conforter ses capitaux propres.L'augmentation de capital en numéraire ne doit pas être confondue avec celle par incorporation de réserves (il ne s'agit alors que d'un transfert de poste comptable à l'intérieur des capitaux propres) ni celle par échange de titres (cas de fusion-acquisition)On parle de mal-investissement lorsque l'investissement est inadéquat : trop élevé (sur-investissement), trop faible (sous-investissement), ou les deux à la fois (i.e. : mal orienté).La décision d'investir ou de ne pas le faire, est toujours une forme de pari sur l'avenir : il n'est donc pas étonnant de rencontrer des investissements inadéquats. Lorsqu'une accumulation d'investisseurs se trouvent commettre la même erreur, plus ou moins simultanément, celle-ci peut générer - au niveau macro-économique, dans une filière d'activité ou dans une zone géographique - des situations pouvant aller de la simple récession à la crise économique de plus grande ampleur (voir l'analyse du cycle économique).En régime d'économie libre, la variable essentielle en la matière est le taux d'intérêt. Trop élevé, il rend impossible l'investissement même dans des projets a priori rentables. Trop bas, il favorise l'investissement dans des projets à la rentabilité trop faible.Des agents économiques trop optimistes peuvent sur-investir et créer des capacités de production excédentaires par rapport à la demande effective exprimée par le marché. À l'échelle d'un pays, ou d'une branche d'activité, l'insuffisance constatée des débouchés par rapport à l'offre ainsi créée va provoquer un effet déflationniste et la faillite des entreprises marginales (celles dont le prix de revient est le plus élevé).Goal-based investingDirection générale des entreprises (France)Oséo (France)Small Business Administration (États-Unis)Direction générale des entreprises (Espagne)Ressource relative à la santé : (en) Medical Subject Headings Ressource relative aux beaux-arts : (en) Grove Art Online  Portail de l’économie   Portail de la finance"
économie;"Il existe plusieurs définitions de la valeur selon le courant de pensée économique. Elles se rattachent à deux conceptions principales qui donnent au mot « valeur » des sens radicalement différents, et impliquent deux conceptions différentes de la relation entre valeur et prix.La conception subjective définit la valeur comme l'expression de l'intérêt qu'un agent particulier porte à un bien ou à un service, qui résulte d'un processus psychologique d'évaluation. C'est une notion subjective et privée dont la formation et l'explication relèvent de la psychologie et non de l'économie, et qui constitue une donnée externe pour le raisonnement économique. Le prix est une notion distincte, qui résulte du fonctionnement effectif des mécanismes du marché, et qui seule a un sens économique. Formation de la valeur et formation du prix sont considérés comme deux processus distincts, seul le second relevant de l'analyse économique,La conception objective pose que tout bien a une valeur indépendante de l'observateur, qui résulte des conditions de sa production et peut être déterminée par un calcul économique à partir des conditions et des coûts de production du bien ou du service. Le prix est alors généralement considéré comme une mesure de cette valeur.La conception subjective est dominante depuis l'origine de la pensée économique (Aristote, Thomas d'Aquin). Elle a été maintenue par les classiques français (Turgot, Say), alors que la conception objective a été proposée d'abord par les Physiocrates avec comme référence la terre, puis par les classiques anglais avec comme référence le travail, et enfin reprise par les économistes marxistes.Bien que fondamentalement adeptes de la conception subjective, les économistes du courant dominant néoclassique utilisent souvent les mots prix et valeur de façon interchangeable, Ce qu'ils appellent « théories de la valeur » sont plus précisément des théories de la formation des prix, censés représenter une « valeur sociale » plus ou moins objectivée. Seuls les économistes du courant « autrichien » s'efforcent d'être fidèles à cette distinction, sans toujours y parvenir.D'autres disciplines s'intéressent aux notions de « valeur » ou de « valeurs » : Voir les articles spécifiques qui leur sont consacrés.L'histoire du concept de Valeur est dans l'article : Valeur en philosophie.Dans cette perspective la valorisation d'un bien est fondée sur un facteur objectif, matérialisé par un critère mesurable, le travail.Les physiocrates et particulièrement François Quesnay mettent en avant la valeur provenant de la nature. Elle repose sur l’idée que seule la production agricole est créatrice de richesses et que toute autre activité artisanale ou commerciale n’est que l’addition de travail aux matières premières issues directement ou indirectement des produits de la terre.Ainsi, l'estimation de la valeur résulte de l'agrégation de « la valeur des produits naturels constituant le capital nécessaire à une production + la valeur des produits naturels nécessaires à la reproduction de la force de travail ».Les économistes Adam Smith et David Ricardo distinguent la valeur d'échange de la valeur d'usage et considèrent que c'est la première qui, en économie, joue un rôle déterminant. Ils cherchent une cause objective pouvant expliquer le prix des marchandises. Suivant William Petty, John Locke, lequel a justifié la propriété individuelle par le travail, et David Hume, les économistes classiques anglais estiment que le travail joue un rôle essentiel dans la détermination de la valeur d'un bien.Adam Smith voit dans le travail la source de la richesse des nations.« The annual labour of every nation is the fund which originally supplies it with all the necessaries and conveniences of life which it annually consumes, and which consist always either in the immediate produce of that labour, or in what is purchased with that produce from other nations. »Selon lui la valeur d'un bien est égale à la quantité de travail que cette marchandise peut acheter ou exiger.« Labour was the first price, the original purchase-money that was paid for all things. It was not by gold or by silver, but by labour, that all the wealth of the world was originally purchased; and its value, to those who possess it, and who want to exchange it for some new productions, is precisely equal to the quantity of labour which it can enable them to purchase or command. »Non seulement le travail est une composante du prix, mais il est aussi à l'origine du profit et de la rente.« The real value of all the different component parts of price, it must be observed, is measured by the quantity of labour which they can, each of them, purchase or command. Labour measures the value not only of that part of price which resolves itself into labour, but of that which resolves itself into rent, and of that which resolves itself into profit. »David Ricardo développe la notion de valeur-travail introduite par Adam Smith et cherche à comprendre comment le travail se transfère en profit et en rente. Il commence par transformer la notion de valeur-travail. Pour lui la valeur d'un bien est égale, non à la quantité de travail qu'il peut commander, mais à la quantité de travail, direct et indirect, nécessaire à sa fabrication.« The value of a commodity, or the quantity of any other commodity for which it will exchange, depends on the relative quantity of labour which is necessary for its production, and not on the greater or less compensation which is paid for that labour. »Pour les penseurs classiques, les taux de profit d'industries différentes tendent à se rapprocher vers une même valeur basse, à mesure que la compétition entre les entreprises augmente. L'idée étant que, si un secteur est plus rentable que les autres, il attire naturellement de nouveaux investisseurs qui quittent d'autres secteurs aux taux de rentabilité plus faibles.« In a country which had acquired its full complement of riches, where in every particular branch of business there was the greatest quantity of stock that could be employed in it, as the ordinary rate of clear profit would be very small. »Ce constat a une implication sur la valeur des biens. Ricardo pose la question de savoir si la valeur-travail est compatible avec un taux de rentabilité uniforme parmi toutes les industries. La réponse est négative si les instruments de production ont des durées de vie différentes (voir ""durability of the instruments of production"" chez Ricardo). Aussi le concept valeur-travail ne semble pas cohérent avec les autres propriétés basiques d'une économie.Karl Marx reprend l'idée de la valeur-travail développé par Ricardo: la valeur d'un bien dépend de la quantité de travail direct et indirect nécessaire à sa fabrication. Mais alors que Ricardo considère le travail comme une commodité ordinaire, Marx juge l'expression 'valeur du travail' incorrecte partant du principe que le travail est à l'origine de toute valeur. Pour Marx les salaires ne représentent pas la valeur du travail mais la location de la force de travail du salarié (Arbeitskraft). Il propose l'explication suivante à l'origine du profit : de la valeur nouvellement créée, le salaire du travailleur ne représente que la part nécessaire à sa propre survie, le reste constituant la plus-value (marxisme) créée par son travail.Pour rendre la valeur-travail compatible avec un taux de plus-value uniforme parmi les industries, Marx radicalise la division du capital introduite par Adam Smith. Il distingue la part nécessaire au paiement des salaires, capital variable (de), du reste, le capital constant. Selon Marx seul le travail permet une augmentation du capital, d'où le terme variable. Cette décomposition permet de poser les bases d'un système concept de valeur-travail compatible un taux uniforme des profits (voir Le problème de transformation chez Marx). Vision des marxistes Les Marxistes apportent les nuances suivantes :L'utilisation de machines dans la production ne change en rien cette analyse objective de la valeur puisqu'une machine ne produit pas de valeur mais transmet simplement la sienne au bien qu’elle produit : la valeur dégagée par une machine est égale à l'usure de celle-ci, car une machine n’est que du travail accumulé (Marx).la valeur d'un bien est affectée par l'expression d'un certain type de rapport social de production, déterminé par l'état des forces productives.la valeur est aussi une propriété émergente du fétichisme de la marchandise qui vient de ce que :les hommes s'en remettent à la circulation des choses dans le cadre concurrentiel de l'équivalence généralisée pour établir des liens productifs entre eux. Elle n'aurait donc de sens que dans le cadre d'une économie de marché.la valeur est l'expression d'un rapport social de production qui se décompose en trois aspects :sa forme (l'échangeabilité qui induit la coordination des producteurs de marchandises sans organisation préalable),sa substance (le travail abstrait qui représente le travail socialement nécessaire pour produire la marchandise)et sa grandeur (la quantité de travail abstrait déterminé par l'état des forces productives).Le concept d'utilité, attribuant à chaque personne des goûts et des besoins différents, a la faveur de la grande majorité des économistes contemporains. L'origine de ce courant est ancienne. On peut le faire remonter à Démocrite, à saint Thomas d'Aquin, et aux scolastiques espagnols.Étienne Bonnot de Condillac évoque l'exemple de la valeur d'un verre d'eau dans le désert pour montrer combien l'utilité et la valeur d'usage sont en réalité le fondement unique de la valeur.Turgot et Say reprennent la notion à leur compte. (Voir à la même époque les travaux du mathématicien Daniel Bernoulli.)Les économistes marginalistes considèrent la valeur comme étant la mesure du désir qu'un agent économique éprouve pour un bien ou un service. C'est alors une appréciation subjective non mesurable, liée aux préférences de la personne compte tenu de sa situation actuelle. William Jevons développe lors d’un congrès en 1862 la notion de « degré final d’utilité » (utilité marginale). Pour reprendre l'exemple du verre d'eau, un homme assoiffé dans le désert est prêt à payer une « fortune » pour UN verre d'eau, un peu moins pour le deuxième quand il s'est déjà abreuvé, encore moins pour le troisième, etc., et ce indépendamment de sa valeur de production. William Jevons introduit donc une subjectivité dans la détermination de la valeur. La théorie néoclassique adopte cette conception de la valeur comme liée à l'utilité dégagée par la dernière unité échangée et à la satisfaction des autres besoins. La formation des prix ne dépend alors plus que de cette utilité marginale.La problématique de l'équilibre général, introduite par Léon Walras et développée dans son formalisme mathématique par Arrow et Debreu, est basée sur le concept d'utilité. Dans ce modèle les prix sont la conséquence de préférences individuelles modélisées par des fonctions d'utilités.L'école autrichienne d'économie, qui s'écarte de l'école néo-classique car refusant des méthodes scientifiques comme la formalisation de modèles mathématiques, développe plus avant une conception subjective de la valeur considérant que l'étude de la formation de la valeur relève de la psychologie et non de l'économie. « Le domaine de notre science est l’action humaine, pas les événements psychologiques qui résultent en une action » ou encore « L'économie commence là où la psychologie s'arrête » (Ludwig von Mises).André Orléan constate que les valeurs d'usage, la valeur marchande et la valorisation boursière sont fonction respectivement de la rareté, de la monnaie et des conventions financières. Les deux dernières sont des concepts construits par la société, la rareté pouvant l'être dans certains cas. Les forces sociales qui se trouvent à l'origine de ces concepts ne peuvent être ni fabriquées, ni contrôlées car elles échappent à l'intentionnalité individuelle. Il en déduit que, dans ces cas, la valeur « est une production collective qui permet la vie en commun. Elle a la nature d'une institution ».Les institutionnalistes accordent une place importante aux institutions. Celles-ci établissent les règles et les moyens permettant aux hommes de fonctionner en tant que société. Elles contribuent à déterminer les comportements et influent sur la formation et transformation des valeurs. Elles font passer d'une conception individuelle à une conception sociale de l'utilité. Jacques Perrin évoque les contraintes environnementales et la montée des inégalités entre les groupes sociaux et entre les pays pour justifier la prise en compte de l'utilité sociale. Pour les institutionnalistes la valeur sociale permet d'apprécier la richesse de la société.André Orléan remarque que la valeur économique n'est pas seulement due à l'utilité ou au travail incorporé mais peut résulter d'un sentiment collectif. Les investisseurs professionnels déterminent leurs positions non pas en fonction de leur propre calcul de la valeur fondamentale du titre mais en fonction de l'évaluation de cette valeur par le marché à l'instar du concours de beauté de Keynes. De même le mimétisme joue dans la détermination de la valeur d'un bien lorsque ce bien est représentatif de prestige et de statut social. Thorstein Veblen, le premier, a mis en évidence l'importance de l'opinion des autres dans la valeur accordée à un bien. L'utilité n'est alors pas forcément absente pour l'acquisition du bien, mais son importance est secondaire par rapport au prestige. Reprenant cette thèse Orléan cite les phénomènes de mode. L'utilité est alors fonction du comportement des autres. Les pratiques du marketing et de la publicité témoignent également de l'importance de la motivation mimétique. Citant Akerlof Robert Boyer rappelle que « les jugements de valeurs rétroagissent sur la possibilité d'obtention d'équilibres économiques efficients ».Les économistes ont décomposé la valeur des biens environnementaux et/ou culturels. La valeur économique totale se compose de valeurs d'usage et de valeurs de non-usage,.Les chercheurs distinguent trois valeurs d'usage :La valeur d'usage direct est liée à l'usage direct du bien. Il s'agit notamment des usages récréatifs, du tourisme ou encore de l'exploitation des ressources naturelles.La valeur d'usage indirect est lié aux effets du site sur des valeurs d'usage direct ailleurs. Pour les biens environnementaux, on parle généralement de la valeur des services écosystémiques. Cela peut aussi désigner des effets économiques induits par les valeurs d'usage direct.La valeur d'option se réfère aux usages futurs. Cela prend en compte le fait que certains éléments peuvent avoir utilité dans le futur, sans que nous la connaissions dès à présent. Par exemple, les végétaux présents dans une forêt peuvent contenir des principes actifs capable de soigner des maladies. Si ces principes actifs sont déjà connus, il s'agit d'une valeur d'usage direct. Si ces principes sont inconnus, alors il s'agit bien d'une valeur d'option, car on souhaite préserver la possibilité d'un usage futur (si l'espèce disparaît, on ne pourra plus rien découvrir grâce à elle).Les économistes distinguent deux ou trois valeurs de non-usage :La valeur de quasi-option (ou valeur d'option informationnelle) est parfois omise. Cette valeur correspond au fait que certaines décisions ont des effets irréversibles. Ainsi, les solutions qui permettent de préserver différentes options futures ont une valeur supplémentaire par rapport aux décisions irréversibles. La valeur de quasi-option désigne donc l'ensemble des gains (monétaires ou non) générés par l'information future, qui aura pu être mobilisée parce que différentes options auront été maintenues.La valeur d'existence correspond à la satisfaction de savoir que le bien existe. Cette satisfaction est indépendante d'un quelconque usage (direct ou indirect) de la part de l'individu concerné, ou même d'un autre individu.La valeur de legs (ou valeur d'héritage) est liée à la transmission du bien aux générations futures. Cela correspond en quelque sorte à la valeur d'existence, mais à long terme. Il s'agit donc de la dimension patrimoniale du bien.Dans le langage courant, et même dans les livres d’économie, « valeur » et « prix » sont souvent employés l’un pour l’autre. Cependant Smith, Ricardo et Marx distinguent valeur d’usage et valeur d’échange. Seule la valeur d’échange a un prix. La valeur d’usage n’est pas susceptible d’être mesurée. Elle s’interprète subjectivement et ne peut être quantifiée par un prix. Pour les néoclassiques valeur d’échange et valeur d’usage ne font qu’un. L’utilité d’un produit est mesurée par le prix que lui fixe le marché. Valeur et prix sont synonymes. Est écarté ce qui ne passe pas par le marché, les productions domestiques et le bénévolat. Concernant les éléments naturels (lumière solaire, air, ressources, etc.) les néoclassiques considèrent qu’ils ont une valeur économique qui se révèle lorsqu’ils sont mis sur le marché. Les tenants de l’écologie profonde, se référant à la distinction déjà faite entre valeur d’usage et valeur d’échange, estiment que les biens naturels ont une valeur qui relève de l’éthique ou du politique. Autrement dit, les ressources naturelles sont des richesses et elles n’acquièrent de valeur économique que par l’intervention et la valorisation du travail humain (une ressource gisant au fond des océans n’a aucune valeur économique si elle est inaccessible).André Orléan, L'empire de la valeur, Seuil, 2011Jacques Perrin, Pourquoi les sciences économiques nous conduisent dans le mur ?, L'Harmattan, 2011Alternatives Economiques, Hors-série pratique, no 31, 2007Thorstein Veblen, Théorie de la classe de loisir, Gallimard, 1970 pour la traduction françaiseSerge Paugam (Sous la direction de), Repenser la solidarité, PUF, 2007Conception objective de la valeurConception subjective de la valeurThéorie de la valeur (marxisme)Valeur travail (économie)Valeur nette comptableValeur nominale et valeur réelleThéorie de la valeur, Le Capital, Karl Marx, 1867. Portail de l’économie   Portail de la finance"
économie;
"le prix""";
économie;"Les facteurs de production sont les ressources mises en œuvre dans la production de biens et de services, par exemple les machines et travailleurs.L'école physiocratique se fonde sur une analyse des facteurs de production. Elle considère que la terre est la ressource naturelle fondamentale, et donc le facteur de production principal. C'est, selon elle, la seule source de la croissance économique. Le point de vue physiocrate est d'autant plus facile à défendre par ses tenants de l'époque que la France, où naît ce courant de pensée, est à l'époque très majoritairement agraire.Le travail de la terre est valorisé. Le capital l'est moins et n'est pas rangé dans la catégorie des facteurs de production ; les auteurs physiocrates reconnaissent toutefois l'importance de l'investissement en ce qu'il influe sur la productivité du sol.Les économistes de l'école classique retenaient deux facteurs de production, formalisés par Adam Smith : le capital et le travail. Smith, toutefois, fonde toute sa pensée sur le travail comme étant l'unique facteur de production, et le capital comme n'étant qu'un dérivé du travail. Le travail est d'autant plus facilement appareillé à une production qu'il est libre. Lorsque la richesse générée par le travail s'accumule, se forme alors le capital (voir Accumulation du capital).Adam Smith résume ainsi que le travail est « facteur nécessaire de la production. Il s'exerce d'abord exclusivement sur les agents naturels, et en particulier sur la terre ; puis son produit arrive à excéder les fonds nécessaires à la subsistance, il s'accumule et se crée ainsi un nouvel auxiliaire pour la production. Cet auxiliaire n'est autre chose que du travail accumulé ; c'est ce qu'on a nommé le capital ».John Maynard Keynes soutient que le travail est le seul véritable facteur de production. Selon lui, « la technique, les ressources naturelles, l'équipement et la demande effective constitu[e]nt le milieu déterminé où ce facteur opère ».Aujourd'hui, les économistes ne retiennent que ces deux facteurs de production, capital et travail. Le facteur capital se décompose en plusieurs sous-éléments :le capital physique (immobilier, matériels de production, biens durables, etc.), qui s'accroît avec l'investissement et, sans investissement, décroît au fil du temps (selon un taux de dépréciation du capital) ;la force de travail des individus : leur énergie musculaire et leur endurance sous l'effort ;le capital humain, qui correspond aux connaissances accumulées par les humains et mobilisables pour travailler (apprentissage, formation d'ingénieur, expérience, etc.) ;le capital immatériel, concept développé par l'économie du savoir, qui correspond à la valeur accumulée par une entreprise sous forme d'organisation, de savoir-faire accumulé, ou d'image de marque. L'économie des pays développés dépend de plus en plus du capital immatériel ;le capital social et le capital culturel, qui sont évoqués depuis peu comme variables explicatives de l'amélioration de la productivité ne résultant pas des autres facteurs ;le facteur « terre et sous-sol » (de plus en plus aménagé par la main de l'homme) fait partie du capital :soit comme une composante d'un facteur naturel plus large, les ressources naturelles incluant la biodiversité (la notion de capital naturel étant liée à celle de durabilité),soit comme la composante foncière du capital (propriété foncière).Pour simplifier, les quatre principaux facteurs de production apparaissent de nos jours être les suivants :le travail matériel ;le capital naturel (la terre en tant que monde matériel naturel) ;le capital physique ;le capital immatériel (savoir-faire, organisation, actifs incorporels s'ils sont comptabilisés, l'esprit entreprise, le travail immatériel, le savoir).Les experts estiment que le capital immatériel représente entre 60 et 70 % de la valeur des entreprises. Certains mettent la connotation de ce quatrième facteur dans le concept de managementL'investissement permet d'augmenter le volume des facteurs de production. La formation peut être considérée comme une forme d'investissement, puisqu'elle augmente les capacités du travailleur.Dans une économie fondée sur la rareté relative (les ressources existent, mais en quantités limitée), la combinaison optimale de ces éléments pour chaque produit ou service offert sur le marché détermine ce qu'on appelle généralement l’intensité des facteurs. On parle ainsi, concernant la quantité de capital utilisée par unité produite, d’intensité capitalistique.Dans une économie dynamique (c'est-à-dire en changement permanent), la croissance économique est assurée :soit par un accroissement des quantités de facteurs de production mobilisés (croissance extensive) ;soit par une amélioration de la combinaison productive de sorte que la même quantité de facteurs engendre davantage de produit (croissance intensive) ;soit par le progrès technique (dans le modèle de Solow), qui augmente la productivité globale des facteurs (travail et capital) ;soit par l'utilisation efficace des ressources naturelles ou leur existence en quantité élevée. Si le produit intérieur brut (PIB) national du Brésil a été multiplié par quatre du milieu des années 1960 à celui des années 1980, le rôle du facteur naturel est essentiel. L'activité industrielle et l'extraction de minéraux occupent 25 % d'actifs et correspondent au tiers du PIB durant la même période. L'énergie à base de canne à sucre fait fonctionner plus de deux millions de moyens de transport et l'énergie fossile permet de subvenir aux neufs dixièmes des besoins énergétiques vers la fin du vingtième siècle. L'environnement subsaharien de la Côte d'Ivoire permet à ce pays, outre la satisfaction de ses besoins internes, d'exporter une grande quantité de matières premières vers le reste du monde.On parle donc selon le cas de meilleure productivité du travail, ou de meilleure allocation des ressources ou des facteurs.En considérant le PIB comme une fonction du capital et du travail, la croissance résulte de trois paramètres :la quantité de capital utilisée ;la quantité de travail utilisée ;et la productivité globale des facteurs.Les nouvelles théories de la croissance et du développement économique s'efforcent de faire de la productivité un facteur endogène, que l'on explique par des variables telles que :l'effort de recherche et développement, et l'investissement dans le travail (effort de formation), lié à la notion de capital-savoir ;l'effort d'investissement collectif en infrastructures, matérielles (routières, de télécommunication) ou non (infrastructures institutionnelles, comme : système juridique, réglementaire, monétaire, éducatif, etc.).La critique de gauche de la conceptualisation des facteurs de production par la science économique pointe du doigt une confusion qui serait réalisée entre les facteurs (les agents) et les moyens (les objets) de production. En levant cette confusion, l'approche agent-objet fait apparaître que seul le travail humain est un facteur (c'est-à-dire un agent) de production car le capital non-financier n'est qu'un moyen de production, et doit par conséquent être classé dans la catégorie des biens & services. Aussi, dans une telle perspective, le capital financier n'est ni facteur ni moyen de production, il n'est que la contrepartie comptable ou juridique des biens & services de consommation/production.Capital immatériel, connaissance et performance, sous la direction d'Ahmed Bounfour, l'Harmattan, 2006.Travail (économie)Partage de la valeur ajoutéeDurabilitéPaul RomerÉconomie du savoirCapital immatérielMode de productionOptimisation (mathématiques)Terre (économie) Portail de l’économie   Portail de la production industrielle"
économie;"Le marketing mix ou mix marketing ou mix est, en marketing management opérationnel, l'ensemble des domaines opérationnels dans lesquels il faut élaborer des stratégies.Ne comptant longtemps et traditionnellement que 4 domaines de décisions, celles relatives au produit, au prix, au point-de-vente et à la publicité-promotion, le marketing mix compte aujourd'hui plus de domaines.Le Chartered Institute of Marketing, la plus importante association mondiale de professionnels du marketing, 40 000 membres, a adopté en 2009 un marketing mix à 7 variablesRetailing mix est l'équivalent du marketing mix appliqué à un point-de-vente, une enseigne de distribution (Carrefour, Leclerc, Casino, etc.) ou un centre commercial (par ex: Qwartz, etc.)Traffic mix est proposé pour application aux lieux de prestation de service en général (hôtels, parcs d'attraction, stations de sports d'hiver, etc.).Le terme « marketing mix » se diffuse après la publication en 1964 de l'article de Neil H. Borden intitulé « le concept de Marketing-mix » . Borden avait d'ailleurs déjà utilisé le mot dans son enseignement dès les premières années 1940 après que James Culiton eut décrit le « Marketing manager » comme un « mélangeur d'ingrédients ». Pour Borden, ces ingrédients incluent douze éléments : le plan produit, la tarification, la marque, les canaux de distribution, la vente personnelle, la publicité, les promotions, le packaging, la présentation, le service, la manutention, la recherche et l'analyse des faits et données.Il reviendra plus tard à E. Jerome Mc Carthy d'opérer un regroupement de ces éléments dans une présentation plus synthétique en 4 catégories, sous la dénomination des « 4P ».Le marketing mix est un mode de répartition d'analyse du marketing. L'objet du marketing étant l'analyse du marché, il avait été choisi par l'usage (de manière arbitraire) de l'analyser par cet outil mnémotechnique simple que sont les 4P. Certains préfèraient parler des 5P, ajoutant le P de Personnes, soit les consommateurs dans le marketing mix. L'analyse doit être répartie, car il n'est pas possible de réaliser raisonnablement une analyse identique dans des situations aux produits, lieux, mode de communication et prix très différents. Ces éléments portent en eux, en effet, le sens final de l'analyse du marché, c'est-à-dire a priori la recherche de l'identité des clients. La description de l'identité des clients est donc une conséquence (et non une cause) de l'analyse.La simplicité de la répartition est un point important à respecter à des fins d'intégration du marketing au sein de sciences de l'organisation (management) et plus particulièrement des sciences économiques. En effet, le marketing mix doit laisser l'opportunité d'un contrôle du marketing (son coût par exemple). Ceci n'est possible que si la répartition aboutit à l'utilisation d'indicateurs de gestion permettant de rapprocher la stratégie marketing de la stratégie d'entreprise. Il faut en effet rappeler que l'objet du marketing est d'obtenir les économies d'échelles par des ventes plus importantes ou plus généralement l'efficience qui va permettre la performance de l'entreprise.La distinction de la stratégie marketing et celle plus globale de l'entreprise peut aussi enfin être considérée comme une cause de la répartition en 4 parties du marketing mix. La stratégie d'entreprise se focalise sur la notion de processus que ne peut intégrer simplement une stratégie marketing focalisée dans les faits sur les produits, la publicité, le lieu de distribution et le prix. C'est que les synergies organisationnelles de l'activité peuvent être éloignées des objectifs du service marketing. Ainsi, le marketing mix étendu tend à intégrer des éléments analysés en amont par la stratégie globale de l'entreprise.D'après Philippe Villemus, « pour optimiser le profit de l'entreprise, la pratique du marketing consiste à construire son offre, compte tenu de la demande, du jeu des autres et des moyens dont on dispose dans un cadre politique choisi ».Faire son marketing mix, c’est établir de manière opérationnelle sa stratégie marketing. On définit ce que l’on vend, comment, où, pourquoi et toutes les variables nécessaires à la mise en vente d’un produit ou d'un service dans les meilleures conditions de marché. Les démarches préalables à la définition du marketing mix — (la stratégie marketing) — sont le diagnostic interne/externe de l'entreprise, la segmentation du marché, le positionnement de la marque et de chacun de ses produits ou services.Le diagnostic aboutit à la définition d'objectifs en termes de segmentation du marché, qui permettra à l'entreprise d'envisager certaines sources de volume de vente.Le marketing mix se fondait essentiellement selon Jerome McCarthy (1960), largement vulgarisé par Philip Kotler sur la règle dite des 4P, ces quatre politiques définissent le produit au sens large et ses implications commerciales au plan :Product : la politique de produit (choix de la gamme de produits : profondeur de gamme, largeur de gamme, etc.). Le mot produit est employé au sens générique et comprend les prestations liées aux produits (emballage, aide à l'utilisation, maintenance...) mais aussi tout le secteur des services en général qui, dans la société post-industrielle, représentent une part de plus en plus grande des offres marketing.Price : la politique de prix (ex. : écrémage, pénétration, prix d'acceptabilité, rentabilité, etc.)Place : la politique de distribution (choix du réseau et des canaux de distribution, force de vente, etc.). La distribution inclut également le commerce électronique.Promotion : la politique de communication (choix du type publicité, promotion, marketing direct, relations publiques, etc.).On aboutit au plan marketing de la période considérée lorsque les actions concernant ces quatre domaines sont :programmées pour une durée donnée (par exemple une année),chiffrées (budgets, objectifs de résultats...)avec leur détail par produit, segments de clientèle et unités de l'entreprise,et une explicitation des moyens affectés et des actions de terrain correspondantes.Bien que le modèle 4P soit l'un des plus connus de l'analyse marketing et encore un des plus enseignés aujourd'hui, il est fortement contesté depuis 1980 par celui de Bernard Booms & Mary BitnerDans le cas particulier des services et des points-de-service, ( CIC, Carrefour, Accor, Club Med, etc.) le modèle dit « des 7P » propose d'enrichir le modèle de base en ajoutant d'autres catégories comme :Process : caractérisée par l'interaction avec le client (ex : accueil, conseil, horaires d'ouverture, etc.).People : capacités de la force de vente (ex : présentation, formation, etc.).Physical evidence ou « Physical support » (support physique) : composantes matérielles du magasin (ex : vitrine, organisation des rayons, etc.), du service (ex : rapport annuel pour un expert-comptable, relevé de compte, carnet de chèque, ou carte bancaire pour une banque), ou identifiant le personnel, qui fait partie intégrante de la production pour un service (ex : uniforme ou tenue du personnel).Cible (Target Market) Controverses sur l'enrichissement du modèle des 4P Les défenseurs du modèle dit des 4P doutent de la validité de ces compléments. Pour eux, le « Process » est essentiellement un problème lié au produit. Le « People » est essentiellement lié tantôt à la production, donc au produit, ou parfois à la promotion. Le « Physical evidence » fait partie de la promotion.Pour Robert Lauterborn (1990), les 4P ont fait leur temps, les C-words prennent le dessus :Une nouvelle formulation s'impose, celle des 4C :P ? CONSOMMATEUR : « Oubliez le produit. Étudiez les désirs et besoins du consommateur. (...) On ne peut vendre que ce qu'un consommateur veut acheter. La frénésie d'achat est terminée. (...) Il faut le séduire en le considérant un par un, avec le quelque chose qu'il désire de façon particulière »P ? COÛT : « Oubliez le prix. Comprenez le coût pour le consommateur de la satisfaction de son désir ou besoin. L'enjeu si vous vendez des hamburgers n'est pas d'offrir un hamburger de plus pour quelques centimes de plus ou de moins. C'est le coût du temps nécessaire pour se rendre chez vous, de la conscience de manger de la viande, et peut-être le coût de la culpabilité de ne pas bien traiter ses enfants. La valeur n'est plus dans le fait d'offrir le plus gros hamburger pour le plus petit prix. C'est une équation complexe induisant autant de solutions valides que de catégories de clients. »P ? COMMODITÉ : « Oubliez le placement. Pensez à la commodité d'achat. Les gens n'ont plus la nécessité d'aller quelque part dans cette époque où les catalogues, les cartes de crédit, les téléphones sont présents chez eux. Et lorsqu'ils décident de se déplacer, ils ne vont pas forcément chez Kroger's. (...) Pensez plus loin que ces nets et jolis points de vente que vous avez investis pendant des années. Efforcez vous de connaitre comment chaque sous-segment du marché préfère acheter et soyez présents partout.».P ? COMMUNICATION : « Oubliez les promotions. Le mot est « communication » . Toute bonne publicité crée un dialogue. La promotion des années 1960 vient de nous, va vers l'extérieur, est manipulatrice. La communication des années 1990 vient du client, se veut coopérative. Ce contraste est en fait la différence fondamentale entre les 4P qui ont servi si bien et si longtemps et les 4C qui pourraient bien être la clé de succès, alors que nous quittons le second millénaire.».Koichi Shimizu (1972), les 4P ont fait leur temps, les C-words prennent le dessus :(C1):Corporation(C-O-S: Competitor-Organiztion-Stakeholder): Société – Le cœur de quatre Cs est la société (la compagnie et non l'organisation de profit). Parce que(organisation, concurrent, partie prenante) dans la société. Des questions à se poser au niveau stratégique... Make or Buy? Se recentrer sur son cœur d’activité SaaS ou PaaS/IaaS? Outil différentiant... Contrôle ou agilité ? Tout outsourcer ou ne rien outsourcer ? Créer une matrice personnalisée Control vs Agility (Behnia, et al., 2008).La boussole de consommateurs et de circonstances (l'environnement) est :(C6):Consommateur: – (L'aiguille de boussole au consommateur) Les facteurs rattachés aux consommateurs peuvent être expliqués par le premier caractère de quatre directions marquées sur le modèle de boussole. Ceux-ci peuvent être souvenus par les directions capitales, dorénavant le modèle de boussole de nom :N = Needs (les besoins)W = Wants (les envies)S = Security (la sécurité)E = Education (l'éducation de consommateur)[ Early adopters du Cloud Public ] Courbe d’adoption de l’innovation (Rogers, 1962). Une infrastructure (et applications) existante difficile à migrer ; Contexte social difficile (change management) ; Problématiques juridiques importantes (protection des données); Comformité ; Besoin de sécurité extrême (règles très dures) ; ou Pas d’activité saisonnière.(C7):Circumstances – (L'aiguille de boussole aux circonstances) En plus du consommateur, il y a des facteurs environnementaux externes incontrôlables différents encerclant les compagnies. Ici il peut aussi être expliqué par le premier caractère des quatre directions marquées sur le modèle de boussole :N = National et international (Politique, juridique et éthique) l'environnement,W = Weather (le Temps),S = E Social et culturel,E= Économique Ceux-ci peuvent aussi être souvenus par les directions capitales marquées sur une boussole. Spécificité des services La légitimité du modèle des 4P est aussi remise en cause au motif que ce modèle ne concernerait :que les produits et services destinés aux particuliersou plus radicalement qu'il ne pourrait valablement concerner les activités de service dans leur ensemble.Des auteurs, notamment Berry (1985), Eiglier et Langeard (1987) ou encore C. Lovelock (1996) ont proposé un nouveau modèle. Celui-ci tient compte des spécificités de la servuction (création d'un service) qui sont l'intangibilité, mais aussi l'hétérogénéité et le caractère périssable de ceux-ci ainsi que la coproduction. Cette dernière caractéristique, centrale, justifie que le client, participant à la production du service, soit particulière sensible au processus, aux évidences physiques perçues au cours de ce dernier et au comportement du personnel en contact. Ne pouvant évaluer a priori le service, de fait de ses attributs d'expérience voir de croyance, le consommateur va s'appuyer sur son expérience de service pour évaluer le service. Les modèles de servuction et le modèle 7P sont donc très complémentaire mais l'approche de Pierre Eiglier et E. Langeard permet de comprendre la dualité des services (Eiglier, 2004) et l'impossibilité de traiter séparément, dans les services les questions liées au marketing de celles liées à la production.La définition d'un positionnement au sein de la segmentation va nourrir l'image de marque de l'entreprise. Élaborée à partir de l'identité de la marque, définie comme une identité en mouvement, une nouvelle approche propose un marketing-mix élargi à 7 variables (5p2i) : politiques de produits/services, de tarification, de distribution, de communication, politique de partenariat, identité sensorielle et innovation,.Le modèle SAVE a été mis au point par Richard Ettenson, Eduardo Conrado, et Jonathan Knowles dans une étude intitulée « Rethinking the 4 P’s » publiée en janvier 2013 par Harvard Business Review. Il est présenté comme une évolution du modèle des 4P afin de couvrir les problématiques récentes du marketing B2B.SAVE est l’acronyme des termes suivants :Solution (Solution) : Présenter un produit par les besoins qu'il comble plutôt que par ses caractéristiques (fonctionnalités, avantages technologiques...).Acces (Accès) : Développer une présence multi-canal afin de considérer l'ensemble de l'expérience d'achat du client, plutôt que de se focaliser sur un seul canal ou lieu de vente.Value (Valeur) : Communiquer sur le bénéficie, la valeur apportée par le produit plutôt que d'expliquer son coût (production, concurrence).Education (Education) : Fournir des informations utiles et personnalisées aux besoins des clients à chaque point de contact plutôt que de s'appuyer sur une communication globale.Le Framework de Dawn Iacobucci  Pour elle, le marketing est une méthodologie dont l'algorithme est : 5C, STP, 7P's.Que l'on peut améliorer en PESTEL, 3C, STP, 7P'S Impact des NTIC dans la relation avec les clients Plus récemment le marketing-mix n'a pu échapper à l'impact des NTIC ( Nouvelles technologies de l'information et de la communication ). L'implication et la participation croissante du consommateur dans la relation d'échange via les techniques du Web 2.0 et plus particulièrement du Marketing 2.0 a ouvert un monde d'opportunités que le marketing-mix ne peut négliger. Opportunité d'un positionnement en termes de marque Côté entreprise, l'intérêt de la marque (marketing) s'est affirmé pour affronter des marchés :tantôt saturés, où il s'agit de se «démarquer» d'une concurrence indifférenciée et/ou de fidéliser une base client profitable grâce à « l'effet Loyauté » tantôt fortement innovants, où il faut réussir l'introduction d'un nouveau concept, qu'il s'agit de faire découvrir et agréer par un consommateur qu'il s'agit dans un premier temps de conquérir et puis de fidéliser lorsque les offreurs challengers se mettent à suivre le mouvement et font à leur tour des offres en vue de participer à la croissance du marché.Appelé aussi Retailing mix.Années 1960(en) E. Jerome McCarthy, Basic Marketing. A Managerial Approach, Richard D. Irwin, 1960.(en) Neil Borden, « The Concept of Marketing Mix », Journal of Advertising Research, June 1964, p. 2-7.Années 1980(en) Bernard Booms & Mary Bitner, Marketing Strategies and Organizational Structures for Service Firms, dans : James Donnelly and William George, Marketing of Services, American Marketing Association, 1981, p. 47-51. (Les 7P)(en) Koichi Shimizu (1989) Advertising Theory and Strategies , (Japanese) first edition, Souseisha Book Company in Tokyo.  (ISBN 4-7944-2030-7) C3034 P3980E) pp.63-102.Années 1990(en) Robert Lauterborn « New Marketing Litany; Four P's passe; C-words take over », dans : Advertising Age, October 1, 1990.(en) Steven Silverman, « An Historical Review and Modern Assessment of the Marketing Mix Concept », 7th Marketing History Conference Proceedings, Vol. VII, 1995.Philip Kotler et Bernard Dubois, « Le marketing mix du distributeur », dans : Marketing Management, 8e édition, Publi-Union, 1994, p. 542-548.(en) Christian Grönroos, « Quo Vadis, Marketing ? Toward a Relationship Marketing Paradigm » , Journal of Marketing Management, nov. 1994, p. 347-360.(en) Christian Grönroos, From Marketing Mix to Relationship Marketing: Towards a Paradigm Shift in Marketing, dans : Management Decision, Vol. 32 No. 2, 1994, p. 4-20.(en), Shostack,(en) Koichi Shimizu (1996) Symbiotic Marketing Strategies , (Japanese) First edition, Souseisha Book Company  (ISBN 4-7944-2158-3) C3034) pp.25-62.(en), Lutz, 2007.Années 2000(en)Koichi Shimizu (2003) Symbiotic Marketing Strategies , (Japanese) 4th edition, Souseisha Book Company  (ISBN 4-7944-2158-3) C3034) pp.25-62.(en), Lutz, 2007.(en) Marketing and the 7Ps. A brief summary of marketing and how it works, Chartered Institute Marketing, 2009Philippe Villemus, Le plan marketing à l'usage des entreprises, Eyrolles, 2008.[The 7Ps of marketing dans www.cim.co.uk/marketingresources, 2009 http://www.cim.co.uk/files/7ps.pdf](en) Dominici, 2009(en) Chai Lee Goi, « A Review of Marketing Mix : 4Ps or More ? » dans : International Journal of Marketing Studies, May 2009, pp. 2-15.2010Isabelle Calmé et autres, « Le marketing mix du point de vente », dans : Introduction à la gestion, 3e édition, Dunod, 2010, p. 128.2011(en) Brian Solis(2011) Engage!: The Complete Guide for Brands and Businesses to Build, Cultivate, and Measure Success in the New Web , John Wiley & Sons, Inc. pp. 201–202.Nathalie Van Laethem, « Comment le mix-marketing est passé de 4P à 10P », Le Blog de la Stratégie marketing, 20 mars 2011,(en) Walter van Waterschoot et Thomas Foscht « The marketing mix — a helicopter view », dans : Michael Baker and Michael Saren, Marketing Theory, Sage, 2011, p. 185-205.2012Véronique Méot, « Des 4P aux 6S… les mutations du marketing mix », dans : Marketing, no 163, 28/11/2012.Philip Kotler, Kevin Keller et Delphine Manceau, « Le marketing mix revisité », dans : Marketing Management, 14e édition, Pearson, 2012, p. 31.(en) Brian Monger, Time to Revisit the Marketing Mix?, Maanz International, 2012.2013(en) Richard Ettenson, Eduardo Conrado, and Jonathan Knowles, « Rethinking the 4 P's », dans : Harvard Business Review, January-February 20132014(en) Koichi Shimizu (2014) Advertising Theory and Strategies , (Japanese) 18th edition, Souseisha Book Company  (ISBN 4-7944-2132-X) C3034) pp.63-102.Gilles Bressy et Christian Konkuyt, Management et économie des entreprises, 11ed, chapitre 15, Éditions Sirey, 2014.Pamela Alison Meager, « Summary: The Concept of the Marketing Mix by Neil H. Borden », March 26, 2014.(en) Dawn Iacobucci, Marketing Management, Cengage Learning, 2014.2016(en)Koichi Shimizu (2016) Symbiotic Marketing Strategies , (Japanese) 5th edition, Souseisha Book Company  (ISBN 4-7944-2482-5) C3034) pp.25-62.(en) 7Cs Compass model(1979)in Japan(fr) L'arbre du marketing mix du 4P au 10PEtudes critiques du marketing Portail du management"
économie;"En économie, le secteur primaire est le premier secteur défini dans la loi des trois secteurs. Il est parmi les trois secteurs économiques définis dans la comptabilité nationale et regroupe les activités liées à l’exploitation de ressources naturelles : agriculture, sylviculture, pêche et activités minières. Le secteur primaire rassemble l'ensemble des activités qui produisent des matières premières non transformées.Le secteur primaire comprend l'agriculture, la pêche, l'exploitation forestière et l'exploitation minière. On désigne parfois les trois dernières par le terme « autres industries primaires ». Les industries primaires sont liées à l'extraction des ressources de la terre.Selon Fortune, le secteur de l’extraction représenterait 27 % de l'économie mondiale comprenant notamment les activités relatives à l’énergie ou les minières. Les vingt plus gros négociants du secteur ont engrangé $191 milliards de profit entre 2003 et 2012.C'est un processus par lequel les gens aménagent leurs écosystèmes pour satisfaire les besoins de leurs sociétés. Elle désigne l’ensemble des savoir-faire et activités ayant pour objet la culture des terres, et, plus généralement, l’ensemble des travaux sur le milieu naturel (pas seulement terrestre) permettant de cultiver et prélever des êtres vivants (végétaux, animaux, voire champignons ou microbes) utiles à l’être humain. En France En 1700, il fallait environ trois heures pour produire un kilogramme de blé, d'où la malnutrition et les famines. À cette période, 80 % de la population active travaillait dans l'agriculture; en 1880, il fallait encore un peu plus d'une heure ; en 1950, 30 minutes. Aujourd'hui, environ une minute. Cela explique l'évolution de la part de l'agriculture : en 1995, l'agriculture représentait en France 6 % de part de la population active ayant un emploi, contre 40 % en 1913[réf. souhaitée]. En 2008, l'agriculture en France pesait 3,5 % du PIB (2008), soit 66,8 milliards d'euros. En 2012, elle ne serait plus que de 2 % du PIB français. En Belgique En 1846, les cultivateurs représentent encore 52 % de la population économiquement active :en 1880, 22 % ;en 1913, 16 % ;actuellement 2 %.En 1846, intervient encore pour plus de 50 % dans le PNB :en 1880, 29 % ;en 1913, 15 % ;actuellement, 0,7 %. En Europe L'emploi dans le secteur agricole est en forte régression pour l'amont de la filière (agriculture) depuis plus d'un siècle, et dans l'UE27 il a encore diminué au XXIe siècle sous l'effet de l'industrialisation et de l'augmentation de la productivité ; selon Eurostat. Ceci correspond à la perte de 3,7 millions d’emplois à temps plein en 10 ans. L'emploi a ainsi baissé de 17 % dans l’UE152 et de 31 % dans les 12 États-membres (NEM122) ayant rejoint l’UE en 2004 et en 2007. En 2009, le secteur de l’agriculture employait dans l’UE27 l’équivalent de 11,2 millions de personnes travaillant à temps plein, dont 5,4 millions dans l’UE15 et 5,8 millions dans les NEM12. Dans le même temps (2000 ? 2009), le revenu réel moyen par actif a augmenté de 5 % (il a même doublé en Lettonie, Estonie et Pologne) de 2000 à 2009. Dans le monde En 2011, la production agricole mondiale était estimée à 4 949 milliards, soit 6,2 % de l'économie mondiale.Dans le secteur primaire, la section des mines est définie comme l'exploitation de différents roches ou minéraux.Secteur économiqueSecteur secondaireSecteur tertiaireSecteur quaternaire Portail de l’économie"
économie;"En économie, le secteur secondaire ou secteur industriel est le second secteur défini dans la loi des trois secteurs. Il est parmi les trois secteurs économiques définis dans la comptabilité nationale et regroupe les activités liées à la transformation des matières premières issues du secteur primaire (industrie manufacturière, construction)Ce secteur, même s’il représente une part relativement modeste du PIB des pays développés (par exemple 13,2 % aux États-Unis en 2006, 20,6 % en France en 2006 et 26,3 % en Suisse en 2005), est considéré comme stratégique ; il fournit des emplois d’ingénieur et fournit du travail de recherche et développement à des entreprises du secteur tertiaire.Selon la CIA, le secteur industriel représentait 30,7 % de l'économie mondiale en 2012. Mais, selon Fortune, le secteur industriel représenterait 13,2 % de l'économie mondiale en 2012 si l'on intègre les activités d'extraction au secteur primaire.Secteur économiqueIndustrieSociété industrielleSecteur secondaire en FranceLe secteur tertiaireSecteur quaternaire Portail de la production industrielle   Portail de l’économie"
économie;"Le secteur tertiaire produit des services, il fait partie du domaine de l'économie. C'est le troisième secteur défini dans la loi des trois secteurs. Il est parmi les trois secteurs économiques définis dans la comptabilité nationale et est de fait défini par complémentarité avec les activités agricoles et industrielles (secteurs primaire et secondaire respectivement).Le secteur tertiaire est composé du :tertiaire principalement marchand (commerce, transports, activités financières, services rendus aux entreprises, services rendus aux particuliers, hébergement-restauration, immobilier, information-communication) ;tertiaire principalement non marchand (administration publique, enseignement, santé humaine, action sociale).Dans les pays développés, c’est de loin le secteur le plus important en nombre d'actifs occupés. En 2012, le secteur tertiaire représentait près de 60 % de l'économie mondiale.La « tertiarisation » de l’économie pose des problèmes statistiques, conceptuels et méthodologiques : les notions de volume, de qualité et de productivité du travail sont probablement à revoir dans le « tertiaire moderne ». L'objet des études futures sera aussi d'examiner si les trois critères d'homogénéité (une part croissante de l'emploi, une relative insensibilité aux crises économiques, et surtout un progrès technique faible), sont aujourd'hui respectés dans un ensemble qui comprend 79 %[réf. nécessaire] de la population active française.Le secteur tertiaire de l'économie, généralement appelé secteur des services, est le troisième des trois secteurs économiques de la théorie des trois secteurs. Les autres sont le secteur secondaire (à peu près identique à celui de la fabrication) et le secteur primaire (matières premières).Le secteur des services consiste en la production de services au lieu de produits finis (en) . Les services (également appelés «biens immatériels») comprennent les soins, les conseils, l'accès, l'expérience et le travail affectif. La production d'informations a longtemps été considérée comme un service, mais certains économistes l'attribuent désormais à un quatrième secteur, le secteur quaternaire .Le secteur tertiaire de l'industrie implique la fourniture de services à d'autres entreprises ainsi qu'aux consommateurs finaux. Les services peuvent impliquer le transport, la distribution (en) et la vente de marchandises du producteur au consommateur, comme cela peut se produire dans la vente en gros et au détail, la lutte contre les ravageurs ou le divertissement. Les produits peuvent être transformés au cours du processus de fourniture du service, comme cela se produit dans l'industrie de la restauration. Cependant, l'accent est mis sur les gens en interagissant avec les gens et servant le client plutôt que des transformations les biens physiques.Il est parfois difficile de définir si une entreprise donnée fait partie intégrante du secteur secondaire ou tertiaire. Et ce ne sont pas seulement les entreprises qui ont été classées comme faisant partie de ce secteur dans certains régimes; le gouvernement et ses services tels que la police ou l'armée, et les organisations à but non lucratif telles que les organismes de bienfaisance et les associations de recherche peuvent également être considérés comme faisant partie de ce secteur.Afin de classer une entreprise en tant que service, on peut utiliser des systèmes de classification tels que la norme de classification industrielle standard internationale des Nations Unies, le système de codes de la classification industrielle standard (SIC) des États-Unis et son nouveau système de remplacement, le système de classification des industries de l'Amérique du Nord. (SCIAN), la nomenclature statistique des activités économiques dans la Communauté européenne (NACE) dans l'UE et des systèmes similaires ailleurs. Ces systèmes de classification gouvernementaux ont un premier niveau de hiérarchie qui indique si les biens économiques sont tangibles ou intangibles.Aux fins de la finance et des études de marché, des systèmes de classification basés sur le marché tels que le Global Industry Classification Standard et l'Industry Classification Benchmark (en) sont utilisés pour classer les entreprises qui participent au secteur des services. Contrairement aux systèmes de classification gouvernementaux, le premier niveau des systèmes de classification basés sur le marché divise l'économie en marchés ou industries fonctionnellement liés. Le deuxième ou le troisième niveau de ces hiérarchies indique alors si des biens ou des services sont produits.Les groupes sociaux non productifs les plus anciens sont les propriétaires fonciers, marchands, militaires et le clergés, etc. Ils établissent leur assise sur le foncier, moyen fondamental de production, sur la régulation, le contrôle et la commercialisation de ses produits, les derniers sur le spirituel, « l'établissement d'une relation au cosmos à travers les religionse »:« Les activités de services sont nées à partir du moment où les activités de survie de l'espèce humaine ont été assurées de façon suffisante pour permettre de dégager un surplus, même temporaire, susceptible d'être attribué à d'autres fonctions et, conjointement, à d'autres individus ou groupes humains, qui ont ainsi acquis la possibilité de s'abstraire, partiellement ou en totalité, des contraintes de la production. En fait, il y a une liaison directe entre le développement d'activités non directement productives et l'organisation d'une société permettant l'appropriation de ces activités par certaines classes sociales. »L'assise du pouvoir de ces classes sociales est toujours justifiée par le « service rendu », et par l'extraction d'une fonction confiée à un corps de « spécialistes », détenteurs d'une compétence qui est de fait déniée aux autres groupes sociaux. Les activités non directement productives se font dans une hiérarchisation croissante.Au cours des 100 dernières années, il y a eu un glissement substantiel des secteurs primaire et secondaire vers le secteur tertiaire dans les pays industrialisés. Ce changement s'appelle la tertiarisation. Le secteur tertiaire est désormais le plus grand secteur de l'économie du monde occidental, et c'est aussi le secteur qui connaît la croissance la plus rapide. En examinant la croissance du secteur des services au début des années 90, le mondialiste Ken'ichi Ohmae a noté le secteur tertiaire représenterait « 70 % de la force de travail aux États-Unis, 60 % au Japon et 50 % à Taïwan »:« In the United States 70 percent of the workforce works in the service sector; in Japan, 60 percent, and in Taiwan, 50 percent. These are not necessarily busboys and live-in maids. Many of them are in the professional category. They are earning as much as manufacturing workers, and often more. »Les économies ont tendance à suivre une progression de développement qui les fait passer d'une forte dépendance à l'agriculture et à l'exploitation minière au développement de l'industrie manufacturière (par exemple, automobiles, textiles, construction navale, acier) et finalement à une structure davantage axée sur les services. La première économie à suivre cette voie dans le monde moderne a été le Royaume-Uni. La vitesse à laquelle d'autres économies ont fait la transition vers des économies de services (ou « post-industrielles ») a augmenté avec le temps.Historiquement, le secteur manufacturier avait tendance à être plus ouvert au commerce international et à la concurrence que les services. Cependant, avec une réduction spectaculaire des coûts et des améliorations de la vitesse et de la fiabilité dans le transport des personnes et la communication de l'information, le secteur des services comprend désormais une partie de la concurrence internationale la plus intense, malgré un protectionnisme résiduel.Vous trouverez ci-dessous une liste de pays par production de services aux taux de change du marché en 2016:Les prestataires de services sont confrontés à des obstacles dans la vente de services auxquels les vendeurs de biens sont rarement confrontés. Les services sont intangibles, ce qui rend difficile pour les clients potentiels de comprendre ce qu'ils recevront et quelle valeur cela représentera pour eux. En effet, certains, comme les consultants et les prestataires de services d'investissement, n'offrent aucune garantie de la valeur pour le prix payé.Étant donné que la qualité de la plupart des services dépend en grande partie de la qualité des personnes qui fournissent les services, les « coûts de personnel » représentent généralement une fraction élevée des coûts des services. Alors qu'un fabricant peut utiliser la technologie, la simplification et d'autres techniques pour réduire le coût des produits vendus, le fournisseur de services est souvent confronté à un schéma constant d'augmentation des coûts.La différenciation des produits est souvent difficile. Par exemple, comment peut-on choisir un conseiller en placement plutôt qu'un autre, étant donné qu'ils sont souvent perçus comme fournissant des services identiques ? La facturation d'une prime pour les services n'est généralement une option que pour les entreprises les plus établies, qui facturent des frais supplémentaires en fonction de la reconnaissance de la marque.Des exemples d'industries tertiaires peuvent inclure:TélécommunicationHôtellerie TourismeMédias de masseSoins de santé/hôpitauxSanté publiquePharmacieTechnologie de l'informationGestion des déchetsConsultantJeu d'argentCommerce de détailBien de grande consommation (FMCG)FranchisageImmobilierÉducationServices financiersBanqueAssuranceInvestment management (en)Services professionnels (en)Services juridiques (en)Conseil en managementTransportÉducationAprès s'être développé jusqu'en 1960 selon un rythme annuel moyen de 1 % en France, l'emploi des branches tertiaires progresse très vivement de 1960 à 1980 (2 % par an), puis encore assez fortement de 1980 à 2000 (+1,7 %). Il ralentit ensuite entre 2000 et 2011 (+0,9 %), avec une quasi-stagnation de 2008 à 2011, voire une légère baisse dans certains services traditionnels aux ménages ou les télécommunications. Ceci est le résultat de deux tendances : une accélération de la croissance de la demande intérieure tertiaire (+ 4,3 % par an en volume entre 1959 et 2012) et des gains de productivité du travail plus faibles que dans le reste de l'économie (+ 2,5 % par an dans les services marchands contre + 4,5 % dans l’industrie). Face à une demande croissante, un secteur dont la productivité progresse relativement plus lentement ne peut que se développer en terme d'emploi. Depuis le début du siècle dernier, le progrès technique a toujours été plus faible dans le tertiaire que dans les autres secteurs. Expliquer « l'explosion » récente de l'emploi tertiaire revient donc à analyser les raisons de l'accélération de la demande tertiaireLa balance commerciale des services est excédentaire en France. Finance On parle dans ce cas là, des garages de voitures ou encore des magasins de ventes qui comprend en particulier le secteur bancaire et celui de l'assurance au point d'être parfois désigné par « Secteur Banque Assurance ». Information  Sécurité Cette branche du secteur tertiaire comprend notamment les activités de police, de milice et d'armée assurant la sécurité des biens et des personnes. Justice Aux États-Unis, 2 % du PIB est obtenu en justice. Bénévolat Selon une étude de l’Insee parue en 2004, le bénévolat représentait aux alentours de 1 point de PIB.Le tertiaire « supérieur » (ou mixte) regroupe les « métiers du savoir » qui fournissent aux entreprises et aux particuliers des prestations intellectuelles complexes. Essentiels au fonctionnement de l’économie et au développement stratégique des entreprises, élément clef du rayonnement et de l’attractivité du territoire, ces métiers constituent également par eux-mêmes un secteur économique majeur, en fort développement. Recherche et éducation Le secteur tertiaire, privé et public, via ses infrastructures (bâtiments tels que bureaux, hôtels, commerces, d'enseignement et les bâtiments administratifs, réseaux internet, serveurs..) est devenu un grand consommateur d'énergie et de foncier ; En France, à la suite de la loi Grenelle 2 de 2010, un décret annoncé pour fin 2012 mais un temps repoussé par le Conseil d’État avant d'être publié en mai 2017 et entrant en vigueur le 11 mai 2017, afin de « favoriser l'efficacité et la sobriété énergétiques », impose aux « collectivités territoriales services de l'Etat, propriétaires et occupants de bâtiments à usage tertiaire privé, professionnels du bâtiment, maîtres d'ouvrage, maîtres d'œuvre, bureaux d'études thermiques, sociétés d'exploitation, gestionnaires immobiliers, fournisseurs d'énergies » une « obligation d'amélioration de la performance énergétique dans les bâtiments à usage tertiaire », sur la base d'une « étude énergétique portant sur tous les postes de consommations du bâtiment », avec un niveau d'économie d'énergie à atteindre avant 2020 et « un ou plusieurs scénarios permettant de diminuer, d'ici 2030 » la consommation d'énergie. Un observatoire doit recueillir les données permettant d'évaluer les résultats et de mettre à jour les données. L'étude énergétique est accompagnée de propositions de travaux d'économie d'énergie et de « recommandations hiérarchisées selon leur temps de retour sur investissement » avec présentation des « interactions potentielles entre ces travaux ». Ce décret permet de mutualiser l'obligation sur l'ensemble d'un patrimoine, et prévoit le cas d'un changement de propriétaire ou de preneur (un dossier dédié sera annexé au contrat de vente ou de bail). Il demande aussi une sensibilisation des occupants au thème des économies d'énergie. La « non-atteinte » des objectifs malgré les actions et travaux entrepris devra pouvoir être justifiée auprès des services de l’État. Un propriétaire d'un ensemble de bâtiments ou de parties de bâtiments concernés peut remplir globalement ses obligations sur l'ensemble de son patrimoine.Michel Braibant, De la désindustrialisation à la tertiairisation, vers un mélange des genres, Paris, Société des écrivains, 2 mai 2015, 188 p. (ISBN 9782342034738).Secteur économiqueBranche d'activitéSecteur quaternaireTertiarisation du travailSecteur bénévoleSociété post-industrielleEffet de structure de fret Portail de l’économie"
économie;"Un secteur économique, secteur d'activité ou secteur d'activité économique est un ensemble d’activité économique, ayant des traits similaires. C'est également une subdivision macroéconomique de l’économie, regroupant l’activité des entreprises qui appartiennent à une même catégorie. Traditionnellement la répartition de l’ensemble de l’activité économique est répartie en trois grands secteurs économiques (primaire, secondaire, tertiaire). Cette classification des différentes activités à vu le jour la première fois en 1947 et c'est le statisticien et économiste Colin Clark qui l'a abordé.L'activité d'un secteur d'activité économique n'est pas tout à fait homogène et comprend des productions ou services secondaires qui relèveraient d'autres items de la nomenclature que celui du secteur considéré, au contraire d'une branche d'activité qui regroupe des unités de production homogènes. Par ailleurs, le terme secteur professionnel ou industriel regroupe lui aussi deux définitions :celle de multiples domaines d’activité économique, et pour laquelle chaque secteur professionnel regroupe des familles de métiers assez proches pour être considérées comme un appareil de production unique (par exemple, le « secteur du bâtiment, de la pêche, du textile et de la confection, de la banque et des assurances),celle de branches industrielles d'activité (code APE), et pour laquelle chaque secteur professionnel regroupe les entreprises ou les administrations qui sont assujetties à une même règlementation sociale, fiscale et professionnelle (par exemple de la sidérurgie, du bâtiment et des Travaux publics, du Commerce et de la grande distribution, de la Fonction publique territoriale). Leur regroupement n'est pas non plus arbitraire.Allan Fisher, Colin Clark et Jean Fourastié ont défini trois secteurs économiques principaux, selon la nature de l'industrie :le secteur primaire concerne la collecte et l'exploitation des ressources naturelles (matériaux, énergie, et certains aliments) ;le secteur secondaire implique les industries de transformation des matières premières ;le secteur tertiaire regroupe les industries du service (essentiellement immatériel : conseil, assurance, intermédiation, formation, études et recherche, administration, services à la personne, sécurité, nettoyage, etc.).Cette classification n'est pas rigide, l'agriculture par exemple ayant été à l'origine classée comme du secteur secondaire (le cultivateur transforme des graines en produits consommables, par exemple), par opposition à la chasse et la simple cueillette.Le secteur primaire correspond aux activités liées à l'extraction des ressources naturelles. Il comprend l'agriculture, la pêche, l'exploitation forestière et l'exploitation minière. On désigne parfois les trois dernières industries par « autres industries primaires ».Le secteur secondaire correspond aux activités liées à la transformation des matières premières, qui sont issues du secteur primaire. Il comprend des activités aussi variées que l’industrie du bois, l’aéronautique et l’électronique, le raffinage du pétrole, la production industrielle, la construction...Le secteur tertiaire regroupe toutes les activités économiques qui ne font pas partie des deux autres, essentiellement des services. Par exemple, le conseil, l’assurance, l'enseignement, la grande distribution, le tourisme, la restauration et les agences immobilières font partie du secteur tertiaire.Le décalage progressif des activités vers le secteur tertiaire (théorie du déversement développée par Alfred Sauvy, théorie des vagues de développement d'Alvin Toffler) a accru le nombre de « travailleurs intellectuels » selon la définition de Peter Drucker. Mais cet enrichissement des tâches ne s'est pas pour l'instant produit de façon massive, contrairement à ce que certains avaient cru vers les années 1960. Certains emplois de vigile, employé de guichet, gardien d'immeuble ou caissière de supermarché appartiennent bien au tertiaire, sans nécessairement représenter un gain en qualité de vie.Cela maintient certains débouchés pour les personnes peu qualifiées, encore que l'objectif éducatif dans de nombreux pays, y compris des pays émergents, soit d'accroître les qualifications et les capacités créatives. Cet objectif est considéré crucial pour faire face aux évolutions concurrentielles dans le cadre de l'économie à la fois mondialisée et tournée de plus en plus vers la connaissance (économie du savoir). Cette orientation vers le tertiaire et la technologie fait que c'est vers des bassins de main d'œuvre peu qualifiée et peu rémunérée et encore peu touché par le tertiaire que se délocalisent certains emplois (mais, il est vrai, pas l'essentiel de la valeur ajoutée).Il importe de distinguer secteur global d'une entreprise et répartition des activités à l'intérieur de celle-ci. Une entreprise du secteur secondaire (fabrication de lingots d'acier, par exemple) doit bien par la force des choses posséder des services administratifs, qui font pour leur part partie du tertiaire. Des exemples classiques des écoles de commerce sont les anciennes marques Téléavia et Caravanair, difficiles à imputer à un des secteurs plutôt qu'à l'autre.On parle parfois de secteur quaternaire qui regrouperait les industries hi-tech, (technologies informatiques, aérospatiale (lancement de satellites), bioindustrie, etc.) et les services très sophistiqués (recherche et éducation de pointe, conseil stratégique, ingénierie financière, médecine de pointe, etc.) généralement pour les pays les plus industrialisés (États-Unis, Union européenne, Japon, etc.).On peut distinguer :d'une part, les entreprises commercialisant des services immatériels (70 % du PIB) de celles commercialisant des produits matériels (30 %) ;et, d'autre part, celles commercialisant ces services ou ces produits auprès d'autres entreprises de celles les commercialisant directement ou indirectement auprès du grand public.Ceci conduit à distinguer quatre grandes catégories de secteur d'activité économique :Celle des entreprises de service grand public : banque Assurance, croisières, divertissement, transport aérien, transport ferroviaire, etc.Celle des entreprises de produits grand public : agroalimentaire, compagnie pétrolières ; cosmétique, constructeurs automobiles, électronique grand public, luxe, pharmacie, etc.)Celle des entreprises de services industriels : courrier et livraison ; publicité ; technologie et services informatiques, transports maritimes, etc.Celle des entreprises de produit industriels : aéronautique, aérospatiale et défense ; bâtiment, construction et travaux publics ; éoliennes ;  chimie ; matériel informatique ; santé ; etc.Les Nomenclatures des secteurs économiquesLa nomenclature d'activités française (NAF)La Classification générale des activités économiques (NOGA), en SuisseSecteur d'utilisation de l'énergieSecteurs institutionnels, utilisés dans les comptabilités nationales.Division du travailActivité économiqueAnalyse sectorielleBranche d'activitéClassement mondial des entreprises leader par secteurClassification type des industriesGlobal Industry Classification StandardNorth American Industry Classification SystemSystème de classification des industries de l'Amérique du NordActifs occupés selon le sexe et le secteur d'activité, en France, InseeNomemclature des secteurs d'activité - NAF rév. 2, 2008, en France, InseeMetadata Nomenclature statistique des activités économiques dans la Communauté européenne, Rév. 2 (2008)Détail Code : G Description :  Commerce Portail de l’économie"
économie;"En économie, un service est une prestation qui consiste en « la mise à disposition d'une capacité technique ou intellectuelle » ou en « la fourniture d'un travail directement utile pour l'usager, sans transformation de matière ». Les services correspondent au secteur tertiaire.Fournir un service correspond à une production économique de nature particulière puisqu'elle ne consiste pas en la fourniture d'un bien tangible à un client. De plus, les services — étant consommés dans le même temps nécessaire pour les produire — sont considérés comme n'étant pas « stockables ».Christopher Lovelock distingue quatre grandes catégories de service (ou de prestation ou de servuction). Il les différencie d'une part par la nature de la prestation : l'action concrète, tangible celle d'un kinésithérapeute ou d'un coiffeur qui fait physiquement quelque chose ou bien l'action psychologique, intellectuelle, immatérielle, d'un professeur, d'un psychothérapeute ou d'un expert-comptable ; et d'autre part, par l'objet du service, ce sur quoi il porte : des personnes (leur corps ou leurs esprit) ou des choses (tangibles ou intangibles comme les chiffres). Cela donne une matrice à quatre composantes :les services concrets rendus aux personnes : les coiffeurs, les transports de personnes, les soins médicaux et chirurgicaux, etc. ;les services concrets portant sur des choses : le transport de fret, le nettoyage à sec, la réparation automobile, le dépannage domestique ou professionnel (ascenseur, etc.) ;les services abstraits s'adressant à l'intelligence ou au sens : enseignement, divertissement ;les services portant sur des entités intangibles, numériques : compte bancaire, crédit, assurances.La production de services est devenue l'activité de production principale des économies développées. Elle est caractérisée par une gestion particulière de la production, celle-ci étant immatérielle, donc non stockable et réunissant simultanément consommation et production. Cela implique généralement une participation du client à la production. Les particularités de l'analyse de la valeur, le caractère précaire de l'innovation dans les services, le découpage classique de la production en front office (en interaction avec le client) et back office (en l'absence du client) demandent des méthodes et des outils différents de la production industrielle.La part des services (représentée grosso modo — pour les raisons évoquées ci-dessus — par le secteur tertiaire) augmente tant en chiffre d'affaires qu'en effectif employé dans la production, la consommation finale et la consommation intermédiaire (voir théorie du déversement énoncée par Alfred Sauvy).On distingue les services marchands, qui sont facilement procurables sur le marché, et les services non marchands, dont l'obtention s'opère dans des cadres et selon des règles plus spécifiques. Services non marchands Les prestations de ces services ne sont pas fournies contre rémunération (régime de la gratuité totale ou du paiement d'une contribution symbolique, ou par intervention d'un tiers payant) :justice ;maintien de l'ordre public (forces de l'ordre) ;défense nationale ;éducation publique et enseignement supérieur ;santé publique ;services sociaux et organismes caritatifs. Services marchands La prestation de ces services est obtenue moyennant un prix, généralement fixé librement par le marché :Écoles et organismes de formation privés ;services financiers, de banque ou d'assurances ;conseil ou services informatiques ;télécommunications ;transports et logistique ;prestations d'études ou de recherches appliquées.Un service public est « une activité d'intérêt général, assurée sous le contrôle de la Puissance publique par un organisme public ou privé bénéficiant de prérogatives lui permettant d'en assurer les obligations (notamment en matière de continuité et d'égalité) et relevant de ce fait en tout ou partie d'un régime de droit administratif (mission dite de service public) ».Toute autre activité de service – qui ne relève pas de l'exception définie par la catégorie précédente – doit être considérée comme une activité de nature privée. En France s'applique le principe constitutionnel de la liberté du commerce et de l'industrie. Services publics Les services publics sont les activités jugées utiles par et pour la collectivité et qui sont assurées dans un cadre particulier. Ce qui signifie qu'elles peuvent être exercées même lorsque les critères de simple rentabilité financière devraient conduire à leur abandon. Ils comprennent :les services publics administratifs (SPA) comme la perception des impôts ;les services publics sociaux comme le service des allocations familiales ;les publics industriels ou commerciaux (SPIC) comme la distribution du gaz ;les services publics professionnels comme l'Ordre des Médecins (qui assure la réglementation et la discipline de la profession médicale). Services privés Dans les pays développés les plus tertiarisés (on parle parfois d'« économie post-industrielle »), comme en France, les services représentent jusqu'à plus de 75 % de la production nationale (mesurée par le PIB)[réf. nécessaire] et sont devenus leur principal moteur de croissance économique.Cette évolution peut toutefois être légèrement relativisée par le fait que les entreprises industrielles externalisent une partie de leur processus de production en faisant appel à des prestataires qui sont classés dans les entreprises de services mais qui participent à la production industrielle.Les services génèrent en outre une grande partie du capital immatériel des entreprises.L'économie des services comporte des enjeux considérables de développement durable.Selon Jean Gadrey, « pour construire une économie écologique des services, il faut d'abord s'intéresser aux bilans écologiques complets de ces activités. Il apparaît alors que l'immatérialité parfois supposée des services est un mythe ». Selon le même auteur, l'activité de services comporte des externalités environnementales importantes qui auraient besoin d'être internalisées.Un exemple d'enjeu dans le secteur des services est constitué par la dématérialisation, qui se fait quelquefois dans un objectif de développement durable. La dématérialisation permettrait ainsi d'économiser du papier, voire de passer au « zéro papier ». Dans les projets de dématérialisation, on utilise massivement des services. Mais établir le bilan global d'une dématérialisation n'est pas aisé ! La dématérialisation agit sur les flux de gestion entre partenaires, pas sur la qualité environnementale des biens vendus.Les enjeux environnementaux concernent la pression environnementale et la contribution des services à l'émission de gaz à effet de serre.Une étude de l'IFEN montre que la pression environnementale directe et indirecte des services n'est pas négligeable.Jean-Marc Jancovici souligne également la contribution des services dans les émissions de gaz à effet de serre à travers ce qu'il appelle les émissions intermédiaires.Certains services sont émetteurs directs (transports, logistique), d'autres sont émetteurs indirects de gaz à effet de serre.Les enjeux sociaux sont également importants :emploi ;formation ;santé et sécurité au travail (stress…) ;salaire ;lutte contre la discrimination.Une partie des services est constituée par les sociétés de conseil et les sociétés informatiques qui fournissent des prestations de service aux entreprises de l'industrie (ou d'autres entreprises de services).Dans les faits, une très faible partie de l'activité des sociétés de conseil s'est orientée vers du conseil en développement durable. Pour les entreprises qui font du conseil en développement durable, encore faut-il que ce concept ne soit pas déformé (voir limites et dérives du concept de développement durable) et que l'entreprise cliente considère le développement durable comme stratégique. La recherche du profit à court terme éclipse trop souvent les questions de fond.Pour les SSII, la durabilité est souvent vue exclusivement sous l'angle du recyclage des équipements informatiques, pas sous l'angle de la gestion. Certes, la durée de vie des matériels et des matériels et logiciels est très courte en informatique. Par ailleurs, il existe de gros problèmes de compatibilité et d'interopérabilité entre systèmes.Les enjeux du développement durable sont porteurs de nouveaux modèles économiques, or les sociétés de conseil et les sociétés informatiques n'ont pas réellement revu leur modèle d'entreprise en fonction de ces enjeux. Selon Jean-Louis Lequeux, alors que le business model « classique » se vit à deux (l'acheteur, le vendeur), les modèles durables et éthiques se conjuguent à trois. Dans un cas comme l'autre, les deux parties reconnaissent à la fois l'existence, ou plutôt le droit à l'existence et le droit au respect, d'une troisième partie :notre planète Terre pour le « durable » ;les hommes, le tissu social et les économies locales pour « l'éthique ».Le business model doit donc tenir compte des attentes des parties prenantes.Gilles Bressy, Christian Konkuyt, Management et économie des entreprises, 12e éd., Sirey, 2018, chap. 17.(en) Christopher H. Lovelock, Jochen Wirtz, Services Marketing. People, Technology, Strategy, 7th Edition, Prentice Hall, 2010.(en) Jochen Wirtz, Patricia Chew, Christopher Lovelock, Essentials of Services Marketing, 2nd Edition, Pearson, 2012.Christopher LovelockPrestataire de servicesEngagement de serviceAccord de serviceDéfinition de « services », site de l’INSEE Portail de l’économie   Portail de la société"
économie;"Les services à la personne regroupent les activités liées à l'assistance des personnes dans leurs tâches quotidiennes à leur domicile. Ils peuvent concerner les services à la famille (garde d'enfants, soutien scolaire, etc.), les services de la vie quotidienne (ménage, jardinage, etc.) ou les services aux personnes fragiles, personnes âgées, enfants de moins de trois ans, personnes handicapées, qui ont besoin d'une aide à la vie quotidienne. Le secteur des services à la personne est une qualification juridique française ouvrant droit à un crédit d'impôt et des exonérations de cotisations sociales. Dans les années 50, les arts ménagers comprennent l'ensemble des techniques qui, dans le cadre du foyer familial, permettent de soutenir la vie physique et d'alimenter la vie intellectuelle. Les arts ménagers apparaissent comme un élément des sciences économiques avec la Production domestique et comme un élément des sciences biologiques et mentales.Une série de lois ont permis d'accélérer l'évolution des métiers liés aux services à la personne. En 1991, sous le gouvernement Édith Cresson, une loi offre une réduction d'impôts de 50 % sur le salaire versé à une personne pour un service à domicile. En 1996, sous le second gouvernement Alain Juppé, une nouvelle loi autorise les entreprises à investir le secteur, en plus des employés indépendants et des associations. Depuis 2000, la TVA appliquée aux entreprises du secteur passe de 19,6 à 5,5 % pour les activités d'assistance aux personnes dépendantes (personnes âgées, personnes handicapées, enfants de moins de 3 ans).L'expression « services à la personne » a été créée par la loi du 26 juillet 2005 n°2005-841, et intégrée dans l'article D.129-35 devenu article D.7231-1 du code du travail. Permettant une simplification des procédures administratives, les services à la personne ont une agence spécifique, l'agence nationale des services à la personne, elle a été dissoute par le décret du 2 juillet 2014 , un numéro de téléphone propre, le 39 39, et un régime fiscal et social particulier, comprenant des avantages sociaux et fiscaux et un mode de paiement particulier le chèque emploi service universel (CESU).En 2021 les députés Bruno Bonnell et François Ruffin, au cours d'une mission parlementaire qu'ils ont mené, proposent plutot l'expression « métiers du lien » pour désigner ces professions. Le terme de services à la personne est trop lié selon eux à des politiques de l'emploi sans réflexion, et le terme de services à domicile cacherait le fait que beaucoup de ces activités se déroulent hors d'un domicile, et n'est pas en relation avec les caractéristiques du travail fourni. Dans ces domaines, selon eux, la compétence et le service s'articulent autour du lien, non du bien. ,Cette mission propose quatre métiers comme les plus représentatifs des enjeux des liens humains dans la société française : accompagnant des élèves en situation de handicap, nourrice, auxiliaire de vie sociale (également nommé « aide à domicile ») et animatrice du périscolaire (voir Accueil périscolaire). Ces métiers, selon les rapporteurs, n'ont pas vraiment de statuts ni de salaires, sont peu visibles et très précarisés, mais essentiels pour la construction de la société française. En 2021, le vieillissement de la population, la hausse du taux d'activité des femmes et l'augmentation de la fragmentation des familles induiront une forte croissance de ces métiers. France Stratégie indique que le métier d'aide à domicile est celui qui a la plus forte croissance en France depuis 10 ans. Une projection sur 2030 prévoit que ce métier sera tenu à ce moment-là par 862 000 personnes, ce qui correspondra à 160 000 créations de poste. Si les conditions d'exercice de ce métier sont améliorées, ces professionnelles pourront atteindre le million. Tous les métiers du lien favoriseraient fortement la croissance économique générale s'ils étaient mieux construits. Ils déplacent la notion de modernité vers la relation humaine. Pour répondre à ces tensions, le gouvernement a annoncé qu'il allait promouvoir une réforme de l'immigration, de façon que beaucoup de femmes étrangères puissent exercer ces métiers. Mais, en l'état, cela revient à enfermer ces femmes dans une précarité durable. Ces métiers sont, en effet, fortement précarisés. Le marché des services à la personne est réparti entre plusieurs acteurs :des employeurs particuliers ;des organismes spécialisés ;des personnes morales ;des entrepreneurs individuels ;le travail dissimulé.Ce dernier, de par sa nature, est difficile à chiffrer. Les activités sont exercées selon plusieurs modes, l'emploi direct entre personnes (employeurs particuliers et salariés), le mode mandataire dans lequel un organisme assure la médiation (recrutement, diverses démarches administratives, [...]) pour le compte du particulier employeur et le mode prestataire dans lequel un organisme, personne morale ou entrepreneur individuel, intervient auprès d'un client dans le cadre d'une prestation de service, réalisée par le chef d'entreprise ou un de ses salariés.Plusieurs rapports affirment que les services à la personne, surtout depuis les différentes lois en leur faveur, sont en hausse soutenue depuis plusieurs années, passant de 300 millions d'heures travaillées en 1994 à plus de 700 millions en 2006,. Il faut cependant prendre en compte que la palette de services pris en compte s'est étendue.La répartition entre les différents acteurs est assez inégale, et en évolution constante. Ainsi, les particuliers employeurs de salariés indépendants représentaient en 1994 96 % des emplois déclarés du secteur, alors qu'en 2007, ils ne représentent que 74 %, même si le nombre d'heures travaillées a largement augmenté sur cette période. Les emplois restants se répartissent entre les associations de prestations et les entreprises. Ces dernières, bien que ne représentant qu'un faible pourcentage des prestations, sont en nette augmentation depuis ces dernières années, passant de 710 structures actives en 2004 à plus de 4 500 en 2007.Le Plan Borloo a fixé le maintien de la TVA à 5,5 % pour les professionnels et, pour les particuliers, une réduction d’impôt sur le revenu de 50 % des sommes versées à un salarié à domicile.Le sénateur centriste Joseph Kergueris présente en septembre 2010 un rapport pour le Sénat selon lequel les services à la personne ont coûté 16 milliards d'euros en 2010 contre 10 milliards en 2005.À la suite de la loi de finances rectificative pour 2011 entrée en vigueur le 1er janvier 2012, la TVA à taux réduit passe de 5,5 à 7 %, sauf pour les services liés aux « gestes essentiels » de la vie quotidienne des personnes en situation de dépendance qui restent à 5,5 %. Par conséquent le secteur des services à la personne est aussi touché.Depuis 2014, le taux de TVA réduit est passé à 10 % et certains services ne bénéficie plus de ce taux réduit tel que le Petit bricolage ou Petit jardinage.En 2019, le gouvernement décide de supprimer l'exonération de cotisations sociales sur l'emploi à domicile dont bénéficiaient les personnes âgées de plus de 70 ans. Selon les données de la Fédération des particuliers-employeurs, 700.000 retraités seront concernés.En 2009, 4 millions de foyers employaient des intervenants à domicile sous le statut particuliers-employeurs selon l'Agence nationale des services à la personne. 92 % des salariés des services à la personne sont des femmes et 74 % d'entre eux sont non diplômés ou ont un diplôme d'études secondaires sans baccalauréat.En 2011, le poids économique du secteur s’élevait à 17,4 milliards d'euros et 1,82 milliard d'heures prestées. 1,2 million de personnes étaient dépendantes selon la Direction de la recherche, des études, de l'évaluation et des statistiques.En 2012, deux millions de salariés âgés en moyenne de 46 ans sont employés dans ce secteur contre 600 000 en 1994. Selon le Centre d'analyse stratégique (CAS), 660 à 825 000 emplois seront créés d'ici 2030. 25 000 associations, entreprises et établissements publics sont actuellement agréés contre 5 500 en 2006. Selon l'enquête réalisé par le CROCIS, 89 % des chefs d'entreprises de services à la personne déclarent rencontrer des difficultés de recrutementEn 2016, l'assurance-maladie recense dans le secteur de l'aide et des services à la personne 94,6 accident du travail pour 1 000 salariés, soit près du triple de la moyenne nationale. Le taux d'absentéisme est de 30 % plus important que dans l'ensemble du secteur de la santé.Béatrice Belle, syndicaliste de la Confédération générale du travail (CGT), souligne qu'en ce qui concerne les aides à domiciles : « Nous avons des amplitudes horaires qui nous font travailler parfois 13 heures par jour, sept jours sur sept, notre employeur peut nous rappeler n’importe quand, même sur nos congés, pour remplacer au pied levé une collègue. Bien sûr, nous ne sommes rémunérées que sur nos interventions, et pas sur les trous entre deux visites. » En outre : « Le turnover est important car les conditions ne sont plus supportables, certaines intervenantes prennent plus de médicaments que les bénéficiaires qu’elles ont en charge ! Ce n’est pas une vie, mais de la survie. » Les aides à domiciles sont également confrontés au problème du coût des déplacements. D'après, Loïc Le Noc, chargé de la branche aide à domicile pour la CFDT : « Aujourd’hui, pour faire un kilomètre, on donne 35 centimes à chaque auxiliaire et ça c’est dans la meilleure des trois conventions. Mais le coût de revient du moindre véhicule est supérieur à 40 centimes. Donc elles s’appauvrissent littéralement à chaque kilomètre… À chaque fois que l’on a tenté de négocier une revalorisation de cette indemnité, les services du ministère nous ont dit niet. »Le taux de syndicalisation est très faible dans la profession d'aides à domicile. Parmi les raisons qui expliquent cette situation figurent les bas salaires, qui rendent difficile pour les aides à domicile d'en consacrer une part au militantisme syndical. D'après une déléguée de la CGT : « Lorsque j'explique à mes collègues que c'est 1 % du salaire mensuel, elles me répondent ""tu as vu les salaires qu'on a ?"" La plupart sont à temps partiel et n'ont plus rien à la fin du mois. Elles préfèrent consacrer 8 euros à nourrir leur famille plutôt qu'à un timbre syndical. » En outre les aides à domicile travaillent seuls, ce qui ne facilite pas l’investissement dans un mouvement collectif pour défendre des revendications.La liste de ces activités fait l'objet d'un décret. Le décret no 2016-750 du 6 juin 2016 fixe la liste des activités mentionnées à l’Article L. 7231-1code du travail.Ces activités sont les suivantes :Entretien de la maison et travaux ménagers ;Petits travaux de jardinage ;Prestations de petit bricolage, dites « hommes toutes mains » ;Garde d'enfant à domicile ;Soutien scolaire et cours à domicile ;Préparation de repas à domicile, y compris le temps passé aux commissions ;Gardiennage et surveillance temporaire, à domicile, de la résidence principale et secondaire ;Assistance administrative à domicile ;Collecte et livraison à domicile de linge repassé, à la condition que cette prestation soit comprise dans une offre de services incluant un ensemble d'activités effectuées à domicile.Assistance aux personnes âgées  qui ont besoin d'une aide personnelle du fait de leur dépendance, à l'exception d'actes de soins relevant d'actes médicaux ;L'allocation personnalisée d'autonomie - APA - Sert au financement de la dépendance elle est attribuée par le conseil général en fonction du niveau de dépendance -GIR- groupe iso-ressource elle est attribuée suivant le niveau de revenu. Les personnes âgées en maison de retraite ou en EHPAD peuvent en bénéficier elle sert à financer le budget Dépendance dans la convention tri- partie des établissements.Les personnes âgées avec des revenus importants qui utilisent les services d'aide à domicile peuvent également déduire cette prestation en fonction de leur niveau de GIR.     Assistance aux personnes handicapées, y compris les activités d'interprète en langue des signes, de technicien de l'écrit et de codeur en langage parlé complété. Le financement de l'aide relève de la compensation du handicap, celle-ci est attribuée sous condition du taux d'invalidité et du projet personnalisé par la Maison départementale des personnes handicapées (MDPH).Garde-malade, à l'exclusion des soins ; financement possible par les mutuelles santé et les assurances.L'APA et La PCH peuvent financer en fonction du projet personnalisé :Aide à la mobilité et transports de personnes ayant des difficultés de déplacement lorsque cette activité est incluse dans une offre de services d'assistance à domicile ;Prestation de conduite du véhicule personnel des personnes dépendantes, du domicile au travail, sur le lieu de vacances, pour les démarches administratives, à la condition que cette prestation soit comprise dans une offre de services incluant un ensemble d'activités effectuées à domicile ;Accompagnement des personnes âgées ou handicapées en dehors de leur domicile (promenades, transports, actes de la vie courante), à condition que cette prestation soit comprise dans une offre de services incluant un ensemble d'activités effectuées à domicile ;Livraison de courses à domicile, à la condition que cette prestation soit comprise dans une offre de services comprenant un ensemble d'activités effectuées à domicile ;Livraison de repas à domicile, à la condition que cette prestation soit comprise dans une offre de services incluant un ensemble d'activités effectuées à domicile ;Assistance informatique et internet à domicile ;Soins et promenades d'animaux domestiques, pour les personnes dépendantes ;Soins d'esthétique et coiffure à domicile pour les personnes dépendantes ;Activités qui concourent directement et exclusivement à coordonner et délivrer les services à la personne et téléassistance.Les associations et les entreprises sont agréées en application de l'article L. 129-1, du code du travail.L'ensemble des textes qui régissent le secteur sont disponibles sur le site de l'ANSP en particulier les articles L 129 et D 129 du code du travail et la loi du 25 juillet sur les services à la personne.Statut possible de l'organisme auprès de la DDTE :Les structures proposant des activités de services à la personne auprès de public fragile (personnes âgées, personnes en situation de handicap, enfants de moins de 3 ans) sont soumises à agrément délivré par le Préfet de département, l'instruction du dossier de demande d'agrément étant réalisée par la DIRECCTE.Par ailleurs, les structures proposant des activités de services à la personne peuvent bénéficier des avantages fiscaux et sociaux dès lors qu'elles se sont déclarées auprès du Préfet de département, et plus précisément auprès des services de la DIRECCTE.La déclaration ouvrant droit aux avantages fiscaux et sociaux n'est pas limitée dans le temps. L'agrément est délivré pour une période de 5 ans. Ces deux procédures sont distinctes.La personne âgée est cliente du service d'aide à domicile. Les intervenants sont salariés du service et sont sélectionnés par le prestataire. Les opérateurs doivent obtenir leur droit d'exercer par le Président du Conseil Départemental.Le service gère le recrutement et la gestion administrative de l'aide à domicile pour le compte de la personne âgée qui est l'employeur de son intervenant. Le mandataire doit disposer d'un agrément délivré par les services de l'Etat afin d'exercer.La personne âgée gère seule l'ensemble des démarches liées au recrutement de l'aide à domicile et devra respecter les formalités liées à l'embauche.Page consacrée aux services à la personne sur le site Service-Public.fr https://www.service-public.fr/particuliers/vosdroits/F13244Décret d'application du 14 octobre 2005. Portail du droit français   Portail du travail et des métiers   Portail des politiques sociales"
économie;"Un système économique est le mode d’organisation de l'activité économique d’une société ou d'une aire géographique donnée, qui détermine la production, la consommation, l'utilisation des ressources, etc. Dans un sens élargi, il peut être également compris comme l'organisation sociale induite par le système. Le système économique influence de nombreux facteurs, comme le niveau de vie des habitants, le niveau des inégalités, les relations avec les autres pays, ou la puissance économique.Un système économique est un mode d'organisation de la société analysé par le prisme économique. Un tel système regroupe l'ensemble des institutions, sociales comme normatives, qui agissent comme une courroie ou un récipiendaire de l'activité économique au niveau micro. Le système forme le niveau macroéconomique de la vie du pays. Les normes du système (via le droit du pays) détermine également la répartition des richesses.Ce système n'est pas neutre économiquement, dans le sens où ses structures conditionnent l'affectation des ressources et la productivité (production unitaire d'un facteur de production utilisé dans la combinaison productive) des facteurs. Sa modulation joue donc un rôle essentiel dans le cadre des politiques publiques visant à générer de la croissance ou à modifier le niveau d'inégalités.Les systèmes économiques varient en fonction des régions et des époques. Les pays occidentaux suivent une organisation fondée sur le capitalisme caractérisé par la propriété privée des moyens de production. Le système économique des pays de l'ancien bloc de l'Est était fondé sur le principe de l’économie communiste dont l'objectif est la propriété collective des moyens de production.Un système économique est lui-même composé d'une multitude de sous-systèmes, plus petits. Les agents qui le composent peuvent y être spécialisés : le système économique vigneron dispose de spécificités face au système économique de l'agroalimentaire auquel il appartient, qui, lui-même, n'est qu'un sous-système d'un ensemble plus grand.Du fait des combinaisons politiques et des modalités que peuvent prendre les institutions, il n'y a pas un seul modèle de système économique. Dans Les trois mondes de l'Etat-providence, Gøsta Esping-Andersen identifie trois types de systèmes économiques, correspondant à un système continental, un système nordique social, et un système anglo-saxon. Des recherches ultérieures ont identifié des systèmes économiques arabo-musulmans et asiatiques.Raymond Aron transpose par ailleurs le concept à la société internationale, en écrivant : « les États, par leurs politiques, contribuent à former le système économique mais celui-ci, inégalement déterminé par les États selon les pesanteurs de chacun d'eux, constitue un système différent du système interétatique, qu'on devrait plutôt qualifier de transnational que d'interétatique ».Un système économique est composé par des relations entre ses acteurs. Pierre-Joseph Proudhon note ainsi en 1850 que le système économique capitaliste d'Europe de l'Ouest est dominé par la figure du travailleur et celle du patron. Les relations entre les agents sont modulées par les politiques publiques, qui peuvent par exemple décider d'un niveau de centralisation plus ou moins élevé de l'activité économique.Le passage à un mode de développement durable suppose d'évoluer vers un nouveau modèle économique. Un système économique peut être refusé par certains de ses agents, voire remplacé. Le communisme est ainsi un système économique concurrent au système économique capitaliste. Un système économique se fond dans les institutions sociales ; dès lors, le changement de système économique requiert une modification desdites institutions.Le concept de souveraineté économique permet de qualifier un système économique dont les principales sources d'approvisionnement de matériaux et de biens stratégiques sont endogènes au système. Un système économique peut, à l'extrême, être autarcique, c'est-à-dire fonctionner sans relations avec d'autres systèmes économiques.Certains auteurs comme Jean-Marie Albertini constatent que le mot système ou modèle est utilisé par les économistes pour déterminer les différences théoriques essentielles entre les pays appartenant au même système (Japon et États-Unis, par exemple) ou, a fortiori entre des pays qui appartiennent à des systèmes économiques et sociaux totalement opposés (URSS et États-Unis lors de l'époque de la Guerre froide). Toutefois, ce mot ne permet pas de décrire l'évolution historique et réelle d'un pays ou d'un système. Pour cela, Jean-Marie Albertini propose d'utiliser le mot « régime ».Développement économique et social Portail de l’économie"
économie;"Une économie du marché est un système économique où les décisions de produire, d'échanger et d'allouer des biens et services rares sont déterminées majoritairement à l'aide d'informations résultant de la confrontation de l'offre et de la demande telle qu'établie par le libre jeu du marché. Confrontation qui détermine les informations de prix, mais aussi de qualité, de disponibilité. Au cœur de l'économie de marché, le mécanisme de l'offre et de la demande concourt à la découverte et à l'établissement des prix. Ce mécanisme opère par arbitrage pour un horizon donné et pour une qualité donnée entre des valeurs représentatives du bien ou du service concerné : d'une part la valeur de son coût intrinsèque (prix de revient) mais aussi d'autre part sa valeur d'échange (prix relatif, c'est-à-dire du prix d'un produit ou d'un service par rapport aux autres).Pour Robert Gilpin la dynamique de l'économie de marché fait intervenir également d'autres facteurs comme la concurrence et l'aptitude à la survie des acteurs dans l'activité économique .Cette dynamique propre au marché représente un facteur expliquant la diffusion de la croissance économique et l'extension géographique des échanges dans un espace plus large, au-delà des frontières politiques des États.Pour Roger Guesnerie « À l'aune de l'esquisse qui est faite ici d'une économie de marché -des marchés appuyés sur la monnaie et le droit-, nombre d'économies historiquement datées ont droit au label d'économies de marché ».D'une manière générale, il serait plus exact de parler des économies de marchés plutôt que de l'économie de marché, tant le système est dépendant des contextes et institutions très diverses qui accompagnent et soutiennent les marchés.Dans cette perspective, la volonté de prendre en compte les aspects sociaux en Europe après la Seconde Guerre mondiale a conduit à l'émergence du concept d'économie dite « sociale de marché », qui a été décliné selon différentes variantes propres aux pays concernés.Aujourd'hui, l'importance croissante accordée à l'environnement peut laisser entrevoir une évolution vers une « économie durable de marché » voire une « économie sociale et durable de marché » .Certains auteurs posent clairement une distinction entre économie de marché et capitalisme.Pour Fernand Braudel, les régimes de production/répartition des biens et services ont évolué selon trois formes historiques successives : celle de la vie matérielle primitive où le processus d'auto-suffisance et d'auto-consommation se déroule de manière très locale, à l'échelle de l'individu, de la famille ou de petits groupes. Ici on produit pour se suffire, uniquement, à soi-même. L'échange et donc le marché n'existent pas.celle de l'économie de marché, telle qu'elle découle des échanges rendus nécessaires par une plus grande spécialisation et une plus large division du travail : chacun produit une catégorie spécifique de bien et doit fatalement échanger avec les autres pour se procurer les biens qu'il ne produit plus et ainsi satisfaire l'ensemble de ses besoins.celle du capitalisme, amorcée par les entreprises de « commerce ou de négoce au long cours » et qui se financiarise inéluctablement pour engendrer un système où l'échange commercial n'est plus que le support ou le prétexte de gains financiers. Pour lui, « le capitalisme dérive par excellence des activités économiques au sommet ou qui tendent vers le sommet. En conséquence, ce capitalisme de haut vol flotte sur la double épaisseur sous-jacente de la vie matérielle et de l'économie cohérente de marché, il représente la zone de haut profit ». D'une façon générale, Braudel distingue deux types d'échanges : « l'un terre à terre, concurrentiel puisque transparent » qui relève de l'économie de marché et « l'autre supérieur, sophistiqué, dominant » qui relève du capitalisme.Pour Robert Gilpin, l'essence du marché réside dans le rôle des prix relatifs dans le processus d'allocation des ressources tandis que celle du capitalisme réside dans la propriété privée des moyens de production. Au niveau théorique, une économie socialiste de marché composée d'acteurs publics et de travailleur non libres est pour lui concevable comme cela est envisagé dans le concept d'économie socialiste de marché.Issue d'un concept et d'une pratique liée à l'ordo-libéralisme, l'expression recouvre aujourd'hui un sens plus large. Ainsi, Mario Monti, le commissaire européen, distingue les économies de marché de type anglo-saxon des économies sociales de marché allemande ou française. Pour lui, l'économie de marché doit non seulement être compatible, mais aussi être en mesure de financer la protection sociale par une imposition redistributive, de même que promouvoir un certain volontarisme des gouvernements et des institutions européennes en faveur de l'économie dans le respect des règles européennes de la concurrence. En effet, dans l'optique libérale la concurrence entraîne la baisse des prix. Cela protège le pouvoir d'achat des individus et favorise l'innovation,. Le capitalisme, lui, encouragerait en réalité des comportements criminels, crapuleux et opportunistes .Il n'y a pas sur le plan théorique d'unanimité quant à la définition précise de l'économie de marché. On constate en revanche l'existence et la pratique de modèles les plus divers où le mécanisme d'économie de marché est amené à coexister et à composer avec des logiques ou contraintes plus ou moins compatibles.Le régime de la concurrence pure et parfaite n'étant pas concrétisé dans la réalité,il n'existe pas « un » marché général où se produit la confrontation de toutes les offres et de toutes les demandes, mais « des » marchés plus ou moins inter-connectés sinon cloisonnés qui donnent lieu à des confrontations partielles. (ex : marché des biens, des matières premières, des services, du travail, des changes, des capitaux, marché monétaire, marché immobilier, etc.)les imperfections de marché prospèrent :Monopole, duopole, oligopole, cartel, entente, position dominante, etc.asymétrie de l'information, délai et effet retard, goulet d'étranglement, etc.la demande exprimée sur un marché est perçue la plupart du temps à travers le prisme déformant de la demande solvable, c'est-à-dire celle qui émane des opérateurs disposant du pouvoir d'achat monétaire suffisant.Des actions collectives peuvent être organisées pour promouvoir ou défendre des valeurs positives ou des règles sociales, culturelles, morales, voire religieuses :l'économie publique ayant une place complémentaire (ex : les services publics), ou centrale (ex: le capitalisme d’État) ;l'économie sociale pour pallier les insuffisances ou déficiences du marché (ex : les mécanismes de protection sociale, d'assistance, de solidarité, d'État providence) ;l'interventionnisme d'État ou dirigisme via l'économie planifiée ou l'économie administrée ;le commerce équitable comme projet d'organisation visant à faire une meilleure place à certains producteurs en danger d'être marginalisés ou évincés du marché courant ;l'élaboration de normes - à caractère volontaire, incitatif ou obligatoire - peuvent contribuer à préciser ou encadrer la définition des pratiques de conception, de production ou de distribution des biens et services pour des motifs de protection de la qualité ou de la sécurité dues au consommateur/usager.l'économie dite non marchande ou domestique ne donne pas lieu à échange rémunéré(ex : jardins familiaux, babysitting non rémunéré, femmes au foyer, aide des grands-parents…).l'économie de troc et/ou l'économie de subsistance se situe relativement à l'écart des flux économiques (ex. : pratiques de certaines zones rurales du tiers monde ou très déshéritées)l'économie spéculative ou exclusivement financière qui introduisent des logiques de type « argent ? marchandise ? argent » dénoncées par certains comme représentant des déconnexions forcées de l'économie réelle. (ex. : spéculation sur les matières premières)l'économie souterraine ou les « trafics » opérés sur des marchés parallèles ou occultes (ex: le trafic de drogue, le travail ou le marché au noir, ou le proxénétisme…).les phénomènes de corruption ou de délit d'initié qui visent par leur nature à fausser le libre jeu des forces du marché.L'économie de marché s'arrête difficilement au niveau d'un seul pays, si vaste soit-il. Au niveau international, elle est d'autant plus développée que les divers pays pratiquent le libre-échange.Cela dit, en pratique beaucoup de pays revendiquent pour leurs exportations les règles applicables à l'économie de marché (sinon la clause de la nation la plus favorisée), en organisant par ailleurs vis-à-vis des importations des règles fort peu réciproques (protectionnisme) :Certains pays issus du collectivisme — comme la Chine — se veulent « économie socialiste de marché », alors qu'ils sont encore des économies marquées par l'empreinte du Capitalisme d'État.D'autres pays bénéficient de conditions de couts qui leur permettent de pratiquer une concurrence jugée « déloyale » par leurs rivaux plus développés.D'autres pays se trouvent dans une situation où la structure des flux échangés les entraine vers la détérioration des termes de l'échange.Un débat donne lieu à un fort questionnement assorti de multiples prises de position :Échanges entre socialistes « réformistes » et socialistes « fondamentalistes » Michel Rocard se targue souvent « d'avoir mis des décennies à apprendre l'économie de marché aux socialistes ».le tournant de la rigueur survenu en 1983 motive la gauche réformiste pour accepter de facto l'économie sociale de marché.L'économie de marché non régulée n'est pas forcément compatible avec les exigences du développement durable. En effet, la recherche de la maximisation du profit par les entreprises ne va pas spontanément dans le sens d'un développement durable, car elle conduit à des raisonnements de court terme (voire de spéculation), et elle tend à la satisfaction des intérêts des seuls actionnaires des entreprises.L'encyclique Caritas in Veritate de Benoît XVI (juillet 2009) indique que les acteurs de la vie économique ne peuvent se limiter au marché seul, mais « que l'économie doit aussi impliquer l'État et la société civile » :« La vie économique a sans aucun doute besoin du contrat pour réglementer les relations d’échange entre valeurs équivalentes. Mais elle a tout autant besoin de lois justes et de formes de redistribution guidées par la politique, ainsi que d’œuvres qui soient marquées par l’esprit du don. L’économie mondialisée semble privilégier la première logique, celle de l’échange contractuel mais, directement ou indirectement, elle montre qu’elle a aussi besoin des deux autres, de la logique politique et de la logique du don sans contrepartie. »« Mon prédécesseur Jean-Paul II avait signalé cette problématique quand, dans Centesimus annus, il avait relevé la nécessité d’un système impliquant trois sujets : le marché, l’État et la société civile. »— Encyclique Caritas in Veritate, chapitre III, § 37 et 38L'intervention de l'État, qui représente les « intérêts publics » (notion à définir), est considérée par certains comme nécessaire. Elle se fait actuellement de la façon suivante :Être exemplaire en matière de développement durable :Définir une stratégie nationale de développement durable,Mettre en œuvre cette stratégie en mettant en place des organisations dédiées au développement durable dans les ministères et les collectivités territoriales (ex. : ministère de l'environnement ou du développement durable),Lancer des actions concrètes comme le Grenelle de l'environnement.Encadrer le marché par l'application forcée des règles le concernant (concurrence, propriété privée, droit du travail, ...).Participer aux différentes initiatives qui ont lieu au niveau international sur le développement durable, sommets de la Terre, sommets de l'eau, protocole de Kyoto et ses suites, réunions sur la biodiversité…Définir de nouvelles règles du jeu :En France, les entreprises cotées en bourse doivent rendre compte des conséquences sociales et environnementales de leur activité (article 116 de la loi sur les Nouvelles Régulations Économiques),Principe du bonus-malus écologique pour les véhicules automobiles,Mise en place d'une finance du carbone.Toutefois, les évaluations portant sur la mise en œuvre des Nouvelles Régulations Économiques en France montrent qu'assez peu d'entreprises se conforment réellement aux exigences de la loi. En effet, le non-respect de la loi n'entraîne aucune sanction vis-à-vis des entreprises. Il s'agit d'un droit mou.On peut imaginer d'autres actions des États :Changer les règles de comptabilisation de la richesse. Par exemple, des études ont montré que le produit intérieur brut (PIB) ne prend pas en compte la diminution du capital naturel. L'INSEE a retenu le PIB comme indicateur de développement durable, alors qu'en réalité les effets à long terme de la croissance économique sur l'environnement ne sont pas pris en compte par le PIB. Il est clairement de la responsabilité des États de définir des instruments de mesure de la croissance économique, en l'occurrence des indices macroéconomiques, qui rendent compte efficacement de la conformité des agents économiques par rapport aux principes de développement durable (PIB vert).Mettre en place une fiscalité favorable aux produits durables (taxe carbone),Adapter l'enseignement,Sensibiliser la société civile en donnant le feu vert à tous les moyens permettant de montrer les dangers sur l'homme des activités qui menacent l'environnement et le développement durable.etc.La société civile intervient par l'intermédiaire de ses représentants, organisés en parties prenantes (organisations professionnelles, organisations syndicales, organisations non gouvernementales…). Par exemple, en matière environnementale, les parties prenantes représentatives sont les ONG (organisations non gouvernementales) (environnementales (WWF, Greenpeace, Amis de la Terre…).Les parties prenantes peuvent se concevoir par rapport aux autorités politiques, ou bien par rapport aux entreprises.Par rapport à la perspective catholique, les sociaux-libéraux s'interrogent sur la notion même d'État. La distinction société civile/État leur pose un problème car elle suppose, à la manière de ce qui existe dans l'Église, une prépondérance donnée à la hiérarchie de l'État sur les citoyens.L'Organisation mondiale du commerce (OMC) octroie le « statut d’économie de marché » (SEM) aux États. Un pays qui importe des produits depuis un pays qui n'en bénéficie pas est autorisé à ne pas tenir compte du prix pratiqué sur le marché intérieur de l’État exportateur.La Chine s'est ainsi vue attribuer ce statut en 2016, conformément à l'accord convenu lors de son adhésion en 2001,. Avant même cette décision de l'OMC, plus de 80 pays dans le monde avaient reconnu le statut d'économie de marché à la Chine. Cependant, les États-Unis s'y opposent,. De son côté, l'Union européenne a mis en place une nouvelle méthodologie anti-dumping qui ne cible plus spécifiquement la Chine : Jean Quatremer estime ainsi qu'« en clair, l’Union va continuer à considérer que la Chine n’est pas un pays à économie de marché, mais sans le proclamer et en évitant les foudres de l’OMC »,. D'autre part, un rapport détaillé de la Commission européenne émettait des doutes en décembre 2017 sur la nature d'« économie de marché » de l'« économie socialiste de marché » de la Chine,.Robert Gilpin, 1987, The Political Economy of International Relation, Princeton University Press.Fernand Braudel, 1985, La Dynamique du capitalisme, Paris, Arthaud, 1985  (ISBN 2080811924)Roger Guesnerie 2006, L'Économie de marché, Le Pommier.John Kenneth Galbraith, Et le système fut rebaptisé. dans Les Mensonges de l'économie, traduction française de Paul Chemla, Paris, Grasset, 2004Michel Lafitte, 2007, Développement durable et économie de marché  (ISBN 2863254782) Portail du libéralisme   Portail de l’économie"
économie;"La comptabilité nationale est une représentation schématique et quantifiée de l'activité économique d'un pays. Elle consiste en une mesure des flux monétaires représentatifs de l'économie d'un pays pendant une période donnée, en principe une année, et les regroupe dans des totaux nommés agrégats, dans un but analytique direct. La comptabilité nationale prend en compte de nombreux indicateurs macroéconomiques, dont le plus important est le PIB (produit intérieur brut), qui correspond à la somme des valeurs ajoutées — auxquels il faut ajouter les impôts nets des subventions sur les produits — des biens et services produits dans un pays donné au cours d'une année. La comptabilité nationale prend en compte de nombreuses informations, contenues dans les documents comptables des entreprises d'une part, mais aussi dans les rapports des institutions administratives. La comptabilité nationale classe ainsi les différents agents économiques en catégories, les secteurs institutionnels, afin de recenser au mieux les différentes informations relatives à l'économie.Les premiers systèmes de comptabilité nationale datent de la Seconde Guerre mondiale, tout d'abord avec l'économiste britannique Keynes qui développe dès 1941 des instruments de mesure de l'économie, puis avec Jan Tinbergen et Wassily Leontief, considérés comme les véritables inventeurs de la comptabilité nationale. La comptabilité nationale s'est ensuite développée dans la plupart des pays développés. Ainsi, dans le cadre du système monétaire européen (SME), les systèmes de comptes nationaux ont été harmonisés autour de normes communes, et les États européens utilisent le même plan comptable : le SEC (système européen de comptabilité).La comptabilité nationale est née de la volonté des États d'intervenir dans une régulation conjoncturelle de l'économie. Selon un article du Figaro en 2009, « l'invention de la comptabilité nationale a été une réponse à la Grande Dépression des années 1930. On ne disposait à l'époque d'aucune statistique générale, en dehors des cours boursiers ou des données de production établies plus ou moins bien par les professions. Dès 1932, avant même l'élection de Roosevelt et le New Deal, le Congrès américain avait demandé à l'économiste Simon Kuznets (couronné par le Prix Nobel en 1971) d'estimer le recul de l'activité globale. Il s'est alors avéré qu'elle avait chuté de 40 % entre 1929 et 1932. »Le premier vrai système de comptabilité nationale fut créé par John Maynard Keynes (qui dirigeait alors la délégation britannique chargée de rédiger les accords de Bretton Woods) en 1941 à la suite de la demande du parlement de Grande-Bretagne. Les collaborateurs de Keynes élaborèrent une série de tableaux illustrant les ressources produites et leur utilisation sous forme de consommation, dépenses publiques, subventions et investissements. En outre, les travaux menés par l'américain Wassily Leontief (« Prix Nobel » d'économie en 1973) et le néerlandais Jan Tinbergen, « Prix Nobel » d'économie en 1969 ont permis de développer des analyses plus proches de celles que nous connaissons aujourd'hui.Les travaux de Richard Stone et de Simon Kuznets sont à l'origine de ce que l'on a baptisé un « modèle normalisé de la comptabilité nationale ».En ce qui concerne les tableaux de synthèse, en particulier le tableau entrées-sorties (TES) le précurseur fut l'économiste d'origine russe naturalisé américain Wassily Leontief.En France, François Quesnay, chef de file de l'école physiocratique, apparaît comme le premier à avoir élaboré un modèle dynamique, en 1758, pour représenter, à une échelle macroéconomique, la comptabilité nationale dans son ensemble. Au xixe siècle, plusieurs économistes ou hommes politiques s'efforcent de quantifier l'activité économique : Lesur dresse un bilan économique de la France en 1817 et y évalue la somme des revenus à cinq milliards ; en 1819, Jean-Antoine Chaptal estime la valeur de la production agricole et manufacturière en s’appuyant sur les données statistiques des préfectures et du cadastre. Des économistes comme François Perroux (également auteur de la théorie des « pôles de croissance ») ont les premiers établi des modèles modernes de comptabilité nationale sous le régime de Vichy et à la Libération. Selon une étude sur le sujet, « ces pionniers aux vues anticipatrices élaborent des outils statistiques et amorcent la réflexion sur la comptabilité nationale, à partir de la fin des années trente, puis pendant l'occupation. Ces économistes non traditionnels (Jean Fourastié) et ces statisticiens de l'Insee (André Vincent, Jacques Dumontier) se joignent ensuite à l'équipe de Jean Monnet à partir de 1945. »La comptabilité nationale a deux vocations principales : modéliser et étudier l'activité économique d'un pays donné pendant une durée précise d'une part, et prévoir l'évolution d'une conjoncture d'autre part. Elle peut ainsi être un outil de prévision pour aider un gouvernement à trouver des solutions ou à relancer la consommation par exemple. Les comptes nationaux sont publiés par trimestre ou par année.La comptabilité nationale est ex-post, elle s'effectue une fois l'année écoulée. Elle se mesure à prix constants, c'est-à-dire qu'elle ne tient pas compte de l'inflation.L'information la plus connue utilisée par la comptabilité nationale est le PIB (Produit intérieur brut). Le PIB est un indicateur macroéconomique nommé agrégat, c’est-à-dire une grandeur globale qui mesure l'activité économique. Il est possible de proposer trois approches du PIB, cependant, on le considère la plupart du temps comme la somme des valeurs ajoutées produites par l'ensemble des unités résidentes, c’est-à-dire les agents économiques effectivement présents sur le territoire pendant au moins 183 jours sur une année.Le PIB a ainsi une triple optique basée sur les grands principes de la comptabilité nationale :la production : PIB = somme des VAB + IP - SUBV. L'approche par la production, met ainsi en relation la somme des valeurs ajoutées brutes, l'impôt sur la production ainsi que les différentes subventions ;la formation de revenu : PIB = RS + EBE + RMB - SUBV + IP, avec RS la rémunération des salariés, EBE l'Excédent brut d'exploitation, RMB les revenus mixtes bruts, SUBV les subventions et IP les impôts sur la production (liés à la production et aux importations) ;la demande : PIB = CF + FBCF + (X-M), avec CF la consommation finale, FBCF la formation brute de capital fixe (l'investissement), X les exportations et M les importations.Le PIB (Produit intérieur brut) ne doit pas être confondu avec le PNB (produit national brut) qui est la somme des revenus primaires reçus effectivement par les agents économiques d'une même nationalité, qu'ils soient situés sur le territoire ou non. On a ainsi la relation PNB = PIB + revenus des facteurs en provenance de l'extérieur - revenus des facteurs versés à l'extérieur.Les différents agents économiques sont regroupés dans différentes branches baptisées unités institutionnelles. Elles constituent les unités de base de la comptabilité nationale.Une unité institutionnelle est un centre de décision autonome pouvant être une personne (ou plusieurs) physique, les économistes disent alors qu'il s'agit d'un ménage, ou une personne morale, c'est-à-dire une entreprise, une administration publique ou une association. Elles sont susceptibles de posséder elles-mêmes des actifs, de souscrire des engagements, de s'engager dans des activités économiques et de réaliser des opérations avec d'autres unités.Ces unités institutionnelles doivent exercer des opérations économiques pendant un an au moins sur le territoire national pour être comptabilisées dans les secteurs institutionnels. Ce territoire est, si on prend l'exemple de la France, la métropole et les départements d'outre-mer, les enclaves territoriales françaises hors du territoire, l'espace aérien, les eaux territoriales et les espaces qui regroupent des ressources appartenant à la France. En revanche, les enclaves étrangères, à l'image de consulats et ambassades présents sur le sol français, ne sont pas considérées comme des unités résidentes.Les unités institutionnelles ayant la même activité principale et la même source principale de revenu sont regroupées en cinq secteurs institutionnels.On distingue cinq secteurs institutionnels résidents :les ménages ;les sociétés non financières (SNF) ;les sociétés financières (SF) ;les administrations publiques (APU) ;les institutions sans but lucratif au service des ménages (ISBLSM).L'ensemble des unités non-résidentes, dans la mesure où elles entretiennent des relations économiques avec des unités résidentes, sont regroupées dans une catégorie appelée reste du monde, parfois baptisée catégorie « plus-un ».La fonction principale des ménages est la consommation à partir de ressources principales obtenues de deux manières :d'une part par la rémunération des facteurs de production, à savoir le travail, la terre, le capital ;d'autre part, par les transferts effectués par d'autres secteurs institutionnels à destination des ménages.Au sein des ménages, on peut distinguer :le ménage « ordinaire » ou « pur », à savoir un ensemble de personnes vivant dans un logement ;le ménage « collectif » qui est constitué par les populations des maisons de retraite, des foyers de travailleurs, etc.On retrouve également dans ce secteur les entreprises individuelles qui sont des unités économiques dont la fonction principale est la production de biens et services pour leur usage final propre. On retrouve ainsi dans cette catégorie les agriculteurs, les artisans, les professions libérales, les petits commerçants, etc.Les sociétés non financières (SNF) regroupent l'ensemble des sociétés et quasi-sociétés dont la fonction principale est de produire des biens et services marchands, c'est-à-dire dont le prix de vente couvre au moins 50 % du coût de production.Les ressources des sociétés et quasi-sociétés non financières sont le résultat de la production et des éventuelles subventions versées par les administrations publiques (collectivités locales).La CN classe actuellement les SNF en trois catégories, selon le contrôle :Les SNF sous contrôle public, c'est-à-dire sous le contrôle de l'État : la SNCF, la RATP… ;Les SNF sous contrôle privé national : Bouygues, Total… ;Les SNF sous contrôle privé étranger : Google France, Toyota France…Les sociétés financières (ou SF) sont constituées par l'ensemble des sociétés et quasi-sociétés dont la principale fonction est d'offrir des services d’intermédiation financière et/ou d'exercer des activités financières auxiliaires. Leurs ressources sont des fonds provenant des engagements financiers.Cinq sous-secteurs institutionnels constituent le secteur institutionnel des sociétés financières :Les banques centrales ;Les autres institutions financières monétaires (la compatibilité nationale y exclut par convention les sociétés d'assurance et les fonds de pension) ;Les intermédiaires financiers ;Les auxiliaires financiers ;Les sociétés d’assurance et les fonds de pension.Les administrations publiques sont regroupées sous le sigle APU. La fonction principale de ces unités institutionnelles est de produire des services non marchands et/ou d'effectuer des opérations de redistribution des revenus ou du patrimoine national. Elles tirent la majeure partie de leurs ressources de contributions obligatoires (impôts).En France, les administrations publiques (APU) se regroupent en trois sous-secteurs :Les APU centrales (APUC) : composées de l'État et des organismes divers APUC (ODAC) ; les universités, le CNRS, l'ANPE… ;Les APU locales (APUL) : régions, départements, communes + OAL (régie de transport municipal, chambre de commerce…) ;Les ASSO (Administration de sécurité sociale) : unités qui distribuent des prestations sociales à partir de cotisations sociales obligatoires + ODASS ; les ressources proviennent des assurances sociales (ex. : hôpitaux publics).Les institutions sans but lucratif au service des ménages (ISBLSM) regroupent diverses structures dont certaines associations (ex. : association de consommateurs, parti politique, syndicat, Église, organisme de charité, etc.). Leurs points communs sont que, d'une part, elles produisent des services pour les ménages, d'autre part, elles sont financées par des cotisations volontaires et parfois par la vente de biens et services marchands, mais dont le but n'est pas d'en tirer de bénéfice.D'un point de vue économique et du fait de la façon dont la comptabilité nationale les prend en compte, les ISBLSM affichent un rôle négligeable ; il en résulte que dans les statistiques globales, leur consommation est ajoutée à celle des ménages. La majorité des organismes à but non lucratif, qui regroupent l'ensemble des entreprises de l'économie sociale, n'est cependant pas regroupée dans cette catégorie des ISBLSM, ce qui contribue à minorer leur importance. Les différentes études menées situent l'importance de l'ensemble du secteur non lucratif (ISBLSM et économie sociale) à environ 10 % des emplois en France.Ce n'est pas un secteur institutionnel et à ce titre on le qualifie parfois de faux secteur, dans la mesure où les opérations ne sont pas décomposées en distinguant des catégories d'agents : il n'y a pas de compte des ménages ou des SNF du reste du monde. Ce secteur « plus un » regroupe ainsi les unités non résidentes qui effectuent des opérations avec l'économie nationale.Les flux sont enregistrés au moment de la réalisation de l'opération. Les flux financiers sont comptabilisés en « flux nets d'acquisition d'actifs » et « flux nets d'engagements contractuels » alors que les autres flux le sont en « emplois » et « ressources ».Il s'agit de l'ensemble des opérations qui concernent la création et l'utilisation des biens et des services.Parmi elles on distingue :La production, qui a évolué dans le temps; les entreprises y jouent un rôle majeur, mais les ménages ainsi que les administrations sont eux aussi considérés comme des producteurs ;La consommation ;La formation brute de capital fixe — FBCF — (c'est-à-dire l'investissement) ;Les opérations avec l'extérieur (c'est-à-dire les importations et les exportations de biens et de services). Ces opérations sont regroupées dans le TRE (tableau des ressources et des emplois).Ce sont les opérations par lesquelles la valeur ajoutée créée par la production est distribuée entre les salariés, les propriétaires d'entreprises et les administrations publiques, puis redistribuée du fait de l'action des administrations publiques (versements d'allocations financées par des prélèvements…).Pour simplifier on peut considérer ici la valeur ajoutée (VA) comme l'ensemble des richesses créées.VA = P - CI : Production - Consommations IntermédiairesUn indicateur, le taux de marge, résume pour l'essentiel la répartition des richesses créées entre les salariés et les propriétaires d'entreprises. Il mesure la part des profits des entreprises (EBE, excédent brut d'exploitation) dans la VA : taux de marge = EBE / VA x 100. Comme la valeur ajoutée se répartit principalement entre salaires et profits, à une hausse du taux de marge correspond une baisse de la part des richesses créées qui revient aux salariés, et une hausse de celle qui revient aux propriétaires des moyens de production (capital).Ces opérations sont regroupées dans le TCEI (tableau des comptes économiques intégrés).Les opérations financières représentent les engagements pris par les agents économiques les uns envers les autres, en contrepartie de monnaie ou de produits. Par exemple les prêts faits par certains représentent des emprunts pour les autres. La comptabilité nationale retrace ces opérations entre les principaux secteurs institutionnels dans le cadre du TOF « tableau des opérations financières ».(Cette partie de l'article fait la liste des principaux comptes. C'est une ébauche à compléter car chacun d'eux reste à présenter). Le compte de production                      P        ?        C        I        =        V        A              {\displaystyle P-CI=VA}  Le compte de production décrit les flux qui composent le processus de production à savoir les consommations intermédiaires qui sont des opérations sur biens et services : son solde est la valeur ajoutée ou la richesse créée. Le compte d'exploitation EBE (excédentaire brut d'exploitation) = Valeur Ajoutée - Salaires - Impôt (production) + Subvention (exploitation)ouEBE= PIB - Salaires - Impôts (production + produit) + Subvention (exploitation + produit) Le compte d'affectation des revenus primaires EBE + Revenus de la propriété reçus + revenus salariés + impôts sur la production - subventions - revenus de la propriété versés = SRPCe compte s'intéresse aux ressources des secteurs c'est la répartition des revenus liés directement au processus de production (revenus primaires). En emploi on a les revenus de la propriété que les secteurs versent. Le compte de distribution secondaire du revenu Srp + Impôts sur le revenu reçus + impôts sur le patrimoine reçus + Prestations sociales reçues + Autres transferts courants reçus+ Cotisations reçues - impôts sur le revenu versés - impôts sur le patrimoine versé - prestations sociales versées - cotisations sociales versés - autres transferts courants versés = RDBCe compte de répartition des revenus secondaires décrit les flux entre les différents secteurs que sont les ménages et les administrations publiques. En ressource de compte les impôts et cotisations sociales sont versés aux administrations publiques. Les ménages reçoivent des prestations sociales. Les autres transferts courants sont versés à l'ensemble des secteurs. En emplois on a les impôts versés et reçus par l'ensemble des secteurs institutionnels. Les cotisations sociales sont versées par les ménages et les entreprises. Le solde obtenu est le revenu disponible brut. Le compte d'utilisation du revenu disponible                     R        D        B        ?        C        F        =        E        B              {\displaystyle RDB-CF=EB}  Ce compte permet de distinguer la part du revenu disponible (RDB=revenu disponible brut) qui sera consacrée à la consommation de biens finaux (CF = consommation finale) de celle qui sera réservée à l'épargne (EB = épargne brute). Ce compte constitue en fait la charnière entre les comptes de résultat (ceux qui représentent des flux) d'une part et les comptes d'accumulation (parfois appelés comptes patrimoniaux et qui représentent des stocks). En effet c'est au départ de l'épargne que se constituent les masses capitalistiques. Le compte de capital Emplois+ FBCF (P51)+ CCF+ VS (P52)+ OV (P53 acquis - cédés)+ AF (NP1 + NP2 acquis - cédés)Ressources+ EB(B8) [solde précédent]+ TC(D9 reçu - D9 versé)Solde : Capacité/Besoin de financement (B9A)FBCF : Formation Brute de Capital Fixe Le compte financier le compte financier mesure la variation de l'actif et le passif financier du secteur institutionnel et du reste du monde.Il permet d'évaluer le patrimoine financier des secteurs institutionnels, en dressant un état de la valeur des actifs détenus et des engagements contractés (passif) à un moment donné. Cette opération a souvent lieu au 31 décembre de l'année.Le TEE est un tableau de synthèse qui donne une présentation simultanée des comptes de flux des secteurs institutionnels et des comptes d'opérations. Il rassemble les opérations économiques et financières de l'économie nationale pour une année donnée. Le TEE permet ainsi de mesurer les résultats économiques globaux, la contribution de chaque secteur institutionnel à ces résultats, ainsi que l'importance des relations entre l'économie nationale et le reste du monde. Il constitue également un outil très important pour la prévision économique.La comptabilité nationale utilise le « tableau économique d’ensemble » (TEE) qui rassemble l’origine et l’utilisation des ressources de chaque secteur (sociétés non financières, instituts de crédit, entreprises d’assurance, administrations publiques, administrations privées, ménages et reste du monde).Il est construit en valeur d'une part, en brut, cvs (corrigé des variations saisonnières) et cjo-cvs (corrigé de l'effet des jours ouvrables et des variations saisonnières) d'autre part. Ainsi que pour le TES, les comptes du TEE ne sont pas publiés.Le TEE se décompose en une succession de lignes et de colonnes qui aboutissent chacune à la mesure d'un solde correspondant. Chaque compte est séparé en emplois (actif) et en ressources (passif). Excepté dans le compte de production, les soldes des différents comptes sont évalués dans les comptes trimestriels tout simplement par solde.Le tableau entrées-sorties distingue les branches et secteurs. La branche est constituée par l'ensemble des activités qui élaborent un produit donné. Ainsi, il y a autant de branches que de produits. Un secteur est constitué par l'ensemble des entreprises ayant la même activité principale. Le TES indique le montant de chaque produit utilisé par les diverses branches de l'économie. Il permet de retrouver l'équilibre pour chaque branche entre les emplois et les ressources. Il permet d'expliquer a posteriori et de simuler a priori les incidences d'une modification des conditions économiques générales.La comptabilité nationale utilise le « tableau entrées-sorties » (TES) qui décrit l’équilibre des opérations sur biens et services pour toutes les branches de l’économie. On entend par branche l’ensemble des unités de production qui fabriquent un même produit. Ainsi le TES permet pour chaque branche et pour l’ensemble de l’économie, de faire ressortir un équilibre entre les emplois et les ressources de la branche. Sa structure repose sur une division par branches et par produits. Il constitue un outil utile aux comptables nationaux. Dans une perspective keynésienne, s’inspirant du tableau économique de Quesnay, le TES a été mis en évidence par l'analyse entrée-sortie de Wassily Leontief pour représenter l’ensemble des opérations des agents économiques au cours d’une période donnée.On va donc tout d’abord rappeler l’égalité de base, puis voir la structure du TES, et enfin son utilité. Rappel de l’égalité de base Ressources=Production_(P) + Importation_(M) + Impôts_(M)Emplois = Consommation intermédiaire (CI) + Consommation finale (CF) + FBCF + Exportations (X) + Variation des stocks (VS)Le TES présente l’équilibre emploi/ressources : P + M = CI + CF + FBCF + X + VSCet équilibre est toujours vérifié dans les comptes en T. La structure du TES En ligne : répartition des produits entre les branches c’est-à-dire le volume de produits utilisés par chaque branche.En colonne : volumes des produits nécessaires à chaque branche pour sa production.Le total des ressources de chaque branche est égal au total des emplois des produits correspondants.Le TES se compose :d'un tableau des emplois intermédiairesd'un tableau des emplois finauxd'un tableau des comptes de productiond'un tableau total des ressources L’utilité du TES Le TES donne une représentation cohérente de la production nationale et permet de représenter les branches qui contribuent le plus à la production nationale. Il permet de faire apparaître le degré d’indépendance des branches en faisant le calcul : (Total des consommations intermédiaires de branche/Production de la branche)*100Ainsi, toute modification de la production dans une branche entraîne des répercussions dans les autres branches.Le TES est aussi un instrument de prévision économique. On peut calculer des coefficients techniques : (Consommation intermédiaire en produit x / Production de la branche y)*100.L’ensemble des coefficients techniques donne une matrice sur laquelle on peut baser des prévisions relativement fiables à court terme. Il est notamment possible de prévoir :l’effet d’entraînement d’une branche sur les autres ;les conséquences sur les branches d’une augmentation globale de la production, des exportations, de la consommation des ménages… ;les conséquences de l’interdépendance des branches (goulets d’étranglement).On peut bien entendu critiquer la difficulté de construction d’un tel tableau pour une économie nationale, ainsi que les erreurs de mesure des grandeurs économiques qu’il renferme.Le TES peut servir de base à la construction d'une matrice de comptabilité sociale, entrée utile pour un modèle d'équilibre général calculable.Abréviations :P : production ;M : importation ;C : consommation ;CI : consommation intermédiaire ;CF : consommation finale ;FBCF : formation brute de capital fixe ;X : exportation ;VS : variation des stocks.Le TOF réunit l'ensemble des statistiques financières relatives aux secteurs institutionnels (SI) et permet d'analyser les aspects financiers de l'économie.En dépit de leur taille et de la masse d'informations qu'ils contiennent, ces tableaux sont d'une structure très simple et leur lecture est assez facile et posée.Comptes nationaux et régionaux de la Belgique publiés par la BNBComptes économiques et financier du CanadaComptes nationaux de la FranceBalance des paiements de la France en 2002Comptes économiques du QuébecDes organismes spécialisés sont chargés de vérifier les comptes nationaux : les Cours des comptes.En France, la loi organique relative aux lois de finances (LOLF), promulguée en août 2001 et mise en œuvre depuis le 1er janvier 2006, modifie en profondeur les finances publiques..La comptabilité nationale est assujettie à un principe de sincérité.Le rapport de la Cour des comptes de juin 2006 fait état de manques de précisions dans le système français de comptabilité nationale :« II ne comprend pas les passifs implicites ; il ignore bon nombre d'actifs ayant une utilité sociale, mais qui ne sont pas valorisés faute d'une valeur marchande de référence ; peu d'actifs incorporels sont recensés ; enfin, il se fonde sur une notion d'actif restrictive, excluant la plus grande partie du capital immatériel – éducation, recherche, santé. »La comptabilité nationale, a été conçue dans les années de reconstruction qui ont suivi la Seconde Guerre mondiale. Il fallait vérifier que le pays retrouvait le niveau de production d'avant guerre, puis qu'il rattrapait celui de l'Amérique. L'attention était focalisée sur le quantitatif, et les contraintes environnementales étaient ignorées.Ainsi conçu, et rigidifié par les institutions de la comptabilité nationale, le PIB ne serait pas adapté à l'économie actuelle, dont le but est différent.Concevoir la comptabilité nationale qui répondrait à des objectifs de développement durable suppose un gros effort intellectuel.Du point de vue environnemental, la comptabilité nationale tient compte actuellement de la consommation de ressources naturelles en tant que consommations intermédiaires.Edith Archambault, La Comptabilité nationale, Economica, 2003,  (ISBN 271784712X)Jean-Paul Piriou, La Comptabilité nationale, Repères, La Découverte, 2004,  (ISBN 2707143367)André Vanoli, Une Histoire de la comptabilité nationale, La Découverte, 2002,  (ISBN 2707137022)Gilbert Abraham-Frois, Économie politique, Economica, 2001],  (ISBN 2717842675) (l'ouvrage comporte une annexe sur la comptabilité nationale, claire et synthétique)Dictionnaire d’économie, J-Y Capul, Olivier Garnier, Hatier, 2005,  (ISBN 2218740591)DJ. Muller, P. Vanhove, PECF. Économie, Dunod, 1999Michel Braibant, Vers un tableau « entrées-sorties » idéal et mondial, Edilivre, 2018  (ISBN 9782414267040)Histoire de la pensée économique Normalisation ÉconométrieComptabilitéPlan comptable Mesure économique Produit intérieur brutTaux d'investissementFormation brute de capital fixe Belgique StatbelBudget fédéral de BelgiqueBanque nationale de BelgiqueBureau fédéral du PlanCour des comptes (Belgique) France InseeBudget de l'État françaisCour des comptes (France)Abrégé de comptabilité nationaleComptes nationaux et régionaux belgesLes définitions de l'InseeUNSTATS Portail de l’économie"
économie;"En économie, un indicateur est une statistique construite afin de mesurer certaines dimensions de l’activité économique, ceci de façon aussi objective que possible. Leurs évolutions ainsi que leurs corrélations avec d'autres grandeurs sont fréquemment analysées à l'aide de méthodes économétriques.Les indicateurs sont construits par l'agrégation d'indices qui figurent dans un document appelé « tableau de bord ». La construction des indicateurs découle d'un choix de conventions qui traduisent plus ou moins bien certaines priorités et valeurs éthiques et morales. Le « Tableau économique » de François Quesnay, l'un des premiers physiocrates qui a vécu au XVIIIe siècle, constitue l'un des premiers exemples d'un tel indicateur visant à mesurer la richesse d'un pays. Depuis les développements des comptes nationaux après la Seconde Guerre mondiale, le produit intérieur brut (PIB) et le produit national brut (PNB) sont les indicateurs les plus courants.Par ailleurs, il existe d'autres indicateurs qui prennent en compte d'autres facteurs ignorés par le PNB et le PIB afin de mesurer le bien-être des habitants d'un pays ; en incluant par exemple des indicateurs de santé, d’espérance de vie, de taux d'alphabétisation. Le Programme des Nations Unies pour le développement (PNUD) a ainsi créé l'indice de développement humain (IDH) dans les années 1990.Des tentatives pour prendre en compte d'autres dimensions telles la sécurité ou pour inclure la « soutenabilité écologique » de l'activité économique dans des indicateurs ont aussi été menées plus récemment.Parmi les nombreux indicateurs économiques très souvent utilisés figurent en premier lieu le Produit intérieur brut (PIB), dont on surveille le taux de croissance afin de mesurer la croissance économique, et le Produit national brut qui permet de comparer les puissances économiques des différentes nations. Sont aussi souvent utilisés le taux d'inflation et des indices du niveau des revenus, de celui de la richesse, ou encore le salaire minimum, le salaire moyen et l'indice de Gini, lesquels fournissent divers aperçus de la répartition et de l'inégalité des revenus. De nombreux indicateurs financiers sont enfin d'usage de plus en plus courant avec l'essor de la mondialisation financière.La mesure de la production d'un pays se fait généralement par le Produit national brut (PNB) et le Produit intérieur brut (PIB). Le PIB est défini comme la valeur totale de la production interne de biens et services dans un pays donné au cours d'une année donnée par les agents résidents à l’intérieur du territoire national. C'est aussi la mesure du revenu provenant de la production dans un pays donné. Ces indicateurs correspondent au développement des comptes nationaux mis en place après la Seconde Guerre mondiale. Ils sont limités du fait des conditions historiques de leur apparition, à la fois dans leur mesure et au niveau conceptuel.Le Produit national brut (PNB) vise à évaluer la valeur des productions nationales réalisées aussi bien sur le territoire d'un pays qu'à l'étranger. Pour ce faire, il retranche du PIB les productions et services réalisés sur le territoire par les non-résidents (donnant lieu au versement de revenus hors du pays) et lui ajoute la valeur des produits et services effectués à l'étranger par des résidents (entreprises ou personnes qui ont donc reçu des paiements de revenus à l'étranger). En dehors de ces ajustements comptables correspondant à la balance des paiements, le PNB présente les mêmes défauts et qualités que le PIB.Pour évaluer la richesse, on utilise souvent le Revenu national brut (RNB) qui fournit une mesure des revenus monétaires acquis durant l'année par les ressortissants d'un pays. Cet agrégat comptable est, au niveau d'un pays, peu différent de la production nationale brute du fait que le PNB est égal à la somme des revenus bruts des secteurs institutionnels, à savoir de la rémunération des salariés, des impôts sur la production et des importations moins les subventions, de l'excédent brut d’exploitation (assimilé au revenu des entreprises) et du solde de revenu avec l'extérieur.Mais les données de patrimoine constituent de meilleures mesures de la richesse proprement dite. Il est difficile toutefois d'obtenir des évaluations comparables du patrimoine quand bien même on se limite seulement aux valeurs monétaires. Le problème devient encore plus ardu si on veut inclure des évaluations du patrimoine physique (immeubles, usines, outils de production, etc.), du patrimoine culturel (monuments, œuvres d'art présentes dans des musées, etc.), et plus encore du patrimoine social. Il serait nécessaire pour cela d'établir des conventions comptables et, si l'on veut effectuer des comparaisons internationales, de se mettre d'accord au niveau mondial sur leur utilisation. Or de telles opérations nécessitent des négociations et des accords internationaux très longs à réaliser.La montée de la mondialisation financière depuis les dérégulations impulsées par les administrations Reagan et Thatcher s’est traduite depuis les années 1980 dans un développement considérable des besoins d’information sur les évolutions des marchés financiers internationaux et les données financières relatives aux obligations d'information et aux états financiers des sociétés cotées. Depuis cette époque, les chiffres de la croissance et du PNB voisinent de plus en plus avec le spectacle des évolutions de l’euro, du dollar et du yen d’un côté, du Dow Jones, du NASDAQ, du Nikkei ou du CAC 40 de l’autre. En effet, les acteurs de la mondialisation que sont les cadres des entreprises financières ou non financières tournées vers l'exportation ont besoin de suivre quotidiennement ces variables de base de leurs arbitrages que sont les taux de change et les niveaux de valorisation boursière. Ainsi, avec la multiplication des expositions des firmes et des nations aux risques de change et aux risques financiers se développent des besoins d’indicateurs en tout genre, de « risque client » ou de « risque pays émergent », associés à chaque type de transactions. La Caisse des dépôts et consignations a créé ainsi des indicateurs synthétiques de libéralisation financière et de crise bancaire informant sur les vulnérabilités associées aux opérations financières mondialisées dans les pays émergents.Les critiques ont été historiquement nombreuses vis-à-vis des indicateurs économiques « classiques ». Marilyn Waring, première femme députée au Parlement néo-zélandais, a souligné que les tâches ménagères et le temps consacré par les parents à l’éducation des enfants, en particulier par les femmes et surtout les femmes dites « inactives », étaient occultés par les mesures de production par individu. En outre, des indicateurs comme le PIB mesurent mal l'économie informelle ou les services domestiques comme le faisait remarquer Alfred Sauvy. Enfin, ils se concentrent sur la valeur ajoutée, et non sur la richesse possédée (stock de capital). Dès lors, une catastrophe naturelle qui détruit de la richesse va pourtant contribuer au PIB à travers l'activité de reconstruction qu'elle va générer. Cette contribution ne reflète pas la destruction de capital, ni le coût de la reconstruction. Cette contradiction était dénoncée dès 1850 par l'économiste français Frédéric Bastiat qui, dans Sophisme de la vitre cassée, écrivait que « la société perd la valeur des objets inutilement détruits », ce qu'il résumait par : « destruction n'est pas profit. »Depuis la fin des années 1980, de multiples mouvements ont mis en cause les capacités du PNB à représenter toutes les dimensions du niveau de vie. Ainsi, au début des années 1990, certaines institutions internationales du système de l'Organisation des Nations unies ont fait un travail de pionnier en proposant de nouveaux indicateurs de développement. La collaboration d'économistes comme Amartya Sen, avec le Programme des Nations unies pour le développement (PNUD), a permis de proposer successivement toute une batterie de nouveaux indicateurs multidimensionnels du développement qui incluent, en plus du PNB, des critères sociaux. Le plus connu est l'Indice de développement humain (IDH). Depuis, de nombreuses autres initiatives se sont multipliées.Le PIB (défini ici comme ""la valeur monétaire des biens et services produits durant une certaine période dans un pays""), est un indicateur très superficiel car :- c'est un indicateur global, qui ne tient pas compte de la répartition de la richesse créée (même en le divisant par la population on obtient qu'un indicateur de la répartition potentielle de la richesse produite, et non de la répartition réelle) ? le PIB/population peut s'accroître avec la richesse d'une minorité de la population tandis qu'une majorité devient plus pauvre ;- c'est un indicateur à court terme, qui ne tient pas compte de l'impact de la production sur la déplétion du capital naturel.- il repose sur l'hypothèse que le prix donne une mesure ""exacte"" de la qualité de la richesse produite globalement, c'est-à-dire de la mesure dans laquelle elle répond aux besoins de l'ensemble de la population; or les sondages d'opinions montrent que les budgets militaires (qui représentent une part importante du PIB et qui sont financés par les impôts payés par la population) ont toujours été supérieurs au niveau souhaité par la population, qui préfère que l'on consacre plus de ressources au sport, à la culture ou aux transports en commun; d'autre part une partie importante du PIB résulte d'achats provoqués par le conditionnement imposé que constitue la publicité dans les lieux publics, et qui donc gonflent artificiellement le PIB.Indice de la puissance économique d'une nation, le PNB mesure la richesse d'un pays. Mais il ne fournit qu'une mesure très approximative du bien-être des habitants qui y vivent. Il ne fournit en effet qu'une agrégation comptable des valeurs des différents biens et services marchands produits, quelles que soient les utilités de ces productions. Par exemple, le PNB ne prend pas en compte les externalités négatives de la production (les dégâts causés à l'environnement, les prélèvements sur le patrimoine, etc.). Il ne mesure pas non plus l'impact de toutes les activités non monétarisées et réalisées hors du champ économique proprement dit (travaux domestiques, éducation des enfants, activités artistiques, etc. – ensemble théorisés par l'opéraïsme italien sous le nom de « travail social », et qui concerne souvent les femmes), lesquelles augmentent le bien-être général.Dans le cas des États-Unis, par exemple, le PNB agglomère indistinctement la production de biens qui ne contribuent pas directement au bien-être des habitants (aides au développement, etc.), avec celles des biens ou services produits et consommés par les américains.L'indice de développement humain (IDH) est le premier des indices créés par le Programme des Nations Unies pour le développement (PNUD). Utilisé depuis les années 1990, l'IDH combine trois facteurs permettant d'apprécier les « capacités » des résidents de ces pays (leurs capabilities selon l'économiste Amartya Sen) : l'espérance de vie à la naissance,l'accès à l'éducation, mesuré à partir de la durée moyenne de scolarisation des adultes (en années) et de la durée attendue de scolarisation des enfants en âge scolaire (en années).ainsi que le niveau de vie réel par habitant calculé à partir du logarithme du revenu national brut par habitant en parité de pouvoir d'achat (PPA).L'IDH classe les pays en établissant la moyenne entre ces trois indices principaux « normalisés » (c'est-à-dire ramenés à une échelle de 0 à 1).Le PNUD publie trois indicateurs synthétiques intégrant d'autres dimensions que l'activité économique :D'abord, à partir de 1995, l'Indicateur Sexospécifique (ou sexué) de développement humain (ISDH), qui permet de corriger l'IDH d'un facteur d'autant plus positif que les différences entre les situations des femmes et des hommes sont moins importantes du point de vue des trois critères pris en compte dans le développement humain.Puis, à partir de 1995 également, l'Indicateur de Participation des Femmes (IPF) à la vie économique et politique, lequel complète le précédent en faisant la moyenne d'un certain nombre de taux de participation des femmes à des postes politiques ou économiques valorisés.L'Indicateur de pauvreté humaine (IPH) est introduit à partir de 1997. Il est construit sous un autre principe que celui des capabilities de Amartya Sen. Il signale les manques, privations ou exclusions fondamentales d'une partie de la population en tenant compte de quatre facteurs : longévité, éducation, emploi et niveau de vie. Deux variantes de calculs sont distinguées :une variante 1 (IPH-1) pour les pays économiquement en développementune variante 2 (IPH-2) pour les pays économiquement développés. Pour les pays développés, l’IPH-2 tient compte de quatre critères auxquels il accorde le même poids : la probabilité de mourir avant soixante ans, l'illettrisme, le pourcentage de personnes en deçà du seuil de pauvreté, soit 50 % du revenu médian, le pourcentage de chômeurs de longue durée.Au début des années 2000, dans la lignée du mouvement impulsé par le PNUD, de nombreuses institutions se sont mises à discuter des limites du PNB pour tenter de les dépasser. En novembre 2004 à Palerme, l'Organisation de coopération et de développement économiques (OCDE) a organisé un premier Forum mondial de l’OCDE sur ce thème. Fin juin 2007, l'OCDE a organisé un second colloque à Istanbul portant sur « les statistiques, les connaissances et les politiques ». Il a débouché sur une déclaration énergique exhortant les bureaux statistiques du monde entier à « ne plus se limiter aux indices économiques classiques comme le produit intérieur brut (PIB) ».Face aux mises en cause multiformes de la mondialisation, il s’agissait d’abord, comme le déclarait le secrétaire général de l'OCDE Angel Gurria, de « mesurer en quoi le monde est devenu meilleur ». Afin de mettre en œuvre et généraliser cette déclaration signée par l’ONU et le PNUD, la Commission européenne a réuni les 19 et 20 novembre 2007 à Bruxelles un colloque international dénommé Beyond the GDP (Au-delà du PIB), durant laquelle son président, José Manuel Durão Barroso, défendait la mise en place de nouveaux indices pour mesurer les problèmes contemporains.Ces réunions institutionnelles ont rassemblé une large part des nombreux indicateurs alternatifs mis au point dans le monde entier afin d'évaluer le bien-être social et environnemental. Parmi ces indicateurs synthétiques alternatifs, certains concernent les problèmes sociaux contemporains, d'autres les inégalités et la pauvreté, la sécurité économique et sociale ou le patrimoine écologique.L’Indice de santé sociale (ISS) a été mis au point aux États-Unis par deux chercheurs, Marc et Marque-Luisa Miringoff. L’ISS est un indice social synthétique visant à compléter le PIB pour évaluer le progrès économique et social. C'est une sorte de résumé des grands problèmes sociaux présents dans le débat public aux États-Unis dans les années 1990. Il se traduit dans seize indicateurs sociaux dont il fait une sorte de moyenne. Sont ainsi regroupés dans cet indice des critères de santé, d'éducation, de chômage, de pauvreté et d'inégalités, d'accidents et de risques divers. L'ISS a acquis une grande réputation internationale en 1996, année de la parution d'un article majeur dans la revue économique Challenge montrant le décrochage des courbes de progression du PNB et de l'ISS aux États-Unis, le premier continuant à progresser alors que le second plongeait durablement après les années 1973-1975. Ce graphique montre ainsi en quoi les années Reagan et Bush père ont porté un rude coup à la santé sociale des États-Unis, laquelle se trouvait en 1996 à un niveau nettement inférieur à celui de 1959, en dépit d’une très belle courbe de croissance économique.Le BIP 40 est un indicateur synthétique de l'évolution des inégalités en France dont le nom est une référence ironique à la fois au PIB (inversé) et au CAC 40. Cet indicateur a été mis au point et présenté à la presse en 2002 par réaction au fait que la santé économique et la santé boursière ont droit à des indices synthétiques fortement médiatisés, alors que ce n'est pas le cas pour ceux de la « santé sociale ». Cela même si l’Insee publie de nombreuses études et indicateurs sur le sujet. L'équipe de militants syndicalistes, d'économistes et de statisticiens français qui ont agrégé des indicateurs pour former le BIP 40 est associée à un réseau associatif militant pour la réduction des inégalités, le Réseau d’alerte sur les inégalités (RAI).De façon récente, des chercheurs de grandes institutions internationales (comme Guy Standing au BIT à Genève) et de pays développés (tels Lars Osberg et Andrew Sharpe au Canada ou Georges Menahem en France) ont mis au point des indicateurs visant à cerner le degré de protection économique des personnes contre les principaux risques de perte ou de diminution forte de leurs revenus, par exemple en matière de chômage, de maladie, de retraite, etc.  L'indicateur de bien-être économique de Osberg et Sharpe Osberg et Sharpe prennent ainsi en compte quatre composantes caractérisant le bien-être des populations dans la construction d’un Indicateur du bien-être économique (IBEE) :les flux effectifs de consommation par habitant, qui incluent la consommation de biens et services marchands, les flux effectifs par habitant de biens et services non marchands et les changements dans la pratique des loisirs ;l’accumulation nette dans la société des stocks de ressources productives, y compris l’accumulation nette de biens corporels et de parcs de logements, l’accumulation nette de capital humain et des investissements en Recherche & Développement (RD), les coûts environnementaux et la variation nette du niveau de l’endettement extérieur ;la répartition des revenus, selon l’indice de Gini sur l’inégalité, ainsi que l’ampleur et l’impact de la pauvreté ;la sécurité économique contre le chômage, la maladie, la précarité des familles monoparentales et des personnes âgées.Grâce à leur indicateur ils sont en mesure de comparer les tendances d’évolution du bien-être économique dans six pays de l’OCDE : États-Unis, Royaume-Uni, Canada, Australie, Norvège et Suède. Des comparaisons sont ainsi données sur le site du laboratoire de ces deux chercheurs canadiens. Une application de l'IBEE au cas de la France a été proposée par Florence Jany-Catrice et Stephan Kampelmann en juillet 2007. L'indicateur de sécurité de Standing à l'OIT Dans les travaux de Guy Standing effectués dans le cadre du Bureau international du travail (BIT), la vision est centrée sur le travail et vise à cerner la sécurité économique dans sept domaines. Dans un deuxième temps, un indice synthétique permet d'effectuer une moyenne de ces sept domaines : les revenus (y compris les prestations sociales), la participation à l’activité économique, la sécurité d’emploi, la sécurité du travail (contre les risques d'accidents ou de maladies professionnels), la sécurité des compétences et qualifications, la sécurité de carrière, et enfin celle de la représentation syndicale et d’expression des salariés. Une série de grandes enquêtes ont ainsi été menées par les missions locales du BIT dans une vingtaine de pays. Les pays scandinaves sont à nouveau aux premières places pour cet indicateur. L'indicateur de sécurité de Menahem En France, Georges Menahem a mis au point en 2005 un indicateur baptisé taux de sécurité économique (TSE). Selon ses dernières publications, la sécurité économique peut être décomposée entre une partie « marchandisée » dépendante des relations salariales et de la vente des produits, et une partie ""démarchandisée"" relative aux prestations et aides auxquelles les individus ont droit indépendamment de leurs relations actuelles avec le marché (comme la retraite, les allocations familiales, de logement, de chômage ou le RMI). Ses estimations sur une trentaine de pays montrent que le taux de sécurité démarchandisée est un bon indicateur de l'efficacité du système de protection sociale : il est maximum en Suède et dans les pays Nordiques, il est encore important dans les pays continentaux tels l'Autriche, l'Allemagne ou la France, mais il est faible au Royaume-Uni et dans les pays Européens du Sud comme l'Italie, la Grèce ou l'Espagne et très limité dans les pays d'Europe Centrale et Orientale tels la Lettonie ou la Lituanie. Quant aux États-Unis, leur taux de sécurité démarchandisée est négatif, ce qui témoigne du mauvais état des protections sociales dans ce pays présenté comme un modèle de l'économie de marché. Ce taux n'est que faiblement positif dans deux autres exemples du modèle « libéral » selon le sociologue danois Gosta Esping-Andersen : en Australie et au Canada, à un niveau à peine plus élevé pour ce dernier car les programmes sociaux y sont plus étendus.L'Empreinte écologique est un indicateur visant à mesurer les pressions économiques sur l'environnement. L’empreinte écologique d’une population est la surface de la planète, exprimée en hectares, dont cette population dépend compte tenu de ce qu’elle consomme. Les principales surfaces concernées sont consacrées à l’agriculture, à la sylviculture, à la pêche, aux terrains construits et aux forêts capables de recycler les émissions de CO2. Il s’agit d’un indicateur synthétique, qui « convertit » en surfaces utiles de multiples pressions humaines sur l’environnement, mais pas toutes.On peut calculer cette empreinte pour une population allant d’un seul individu à celle de la planète, et par grands « postes » de la consommation. Par exemple, la consommation alimentaire annuelle moyenne d’un Français exige 1,6 hectare dans le monde ; son empreinte totale (alimentation, logement, transports, autres biens et services) est de 5,3 hectares. Pour un Américain, on obtient 9,7 hectares : le record du monde. Or l’empreinte par personne « supportable » par la planète aujourd’hui, compte tenu des rythmes naturels de régénération des ressources était de 2,9 hectares en 1970, et elle ne cesse de diminuer sous l’effet de la progression de la population, de la régression des terres arables, des forêts, des ressources des zones de pêche, etc. Elle est passée à 2 hectares en 1990 et elle n’est plus que de 1,8 hectare en 2001. Si tous les habitants de la planète avaient le mode de vie des Américains, il faudrait 5,3 planètes pour y faire face. Si tous avaient le niveau de vie moyen des Français, il en faudrait près de trois.De nombreux rapports ont déjà été produits, dont ceux particulièrement documentés et fiables du Global environmental conservation organization (soit le World Wide Fund for Nature ou WWF). Mais leurs conséquences sont limitées compte tenu de la faible visibilité dans la sphère publique de ce problème, ses conséquences négatives sur la vie quotidienne ne touchant pas encore vraiment les acteurs économiques, politiques et médiatiques dominants et les nations les plus favorisées même si leur empreinte écologique est pourtant de loin la plus importante. De ce fait, elles peuvent croire encore dans les bénéfices d’une croissance matérielle soutenue et indéfinie, les indicateurs des limites de notre planète matériellement finie étant difficiles à percevoir.L'empreinte écologique est un indicateur abstrait et synthétique qui ne traduit qu'une faible part des conséquences du dérèglement du climat et des dégradations des écosystèmes. La comparaison de l'empreinte de l'Afrique et de l'Europe montre certes que les pays les plus pauvres ont encore, pour quelque temps, une empreinte écologique par personne très supportable par la planète, ce qui permet aux pays favorisés d'utiliser bien plus que leur surface. Ainsi, les dommages restent au faible niveau des premiers signes que nous observons actuellement. Selon le WWF, ce résultat traduit une dette écologique des pays riches par rapport aux pays pauvres : les premiers « empruntant » aux seconds d’énormes surfaces de ressources naturelles, terres arables, forêts, aux pays du Sud. Tout se passe comme s'ils y exportaient leur pollution, au moins celle qui ne connait pas de frontière, à commencer par celle des gaz à effet de serre.Mais l'empreinte écologique est limitée car elle ne permet d'illustrer que très indirectement l'importance des conséquences du réchauffement climatique :L’accélération du réchauffement climatique dans la période récente directement liée aux émissions d’origine humaine de gaz à effet de serre.La dimension des catastrophes humaines mondiales prévisibles au-delà d’un réchauffement de deux degrés : sècheresses, inondations et tempêtes, élévation du niveau des mers, etc.L'importance et la diversité de ces catastrophes à venir suggère qu'il faudrait compléter l'empreinte écologique par une batterie d'indicateurs d'inégalités économiques et sociales afin d'évaluer en quoi certaines populations plus pauvres sont davantage touchées par ce que les populations riches nomment les « aléas » climatiques. La moitié de la population mondiale vit ainsi dans des zones côtières qui risquent d'être submergées si le niveau des mers s’élevait d’un mètre, évolution possible pour le siècle à venir selon le Groupe d'experts intergouvernemental sur l'évolution du climat (GIEC) si les tendances actuelles persistent.Jean Gadrey et Florence Jany-Catrice, Les nouveaux indicateurs de richesse, Édition La Découverte, 2005Dominique Méda,  Au-delà du PIB. Pour une autre mesure de la richesse, Champs-Flammarion, 2008Rapport mondial sur le développement humain 2005 : La coopération internationale à la croisée des chemins : l’aide, le commerce et la sécurité dans un monde marqué par les inégalités, New York, Programme des Nations unies pour le développement, janvier 2005, 385 p. (ISBN 2-7178-5114-3, lire en ligne), chapitre 2 « Inégalité et développement humain ».WWF: Global environmental conservation organization (ou Organisation de conservation de l'environnement mondial)PIB et développement durableHappy planet indexIndice rouge à lèvresIndicateur du développementLes grands indicateurs de l'économie française, site de l'InseeLes indicateurs économiques du Loiret , iD Loiret Portail de l’économie"
économie;"La macroéconomie est une discipline de l'économie qui étudie le système économique au niveau agrégé à travers les relations entre les grands agrégats économiques que sont le revenu, l'investissement, la consommation. La macroéconomie constitue l'outil essentiel d'analyse des politiques économiques des États ou des organisations internationales. Il s'agit d'expliquer les mécanismes par lesquels sont produites les richesses à travers le cycle de la production, de la consommation, et de la répartition des revenus au niveau national ou international.La macroéconomie est une branche de la science économique. Elle se consacre à l'étude des grandes variables économiques nationales ou internationales, et aux relations entre ces variables. Elle repose sur une approche globale, quoiqu'elle puisse se fonder sur des comportements microéconomiques et micro-ondes.Parce qu'elle fonctionne par la comparaison d'agrégats, la macroéconomie est avant-tout une représentation hiérarchisée de l'économie, articulée entre ses agents, via des flux. Elle cherche à expliciter ces relations et à prédire leur évolution face à une modification de certaines variables. La macroéconomie permet par exemple d'estimer la réaction d'un système économique possédant telles caractéristiques face à un choc pétrolier, ou à une politique économique particulière.Contrairement à la microéconomie, qui favorise les raisonnements en équilibre partiel, la macroéconomie se place toujours dans une perspective d'équilibre général, ce qui l'amène à accorder plus d'attention au bouclage des modèles et à la dynamique de création et de maintien d'institutions essentielles, comme les marchés, la monnaie.La macroéconomie a évolué à travers le temps pour devenir plus précise et plus sûre. Ses origines se trouvent dans les premiers travaux économiques du XVIIIe siècle (voir Histoire de la pensée économique), mais elle prend véritablement forme grâce aux travaux de John Maynard Keynes. Sa théorie, le keynésianisme, se fonde sur ce qui fut appelée la macroéconomie keynésienne, qui est l'interprétation keynésienne de la macroéconomie. Elle crée des outils de base de la macroéconomie (le modèle IS/LM, la courbe de Phillips, etc.).La macroéconomie n'est aujourd'hui plus rattachée à une quelconque école de pensée. Elle a évolué vers la construction de modèles économiques complexes, incluant à la fois des relations supposées entre variables et des relations comptables servant à définir les agrégats. Très utilisés pour analyser et prévoir les résultats des politiques économiques, ces vastes modèles qualifiés, le plus souvent, d'économétriques (les plus frustes comportent une dizaine d'équations, les plus complexes dépassent les 1 500) sont à l'heure actuelle employés par la plupart des gouvernements, institutions statistiques (comme l'INSEE), organisations internationales (OCDE) et certains acteurs privés voulant disposer de leurs propres prévisions quant à la conjoncture.Les penseurs de la Grèce antique, tels que Platon, Aristote et Xénophon, avancent des idées économiques. Celles-ci sont toutefois souvent liées à la gestion de la maisonnée, et l'économie désigne l'art de bien administrer sa maison. La microéconomie primitive naît ainsi avant la macroéconomie. Certains auteurs font toutefois remarquer que, chez Platon notamment, la place de l'économie de la cité est parfois pensée dans le cadre des relations géopolitiques,. Il y aurait donc des bribes de pensée macro chez les Grecs.Au XVIIe siècle, des premières réflexions sur la monnaie et l'économie agricole émergent. Les pensées développées sont assez primitives, et il faut attendre le XVIIIe siècle, avec son courant physiocrate, pour qu'un premier système macroéconomique soit ébauché. Cette représentation se trouve dans l'ouvrage de François Quesnay appelé le Tableau économique. Quesnay, médecin de la famille royale, cherchait à représenter l'économie sur les bases de la circulation du sang, comme un système cohérent et dynamique,. Certains auteurs rechignent toutefois à utiliser le terme de macroéconomie pour désigner la pensée de Quesnay, et préfèrent celui de circuit économique.Les Classiques pensent à la fois le niveau micro et le niveau macro. Adam Smith étudie les questions de répartition des richesses et de fonctionnement global de l'économie, tandis que David Ricardo marque l'histoire de la théorie du commerce international. Ils posent donc les jalons de la macroéconomie, conceptualisant le marché et sa concurrence, et mobilisant une loi générale (la loi des débouchés élaborée par Jean Baptiste de Say) qui explique l'économie comme un système. Ils ont toutefois également une pensée micro, car ils réfléchissent à la théorie de la valeur. La pensée des Classiques va jusqu'à créér des relations entre la micro et la macroéconomie. Adam Smith considère, ainsi, que la satisfaction des intérêts particuliers (niveau micro) permet la réalisation de l'intérêt général (niveau macro).Karl Marx propose à son tour une représentation schématique de l'économie industrielle de son époque. Il conceptualise le cycle monnaie-marchandise-monnaie et la baisse tendancielle du taux de profit. Il raisonne en classes sociales, comme ses prédécesseurs, quoiqu'il les redéfinisse. S'attachant à traiter d'agrégats afin de ne pas se perdre dans l'individualité, il adopte une approche macro à l'économie. Il est à ce titre cité dans les manuels de macroéconomie de référence comme un précurseur.Parallèlement, les fondateurs de l'école néoclassique ont utilisé la théorie marginaliste, pour agréger les comportements des agents économiques, c'est-à-dire les consommateurs et les producteurs. Cette microéconomie agrégée, approche souvent à la base de certaines théories macroéconomiques, est à la base de la théorie de l'équilibre général de Léon Walras, et complété par Kenneth Arrow et Gérard Debreu. Cette vision de l'économie ne peut toutefois pas se confondre avec la macroéconomie, étant donné qu'elle ne se base que sur des comportements individuels, et n'analyse pas l'économie dans son ensemble.La distinction systématique entre la microéconomie et la macroéconomie n'émerge vraiment qu'au cours des années 1930. Les travaux de John Maynard Keynes, dont notamment la Théorie générale de l'emploi, de l'intérêt et de la monnaie de 1936, indiquent un intérêt renouvelé pour l'étude agrégée des grandes variables, détachées des questionnements propres à l'économie du niveau micro. Il est question pour cette macroéconomie d'étudier les grandes variables économiques agrégées afin de fournir aux décideurs les clefs de compréhension du monde et des moyens d'action.Keynes écrit ainsi, en 1939, dans la préface à l'édition française de la Théorie générale : « J'ai appelé ma théorie une théorie générale. Par là, j'ai voulu signifier que j'étais principalement intéressé par le comportement du système économique dans son ensemble, avec des revenus globaux, des profits globaux, un produit global, un emploi global, un investissement global, une épargne globale, plutôt qu'avec des revenus, des profits, un produit, l'emploi, l'investissement d'industries, d'entreprise set d'individu particuliers. » Il continue : « je soutiens que l'on a commis d'importantes erreurs en étendant au système dans son ensemble ce qui a été établi correctement pour une de ses parties prise isolément. »L'après-guerre voit l'extension du keynésianisme et sa mise à jour. Repris aux États-Unis, la synthèse néoclassique, macroéconomique, forme le socle des politiques économiques occidentales des Trente Glorieuses. Le monde académique accepte la séparation nette entre microéconomie et macroéconomie. La microéconomie se spécialise alors sur les problèmes d'allocation des ressources par le moyen des prix relatifs (prix d'un produit, ou de plusieurs produits, exprimé par un ou plusieurs autres produits), alors que la macroéconomie étudie la production globale et le niveau des prix.Les échecs de la synthèse néoclassique à prévoir et à enrayer la stagflation (hausse des prix et baisse de la croissance) conduit, au XXe siècle, à l'émergence de nouvelles écoles de pensée qui rénovent les théories macroéconomiques dominantes. Le monétarisme, la nouvelle économie classique et la nouvelle économie keynésienne font partie de ces mouvements de pensée.Robert E. Lucas de la nouvelle économie classique critique la macroéconomie pour son manque de fondements microéconomiques. Cette critique dite de Lucas souligne que la synthèse néoclassique a formé un cadre macroéconomique qui a trop gommé la microéconomie, c'est-à-dire l'étude de la réaction des agents. En effet, l'école de la synthèse les considère généralement comme passifs et incapables d'anticiper les effets des politiques économiques.Parallèlement, les progrès réalisés par le monde académique permet la construction de modèles de plus en plus complexes et élaborés. Rendue possible par l'augmentation des capacités de calcul des ordinateurs ainsi que la généralisation des techniques d'optimisation dynamique, l'explosion des données économiques permettent à la macroéconométrie d'affiner ses capacités prédictives. La critique de Lucas mène aussi à la création de la microéconométrie.Au début du XXIe siècle, des économistes cherchent à dépasser la distinction entre microéconomie et macroéconomie. La plupart des modèles macroéconomiques actuels font l'hypothèse qu'ils ne constituent qu'une simplification de la réalité, dont ils étudient un aspect particulier, comme l'effet de l'innovation sur la croissance, ou des structures monétaires sur l'investissement. De ce fait, ils mélangent relations macroéconomiques et extensions au niveau macroéconomique de relations microéconomiques pour autant que ces extensions soient compatibles avec les faits stylisés qu'on cherche à analyser.La croissance désigne l'augmentation de la richesse produite dans un système économique. Ses causes sont l'un des sujets majeurs de la macroéconomie, qui a proposé différents modèles explicatifs. Le premier grand modèle fut le modèle de la croissance exogène. Il a été dépassé par les modèles de la croissance endogène. Au niveau de ces derniers, le prix Nobel d'économie, l'américain Paul Romer a apporté des contributions majeures.La consommation est l'une des variables majeures étudiées en macroéconomie. Elle est l'une des composantes de la croissance. Elle peut être issue des ménages, des entreprises ou de la puissance publique. La consommation peut être issue de l'extérieur, auquel cas il s'agira d'exportations. En cas d'un manque d'autosuffisance, la consommation peut s'adresser au reste du monde et correspond à des importations.La macroéconomie s'intéresse aux stocks et aux flux. Le stock est une quantité à un moment donnée. Le flux est exprimé en unité de temps (par exemple, sur un an). La dette est un stock constitué de l'ensemble des flux de déficits.La macroéconomie étudie les grands déséquilibres internationaux. Les déficits commerciaux, les déficits budgétaires, les situations de déséquilibres sur les différents marchés internationaux..Toutes les pages avec « macroéconomie » dans le titre(fr) Bernard Guerrien, Une brève histoire de la macroéconomie [lire en ligne (page consultée le 11/09/2015)]Paul Krugman et Robin Wells, Macroéconomie, De Boeck, coll. « Ouvertures économiques », 2009, 2e éd., 998 p.Gregory N. Mankiw, Macroéconomie, De Boeck, coll. « Ouvertures économiques », 2003, 3e éd., 655 p.Olivier Blanchard, Daniel Cohen, Macroéconomie, Pearson éducation, 2002,  (ISBN 2-84211-121-4)David Romer, Macroéconomie approfondie, McGraw Hill / EdiscienceMichel De Vroey, Pierre Malgrange, La théorie et la modélisation macroéconomiques, d’hier à aujourd’hui Document de travail PSE Lire en ligne(en) Gregory Mankiw,""The Macroeconomist as Scientist and Engineer"", NBER Working Paper 12349, juin 2006 Lire en ligneMichael Wickens, Analyse Macroéconomique Approfondie, De Boeck, 2010,  (ISBN 2-8041-6193-5)Gilbert Abraham-Frois, « La macroéconomie en l’an 2000 », Revue économique, vol. 52, no 3,? 2001 (lire en ligne, consulté le 28 juin 2011)Pascal Salin, Macroéconomie, PUF, coll. « Premier cycle », 1991, 400 p. (ISBN 978-2-13-043530-3, lire en ligne) Portail de l’économie"
économie;"Le marketing management ou la direction de la mercatique est le processus de gestion de l’ensemble complexe des méthodologies permettant à l’organisation d’être compétitive dans un univers concurrentiel dynamique sur différents marchés.Il ne faut pas confondre le marketing management, souvent appelé, par raccourci, « marketing », avec le concept du marketing ou l’esprit marketing, qui sont, eux, une culture organisationnelle, l’apprentissage de ces méthodologies, de leur respect et de leur intériorisation par l’ensemble des collaborateurs de l'entreprise.Les méthodologies du marketing management s'appliquent, quant à elles, et quelle que soit la taille de l'entreprise ou de l'organisation, à deux niveaux :Le marketing opérationnel : il s’agit — au niveau marque, service ou produit — de l’optimisation, de la coordination et de la combinaison des leviers (le marketing mix) permettant d'agir sur l’offre, ainsi que de la mise en œuvre d’outils et de techniques choisis pour conquérir des parts de marché.Le marketing stratégique : il s’agit du choix du/des marché(s) sur le(s) quel(s) l’entreprise ou l’organisation décide de se battre et de la définition générale de son attitude et de son positionnement face aux concurrents qui s’y trouvent.Ces méthodologies s’orientent soit vers le marketing de la demande lorsqu’il s’agit de l’appliquer à la demande exprimée par le marché ; soit vers le marketing de l’offre — ou de l’innovation — lorsqu’elle est appliquée à optimiser une offre inédite, technologique ou servicielle.Tous les bons auteurs : Gary Armstrong, Pierre-Louis Dubois, Philip Kotler, Jacques Lendrevie et Julien Lévy, etc. et toutes les grandes associations : American Marketing Association, Association française du marketing, etc. présentent leur définition du marketing management qu'ils abrègent généralement en « marketing ».L'approche du marketing est diverse. C'est, suivant les auteurs : une « méthode », un « ensemble de techniques et de méthodes », un « moyen d'action », une « stratégie d'adaptation », un « processus social et managérial », un « ensemble de décisions et d'actes de gestion », un « ensemble de moyens », une « méthodologie », une « fonction », etc. Dubois, Jolibert, Gavard-Perret et Fournier (2013) Pour Pierre-Louis Dubois, Alain Jolibert, Marie-Laure Gavard-Perret et Christophe Fournier :« Le marketing [management] est l’ensemble des processus mis en œuvre par une organisation (ou autre entité sociale) destiné à comprendre, influencer dans le sens de ses objectifs et contrôler les conditions de l’échange entre elle-même et d’autres entités, individus, groupes ou organisations afin de créer de la valeur pour l’ensemble des parties prenantes. »« Le marketing [management] est alors considéré comme un ensemble de « processus » comprenant des techniques, des méthodes et surtout une méthodologie. Ces techniques et méthodes seront « mises en œuvre  », l’action étant l’un des fondements des sciences de l’action, qui supposent à la fois connaissance, décision et action ». Armstrong, Kotler, Le Nagard-Assayag et Lardinoit (2013) Pour Gary Armstrong et Philip Kotler, adaptés par Le Nagard-Assayag et Lardinoit, le marketing [management] est un processus social et managérial :« Le marketing [management] est un processus social et managérial qui permet à des personnes ou à des organisations de créer de la valeur et de l’échanger avec d’autres, afin d’obtenir ce dont elles ont besoin et d’en retirer de la satisfaction. »« Dans le contexte plus étroit de l’entreprise, le marketing [management] suppose l’établissement de relations d’échanges rentables avec les clients, fondées sur la notion de valeur à long terme. », p. 3. Le Mercator (Lendrevie et Lévy, 2014) Jacques Lendrevie et Julien Lévy donnent, dans leur Mercator, deux définitions du marketing [management] : une définition sommaire et développée, qui soulignent l'importance de l'influence et de la valeur perçue :Une définition sommaire : « Le marketing [management] est un moyen d’action qu’utilisent les organisations pour influencer en leur faveur le comportement des publics dont elles dépendent », p. 3.Une définition développée : « Le marketing [management] est la stratégie d’adaptation des organisations à des marchés concurrentiels pour influencer en leur faveur le comportement des publics dont elles dépendent, par une offre dont la valeur perçue est durablement supérieure à celles des concurrents. Dans le secteur marchand, le rôle du marketing est de créer de la valeur économique pour l’entreprise en créant de la valeur perçue par les clients », p. 5. Houver (2016) Pour Nathalie Houver :« Le marketing [management] est un ensemble de techniques et de méthodes permettant aux entreprises de conquérir, de séduire, de satisfaire, de fidéliser ses clients tout en préservant sa rentabilité ». Hiam et Heilbrun (2016) Alexander Hiam et Benoît Heilbrun considèrent le marketing [management] comme :« Une démarche d’ensemble que les organisations (donc pas seulement les entreprises) mettent en œuvre pour créer de la valeur sur leur marché de référence (quitte à créer ce marché comme ce fut le cas pour toutes les grandes marques !) ».Les associations proposent des définitions qui ont une inertie beaucoup plus longue mais qui regroupent des synthèses nationales. American Marketing Association (2012) Pour l'American Marketing Association, le marketing management est « le processus de fixation des objectifs marketing d’une organisation (en tenant compte de ses ressources internes et des opportunités du marché), de planification et d’exécution de ses activités pour atteindre ces objectifs et de mesure des progrès dans leur réalisation ». Association française du marketing (2016) Pour l'Association française du marketing :« Le marketing management est le regroupement fonctionnel et opérationnel des méthodologies et des pratiques marketing mises en œuvre de façon coordonnée par des organisations pour atteindre leurs objectifs de compétitivité concurrentielle, ces méthodologies et ses pratiques comprenant :l’étude des différents publics, de leurs besoins, usages, désirs et aspirations ;la création d’offres de produits, de services et d’expériences ;la diffusion de ces offres dans une perspective marchande ou non.Ceci implique pour elle :l’établissement de relations équitables avec les différents partenaires de ces organisations, dans le respect des réglementations ;la prise en compte des conséquences futures de ces pratiques sur l’ensemble des parties prenantes et sur la société au sens large ».Le marketing est une façon de penser. Le marketing management est une façon de faire. « Faire du marketing » signifie, en fait, faire du marketing management.Si la formalisation et la diffusion publique des principes du marketing management datent des années soixante, avec la parution du livre de Jerome McCarthy, Basic Marketing. A Managerial Approach, dans lequel il introduit le concept de marketing mix inventé en 1942 par Neil Borden, la pratique du marketing management est beaucoup plus ancienne, le premier produit — au sens moderne du terme — à être lancé en respectant les principes étant L’Eau de mélisse des Carmes Boyer, en 1611.Peter Drucker pense, quant à lui, que le marketing management a été inventé en 1673 au Japon par Mitsui avec son grand magasin Mitsukoshi, puis, vers 1850, aux États-Unis, dans un autre domaine, par Cyrus McCormick avec ses moissonneuses-batteuses.Mais c’est très vite la création à Paris, en 1852, du grand magasin Au Bon Marché d'Aristide Boucicaut et à Philadelphie, en 1861, du Grand Dépôt de John Wanamaker.Dans le domaine des produits de grande consommation, citons, par exemple, les lancements de la sauce Tabasco en 1868, des savonnettes Ivory Soap par Procter & Gamble en 1879, des savonnettes Sunlight par Lever Brothers en Grande-Bretagne, en 1884, appliquant tous, de façon informelle, les principes du marketing management.L'invention du marketing management (ébauche)                                   L'activité économique mondiale — l'argent qui s'échange contre des biens — n'est pas homogène.Si l'on fait nettement la distinction entre :les marchés des services et produits commercialisés par des entreprises, comme IBM, Intel, GE, Airbus, Alstom, Michelin, etc., marketing business to business auprès d'autres entreprises, à des clients, acheteurs professionnels ;les marchés des services et produits de grande consommation et qui s'adresse à des consommatrices et consommateurs — c'est celui de Danone, Louis Vuitton, Coca-Cola, etc.nous parvenons à quatre grands types de marchés.Les quatre grands types de marché et de marketing.Ces quatre grands types de marché sur lesquels les marques ont des fonctions et des valeurs économiques très différentes génèrent des business modèles, des marketing managements et des marketing mix très différents. Les composants du marketing mix de Chronopost n'ont rien à voir avec ceux de No 5 de Chanel ni avec ceux d'une marque de petits pois.L'ensemble de l'activité économique mondiale peut être divisée en secteur d'activité économique — on dit aussi « industrie ». Les diverses classifications existantes s'accordent pour en dénombrer environ deux douzaines : le secteur du tourisme, l'industrie agro-alimentaire, le luxe, etc. Chacun d'entre eux va générer un marketing particulier.Les grands domaines des 24 grands secteurs d'activité économiquesPar ailleurs, quelle que soit sa taille, TPE ou multinationale ; la nature de son activité, production ou distribution ; et la nature de sa production : produits ou services, une entreprise utilise la méthodologie du marketing management dans quatre contextes différents ;d'une part, au niveau stratégique ou au niveau opérationnel ;d'autre part, pour gérer la demande exprimée par le marché ou pour gérer les nouvelles offres proposées aux marchés.Contextes qui génèrent quatre grands types de décision marketing management :I. Un marketing (management) opérationnel de la demande,II. Un marketing stratégique de la demande,III. Un marketing opérationnel de l'offre,IV. Un marketing stratégique de l'offre.Ce qui donne la matrice suivante :Les quatre grands types de décisions du marketing managementLe Marketing Resource Management est à la fois un outil, un processus et une stratégie d’organisation qui s'inscrit dans la continuité du marketing management.Le marketing resource management ou MRM regroupe un ensemble de technologies, de moyens et de processus de production associés à la création et à l’organisation des ressources marketing.Un outil de MRM est une solution technologique qui vient supporter le marketing management, de la conception des ressources marketing jusqu'à leur diffusion.2011Catherine Deydier et Olivier Dauchez, L’Eau de mélisse des Carmes Boyer de 1611 à 2011. 400 ans de bienfaits, Éditions Larivière, 2011.Emmanuelle Le Nagard-Assayag & Delphine Manceau, Le marketing de l'innovation. De la création au lancement de nouveaux produits, 2e édition, Dunod, 2011.2013Sophie Rieunier, Marketing sensoriel du point de vente. Créer et gérer l'ambiance des lieux commerciaux, 4e éd., Dunod, 2013.Pierre-Louis Dubois, Alain Jolibert, Marie-Laure Gavard-Perret, Christophe Fournier, Le Marketing. Fondements et Pratique, 5e édition, Économica, 2013.Gary Armstrong, Philip Kotler. Adapté par Emmanuelle Le Nagard-Assayag et Thierry Lardinoit, Principes de marketing, 11e édition, 2013.2014Jacques Lendrevie et Julien Lévy, Mercator, 11e édition, Dunod, 2014.Jean-Pierre Helfer, Jacques Orsoni, Ouidade Sabri, Marketing (1990), 13e édition, Vuibert, 2014.2015(de) Philip Kotler, Kevin Keller, Marc Opresnik, Marketing-Management. Konzepte - Instrumente - Unternehmensfallstudien, Pearson Studium, 2015.Philip Kotler, Delphine Manceau, Marketing Management, Pearson, 2015.Soraya Cabezon, Marine Lapierre, Internet Marketing 2016, Elenbi Éditeur, 2015.2016Nathalie Houver, Le Petit Marketing, Les pratiques clés en 14 fiches, 6e éd., Dunod, 2016.Alexander Hiam, Benoît Heilbrunn, Le Marketing pour les Nuls, 3e édition, First, 2016.Arnaud de Baynast, Jacques Lendrevie, Julien Lévy, Mercator, 12ième ed. Dunod, 2017.Philip Kotler, Hermawan Kartajaya, Iwan Setiawan, Marc Vandercammen - Marketing 4.0 : Le passage au digital, De Boeck supérieur, 2017.Gilles Bressy, Christian Konkuyt, Management et économie des entreprises 12ième ed. Ch 14 et 15, Aide-mémoire Sirey, Ed. Dalloz, 2018.Sandrine Medioni, Sarah Benmoyal Bouzaglo, Marketing digital, Dunod, 2018.Etudes critiques du marketing et de la consommation Portail du management"
économie;
"signifi""";
économie;"La parité de pouvoir d'achat (PPA) (on parle de valeurs mesurées en parité de pouvoir d'achat) est une méthode utilisée en économie pour établir une comparaison, entre pays, du pouvoir d'achat des devises nationales, ce qu’une simple utilisation des taux de change ne permet pas de faire.Le pouvoir d'achat d’une quantité donnée d’argent dépend en effet du coût de la vie, c’est-à-dire du niveau général des prix. La PPA permet de mesurer combien une devise permet d’acheter de biens et services dans chacune des zones que l’on compare.Les économistes forment un « panier » normalisé de biens et de services, dont le contenu peut être sujet à discussion (à ce sujet, voir en:Discussion and clarification of PPP).La monnaie couramment utilisée comme référence est le dollar américain, pris à une année donnée.Dans un marché global et unifié, sans coûts de transport, les produits identiques ont tous le même prix au même instant et à tous les endroits de ce marché : c'est la loi de prix unique.Cette loi, de nature microéconomique, est théorique. Elle se définit produit par produit, manufacturé ou non (par exemple le cuivre, le café, le ciment, les pneus d'une dimension donnée, une canette de boisson, un hamburger, ce dernier parfois utilisé comme indice rudimentaire de PPA à lui tout seul — l'indice Big Mac). Le monde réel fournit des exemples d'autant plus proches de cette situation théorique que les produits considérés sont mieux standardisés et moins coûteux à transporter.Pour la plupart des produits au contraire, les hypothèses sur lesquelles cette loi repose ne sont pas vérifiées, car le monde est loin d'être un marché unique : les coûts de transport ne sont pas nuls, les réglementations diffèrent en fonction des pays, les droits de douane appliqués aux importations augmentent leurs prix de vente. Par ailleurs, les coûts de fabrication varient fortement en fonction des pays : certaines ressources naturelles sont plus ou moins abondantes, le climat varie, le coût de la main-d'œuvre varie fortement. Les prix sont donc différents d'un endroit à l'autre.Cependant on peut considérer que le consommateur d'un pays substitue à certains produits plus chers dans son pays certains autres moins chers[réf. souhaitée]. Il y a donc non pas un seul produit, mais un ensemble de produits nécessaires à la vie du consommateur moyen. C'est le « panier », qui reflète les habitudes de consommation : au Japon la quantité de protéines nécessaire à la vie est apportée par du soja et du poisson alors qu'en France elle est apportée par de la viande de volaille ou de bovidés.La loi de parité du pouvoir d'achat exprime un coût égal du panier dans tous les pays ayant un niveau de vie raisonnablement comparable. C'est une loi de nature macroéconomique.Les taux de change PPA sont utilisés avant tout dans les comparaisons internationales de niveau de vie. La comparaison internationale des PIB conduit à ne pas prendre en compte les différences de prix existant entre les pays. Les écarts entre les taux de change réels et les taux de change PPA peuvent être significatifs. Ainsi, lorsque le yen, la monnaie japonaise, est surévalué, comme en 1999, le PIB par habitant paraît beaucoup plus élevé que son équivalent américain, alors que mesuré en PPA, il est en réalité beaucoup plus bas.Cette méthode permet de s'affranchir de trois problèmes :les taux de change des devises peuvent connaître des variations subites et brutales sans qu'il y ait modification des conditions économiques. Une comparaison internationale des évolutions à court terme serait faussée par une utilisation des taux de change du marché ;les devises des pays pauvres sont systématiquement sous-évaluées sur le marché des changes du fait de leur moindre productivité (c’est l’effet Balassa-Samuelson) ;certains pays fixent administrativement le taux de change de leur devise. Cela a pour effet de fausser les statistiques et les comparaisons internationales. C'était en particulier le cas des pays d’Europe de l'Est avant 1989.L'utilisation des PPA permet de s'affranchir de ces trois effets.La PPA est parfois utilisée comme un indicateur de la sous-évaluation ou surévaluation d'une devise par rapport à une autre sur le marché des changes. L'exercice est hasardeux, compte tenu des incertitudes inhérentes à cet instrument de mesure.La PPA absolue définit un cours de change entre deux monnaies. Elle est déterminée en définissant un panier de consommation dans un pays A et en évaluant le prix d’un panier « semblable » dans un pays B par la formule :                    P        P                  A                      t                          =                                            P                              t                                                    P                              t                                            ?                                                    .              {\displaystyle PPA_{t}={\frac {P_{t}}{P_{t}^{*}}}.}  où PPAt est la PPA absolue entre les deux pays sur la période t, et Pt est le prix sur la période t du panier de référence dans le pays A. L'autre pays, B, est marqué par un astérisque.Pour prendre un exemple chiffré, fictif, si un panier de produits évalués à 1 000 $ aux États-Unis a un coût moyen de 900 € en France, alors le taux de change en PPA du dollar par rapport à l'euro sera de 0,90. Ce taux est calculé indépendamment du cours de l’euro en dollar sur les marchés des changes, qui peut fortement fluctuer.La PPA relative mesure la variation de la PPA entre deux périodes. Elle s'exprime ainsi :                                                        P              P                              A                                  t                                                                    P              P                              A                                  t                  ?                  1                                                                    =                                                            P                                  t                                                            /                                            P                                  t                  ?                  1                                                                                    P                                  t                                                  ?                                                            /                                            P                                  t                  ?                  1                                                  ?                                                                          {\displaystyle {\frac {PPA_{t}}{PPA_{t-1}}}={\frac {P_{t}/P_{t-1}}{P_{t}^{*}/P_{t-1}^{*}}}}  où PPAt est le taux de change et Pt est le prix à la période t. Le pays étranger est marqué par un astérisque.Une variation de la PPA relative permet de mettre en évidence un différentiel d’inflation entre deux régions du monde.Plusieurs arguments limitent la pertinence et l’usage des PPA :les PPA peuvent varier de façon très importante suivant le choix du panier de produits. En ce sens, il est soumis aux mêmes limitations que les indices des prix ;les habitudes de consommation et les choix sont parfois très variables entre pays. Les produits consommés par les populations en dépendent et construire deux paniers équivalents est un travail très subjectif ;les différences de qualité pour deux produits mis en équivalence sont difficiles à évaluer ;les prix peuvent beaucoup varier à l’intérieur d’un même pays. Le prix d’un verre de bière est beaucoup plus élevé dans un bar sur les Champs-Élysées que dans un village du Massif central ;les prix des produits importés dépendent du taux de change. Une modification du taux de change a donc une influence sur la PPA alors que celle-ci est construite pour définir une parité de change décorrélée du marché des changes.Des indicateurs comme l’indice Big Mac, construit initialement par The Economist, ou l’indice iPod, assez frustes, sont parfois utilisés à des fins pédagogiques.Liste des pays par PIB (PPA)Liste historique des régions et pays par PIB (PPA)Programme de comparaison internationale(en) Penn World Table: tableaux de référence sur la parité de pouvoir d'achat et les revenus nationaux convertis en prix internationaux pour plus de 169 pays sur la période 1950 jusqu'à récemment. Site de l'université de Groningue.(fr) Le marché ou la PPA : Quelle base de comparaison choisir ? - Étude du FMI, mars 2007 [PDF](fr) Parités de pouvoir d’achat : mesure et utilisations - Cahier statistique de l'OCDE sur les PPA, mars 2002 [PDF](fr) Prix et salaires 2015 - Est-ce que je gagne assez pour me permettre la vie que je veux ? [PDF] Portail de l’économie"
économie;"Un processus d'affaires, également appelé processus métier ou processus d'entreprise ou processus opérationnel (en anglais « Business process »), désigne un ensemble d'activités corrélées ou en interaction qui contribue aux finalités des affaires d'une organisation. Il peut être structuré en procédés d'affaires, c'est-à dire selon ISO 19510 en un ou plusieurs ensembles définis d'activités qui représentent les étapes nécessaires pour atteindre des objectifs relatifs aux affaires, y compris les flux et utilisations d'informations et de ressources.Bien que le terme de « processus métier » soit souvent utilisé dans un sens général de manière interchangeable avec « processus d'affaires », il peut aussi être employé de manière plus restrictive pour marquer la différence entre les processus spécifiques au métier de l'entreprise et les processus plus généraux de support,,. L'idée du processus émerge au XVIIIe siècle, sans être nommée, en relation avec le concept de métier. En effet, en 1751 parait le premier tome de l'Encyclopédie ou Dictionnaire raisonné des sciences, des arts et des métiers de Diderot et d'Alembert, un ouvrage de référence qui décrit minutieusement de nombreux métiers. Le cinquième tome paru en 1755 contient par exemple un article de l'encyclopédiste Delaire relatant en détail les 18 étapes de la fabrication d'épingles, de l'arrivée du fil de laiton et son jaunissement, jusqu'à l'emballage du produit fini et ses débouchés. On ne parle alors pas encore de processus. Le terme procédé est, lui, déjà utilisé dans l'Encyclopédie pour décrire des techniques employées pour certaines étapes (exemple : procédés métallurgiques, ou différents procédés d’émaillage).En 1776, l'économiste écossais Adam Smith publie Recherche sur la nature et les causes de la richesse des nations, ouvrage dans lequel il élabore sur la base d'un exemple très similaire, le principe de la division du travail source de productivité. L'industrialisation se développe en suivant ce modèle économique basé sur le principe de spécialisation. Vers 1880, l'ingénieur américain Frederick Winslow Taylor approfondit ce principe de spécialisation avec l'organisation scientifique du travail sur laquelle se base le taylorisme. Les processus et les activités ultra-spécialisées deviennent la norme des industries de production de masse. En conséquence, les entreprises se structurent par fonction; ainsi, dès 1916, l'ingénieur français Henri Fayol identifie 6 grands groupes de fonctions, dont les fonctions techniques, commerciales, et financières.Dans les années 1950, l'ingénieur japonais Taiichi ?no cherche à fluidifier les processus de production de Toyota, et met au point les principes du juste-à-temps basé sur la réduction des délais et les inefficacités engendrés par un trop grand morcellement de l'activité et par un manque d'implication du personnel. A la même époque, l'américain Edwards Deming commence à propager sa théorie sur la qualité. Néanmoins, ces deux théories vont à l'encontre du taylorisme toujours en vogue. Ce ne sera donc que vers la fin des années 1970 que se popularisent les principes du management par la qualité totale (TQM) et la vision plus complète que celle-ci requiert sur les activités.   Les années 1980 voient alors renaître un intérêt managérial pour les processus vus dans leur globalité, que ce soit du point de vue de la stratégie, des finances ou des opérations. En 1985, l'économiste Michael Porter développe ainsi dans son ouvrage Competitive Advantage la notion de chaine de création de la valeur. Celle-ci s'articule autour d'« activités » transverses de l'entreprise et de combinaisons d'activités au travers de plusieurs entreprises. Son analyse ne s'intéresse alors pas aux aspects opérationnels ou organisationnels, mais aux avantages concurrentiels qu'offre une bonne combinaison de segments stratégiques avec des synergies entre processus. En 1986, le Consortium for Advanced Manufacturing International (CAM-I) publie de son côté Cost Management in Today's Advanced Manufacturing Environment: The CAM-I Conceptual Design, qui définit l'analyse de coûts par activité (ABC), et son corollaire, une analyse financière des processus. Enfin, en 1987, l'Organisation Internationale de Normalisation (ISO) adopte la norme de management de la qualité ISO 9001, qui se base sur une approche par processus.       Dans les années 1990, le concept de ré-ingénierie des processus d'affaires se développe, capitalisant sur l'approche par les processus et les possibilités qu'offrent les nouvelles technologies, pour « repenser les processus d'affaires ». Les processus d'affaires confortent ainsi leur place dans la boite à outils du management pour planifier, gérer et optimiser la conduite des affaires.       Un processus d'affaires correspond à un ensemble cohérent d'activités relatif aux affaires d'une organisation privée ou publique. Il regroupe des activités corrélées ou en interaction, réparties au travers des structures organisationnelles et concourant à une même finalité. Un processus d'affaires peut être découpé en processus plus simples, jusqu'à en arriver aux activités élémentaires.Les processus peuvent être catégorisés par niveau de synthèse : Au plus haut niveau, on distingue les processus d'entreprise. Ils désignent les grandes familles d'activités de l'entreprise, comme le développement de la stratégie, le développement de produits, le développement de capacités de production, et l'exécution des commandes. Ce sont des processus transverses auxquels participent plusieurs directions/fonctions d'entreprise. Une cartographie de ces processus présente de façon synthétique la structure de l'ensemble des activités, au même titre que l'organigramme présente la structure de l'organisation.A un niveau intermédiaire on distingue les processus métiers. Ils désignent des processus propres à une famille de métiers, comme les processus de production, les processus financiers, et les processus logistiques. Ce sont des processus propres à un métier (éventuellement décliné par marché ou secteur d'activité) ou à l'une des grandes fonctions de l'entreprise.Au bas de l'échelle on trouve l'activité élémentaire, pour laquelle il n'y a pas de sens à la découper davantage. La norme ISO 19510 désigne ce type d'activités par le terme « tâche ».Cette terminologie n'est toutefois pas utilisée de manière homogène. En effet, les termes « processus d'entreprise » et « processus métier » sont souvent utilisés de façon interchangeable pour désigner un processus d'affaires sans nécessairement faire la distinction qui peut exister (par exemple entre l'anglais « business process » et « enterprise process » ou entre en allemand entre « Geschäftsprozess » et « Unternehmensprozess »).Les processus peuvent également être catégorisés en fonction de leur finalités. C'est l'approche qu'a utilisée par exemple American Productivity and Quality Center (APQC), c'est-à-dire le Centre américain pour la productivité et la qualité pour son référentiel de classification de processus, dont l'un des objectifs est aussi de faciliter les comparaisons entre différentes entreprises d'un même secteur d'activité. Les processus d'affaires sont nommés :  soit par le début et la fin de l'activité, dans le cas de processus complexes, afin d'éviter la confusion avec le nom d'une direction (par exemple : « concept à prototype » au lieu de développement produit ou « prospect à commande » au lieu de ventes) ;soit à l'aide d'un verbe, pour éviter la confusion avec le résultat du processus (par exemple : « payer les factures », « traiter les commandes clients » et « facturer les clients »).Pour chaque processus on peut clarifier les limites avec les autres processus, décrire les objectifs et les activités, et identifier les entrées (par exemple matières, finances, travail, informations) qui seront consommées par le processus, les sorties (c'est-à-dire les produits par exemple matériels, financiers, ou services intangibles) qui seront produites, les entités organisationnelles ou acteurs impliqués, ainsi que les fournisseurs et clients internes ou externes.Les processus peuvent être représentés par exemple sous forme :  arborescente, pour montrer la décomposition ;de liste hiérarchique avec une numérotation décimale pour montrer les différents niveaux de décomposition ;de chaîne de valeur ou de chaîne de processus schématisant la succession d'activités ;d'un organigramme avec ou sans répartition des étapes par entité organisationnelle ou par chronologie ;d'un modèle de processus d'affaire en suivant la notation de la norme ISO/CEI 19510.Au-delà de la simple représentation textuelle ou graphique des processus d'affaires à des fins de partage de connaissances, les différentes techniques de modélisation de processus peuvent être appliqués aux processus d'affaires, à des fins d'analyse, de simulation, de conception ou de mise en œuvre.      La principale application des processus d'affaires est la conduite des affaires. En effet, selon Hammer et Champy, « Les processus n'ont pas été inventés pour que l'on écrive à leur sujet. Chaque entreprise sur la planète est constituée de processus. Les processus sont ce que les entreprises font ». L'analyse de l'entreprise sous l'angle de ses processus d'affaires et la modélisation de ceux-ci ont de nombreuses applications, et en particulier :  le management des processus d'affaires, pour gérer l'exécution des activités et suivre les performances ;le management de la qualité, pour définir améliorer la qualité de l'exécution des processus industriels et de leurs produits ;l'optimisation de processus, pour améliorer les déroulement des processus et la création de la valeur ;la réingénierie des processus d'affaires, pour repenser radicalement les activités de l'entreprise, y inclus sa stratégie ;dans les études d'impacts, par exemple pour anticiper les effets d'opérations de fusions/acquisitions ou de partenariats ;dans la méthode des coûts par activité, qui analyse la formation des coûts par activité et mesure la performance des processus transverses.Les termes processus, procédé et procédure sont voisins. Ils ont pour même origine le verbe latin « procedere », qui signifie « aller de l'avant, progresser », mais portent des nuances dans leur signification :    Un processus est un ensemble d’activités en vue d'une même finalité. Ainsi par exemple, la norme ISO 9000 dans sa révision de 2015 définit le processus comme un « ensemble d'activités corrélées ou en interaction qui utilise des éléments d'entrée pour produire un résultat escompté » ;Un procédé est une technique, une méthode ou une pratique pour obtenir un résultat déterminé. Ainsi par exemple, la norme ISO/CEI 19510 définit un procédé d'affaires comme « un ensemble défini d'activités d'affaires qui représente les étapes nécessaires pour atteindre un objectif d'affaires. il inclut les flux et utilisations d'informations et de matière » ;Une procédure est un ensemble de formalités, démarches et règles à appliquer dans une situation déterminée. Elles décrit de façon précise la succession de tâches à réaliser, les rôles et responsabilités, ainsi que les règles et critères applicables. Elle est complétée par des modes opératoires qui précisent pour chaque tâche de la procédure, les instructions à suivre par un agent pour l'exécuter.En pratique, il est souvent difficile de distinguer un processus et un procédé. En effet, selon les définitions ci-dessus, un processus se distingue du procédé en ce qu'il ne présuppose pas une technique, une méthode ou une pratique définie pour atteindre son objectif. Toutefois, lorsqu'il est analysé, que ses activités sont décrites en détail, et qu'il est mis en œuvre de manière systématique, le processus répond alors également à la définition d'un procédé. Par ailleurs, les deux termes se traduisent en anglais par « process ».  Le processus d'affaire est un terme générique qui couvre tous les processus des entreprises privées et organisations publiques. Il n'est pas limité aux processus de gestion administrative. Le terme généralise la notion de processus tel qu'il a été utilisé dans le contexte du management de la qualité. Celui-ci était en effet défini, avant la version 2015 de ISO 9001, comme un processus de transformation, ce qui lui donnait une connotation de processus industriel ou de processus de production.Le processus d'affaire ne doit pas être confondu avec la gestion par affaire. Dans le domaine de l'ingénierie, la gestion par affaire est un mode de gestion qui « a pour but d'organiser et de suivre chaque affaire du devis jusqu'à l'installation sur site et éventuellement l'après-vente ». Une affaire correspond alors à un système complexe conçu, réalisé ou configuré pour répondre à un besoin client et correspond à un projet client. La gestion par affaire correspond donc à une configuration particulière de processus (en anglais « engineer to order »).     Le processus d'affaire n'a pas nécessairement une vocation de permanence, de continuité ou de répétitivité. Un processus peut être ainsi être lui-même défini en résultat d'un autre processus. Ainsi la norme ISO 21500 définit un projet comme étant « un ensemble unique de processus, constitués d’activités (...) pour atteindre les objectifs du projet ».  Les processus d'affaires couvrent par définition toutes les activités internes de l'entreprise. Celles-ci comprennent également les activités relatives aux échanges avec l'extérieur, qu'il s'agisse de partenaires commerciaux (par exemple : clients, fournisseurs, sociétés du groupe) ou de partenaires institutionnels (par exemple : banques, investisseurs, État).  Une intégration plus approfondie des processus internes de l'entreprise avec les processus métiers de ses partenaires, par exemple à l'aide d'interconnexions entre systèmes d'information, permet de constituer une entreprise étendue et de bénéficier de gains d'efficacité dans la chaine de valeur globale. C'est une alternative flexible par rapport à une stratégie de Croissance par adjacence.Les progiciels de gestion intégré (PGI), également appelés ERP (acronyme de l'anglais « Enterprise Resource Planing » c'est-à-dire « planification des ressources de l'entreprise ») sont apparus dans les années 1970, initialement pour répondre à un besoin de planification intégrée dans le domaine de la gestion de production. Ils se sont développés dans les années 1980 pour couvrir l'ensemble des activités de l'entreprise.  Avec leurs fonctions logicielles intégrant les différentes activités et données de l'entreprise selon un modèle de processus d'affaires standardisé et éprouvé, les ERP sont un facilitateur pour la mise en œuvre de processus d'entreprise efficaces. Toutefois, l'implémentation de ces systèmes doit s'accompagner d'un changement dans la définition des processus d'affaires existants. De plus, il est nécessaire d'identifier les processus d'affaires qui correspondent au cœur de métier et qui sont source d'avantages concurrentiels pour veiller à ce que la standardisation imposée par l'ERP ne cause pas la perte de cet avantage.  Dès les années 1980, mus par la recherche de gains de productivité, certains secteurs d'activités commencent à interconnecter les systèmes d'informations de clients et des fournisseurs. On ne parle pas encore de commerce électronique, mais d'échange de données informatisées (EDI). Le but est alors de gagner en efficacité dans les processus d'affaires en automatisant la passation de commande et la facturation. En France, le secteur automobile se dote ainsi dès 1984 de GALIA pour réfléchir aux formats informatiques d'échange. En 1988, l'Organisation Internationale de Normalisation (OSI) adopte EDIFACT, une norme élaborée par l'UN/CEFACT qui définit des messages électroniques standardisés pour un large éventail de transactions commerciales. Dans les années 2000, le commerce électronique et en particulier le commerce électronique entre entreprises se développe sous l'impulsion d'internet, dont les technologies sont plus souples et moins onéreuses à mettre en œuvre que celles utilisées pour EDIFACT, Cette évolution technologique est un facilitateur pour intégrer les processus d'affaires et concevoir une gestion de la chaîne logistique étendue en bénéficiant de l’interopérabilité des systèmes. En 2004, l'ISO lance la série de normes ISO 15000, élaborée conjointement par UN/CEFACT et OASIS, qui définit la technologie ebXML qui vise à simplifier le commerce électronique et réduire les coûts par une standardisation accrue.Les technologies liées au workflow (c'est-à-dire « flux de travaux » en français) et à l'exécution de processus métiers se développent à partir des années 2000. Leur but est de faciliter la mise en œuvre informatique des processus d'affaires, avec des mécanismes qui vont automatiser la coopération entre plusieurs systèmes (dont des systèmes des clients ou des fournisseurs) pour dérouler les étapes dans le bon ordre, et en appliquant les règles.    Ces technologies reposent en particulier sur des moteurs de workflow et les chorégraphies de services web. Techniquement, ces solutions ont été rendues possibles suite à d'un côté l'émergence de langages informatiques de description de processus d'affaires, dont XPDL en 1998, et surtout BPEL en 2004, de l'autre le recours accru à des architectures informatiques orientées services et des services web, deux technologies facilitant l’interconnexion et l’interopérabilité de systèmes. Toutefois, c'est la naissance de la technique de modélisation BPMN en 2004 et son adossement à BPEL à partir de 2006, qui permettra de lier la technologie au monde métier des processus d'affaires.      EntrepriseFonctionnement et organisation de l'entrepriseBusiness Process Model and Notation (BPMN), norme internationale ISO/IEC 19510:2013 intitulée ""Modèle de procédé d'affaire et notation de l'OMG""Les processus d'affaires dans le contexte d'ebXML Portail du management   Portail d’Internet   Portail du commerce"
économie;"La stagnation économique, ou plus simplement stagnation, est une période de faible croissance économique (mesurée à l'aide du PIB). Le taux de croissance du PIB est inférieur à la croissance potentielle.L'idée d'une stagnation économique est ancienne en science économique. Thomas Malthus développe une pensée (malthusianisme économique) centrée sur le rapport entre la démographie et la stagnation de l'économie. Plusieurs siècles plus tard, Alvin Hansen conceptualise et précise la notion de stagnation séculaire. Elle désigne, selon lui, une situation économique où la fin de la croissance démographique et du progrès technique conduisent à une période d'activité économique anémique. La stagnation séculaire est médiatisée dans les années 2000. Elle est reprise par Lawrence Summers. La lente reprise après la grande récession de 2007-2008 et la baisse du taux d'inflation à des valeurs inférieures à 2 % sont des signes qui rappellent les craintes exprimées par Hansen. Le seuil d'un taux d'intérêt nul peut empêcher d'atteindre l'égalité entre épargne et investissement et le plein emploi. Il est possible de voir apparaître une déflation déstabilisante avec un taux d'intérêt réel encore plus élevé. Par ailleurs, des taux d'intérêt très bas voire presque nuls mettent en péril la stabilité financière avec la création de bulles spéculatives.Les autres raisons sont la faible hausse de la population active, la hausse des inégalités, le progrès technique et la technologie de l'information qui diminuent la demande de biens d'équipement.Les craintes de Hansen ne se sont pas vérifiées à cause du baby boom et des nouvelles découvertes technologiques. Aujourd'hui, il y a toujours des optimistes en ce qui concerne les effets du progrès scientifique mais personne ne prévoit un nouveau baby boom.D'autres économistes, comme Robert J. Gordon, ont travaillé sur cette théorie notamment dans le domaine de la productivité (ou de l'offre). Du point de vue de Gordon, l'informatique génère peu de gains de productivité par rapport aux inventions et innovations précédentes, de sorte que la croissance économique pourrait être plus faible à l'avenir.Ralentissement économiqueCrise économiqueCroissance économiqueDépression économiqueStagflationG.B. Eggertsson, N.R. Mehrotra, A Model Of Secular Stagnation, NBER Working Paper, Cambridge, 2014B. Eichengreen, Secular Stagnation: The Long View, NBER Working Paper, Cambridge, 2015Alvin Hansen, "" Economic Progress and Declining Population Growth "", American Economic Review, 1939, p. 1-15C. Teulings and R. Baldwin, Secular Stagnation: Facts, Causes and Cures, CEPR Press, 2014C. Homs, Stagnation séculaire ou agonie du capital ?, avril 2016. Portail de l’économie"
économie;Le taux de croissance économique est un indicateur utilisé pour mesurer la croissance de l'économie d'un pays d'une année sur l'autre. Il est défini par la formule suivante qui relie les produits intérieurs bruts (PIB) de l'année N et de l'année N-1 :                    t        a        u        x                 d        e                 c        r        o        i        s        s        a        n        c        e        =                                            P              I                              B                                  a                  n                  n                  e                  e                                     N                                            ?              P              I                              B                                  a                  n                  n                  e                  e                                     N                  ?                  1                                                                    P              I                              B                                  a                  n                  n                  e                  e                                     N                  ?                  1                                                                    ×        100              {\displaystyle taux\ de\ croissance={\frac {PIB^{annee\ N}-PIB^{annee\ N-1}}{PIB^{annee\ N-1}}}\times 100}  où les PIB sont mesurés en volume (pour éviter de considérer l'inflation des prix comme de la croissance économique). Ou ln (PIB annee N \ PIB annee N - 1).On peut également utiliser les valeurs des PIB en prix, en mesurant les PIB des années N et N-1 en prix constants (prix en base 2000 par exemple).Le taux de croissance est généralement mesuré annuellement (en moyenne annuelle) ou trimestriellement (d'un trimestre au suivant).Les économistes ont eu des difficultés à calculer un taux de croissance mondiale en raison de la disparité des économies et de la déconnexion des cycles économiques d'un pays à l'autre. Ils ont préféré évaluer les cycles économiques mondiaux par décennie. Ainsi, les très fortes croissance mondiale des années 1830 et croissance mondiale des années 1850, sont suivies par la Grande Dépression (1873-1896). De même, la grande dépression des années 1930 fait suite à la croissance économique de la Belle Époque et à la puissante expansion des années 1920. Ensuite, la très forte croissance des années 1950, socle des Trente Glorieuses, est portée par la reconstruction après la guerre.Le calcul de taux de croissance ne se limite bien sûr pas au PIB, et le taux de croissance de toute autre variable se calcule de la même manière.Le taux de croissance annuel composé (en anglais : CAGR ou Compound Annual Growth Rate) est le taux qui respecte l'équation suivante :                              C          A          G          R                (                  t                      0                          ,        t        )        =                              (                                                            V                  (                  t                  )                                                  V                  (                                      t                                          0                                                        )                                                      )                                              1                              t                ?                                  t                                      0                                                                                      ?        1              {\displaystyle \mathrm {CAGR} (t_{0},t)=\left({\frac {V(t)}{V(t_{0})}}\right)^{\frac {1}{t-t_{0}}}-1}  Il s'agit d'un ratio à progression géométrique qui donne un taux de croissance constant sur la période étudiée                     (        t        ?                  t                      0                          )              {\displaystyle (t-t_{0})}  .Le taux de croissance annuel moyen (TCAM) permet de calculer un taux d'évolution moyen sur une durée de n périodes. D'autres dénominations existent, telles TAMA (Taux Annuel Moyen d'Accroissement), TAAM (Taux d'Accroissement Annuel Moyen), ou TAMV (Taux Annuel Moyen de Variation). Formule Le taux de croissance annuel moyen, exprimé en pourcentage, sur                     n              {\displaystyle n}   périodes (années, mois, semaines, etc.) est donné par la formule :                    T        C        A        M        =                  (                                                                                                                                                                                                      valeur finale                                                                                                                                                                                                              valeur initiale                                                                                                              n                                                      ?            1                    )                      {\displaystyle TCAM=\left({\sqrt[{n}]{\cfrac {\text{valeur finale}}{\text{valeur initiale}}}}-1\right)}  Exemple : si entre 1997 et 2008, le montant des crédits distribués est passé de 48003,1 à 249012,3 millions de francs congolais, le taux de croissance annuel moyen sur ces onze années est donné par la formule                              (                                                                                          249012                    ,                    3                                                        48003                    ,                    1                                                                    11                                                      ?            1                    )                ?        16        ,        14        %              {\displaystyle \left({\sqrt[{11}]{\frac {249012,3}{48003,1}}}-1\right)\approx 16,14\%}  Soit un taux d'accroissement annuel moyen de 16,14 %Il faut préciser que ce chiffre est exprimé en pourcentage. Intérêt L'intérêt du TCAM est de fournir une indication sur le taux de croissance moyen sur une période donnée. Et la comparaison de deux TCAM permet, par suite, de comparer les fluctuations du phénomène observé relativement à ces deux périodes. Par exemple, en économie, une application du TCAM est de comparer la croissance pendant les Trente Glorieuses (1945-1975) avec la croissance de la période suivante (1975 à aujourd'hui).Cet outil de calcul est en outre employé en démographie pour décrire le taux d’accroissement de la population entre deux recensements (solde démographique relatif). Limites Le TCAM donne une moyenne des évolutions annuelles mais ne tient pas compte des variations internes de la période étudiée. En effet seules les valeurs initiales et finales entrent dans son calcul. Exemples Population en Lorraine en 2007 : 2 339 881 habitantsPopulation en Lorraine en 2012 : 2 349 816 habitantsTCAM =                     [                                            2349816                              /                            2339881                                      5                                      ?        1        ]        =        0.085        %              {\displaystyle [{\sqrt[{5}]{2349816/2339881}}-1]=0.085\%}  Interprétation : Entre 2007 et 2012 la population lorraine a augmenté de 0,085 % tous les ans. Démonstration de la formule On vérifie simplement qu'en multipliant la valeur initiale V1 par (1+T) n fois on obtient bien la valeur finale V2 :                    1        +        T        =                                                            V                                  2                                                            V                                  1                                                                    n                                            {\displaystyle 1+T={\sqrt[{n}]{\frac {V_{2}}{V_{1}}}}}                                V                      1                          ×                              (                                                                              V                                  2                                                            V                                  1                                                                    n                                                                          )                                            n                          =                  V                      2                                {\displaystyle V_{1}\times {\Biggr (}{\sqrt[{n}]{\frac {V_{2}}{V_{1}}}}{\Biggr )}^{n}=V_{2}}   Portail de l’économie   Portail de la finance   Portail des probabilités et de la statistique
économie;
", The h""";
économie;"Le taux d'inflation optimal est un taux d'inflation hypothétique qui doit assurer à un système économique une augmentation saine de son niveau des prix.Le concept de taux d'inflation optimal est débattu par des économistes, académiques comme institutionnels. Les banques centrales se questionnent sur l'existence d'un tel taux car leur mandat exige souvent qu'elles assurent un taux d'inflation qui corresponde aux besoins de leur économie.Le monétariste Milton Friedman, dans un papier de 1969. Il considère que la banque centrale doit maintenir son taux d'intérêt nominal à 0, car un taux d'intérêt positif provoque une distorsion de l'économie. L'inflation devrait ainsi être négative. L'école de la nouvelle économie keynésienne a longtemps considéré le taux d'inflation optimal comme 0 %. Cela se base sur deux arguments. Le premier est celui des coûts des menus ; le deuxième est celui de la mauvaise allocation des ressources. En effet, si l'inflation est autre que zéro, les entreprises doivent modifier leurs prix, mais comme cela est difficile et lent du fait des rigidités nominales, cela n'est pas possible dans l'immédiat et crée une dispersion des prix, et donc une mauvaise allocation des ressources.En plus de cela, l'inflation a un effet redistributif adverse. L'inflation affecte les couches sociales de manière différenciée ; il s'agit d'une taxe sur les pauvres, qui subissent l'augmentation de l'inflation de plein fouet dans leur consommation quotidienne.La Banque centrale européenne a estimé un taux d'inflation optimal pour la zone euro à 2 % lors de sa revue stratégique de 2021. Ce taux d'inflation a plusieurs qualités, comme le fait d'être assez faible pour éviter une modification fréquente des prix par les agents de l'offre (coût des menus). Il évite aussi une volatilité de l'inflation. De plus, cette cible d'inflation assure une marge de sécurité contre le risque de déflation.Une cible d'inflation élevée permet de donner une marge de manœuvre aux banques centrales. Elles ne peuvent, en effet, faire baisser leurs taux directeurs en-dessous du taux plancher zéro ; or, si la cible d'inflation est de 2 % ou 3 %, la banque centrale dispose d'une plus grande marge pour faire baisser les taux directeurs.Certains économistes, comme Xavier Ragot, soutiennent qu'un niveau d'inflation aux alentours de 3 % est nécessaire sur le long terme pour huiler le système économique et atteindre un niveau optimal d'investissement. Il obtient ce résultat à partir d'un modèle à générations imbriquées. Ce modèle montre en effet que les banques centrales doivent faire baisser le taux d'intérêt réel, en présence de contraintes de crédit, pour stimuler la croissance ; or, pour faire baisser ce taux, elles peuvent jouer sur l'inflation. Cet effet est également remarqué par Weil (1991), Weiss (1980) et Summers (1981). Portail de la finance   Portail de l’économie"
économie;"Le prix, exprimé en un montant de référence (en général monétaire), est la traduction de la compensation qu'un opérateur est disposé à remettre à un autre en contrepartie de la cession d'un bien ou un service. Le prix mesure la valeur vénale d'une transaction et en constitue l'un des éléments essentiels.Le mécanisme de formation des prix est un des concepts centraux de la microéconomie, spécialement dans le cadre de l'analyse de l'économie de marché, où les prix jouent un rôle primordial dans la recherche et la définition d'un prix dit « d'équilibre » (alors qu'ils jouent un rôle plus mineur dans une économie administrée).Les niveaux de prix possibles sont en nombre potentiellement infini, selon les acteurs économiques, selon leurs estimations de la valeur de la chose pour eux-mêmes et pour les autres (spéculation). Si une transaction se réalise effectivement, le prix traduit le compromis entre les estimations de l'acheteur et celles du vendeur (reflet de l'offre et la demande).Le mécanisme de détermination des prix peut être affecté par d'autres facteurs :éventuelles imperfections régnant sur le marché (monopole, oligopole, pénurie, marché noir, etc.),contraintes légales lorsqu'il en existe (les prix n'étant pas toujours libres : « prix imposés » ou « administrés »),considérations techniques, telles que la méthode de mise en marché (commerce national et international, commerce de gros, commerce de détail, enchères, etc.) ou les contraintes que cela implique (délais de transmission des offres, définitions des priorités entre offres, ...).Selon l'objet concerné, le périmètre et la méthode de détermination du prix varie. On rencontre ainsi différentes sortes de prix :le prix d'achat.le prix de vente, qui indique le prix auquel un commerçant déclare être disposé à céder la chose et qui ne doit pas être inférieur au coût de revient (interdiction légale de la vente à perte) ;le prix de revient, censé refléter l'ensemble des dépenses liées aux intrants et à la fabrication d'un produit ou d'un service ; Le prix de revient ou coût de revient est égal au coût de production majoré des frais de transport ;le prix d'acceptabilité ou prix psychologique, qui définit le prix qu'une grande partie de la clientèle trouve justifié pour l'acquisition d'un bien ou d'un service ;le prix de cession, qui indique le prix auquel est facturée une cession entre deux services d'une même entreprise ou entre deux filiales d'un même groupe. En matière de comptabilité des entreprises, les prix de cession concernent les biens immobiliers (qui ont une longue durée de vie à l'instar des constructions ou des terrains non bâtis) par opposition aux prix de vente qui concernent, eux, les produits courants c'est-à-dire ceux qui sont relatifs à l'activité normale de l'entreprise.Raymond Barre distingue plusieurs types de prix en fonction du degré de liberté du marché (prix libres, prix administrés), du stade d'élaboration du produit (prix du gros ou de détail) ou encore de la nature des produits vendus (prix des produits agricoles, industriels ou des services)L'importance du système de prix libres a été mise en avant et débattue en particulier dans les années 1920-1930.Une vive controverse sur la question du calcul économique oppose les économistes de l'école autrichienne d'économie, Ludwig von Mises puis, ultérieurement Friedrich Hayek, aux tenants du socialisme de marché, Oskar Lange au premier chef. Pour Ludwig von Mises, le système de prix libres est le seul moyen de coordination des actions des millions d'individus qui composent l'économie d'un pays. Friedrich Hayek relaie cette idée et insiste pour sa part sur le rôle des prix comme vecteur de transmission de l'information disponible aux individus.L'économiste Milton Friedman résume cela en écrivant que le système de prix libres remplit trois fonctions :transmission de l'information sur l'offre et la demande ;.incitation pour les producteurs à s'orienter vers les secteurs aux prix élevés et, partant, à permettre un retour à l'équilibre ;répartition des revenus.Dans une économie planifiée, les prix n'ont pas la même importance. L'appareil productif peut s'en passer : au lieu de chercher à maximiser la valeur ajoutée de sa production comme il le ferait dans une économie de marché, un producteur peut se voir attribuer un quota de matières premières et un objectif de production ; les prix sont fixés par les pouvoirs publics à un niveau considéré comme « souhaitable », mais ils ne sont pas directement connectés aux décisions d'allocations des matières premières ou d'objectif de production, qui sont fixés par ailleurs. Il peut en résulter une pénurie (file d'attente et marché noir) ou un rationnement, si le prix est inférieur à l'utilité pour les consommateurs, ou des excès de production dans le cas contraire.En outre, certaines situations (par exemple, la guerre) incitent les autorités à recourir au contrôle des prix (ou du moins du prix de certains produits jugés nécessaires), ou à influer sur l'offre (protectionnisme, subvention...) et la demande.En réalité, la liberté totale des prix est rarement constatée, même dans les économies réputées les plus libérales, notamment à cause de l'impact de la fiscalité, de lois anti-dumping, des subventions, des engagements pris dans le cadre de contrats pluri-annuels, etc.Sur un marché libre le prix reflète l'équilibre entre l'offre et la demande. Mais les auteurs classiques (Adam Smith, David Ricardo, John Stuart Mill, ...) et Karl Marx considèrent qu'il est soumis plus aux influences de l'offre (coût exprimé par une certaine quantité de travail) que de la demande. Pour Karl Marx l'équilibre tend à se fixer autour de la valeur du travail incorporé. Ricardo estime également que le ""prix réel"" correspond à la quantité de travail incorporé mais constate que le ""prix courant"" est fonction de l'offre et de la demande. Le prix courant aurait tendance à se rapprocher du prix naturel. Selon Adam Smith le prix se dissocie de la ""valeur réelle"" car il tient compte de la valeur de la monnaie qui, elle, est variable. À partir de la fin du dix-neuvième siècle, les auteurs marginalistes (Léon Walras, Stanley Jevons, ...) estiment que le prix ou ""la valeur d'échange"" ne dépend pas de l'offre mais de la demande et donc de l'utilité exprimée par le consommateur. Malgré les influences de la demande sur la détermination du prix du marché admises par les marginalistes, Alfred Marshall considère que, de toute façon, on ne peut pas se passer du concours des deux (i.e l'offre et la demande) pour la fixation du prix. André Orléan estime que la fixation d'un prix peut s'établir par mimétisme et non en fonction du travail incorporé (côté offre) ou de l'utilité (côté demande),. Pour Jacques Perrin, les institutions jouent ou doivent jouer un rôle dans la constitution des prix en prenant en compte l'utilité sociale. Le prix n'est pas donc déterminé par l'unique confrontation de l'offre et de la demande qui sont exprimées par des agents économiques ""rationnels"". Elles subissent d'autres influences (psychologiques, sociologiques et institutionnelles). Par ailleurs, l'offre et la demande sont exprimées dans le temps. Ce dernier peut avoir une influence capitale dans les décisions des producteurs et des acheteurs. Avant de produire, de vendre ou d'acheter, ils procèdent notamment à des études futures du marché pour exploiter les opportunités avantageuses et éviter les menaces sources de risques majeurs.Les libéraux, en faisant appel au concept du consommateur-roi de Paul A. Samuelson, considèrent que les consommateurs, par leur pouvoir d'augmenter ou de baisser librement la demande exprimée sur le marché des biens de consommation, déterminent les prix et donnent le signal aux entreprises d'augmenter ou de baisser l'offre conséquente. Par conséquent, toute chose étant égale par ailleurs, les entreprises vont augmenter ou diminuer les demandes portant sur les marchés du travail, des biens de production et des capitaux déterminant ainsi les taux de salaire, d'intérêt et les prix sur le marché des biens de production. Cependant, John K. Galbraith a montré, dans les années 1960, que le fonctionnement réel de l'économie contemporaine ne correspond pas à ce schéma théorique. Les entreprises, en agissant sur le marché des biens de consommation (études des besoins du consommateur, étude de la concurrence, promotion des ventes) conditionnement la demande du consommateur aussi bien sur ce marché que sur ceux des biens de production, du travail et des capitaux et le privent de toute initiative. De plus, la liberté du consommateur est contrariée par la dépenses budgétaires de l'État visant à accroître les investissements publics pour augmenter la croissance économique. Cette nouvelle stratégie des entreprises est appelée par John K. Galbraith la "" filière inversée "".L'évolution des prix n'est pas l'inflation (l'augmentation du niveau général des prix), qui ne mesure le prix que par de la monnaie, alors que l'évolution des prix en général dépend du fonctionnement de l'économie, qui modifie le prix relatif des biens (i.e le prix d'un bien exprimé par d'autres biens). Cependant la mesure du prix de la monnaie ne peut être fait qu'indirectement, par mesure du prix d'un panier représentatif de biens : si le prix de ce panier augmente, c'est que la valeur (relative) de la monnaie diminue, et inversement.Il existe différents indices de prix pour différentes classes de biens et pour différents usages :les prix à la consommation sont mesurés par l'Indice des prix à la consommation (IPC ou, en anglais, CPI) ;les prix à la production sont mesurés séparément, et correspondent aux Coûts de production ;l'indice du coût de la construction ou l'indice de référence des loyers mesurent l'évolution du prix du logement ;etc.Pour un bien, on parle de « prix nominal » lorsque l'on fait référence au prix exprimé dans une monnaie donnée. On parle de « prix réel » lorsque l'on extrait du prix nominal la part due à l'évolution de la valeur de la monnaie, c'est-à-dire l'inflation.Les prix définissent une distance dans l'espace des commodités préservant la valeur des échanges.Soit A et B, Alphonse et Brigitte, deux agents économiques qui possèdent chacun les vecteurs commodités a et b, par exemple                     a        =        (        1        ,        1        ,        3        ,        0        ,        10        )              {\displaystyle a=(1,1,3,0,10)}   et                     b        =        (        0        ,        1        ,        4        ,        1        ,        100        )              {\displaystyle b=(0,1,4,1,100)}   avec l'ordre (voiture,table,chaises,machine à laver,monnaie), et p le vecteur des prix, la distance comparant la richesse des deux agents est définie par                    d        (        A        ,        B        )        =        d        (        a        ,        b        )        =                  |                          ?                      i                                    p                      i                          (                  a                      i                          ?                  b                      i                          )                  |                      {\displaystyle d(A,B)=d(a,b)=|\sum _{i}p_{i}(a_{i}-b_{i})|}  Cette pseudo-distance définit une relation d'équivalence dans l'espace des commodités qui préserve les écarts de richesse lors d'un échange. Par exemple si d(A,B) est de 100 et A et B échangent une même valeur de commodités d(a',b')= 0, i.e. A donne a' à B et B donne b' à A, après l'échange d(A,B) est toujours égale à 100. Cette préservation de la valeur ne serait pas vérifiée si d était une autre distance.Contrôle des loyersJuste prixLoi du maximum généralMarginalismeOffre et demandeParadoxe de l'eau et du diamantPrix prédateursValeurDistinction entre l'acompte et les arrhes(en) Price Theory par Milton Friedman(en) Theory of Price par George StiglerAndré Orléan, L'empire de la valeur, Seuil, 2011(en) Le système de prix libres, Henry Hazlitt(en) Four Thousand Years of Price Control, Ludwig von Mises Institute Portail de l’économie   Portail du management   Portail du commerce"
Informatique;"Un algorithme est une suite finie et non ambiguë d'instructions et d’opérations permettant de résoudre une classe de problèmes.Le mot algorithme vient d'Al-Khwârizmî (en arabe : ?????????), nom d'un mathématicien persan du IXe siècle.Le domaine qui étudie les algorithmes est appelé l'algorithmique. On retrouve aujourd'hui des algorithmes dans de nombreuses applications telles que le fonctionnement des ordinateurs, la cryptographie, le routage d'informations, la planification et l'utilisation optimale des ressources, le traitement d'images, le traitement de textes, la bio-informatique, etc.Un algorithme est une méthode générale pour résoudre un type de problèmes. Il est dit correct lorsque, pour chaque instance du problème, il se termine en produisant la bonne sortie, c'est-à-dire qu'il résout le problème posé.L'efficacité d'un algorithme est mesurée notamment par :sa durée de calcul ;sa consommation de mémoire vive (en partant du principe que chaque instruction a un temps d'exécution constant) ;la précision des résultats obtenus (par exemple avec l'utilisation de méthodes probabilistes) ;sa scalabilité (son aptitude à être efficacement parallélisé) ;etc.Les ordinateurs sur lesquels s'exécutent ces algorithmes ne sont pas infiniment rapides, car le temps de machine reste une ressource limitée, malgré une augmentation constante des performances des ordinateurs. Un algorithme sera donc dit performant s'il utilise avec parcimonie les ressources dont il dispose, c'est-à-dire le temps CPU, la mémoire vive et (objet de recherches récentes) la consommation électrique. L’analyse de la complexité algorithmique permet de prédire l'évolution en temps calcul nécessaire pour amener un algorithme à son terme, en fonction de la quantité de données à traiter.Donald Knuth (1938-) liste, comme prérequis d'un algorithme, cinq propriétés :finitude : « un algorithme doit toujours se terminer après un nombre fini d’étapes » ;définition précise : « chaque étape d'un algorithme doit être définie précisément, les actions à transposer doivent être spécifiées rigoureusement et sans ambiguïté pour chaque cas » ;entrées : « quantités qui lui sont données avant qu'un algorithme ne commence. Ces entrées sont prises dans un ensemble d'objets spécifié » ;sorties : « quantités ayant une relation spécifiée avec les entrées » ;rendement : « toutes les opérations que l'algorithme doit accomplir doivent être suffisamment basiques pour pouvoir être en principe réalisées dans une durée finie par un homme utilisant un papier et un crayon ».George Boolos (1940-1996), philosophe et mathématicien, propose la définition suivante :« Des instructions explicites pour déterminer le nième membre d'un ensemble, pour n un entier arbitrairement grand. De telles instructions sont données de façon bien explicite, sous une forme qui puisse être utilisée par une machine à calculer ou par un humain qui est capable de transposer des opérations très élémentaires en symboles. »Gérard Berry (1948-), chercheur en science informatique, en donne la définition grand public suivante :« Un algorithme, c’est tout simplement une façon de décrire dans ses moindres détails comment procéder pour faire quelque chose. Il se trouve que beaucoup d’actions mécaniques, toutes probablement, se prêtent bien à une telle décortication. Le but est d’évacuer la pensée du calcul, afin de le rendre exécutable par une machine numérique (ordinateur…). On ne travaille donc qu’avec un reflet numérique du système réel avec qui l’algorithme interagit. »Les algorithmes sont des objets historiquement dédiés à la résolution de problèmes arithmétiques, comme la multiplication de deux nombres. Ils ont été formalisés bien plus tard avec l'avènement de la logique mathématique et l'émergence des machines qui permettaient de les mettre en œuvre, à savoir les ordinateurs.La plupart des algorithmes ne sont pas numériques.On peut distinguer :des algorithmes généralistes qui s'appliquent à toute donnée numérique ou non numérique : par exemple les algorithmes liés au chiffrement, ou qui permettent de les mémoriser ou de les transmettre ;des algorithmes dédiés à un type de données particulier (par exemple ceux liés au traitement d'images).Voir aussi : Liste de sujets généraux sur les algorithmes (en)L'algorithmique intervient de plus en plus dans la vie quotidienne.Une recette de cuisine peut être réduite à un algorithme si on peut réduire sa spécification aux éléments constitutifs :des entrées (les ingrédients, le matériel utilisé) ;des instructions élémentaires simples (frire, flamber, rissoler, braiser, blanchir, etc.) dont les exécutions dans un ordre précis amènent au résultat voulu ;un résultat : le plat préparé.Cependant, les recettes de cuisine ne sont en général pas présentées rigoureusement sous forme non ambiguë : il est d'usage d'y employer des termes vagues laissant une liberté d'appréciation à l'exécutant alors qu'un algorithme non probabiliste stricto sensu doit être précis et sans ambiguïté.Le tissage, surtout tel qu'il a été automatisé par le métier Jacquard, est une activité que l'on peut dire algorithmique.Un casse-tête, comme le cube Rubik, peut être résolu de façon systématique par un algorithme qui mécanise sa résolution.En sport, l'exécution de séquences répondant à des finalités d'attaque, de défense, de progression, correspond à des algorithmes (dans un sens assez lâche du terme). Voir en particulier l'article tactique (football).En soins infirmiers, le jugement clinique est assimilable à un algorithme. Le jugement clinique désigne l'ensemble des procédés cognitifs et métacognitifs qui aboutissent au diagnostic infirmier. Il met en jeu des processus de pensée et de prise de décision dans le but d’améliorer l’état de santé et le bien-être des personnes que les soignants accompagnent.Un code juridique, qui décrit un ensemble de procédures applicables à un ensemble de cas, est un algorithme.Les progrès de ce qu'on appelle l'intelligence artificielle s'appuient sur un algorithmique de plus en plus complexe qui devient l'un des rouages cachés du Web 2.0 et des grands réseaux sociaux.À partir des années 2000, ce qui est appelé « algorithmique » est un ensemble de « boîtes noires » (autrement dit de processus informatiques dont on ne sait pas ce qu'il y a à l'intérieur) qui exploitent et influencent les comportements inconscients des consommateurs, et des électeurs.Au milieu des années 2010 la plate-forme logicielle Ripon permet secrètement l'élection de Donald Trump. Elle le fait grâce à une intelligence artificielle s'appuyant sur des logiciels issus de la guerre psychologique telle que développée en Afghanistan, et désormais nourrie du big data disponible sur l'Internet, et en particulier de données personnelles piratées dans plusieurs dizaines de millions de comptes Facebook. Ce piratage a été réalisé par Cambridge analytica au Royaume-Uni (devenu Emerdata en aout 2017) sur la plate-forme Facebook insuffisamment protégée. Les données ont été analysées et utilisées par sa société-sœur canadienne, Aggregate IQ, sous le contrôle du groupe SCL (leur société-mère) via Ripon. Cette plateforme Ripon ayant été conçue pour produire des profils psychographiques et des processus d'utilisation dans des campagnes électorales microciblées. Ces campagnes visaient à influer sur les émotions des électeurs, pour modifier leurs intentions de vote, ou les inciter à rester ou devenir abstentionnistes,,.Ces processus plus ou moins frauduleux (la législation de protection des individus sur l'Internet étant encore émergente) seront découvertes tardivement, dans le cadre du scandale Facebook-Cambridge Analytica/Aggregate IQ, après que ces outils aient conduits à l'élection de D. Trump, puis au Brexit, et qu'ils aient influencé au moins une vingtaine d'élections ou de référendums dans le monde. Dans les années 2010, les lanceurs d'alertes comme le canadien Christopher Wylie, Carole Cadwalladr, Shahmir Sanni, Brittany Kaiser, David Caroll, des journalistes comme Carole Cadwalladr et des ONG telles que AlgorithmWatch alertent sur les dérives éthiques qu'ils constatent dans l'usage malhonnête des algorithmes.Dans la vie quotidienne, un glissement de sens s'est opéré, ces dernières années, dans le concept d'« algorithme » qui devient à la fois plus réducteur, puisque ce sont pour l'essentiel des algorithmes de gestion du big data, et d'autre part plus universel en ce sens qu'il intervient dans tous les domaines du comportement quotidien. La famille des algorithmes dont il est question effectue des calculs à partir de grandes masses de données (les big data). Ils réalisent des classements, sélectionnent des informations et en déduisent un profil, en général de consommation, qui est ensuite utilisé ou exploité commercialement. Les implications sont nombreuses et touchent les domaines les plus variés. Mais les libertés individuelles et collectives pourraient être finalement mises en péril, comme le montre la mathématicienne américaine Cathy O'Neil dans le livre Weapons of Math Destruction, publié en 2016 et sorti en français en 2018 sous le titre Algorithmes : la bombe à retardement (aux éditions Les Arènes).« Aujourd’hui, les modèles mathématiques et les algorithmes prennent des décisions majeures, servent à classer et catégoriser les personnes et les institutions, influent en profondeur sur le fonctionnement des États sans le moindre contrôle extérieur. Et avec des effets de bords incontrôlables. […] Il s’agit d’un pouvoir utilisé contre les gens. Et pourquoi ça marche ? Parce que les gens ne connaissent pas les maths, parce qu’ils sont intimidés. C’est cette notion de pouvoir et de politique qui m’a fait réaliser que j’avais déjà vu ça quelque part. La seule différence entre les modèles de risque en finances et ce modèle de plus-value en science des données, c’est que, dans le premier cas, en 2008, tout le monde a vu la catastrophe liée à la crise financière. Mais, dans le cas des profs, personne ne voit l’échec. Ça se passe à un niveau individuel. Des gens se font virer en silence, ils se font humilier, ils ont honte d’eux. »Dans cet ouvrage, l'auteure alerte le lecteur sur les décisions majeures que nous déléguons aujourd'hui aux algorithmes dans des domaines aussi variés que l'éducation, la santé, l'emploi et la justice, sous prétexte qu'ils sont neutres et objectifs, alors que, dans les faits, ils donnent lieu à « des choix éminemment subjectifs, des opinions, voire des préjugés insérés dans des équations mathématiques ».L'opacité des algorithmes est l'une des raisons principales de ces critiques. Une meilleure information sur leur mode de fonctionnement spécifique permettrait de rendre plus clair le « contrat social passé entre les internautes et les calculateurs ». La description pour chaque algorithme de son propre principe de classement de l'information aide l'utilisateur à mieux comprendre les choix proposés par l'algorithme et les résultats obtenus.Les philosophes Wendell Wallach et Colin Allen ont soulevé des questions liées à l'implantation par les programmeurs de règles morales dans les algorithmes d'intelligence artificielle : « Aujourd'hui, les systèmes [automatiques] s'approchent d'un niveau de complexité qui, selon nous, exige qu'ils prennent eux-mêmes des décisions morales […]. Cela va élargir le cercle des agents moraux au-delà des humains à des systèmes artificiellement intelligents, que nous appellerons des agents moraux artificiels ». Dans son livre Faire la morale aux robots : une introduction à l'éthique des algorithmes, Martin Gibert met en évidence le rôle de la programmation dans l'éthique des robots, en traitant plus précisément des enjeux moraux liés à la construction des algorithmes. Il définit un algorithme comme « rien de plus qu'une suite d'instructions – ou de règles – pour parvenir à un objectif donné ». L'éthique des algorithmes poserait donc une question : « Quelles règles implanter dans les robots, et comment le faire ? ». Gibert souligne notamment l'ambiguïté de ces agents moraux artificiels :« Les agents moraux artificiels (AMA) ne sont pas cependant des agents moraux au sens fort du terme. Contrairement aux humains, ils ne semblent pas imputables [sic] de leurs actes. Ils n'ont toutefois pas besoin de l'être pour prendre des décisions moralement significatives et soulever tout un tas de questions en éthique des algorithmes. »Analyse de la complexité des algorithmesAlgorithmiqueCorrection d'un algorithmeBiais algorithmiqueRégulation des algorithmesRessource relative à la santé : (en) Medical Subject Headings Qu’est-ce qu'un algorithme ? par Philippe Flajolet et Étienne Parizot sur la revue en ligne IntersticesDéfinition du terme « algorithme » par des savants Portail de l'informatique théorique"
Informatique;"En informatique, le code source est un texte qui présente les instructions composant un programme sous une forme lisible, telles qu'elles ont été écrites dans un langage de programmation. Le code source se matérialise généralement sous la forme d'un ensemble de fichiers texte.Le code source est souvent traduit — par un assembleur ou un compilateur — en code binaire composé d'instructions exécutables par le processeur. Il peut sinon être directement interprété à l'exécution du programme. Dans ce deuxième cas, il est parfois traduit au préalable en un code intermédiaire dont l'interprétation est plus rapide.L'expression est une traduction de l'anglais source code. Les expressions omettant le terme de code sont communes : les sources, le source.Dans les tout premiers temps de l'informatique, les programmes étaient entrés dans la mémoire de l'ordinateur par l'intermédiaire des interrupteurs du pupitre de commande, sous forme du codage binaire des instructions machines. Ce qui ne convenait qu'à de tout petits programmes. Ils ont ensuite été chargés depuis des bandes perforées, puis des cartes perforées.Très rapidement, les programmes ont été rédigés dans un langage symbolique, langage d'assemblage ou langage évolué comme Fortran, Cobol, puis traduit automatiquement par un programme (assembleur, compilateur).Avec l'apparition des disques magnétiques et des consoles interactives, des éditeurs de lignes puis des éditeurs de textes ont été utilisés pour taper et modifier le code source.Les possibilités limitées des ordinateurs de l'époque nécessitaient souvent l'impression du code source sur papier continu (en) avec des bandes Carol.Aujourd'hui, il existe des environnements de développement, dits Environnement de développement intégré (IDE, Integrated Development Environment), qui intègrent notamment les tâches d'édition et de compilation.Un logiciel est une suite d'instructions données à une machine. Un processeur ne peut exécuter que des instructions représentées sous une forme binaire particulière. Sauf mécanismes expérimentaux, il n'est pas possible pour un être humain de saisir directement un code binaire dans la représentation qu'en attend le processeur : un être humain ne peut pas écrire directement les champs de bits aux adresses attendues. Il est obligé de passer par un code distinct appelé code source, et qui est par la suite traduit dans la représentation binaire attendue par la machine puis chargé et exécuté par la cible.Toutefois, l'écriture d'un code sous forme binaire, même dans un fichier séparé, pose de nombreux problèmes de compréhension aux êtres humains. C'est une représentation uniquement constituée d'une suite ininterrompue de 0 et de 1 qui est difficile à lire, à écrire et à maintenir sans assistance technique. La diversité des microprocesseurs et des composants présents dans un ordinateur ou automate, implique qu'un code binaire généré pour un système ne puisse pas être a priori le même que sur une machine distincte. Aussi, il existe autant de codes binaires que de configurations et une complexité accrue excluant que l'être humain puisse concevoir simplement un code binaire de grande ampleur.Pour éviter ces écueils, et puisqu'une traduction est toujours nécessaire, l'être humain écrit un code textuel afin qu'il soit plus lisible, plus compréhensible et plus simple à maintenir : c'est le code source écrit dans un langage de programmation. Il est, dans la plupart des cas, plus lisible, plus simple à écrire et indépendant du système cible. Un programme tiers (compilateur, interpréteur ou machine virtuelle) se charge de la traduction du code source en code binaire exécutable par la cible.Le code généré par l'être humain est appelé code source ; la façon dont est rédigé ce code source est appelée langage de programmation ; le traducteur de ce code dans sa représentation binaire est appelé compilateur, interpréteur ou machine virtuelle selon les modalités de la traduction.Dans la plupart des langages, on peut distinguer différents éléments dans un code source :les éléments décrivant l’algorithme et les données (le code source proprement dit) :des symboles identifiant des variables, des mots clefs dénotant des instructions, des représentations de données ;des constantes littérales.les commentaires, qui documentent le code source le plus souvent en langage naturel, destinés aux relecteurs du code source. Ils ne sont pas nécessaires à la production du code exécutable mais peuvent être utilisés par le compilateur pour, par exemple, produire automatiquement de la documentation.Un code est plus facile à lire et à écrire avec un éditeur fournissant une coloration syntaxique permettant de distinguer les différents éléments du code source. Les commentaires peuvent par exemple être mis en vert.Exemple de code en Ruby :Autre exemple de code en Ruby :Autre exemple de code en Ruby :L'analogie du code source et de la recette de cuisine est souvent employée dans une volonté de vulgarisation. Une recette est une liste organisée d'ingrédients dont les quantités et les fonctions sont définies. Le but est d'obtenir le résultat voulu par le cuisinier, selon une technique et un enchaînement d'opérations déterminés.Ainsi le code source peut être apparenté à une recette de cuisine.Ainsi, une personne dégustant un plat est en mesure de deviner les ingrédients qui le composent et d'imaginer comment le réaliser. Néanmoins, pour un plat très raffiné et subtil (comme pourrait l'être un programme), il est fort probable qu'elle ignore le mode opératoire du cuisinier. Pour le connaître, une recette détaillée serait nécessaire (pour un programme, la recette peut compter plusieurs millions de lignes de code). La solution alternative à cela serait d'acheter des plats préparés, c'est un peu ce que l'on fait lorsqu'on achète des logiciels.Le code source peut être public ou privé (voir logiciel libre et logiciel propriétaire). Toutefois, le code binaire n'étant qu'une traduction du code source, il est toujours possible d'étudier un logiciel à partir de son code binaire. La légalité des techniques utilisées à ces fins dépend du pays et de l'époque. Elle peut notamment être mise en œuvre pour percer les secrets d'une machine comme l'ES3B. Portail de la programmation informatique"
Informatique;"La cryptographie est une des disciplines de la cryptologie s'attachant à protéger des messages (assurant confidentialité, authenticité et intégrité) en s'aidant souvent de secrets ou clés. Elle se distingue de la stéganographie qui fait passer inaperçu un message dans un autre message alors que la cryptographie rend un message supposément inintelligible à autre que qui-de-droit.Elle est utilisée depuis l'Antiquité, mais certaines de ses méthodes les plus modernes, comme la cryptographie asymétrique, datent de la fin du XXe siècle.Le mot cryptographie vient des mots en grec ancien kruptos (???????) « caché » et graphein (???????) « écrire ». Beaucoup des termes de la cryptographie utilisent la racine « crypt- », ou des dérivés du terme « chiffre » :chiffrement : transformation à l'aide d'une clé d'un message en clair (dit texte clair) en un message incompréhensible (dit texte chiffré) pour celui qui ne dispose pas de la clé de déchiffrement (en anglais encryption key ou private key pour la cryptographie asymétrique) ;chiffre : un ensemble de règles permettant d'écrire et de lire dans un langage secret ;cryptogramme : message chiffré ;cryptosystème : algorithme de chiffrement ;décrypter : retrouver le message clair correspondant à un message chiffré sans posséder la clé de déchiffrement (terme que ne possèdent pas les anglophones, qui eux « cassent » des codes secrets) ;cryptographie : étymologiquement « écriture secrète », devenue par extension l'étude de cet art (donc aujourd'hui la science visant à créer des cryptogrammes, c'est-à-dire à chiffrer) ;cryptanalyse : science analysant les cryptogrammes en vue de les décrypter ;cryptologie : science regroupant la cryptographie et la cryptanalyse ;cryptolecte : jargon réservé à un groupe restreint de personnes désirant dissimuler leur communication.Plus récemment sont apparus les termes « crypter » (pour chiffrer) et « cryptage » pour chiffrement. Ceux-ci sont acceptés par l'Office québécois de la langue française dans son grand dictionnaire terminologique, qui note que « La tendance actuelle favorise les termes construits avec crypt-. ». Le Grand Robert mentionne également « cryptage », et date son apparition de 1980. Cependant le Dictionnaire de l'Académie française n'intègre ni « crypter » ni « cryptage » dans sa dernière édition (entamée en 1992). Ces termes sont d'ailleurs considérés comme incorrects par exemple par l'ANSSI, qui met en avant le sens particulier du mot « décrypter » (retrouver le message clair à partir du message chiffré sans connaître la clef) en regard du couple chiffrer/déchiffrer.La cryptographie est utilisée depuis l'antiquité, et l'une des utilisations les plus célèbres pour cette époque est le chiffre de César, nommé en référence à Jules César qui l'utilisait pour ses communications secrètes. Mais la cryptographie est bien antérieure à cela : le plus ancien document chiffré est une recette secrète de poterie datant du XVIe siècle av. J.-C., notée sur une tablette d'argile qui a été découverte dans l'actuel Irak.L'historien en cryptographie David Kahn considère l'humaniste Leon Battista Alberti comme le « père de la cryptographie occidentale », grâce à trois avancées significatives : « la plus ancienne théorie occidentale de cryptanalyse, l'invention de la substitution polyalphabétique, et l'invention du code de chiffrement ».Bien qu'éminemment stratégique, la cryptographie est restée pendant très longtemps un art, pour ne devenir une science qu'au XXIe siècle. Avec l'apparition de l'informatique, son utilisation se popularise et se vulgarise, quitte à se banaliser et à être utilisée à l'insu de l’utilisateur[réf. nécessaire].Enfin, la Cryptographie post-quantique est une sous-discipline de la cryptographie qui cherche à proposer des algorithmes résistant au calculateur quantique.Les domaines d'utilisations de la cryptographie sont vastes et vont du domaine militaire, au commercial, en passant par la protection de la vie privée.Les techniques de cryptographie sont parfois utilisées pour protéger notre vie privée. Ce droit est en effet plus facilement bafoué dans la sphère numérique. Ainsi les limites de la cryptographie quant à sa capacité à préserver la vie privée soulève des questionnements. Deux exemples qui illustrent bien ce sujet sont à trouver dans le domaine de la santé et celui de la blockchain.La santé est un domaine sensible quant à la protection des données : le secret médical est remis en question avec l’informatisation de la médecine.La cryptographie permet en théorie de protéger les données médicales pour qu’elles ne soient pas accessible à n’importe qui, mais elle n’est pas suffisante.Car tant que le droit n’est pas suffisamment large[pas clair], il existe des failles qui permettent à certains acteurs d’utiliser des données personnelles dès l'accord de l'usager donné, or cet accord est exigé pour l'accès au service, faisant ainsi perdre à l'utilisateur la possibilité de contrôle de ses  accès à nos données personnelles.De plus l’inviolabilité des données médicales est remise en question par les développements qui permettent le déchiffrement de ces données, en effet selon Bourcier et Filippi, l’« anonymat ne semble plus garanti de façon absolue en l’état actuel des techniques de cryptographie ». Avec cette double constatation ils proposent de protéger nos données médicales avec une réforme juridique qui permettrait de faire rentrer les données personnelles médicales non pas dans le droit à la vie privée qui est un droit personnel, mais dans un droit collectif qui permettrait de protéger plus efficacement des données telles que les données génétiques qui concernent plusieurs individus. La création d’un droit collectif pour la santé permettrait ainsi de compenser les limites de la cryptographie qui n’est pas en mesure d’assurer à elle seule la protection de ce type de données.La blockchain est elle aussi l’une des applications de la cryptographie en lien avec la protection de la vie privée. C’est un système décentralisé qui se base entre autres sur des techniques de cryptographie destinées à assurer la fiabilité des échanges tout en garantissant en principe la vie privée. Qui dit système décentralisé implique qu’il n’y a pas de tierce personne par laquelle passe les informations. Ainsi seuls les individus concernés ont accès aux données vu que les données sont chiffrées, d’où un respect important de la vie privée. En pratique cela dit, ce système présente des limites : « la décentralisation est acquise au prix de la transparence ». En effet un tel système ne protège pas les informations concernant la transaction : destinataire, date, et autres métadonnées qui sont nécessaires pour s’assurer de la légitimité. Ainsi une protection complète de la vie privée en blockchain nécessite que ces métadonnées soient elles aussi protégées, puisque celles-ci sont transparentes et donc visibles par tout le monde. Cette protection supplémentaire est rendue possible par de nouvelles techniques d'anonymisation des signatures telles que la signature aveugle, qui sont réputées de garantir la légitimité des transactions sans les rendre publiques. Mais ce processus n’est pas encore applicable partout et n’est qu’à l’état embryonnaire pour certaines techniques. Malgré tout avec le temps de plus en plus de systèmes permettront de résoudre cette limitation.[Quand ?]Le cadre législatif de la cryptographie est variable et sujet aux évolutions.D’une part, il est sujet aux évolutions des technologies, de leur efficacité et de leur accessibilité. En effet la démocratisation d’Internet et des ordinateurs personnels fondent un nouveau cadre dans les années 80-90, comme nous le verrons avec l’exemple de la loi française.D’autre part, ces lois évoluent selon le contexte politique. En effet, à la suite des attentats du 11 septembre 2001, les gouvernements occidentaux opèrent une reprise du contrôle des données circulant sur Internet et de toutes les données potentiellement cachées par la cryptographie.Cela se fait de plusieurs façons : d’une part, par la mise en place de lois obligeant les fournisseurs de systèmes de communication, cryptés ou non, à fournir à certaines entités étatiques des moyens d’accéder à toutes ces données. Par exemple en France, alors qu’en 1999, la loi garantit la protection des communications privées par voie électronique, celle-ci subit l’amendement à la Loi no 91-646 du 10 juillet 1991 relative au secret des correspondances émises par la voie des communications électroniques. Cet amendement formalise précisément le moyen législatif d’accéder à des données encryptées décrit précédemment.D’autre part, certains services gouvernementaux développent des systèmes d’inspection de réseaux afin de tirer des informations malgré le chiffrement des données. On peut notamment citer le programme de surveillance électronique Carnivore aux États-Unis.Toutefois, la réglementation sur les systèmes de cryptographie ne laisse que peu de place à un contrôle par des entités telles que des gouvernements. En effet, les logiciels et algorithmes les plus performants et répandus sont issus de la connaissance et des logiciels libres comme PGP ou OpenSSH. Ceux-ci offrent une implémentation fonctionnelle des algorithmes de chiffrement modernes pour assurer le chiffrement de courriels, de fichiers, de disques durs ou encore la communication dite sécurisée entre plusieurs ordinateurs. Ces logiciels étant sous licence libre, leur code source est accessible, reproductible et modifiable. Cela implique qu’il est techniquement très difficile de les rendre exclusifs à une entité — étatique par exemple — et d’en avoir le contrôle. Le chiffrement devient alors utilisable par nombre de personnes, permettant de contrevenir à une loi.Bien que la cryptographie puisse paraître être une opportunité pour la démocratie au premier abord, la réalité n’est pas forcément si unilatérale. Il est clair que l’utilisation de cette technologie permet de protéger la liberté d’expression. Toutefois, cela ne suffit pas à dire que la cryptographie est bénéfique à la démocratie, puisque l'enjeu démocratique dépasse la simple liberté l’expression. En particulier, la démocratie suppose un système de lois et de mécanismes de sanctions qui mène la liberté d’expression vers une activité politique constructive.Avec l’apparition de la cryptographie électronique et dans un monde toujours plus numérisé, la politique doit aussi s’adapter. Winkel observe trois politiques différentes pour les gouvernements: la stratégie libérale, la stratégie de prohibition et la stratégie du tiers de confiance. Stratégie de prohibition La stratégie de prohibition consiste à restreindre l’utilisation de la cryptographie en imposant des contrôles d’import-export, des restrictions d’utilisation ou encore d’autres mesures pour permettre à l’État et ses institutions de mettre en œuvre dans le monde virtuel la politique (principes et lois) du « vrai » monde. Cette stratégie est généralement appliquée dans des pays à régime politique autoritaire, par exemple en Chine avec le Grand Firewall ou en Corée du Nord. Stratégie du tiers de confiance La stratégie du tiers de confiance a pour but de garder la balance qu’il existe dans le « vrai » monde entre d’un côté la législation et les potentielles sanctions de l’État et de l’autre la protection de secrets économiques ou de la sphère privée, dans le monde virtuel. La mise en place d’un tel système est toutefois plus technique.Le principe consiste en un dépôt des copies des clés d’encryption des utilisateurs dans les mains d’un tiers de confiance. Celui-ci pourrait ensuite répondre à une demande d'une autorité légale compétente et lui transmettre une clef - par exemple à des fins d’audit - à condition que cette demande ait suivi une procédure bien définie. Cette solution, bien que paraissant optimale du point de vue de la théorie démocratique, présente déjà un certain nombre de difficultés techniques comme la mise en place et l'entretien de l’infrastructure requise. De plus, il est utopique d’imaginer que la mise en place de cadres légaux plus sévères découragera les criminels et organisations anticonstitutionnelles d’arrêter leurs activités. Cela s’applique à la stratégie du tiers de confiance et à celle de prohibition. Stratégie libérale La stratégie libérale répandue dans le monde laisse un accès ""total"" aux technologies de cryptographie, pour sécuriser la vie privée des citoyens, défendre la liberté d’expression dans l’ère numérique, laisser les entreprises garder leurs secrets et laisser les entreprises exporter des solutions informatiques sécurisées sur les marchés internationaux.Cependant, les criminels et opposants de la Constitution[Laquelle ?] peuvent utiliser cette technologie à des fins illicites — ou anticonstitutionnelles —[Laquelle ?] comme  armes, drogue ou pédopornographie sur le Dark Web. Autres formes de législation Les États-Unis et la France interdisent l'exportation de certaines formes de cryptographie, voir Lois sur les chiffrement sur wikipedia anglophone.Les premiers algorithmes utilisés pour le chiffrement d'une information étaient assez rudimentaires dans leur ensemble. Ils consistaient notamment au remplacement de caractères par d'autres. La confidentialité de l'algorithme de chiffrement était donc la pierre angulaire de ce système pour éviter un décryptage rapide.Exemples d'algorithmes de chiffrement faibles :ROT13 (rotation de 13 caractères, sans clé) ;Chiffre de César (décalage de trois lettres dans l'alphabet sur la gauche) ;Chiffre de Vigenère (introduit la notion de clé).Les algorithmes de chiffrement symétrique se fondent sur une même clé pour chiffrer et déchiffrer un message. L'un des problèmes de cette technique est que la clé, qui doit rester totalement confidentielle, doit être transmise au correspondant de façon sûre. La mise en œuvre peut s'avérer difficile, surtout avec un grand nombre de correspondants car il faut autant de clés que de correspondants.Quelques algorithmes de chiffrement symétrique très utilisés :Chiffre de Vernam (le seul offrant une sécurité théorique absolue, à condition que la clé ait au moins la même longueur que le message à chiffrer, qu'elle ne soit utilisée qu'une seule fois et qu'elle soit totalement aléatoire)DES3DESAESRC4RC5MISTY1et d'autres (voir la liste plus exhaustive d'algorithmes de cryptographie symétrique).Pour résoudre le problème de l'échange de clés, la cryptographie asymétrique a été mise au point dans les années 1970. Elle se base sur le principe de deux clés :une publique, permettant le chiffrement ;une privée, permettant le déchiffrement.Comme son nom l'indique, la clé publique est mise à la disposition de quiconque désire chiffrer un message. Ce dernier ne pourra être déchiffré qu'avec la clé privée, qui doit rester confidentielle.Quelques algorithmes de cryptographie asymétrique très utilisés :RSA (chiffrement et signature) ;DSA (signature) ;Protocole d'échange de clés Diffie-Hellman (échange de clé) ;et d'autres ; voir cette liste plus complète d'algorithmes de cryptographie asymétrique.Le principal inconvénient de RSA et des autres algorithmes à clés publiques est leur grande lenteur par rapport aux algorithmes à clés secrètes. RSA est par exemple 1000 fois plus lent que DES. En pratique, dans le cadre de la confidentialité, on s'en sert pour chiffrer un nombre aléatoire qui sert ensuite de clé secrète pour un algorithme de chiffrement symétrique. C'est le principe qu'utilisent des logiciels comme PGP par exemple.La cryptographie asymétrique est également utilisée pour assurer l'authenticité d'un message. L'empreinte du message est chiffrée à l'aide de la clé privée et est jointe au message. Les destinataires déchiffrent ensuite le cryptogramme à l'aide de la clé publique et retrouvent normalement l'empreinte. Cela leur assure que l'émetteur est bien l'auteur du message. On parle alors de signature ou encore de scellement.La plupart des algorithmes de cryptographie asymétrique sont vulnérables à des attaques utilisant un calculateur quantique, à cause de l'algorithme de Shor. La branche de la cryptographie visant à garantir la sécurité en présence d'un tel adversaire est la cryptographie post-quantique.Une fonction de hachage est une fonction qui convertit un grand ensemble en un plus petit ensemble, l'empreinte. Il est impossible de la déchiffrer pour revenir à l'ensemble d'origine, ce n'est donc pas une technique de chiffrement.Quelques fonctions de hachage très utilisées :MD5 ;SHA-1 ;SHA-256 ;et d'autres ; voir cette liste plus complète d'algorithmes de hachage.L'empreinte d'un message ne dépasse généralement pas 256 bits (maximum 512 bits pour SHA-512) et permet de vérifier son intégrité.Projet NESSIEAdvanced Encryption Standard processLes cryptologues sont des experts en cryptologie : ils conçoivent, analysent et cassent les algorithmes (voir cette liste de cryptologues).Le mouvement Cypherpunk, qui regroupe des partisans d'une idéologie dite « cyber libertarienne », est un mouvement créé en 1991 œuvrant pour défendre les droits civils numériques des citoyens, à travers la cryptographie.Essentiellement composé de hackers, de juristes et de militants de la liberté sur le web ayant pour objectif commun une plus grande liberté de circulation de l'information, ce groupe s'oppose à toute intrusion et tentative de contrôle du monde numérique par des grandes puissances, en particulier les États.Les crypto-anarchistes considèrent la confidentialité des données privées comme un droit inhérent. En s'inspirant du système politique libéral américain, ils défendent le monde numérique en tant qu'espace à la fois culturel, économique et politique à l'intérieur d'un réseau ouvert et décentralisé, où chaque utilisateur aurait sa place et pourrait jouir de tous ses droits et libertés individuelles.Les crypto-anarchistes cherchent à démontrer que les libertés numériques ne sont pas des droits à part, contraints d’exister seulement dans le domaine technique qu’est internet mais que maintenant le numérique est un élément important et omniprésent dans la vie quotidienne, et ainsi, il est primordial dans la définition des libertés fondamentales des citoyens. Les droits et libertés numériques ne doivent pas être considérées comme moins importante que celles qui régissent le monde matériel.La création des crypto-monnaies en mai 1992[réf. souhaitée], remplit un des objectifs du mouvement en offrant une monnaie digitale intraçable en ligne mais permet également l'expansion de marchés illégaux sur le web.L’apparition de nouvelles techniques (logiciels de surveillance de masse comme Carnivore, PRISM, XKeyscore...) a en fait mené à plus de surveillance, moins de vie privée, et un plus grand contrôle de la part des États qui se sont approprié ces nouvelles technologies.Crypto-anarchistes (pour l’anonymisation des communications) et États (pour le contrôle des communications) s’opposent le long de ces arguments.Un axiome central du mouvement Cypherpunk est que, pour rééquilibrer les forces entre l’État et les individus, il faut la protection des communications privées ainsi que la transparence des informations d’intérêt public, comme l’énonce la devise : « Une vie privée pour les faibles et une transparence pour les puissants ».Dans ce sens, Julian Assange (un des plus importants membres du mouvement Cypherpunk) a créé WikiLeaks, un site qui publie aux yeux de tous, des documents et des secrets d’État initialement non connus du grand public.Les événements du 11 septembre 2001 ont été des arguments de poids pour les États, qui avancent qu'une régulation et un contrôle du monde d'internet sont nécessaires afin de préserver nos libertés.L'apparition de lanceurs d'alerte comme Edward Snowden en 2013 est un événement important en faveur du mouvement crypto-anarchiste qui s'oppose au contrôle de l’État dans le monde numérique.D'autres groupes/mouvements importants sont créés pour défendre les libertés d’internet, partageant des objectifs avec le mouvement Cypherpunk :Les Anonymous qui défendent la liberté d'expression sur internet et en dehors.L'Electronic Frontier Foundation (EFF) qui défend la confidentialité des données numériques.Le Parti Pirate qui défend l’idée des partages des données et se bat pour les libertés fondamentales sur Internet (partage d’informations, de savoirs culturels et scientifiques qui sont parfois bannis d’internet).David Kahn (trad. de l'anglais par Pierre Baud, Joseph Jedrusek), La guerre des codes secrets [« The Codebreakers »], Paris, InterEditions, 1980, 405 p. (ISBN 2-7296-0066-3).Simon Singh (trad. Catherine Coqueret), Histoire des codes secrets [« The Code Book »], Librairie générale française (LFG), coll. « Le Livre de Poche », 3 septembre 2001, 504 p., Poche (ISBN 2-253-15097-5, ISSN 0248-3653, OCLC 47927316).Jacques Stern, La Science du secret, Paris, Odile Jacob, coll. « Sciences », 5 janvier 1998, 203 p. (ISBN 2-7381-0533-5, OCLC 38587884, lire en ligne)Gilles Zémor, Cours de cryptographie, Paris, Cassini, 15 décembre 2000, 227 p. (ISBN 2-84225-020-6, OCLC 45915497).« L'art du secret », Pour la science, dossier hors-série, juillet-octobre 2002.Douglas Stinson (trad. de l'anglais par Serge Vaudenay, Gildas Avoine, Pascal Junod), Cryptographie : Théorie et pratique [« Cryptography : Theory and Practice »], Paris, Vuibert, coll. « Vuibert informatique », 28 février 2003, 337 p., Broché (ISBN 2-7117-4800-6, ISSN 1632-4676, OCLC 53918605)(en) Handbook of Applied Cryptography, A.J. Menezes, éd. P.C. van Oorschot et S.A. Vanstone - CRC Press, 1996. Disponible en ligne : [1]Site thématique de la sécurité des systèmes d'information : site officiel de l'Agence nationale de la sécurité des systèmes d'information sur la question de la sécurité informatique. Présentation de la cryptographie, des signatures numériques, de la législation française sur le sujet, etc.Bruce Schneier (trad. de l'anglais par Laurent Viennot), Cryptographie appliquée [« Applied cryptography »], Paris, Vuibert, coll. « Vuibert informatique », 15 janvier 2001, 846 p., Broché (ISBN 2-7117-8676-5, ISSN 1632-4676, OCLC 46592374).Niels Ferguson, Bruce Schneier (trad. de l'anglais par Henri-Georges Wauquier, Raymond Debonne), Cryptographie : en pratique [« Practical cryptography »], Paris, Vuibert, coll. « En pratique / Sécurité de l'information et des systèmes », 18 mars 2004, 338 p., Broché (ISBN 2-7117-4820-0, ISSN 1632-4676, OCLC 68910552).Pierre Barthélemy, Robert Rolland et Pascal Véron (préf. Jacques Stern), Cryptographie : principes et mises en œuvre, Paris, Hermes Science Publications : Lavoisier, coll. « Collection Informatique », 22 juillet 2005, 414 p., Broché (ISBN 2-7462-1150-5, ISSN 1242-7691, OCLC 85891916).Auguste Kerckhoffs, La Cryptographie militaire, L. Baudoin, 1883.Marcel Givierge, Cours de cryptographie, Berger-Levrault, 1925.Jean-Guillaume Dumas, Pascal Lafourcade, Patrick Redon, Architectures de sécurité pour internet - 2e éd. Protocoles, standards et déploiement , Dunod 2020.Jean-Guillaume Dumas, Jean-Louis Roch, Sébastien Varrette, Eric Tannier,Théorie des codes - 3e éd. : Compression, cryptage, correction, Dunod 2018.Jean-Guillaume Dumas, Pascal Lafourcade, Etienne Roudeix, Ariane Tichit, Sébastien Varrette, Les NFT en 40 questions: Comprendre les jetons Non Fungible, Dunod 2022.Jean-Guillaume Dumas, Pascal Lafourcade, Ariane Tichit, Sébastien Varrette, Les blockchains en 50 questions - 2éd.: Comprendre le fonctionnement de cette technologie, Dunod 2022.Pascal Lafourcade, Malika More, 25 énigmes ludiques pour s'initier à la cryptographie, Dunod 2021.Henry Mamy, « La cryptographie », dans Science et Guerre, vol. 16, Bernard Tignole éditeur, 1888 (lire en ligne), disponible sur GallicaLa Cryptogr@phie expliquée!, démonstrations avec des applets Java.ACrypTA, cours, exercices, textes, liens concernant la cryptographie.Ars cryptographica , vulgarisation très complète.Cryptographie, ressources, algorithmes, des ressources sur les algorithmes cryptographiques de dernière génération et sur la cryptographie classique.Cryptographie, du chiffre et des lettres, exposé de François Cayre sur le site Interstices.(en) Handbook of Applied Cryptography, une référence de plus de 800 pages dont l'édition de 1996 peut être téléchargée gratuitement Portail de la cryptologie   Portail de la sécurité de l’information   Portail de la sécurité informatique"
Informatique;
"is caté""";
Informatique;"Un langage de programmation est une notation conventionnelle destinée à formuler des algorithmes et produire des programmes informatiques qui les appliquent. D'une manière similaire à une langue naturelle, un langage de programmation est composé d'un alphabet, d'un vocabulaire, de règles de grammaire, de significations, mais aussi d'un environnement de traduction censé rendre sa syntaxe compréhensible par la machine,.Les langages de programmation permettent de décrire d'une part les structures des données qui seront manipulées par l'appareil informatique, et d'autre part d'indiquer comment sont effectuées les manipulations, selon quels algorithmes. Ils servent de moyens de communication par lesquels le programmeur communique avec l'ordinateur, mais aussi avec d'autres programmeurs ; les programmes étant d'ordinaire écrits, lus, compris et modifiés par une équipe de programmeurs.Un langage de programmation est mis en œuvre par un traducteur automatique : compilateur ou interprète. Un compilateur est un programme informatique qui transforme dans un premier temps un code source écrit dans un langage de programmation donné en un code cible qui pourra être directement exécuté par un ordinateur, à savoir un programme en langage machine ou en code intermédiaire, tandis que l’interprète réalise cette traduction « à la volée ».Les langages de programmation offrent différentes possibilités d'abstraction et une notation proche de l'algèbre, permettant de décrire de manière concise et facile à saisir les opérations de manipulation de données et l'évolution du déroulement du programme en fonction des situations. La possibilité d'écriture abstraite libère l'esprit du programmeur d'un travail superflu, notamment de prise en compte des spécificités du matériel informatique, et lui permet ainsi de se concentrer sur des problèmes plus avancés.Chaque langage de programmation supporte une ou plusieurs approches de la programmation – paradigmes. Les notions induisant le paradigme font partie du langage de programmation et permettent au programmeur d'exprimer dans le langage une solution qui a été imaginée selon ce paradigme.Les premiers langages de programmation ont été créés dans les années 1950 en même temps que l'avènement des ordinateurs. Cependant, de nombreux concepts de programmation ont été initiés par un langage ou parfois plusieurs langages, avant d'être améliorés puis étendus dans les langages suivants. La plupart du temps la conception d'un langage de programmation a été fortement influencée par l'expérience acquise avec les langages précédents.Un langage de programmation est construit à partir d'une grammaire formelle, qui inclut des symboles et des règles syntaxiques, auxquels on associe des règles sémantiques. Ces éléments sont plus ou moins complexes selon la capacité du langage. Les modes de fonctionnement et de définition de la complexité d'un langage de programmation sont généralement déterminés par leur appartenance à l'un des degrés de la hiérarchie de Chomsky.Sous un angle théorique, tout langage informatique peut être qualifié de langage de programmation s'il est Turing-complet, c'est-à-dire qu'il permet de représenter toutes les fonctions calculables au sens de Turing et Church (en admettant néanmoins pour exception à la théorie que la mémoire des ordinateurs n'est pas un espace infini).Les règles de syntaxeDéfinies par une grammaire formelle, elles régissent les différentes manières dont les éléments du langage peuvent être combinés pour obtenir des programmes. La ponctuation (par exemple l'apposition d'un symbole ; en fin de ligne d'instruction d'un programme) relève de la syntaxe.Le vocabulaireParmi les éléments du langage, le vocabulaire représente l'ensemble des instructions construites d’après des symboles. L'instruction peut être mnémotechnique ou uniquement symbolique comme quand elle est représentée par des symboles d'opérations tels que des opérateurs arithmétiques (« + » et « - ») ou booléens (&& pour le et logique par exemple). On parle aussi parfois de mot clé pour désigner une instruction (par abus de langage car le concept de mot clé ne recouvre pas celui des symboles qui font pourtant eux aussi partie du vocabulaire).La sémantiqueLes règles de sémantique définissent le sens de chacune des phrases qui peuvent être construites dans le langage, en particulier quels seront les effets de la phrase lors de l'exécution du programme. La science l’étudiant est la sémantique des langages de programmation.L’alphabetL'alphabet des langages de programmation est basé sur les normes courantes comme ASCII, qui comporte les lettres de A à Z sans accent, des chiffres et des symboles, ou Unicode pour la plupart des langages modernes (dans lesquels l'utilisation se limite en général aux chaînes de caractères littérales et aux commentaires, avec quelques exceptions notables comme C? qui autorisent également les identifiants unicode).La plupart des langages de programmation peuvent prévoir des éléments de structure complémentaires, des méthodes procédurales, et des définitions temporaires et variables et des identifiants :Les commentairesLes commentaires sont des textes qui ne seront pas traduits. Ils peuvent être ajoutés dans les programmes pour y laisser des explications. Les commentaires sont délimités par des marques qui diffèrent d'un langage de programmation à l'autre tel que « -- », « /* » ou « // ».Les identifiantsLes éléments constitutifs du programme, tels que les variables, les procédures ou les types servent à organiser le programme et son fonctionnement. On peut ainsi, par exemple, diviser un programme en fonctions ou lui donner une structure par objets : ces éléments de structure sont définis par des identifiants ou des procédures par mot clé selon le langage.Un langage de programmation offre un cadre pour élaborer des algorithmes et exprimer des diagrammes de flux,. Il permet en particulier de décrire les structures des données qui seront manipulées par l'appareil informatique et quelles seront les manipulations. Un langage de programmation sert de moyen de communication avec l'ordinateur mais aussi entre programmeurs : les programmes étant d'ordinaire écrits, lus et modifiés par une équipe de programmeurs.Un langage de programmation offre un ensemble de notions qui peuvent être utilisées comme primitives pour développer des algorithmes. Les programmeurs apprécient que le langage soit clair, simple et unifié, qu'il y ait un minimum de notions qui peuvent être combinées selon des règles simples et régulières. Les qualités d'un langage de programmation influent sur la facilité avec laquelle les programmes pourront être écrits, testés, puis plus tard compris et modifiés.La facilité d'utilisation, la portabilité et la clarté sont des qualités appréciées des langages de programmation. La facilité d'utilisation, qui dépend de la syntaxe, du vocabulaire et des symboles, influence la lisibilité des programmes écrits dans ce langage et la durée d'apprentissage. La portabilité permet à un programme écrit pour être exécuté par une plateforme informatique donnée (un système d'exploitation) d'être transféré en vue d'être exécuté sur une autre plateforme.Les programmeurs apprécient que la syntaxe permette d'exprimer la structure logique inhérente au programme. Un des soucis en programmation est d'éviter des pannes, qu'il soit possible de les détecter, les éviter et les rectifier ; ceci est rendu possible par des mécanismes internes des langages de programmation. Des vérifications implicites sont parfois effectuées en vue de déceler des problèmes.Les programmeurs apprécient qu'un langage de programmation soit en ligne avec les bonnes pratiques de programmation et d'ingénierie, qu'il encourage la structuration du programme, facilite la maintenance des programmes et qu'il dissuade, voire interdise les mauvaises pratiques. L'utilisation de l'instruction goto, par exemple, qui existe depuis les premiers langages de programmation, est considérée comme une mauvaise pratique. Son utilisation est déconseillée, voire impossible dans les langages de programmation récents.L'alignement sur les standards industriels, la possibilité d'utiliser des fonctionnalités écrites dans un autre langage de programmation et l'exécution simultanée de plusieurs threads sont des possibilités appréciées des langages de programmation.Un langage de programmation repose sur un ensemble de notions telles que les instructions, les variables, les types et les procédures ou fonctions, qui peuvent être utilisées comme primitives pour développer des algorithmes. Une instruction Un ordre donné à un ordinateur. Une variable Un nom utilisé dans un programme pour faire référence à une donnée manipulée par programme. Une constante Un nom utilisé pour faire référence à une valeur permanente. Une expression littérale Une valeur mentionnée en toutes lettres dans le programme. Un type Chaque donnée a une classification, celle-ci influe sur la plage de valeurs possibles, les opérations qui peuvent être effectuées et la représentation de la donnée sous forme de bits. Chaque langage de programmation offre une gamme de types primitifs, incorporés dans le langage. Certains langages offrent la possibilité de créer des nouveaux types.Les types de données primitifs courants sont les nombres entiers, les nombres réels, le booléen, les chaînes de caractères et les pointeurs.Plus précisément, le type booléen est un type qui n'a que deux valeurs, vrai et faux, tandis que le type pointeur fait référence à une donnée qui se trouve quelque part en mémoire. Une structure de données Une manière caractéristique d'organiser un ensemble de données en mémoire, qui influe sur les algorithmes utilisés pour les manipuler. Les structures courantes sont les tableaux, les enregistrements, les listes, les piles, les files et les arbres. Une déclaration Une phrase de programme qui sert à renseigner au traducteur (compilateur, interpréteur...) les noms et les caractéristiques des éléments du programme tels que des variables, des procédures, de types, etc.Des vérifications sont effectuées au moment de la compilation ou lors de l'exécution du programme, pour assurer que les opérations du programme sont possibles avec les types de données qui sont utilisés. Dans un langage fortement typé, chaque élément du programme a un type unique, connu et vérifié au moment de la compilation, ce qui permet de déceler des erreurs avant d'exécuter le programme. Les procédures, fonctions, méthodes Divers langages de programmation offrent la possibilité d'isoler un fragment de programme et d'en faire une opération générale, paramétrable, susceptible d'être utilisée de façon répétée. Ces fragments sont appelés procédures, fonctions ou méthodes, selon le paradigme. Les modules Les langages de programmation peuvent également offrir la possibilité de découper un programme en plusieurs pièces appelées modules, chacune ayant un rôle déterminé, puis de combiner les pièces.Les notions de procédure et de module sont destinées à faciliter la création de programmes complexes et volumineux en assistant la prise en charge de cette complexité. Ces fonctions permettent en particulier la modularité et l'abstraction.Un paradigme est une façon d'approcher la programmation. Chaque paradigme amène sa philosophie de la programmation ; une fois qu'une solution a été imaginée par un programmeur selon un certain paradigme, un langage de programmation qui suit ce paradigme permettra de l'exprimer. Impératif, déclaratif, fonctionnel, logique, orienté objet, concurrent, visuel, événementiel et basé web sont des paradigmes de programmation. Chaque langage de programmation reflète un ou plusieurs paradigmes, apportant un ensemble de notions qui peuvent être utilisées pour exprimer une solution à un problème de programmation. Au cours de l'histoire, les scientifiques et les programmeurs ont identifié les avantages et les limitations d'un style de programmation et apporté de nouveaux styles. La plupart des langages de programmation contemporains permettent d'adopter plusieurs paradigmes de programmation à condition que ceux-ci soient compatibles.Le paradigme impératif ou procédural est basé sur le principe de l'exécution étape par étape des instructions tout comme on réalise une recette de cuisine. Il est basé sur le principe de la machine de Von Neumann. Un ensemble d'instructions de contrôle de flux d'exécution permet de contrôler l'ordre dans lequel sont exécutées les instructions qui décrivent les étapes. Le C, le Pascal, le Fortran et le Cobol sont des exemples de langage de programmation qui implémentent le paradigme impératif.Il y a essentiellement deux paradigmes déclaratifs ; ce sont le paradigme fonctionnel et le paradigme logique. En paradigme fonctionnel, le programme décrit des fonctions mathématiques. En paradigme logique, il décrit des prédicats : c'est-à-dire des déclarations qui, une fois instanciées, peuvent être vraies ou fausses ou ne pas recevoir de valeur de vérité (quand l'évaluation du prédicat ne se termine pas). Dans un modèle d'implantation, une machine abstraite effectue les opérations nécessaires pour calculer le résultat de chaque fonction ou chaque prédicat. Dans ces paradigmes, une variable n'est pas modifiée par affectation. Une des caractéristiques principales est la transparence référentielle, qui fait qu'une expression peut être remplacée par son résultat sans changer le comportement du programme. Fonctionnel Le paradigme fonctionnel a pour principe l'évaluation de formules, afin d'utiliser le résultat pour d'autres calculs ; il s'appuie sur la récursivité et il a pour modèle le lambda-calcul, plus précisément la réduction en forme normale de tête. Tous les calculs évaluent des expressions ou font appel à des fonctions. Pour simplifier, le résultat d'un calcul sert pour le calcul ou les calculs qui ont besoin de son résultat jusqu'à ce que la fonction qui produit le résultat du programme ait été évaluée. Le paradigme fonctionnel a été introduit par les langages Lisp et ISWIM ainsi qu'en ce qui concerne les fonctions récursives par Algol 60, dans les années 1960. Des langages tels que Ruby et Scala supportent plusieurs paradigmes dont le paradigme fonctionnel, tandis qu'Haskell ne supporte que le paradigme fonctionnel et OCaml privilégie le paradigme fonctionnel qu'il partage avec le paradigme objet et une petite dose d'impératif. Logique Le paradigme logique vise à répondre à une question par des recherches dans un ensemble, en utilisant des axiomes, des requêtes et des règles de déduction. L'exécution d'un programme est une cascade de recherches de faits dans un ensemble, en invoquant des règles de déduction. Les données obtenues, peuvent être associées à un autre ensemble de règles et peuvent alors être utilisées dans le cadre d'une autre recherche. L'exécution du programme se fait par évaluation : le système effectue une recherche de toutes les affirmations qui, par déduction, correspondent à au moins un élément de l'ensemble. Le programmeur exprime les règles, et le système pilote le processus. Le paradigme logique a été introduit par le langage Prolog en 1970.Le paradigme orienté objet est destiné à faciliter le découpage d'un grand programme en plusieurs modules isolés les uns des autres. Il introduit les notions d'objet et d'héritage. Un objet contient les variables et les fonctions en rapport avec un sujet. Les variables peuvent être privées, c'est-à-dire qu'elles peuvent être manipulées uniquement par l'objet qui les contient. Un objet contient implicitement les variables et les fonctions de ses ancêtres, et cet héritage aide à réutiliser du code. Le paradigme orienté objet permet d'associer fortement les données avec les procédures. Il a été introduit par le langage Simula dans les années 1960 et est devenu populaire dans les années 1980, quand l'augmentation de la puissance de calcul des ordinateurs a permis d'exécuter des grands programmes. Divers langages de programmation ont été enrichis en vue de permettre la programmation orientée objet ; c'est le cas de C++ (dérivé du langage C), Simula, Smalltalk, Swift et Java sont des langages de programmation en paradigme orienté objet.En paradigme concurrent un programme peut effectuer plusieurs tâches en même temps. Ce paradigme introduit les notions de thread, d'attente active et d'appel de fonction à distance. Ces notions ont été introduites dans les années 1980 lorsque, à la suite de l'évolution technologique, un ordinateur est devenu une machine comportant plusieurs processeurs et capable d'effectuer plusieurs tâches simultanément. Les langages de programmation contemporains de 2013 tels que C++ et Java sont adaptés aux microprocesseurs multi-cœur et permettent de créer et manipuler des threads. Plus récemment, on a vu apparaître des langages intégralement orientés vers la gestion de la concurrence, comme le langage Go.Dans la grande majorité des langages de programmation, le code source est un texte, ce qui rend difficile l'expression des objets bidimensionnels. Un langage de programmation tel que Delphi ou C# permet de manipuler des objets par glisser-déposer et le dessin ainsi obtenu est ensuite traduit en une représentation textuelle orientée objet et événementielle. Le paradigme visuel a été introduit à la fin des années 1980 par Alan Kay dans le langage Smalltalk, dans le but de faciliter la programmation des interfaces graphiques.Alors qu'un programme interactif pose une question et effectue des actions en fonction de la réponse, en style événementiel le programme n'attend rien et est exécuté lorsque quelque chose s'est passé. Par exemple, l'utilisateur déplace la souris ou presse sur un bouton. Dans ce paradigme, la programmation consiste à décrire les actions à prendre en réponse aux événements. Et une action peut en cascade déclencher une autre action correspondant à un autre événement. Le paradigme événementiel a été introduit par le langage Simula dans les années 1970. Il est devenu populaire à la suite de l'avènement des interfaces graphiques et des applications web.Avec l’avènement de l'Internet dans les années 1990, les données, les images ainsi que le code s'échangent entre ordinateurs. Si un résultat est demandé à un ordinateur, celui-ci peut exécuter le programme nécessaire et envoyer le résultat. Il peut également envoyer le code nécessaire à l'ordinateur client pour qu'il calcule le résultat lui-même. Le programme est rarement traduit en langage machine, mais plutôt interprété ou traduit en une forme intermédiaire, le bytecode, qui sera exécuté par une machine virtuelle, ou traduit en langage machine au moment de l'exécution (just-in-time). Java, PHP et Javascript sont des langages de programmation basée web.L'utilisation d'un langage est rendue possible par un traducteur automatique. Un programme qui prend un texte écrit dans ce langage pour en faire quelque chose, en général soit :Un programme qui traduit le texte dans un langage qui permettra son exécution, tel le langage machine, le bytecode ou le langage assembleur.Un programme qui exécute les instructions demandées. Il joue le même rôle qu'une machine qui reconnaîtrait ce langage.Chaque appareil informatique a un ensemble d'instructions qui peuvent être utilisées pour effectuer des opérations. Les instructions permettent d'effectuer des calculs arithmétiques ou logiques, déplacer ou copier des données, ou bifurquer vers l'exécution d'autres instructions. Ces instructions sont enregistrées sous forme de séquences de bits, où chaque séquence correspond au code de l'opération à effectuer et aux opérandes, c'est-à-dire aux données concernées ; c'est le langage machine.La traduction s'effectue en plusieurs étapes. En premier lieu, le traducteur effectue une analyse lexicale où il identifie les éléments du langage utilisés dans le programme. Dans l'étape suivante, l'analyse syntaxique, le traducteur construit un diagramme en arbre qui reflète la manière dont les éléments du langage ont été combinés dans le programme, pour former des instructions. Puis, lors de l'analyse sémantique, le traducteur détermine s'il est possible de réaliser l'opération et les instructions qui seront nécessaires dans le langage cible.Dans le langage de programmation assembleur, des mots aide-mémoire (mnémonique) sont utilisés pour référer aux instructions de la machine. Les instructions diffèrent en fonction des constructeurs et il en va de même pour les mnémoniques. Un programme assembleur traduit chaque mnémonique en la séquence de bits correspondante.Les langages de programmation fonctionnent souvent à l'aide d'un runtime.Un runtime (traduction : exécuteur) est un ensemble de bibliothèques logicielles qui mettent en œuvre le langage de programmation, permettant d'effectuer des opérations simples, telles que copier des données, mais aussi des opérations beaucoup plus complexes.Lors de la traduction d'un programme vers le langage machine, les opérations simples sont traduites en les instructions correspondantes en langage machine tandis que les opérations complexes sont traduites en des utilisations des fonctions du runtime. Dans certains langages de programmation, la totalité des instructions sont traduites en des utilisations du runtime qui sert alors d'intermédiaire entre les possibilités offertes par la plateforme informatique et les constructions propres au langage de programmation.Chaque langage de programmation a une manière conventionnelle de traduire l'exécution de procédures ou de fonctions, de placer les variables en mémoire et de transmettre des paramètres. Ces conventions sont appliquées par le runtime. Les runtime servent également à mettre en œuvre certaines fonctionnalités avancées des langages de programmation telles que le ramasse-miettes, ou la réflexion.Les langages de programmation sont couramment auto-implémentés, c'est-à-dire que le compilateur pour ce langage de programmation est mis en œuvre dans le langage lui-même. Exemple : un compilateur pour le langage Pascal peut être écrit en langage Pascal.Les fonctionnalités avancées telles que le ramasse-miettes (anglais garbage collector), la manipulation des exceptions, des événements ou des threads, ainsi que la liaison tardive et la réflexion sont mises en œuvre par les runtime des langages de programmation.Un mécanisme qui supprime les variables inutilisées et libère l'espace mémoire qui leur avait été réservé.Un fait inattendu, souvent accidentel, entraîne l'échec du déroulement normal du programme, et ce fait exceptionnel doit être pris en charge par le programme avant de pouvoir continuer. Certains langages de programmation permettent de provoquer délibérément l'arrêt du déroulement normal du programme.Une procédure qui va être exécutée lorsqu'une condition particulière est rencontrée. Les événements sont notamment utilisés pour mettre en œuvre les interfaces graphiques.Une suite d'instructions en train d'être exécutée. Les langages de programmation qui manipulent les threads permettent d'effectuer plusieurs tâches simultanément. Cette possibilité d'exécution simultanée, offerte par les systèmes d'exploitation, est également offerte en allégé par les runtime des langages de programmation.Le procédé de liaison (anglais late binding ou dynamic binding) consiste à associer chaque identifiant d'un programme avec l'emplacement de mémoire concerné. Cette opération peut être effectuée lors de la traduction du programme, au cours de l'exécution du programme ou juste avant, elle est dite tardive lorsque l'opération de liaison est effectuée très tard, juste avant que l'emplacement concerné ne soit utilisé.La possibilité pour un programme d'obtenir des informations concernant ses propres caractéristiques. Des instructions du langage de programmation permettent à un programme d'obtenir des informations sur lui-même et de les manipuler comme des données.Une structure permettant de manipuler des traits impératifs dans des langages fonctionnels purs.Bien que la notion de programme apparaisse progressivement au cours de la deuxième moitié du XIXe siècle, les premiers langages de programmation n'apparaissent qu'autour de 1950. Chacun pouvant créer son propre langage, il est impossible de déterminer le nombre total de langages existant à l'heure actuelle.On peut aussi classer les langages de programmation en fonction de leur utilisation car beaucoup de langages sont spécialisés à une application ou à un domaine particulier.Ce type de langage est utilisé pour une plus grande interaction entre un client et un serveur.Du côté du serveur web, cela permet de produire des pages dont le contenu est généré à chaque affichage. Ces langages sont par ailleurs souvent couplés avec un langage pour communiquer avec des bases de données (exemples : PHP, LiveCode).Côté client (en général le navigateur web), ces langages offrent la possibilité de réagir à certaines actions de l'utilisateur sans avoir à questionner le serveur. Par exemple, le JavaScript d'une page Web peut réagir aux saisies de l'utilisateur dans un formulaire (et vérifier le format des données).Certains langages permettent de développer à la fois les aspects client et serveur. C'est le cas d'Ocsigen, de Hop, de Dart ou bien encore du Server-Side JavaScript.On désigne parfois par langage de programmation théorique les systèmes formels utilisés pour décrire de façon théorique le fonctionnement des ordinateurs. Ils ne servent pas à développer des applications mais à représenter des modèles et démontrer certaines de leurs propriétés.On peut citer la machine de Turing et le ?-calcul de Church, qui datent tous les deux des années 1930, et donc antérieurs à l'invention de l'ordinateur. Le ?-calcul a par la suite servi de base théorique à la famille des langages de programmation fonctionnelle. Dans les années 1980, Robin Milner a mis au point le ?-calcul pour modéliser les systèmes concurrents.Les langages exotiques ont pour but de créer des grammaires complètes et fonctionnelles mais dans un paradigme éloigné des conventions. Beaucoup sont d'ailleurs considérés comme des blagues.Ces langages sont généralement difficiles à mettre en pratique et donc rarement utilisés.ABEL, langage pour la programmation électronique des PLDCDuce, langage fonctionnel d'ordre supérieur pour la manipulation de documents au format XML.Forme de Backus-Naur (BNF), formalisation des langages de programmationPROMELA, langage de spécification de systèmes asynchronesVRML, description de scènes en trois dimensions Langages synchrones Langages de programmation synchrones pour les systèmes réactifs : Esterel, Lustre. Langages à vocation pédagogique Les pseudo-codes ont généralement un but uniquement pédagogique.Logo est un langage fonctionnel simple à apprendre.Dans les années 1990, le langage BASIC était souvent conseillé pour débuter. Il avait cependant la réputation de favoriser la prise de mauvaises habitudes de programmation.Le Processing est un langage simplifié qui s'appuie sur Java. Il permet un développement d'applications fenêtrées sur tout type d'ordinateur équipé de Java.L'Arduino est un langage simplifié s'appuyant sur C/C++. Il permet un développement simple de projets électroniques à partir de carte Arduino (AVR).L'ArduinoEDU est un langage encore plus simple, en français, pour les grands débutants s'appuyant sur le langage C/C++/Arduino. Il permet un développement très simple de projets électroniques à partir de cartes Arduino (AVR).Flowgorithm est un outil de création et modification graphique de programmes informatiques sous forme d'Algorigramme. Langages pour l'électronique numérique Verilog, VHDL : langages de description matérielle, permettant de synthétiser de l'électronique numérique (descriptions de portes logiques) et d'en simuler le fonctionnementSystemC, langage de description matérielle de plus haut niveau que les précédents et permettant une simulation plus rapide Langages pour la statistique R, SAS et xLispStat sont à la fois un langage de statistiques et un logiciel. Langages de programmation de Commande Numérique (C.N.) Une machine-outil automatisée, ou Commande Numérique (C.N.), a besoin d'un langage de programmation pour réaliser les opérations de tournage ou de fraisage… Langages de programmation des automates programmables industriels (API) Sequential function chart, langage graphique, dérivé du grafcet (NB : le grafcet définit les spécifications de façon graphique).Langage Ladder, langage graphique. Langages de programmation audio Nyquist est un langage de synthèse et d'analyse sonore. Pure Data est un logiciel de création musicale graphique qui repose sur un langage de programmation procédural.Six chercheurs de trois universités portugaises ont mené une étude comparative de 27 langages de programmation, intitulée « Energy Efficiency Across Programming Languages ». Ils ont étudié la consommation d'énergie, le temps d'exécution et l'utilisation de la mémoire. Pour obtenir un ensemble de programmes comparables, les chercheurs ont exploré le Computer Language Benchmarks Game (CLBG).Le tableau obtenu présente les résultats globaux (en moyenne) pour la consommation d'énergie (Energy), le temps d'exécution (Time) et la consommation maximale de la mémoire (Mb) normalisés par rapport au langage le plus efficace pour le critère mesuré.Les cinq meilleurs langages sont :C : 1,00Rust : 1,03C++ : 1,34Ada : 1,70Java : 1,98C : 1,00Rust : 1,04C++ : 1,56Ada : 1,85Java : 1,89Pascal : 1,00Go : 1,05C : 1,17Fortran : 1,24C++ : 1,34La popularité de chaque langage est difficilement quantifiable ; néanmoins, il existe l'index TIOBE, calculé mensuellement, qui se base sur le nombre de formations/cours destinée aux ingénieurs et le nombre de revendeurs/free-lance spécialisés dans un langage de programmation. C'est une information parcellaire mais qui peut donner un ordre d'idée sur les tendances en matière de préférence des programmeurs.Chaque appareil informatique a un ensemble d'instructions qui peuvent être utilisées pour effectuer des opérations. Les instructions permettent d'effectuer des calculs arithmétiques ou logiques, déplacer ou copier des données, ou bifurquer vers l'exécution d'autres instructions. Ces instructions sont enregistrées sous forme de séquences de bits, où chaque séquence correspond au code de l'opération à effectuer et aux opérandes, c'est-à-dire aux données concernées ; c'est le langage machine.La traduction s'effectue en plusieurs étapes. En premier lieu, le traducteur effectue une analyse lexicale où il identifie les éléments du langage utilisés dans le programme. Dans l'étape suivante, l'analyse syntaxique, le traducteur construit un diagramme en arbre qui reflète la manière dont les éléments du langage ont été combinés dans le programme, pour former des instructions. Puis, lors de l'analyse sémantique, le traducteur détermine s'il est possible de réaliser l'opération et les instructions qui seront nécessaires dans le langage cible.Dans le langage de programmation assembleur, des mots aide-mémoire (mnémonique) sont utilisés pour référer aux instructions de la machine. Les instructions diffèrent en fonction des constructeurs et il en va de même pour les mnémoniques. Un programme assembleur traduit chaque mnémonique en la séquence de bits correspondante.Les langages de programmation fonctionnent souvent à l'aide d'un runtime.Liste de langages de programmationIdentificateurTuring-complet(en) Chronologie des langages de programmation Portail de la programmation informatique"
Informatique;
"hes de """;
Informatique;"La programmation, appelée aussi codage dans le domaine informatique, désigne l'ensemble des activités qui permettent l'écriture des programmes informatiques. C'est une étape importante du développement de logiciels (voire de matériel).L'écriture d'un programme se fait dans un langage de programmation. Un logiciel est un ensemble de programmes (qui peuvent être écrits dans des langages de programmation différents) destiné à la réalisation de certaines tâches par un (ou plusieurs) utilisateurs du logiciel.La programmation représente donc ici la rédaction du code source d'un logiciel. On utilise plutôt le terme développement pour dénoter l'ensemble des activités liées à la création d'un logiciel et des programmes qui le composent. Cela inclut la spécification du logiciel, sa conception, puis son implémentation proprement dite au sens de l'écriture des programmes dans un langage de programmation bien défini, ainsi que la vérification de sa correction, etc.La première machine programmable (c’est-à-dire machine dont les possibilités changent quand on modifie son programme) est probablement le métier à tisser de Jacquard, qui a été réalisé en 1801. La machine utilisait une suite de cartons perforés. Les trous indiquaient le motif que le métier suivait pour réaliser un tissage ; avec des cartes différentes le métier produisait des tissages différents. Cette innovation a été ensuite améliorée par Herman Hollerith d'IBM pour le développement de la fameuse carte perforée d'IBM.En 1936, la publication de l'article fondateur de la science informatique On Computable Numbers with an Application to the Entscheidungsproblem par Alan Turing allait donner le coup d'envoi à la création de l'ordinateur programmable. Il y présente sa machine de Turing, le premier calculateur universel programmable, et invente les concepts et les termes de programmation et de programme.Les premiers programmes d'ordinateur étaient réalisés avec un fer à souder et un grand nombre de tubes à vide (plus tard, des transistors). Les programmes devenant plus complexes, cela est devenu presque impossible, parce qu'une seule erreur rendait le programme entier inutilisable. Avec les progrès des supports de données, il devint possible de charger le programme à partir de cartes perforées, contenant la liste des instructions en code binaire spécifique à un type d'ordinateur particulier. La puissance des ordinateurs augmentant, on les utilisa pour faire les programmes, les programmeurs préférant naturellement rédiger du texte plutôt que des suites de 0 et de 1, à charge pour l'ordinateur d'en faire la traduction lui-même.Avec le temps, de nouveaux langages de programmation sont apparus, faisant de plus en plus abstraction du matériel sur lequel devraient tourner les programmes. Ceci apporte plusieurs facteurs de gain : ces langages sont plus faciles à apprendre, un programmeur peut produire du code plus rapidement, et les programmes produits peuvent tourner sur différents types de machines.JavaScriptPHPRubyJavaSwiftC#, C ou C++PythonJuliaScalaRL'immense majorité des programmes qui s'exécutent sur nos ordinateurs, téléphones et autres outils électroniques sont écrits dans des langages de programmation dits impératifs : les lignes du programme sont exécutées les unes après les autres. Chaque ligne du programme effectue soit une opération simple, soit exécute une fonction qui est elle-même une suite d'opérations simples.Le programme « Hello World! » est par tradition le premier programme écrit par tout programmeur, censé illustrer la syntaxe du langage de programmation. Le programme a pour unique fonction d'afficher le texte ""Hello World!"" dans la console ou dans une fenêtre de l'interface graphique.Voici une version d'un programme « Hello World! »:Le programme suivant écrit en langage simplifié et avec des commentaires, demande simplement à l'utilisateur d'entrer au clavier deux nombres entiers, et affiche leur quotient.Dans ce programme, les principales fonctionnalités de la programmation impérative sont utilisées : des variables de type nombre entier, nombre à virgule, chaîne de caractère, fonction calculant un résultat à partir de paramètres, fonction effectuant une tâche telle qu'afficher un message à l'écran, instruction if permettant d'exécuter un code ou un autre en fonction de la valeur de telle ou telle variable.Dans un programme informatique typique, on trouvera suivant les langages des boucles while ou for qui permettent d'exécuter un morceau de code en boucle ou simplement un certain nombre de fois, des new pour l'allocation dynamique de données (par exemple des tableaux), et très souvent des éléments de programmation objet permettant de structurer différemment le code et de créer des types de données personnalisés, ou encore des exceptions pour gérer certains cas d'erreurs plus facilement.On remarque que pour effectuer une tâche très simple, le code informatique peut être très laborieux, et encore ici on ne traite pas les erreurs (si l'utilisateur tape un mot au lieu d'un nombre), et l'affichage est minimaliste. C'est pourquoi les langages de programmation n'ont jamais cessé d'évoluer, dans le but d'aider le programmeur qui souhaite réaliser des programmes rapides à s'exécuter, sans dysfonctionnements, et surtout simples à écrire, du moins le plus possible.La phase de conception définit le but du programme. Si on fait une rapide analyse fonctionnelle d'un programme, on détermine essentiellement les données qu'il va traiter (données d'entrée), la méthode employée (appelée l'algorithme), et le résultat (données de sortie). Les données d'entrée et de sortie peuvent être de nature très diverse. On peut décrire la méthode employée pour accomplir le but d'un programme à l'aide d'un algorithme. La programmation procédurale et fonctionnelle est basée sur l'algorithmique. On retrouve en général les mêmes fonctionnalités de base. Programmation impérative ""Si""Si prédicatAlors faire ceciSinon faire cela""Tant que""Tant que prédicatFaire ...""Pour""Pour variable allant de borne inférieure à borne supérieureFaire ...""Pour"" (variante)Pour variable dans conteneurfaire ...Une fois l'algorithme défini, l'étape suivante est de coder le programme. Le codage dépend de l'architecture sur laquelle va s'exécuter le programme, de compromis temps-mémoire, et d'autres contraintes. Ces contraintes vont déterminer quel langage de programmation utiliser pour « convertir » l'algorithme en code source.Le code source n'est (presque) jamais utilisable tel quel. Il est généralement écrit dans un langage ""de haut niveau"", compréhensible pour l'homme, mais pas pour la machine. Compilation Certains langages sont ce qu'on appelle des langages compilés. En toute généralité, la compilation est l'opération qui consiste à transformer un langage source en un langage cible. Dans le cas d'un programme, le compilateur va transformer tout le texte représentant le code source du programme, en code compréhensible pour la machine, appelé code machine.Dans le cas de langages dits compilés, ce qui est exécuté est le résultat de la compilation. Une fois effectuée, l'exécutable obtenu peut être utilisé sans le code source.Il faut également noter que le résultat de la compilation n'est pas forcément du code machine correspondant à la machine réelle, mais peut être du code compris par une machine virtuelle (c'est-à-dire un programme simulant une machine), auquel cas on parlera de bytecode. C'est par exemple le cas en Java. L'avantage est que, de cette façon, un programme peut fonctionner sur n'importe quelle machine réelle, du moment que la machine virtuelle existe pour celle-ci.Dans le cas d'une requête SQL, la requête est compilée en une expression utilisant les opérateurs de l'algèbre relationnelle. C'est cette expression qui est évaluée par le système de gestion de bases de données. Interprétation D'autres langages ne nécessitent pas de phase spéciale de compilation. La méthode employée pour exécuter le programme est alors différente. La phase de compilation est la plupart du temps incluse dans celle d’exécution. On dit de ce programme qu'il interprète le code source. Par exemple, Python ou Perl sont des langages interprétés. Avantages, inconvénients Les avantages généralement retenus pour l'utilisation de langages « compilés », est qu'ils sont plus rapides à l'exécution que des langages interprétés, car l'interprète doit être lancé à chaque exécution du programme, ce qui mobilise systématiquement les ressources.Traditionnellement, les langages interprétés offrent en revanche une certaine portabilité (la capacité à utiliser le code source sur différentes plates-formes), ainsi qu'une facilité pour l'écriture du code. En effet, il n'est pas nécessaire de passer par la phase de compilation pour tester le code source. Il n'est pas non plus nécessaire de disposer d'un autre programme (debugger) afin d’ôter les bugs du programme, c'est l’interpréteur qui permet d'afficher directement le contenu des variables du programme. Appellation impropre Il faut noter qu'on parle abusivement de langages compilés ou interprétés. En effet, le caractère compilé ou interprété ne dépend pas du langage, qui n'est finalement qu'une grammaire et une certaine sémantique. D'ailleurs, certains langages peuvent être utilisés interprétés ou compilés. Par exemple, il est très courant d'utiliser Ruby avec un interprète, mais il existe également des compilateurs pour ce langage. On notera toutefois, qu'il peut être important de préciser comment le code source est exécuté. En effet, rares sont les organismes qui proposent à la fois un compilateur et un interpréteur, les résultats du programme peuvent différer à l'exécution, même si la norme du langage est clairement définie.Néanmoins, l'usage qu'on fait des langages est généralement fixé.C'est l'une des étapes les plus importantes de la création d'un programme. En principe, tout programmeur se doit de vérifier chaque partie d'un programme, de le tester. Il existe différents types de test. On peut citer en particulier :Test unitaireTest d'intégrationTest de performanceIl convient de noter qu'il est parfois possible de vérifier un programme informatique, c'est-à-dire prouver, de manière plus ou moins automatique, qu'il assure certaines propriétés.AlgorithmiqueGestion de versionsOptimisation du codeProgrammation systèmeRefactoringTest d'intégrationTest unitaireUn paradigme est un style fondamental de programmation, définissant la manière dont les programmes doivent être formulés.Un paradigme est la façon dont sont traitées les solutions aux problèmes et un style fondamental de programmation, définissant la manière dont les programmes doivent être formulés. Chaque paradigme amène sa philosophie de la programmation ; une fois qu'une solution a été imaginée par un programmeur selon un certain paradigme, un langage de programmation qui suit ce paradigme permettra de l'exprimer.Le paradigme impératif est le plus répandu, les opérations sont une suite d’instructions exécutées par l'ordinateur pour modifier l'état du programme. Programmation procédurale La programmation procédurale est un sous-ensemble de la programmation impérative. Elle introduit la notion de routine ou fonction qui est une sorte de factorisation de code, chaque procédure peut être appelée à n’importe quelle étape du programme. Ce paradigme permet aussi de supprimer les instructions goto,,Ce paradigme est très répandu, il est présent dans des langages comme le C, le COBOL ou le FORTRAN. Programmation structurée Apparue dans les années 70, la programmation structurée est un sous-ensemble de la programmation impérative. Elle est née avec les travaux de Nicklaus Wirth pour son Algol W et l'article fondateur de Dijkstra dans Communications of the ACM, visant à supprimer l’instruction goto.Tous les langages procéduraux peuvent faire de la programmation structurée, mais certains comme le FORTRAN s'y prêtent très mal.En programmation déclarative, le programme est indépendant de l’état de la machine, il s’affranchit donc de tout effet de bord et un appel à une même fonction produira toujours le même résultat.Le programme s’écrit non pas comme une suite d’instruction pour résoudre un problème mais (contrairement à la programmation impérative) comme la solution au problème. Programmation fonctionnelle La programmation fonctionnelle se base sur plusieurs principes comme : l’immutabilité, les fonctions pures (qui ne dépendent pas de l’état de la machine) et les lambda-calcul.Aujourd’hui, nombreux sont les langages qui offrent une approche fonctionnelle au programmeur. Certains comme LISP ou Scala sont intrinsèquement fonctionnels. D’autres comme JavaScript, Java ou PHP ont ajouté cette possibilité par la suite. Programmation logique La programmation logique consiste à exprimer les problèmes et les algorithmes sous forme de prédicats à l’aide d'une base de faits, d'une base de règles et d'un moteur d'inférence.La programmation orientée objet (abrégé POO) consiste en la définition et l'interaction de briques logicielles appelées objets ; ces objets représentes un concept, une idée. Chaque objet contient des attributs et des méthodes en rapport avec un sujet. Programmation orientée prototype La programmation orientée prototype est un sous ensemble de la programmation orientée objet. Dans ce paradigme, chaque objet est créé à partir d’un prototype qui est lui-même un objet. Le prototype a donc une existence physique en mémoire et est mutable contrairement aux classes.Le JavaScript, le Lua ou le Self sont des exemples de langages utilisant ce paradigme. Programmation orientée classe La programmation orientée classe est basée sur la notion de classes. Une classe est statique, c’est la représentation abstraite de l’objet, c’est à ce niveau que se passe l’héritage. Tout objet est donc l’instance d’une classe.Les langages à classes peuvent être sous forme fonctionnelle (CLOS) comme sous forme impérative (C++, Java), voir les deux (Python, OCaml).Programmation concurrenteProgrammation orientée aspectProgrammation orientée composantProgrammation par contratProgrammation par contraintesProgrammation par intentionProgrammation réactiveÉconomie d'énergie d'un programme informatiqueParadigme (programmation)Style de programmationChronologie des langages de programmationListe des langages de programmationProgrammation webForme de Backus-Naur (BNF), une grammaire de description de langageGoogle Code Jam, un concours international annuel de programmation très prisé des mordus d'algorithmique et de programmationKata (programmation)Éditeur de texteCode créatif Portail de la programmation informatique"
Informatique;"Un programme informatique est un ensemble d'instructions et d’opérations destinées à être exécutées par un ordinateur.Un programme source est un code écrit par un informaticien dans un langage de programmation. Il peut être compilé vers une forme binaire ou directement interprété.Un programme binaire décrit les instructions à exécuter par un microprocesseur sous forme numérique. Ces instructions définissent un langage machine,.Un programme fait généralement partie d'un logiciel que l'on peut définir comme un ensemble de composants numériques destiné à fournir un service informatique. Un logiciel peut comporter plusieurs programmes. On en retrouve ainsi dans les appareils informatiques (ordinateur, console de jeux, guichet automatique bancaire…), dans des pièces de matériel informatique, ainsi que dans de nombreux dispositifs électroniques (imprimante, modem, GPS, téléphone mobile, machine à laver, appareil photo numérique, décodeur TV numérique, injection électronique, pilote automatique…).Les programmes informatiques sont concernés par le droit d'auteur et font l'objet d'une législation proche des œuvres artistiques.En 1842, la comtesse Ada Lovelace crée des diagrammes pour la machine analytique de Charles Babbage. Ces diagrammes sont considérés aujourd'hui comme étant les premiers programmes informatiques au monde. Toutefois, cette théorie fait l'objet de controverses car Babbage a également écrit lui-même ses premiers programmes pour sa machine analytique, bien que la majorité n'ait jamais été publiée. Par exemple, Bromley note des exemples de programmes préparés par Babbage entre 1837 et 1840 : toutes ses notes sont antérieures à ceux écrits par Lovelace. Cependant, le concept de programmation et de programme enregistré est d'abord formulé de manière théorique en 1936 par Alan Turing.Dans les années 1940, les premiers ordinateurs, comme le Z3 ou le Mark I, sont créés. Les programmes informatiques étaient alors conçus par des analystes, rédigés par des programmeurs et saisis par des opératrices sur des bandes type télex ou des cartes en carton perforé. Exécuter un programme consistait à entrer la bande ou la pile de cartes correspondante dans un lecteur électro-mécanique.Le premier système d'exploitation a été développé en 1954. La même année sont apparus les premiers assembleurs et le premier compilateur pour le langage Fortran.L'enseignement de la programmation était d'abord organisé chez les constructeurs d'ordinateurs et dans les premières universités où ces machines sont installées – dès le début des années 1950 en Angleterre et aux États-Unis, puis au milieu de la même décennie en Europe continentale et au Japon. Ce sont des cours techniques, mais la complexification croissante du sujet (compilateurs, systèmes) entraînera progressivement la constitution d'une science nouvelle.L'avènement de la programmation structurée vers 1970 a grandement simplifié le travail des programmeurs et permis la création de programmes traitant des tâches plus nombreuses et plus complexes. Il en va de même avec l'avènement de la programmation orientée objet entre 1980 et 1990. Conformément à la phrase d'Edsger Dijkstra : « Les progrès ne seront possibles que si nous pouvons réfléchir sur les programmes sans les imaginer comme des morceaux de code exécutable ». De nouveaux langages de programmation ou de métaprogrammation sont régulièrement créés dans le but de simplifier et d’accélérer les possibilités offertes par programmation.Enfin, la miniaturisation des ordinateurs et la généralisation des interfaces graphiques ont largement contribué à la démocratisation de l'utilisation de l'ordinateur, au point que dans les années 2010, la généralisation des smartphones permet aux utilisateurs d’exécuter des programmes informatiques en permanence.La programmation consiste, partant d'une idée, à effectuer un travail de réflexion qui aboutit à la rédaction d'algorithmes dans un langage de programmation. Les langages de programmation ont été créés dans l'optique de faciliter le travail du programmeur en raccourcissant le chemin qui va de l'idée au code source.Les programmes sont créés par des programmeurs ou des ingénieurs logiciels. Les programmeurs travaillent principalement sur l'écriture de programmes tandis que les ingénieurs logiciels travaillent à toutes les étapes de la création du programme. Ils appliquent une démarche formelle et rigoureuse basée sur le génie industriel et les techniques de management.Avant de commencer à écrire un programme destiné à résoudre un problème, le programmeur doit déterminer les caractéristiques du problème à résoudre. Ceci se fait en plusieurs étapes indépendantes du langage de programmation utilisé. La technique courante est celle d'un cycle de développement, qui comporte des étapes de définition, de conception, d'écriture, de test, d'installation et de maintenance.Le problème est tout d'abord examiné en détail en vue de connaître l'étendue du programme à créer. L'étape suivante consiste à choisir des solutions et des algorithmes, puis décrire leur logique sous forme de diagrammes, en vue de clarifier le fonctionnement du programme et faciliter son écriture.Une fois le programme écrit, celui-ci subit une suite de tests. Les résultats produits par le programme sont comparés avec des résultats obtenus manuellement. De nombreux tests sont nécessaires et les mêmes tests sont exécutés plusieurs fois. Le programme est ensuite installé dans la machine de l'utilisateur final qui fera ses premières observations, puis sera modifié en fonction des commentaires faits par l'utilisateur et des inconvénients signalés.Les besoins des utilisateurs et des systèmes informatiques varient continuellement, et le programme est régulièrement reconstruit et modifié en vue d'être adapté aux besoins. De nouvelles fonctions y sont ajoutées et des erreurs qui n'avaient pas été décelées auparavant sont corrigées.Le but du cycle de développement est de réduire les coûts de fabrication tout en augmentant la qualité du programme. Les qualités recherchées sont l'efficacité, la flexibilité, la fiabilité, la portabilité et la robustesse. Il doit également être convivial et facile à modifier.Il est attendu d'un programme qu'il demande peu d'effort de programmation, que les instructions demandent peu de temps et nécessitent peu de mémoire, qu'il peut être utilisé pour de nombreux usages et donne les résultats attendus quels que soient les changements — permanents ou temporaires — du système informatique.Il est également attendu qu'il peut être facilement transféré sur un modèle d'ordinateur différent de celui pour lequel il est construit, qu'il produit des résultats probants y compris lorsque les informations entrées sont incorrectes, qu'il peut être facilement compris par un usager novice et que le code source peut être facilement modifié par la suite.Un langage de programmation est une notation utilisée pour exprimer des algorithmes et écrire des programmes. Un algorithme est un procédé pour obtenir un résultat par une succession de calculs, décrits sous forme de pictogrammes et de termes simples dans une langue naturelle. Jusqu'en 1950, les programmeurs exprimaient les programmes dans des langages machines ou assembleur, des langages peu lisibles pour des êtres humains et où chaque instruction fait peu de choses, ce qui rendait le travail pénible et le résultat sujet à de nombreuses erreurs. Dès 1950, les programmes ont été décrits dans des langages différents dédiés à l'humain et plus à la machine — des langages de programmation –, ce qui rendait les opérations plus simples à exprimer. Le programme était ensuite traduit automatiquement sous une forme qui permet d'être exécuté par l'ordinateur.Sur demande, l'ordinateur exécutera les instructions du programme. Bien qu'il exécute toujours exactement ce qui est instruit et ne se trompe jamais, il peut arriver que les instructions qu'il exécute soient erronées à la suite d'une erreur humaine lors de l'écriture du programme. Les langages de programmation visent à diminuer le nombre de ces bugs ; ceux-ci sont cependant inévitables dans des programmes de plusieurs milliers de lignes. Un programme de traitement de texte peut être fait de plus de 750 000 lignes de code et un système d'exploitation peut être fait de plus de 50 millions de lignes. En moyenne un programmeur prépare, écrit, teste et documente environ 20 lignes de programme par jour, et la création de grands programmes est le fait d'équipes et peut nécessiter plusieurs mois, voire plusieurs années.La programmation est un sujet central en informatique. Les instructions qu'un ordinateur devra exécuter doivent pouvoir être exprimées de manière précise et non ambiguë. Pour ce faire, les langages de programmation combinent la lisibilité de l'anglais avec l'exactitude des mathématiques. Les programmes sont créés par des programmeurs ou des ingénieurs logiciels. La création d'un programme comprend une série d'activités telles que la conception, l'écriture, le test et la documentation. En vue d'obtenir un programme de meilleure qualité, le travail de programmation se fait selon une démarche systématique et planifiée,.Un langage de programmation est un vocabulaire et un ensemble de règles d'écriture utilisées pour instruire un ordinateur d'effectuer certaines tâches. La plupart des langages de programmation sont dits de haut niveau, c'est-à-dire que leur notation s'inspire des langues naturelles (généralement l'anglais).Le processeur est le composant électronique qui exécute les instructions. Chaque processeur est conçu pour exécuter certaines instructions, dites instructions machine. La palette d'instructions disponibles sur un processeur forme le langage machine. Par exemple, le processeur Intel 80486 a une palette de 342 instructions.Le langage d'assemblage est une représentation textuelle des instructions machine, un langage de bas niveau, qui permet d'exprimer les instructions machine sous une forme symbolique plus facile à manipuler, où il y a une correspondance 1-1 entre les instructions machines et les instructions en langage d'assemblage.Les langages de programmation de haut niveau permettent d'exprimer des instructions de manière synthétique, en faisant abstraction du langage machine. Par rapport au langage d'assemblage, ils permettent d'exprimer des structures, permettent d'écrire des programmes plus rapidement, avec moins d'instructions. Les programmes écrits dans des langages de haut niveau sont plus simples à modifier et portables, et peuvent fonctionner avec différents processeurs. Cependant un programme exprimé en langage de haut niveau, puis compilé est moins efficace et comporte plus d'instruction que s'il avait été exprimé en langage d'assemblage.Entre 1950 et 2000, plus de 50 langages de programmation sont apparus. Chacun apportait un lot de nouveaux concepts, de raffinements et d'innovations. Jusque dans les années 1950, l'utilisation des langages de programmation était semblable à l'écriture d'instructions machines. L'innovation des années 1960 a été de permettre une notation proche des mathématiques pour écrire des instructions de calcul. Les innovations des années 1970 ont permis l'organisation et l'agrégation des informations manipulées par les programmes — voir structure de données et structure de contrôle. Puis l'arrivée de la notion d'objet a influencé l'évolution des langages de programmation postérieurs à 1980.Ci-dessous, le programme Hello world exprimé en langage de programmation Java :Le même programme, exprimé dans le langage d'assemblage des processeurs x86 :Un programme est typiquement composé d'un ensemble de procédures et de fonctions. Une procédure est une suite d'instructions destinées à réaliser une opération ; par exemple, trier une liste. Une fonction est une suite d'instructions destinées à produire un résultat ; par exemple, un calcul.Un bug est un défaut de construction dans un programme. Les instructions que l'appareil informatique exécute ne correspondent pas à ce qui est attendu, ce qui provoque des dysfonctionnements et des pannes. La pratique de la programmation informatique nécessite des outils pour traquer ou éviter les bugs, ou vérifier la correction du programme.L'exécution des programmes est basée sur le principe de la machine à programme enregistré de John von Neumann : les instructions de programme sont exécutées par un processeur. Ce composant électronique exécute chaque instruction de programme par une succession d'opérations charger/décoder/exécuter : l'instruction est tout d'abord copiée depuis la mémoire vers le processeur, puis elle est décomposée bit par bit pour déterminer l'opération à effectuer, qui est finalement exécutée. La plupart des opérations sont arithmétiques (addition, soustraction) ou logiques. L'exécution de programmes par le processeur central (anglais CPU) contrôle la totalité des opérations effectuées par l'ordinateur.L'exécution du cycle charger-décoder-exécuter est rythmée par une horloge branchée au processeur.En 2011, la fréquence d'horloge supportée par les processeurs contemporains se compte en mégahertz ou en gigahertz, ce qui correspond à des millions, voire des milliards de cycles par seconde.Les processeurs contemporains peuvent traiter plusieurs instructions simultanément : lorsqu'une instruction est chargée, le processeur charge immédiatement l'instruction suivante, sans attendre que cette instruction soit décodée puis exécutée, et les processeurs peuvent également charger/décoder/exécuter plusieurs instructions en un seul cycle d'horloge. Processus Pour être exécuté, un programme doit être chargé dans la mémoire de la machine. Le chargement d'un programme peut être soit automatique ou programmé lors de l'amorce de l'ordinateur par exemple, soit interactif et être déclenché par un ordre d'exécution explicite de l'utilisateur (une commande explicite, un appui sur une touche, un bouton, une icône…). Suivant la nature de l'action à effectuer, un programme peut être exécuté de manière ponctuelle (impression d'un texte), de manière répétitive (mise à jour de coordonnées GPS) ou de manière (presque) permanente (surveillance de capteurs).Un programme est une suite d'instructions qui spécifie étape par étape, de manière non-ambiguë, des représentations de données et des calculs. Les instructions sont destinées à manipuler les données lors de l'exécution du programme. Le programme lui-même est défini par un (ou des) algorithme(s) ou par une spécification. Un programme décrit de manière exacte les différentes étapes d'un algorithme : ce qu'il y a à faire, quand et avec quelles informations. Selon l'architecture de von Neumann créée en 1945, un programme est chargé dans la mémoire de l'ordinateur, ce qui permet de l'exécuter de manière répétée sans intervention humaine, et surtout d'utiliser la même machine pour exécuter autant de programmes que l'on veut. La mémoire dédiée aux programmes est aussi la mémoire dédiée aux données, ce qui permet de traiter les programmes comme des données comme les autres (par exemple, écrire de nouveaux programmes de la même manière qu'on écrirait un document textuel), puis de les exécuter.Des programmes peuvent être exécutés non seulement par les ordinateurs, mais par les nombreux appareils qui sont basés sur des composants informatiques – par exemple, certains robots ménagers, téléphones, fax, instruments de mesure, récepteur de télévision, ainsi que les pièces de matériel informatique telles que les disques durs, les routeurs, les imprimantes, les consoles de jeux vidéo, les assistants personnels et les automates bancaires. Contrairement aux ordinateurs, ces appareils ne contiennent souvent pas de système d'exploitation, les programmes sont enregistrés dans l'appareil lors de la fabrication et la vitesse d'exécution des programmes est souvent d'importance mineure.Sans contre-indication, les instructions d'un programme sont exécutées une après l'autre, de manière linéaire. Les langages de programmation permettent d'exprimer des alternatives : une suite d'instructions est exécutée uniquement si une condition donnée est remplie, dans le cas contraire une autre suite est exécutée. Les langages de programmation permettent également de faire répéter l'exécution d'une suite d'instructions jusqu'à ce qu'une condition donnée soit remplie.L'exécution se déroule de manière différente suivant si le langage de programmation s'utilise avec un compilateur ou un interpréteur.Un compilateur lit le programme source en entier et le transforme en instructions machines. La transformation peut se faire en plusieurs étapes et nécessiter plusieurs lectures du programme. Une fois traduit, le programme est ensuite enregistré en vue d'être plus tard copié en mémoire et exécuté par le processeur tel quel ;Un interpréteur opère ligne par ligne : lit une ligne de programme source, puis exécute immédiatement les instructions machines correspondantes. L'avantage d'un interpréteur est que les erreurs peuvent être immédiatement corrigées. Le désavantage est que l'exécution du programme est 10 à 100 fois moins rapide que si le programme avait été préalablement traduit et exécuté tel quel.Les ordinateurs modernes démarrent à leur lancement un programme « maître » dit système d'exploitation,. Il permet d'exécuter des sous-programmes qui peuvent alors profiter des fonctionnalités offertes par le système et qui dans certains cas doivent s'adapter à cet environnement. Les systèmes d'exploitation contemporains permettent d'exécuter simultanément plusieurs programmes dans des processus, même avec un seul processeur : un programme planificateur (en anglais : scheduler) du système d'exploitation interrompt régulièrement le programme en cours d'exécution pour donner la main à un autre. La vitesse de rotation donne l'illusion que les programmes sont exécutés en même temps.Un sous-programme du système d'exploitation peut lui-même être un environnement permettant d'exécuter des programmes (avec une interface différente) ; par exemple, une machine virtuelle.En droit, un programme est une œuvre écrite, protégée par le droit d'auteur. Celui-ci s'applique au programme du moment qu'il est enregistré de manière permanente, même s'il n'existe pas d'édition sur papier. Le droit d'auteur protège autant le programme source que le programme binaire.[Parsons Oja 2008] (en) June Jamrich Parsons et Dan Oja, New Perspectives on Computer Concepts, Cengage Learning, 2008, 800 p. (ISBN 978-1-4239-2518-7 et 1-4239-2518-1, lire en ligne). Portail de l’informatique   Portail de la programmation informatique"
Informatique;"Un système de traitement de l'information est un système constitué d'un ensemble de composants (mécaniques, électroniques, chimiques, photoniques ou biologiques) permettant de traiter automatiquement des informations. Il est régi par la théorie de l'information et c'est l'élément central de tout appareil informatique.Les premiers systèmes permettant de traiter automatiquement des informations étaient des appareils mécaniques tel que la pascaline ou encore la machine d'Anticythère qui représentent les tout premiers calculateurs spécifiques à l'inverse de l'ordinateur qui est programmable et universel.Aujourd'hui[Quand ?] il existe une panoplie d'appareils en électronique numérique permettant de traiter automatiquement des informations : commutateur réseau, ordinateur, console de jeu, calculatrice, certaines cartes à puce, distributeur automatique de billets, enregistreur vidéo personnel, GPS, téléphone portable, etc.Avant d'être un appareil fonctionnel, le système de traitement de l'information est d'abord un objet d'étude conceptuelle qui propose une approche systémique à l'informaticien. Il permet d'élargir les champs de recherche en informatique et permet de catégoriser les systèmes de traitement par rapport à leurs capacités plutôt qu'à leurs fonctionnements.Un système de traitement de l'information se caractérise par un minimum de quatre unités :l'unité d'entrée (anglais : input), recueille l'information ;l'unité de stockage (anglais : storage) conserve toute ou une partie de l'information ;l'unité de traitement (anglais : processing), transforme l'état de l'information ;l'unité de sortie (anglais : output) présente le résultat de la modification.L'évolution de l'état d'une information ne peut être observée qu'en accord avec la notion de temps. L'horloge est le système de traitement de l'information le plus élémentaire. Tous les systèmes de traitement de l'information sont jusqu'à présent mus par un système de balancement ou de mouvement oscillatoire dont la source d'entraînement est naturelle (ex. : l'actionnement d'une manivelle) ou artificielle (rotation d'un moteur, alimentation électrique d'un quartz, etc.).L'information est représenté par un  signal de type analogique ou numérique et qui peut être véhiculé sous forme matériel (engrenage, molécule, etc.) ou énergétique (particule élémentaire).La Pascaline est le premier calculateur mécanique. Il a été construit par Blaise Pascal en 1642.La première conceptualisation d'un système de traitement de l'information programmable et universel est appelé machine de Turing. C'est la thèse de Turing qui est aujourd'hui considérée comme l'article fondateur de l'ordinateur.Le premier calculateur électronique à utiliser le système binaire est l'EDVAC. Construit en 1945, il occupait une salle de 45 m2, et pesait près de 8 tonnes.C'est l'invention du transistor en 1947 et celle du circuit intégré en 1958 qui ont permis la miniaturisation électronique des systèmes de traitement de l'information.La première console de jeu, l'Odyssey a été construite en 1973. C'était un système de traitement de l'information qui n'était pas Turing-complet.Le premier type d'informations que les systèmes étaient capables de manipuler étaient des nombres, puis des textes, jusqu'à l'arrivée dans les années 1980 des systèmes multimédia, c'est-à-dire capables de manipuler divers types d'informations: images, sons, vidéos…Exemples d'informations :nombres : prix, poids, volume, température, vitesse, pression…textes : courrier, publications, articles de presse…images : plans, dessins, graphiques, diagrammes, cartes géographiques, photos, images 3D…sons : paroles, chants, bruitages ou musique ,vidéo : prises de vue, clips ou films ;les instructions d'un programme informatique sont aussi des informations.Dans le système de traitement de l'information, les informations circulent sous forme de suite de bits (chiffres en base 2) et le octet groupés dans des fichiers ou des enregistrements (voir : électronique numérique).Un format désigne la manière dont les bits sont disposés à l'intérieur du fichier ou de l'enregistrement pour stocker l'information.Un protocole est un ensemble de règles normalisées qui, lorsqu'elles sont appliquées de manière commune par deux appareils, leur permettent de s'échanger des informations.Les règles établies par un protocole concernent autant le point de vue électronique (tensions, fréquences) que le point de vue informationnel (choix des informations, format) ainsi que le déroulement des opérations de communication (qui initie la communication, comment réagit le correspondant, combien de temps dure la communication…).Les protocoles sont utilisés en informatique et en télécommunication (téléphonie, télévision).Un système de traitement de l'information est composé de quatre unités :l'unité d'entrée (anglais : input), qui permet de faire entrer les informations dans le système ;l'unité de stockage (anglais : storage) qui permet de conserver les informations ;l'unité de traitement (anglais : processor) qui comme son nom l'indique va traiter les informations ;l'unité de sortie' (anglais : output) qui permet de faire sortir les résultats des traitements.Les informations peuvent être introduites par une personne à l'aide d'un clavier et d'une souris, enregistrées à l'aide de différents appareils — microphone, caméra, scanner, appareil photo, ou apportées par des dispositifs de télécommunication (voir: réseau informatique).Les premiers appareils permettant d'introduire des informations étaient des lecteurs de cartes perforées. Un dispositif semblable à celui des pianos mécaniques. Cette technologie datant du XVIIIe siècle a été utilisée en informatique jusque dans les années 1980.La première étape des traitements consiste en la réceptions des informations en provenance des différents appareils.La numérisation est le procédé qui consiste à transformer une information provenant du monde réel en une suite de chiffres qui seront utilisés dans le système de traitement de l'information.Les informations stockées peuvent être des informations qui viennent d'être entrées dans la machine, ou résultats d'un traitement.Les informations sont conservées dans des dispositifs de stockage tels que disque durs, DVD, CD-ROM ou mémoire flash.La possibilité de stocker des informations existe depuis les années 1960, auparavant les informations étaient traitées à mesure qu'elles étaient entrées.Le système de traitement des informations effectue les traitements en suivant scrupuleusement les  instructions d'un programme informatique.Les traitements peuvent consister à :à partir de certaines informations, d'obtenir d'autres informations, par exemple par calcul ;transformer les informations ;stocker les informations dans le système d'informations, en vue d'effectuer des traitements plus tard ;extraire des informations préalablement stockées.Exemples de traitements :tri, classement, recherche ;calculs de comptabilité, statistiques, analyses de physique ou d'économie ;correction orthographique ;reconnaissance de texte ;reconnaissance vocale ;traitement d'images tels que fausse couleur, négatif.Un composant électronique qui effectue des traitements s'appelle un processeur.La sortie est l'étape finale des traitements qui consiste à faire sortir les résultats du système d'informations.Selon leur nature, les informations des résultats peuvent être restituées sur un écran, du papier par une imprimante ou un traceur, des enceintes audio ou tout autre appareil. Les informations peuvent aussi être transportées vers d'autres systèmes par des moyens de télécommunication (voir : réseau informatique).Boîte noireInformatiqueOrdinateurSystème binaireMario Borillo, Informatique pour les sciences de l'homme : limites de la formalisation du raisonnement, Éditions Mardaga, 1984  (ISBN 2-8700-9202-4) Portail de l’informatique"
Informatique;"Un système est un ensemble d'éléments interagissant entre eux selon certains principes ou règles. Par exemple une molécule, le système solaire, une ruche, une société humaine, un parti, une armée etc.Un système est déterminé par :sa frontière, c'est-à-dire le critère d'appartenance au système (déterminant si une entité appartient au système ou fait au contraire partie de son environnement) ;ses interactions avec son environnement ;ses fonctions (qui définissent le comportement des entités faisant partie du système, leur organisation et leurs interactions) ;Certains systèmes peuvent également avoir une mission (ses objectifs et sa raison d'être) ou des ressources, qui peuvent être de natures différentes (humaine, naturelle, matérielle, immatérielle...).Un sous-système (ou module ou composant) est un système faisant partie d'un système de rang supérieur.La notion de « système » dans son acception moderne remonte au XVIIIe siècle. Le Traité des systèmes d'Étienne Condillac en donne en 1749 la définition suivante : « le système est ce qui permet à l'esprit humain de saisir l'enchaînement des phénomènes ». Il pose les bases d'un concept qui aura un développement exceptionnel dans le domaine des sciences exactes, puis dans tous les domaines de la connaissance. Les systèmes n'existent pas dans la nature, mais sont des projections de l'esprit humain pour la modélisation et l'analyse des phénomènes naturels.Le concept de système est à la base de domaines scientifiques comme la cybernétique, qui au concept de « boîte noire », à entrées et sorties uniquement, ajoute la notion de boucle de rétroaction des sorties sur les entrées. Dans les sciences humaines, selon André Comte-Sponville, un système est « un ensemble d'idées que l'on considère dans leur cohérence plutôt que dans leur vérité ». Le caractère universel du concept a conduit des chercheurs à vouloir élaborer une théorie générale des systèmes (L. von Bertalanffy, 1949), un programme très ambitieux, qui est surtout un cadre conceptuel pour les applications concrètes dans les différents domaines scientifiques.En grec ancien, sust?ma signifie « organisation, ensemble », terme dérivé du verbe ????????? sunist?mi (de ??? ?????? sun hist?mi : « établir avec »), qui signifie « mettre en rapport, instituer, établir ».Un système peut être ouvert, fermé, ou isolé selon son degré d’interaction avec son environnement.Un système peut également être :soit de nature conceptuelle, comme :une construction théorique que forme l’esprit sur un sujet (ex. : une idée expliquant un phénomène physique et représentée par un modèle mathématique),un ensemble de propositions, d’axiomes, de principes et de conclusions qui forment un corps de doctrine ou un tout scientifique (ex. en philosophie : le système d’Aristote, ex. en physique : le système newtonien) ;soit de nature concrète, comme :un ensemble d'éléments en interactions (ex. : un écosystème, le système climatique),un ensemble d’éléments qui se coordonnent pour concourir à un résultat (ex. : le système nerveux),un ensemble de composants et de processus organisés ou institutionnalisés pour assurer une fonction (ex. : un système éducatif, un système de production, un système de défense),un ensemble de divers éléments analogues,un appareillage, dispositif, ou machine (automate) assurant une fonction déterminée (ex. : système d’éclairage, système automobile),un réseau, plus ou moins important et autonome, dont les éléments présentent la particularité de répondre en tout ou partie à un même objectif.Les systèmes sont au cœur de deux disciplines. La première, de nature applicative, est l'ingénierie des systèmes, démarche rationnelle pour la conception et l'ingénierie d'un système industriel, qui étudie tout son cycle de vie (exploitation, maintenance, démantèlement). La seconde, de nature plus fondamentale, est la théorie des systèmes, à travers l'étude et la recherche des propriétés générales des systèmes (contrôlabilité, stabilité, équivalence, linéarité, etc.) et le développement de méthodes pour en  décrire certains types.Système astronomique Système dynamique : système cybernétique, en général bouclé et modélisé par des équations différentielles, caractérisé par des variables d'état dont on cherche à prévoir les variations dans le temps.Système biologique : système en biologie ;Système en anatomie (aussi appelé appareil) : ensemble d'organes interagissant au sein d'un organisme dans la réalisation d'une fonction biologique commune (ex. : système digestif, système excréteur, système nerveux, etc.).Système cristallin : ensemble de formes géométriques types, caractérisées par leurs propriétés de symétrie fractale ou non, que peut prendre un cristal (système cubique).Écosystème : ensemble formé par une communauté d'êtres vivants et son environnement biologique, géologique, édaphique, hydrologique, climatique, etc.Système : terme utilisé dans la rhétorique antisystème.Système de santé : ensemble de tout ce qui contribue à promouvoir ou à protéger la santé.Système économique : mode d’organisation et de fonctionnement de l'activité économique. Un Secteur (ou filière) peut notamment être analysé comme un système autour des enjeux communs aux entreprises du secteur, les interactions entre fournisseurs et clients intermédiaires, sa régulation, etc.Système financier : ensemble des organisations, des établissements, des acteurs et des systèmes d'information bancaires et financiers à l'échelle mondiale.Système politique comme :Système communiste : ensemble des lois et doctrines qui vise à collectiviser les moyens de production et pratiqué pendant 70 ans par l'URSS ;Système libéral : ensemble des lois et doctrines qui permettent la liberté de propriété des moyens de production et pratiqué dans les économies de marché du monde occidental et asiatique.Système de coordonnées géographiquesSystème d'information géographiqueD'une manière générale, lorsque l'on parle simplement du système en informatique, en robotique ou en bureautique, on désigne l’ensemble des technologies (matériels et logiciels) qui constituent un appareil ou un réseau informatique :Système de traitement de l'information : représentation théorique d'un système en informatique.Système informatisé : machine automatique commandée par un ou plusieurs systèmes de traitement de l'information.Système d'exploitation : ensemble structuré et hiérarchisé de programmes et de processus regroupés autour d'un programme-maître appelé noyau, qui gère les divers éléments d'un appareil informatique.Système reconfigurable : système informatique matériel capable d'avoir sa structure interne modifiée afin d'adapter ses réponses à son environnement; système informatique logiciel susceptible d'être reconfiguré à distance par un automate ou un opérateur (avec ou sans fil de connexion)Système d'information : stratégie informatique mise en œuvre au sein d'une organisation humaine.Système de contrôle temps réel : ensemble de capteurs et d'actionneurs exhibant un comportement collectif en partageant leurs états selon un pas temporel court imposé appelé latence.Système mécatronique : ensemble complexe et structuré de composants mécaniques, électroniques et informatiques en interaction permanente et assurant une fonction d'usage (ex. : automobile, aéronef, train, lanceur spatial, ascenseur, téléphone mobile, centrale nucléaire…).Système formel : système logique composé d'un langage, d'un ensemble de règles de déductions et d'un ensemble d'axiomes.Système d'équations : ensemble de plusieurs équations devant être satisfaites simultanément :Système d'équations linéaires, algébriques ;Système d'équations différentielles.Systèmes dynamiques : branche de recherche des mathématiques.Systèmes de références :Système de numérationSystème de relations : ensemble de relations qui doivent être satisfaites simultanément.Système de référence : système d'axes par rapport auxquels on définit le mouvement d'un corps dans un espace à trois dimensions.Système de vecteurs : ensemble composé d'un nombre fini de vecteurs mobiles sur leur ligne d'action.Système : ensemble de portées devant être lues simultanément dans une partition de musique.Le « système de peintures » est l'ensemble des couches de peinture (« feuils ») appliquées sur un support de peinture (« subjectile »), respectant les compatibilités entre elles,. Les essais de Marcelin Pleynet regroupés dans son Système de la peinture (1977) se rapportent à la peinture moderne, sans rapport avec cet usage technique.Système : « Le système est l'ajointement de l'être lui-même, non pas seulement un cadre venant s'appliquer du dehors à l'étant et encore moins une collection arbitraire » Martin Heidegger. Le plus célèbre de ces systèmes construit de bout en bout selon des enchaînements logiques est celui de Spinoza, connu sous le titre d'Éthique.Système physique : ensemble d'éléments physiques concrets ou idéalisés (objet, point matériel, fluide, gaz parfait, champ électromagnétique…) dont on cherche à connaître la dynamique propre. Appelé simplement système en physique.Système de forces : ensemble d'un nombre fini de forces supposées appliquées à un même corps solide.Système d'unités :Système d'unités naturelles : système d'unités physiques fondé sur l'emploi d'un nombre minimum d'unités fondamentales indépendantes, choisies de façon à réduire à l'unité les coefficients numériques figurant dans certaines formules physiques très importantes, choisies comme fondamentales.Système international d'unités (SI) : système de mesure officiel en France depuis 1962, qui prolonge le système MKSA - système de mesure dont les unités fondamentales sont le mètre, le kilogramme, la seconde et l'ampère - par l'adjonction du kelvin et de la candela.Système métrique : ensemble coordonné d'unités servant à la mesure des différentes grandeurs.Système thermodynamique.Système familial (ou structure, modèle, type familial) : un groupe de parenté envisagé, en psychologie systémique et chez les historiens de la famille, sous l'angle du holisme méthodologique.Système social : l'organisation d'une société (groupe humain).Deux types de systèmes sont distingués en urbanisation des systèmes d'information :le système métier, qui est formé de l'ensemble des services et des processus de l'entreprise, des organisations qui les mettent en œuvre et des objets métiers associés ;le système informatique, qui est l'ensemble structuré des composants logiciels, matériels et des données, permettant d’automatiser tout ou partie du système métier.Système complexe : deux définitions différentes d'un système complexe coexistent : 1) ensemble de composants en interaction dont l'intégration permet de réaliser une mission commune, 2) ensemble d'agents simples qui, par leur interaction, amènent la structure globale du système à être modifiée de manière chaotique.Jacques Bouveresse, Qu'est-ce qu'un système philosophique ? : Cours 2007 et 2008, Paris, Collège de France, coll. « La philosophie de la connaissance », 2012, 200 p. (ISBN 978-2-7226-0152-9, lire en ligne).Martin Heidegger (trad. de l'allemand par Jean-François Courtine), Schelling : Le traité de 1809 sur l'essence de la liberté humaine, Paris, Gallimard, coll. « Bibliothèque de Philosophie », 1993, 349 p. (ISBN 2-07-073792-6).Daniel Krob, « Éléments de systémique. Architecture de systèmes », dans A. Berthoz, J.L. Petit, Complexité-Simplexité, Collège de France, 2014 (ISBN 9782722603301, DOI 10.4000/books.cdf.3388, lire en ligne).(en) Herbert Simon, The Architecture of Complexity, vol. 106, coll. « Proceedings of the American Philosophica », décembre 1962, 467-482 p., chap. 6.(en) Ludwig von Bertalanffy, General System Theory : Foundations, Development, Applications, Paris, George Braziller, 1976 (ISBN 978-2-04-007504-0).(en) Jules Vuillemin, What are Philosophical Systems?, Cambridge, University Press, 1986, 176 p. (ISBN 978-0-521-11228-4).Analyse décisionnelle des systèmes complexesAnalyse systémiqueArchitecture d'un systèmeSystème complexe(de) Definitionen von ""System"" (1572–2002) par Roland Müller.(en) Publications with the title ""System"" (1600–2008) par Roland Müller. Portail des sciences"
Informatique;En informatique théorique, un algorithme d'approximation est une méthode permettant de calculer une solution approchée à un problème algorithmique d'optimisation. Plus précisément, c'est une heuristique garantissant à la qualité de la solution qui fournit un rapport inférieur (si l'on minimise) à une constante, par rapport à la qualité optimale d'une solution, pour toutes les instances possibles du problème.L'intérêt de tels algorithmes est qu'il est parfois plus facile de trouver une solution approchée qu'une solution exacte, le problème pouvant par exemple être NP-complet mais admettre un algorithme d'approximation polynomial. Ainsi, dans les situations où l'on cherche une bonne solution, mais pas forcément la meilleure, un algorithme d'approximation peut être un bon outil.Selon la nature du problème (maximisation, minimisation, etc.) la définition peut varier, on donne ici la définition classique pour un problème de minimisation avec facteur d'approximation constant.Pour un problème de minimisation ayant une solution optimale de valeur                               z                      ?                                {\displaystyle z^{*}}  , un algorithme d'approximation de facteur                     ?        >        1              {\displaystyle \rho >1}   (i.e. un algorithme                     ?              {\displaystyle \rho }  -approché) est un algorithme donnant une solution de valeur                     z              {\displaystyle z}  , avec la garantie que                     z        ?        ?                  z                      ?                                {\displaystyle z\leq \rho z^{*}}  .Un schéma d'approximation est un algorithme prenant comme entrée les données du problèmes mais aussi une valeur                     ?        >        0              {\displaystyle \epsilon >0}   et calculant une solution approchée avec un facteur                     (        1        +        ?        )              {\displaystyle (1+\epsilon )}  . Si le temps de calcul est polynomial en la taille de l'entrée et en                     1                  /                ?              {\displaystyle 1/\epsilon }  , on parle de schéma d'approximation en temps polynomial (abrégé PTAS en anglais).Par exemple pour le problème du transversal minimum puisque tout transversal formé par les sommets incidents aux arêtes d'un couplage maximal pour l'inclusion a une cardinalité inférieure à deux fois l'optimum.C'est aussi le cas pour le cas particulier du voyageur de commerce où les poids satisfont les inégalités triangulaires car alors, le poids minimum d'un arbre couvrant est toujours inférieur à deux fois l'optimum. En affinant cette approche en utilisant un couplage bien choisi on peut aussi obtenir un facteur d'approximation de 3/2, avec l'algorithme de Christofides.Parmi les techniques utilisées, on compte les méthodes d'algorithmique classique, par exemple un algorithme glouton permet parfois d'obtenir une bonne approximation à défaut de calculer une solution optimale. On peut aussi citer des algorithmes de recherche locale et de programmation dynamique.Beaucoup d'algorithmes sont basées sur l'optimisation linéaire. On peut par exemple arrondir une solution fractionnaire ou utiliser un schéma primal-dual. Une technique plus avancée est d'utiliser l'optimisation SDP, comme pour le problème de la coupe maximum.Parmi les problèmes NP-complets certains sont dits difficile à approximer, c'est-à-dire qu'ils n'admettent pas d'algorithme d'approximation si l'on suppose certaines hypothèses, par exemple P différent de NP ou bien la conjecture des jeux uniques.Le problème du voyageur de commerce dans un graphe                     G        =        (        V        ,        E        )              {\displaystyle G=(V,E)}   avec des poids quelconques (positifs) est un problème qui n'admet pas d'algorithme d'approximation. En effet, tout algorithme d'approximation pour ce problème dans le graphe complet                               K                      V                                {\displaystyle K_{V}}   où les arêtes de                     E              {\displaystyle E}   ont une valeur nulle et les autres la valeur 1 fournit une réponse au problème de décision NP-complet de statuer sur l'hamiltonicité d'un graphe (en l'occurrence                     G              {\displaystyle G}   est hamiltonien si et seulement si l'algorithme approché fournit une solution de valeur nulle).Un autre problème est le problème du k-centre métrique qui admet une réduction simple au problème de l'ensemble dominant.Il existe des techniques plus complexes pour montrer des résultats de difficulté d'approximation. Elles tournent essentiellement autour du théorème PCP.Plus récemment la conjecture des jeux uniques a été utilisée pour montrer des résultats plus forts.Des algorithmes d'approximation ont été découverts avant même la mise en place de la théorie de la NP-complétude, par exemple par Paul Erd?s dans les années 1960, pour le problème de la coupe maximum ou Ronald Graham, pour les algorithmes d'ordonnancement,. Cependant c'est à la suite de cette théorie que le domaine s'est vraiment développé. L'utilisation de l'optimisation linéaire est due à László Lovász dans les années 1970, pour le problème de couverture par ensembles.Dans les années 1990, le théorème PCP a été une avancée très importante pour la non-approximabilité. Portail de l'informatique théorique
Informatique;"L'apprentissage automatique, (en anglais : machine learning, litt. « apprentissage machine, »), apprentissage artificiel ou apprentissage statistique est un  champ d'étude de l'intelligence artificielle qui se fonde sur des approches mathématiques et statistiques pour donner aux ordinateurs la capacité d'« apprendre » à partir de données, c'est-à-dire d'améliorer leurs performances à résoudre des tâches sans être explicitement programmés pour chacune. Plus largement, il concerne la conception, l'analyse, l'optimisation, le développement et l'implémentation de telles méthodes. On parle d'apprentissage statistique car l'apprentissage consiste à créer un modèle dont l'erreur statistique moyenne est la plus faible possible.L'apprentissage automatique comporte généralement deux phases. La première consiste à estimer un modèle à partir de données, appelées observations, qui sont disponibles et en nombre fini, lors de la phase de conception du système. L'estimation du modèle consiste à résoudre une tâche pratique, telle que traduire un discours, estimer une densité de probabilité, reconnaître la présence d'un chat dans une photographie ou participer à la conduite d'un véhicule autonome. Cette phase dite « d'apprentissage » ou « d'entraînement » est généralement réalisée préalablement à l'utilisation pratique du modèle. La seconde phase correspond à la mise en production : le modèle étant déterminé, de nouvelles données peuvent alors être soumises afin d'obtenir le résultat correspondant à la tâche souhaitée. En pratique, certains systèmes peuvent poursuivre leur apprentissage une fois en production, pour peu qu'ils aient un moyen d'obtenir un retour sur la qualité des résultats produits.Selon les informations disponibles durant la phase d'apprentissage, l'apprentissage est qualifié de différentes manières. Si les données sont étiquetées (c'est-à-dire que la réponse à la tâche est connue pour ces données), il s'agit d'un apprentissage supervisé. On parle de classification ou de classement si les étiquettes sont discrètes, ou de régression si elles sont continues. Si le modèle est appris de manière incrémentale en fonction d'une récompense reçue par le programme pour chacune des actions entreprises, on parle d'apprentissage par renforcement. Dans le cas le plus général, sans étiquette, on cherche à déterminer la structure sous-jacente des données (qui peuvent être une densité de probabilité) et il s'agit alors d'apprentissage non supervisé. L'apprentissage automatique peut être appliqué à différents types de données, tels des graphes, des arbres, des courbes, ou plus simplement des vecteurs de caractéristiques, qui peuvent être des variables qualitatives ou quantitatives continues ou discrètes.Depuis l'antiquité, le sujet des machines pensantes préoccupe les esprits. Ce concept est la base de pensées pour ce qui deviendra ensuite l'intelligence artificielle, ainsi qu'une de ses sous-branches : l'apprentissage automatique.La concrétisation de cette idée est principalement due à Alan Turing (mathématicien et cryptologue britannique) et à son concept de la « machine universelle » en 1936, qui est à la base des ordinateurs d'aujourd'hui. Il continuera à poser les bases de l'apprentissage automatique, avec son article sur « L'ordinateur et l'intelligence » en 1950, dans lequel il développe, entre autres, le test de Turing.En 1943, le neurophysiologiste Warren McCulloch et le mathématicien Walter Pitts publient un article décrivant le fonctionnement de neurones en les représentant à l'aide de circuits électriques. Cette représentation sera la base théorique des réseaux neuronaux.Arthur Samuel, informaticien américain pionnier dans le secteur de l'intelligence artificielle, est le premier à faire usage de l'expression machine learning (en français, « apprentissage automatique ») en 1959 à la suite de la création de son programme pour IBM en 1952. Le programme jouait au Jeu de Dames et s'améliorait en jouant. À terme, il parvint à battre le 4e meilleur joueur des États-Unis,.Une avancée majeure dans le secteur de l'intelligence machine est le succès de l'ordinateur développé par IBM, Deep Blue, qui est le premier à vaincre le champion mondial d'échecs Garry Kasparov en 1997. Le projet Deep Blue en inspirera nombre d'autres dans le cadre de l'intelligence artificielle, particulièrement un autre grand défi : IBM Watson, l'ordinateur dont le but est de gagner au jeu Jeopardy!. Ce but est atteint en 2011, quand Watson gagne à Jeopardy! en répondant aux questions par traitement de langage naturel.Durant les années suivantes, les applications de l'apprentissage automatique médiatisées se succèdent bien plus rapidement qu'auparavant.En 2012, un réseau neuronal développé par Google parvient à reconnaître des visages humains ainsi que des chats dans des vidéos YouTube,.En 2014, 64 ans après la prédiction d'Alan Turing, le dialogueur Eugene Goostman est le premier à réussir le test de Turing en parvenant à convaincre 33 % des juges humains au bout de cinq minutes de conversation qu'il est non pas un ordinateur, mais un garçon ukrainien de 13 ans.En 2015, une nouvelle étape importante est atteinte lorsque l'ordinateur « AlphaGo » de Google gagne contre un des meilleurs joueurs au jeu de Go, jeu de plateau considéré comme le plus dur du monde.En 2016, un système d'intelligence artificielle à base d'apprentissage automatique nommé LipNet parvient à lire sur les lèvres avec un grand taux de succès,.L'apprentissage automatique (AA) permet à un système piloté ou assisté par ordinateur comme un programme, une IA ou un robot, d'adapter ses réponses ou comportements aux situations rencontrées, en se fondant sur l'analyse de données empiriques passées issues de bases de données, de capteurs, ou du web.L'AA permet de surmonter la difficulté qui réside dans le fait que l'ensemble de tous les comportements possibles compte tenu de toutes les entrées possibles devient rapidement trop complexe à décrire et programmer de manière classique (on parle d'explosion combinatoire). On confie donc à des programmes d'AA le soin d'ajuster un modèle pour simplifier cette complexité et de l'utiliser de manière opérationnelle. Idéalement, l'apprentissage visera à être non supervisé, c'est-à-dire que les réponses aux données d’entraînement ne sont pas fournies au modèle.Ces programmes, selon leur degré de perfectionnement, intègrent éventuellement des capacités de traitement probabiliste des données, d'analyse de données issues de capteurs, de reconnaissance (reconnaissance vocale, de forme, d'écriture…), de fouille de données, d'informatique théorique…L'apprentissage automatique est utilisé dans un large spectre d'applications pour doter des ordinateurs ou des machines de capacité d'analyser des données d'entrée comme : perception de leur environnement (vision, Reconnaissance de formes tels des visages, schémas, segmentation d'image, langages naturels, caractères dactylographiés ou manuscrits ; moteurs de recherche, analyse et indexation d'images et de vidéo, en particulier pour la recherche d'image par le contenu ; aide aux diagnostics, médical notamment, bio-informatique, chémoinformatique ; interfaces cerveau-machine ; détection de fraudes à la carte de crédit, cybersécurité, analyse financière, dont analyse du marché boursier ; classification des séquences d'ADN ; jeu ; génie logiciel ; adaptation de sites Web ; robotique (locomotion de robots, etc.) ; analyse prédictive dans de nombreux domaines (financière, médicale, juridique, judiciaire), diminution des temps de calcul pour les simulations informatiques en physique (calcul de structures, de mécanique des fluides, de neutronique, d'astrophysique, de biologie moléculaire, etc.),, optimisation de design dans l'industrie,,, etc.Exemples :un système d'apprentissage automatique peut permettre à un robot ayant la capacité de bouger ses membres, mais ne sachant initialement rien de la coordination des mouvements permettant la marche, d'apprendre à marcher. Le robot commencera par effectuer des mouvements aléatoires, puis, en sélectionnant et privilégiant les mouvements lui permettant d'avancer, mettra peu à peu en place une marche de plus en plus efficace[réf. nécessaire] ;la reconnaissance de caractères manuscrits est une tâche complexe car deux caractères similaires ne sont jamais exactement identiques. Il existe des systèmes d'apprentissage automatique qui apprennent à reconnaître des caractères en observant des « exemples », c'est-à-dire des caractères connus. Un des premiers système de ce type est celui de reconnaissance des codes postaux US manuscrits issu des travaux de recherche de Yann Le Cun, un des pionniers du domaine ,, et ceux utilisés pour la  reconnaissance d'écriture ou OCR.Les algorithmes d'apprentissage peuvent se catégoriser selon le mode d'apprentissage qu'ils emploient.Si les classes sont prédéterminées et les exemples connus, le système apprend à classer selon un modèle de classification ou de classement ; on parle alors d'apprentissage supervisé (ou d'analyse discriminante). Un expert (ou oracle) doit préalablement étiqueter des exemples. Le processus se passe en deux phases. Lors de la première phase (hors ligne, dite d'apprentissage), il s'agit de déterminer un modèle à partir des données étiquetées. La seconde phase (en ligne, dite de test) consiste à prédire l'étiquette d'une nouvelle donnée, connaissant le modèle préalablement appris. Parfois il est préférable d'associer une donnée non pas à une classe unique, mais une probabilité d'appartenance à chacune des classes prédéterminées ; on parle alors d'apprentissage supervisé probabiliste.Fondamentalement, le machine learning supervisé revient à apprendre à une machine à construire une fonction f telle que Y = f(X), Y étant un (ou plusieurs) résultat(s) d'intérêt calculé en fonction de données d'entrées X effectivement à la disposition de l'utilisateur. Y peut être une grandeur continue (une température par exemple), et on parle alors de régression, ou discrète (une classe, chien ou chat par exemple), et on parle alors de classification.Des cas d'usage typiques d'apprentissage automatique peuvent être d'estimer la météo du lendemain en fonction de celle du jour et des jours précédents, de prédire le vote d'un électeur en fonction de certaines données économiques et sociales, d'estimer la résistance d'un nouveau matériau en fonction de sa composition, de déterminer la présence ou non d'un objet dans une image. L'analyse discriminante linéaire ou les SVM en sont d'autres exemples typiques. Autre exemple, en fonction de points communs détectés avec les symptômes d'autres patients connus (les exemples), le système peut catégoriser de nouveaux patients, au vu de leurs analyses médicales, en risque estimé de développer telle ou telle maladie.Quand le système ou l'opérateur ne dispose que d'exemples, mais non d'étiquette, et que le nombre de classes et leur nature n'ont pas été prédéterminées, on parle d'apprentissage non supervisé ou clustering en anglais. Aucun expert n'est requis. L'algorithme doit découvrir par lui-même la structure plus ou moins cachée des données. Le partitionnement de données, data clustering en anglais, est un algorithme d'apprentissage non supervisé.Le système doit ici — dans l'espace de description (l'ensemble des données) — cibler les données selon leurs attributs disponibles, pour les classer en groupes homogènes d'exemples. La similarité est généralement calculée selon une fonction de distance entre paires d'exemples. C'est ensuite à l'opérateur d'associer ou déduire du sens pour chaque groupe et pour les motifs (patterns en anglais) d'apparition de groupes, ou de groupes de groupes, dans leur « espace ». Divers outils mathématiques et logiciels peuvent l'aider. On parle aussi d'analyse des données en régression (ajustement d'un modèle par une procédure de type moindres carrés ou autre optimisation d'une fonction de coût). Si l'approche est probabiliste (c'est-à-dire que chaque exemple, au lieu d'être classé dans une seule classe, est caractérisé par un jeu de probabilités d'appartenance à chacune des classes), on parle alors de « soft clustering » (par opposition au « hard clustering »).Cette méthode est souvent source de sérendipité. ex. : Pour un épidémiologiste qui voudrait dans un ensemble assez large de victimes de cancer du foie tenter de faire émerger des hypothèses explicatives, l'ordinateur pourrait différencier différents groupes, que l'épidémiologiste chercherait ensuite à associer à divers facteurs explicatifs, origines géographique, génétique, habitudes ou pratiques de consommation, expositions à divers agents potentiellement ou effectivement toxiques (métaux lourds, toxines telle que l'aflatoxine, etc.).Contrairement à l’apprentissage supervisé où l’apprentissage automatique consiste à trouver une fonction f telle que Y = f(X), où Y est un résultat connu et objectif (par exemple Y = « présence d’une tumeur » ou « absence de tumeur » en fonction de X = image radiographique), dans l’apprentissage non supervisé, on ne dispose pas de valeurs de Y, uniquement de valeurs de X (dans l’exemple précédent, on disposerait uniquement des images radiographiques sans connaissance de la présence ou non d’une tumeur. L'apprentissage non supervisé pourrait découvrir deux ""clusters"" ou groupes correspondant à ""présence"" ou ""absence"" de tumeur, mais les chances de réussite sont moindres que dans le cas supervisé où la machine est orientée sur ce qu'elle doit trouver).L’apprentissage non supervisé est généralement moins performant que l’apprentissage supervisé, il évolue dans une zone « grise » où il n’y a généralement pas de « bonne » ou de « mauvaise » réponse mais simplement des similarités mathématiques discernables ou non. L’apprentissage non supervisé présente cependant l’intérêt de pouvoir travailler sur une base de données de X sans qu’il soit nécessaire d’avoir des valeurs de Y correspondantes, or les Y sont généralement compliqués et/ou coûteux à obtenir, alors que les seuls X sont généralement plus simples et moins coûteux à obtenir (dans l’exemple des images radiographiques, il est relativement aisé d’obtenir de telles images, alors qu’obtenir les images avec le label « présence de tumeur » ou « absence de tumeur » nécessite l’intervention longue et coûteuse d’un spécialiste en imagerie médicale).L’apprentissage non supervisé permet potentiellement de détecter des anomalies dans une base de données, comme des valeurs singulières ou aberrantes pouvant provenir d’une erreur de saisie ou d’une singularité très particulière. Il peut donc s’agir d’un outil intéressant pour vérifier ou nettoyer une base de données.Effectué de manière probabiliste ou non, il vise à faire apparaître la distribution sous-jacente des exemples dans leur espace de description. Il est mis en œuvre quand des données (ou « étiquettes ») manquent… Le modèle doit utiliser des exemples non étiquetés pouvant néanmoins renseigner. ex. : En médecine, il peut constituer une aide au diagnostic ou au choix des moyens les moins onéreux de tests de diagnostic.Probabiliste ou non, quand l'étiquetage des données est partiel. C'est le cas quand un modèle énonce qu'une donnée n'appartient pas à une classe A, mais peut-être à une classe B ou C (A, B et C étant trois maladies par exemple évoquées dans le cadre d'un diagnostic différentiel).L’apprentissage auto-supervisé consiste à construire un problème d’apprentissage supervisé à partir d’un problème non supervisé à l’origine.Pour rappel, l’apprentissage supervisé consiste à construire une fonction Y = f(X) et nécessite donc une base de données où l’on possède des Y en fonction des X (par exemple, en fonction du texte X correspondant à la critique d’un film, retrouver la valeur du Y correspondant à la note attribuée au film), alors que dans l’apprentissage non supervisé, on dispose uniquement des valeurs de X et pas de valeurs de Y (on disposerait par exemple ici uniquement du texte X correspondant à la critique du film, et pas de la note Y attribuée au film).L’apprentissage auto-supervisé consiste donc à créer des Y à partir des X pour passer à un apprentissage supervisé, en ""masquant"" des X pour en faire des Y. Dans le cas d'une image, l'apprentissage auto-supervisé peut consister à reconstruire la partie manquante d'une image qui aurait été tronquée. Dans le cas du langage, lorsqu’on dispose d’un ensemble de phrases qui correspondent aux X sans cible Y particulière, l’apprentissage auto-supervisé consiste à supprimer certains X (certains mots) pour en faire des Y. L’apprentissage auto-supervisé revient alors pour la machine à essayer de reconstruire un mot ou un ensemble de mots manquants en fonction des mots précédents et/ou suivants, en une forme d’auto-complétion. Cette approche permet potentiellement à une machine de « comprendre » le langage humain, son sens sémantique et symbolique. Les modèles IA de langage comme BERT ou GPT-3 sont conçus selon ce principe. Dans le cas d’un film, l’apprentissage auto-supervisé consisterait à essayer de prédire les images suivantes en fonction des images précédentes, et donc à tenter de prédire « l’avenir » sur la base de la logique probable du monde réel.Certains chercheurs, comme Yann Le Cun, pensent que si l’IA générale est possible, c’est probablement par une approche de type auto-supervisé qu’elle pourrait être conçue, par exemple en étant immergée dans le monde réel pour essayer à chaque instant de prédire les images et les sons les plus probables à venir, en comprenant qu’un ballon en train de rebondir et de rouler va encore continuer à rebondir et à rouler, mais de moins en moins haut et de moins en moins vite jusqu’à s’arrêter, et qu'un obstacle est de nature à arrêter le ballon ou à modifier sa trajectoire, ou à essayer de prédire les prochains mots qu’une personne est susceptible de prononcer ou le prochain geste qu’elle pourrait accomplir. L’apprentissage auto-supervisé dans le monde réel serait une façon d’apprendre à une machine le sens commun, le bon sens, la réalité du monde physique qui l’entoure, et permettrait potentiellement d’atteindre une certaine forme de conscience. Il ne s’agit évidemment que d’une hypothèse de travail, la nature exacte de la conscience, son fonctionnement et sa définition même restant un domaine actif de recherche.L'algorithme apprend un comportement étant donné une observation. L'action de l'algorithme sur l'environnement produit une valeur de retour qui guide l'algorithme d'apprentissage.Par exemple, l'algorithme de Q-learning est un exemple classique.L'apprentissage par renforcement peut aussi être vu comme une forme d'apprentissage auto-supervisé. Dans un problème d'apprentissage par renforcement, il n'y a en effet à l'origine pas de données de sorties Y, ni même de données d'entrée X, pour construire une fonction Y = f(X). Il y a simplement un ""écosystème"" avec des règles qui doivent être respectées, et un ""objectif"" à atteindre. Par exemple, pour le football, il y a des règles du jeu à respecter et des buts à marquer. Dans l'apprentissage par renforcement, le modèle crée lui-même sa base de donnes en ""jouant"" (d'où le concept d'auto-supervisé) : il teste des combinaisons de données d'entrée X et il en découle un résultat Y qui est évalué, s'il est conforme aux règles du jeu et atteint son objectif, le modèle est récompensé et sa stratégie est ainsi validée, sinon le modèle est pénalisé. Par exemple pour le football, dans une situation du type ""ballon possédé, joueur adverse en face, but à 20 mètres"", une stratégie peut être de ""tirer"" ou de ""dribbler"", et en fonction du résultat (""but marqué"", ""but raté"", ""balle toujours possédée, joueur adverse franchi""), le modèle apprend de manière incrémentale comment se comporter au mieux en fonction des différentes situations rencontrées.L’apprentissage par transfert peut être vu comme la capacité d’un système à reconnaître et à appliquer des connaissances et des compétences, apprises à partir de tâches antérieures, sur de nouvelles tâches ou domaines partageant des similitudes. Il s'agit d'identifier les similitudes entre la ou les tâche(s) cible(s) et la ou les tâche(s) source(s), puis de transférer la connaissance de la ou des tâche(s) source(s) vers la ou les tâche(s) cible(s),.Une application classique de l’apprentissage par transfert est l’analyse d’images. Pour une problématique de classification, l’apprentissage par transfert consiste à repartir d’un modèle existant plutôt que de repartir de zéro. Si par exemple on dispose déjà d’un modèle capable de repérer un chat parmi tout autre objet du quotidien, et que l’on souhaite classifier les chats par races, il est possible que réentraîner partiellement le modèle existant permette d’obtenir de meilleures performances et à moindre coût qu’en repartant de zéro,. Un modèle souvent utilisé pour réaliser un apprentissage par transfert de ce type est VGG-16, un réseau de neurones conçu par l'Université d'Oxford, entraîné sur ~14 millions d'images, capable de classer avec ~93% de précision mille objets du quotidien.Les algorithmes se classent en quatre familles ou types principaux :régressionclassificationpartitionnement de donnéesréduction de dimensions.Plus précisément :la régression linéaire ;la régression logistique ;les machines à vecteur de support ;les réseaux de neurones, dont les méthodes d'apprentissage profond (deep learning en anglais) pour un apprentissage supervisé ou non-supervisé ;la méthode des k plus proches voisins pour un apprentissage supervisé ;les arbres de décision, méthodes à l'origine des Random Forest, par extension également du boosting (notamment XGBoost) ;les méthodes statistiques comme le modèle de mixture gaussienne ;l'analyse discriminante linéaire ;les algorithmes génétiques et la programmation génétique ;le boosting ;le bagging.Ces méthodes sont souvent combinées pour obtenir diverses variantes d'apprentissage. Le choix d'un algorithme dépend fortement de la tâche à résoudre (classification, estimation de valeurs…), du volume et de la nature des données. Ces modèles reposent souvent sur des modèles statistiques.La qualité de l'apprentissage et de l'analyse dépendent du besoin en amont et a priori de la compétence de l'opérateur pour préparer l'analyse. Elle dépend aussi de la complexité du modèle (spécifique ou généraliste), de son adéquation et de son adaptation au sujet à traiter. In fine, la qualité du travail dépendra aussi du mode (de mise en évidence visuelle) des résultats pour l'utilisateur final (un résultat pertinent pourrait être caché dans un schéma trop complexe, ou mal mis en évidence par une représentation graphique inappropriée).Avant cela, la qualité du travail dépendra de facteurs initiaux contraignants, liées à la base de données :nombre d'exemples (moins il y en a, plus l'analyse est difficile, mais plus il y en a, plus le besoin de mémoire informatique est élevé et plus longue est l'analyse) ;nombre et qualité des attributs décrivant ces exemples. La distance entre deux « exemples » numériques (prix, taille, poids, intensité lumineuse, intensité de bruit, etc.) est facile à établir, celle entre deux attributs catégoriels (couleur, beauté, utilité…) est plus délicate ;pourcentage de données renseignées et manquantes ;bruit : le nombre et la « localisation » des valeurs douteuses (erreurs potentielles, valeurs aberrantes…) ou naturellement non-conformes au pattern de distribution générale des « exemples » sur leur espace de distribution impacteront sur la qualité de l'analyse.L'apprentissage automatique ne se résume pas à un ensemble d'algorithmes, mais suit une succession d'étapes,.Définir le problème à résoudre.Acquérir des données : l'algorithme se nourrissant des données en entrée, c'est une étape importante. Il en va de la réussite du projet, de récolter des données pertinentes et en quantité et qualité suffisantes, et en évitant tout biais dans leur représentativité.Analyser et explorer les données. L'exploration des données peut révéler des données d'entrée ou de sortie déséquilibrées pouvant nécessiter un rééquilibrage, le machine learning non supervisé peut révéler des clusters qu'il pourrait être utile de traiter séparément ou encore détecter des anomalies qu'il pourrait être utile de supprimer.Préparer et nettoyer les données : les données recueillies doivent être retouchées avant utilisation. En effet, certains attributs sont inutiles, d’autre doivent être modifiés afin d’être compris par l’algorithme (les variables qualitatives doivent être encodées-binarisées), et certains éléments sont inutilisables car leurs données sont incomplètes (les valeurs manquantes doivent être gérées, par exemple par simple suppression des exemples comportant des variables manquantes, ou par remplissage par la médiane, voire par apprentissage automatique). Plusieurs techniques telles que la visualisation de données, la transformation de données (en) ou encore la normalisation (variables projetées entre 0 et 1) ou la standardisation (variables centrées - réduites) sont employées afin d'homogénéiser les variables entre elles, notamment pour aider la phase de descente de gradient nécessaire à l'apprentissage.Ingénierie ou extraction de caractéristiques : les attributs peuvent être combinés entre eux pour en créer de nouveaux plus pertinents et efficaces pour l’entraînement du modèle. Ainsi, en physique, de la construction de nombres adimensionnels adaptés au problème, de solutions analytiques approchées, de statistiques pertinentes, de corrélations empiriques ou l'extraction de spectres par transformée de Fourier ,. Il s'agit d'ajouter l'expertise humaine au préalable de l'apprentissage machine pour favoriser celui-ci.Choisir ou construire un modèle d’apprentissage : un large choix d'algorithmes existe, et il faut en choisir un adapté au problème et aux données. La métrique optimisée doit être choisie judicieusement (erreur absolue moyenne, erreur relative moyenne, précision, rappel, etc.)Entraîner, évaluer et optimiser : l'algorithme d'apprentissage automatique est entraîné et validé sur un premier jeu de données pour optimiser ses hyperparamètres.Test : puis il est évalué sur un deuxième ensemble de données de test afin de vérifier qu'il est efficace avec un jeu de donnée indépendant des données d’entraînement, et pour vérifier qu'il ne fasse pas de surapprentissage.Déployer : le modèle est alors déployé en production pour faire des prédictions, et potentiellement utiliser les nouvelles données en entrée pour se ré-entraîner et être amélioré.Expliquer : déterminer quelles sont les variables importantes et comment elles impactent les prédictions du modèle en général et au cas par casLa plupart de ces étapes se retrouvent dans les méthodes et processus de projet KDD, CRISP-DM et SEMMA, qui concernent les projets d'exploration de données.Toutes ces étapes sont complexes et requièrent du temps et de l'expertise, mais il existe des outils permettant de les automatiser au maximum pour ""démocratiser"" l'accès à l'apprentissage automatique. Ces approches sont dites ""Auto ML"" (pour machine learning automatique) ou ""No Code"" (pour illustrer que ces approches ne nécessitent pas ou très peu de programmation informatique), elles permettent d'automatiser la construction de modèles d'apprentissage automatique pour limiter au maximum le besoin d'intervention humaine. Parmi ces outils, commerciaux ou non, on peut citer Caret, PyCaret, pSeven, Jarvis, Knime, MLBox ou DataRobot.La voiture autonome paraît en 2016 réalisable grâce à l’apprentissage automatique et les énormes quantités de données générées par la flotte automobile, de plus en plus connectée. Contrairement aux algorithmes classiques (qui suivent un ensemble de règles prédéterminées), l’apprentissage automatique apprend ses propres règles.Les principaux innovateurs dans le domaine insistent sur le fait que le progrès provient de l’automatisation des processus. Ceci présente le défaut que le processus d’apprentissage automatique devient privatisé et obscur. Privatisé, car les algorithmes d’AA constituent des gigantesques opportunités économiques, et obscurs car leur compréhension passe derrière leur optimisation. Cette évolution peut potentiellement nuire à la confiance du public envers l’apprentissage automatique, mais surtout au potentiel à long terme de techniques très prometteuses.La voiture autonome présente un cadre test pour confronter l’apprentissage automatique à la société. En effet, ce n’est pas seulement l’algorithme qui se forme à la circulation routière et ses règles, mais aussi l’inverse. Le principe de responsabilité est remis en cause par l’apprentissage automatique, car l’algorithme n’est plus écrit mais apprend et développe une sorte d’intuition numérique. Les créateurs d’algorithmes ne sont plus en mesure de comprendre les « décisions » prises par leurs algorithmes, ceci par construction mathématique même de l’algorithme d’apprentissage automatique.Dans le cas de l’AA et les voitures autonomes, la question de la responsabilité en cas d’accident se pose. La société doit apporter une réponse à cette question, avec différentes approches possibles. Aux États-Unis, il existe la tendance à juger une technologie par la qualité du résultat qu’elle produit, alors qu’en Europe le principe de précaution est appliqué, et on y a plus tendance à juger une nouvelle technologie par rapport aux précédentes, en évaluant les différences par rapport à ce qui est déjà connu. Des processus d’évaluation de risques sont en cours en Europe et aux États-Unis.La question de responsabilité est d’autant plus compliquée que la priorité chez les concepteurs réside en la conception d’un algorithme optimal, et non pas de le comprendre. L’interprétabilité des algorithmes est nécessaire pour en comprendre les décisions, notamment lorsque ces décisions ont un impact profond sur la vie des individus. Cette notion d’interprétabilité, c’est-à-dire de la capacité de comprendre pourquoi et comment un algorithme agit, est aussi sujette à interprétation.La question de l’accessibilité des données est sujette à controverse : dans le cas des voitures autonomes, certains défendent l’accès public aux données, ce qui permettrait un meilleur apprentissage aux algorithmes et ne concentrerait pas cet « or numérique » dans les mains d’une poignée d’individus, de plus d’autres militent pour la privatisation des données au nom du libre marché, sans négliger le fait que des bonnes données constituent un avantage compétitif et donc économique,.La question des choix moraux liés aux décisions laissées aux algorithmes d'AA et aux voitures autonomes en cas de situations dangereuses ou mortelles se pose aussi. Par exemple en cas de défaillance des freins du véhicule, et d'accident inévitable, quelles vies sont à sauver en priorité: celle des passagers ou bien celle des piétons traversant la rue ?Dans les années 2000-2010, l'apprentissage automatique est encore une technologie émergente, mais polyvalente, qui est par nature théoriquement capable d'accélérer le rythme de l'automatisation et de l'autoaprentissage lui-même. Combiné à l'apparition de nouveaux moyens de produire, stocker et faire circuler l'énergie, ainsi qu'à l'informatique ubiquiste, il pourrait bouleverser les technologies et la société comme l'ont fait la machine à vapeur et l'électricité, puis le pétrole et l'informatique lors des révolutions industrielles précédentes.L'apprentissage automatique pourrait générer des innovations et des capacités inattendues, mais avec un risque selon certains observateurs de perte de maîtrise de la part des humains sur de nombreuses tâches qu'ils ne pourront plus comprendre et qui seront faites en routine par des entités informatiques et robotisées. Ceci laisse envisager des impacts spécifiques complexes et encore impossibles à évaluer sur l'emploi, le travail et plus largement l'économie et les inégalités. Selon le journal Science fin 2017 : « Les effets sur l'emploi sont plus complexes que la simple question du remplacement et des substitutions soulignées par certains. Bien que les effets économiques du BA soient relativement limités aujourd'hui et que nous ne soyons pas confrontés à une « fin du travail » imminente comme cela est parfois proclamé, les implications pour l'économie et la main-d'œuvre sont profondes ».Il est tentant de s'inspirer des êtres vivants sans les copier naïvement pour concevoir des machines capables d'apprendre. Les notions de percept et de concept comme phénomènes neuronaux physiques ont d'ailleurs été popularisés dans le monde francophone par Jean-Pierre Changeux. L'apprentissage automatique reste avant tout un sous-domaine de l'informatique, mais il est étroitement lié op"
;
"rationn""";
Informatique;Dans le domaine informatique et de l'intelligence artificielle, l'apprentissage non supervisé désigne la situation d'apprentissage automatique où les données ne sont pas étiquetées. Il s'agit donc de découvrir les structures sous-jacentes à ces données non étiquetées. Puisque les données ne sont pas étiquetées, il est impossible à l'algorithme de calculer de façon certaine un score de réussite.L'absence d'étiquetage ou d'annotation caractérise les tâches d'apprentissage non supervisé et les distingue donc des tâches d'apprentissage supervisé.L'introduction dans un système d'une approche d'apprentissage non supervisé est un moyen d'expérimenter l'intelligence artificielle. En général, des systèmes d'apprentissage non supervisé permettent d'exécuter des tâches plus complexes que les systèmes d'apprentissage supervisé, mais ils peuvent aussi être plus imprévisibles. Même si un système d'IA d'apprentissage non supervisé parvient tout seul, par exemple, à faire le tri entre des chats et des chiens, il peut aussi ajouter des catégories inattendues et non désirées, et classer des races inhabituelles, introduisant plus de bruit que d'ordre.L'apprentissage non supervisé consiste à apprendre sans superviseur. Il s’agit d’extraire des classes ou groupes d’individus présentant des caractéristiques communes. La qualité d'une méthode de classification est mesurée par sa capacité à découvrir certains ou tous les motifs cachés.On distingue l'apprentissage supervisé et non supervisé. Dans le premier apprentissage, il s’agit d’apprendre à classer un nouvel individu parmi un ensemble de classes prédéfinies: on connaît les classes a priori. Tandis que dans l'apprentissage non supervisé, le nombre et la définition des classes ne sont pas donnés a priori.Différence entre les deux types d'apprentissage. Apprentissage supervisé On dispose d'éléments déjà classésExemple : articles en rubrique cuisine, sport, culture...On veut classer un nouvel élémentExemple: lui attribuer un nom parmi cuisine, sport, culture... Apprentissage non supervisé On dispose d'éléments non classésExemple : une fleurOn veut les regrouper en classesExemple: si deux fleurs ont la même forme, elles sont en rapport avec une même plante correspondante.Il existe deux principales méthodes d'apprentissage non supervisées :Les méthodes par partitionnement telles que les algorithmes des k-moyennes ou k-médoïdes.Les méthodes de regroupement hiérarchique.Les techniques d'apprentissage non supervisé peuvent être utilisées pour résoudre, entre autres, les problèmes suivants :le partitionnement de données (par exemple avec l'algorithme des k-moyennes, le regroupement hiérarchique),l'estimation de densité de distribution (distribution de mélange, estimation par noyau),la réduction de dimension (analyse en composantes principales, carte auto-adaptative)L'apprentissage non supervisé peut aussi être utilisé en conjonction avec une inférence bayésienne pour produire des probabilités conditionnelles pour chaque variable aléatoire étant donné les autres.K-means clustering (K-moyenne)Dimensionality Reduction (Réduction de la dimensionnalité)Principal Component Analysis (Analyse en composantes principales)Singular Value Decomposition (Décomposition en valeurs singulières)Independent Component Analysis (Analyse en composantes indépendantes)Distribution models (Modèles de distribution)Hierarchical clustering (Classification hiérarchique)Le regroupement ou Clustering est la technique la plus utilisée pour résoudre les problèmes d'apprentissage non supervisé. La mise en cluster consiste à séparer ou à diviser un ensemble de données en un certain nombre de groupes, de sorte que les ensembles de données appartenant aux mêmes groupes se ressemblent davantage que ceux d’autres groupes. En termes simples, l’objectif est de séparer les groupes ayant des traits similaires et de les assigner en grappes.Voyons cela avec un exemple. Supposons que vous soyez le chef d’un magasin de location et que vous souhaitiez comprendre les préférences de vos clients pour développer votre activité. Vous pouvez regrouper tous vos clients en 10 groupes en fonction de leurs habitudes d’achat et utiliser une stratégie distincte pour les clients de chacun de ces 10 groupes. Et c’est ce que nous appelons le Clustering.Le clustering consiste à grouper des points de données en fonction de leurs similitudes, tandis que l’association consiste à découvrir des relations entre les attributs de ces points de données:Les techniques de clustering cherchent à décomposer un ensemble d'individus en plusieurs sous ensembles les plus homogènes possiblesOn ne connaît pas la classe des exemples (nombre, forme, taille)Les méthodes sont très nombreuses, typologies généralement employées pour les distinguer  Méthodes de partitionnement / Méthodes hiérarchiquesAvec recouvrement / sans recouvrementAutre : incrémental / non incrémentalD'éventuelles informations sur les classes ou d'autres informations sur les données n'ont pas d'influence sur la formation des clusters, seulement sur leur interprétation.L'un des algorithmes le plus connu et utilisé en clustering est la K-moyenne.Cet algorithme va mettre dans des “zones” (Cluster), les données qui se ressemblent. Les données se trouvant dans le même cluster sont similaires.L’approche de K-Means consiste à affecter aléatoirement des centres de clusters (appelés centroids), et ensuite assigner chaque point de nos données au centroid qui lui est le plus proche. Cela s’effectue jusqu’à assigner toutes les données à un cluster. Portail de l’informatique   Portail des probabilités et de la statistique   Portail des données
Informatique;L'apprentissage supervisé (supervised learning en anglais) est une tâche d'apprentissage automatique consistant à apprendre une fonction de prédiction à partir d'exemples annotés, au contraire de l'apprentissage non supervisé. On distingue les problèmes de régression des problèmes de classement. Ainsi, on considère que les problèmes de prédiction d'une variable quantitative sont des problèmes de régression tandis que les problèmes de prédiction d'une variable qualitative sont des problèmes de classification.Les exemples annotés constituent une base d'apprentissage, et la fonction de prédiction apprise peut aussi être appelée « hypothèse » ou « modèle ». On suppose cette base d'apprentissage représentative d'une population d'échantillons plus large et le but des méthodes d'apprentissage supervisé est de bien généraliser, c'est-à-dire d'apprendre une fonction qui fasse des prédictions correctes sur des données non présentes dans l'ensemble d'apprentissage.Soit                     (        ?        ,                              A                          ,                  P                )              {\displaystyle (\Omega ,{\mathcal {A}},\mathbb {P} )}  , un espace probabilisé.Soit                     (                              X                          ,                                            F                                            X                          )        ,        (                              Y                          ,                                            F                                            Y                          )              {\displaystyle ({\mathcal {X}},{\mathcal {F}}_{X}),({\mathcal {Y}},{\mathcal {F}}_{Y})}   deux espaces mesurables. On peut définir une base de données d'apprentissage (ou ensemble d'apprentissage) comme un ensemble de couples entrée-sortie                     (                  x                      n                          ,                  y                      n                                    )                      1            ?            n            ?            N                                {\displaystyle (x_{n},y_{n})_{1\leq n\leq N}}   où chaque                               x                      n                          ?                              X                                {\displaystyle x_{n}\in {\mathcal {X}}}   et                               y                      n                          ?                              Y                                {\displaystyle y_{n}\in {\mathcal {Y}}}   sont des réalisations respectives des variables aléatoires                               X                      n                                {\displaystyle X_{n}}   et                               Y                      n                                {\displaystyle Y_{n}}  . Les couples de la suite                     (        (                  X                      n                          ,                  Y                      n                          )                  )                      n            ?            N                                {\displaystyle ((X_{n},Y_{n}))_{n\leq N}}   sont indépendants et identiquement distribués suivant la loi d'un couple                     (        X        ,        Y        )              {\displaystyle (X,Y)}   à valeurs dans                     (                              X                          ×                              Y                          ,                                            F                                            X                          ?                                            F                                            Y                          )              {\displaystyle ({\mathcal {X}}\times {\mathcal {Y}},{\mathcal {F}}_{X}\otimes {\mathcal {F}}_{Y})}  . On rappelle que cette loi est caractérisée par une mesure de probabilité                                           P                                (            X            ,            Y            )                                {\displaystyle \mathbb {P} _{(X,Y)}}   définie pour tout évènement                     A        ?                                            F                                            X                          ?                                            F                                            Y                                {\displaystyle A\in {\mathcal {F}}_{X}\otimes {\mathcal {F}}_{Y}}   par                                           P                                (            X            ,            Y            )                          (        A        )        =                  P                [        (        X        ,        Y                  )                      ?            1                          (        A        )        ]              {\displaystyle \mathbb {P} _{(X,Y)}(A)=\mathbb {P} [(X,Y)^{-1}(A)]}  Par exemple                               X                      n                                {\displaystyle X_{n}}   suit une loi uniforme et                               Y                      n                          =        f        (                  X                      n                          )        +                  ?                      n                                {\displaystyle Y_{n}=f(X_{n})+\epsilon _{n}}   où                               ?                      n                                {\displaystyle \epsilon _{n}}   est un bruit centré. Dans ce cas, la méthode d'apprentissage supervisé utilise cette base d'apprentissage pour déterminer une estimation de f notée g et appelée indistinctement fonction de prédiction, hypothèse ou modèle qui à une nouvelle entrée x associe une sortie g(x). Le but d'un algorithme d'apprentissage supervisé est donc de généraliser pour des entrées inconnues ce qu'il a pu « apprendre » grâce aux données déjà annotées par des experts, ceci de façon « raisonnable ». On dit que la fonction de prédiction apprise doit avoir de bonnes garanties en généralisation.Plus généralement, l'objectif de l'apprentissage supervisé est d'apprendre une fonction                     f              {\displaystyle f}   qui « minimise l'écart entre les variables aléatoires                     f        (        X        )              {\displaystyle f(X)}   et                     Y              {\displaystyle Y}   ». Pour définir cet écart, nous introduisons une fonction de perte                     L        :                              Y                          ×                              Y                          ?                              R                                +                                {\displaystyle L:{\mathcal {Y}}\times {\mathcal {Y}}\rightarrow \mathbb {R} _{+}}   qui quantifie la distance entre une prédiction du modèle                     f        (        x        )              {\displaystyle f(x)}   et une sortie attendue                     y              {\displaystyle y}  . À partir de cette fonction, nous pouvons définir le risque statistique d'une modèle                     f              {\displaystyle f}  . Il est noté                     R              {\displaystyle R}   et est défini par :En pratique, on n'a jamais accès directement à                                           P                                (            X            ,            Y            )                                {\displaystyle \mathbb {P} _{(X,Y)}}  , en revanche il est possible de l'estimer à partir du jeu de données en utilisant la mesure empirique                                           P                                (            X            ,            Y            )                                N                                {\displaystyle \mathbb {P} _{(X,Y)}^{N}}   définie pour tout                     A        ?                                            F                                            X                          ?                                            F                                            Y                                {\displaystyle A\in {\mathcal {F}}_{X}\otimes {\mathcal {F}}_{Y}}   par                                           P                                (            X            ,            Y            )                                N                          (        A        )        =                                            1              N                                                ?                      n            =            1                                N                                    ?                      (                          X                              n                                      ,                          Y                              n                                      )                          (        A        )              {\displaystyle \mathbb {P} _{(X,Y)}^{N}(A)={\dfrac {1}{N}}\sum _{n=1}^{N}\delta _{(X_{n},Y_{n})}(A)}  .Dès lors, un algorithme d'apprentissage supervisé mettra en œuvre des algorithmes d'optimisation afin de trouver une fonction                     f              {\displaystyle f}   qui minimise le risque empirique                               R                      N                          (        f        )        =                                            1              N                                                ?                      n            =            1                                N                          L        (                  Y                      n                          ,        f        (                  X                      n                          )        )              {\displaystyle R_{N}(f)={\dfrac {1}{N}}\sum _{n=1}^{N}L(Y_{n},f(X_{n}))}  . Il faut noter que                               R                      N                                {\displaystyle R_{N}}   n'est rien d'autre que la moyenne des écart (au sens de                     L              {\displaystyle L}  ) entre les prédictions du modèle et les sorties attendues.On distingue trois types de problèmes solubles avec une méthode d'apprentissage automatique supervisée :                                          Y                          ?                  R                      {\displaystyle {\mathcal {Y}}\subset \mathbb {R} }   : lorsque la sortie que l'on cherche à estimer est une valeur dans un ensemble continu de réels, on parle d'un problème de régression. La fonction de prédiction est alors appelée un régresseur.                                          Y                          =        {        1        ,        …        ,        I        }              {\displaystyle {\mathcal {Y}}=\{1,\ldots ,I\}}   : lorsque l'ensemble des valeurs de sortie est fini, on parle d'un problème de classification, qui revient à attribuer une étiquette à chaque entrée. La fonction de prédiction est alors appelée un classifieur.Lorsque                                           Y                                {\displaystyle {\mathcal {Y}}}   est un ensemble de données structurées, on parle d'un problème de prédiction structurée, qui revient à attribuer une sortie complexe à chaque entrée. Par exemple, en bio-informatique le problème de prédiction de réseaux d’interactions entre gènes peut être considéré comme un problème de prédiction structurée dans laquelle l'ensemble possible des sorties structurées est l'ensemble de tous les graphes modélisant les interactions possibles.Une bonne estimation de                     f              {\displaystyle f}   vérifierait                     f        (        X        )        =                  E                (        Y                  |                X        )              {\displaystyle f(X)=\mathbb {E} (Y|X)}  . On estimerait donc                     Y              {\displaystyle Y}   par son espérance conditionnelle par rapport à                     X              {\displaystyle X}  . Le théorème suivant montre l'intérêt d'utiliser la fonction de perte quadratique dans le cas d'une régression.BoostingMachine à vecteurs de supportMélanges de loisRéseau de neurones artificielsMéthode des k plus proches voisinsArbre de décisionClassification naïve bayésienneInférence grammaticaleEspace de versionsVision par ordinateurReconnaissance de formesReconnaissance de l'écriture manuscriteReconnaissance vocaleTraitement automatique de la langueBio-informatiqueReconnaissance optique de caractèresVincent Barra, Antoine Cornuéjols, Laurent Miclet, Apprentissage Artificiel : Concepts et algorithmes, Eyrolles, 2021 (ISBN 978-2-416-001-04-8) [détail des éditions](en) Tom M. Mitchell, Machine Learning, 1997 [détail des éditions](en) Christopher M. Bishop, Pattern Recognition And Machine Learning, Springer, 2006 (ISBN 0-387-31073-8) [détail des éditions] Portail des probabilités et de la statistique   Portail de l’informatique   Portail des données
Informatique;"L'informatique théorique est l'étude des fondements logiques et mathématiques de l'informatique. C'est une branche de la science informatique et la science formelle. Plus généralement, le terme est utilisé pour désigner des domaines ou sous-domaines de recherche centrés sur des vérités universelles (axiomes) en rapport avec l'informatique. L'informatique théorique se caractérise par une approche par nature plus mathématique et moins empirique de l'informatique et ses objectifs ne sont pas toujours directement reliés à des enjeux technologiques. De nombreuses disciplines peuvent être regroupées sous cette dénomination diffuse dont :la théorie de la calculabilité,l'algorithmique et la théorie de la complexité,la théorie de l'information,l'étude de la sémantique des langages de programmation,la logique mathématique,la théorie des automates et des langages formels.Dans cette section, nous donnons une histoire de l'informatique théorique en nous appuyant sur différentes sources :l'hommage à Maurice Nivat dans le bulletin de la SIF,l'histoire du CRIN,le livre Models of Computation de John E. Savage.Les logiciens Bertrand Russel, David Hilbert et George Boole furent des précurseurs de l'informatique théorique. Mais cette branche de l'informatique a surtout vu le jour à partir des travaux d'Alan Turing et Alonzo Church en 1936, qui ont introduit les modèles formels de calculs (les machines de Turing et le lambda calcul). Ils ont montré l'existence de machines universelles capables de simuler toutes les machines du même type, par exemple les machines de Turing universelles. En 1938, Claude Shannon montre que l'algèbre booléenne explique le comportement des circuits avec des relais électromécaniques. En 1945, John von Neumann introduit la notion d'architecture de von Neumann à la base des ordinateurs. En 1948, Claude Shannon publie A Mathematical Theory of Communication, fondateur de la théorie de l'information. En 1949, il annonce les fondements de la théorie de la complexité des circuits booléens.La programmation des ordinateurs de l'époque était difficile. Par exemple, il était nécessaire de brancher ou débrancher  une centaine de câbles sur l'ordinateur ENIAC afin de réaliser une simple opération.Une contribution importante des années 1950 est la création de langages de programmation comme FORTRAN, COBOL et LISP, qui offrent des constructions de haut-niveau pour écrire :des instructions conditionnelles,des structures de contrôle telles que des boucles de test,des procédures.La théorie des langages et des automates est un sujet important dans les années 1950, car il permet de comprendre l'expressivité des langages informatiques et leur mise en œuvre. Les machines à états finis et les automates à pile sont formellement définis. Puis Michael Oser Rabin et Dana Stewart Scott étudient mathématiquement le pouvoir expressif et les limites de ces modèles.En 1964, Noam Chomsky définit la hiérarchie de Chomsky. Plusieurs classes de langages (langages rationnels, langages algébriques, langages avec contextes, langages récursivement énumérables) correspondent à des types de machines théoriques différentes (automates finis, automates à pile, machine de Turing à mémoire linéaire, machine de Turing). Différentes variantes de ces classes de langages et machines sont étudiés.Hartmanis, Lewis et Stearns et d'autres classifient les langages selon le temps et/ou la mémoire qu'il faut pour les calculer. Ce sont les balbutiements de la théorie de la complexité.Durant les années 1970, la théorie de la complexité se développe. Les classes de complexité P et NP sont définis ; la NP-complétude est définie indépendamment par Stephen Cook et Leonid Levin. Richard Karp a démontré l'importance des langages NP-complets. La question P = NP est posée et les chercheurs conjecturaient que l'on pourrait la résoudre via la correspondance entre machines de Turing et circuits.Se développent aussi les méthodes formelles pour vérifier les programmes. On définit des sémantiques formelles aux langages de programmation.Se développement aussi des connections entre le modèle de base de données relationnelles et le calcul des relations, afin de réaliser des requêtes dans des bases de données de manière efficace.Ces années ont été florissantes également en algorithmique. Donald Knuth a beaucoup influencé le développement de l'algorithmique ainsi que Aho, Hopcroft et Ullman.Les années 1980 et 1990 sont propices au développement du calcul parallèle et des systèmes distribués.Il n'est pas facile de cerner précisément ce que l'on entend par « informatique théorique ». Le terme renvoie plutôt à une façon d'aborder les questions informatiques sous un angle plus mathématique et formel, en faisant souvent abstraction des aspects plus pratiques de l'informatique. En ce sens, l'informatique théorique est parfois considérée comme une branche des mathématiques discrètes. Ses objectifs se caractérisent généralement par une volonté d'identifier en principe les possibilités et les limites des ordinateurs.Le Special Interest Group on Algorithms and Computation Theory (SIGACT), regroupement affilié à l'Association for Computing Machinery (ACM) et voué au soutien à la recherche en informatique théorique en donne une définition assez large qui comprend des domaines aussi divers que l'algorithmique et les structures de données, la théorie de la complexité, le parallélisme, le VLSI, l'apprentissage automatique, la bio-informatique, la géométrie algorithmique, la théorie de l'information, la cryptographie, l'informatique quantique, la théorie algorithmique des nombres et de l'algèbre, la sémantique des langages de programmation, les méthodes formelles, la théorie des automates et l'étude de l'aléatoire en informatique.Les chercheurs en informatique théorique français sont regroupés au sein du GdR Informatique Mathématique et adhèrent à l'Association française d'informatique fondamentale, membre de la Société informatique de France au niveau français et membre de l'EATCS au niveau européen.La définition donnée par ACM SIGACT est à la fois trop restreinte en ce que la liste n'est pas exhaustive et trop large puisque plusieurs des domaines mentionnés ne sont pas uniquement axés sur des enjeux purement théoriques.Cette discipline tente de découvrir, d'améliorer et d'étudier de nouveaux algorithmes permettant de résoudre des problèmes avec une plus grande efficacité.Certains programmes sensibles nécessitent une parfaite fiabilité et de ce fait des outils mathématiques à mi-chemin entre l'algorithmique, la modélisation et l'algèbre sont développés afin de permettre de vérifier formellement les programmes et algorithmes.La théorie de l'information résulte initialement des travaux de Ronald A. Fisher. Ce statisticien théorise l'information dans sa théorie des probabilités et des échantillons. Techniquement, « l'information » est égale à la valeur moyenne du carré de la dérivée du logarithme de la loi de probabilité étudiée. À partir de l'inégalité de Cramer, la valeur d'une telle « information » est proportionnelle à la faible variabilité des conclusions résultantes. En d'autres termes, Fisher met l'information en relation avec le degré de certitude. D'autres modèles mathématiques ont complété et étendu de façon formelle la définition de l'information.Claude Shannon et Warren Weaver renforcent le paradigme. Ingénieurs en télécommunication, leurs préoccupations techniques les ont conduits à vouloir mesurer l'information pour en déduire les fondamentaux de la Communication (et non une théorie de l'information). Dans Théorie Mathématique de la Communication en 1948, ils modélisent l'information pour étudier les lois correspondantes : bruit, entropie et chaos, par analogie générale aux lois d'énergétique et de thermodynamique.Leurs travaux complétant ceux d'Alan Turing, de Norbert Wiener et de Von Neuman (pour ne citer que les principaux) constituent le socle initial des « Sciences de l'information ». La théorie de l'information s'appuie principalement sur deux notions caractéristiques que sont la variation d'incertitude et l'entropie (« désordre » au sens d'absence d'ordre et donc d'information dans l'ensemble considéré, d'où l'indétermination). Déclinant ces principes et ceux d'autres sciences dures, les technologies s'occupent de la façon d'implémenter, d'agencer et de réaliser des solutions pour répondre aux besoins des sociétés humaines.Certains chercheurs tentent de tirer des parallèles entre les concepts d'entropie en physique et d'entropie en informatique afin d'obtenir une formulation informatique de la cosmologie et de la réalité physique de notre monde qui, selon certains, pourraient trouver des clés dans des outils mathématiques que sont les automates cellulaires.Certains ne voient dans l'informatique qu'une déclinaison technologique de l'automatisation des traitements (incluant la transmission et le transport) d'information et considèrent l'usage des termes « sciences de l'informatique » comme incorrects, ces mêmes préfèrent donc l'appellation « technologies de l'information et de la communication » parce qu'ils disent qu'elle recouvre mieux les différents composants (systèmes de traitements, réseaux, etc.) de l'informatique au sens large. Cette vision très restrictive du terme « informatique » n'est pas partagée par tout le monde et d'ailleurs beaucoup d'anglo-saxons envient la richesse du mot « informatique » et commencent à l'emprunter,.La théorie des graphes permet de modéliser de nombreux problèmes discrets : calculs de trajets, allocations de ressource, problèmes SAT... On peut citer le théorème des quatre couleurs comme résultat classique de cette branche de l'informatique.La théorie de la complexité permet de classifier les algorithmes selon leur temps d'exécution asymptotique. C'est-à-dire selon leur comportement lorsqu'ils sont utilisés sur de très grandes données. C'est dans cette branche de l'informatique que se situe le célèbre problème P=NP par exemple.La théorie de la calculabilité a pour objet la caractérisation des fonctions qu'un algorithme peut calculer. En effet, il est possible de montrer qu'il existe des fonctions qui ne sont pas calculables par un ordinateur, et il est dès lors intéressant de savoir lesquelles le sont. Le problème du castor affairé ou la fonction d'Ackermann sont des exemples classiques d'objets étudiés dans ce domaine.La théorie des langages, souvent liée à la théorie des automates, s'intéresse à la reconnaissance d'ensemble de mots sur un vocabulaire donné. Elle est utilisée dans les algorithmes de traitement de la langue naturelle par exemple : traduction automatique, indexation automatique de documents, etc. ainsi que dans ceux des langues artificielles comme les langages de programmation : compilation, interprétation.La logique formelle est un outil fondamental de l'informatique, on y trouve notamment la théorie des types, le lambda calcul et la réécriture comme outils de base de la programmation fonctionnelle et des assistants de preuve.Discrete Mathematics and Theoretical Computer ScienceInformation and ComputationTheory of Computing (accès ouvert)Journal of the ACMSIAM Journal on Computing (SICOMP)SIGACT NewsTheoretical Computer ScienceTheory of Computing SystemsInternational Journal of Foundations of Computer ScienceFoundations and Trends in Theoretical Computer Science (en)Journal of Automata, Languages and CombinatoricsActa InformaticaFundamenta InformaticaeACM Transactions on AlgorithmsInformation Processing LettersOpen Computer Science (journal en accès ouvert)Lipton, Richard J. et Regan, Kenneth W., People, problems, and proofs. : Essays from Gödel’s lost letter: 2010, Berlin, Springer, 2013, XVIII-333 p. (ISBN 978-3-642-41421-3, zbMATH 1305.68025).Liste de publications importantes en informatique théorique Portail de l'informatique théorique   Portail de la logique   Portail des mathématiques   Portail de l’informatique"
Informatique;
"qui, à """;
Informatique;"L'interaction homme-machine, appelé IHM, s’intéresse à la conception et au développement de systèmes interactifs en prenant en compte ses impacts sociétaux et éthiques. Les humains interagissent avec les ordinateurs qui les entourent et cette interaction nécessite des interfaces qui facilitent la communication entre l'humain et la machine. La facilitation de l'utilisation de dispositifs devient de plus en plus importante avec le nombre croissant d'interfaces numériques dans la vie quotidienne. L'IHM a pour but de trouver les moyens les plus efficaces, les plus accessibles et les plus intuitifs pour les utilisateurs de compléter une tâche le plus rapidement et le plus précisément possible. L'IHM, s'appuie notamment sur la linguistique, sur la vision par ordinateur et sur l'humain.L'interaction homme-machine est un domaine pluridisciplinaire entre ingénierie (informatique, électronique, mécanique…), science de la nature (sciences cognitives, psychologie, sociologie…) et art et design (design de produit, design interactif, ergonomie…).L'histoire de l'interaction homme-machine est aussi vieille que l'histoire de l'informatique. En 1945, Vannevar Bush décrit un système électronique imaginaire qui permet la recherche d'information et qui invente les concepts de navigation, d'indexation et d'annotation. En 1963, Ivan Sutherland a créé Sketchpad qui est considéré comme l’ancêtre des interfaces graphiques modernes. En 1964, Douglas Engelbart invente la souris pour facilement désigner des objets sur son écran. Dans les années 1970 et 80, les laboratoires de Xerox ont révolutionné les systèmes interactifs avec la sortie de Xerox Star et la présentation de What you see is what you get. Au début des années 1990, Robert Cailliau et Tim Berners-Lee inventent un système hypertexte qui entourera la planète, World Wide Web. En 1991, Mark Weiser présente sa vision de l'Informatique ubiquitaire qui envisage des écrans et des ordinateurs multiples capables de communiquer entre eux pour permettre l'utilisateur à accéder à l'information en toute circonstance. Cette vision préfigure clairement l'avènement des assistants personnels, Tablet PC et smartphones d'aujourd'hui.Il existe de nombreuses manières pour qu'un humain puisse interagir avec les machines qui l'entourent. Ces manières sont très dépendantes des dispositifs d'interactions et des forces ou compétences que l'être humain ne peut étendre qu’extérieurement.L'informatique a évolué très rapidement de ses débuts dans les années 1940 à aujourd'hui. Organes d'entrée Les premiers ordinateurs étaient utilisés sous forme de traitement par lots et toutes les entrées (programmes et données) étaient alimentées en entrée par des cartes perforées, des rubans perforés ou des bandes magnétiques. Il y avait un clavier pour interagir avec le système (console système).Avec l'arrivée de la micro-informatique, on a commencé à utiliser des cassettes audio et des claviers, puis des disquettes et des souris informatiques avant de passer aux écrans tactiles. Un système de pointage tel que la souris permet d'utiliser un ordinateur avec le paradigme WIMP qui s'appuie sur les interfaces graphiques pour organiser la présentation d'informations à l'utilisateur.Enfin, avec les assistants personnels intelligents, la voix devient un organe d'entrée intéressant en raison du taux potentiel de mots par minute qu'elle permet. Organes de sortie Les premiers organes de sorties ont été les imprimantes, les perforateurs de cartes et les perforateurs de ruban secondés ensuite par bandes magnétiques. La console système était équipée d'une imprimante, remplacée par la suite par un écran.Avec l'arrivée de la micro-informatique, on a utilisé d'abord des cassettes audio, puis des disquettes avant d'utiliser des CD puis des DVD. Organes interactifs Certaines techniques tentent de rendre l'interaction plus naturelle :la reconnaissance automatique de la parole ou de gestes permet d'envoyer des informations à un ordinateur ;la synthèse vocale permet d'envoyer un signal audio compréhensible par l'être humain ;les gants électroniques offrent une interaction plus directe que la souris ;les visiocasques essayent d'immerger l'être humain dans une réalité virtuelle ou d'augmenter la réalité ;les tables interactives permettent un couplage fort entre la manipulation directe par l'être humain sur une surface et le retour d'information.Dans le domaine de l'automatisation, les écrans tactiles sont des IHMs très populaires afin de centraliser le contrôle d'un procédé sur un seul écran. Ainsi, il est possible d'afficher plusieurs informations et de mettre à la disposition de l'opérateur des commandes qui affecteront le procédé. Les IHMs permettent aussi de remplacer des stations de boutons. Ils sont surtout utilisés en complément avec un API (automate programmable industriel) pour avoir un affichage des états des entrées/sorties et des alarmes du système.En informatique industrielle, les automates sont encore très souvent pilotés par des baies équipées de boutons poussoirs et de voyants. Les systèmes autonomes de type véhicules automatiques et drones tendent à peu à peu à intégrer une « interface adaptative », voire une intelligence artificielle embarquée.Dans l'automobile, l'être humain a, d'abord, interagi avec de simples moyens mécaniques. L'évolution de l'informatique et de la robotique fait que de plus en plus de capteurs et d'informations sont disponibles pour le conducteur qui doit choisir l'action à effectuer par l'intermédiaire :du volant ;de la pédale de frein ;d'interrupteurs divers (éclairage, régulateur de vitesse, etc.).On peut observer que les IHM sont de plus en plus déconnectées de l'implémentation réelle des mécanismes contrôlés. Dans son article de 1995, The Myth of Metaphor, Alan Cooper distingue trois grands paradigmes d'interface :Le paradigme technologique : l'interface reflète la manière dont le mécanisme contrôlé est construit. Cela conduit à des outils très puissants mais destinés à des spécialistes qui savent comment fonctionne la machine à piloter.Le paradigme de la métaphore qui permet de mimer le comportement de l'interface sur celui d'un objet de la vie courante et donc déjà maîtrisé par l'utilisateur. Exemple : la notion de document.Le paradigme idiomatique qui utilise des éléments d'interface au comportement stéréotypé, cohérent et donc simple à apprendre mais pas nécessairement calqué sur des objets de la vie réelle.L'interaction est dite multimodale si elle met en jeu plusieurs modalités sensorielles et motrices. Un système interactif peut contenir un ou plusieurs de ces modes d'interaction :Mode parlé : commandes vocales, guides vocaux…Mode écrit : entrées par le clavier et la tablette graphique, affichage du texte sur l'écran…Mode gestuel : désignation 2D ou 3D (souris, gants de données, écran tactile), retour d'effort…Mode visuel : graphiques, images, animations…D'un point de vue organique, on peut distinguer trois types d'IHM :Les interfaces d'acquisition : bouton, molette, souris, clavier accord, joystick, clavier d'ordinateur, clavier MIDI, télécommande, capteur de mouvement, microphone avec la reconnaissance vocale, etc.Les interfaces de restitution : écran, témoin à LED, voyant d'état du système, haut parleur, etc.Les interfaces combinées : écran tactile, multi-touch et les commandes à retour d'effort.Ce domaine évolue vers une interface plus large et pervasive de type « humain-environnement ».« Il serait sot de nier l'importance de la communication efficace entre l'homme et la machine, aussi bien que l'inverse. Ma prévision est toutefois que la vraie révolution des prochaines décennies viendra davantage encore de ce que les hommes ont à se dire par l'intermédiaire des machines. »— James Cannavino, The Next Generation of Interactive Technologies (Juillet 1989)L'immersion dans les mondes virtuels devrait également être rendue plus « réaliste ».Des jeux comme Le Deuxième Monde, Everquest ou Wolfenstein: Enemy Territory, où plusieurs joueurs évoluent en immersion globale dans un paysage commun, donnent une idée des nouvelles relations que peuvent mettre en place des interfaces réalistes.La plus grande association d'IHM est le pôle d'intérêt commun SIGCHI de l'Association for Computing Machinery (ACM). SIGCHI organise les conférences Conference on Human Factors in Computing Systems (CHI), MobileHCI, TEI et plusieurs autres.En France, l'association francophone d'Interaction humain-machine (AFIHM) organise la Conférence francophone IHM tous les ans. L'AFIHM parraine diverses manifestations et en particulier des Écoles d'été et les Rencontres Jeunes Chercheurs en Interaction (RJC-IHM). Portail de l’informatique"
Informatique;
"e la dé""";
Informatique;"L’optimisation combinatoire, (sous-ensemble à nombre de solutions finies de l'optimisation discrète), est une branche de l'optimisation en mathématiques appliquées et en informatique, également liée à la recherche opérationnelle, l'algorithmique et la théorie de la complexité.Dans sa forme la plus générale, un problème d'optimisation combinatoire (sous-ensemble à nombre de solutions finies de l'optimisation discrète) consiste à trouver dans un ensemble discret un parmi les meilleurs sous-ensembles (ou solutions) réalisables, la notion de meilleure solution étant définie par une fonction objectif. Formellement, étant donnés :un ensemble discret                     N              {\displaystyle N}   fini;une fonction d'ensemble                     f        :                  2                      N                          ?                  R                      {\displaystyle f:2^{N}\rightarrow \mathbb {R} }  , dite fonction objectif ;et un ensemble                                           R                                {\displaystyle {\mathcal {R}}}   de sous-ensembles de                     N              {\displaystyle N}  , dont les éléments sont appelés les solutions réalisables,un problème d'optimisation combinatoire consiste à déterminer                              max                      S            ?            N                          {        f        (        S        )        :                S        ?                              R                          }        .              {\displaystyle \max _{S\subseteq N}\{f(S):\,S\in {\mathcal {R}}\}.}  L'ensemble des solutions réalisables ne saurait être décrit par une liste exhaustive car la difficulté réside ici précisément dans le fait que le nombre des solutions réalisables rend son énumération impossible, ou bien qu'il est NP-complet de dire s'il en existe ou non. La description de                                           R                                {\displaystyle {\mathcal {R}}}   est donc implicite, il s'agit en général de la liste, relativement courte, des propriétés des solutions réalisables.La plupart du temps, on est dans les cas particuliers suivants :                    N        =        {        1        ,        2        ,        …        ,        n        }              {\displaystyle N=\{1,2,\ldots ,n\}}   et                                           R                          =        X        ?                              Z                                n                                {\displaystyle {\mathcal {R}}=X\cap \mathbb {Z} ^{n}}   où                     X        ?                              R                                n                                {\displaystyle X\subset \mathbb {R} ^{n}}   et la cardinalité de                                           R                                {\displaystyle {\mathcal {R}}}   est exponentielle en                     n              {\displaystyle n}   ;                                          R                          =        c        o        n        v        (                              R                          )        ?                              Z                                n                                {\displaystyle {\mathcal {R}}=conv({\mathcal {R}})\cap \mathbb {Z} ^{n}}   donc les propriétés de                                           R                                {\displaystyle {\mathcal {R}}}   se traduisent en contraintes linéaires, c'est-à-dire que                     X              {\displaystyle X}   et un polyèdre de                                           R                                n                                {\displaystyle \mathbb {R} ^{n}}   ;                    f        (        S        )        =                  ?                      s            ?            S                          f        (        s        )              {\displaystyle f(S)=\sum _{s\in S}f(s)}  .Le problème du voyageur de commerce (TSP) où un voyageur de Commerce doit parcourir les N villes d'un pays en effectuant le trajet le plus court. Une résolution par énumération nécessite de calculer ((N-1)!)/2 trajets entre des villes. En particulier, pour 24 villes, il faudrait en générer (23!)/2                     ?              {\displaystyle \approx }   2,5×1022. Pour fournir un ordre de grandeur comparatif, une année ne compte qu'environ 3,16×1013 microsecondes.Trouver une solution optimale dans un ensemble discret et fini est un problème facile en théorie : il suffit d'essayer toutes les solutions, et de comparer leurs qualités pour voir la meilleure. Cependant, en pratique, l'énumération de toutes les solutions peut prendre trop de temps ; or, le temps de recherche de la solution optimale est un facteur très important et c'est à cause de lui que les problèmes d'optimisation combinatoire sont réputés si difficiles. La théorie de la complexité donne des outils pour mesurer ce temps de recherche. De plus, comme l'ensemble des solutions réalisables est défini de manière implicite, il est aussi parfois très difficile de trouver ne serait-ce qu'une solution réalisable.Quelques problèmes d'optimisation combinatoire peuvent être résolus (de manière exacte) en temps polynomial par exemple par un algorithme glouton, un algorithme de programmation dynamique ou en montrant que le problème peut être formulé comme un problème d'optimisation linéaire en variables réelles.Dans la plupart des cas, le problème est NP-difficile et, pour le résoudre, il faut faire appel à des algorithmes de branch and bound, à l'optimisation linéaire en nombres entiers ou encore à la programmation par contraintes.En pratique, la complexité physiquement acceptable n'est souvent que polynomiale. On se contente alors d'avoir une solution approchée au mieux, obtenue par une heuristique ou une métaheuristique.Pour le problème du VRP envisagé à large échelle, Richard Karp zone les villes, résout le problème zone par zone, puis assemble au mieux les parcours locaux.Pour un problème d'optimisation d'affectation d'équipages, l'optimisation linéaire en variables réelles dégrossit le problème : on considère comme définitives les valeurs des variables ainsi collées aux extrêmes de leur domaine, et on n'applique les méthodes d'optimisation combinatoire que sur le problème résiduel.Pour certains problèmes, on peut prouver une garantie de performance, c'est-à-dire que pour une méthode donnée l'écart entre la solution obtenue et la solution optimale est borné. Dans ces conditions, à domaine égal, un algorithme est préférable à un autre si, à garantie égale ou supérieure, il est moins complexe. Portail des mathématiques   Portail de l'informatique théorique"
Informatique;"Un robot est un dispositif mécatronique (alliant mécanique, électronique et informatique) conçu pour accomplir automatiquement des tâches imitant ou reproduisant, dans un domaine précis, des actions humaines. La conception de ces systèmes est l'objet d'une discipline scientifique, branche de l'automatisme nommée robotique.Le terme robot apparaît pour la première fois dans la pièce de théâtre (science-fiction) R. U. R. (Rossum's Universal Robots), écrite en 1920 par l'auteur Karel ?apek. Le mot a été créé par son frère Josef à partir du mot tchèque « robota » qui signifie « travail, besogne, corvée ».Les premiers robots industriels apparaissent, malgré leur coût élevé, au début des années 1970. Ils sont destinés à exécuter certaines tâches répétitives, éprouvantes ou toxiques pour un opérateur humain : peinture ou soudage des carrosseries automobiles. Aujourd'hui, l'évolution de l'électronique et de l'informatique permet de développer des robots plus précis, plus rapides ou avec une meilleure autonomie. Industriels, militaires ou spécialistes chirurgicaux rivalisent d'inventivité pour mettre au point des robots assistants les aidant dans la réalisation de tâches délicates ou dangereuses. Dans le même temps apparaissent des robots à usages domestiques : aspirateur, tondeuses, etc.L'usage du terme « robot » s'est galvaudé pour prendre des sens plus larges : automate distributeur, dispositif électro-mécanique de forme humaine ou animale, logiciel servant d'adversaire sur les plateformes de jeu, bot informatique.Le terme robot est issu des langues slaves et formé à partir du radical rabot, rabota (?????? en russe) qui signifie travail, corvée que l'on retrouve dans le mot Rab (???), esclave en russe. Ce radical présent dans les autres langues slaves (ex. : travailleur = robotnik en polonais, ???????? en biélorusse, pracovník en tchèque) provient de l'indo-européen *orbho- qui a également donné naissance au gotique arbais signifiant besoin, nécessité, lui-même source de l'allemand Arbeit, travail.Il fut initialement utilisé par l’écrivain tchécoslovaque Karel ?apek dans sa pièce de théâtre R. U. R. (Rossum's Universal Robots), écrite en 1920. Cette pièce fut jouée pour la première fois en public au Théâtre national à Prague le 25 janvier 1921. Bien que Karel ?apek soit souvent considéré comme l’inventeur du mot, il a lui-même désigné son frère Josef, peintre et écrivain, comme étant l’inventeur réel du terme.Ainsi certains assurent que le mot robot fut d’abord utilisé dans la courte pièce Opilec de Josef ?apek (The Drunkard), publiée dans la collection Lelio en 1917. Selon la Société des frères ?apek à Prague, ce serait néanmoins inexact. Le mot employé dans Opilec est automate, alors que c'est bien dans R.U.R. que le mot robot est apparu pour la première fois.Alors que les « robots » de Karel ?apek étaient des humains organiques artificiels, le mot robot fut emprunté pour désigner des humains « mécaniques ». Le terme androïde peut signifier l’un ou l’autre, alors que le terme cyborg (« organisme cybernétique » ou « homme bionique ») désigne une créature faite de parties organiques et artificielles.Quant au terme robotique, il fut introduit dans la littérature en 1942 par Isaac Asimov dans son livre Runaround. Il y énonce les « trois règles de la robotique » qui deviendront, par la suite, dans les œuvres de sciences fiction les Trois lois de la robotique.Un robot est un assemblage complexe de pièces mécaniques, électro-mécanique ou pièces électroniques. L'ensemble est piloté par une unité centrale appelée « système embarqué » : une simple séquence d'automatisme, un logiciel informatique ou une intelligence artificielle suivant le degré de complexité des tâches à accomplir. Lorsque les robots autonomes sont mobiles, ils possèdent également une source d'énergie embarquée : généralement une batterie d'accumulateurs électriques ou un générateur électrique couplé à un moteur à essence pour les plus énergivore.Les principales sortes de capteurs sont :Les sondeurs (ou télémètres) à ultrason ou Laser. Ces derniers sont à la base des scanners laser permettant à l'unité centrale du robot de prendre « conscience » de son environnement en 3D.Les caméras sont les yeux des robots. Il en faut au moins deux pour permettre la vision en trois dimensions. Le traitement automatique des images pour y détecter les formes, les objets, voire les visages, demande en général un traitement matériel car les microprocesseurs embarqués ne sont pas assez puissants pour le réaliser.Les roues codeuses permettent au robot se déplaçant sur roues, des mesures de déplacement précises en calculant les angles de rotation (information proprioceptive).Les microprocesseurs ou les microcontrôleurs sont des éléments essentiel du système de pilotage d'un robot. Ils permettent l'exécution de séquences d'instruction ou de logiciels commandant la réalisation d'actions ou de fonctions du robot. On trouve souvent, dans les robots de petite taille, des composants à très faible consommation électrique, car ils ne peuvent emporter que des sources d'énergie limitées.Les actionneurs les plus communs sont :des moteurs électriques rotatifs, qui sont fréquemment associés à des réducteurs mécaniques à engrenages.des vérins pneumatique, plus rarement hydraulique, alimentés par une pompe et permettant des actions toniques.Un actionneur est le constituant d'un système mécanique (exemple : bras, patte, roue motrice…) réalisant une action motrice suivant un degré de liberté. Il anime les interfaces haptiques réalisant les actions de saisies d'objets dans les applications de télémanipulation.Une certaine capacité d'adaptation à un environnement inconnu peut, dans les systèmes semi-autonomes actuels, être assurée pourvu que l'inconnu reste relativement prévisible : l'exemple déjà opérationnel du robot aspirateur en est une bonne illustration : le logiciel qui pilote cet appareil est en mesure de réagir aux obstacles qui peuvent se rencontrer dans une habitation, de les contourner, de les mémoriser. Il sauvegarde le plan de l'appartement et peut le modifier en cas de besoin. Il retourne en fin de programme se connecter à son chargeur. Il doit donc fournir une réponse correcte au plus grand nombre possible de stimulations, qui sont autant de données entrées, non par un opérateur, mais par l'environnement.L'autonomie suppose que le programme d'instructions prévoit la survenue de certains événements, puis la ou les réactions appropriées à ceux-ci. Lorsque l'aspirateur évite un buffet parce qu'il sait que le buffet est là, il exécute un programme intégrant ce buffet, par exemple les coordonnées X-Y de son emplacement. Si ce buffet est déplacé ou supprimé, le robot est capable de modifier son plan en conséquence et de traiter une zone du sol qu'il ne prenait pas en compte jusqu'alors.Les ancêtres des robots sont les automates.Le premier automate est le pigeon volant d'Archytas de Tarente aux alentours de 400 av.J.-C. Un automate très évolué fut présenté par Jacques de Vaucanson en 1738 : il représentait un homme jouant d’un instrument de musique à vent. Jacques de Vaucanson créa également un automate représentant un canard mangeant et refoulant sa nourriture après ingestion de cette dernière.Unimate est le premier robot industriel créé. Il fut intégré aux lignes d'assemblage de General Motors en 1961.En 1970, le robot lunaire Lunokhod 1, envoyé par l'Union soviétique, a voyagé sur une distance de 10 km et a transmis plus de 20 000 images.Le 25 octobre 2017, Sophia est le premier robot à avoir une nationalité. Avec l'obtention de la nationalité saoudienne,, cela a suscité la controverse, car il n'est pas évident de savoir si cela implique que Sophia peut voter ou se marier, ou si un arrêt délibéré du système peut être considéré comme un meurtre.La robotique possède de nombreux domaines d'application. Les robots ont été installés dans les industries, ce qui permet de faire des tâches répétitives avec une précision constante. À la suite de l'évolution des techniques on retrouve des robots dans des secteurs de pointe tels que le spatial, médecine, chez les militaires.Depuis quelques années on les retrouve même à domicile.L'image d'êtres automatisés est ancienne, des traces étant présentes dès l’Antiquité gréco-romaine. Pour autant, le sujet a largement évolué, allant du mythe de la création d'êtres humains par les hommes à la prise de pouvoir de ces êtres artificiels, et allant de l'utilisation des matériaux basiques (boue, morceaux humains) à l'utilisation des techniques et sciences modernes. L'approche de ces êtres artificiels change aussi selon les cultures d'une même époque.Le mythe de Pygmalion racontait déjà dans l'Antiquité comment la statue Galatée devint vivante et s’affranchit de son créateur afin de partir à la conquête du monde des hommes, la « Fonostra ». Il ne s'agit toutefois pas d'un robot au sens propre du terme, puisque Galatée n'a pas été conçue pour être autonome. Son autonomie est le fruit de la volonté divine, et non de celle de son créateur ; elle ne dépend ni de l'intelligence de celui-ci, ni des mécanismes (inexistants) qui la composent.Le premier exemple d’un robot de forme humaine fut donné par Léonard de Vinci en 1495. Ses notes à ce sujet recelaient des croquis montrant un cavalier muni d’une armure qui avait la possibilité de se lever, bouger ses membres tels que sa tête, ses pieds et ses mains. Le plan était probablement basé sur ses recherches anatomiques compilées dans l’homme vitruvien. On ne sait pas s’il a tenté de construire ce robot.Lorsque la technologie arriva au point où l’on put préfigurer des créatures mécaniques, les réponses littéraires au concept de robot suscitèrent la crainte que les humains soient remplacés par leurs propres créations.Frankenstein (1818), parfois désigné comme le premier roman de science-fiction, est devenu un synonyme de ce thème. Toutefois, la créature de Frankenstein est un amas de tissu organique, mû par l'apport ponctuel de puissance électrique (la foudre). Le robot n'est pas encore apparu comme tel.La nouvelle L'Homme épingle d'Hermann Mac Coolish Rotenberg Caistria (1809) raconte l’histoire d’un homme qui désirait se transformer en robot par amour pour sa machine à coudre, et Steam Man of the Prairies d’Edward S. Ellis (1865) exprime la fascination américaine de l’industrialisation. La littérature concernant la robotique connut des sommets notables avec l’Homme électrique de Luis Senarens en 1885.En France, le roman L'Ève future de Villiers de L'isle-Adam en 1883 tourne autour de la figure moderne du robot : création métallique, mobile par électricité, et autonome. Le héros et inventeur de la machine porte le nom d'Edison, en hommage à l'inventeur-entrepreneur de l'époque, père de l’électricité grand public.En 1900, la littérature enfantine et les illustrations de W.W. Denslow laissent apparaître dans Le_Magicien_d'Oz un bûcheron de fer-blanc comme un robot. En littérature Le mot robot est créé par Karel ?apek, dans sa pièce de théâtre : R. U. R. (Rossum's Universal Robots), mise en scène à Prague en 1921. Dans une petite île, un industriel humain a créé une chaîne de montages d'où sortent des serviteurs de métal, pour être envoyés partout dans le monde. Les robots se révolteront, prenant le contrôle de leur chaîne de montage, et chercheront à construire toujours plus de robots.Le thème prit donc une consonance économique et philosophique.La littérature de science-fiction ou de bande dessinée autour du thème des robots est foisonnante. Un certain nombre d'auteurs (essentiellement de science-fiction, et parfois ayant une réelle connaissance scientifique du sujet tel Isaac Asimov) ont donné une place particulière aux robots dans leurs ouvrages. Isaac Asimov est le premier à utiliser le mot robotique en 1941. Dans ses nombreux romans où apparaissent des robots (regroupés dans Le Grand Livre des robots), il s'intéressa tout particulièrement à leur interaction avec la société et à la manière dont cette dernière les accepte. Certains de ces romans ont d'ailleurs fait l'objet d'une adaptation cinématographique. Exemples :Isaac Asimov (Qui est également l'inventeur de la notion de robotique avant même que cette science ne soit reconnue)Les Robots, 1967 ((en) I, Robot, 1950), trad. Pierre Billon  (ISBN 978-2-290-34248-0, 2-290-31290-8, 2-277-13453-8 et 2-277-12453-2)Un défilé de robots, 1967 ((en) The Rest of the Robots, 1964), trad. Pierre Billon  (ISBN 978-2-277-12542-6 et 2-290-31125-1)Nous les robots, 1982 ((en) The Complete robot, 1982)  (ISBN 2-258-03291-1)Le Robot qui rêvait, 1988 ((en) Robot Dreams, 1986), trad. France-Marie Watkins  (ISBN 978-2-277-22388-7 et 2-290-31715-2)Les Cavernes d'acier, 1956 ((en) The Caves of Steel, 1953), trad. Jacques Brécard  (ISBN 978-2-277-12404-7 et 2-290-32794-8)Face aux feux du soleil, 1961 ((en) The Naked Sun, 1956), trad. André-Yves Richard  (ISBN 978-2-277-12468-9 et 2-290-32794-8)Les Robots de l'aube, 1984 ((en) Robots of Dawn, 1983), trad. France-Marie Watkins  (ISBN 2-290-33275-5)Les Robots et l'Empire, 1986 ((en) Robots and Empire, 1985), trad. Jean-Paul Martin  (ISBN 978-2-277-21996-5, 2-277-21996-7 et 2-290-31116-2)Douglas AdamsLe Guide du voyageur galactique, 1982 ((en) The Hitchhiker's Guide to the Galaxy, 1979), trad. Jean Bonnefoy, avec Marvin, son robot dépressif  (ISBN 2-207-30340-3).Philip K. Dick avec Le Grand O, James P. Crow, Service avant achat, Au service du maître, L'Ancien Combattant, Le Canon, Autofab (présence d'I.A.), Nanny, La Fourmi électrique, Nouveau Modèle, L'Imposteur, Progéniture...Les androïdes rêvent-ils de moutons électriques ?, 1976 ((en) Do Androids Dream of Electric Sheep ?, 1968), trad. Serge Quadruppani qui a inspiré le film Blade Runner  (ISBN 2-85184-066-5).Fredric BrownDeuxième chance, dans le recueil Fantômes et Farfafouilles  (ISBN 2-207-30065-X).Stanislas LemLe Bréviaire des robots, Denoël, coll. Présence du futur n° 96, 1967 ((pl) , 1961), trad. Halina Sadowska  (ISBN 2-07-034105-4)Contes inoxydables, Denoël, coll. Présence du futur n° 330, 1981 ((pl) Bajki robotów, 1964), trad. Dominique Sila  (ISBN 2-207-50330-5)Pierre BoulleLe Parfait Robot, dans le recueil Contes de l'absurde  (ISBN 2-266-00609-6).Jean-Pierre Andrevon dans de nombreuses nouvelles.Enrico Grassani, Automi. Passato, presente e futuro di una nuova ""specie"", Editoriale Delfino, Milano 2017,  (ISBN 978-88-97323-66-2) Au cinéma Les robots sont présents dans de nombreuses œuvres cinématographiques. Ces robots peuvent être des ennemis de l'Homme (par exemple dans Terminator), parfois trop intelligents pour rester des serviteurs (2001, l'Odyssée de l'espace, Blade Runner). Ces robots peuvent pourtant aussi être foncièrement bons, comme le sont R2-D2 et C-3PO dans Star Wars (1977), ou les robots de L'Homme bicentenaire et I, Robot (deux films adaptés de nouvelles d'Isaac Asimov).Citons aussi le film classique Metropolis (1927). Mais également Short Circuit, Matrix (les sentinelles), WALL-E, Robots, Transformers.Les protagonistes sont dans Robots (film, 2005).Un androïde dans Enthiran.La robotique dans Robots (film, 1988) (en).Dans Runaway : L'Évadé du futur il y a un futur avec des robots.Le robot géant dans The Mechanical Man (en).Des robots sont présents comme le ED-209 dans RoboCop.David qui est un enfant androïde dans A.I. Intelligence artificielle.La femme androïde dans Ex machina (film).Des robots deviennent incontrôlable dans Shopping (film, 1986).Dans le film ""Automatic"" de 1995, il y a un androïde héroïque joué par Olivier Gruner.Un androïde nommé Solo dans Le Guerrier d'acier.Dans Robosapien: Rebooted il y a un Robosapien qui est ami avec un môme nommé Henry.Les courts métrage de robots dans Robot Carnival. Dans la culture populaire Plusieurs séries télévisées comportent un certain nombre de robots ou d'androïdes. On peut ainsi citer les Réplicateurs de Stargate SG-1, les Cybermen de Doctor Who, les hubots de Real Humans (Äkta människor), ou encore les Cylons de Battlestar Galactica. Dans chaque univers, le robot a une place différente. Ainsi, les hubots de Real Humans ont découvert la notion de liberté de pensée et veulent s'affranchir des humains, tandis que les robots de la série Futurama vivent au sein même de leur société sans relation d'infériorité.Il existe aussi des mangas traitant le sujet (Astro, le petit robot, Dragon Ball Z, Medabots) ainsi que de nombreux jeux vidéo (Megaman, Sonic the hedgehog).Enfin, la série Il était une fois... l'Espace en présente de nombreux, soit hostiles soit grandement utiles. Leur présence permet de réfléchir en profondeur sur le libre-arbitre et la volonté d'indépendance.Des robots qui vivent avec des humains dans Cubix.Dans Zentrix, il y a des robots artificiels dans les combats.Dans Mon robot et moi, il y a des écoliers avec des robots.Dans Anatane et les enfants d'Okura, les robots enlèvent des jeunes hommes masculins pour les zombifier soit les changer en criminel de guerre.Des robots Faro dans le jeu vidéo Horizon Zero Dawn.Des androïdes, appelés « hôtes » (hosts) dans Westworld (série télévisée).Dans Le Maître des bots, il y a des robots intelligents comme des humains.Un robot géant dans Goldorak.Un robot géant dans Mazinger Z.Des robots géants dans Gundam Wing.Dans Chair de poule (collection), ""La Punition de la mort"" avec des doubles robotiques.John le robot dans ""Voyage sur la planète préhistorique"".Elias le robot dans ""Starcrash : Le Choc des étoiles"".Une série culte japonaise avec des robots Ganbare!! Robocon (en).Un robot géant dans Giant Robo (en).Isaac, Asimov, I, robot, Bantam Books, 2004, 304 p. (ISBN 978-0-553-90033-0 et 0553900331, OCLC 233705973, lire en ligne)Laurence Devillers, Des robots et des hommes : mythes, fantasmes et réalité, Paris, Plon, 2017, 236 p. (ISBN 978-2-259-25227-0 et 2259252273, OCLC 975286502)(en) Ritter, Helge, Human centered robot systems : cognition, interaction, technology, Berlin/Heidelberg, Springer, 2009, 217 p. (ISBN 978-3-642-10403-9 et 3642104037, OCLC 505433316, lire en ligne)Serge Tisseron, Le jour où mon robot m'aimera : vers l'empathie artificielle, Albin Michel, 2015, 208 p. (ISBN 978-2-226-38232-0 et 2226382321, OCLC 990819243)Ressource relative à la littérature : (en) The Encyclopedia of Science Fiction Ressource relative à la bande dessinée : (en) Comic Vine Actualité en Robotique et tutoriels : Zone Robotique Les recherches en robotique au CNRSVidéo : Conférence sur les Robots - Futuroscope - 22 décembre 2012Le robot a fêté ses 85 ans ! sur le site de Radio Praguehttps://video.vice.com/en_us/video/slutever-harmony-the-sex-robot/5aa6edcbf1cdb36f616c77a2%3Fjwsource%3Dclhttps://nixxons.fandom.com/wiki/Tara_(android)https://muc.fandom.com/wiki/Tara_the_Androidhttps://www.rts.ch/play/radio/le-journal-du-matin/audio/les-paysans-de-vinci-25-les-androdes-de-monsieur-wu?id=6993865http://french.peopledaily.com.cn/VieSociale/n3/2016/1117/c31360-9143061.htmlhttps://edition.cnn.com/2019/11/08/tech/mit-cheetah-robot-herd/index.htmlhttps://www.cs.cmu.edu/~coral-downloads/legged/movies/index.htmlhttps://www.newsbytesapp.com/news/science/watch-mit-robots-play-soccer-and-trip-excitedly/storyhttps://techcrunch.com/2019/11/09/watch-mits-mini-cheetah-robots-frolic-fall-flip-and-play-soccer-together/?guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&guce_referrer_sig=AQAAANH7n2ThssxdOlErjsMVigCGnoLnrRBTW9g1Bybh5KALAN-jZJvBLS5Ncs_qncr_i397tPsE9nCJBpwYTMi7JzzukqTgRsbrtVWMjC7rqlzVahCSI0kEkne3yRte5KY4Jhoy5W6BqW7v4AcmKxMNGc8lmZOkaDy4e7Bfco0s_qyg&guccounter=2https://abcnews.go.com/US/mits-adorable-back-flipping-robots-enjoy-frolicking-autumn/story?id=66853771https://techxplore.com/news/2019-11-fall-madness-mit-mini-cheetah.htmlhttps://tvmag.lefigaro.fr/programme-tv/programme/la-science-face-au-terrorisme-f156613689https://www.programme-tv.net/programme/culture-infos/r1549459761-la-science-face-au-terrorisme/16230392-la-robotique/https://objetconnecte.net/terrorisme-daesh-robots-futur/https://blog.francetvinfo.fr/bureau-londres/2017/12/07/des-robots-tueurs-seront-bientot-entre-les-mains-des-terroristes-selon-la-chambre-des-lords.htmlhttps://www.lesechos.fr/2015/07/robots-tueurs-la-mise-en-garde-des-grands-noms-de-la-tech-268609https://www.20minutes.fr/high-tech/1835379-20160427-chine-devoile-anbot-premier-robot-anti-terroriste-anti-emeutehttps://www.allocine.fr/tag-1245/films/https://www.senscritique.com/top/resultats/Les_meilleurs_films_avec_des_robots/785910https://www.senscritique.com/liste/Des_robots_plein_de_robots/1138091https://www.senscritique.com/liste/Les_Robots_au_cinema/141687https://www.senscritique.com/liste/Le_robot_au_cinema/539769https://www.cinetrafic.fr/top/film/robothttps://www.cinetrafic.fr/liste-film/1343/1/les-robots-et-les-androideshttps://www.tomsguide.fr/30-robots-qui-ont-marque-lhistoire-du-cinema/https://www.vodkaster.com/listes-de-films/les-robots-au-cinema/732069https://www.liberation.fr/cinema/2015/07/29/cinq-films-ou-les-robots-se-retournent-contre-les-hommes_1355549/ Portail de la robotique   Portail de la science-fiction"
Informatique;"C++ est un langage de programmation compilé permettant la programmation sous de multiples paradigmes, dont la programmation procédurale, la programmation orientée objet et la programmation générique. Ses bonnes performances, et sa compatibilité avec le C en font un des langages de programmation les plus utilisés dans les applications où la performance est critique.Créé initialement par Bjarne Stroustrup dans les années 1980, le langage C++ est aujourd'hui normalisé par l'ISO. Sa première normalisation date de 1998 (ISO/CEI 14882:1998), ensuite amendée par l'erratum technique de 2003 (ISO/CEI 14882:2003). Une importante mise à jour a été ratifiée et publiée par l'ISO en septembre 2011 sous le nom de ISO/IEC 14882:2011, ou C++11. Depuis, des mises à jour sont publiées régulièrement : en 2014 (ISO/CEI 14882:2014, ou C++14), en 2017 (ISO/CEI 14882:2017, ou C++17) puis en 2020 (ISO/IEC 14882:2020, ou C++20).En langage C, ++ est l'opérateur d'incrémentation, c'est-à-dire l'augmentation de la valeur d'une variable de 1. C'est pourquoi C++ porte ce nom : cela signifie que C++ est un niveau au-dessus de C.Bjarne Stroustrup commence le développement de C with Classes (C avec classes) en 1979. Il travaille alors dans les laboratoires Bell où il est notamment collègue de l'inventeur du C Dennis Ritchie. L'idée de créer un nouveau langage venait de l'expérience en programmation de Stroustrup pour sa thèse de doctorat. Il s'agissait en l'occurrence d'améliorer le langage C. Stroustrup trouvait que Simula avait des fonctionnalités très utiles pour le développement de gros programmes mais qu'il était trop lent pour être utilisé en pratique (cela était dû à un problème d'implémentation du compilateur Simula), tandis que BCPL était rapide mais de trop bas niveau et non adapté au développement de gros logiciels. Quand Stroustrup commença à travailler aux laboratoires Bell, on lui demanda d'analyser le noyau UNIX en vue de faire du calcul distribué. Se rappelant sa thèse, Stroustrup commença à améliorer le langage C avec des fonctionnalités similaires à celle de Simula. C fut choisi parce qu'il est rapide, portable et d'usage général. En outre, il était une bonne base pour le principe original et fondateur de C++ : « vous ne payez pas pour ce que vous n'utilisez pas ». Dès le départ, le langage ajoutait à C la notion de classe (avec encapsulation des données), de classe dérivée, de vérification des types renforcés (typage fort), d'« inlining », et d'argument par défaut.Alors que Stroustrup développait C with classes, il écrivit CFront, un compilateur qui générait du code source C à partir de code source C with classes. La première commercialisation se fit en octobre 1985. En 1983 le nom « C++ » est inventé, et en 1984 le nom du langage passa de C with classes à celui de « C++ ». Parmi les nouvelles fonctionnalités qui furent ajoutées au langage, il y avait les fonctions virtuelles, la surcharge des opérateurs et des fonctions, les références, les constantes, le contrôle du typage amélioré et les commentaires en fin de ligne. En 1985 fut publiée la première édition de The C++ Programming Language, apportant ainsi une référence importante au langage qui n'avait pas encore de standard officiel. En 1989, c'est la sortie de la version 2.0 de C++. Parmi les nouvelles fonctionnalités, il y avait l'héritage multiple, les classes abstraites, les fonctions membres statiques, les fonctions membres constantes, et les membres protégés. En 1990, The Annotated C++ Reference Manual (« ARM ») fut publié apportant les bases du futur standard. Les ajouts de fonctionnalités tardifs qu'il comportait couvraient les templates, les exceptions, les espaces de noms, les nouvelles conversions et le type booléen.Pendant l'évolution du langage C++, la bibliothèque standard évoluait de concert. Le premier ajout à la bibliothèque standard du C++ concernait les flux d'entrées/sorties qui apportaient les fonctionnalités nécessaires au remplacement des fonctions C traditionnelles telles que printf et scanf. Ensuite, parmi les ajouts les plus importants, il y avait la Standard Template Library. Après des années de travail, un comité réunissant l'ANSI et l'ISO standardisa C++ en 1998 (ISO/CEI 14882:1998), l'année où le comité de standardisation se réunissait à Sophia Antipolis dans le sud de la France. Pendant quelques années après la sortie officielle du standard, le comité traita des problèmes remontés par les utilisateurs, et publia en 2003 une version corrigée du standard C++.Personne ne possède le langage C++. Il est libre de droits ; cependant, le document de standardisation n'est quant à lui pas disponible gratuitement.On peut considérer que C++ « est du C » avec un ajout de fonctionnalités. Cependant, plusieurs programmes syntaxiquement corrects en C ne le sont pas en C++, à commencer bien sûr par ceux qui font usage d'identificateurs correspondant à des mots-clefs en C++.Parmi les fonctionnalités ajoutées figurent :le typage des « prototypes » de fonctions (repris dans ANSI C89) ;La surcharge des fonctions ;les déclarations reconnues comme instructions (repris dans C99) ;les opérateurs new et delete pour la gestion d'allocation mémoire ;le type de données bool (booléen) ;les références & ;les variables et les fonctions membres const (repris partiellement par C à la fin des années 1980) ;les fonctions inline (repris dans C99) ;les paramètres par défaut dans les fonctions ;les référentiels lexicaux (espaces de noms) et l'opérateur de résolution de portée :: ;les classes, ainsi que tout ce qui y est lié : l'héritage, les fonctions membres, les fonctions membres virtuelles, les constructeurs et le destructeur ;la surcharge des opérateurs ;les templates ;la gestion d'exceptions ;l'identification de type pendant l'exécution (RTTI : run-time type information) ;le commentaire sur une ligne introduit par // (existant dans BCPL, repris dans C99) ;les références de rvalue && (C++11) ;la déduction de type à la compilation via auto (C++11) ;les expressions constantes constexpr (C++11);les fonctions lambda (C++11, étendu dans tous les standards publiés depuis) ;les boucles for basées sur une plage (C++11, étendu en C++20) ;les modules via import, export et module (C++20) ;les contraintes et concepts via concept et requires (C++20) ;les fonctions immédiates consteval (C++20) ;les coroutines (C++20) ;La compilation d'un programme en C++ effectue également un contrôle plus minutieux du typage.La bibliothèque standard du C++ englobe la Standard Template Library (STL) qui met à la disposition du programmeur des outils puissants comme des collections (conteneurs) et des itérateurs.À l'origine, la STL était une bibliothèque développée par Alexander Stepanov qui travaillait pour Hewlett-Packard. Dans la norme, celle-ci n'est pas appelée STL, car elle est considérée comme faisant partie de la bibliothèque standard de C++. Toutefois, beaucoup de personnes l'appellent encore de cette manière pour distinguer d'une part, les fonctions d'entrées/sorties comprises dans cette bibliothèque et, d'autre part, celles fournies par la bibliothèque C.Comme en C, l'utilisation d'une bibliothèque peut se faire par l'intermédiaire de la directive #include (suivie du nom du fichier d'en-tête), et certaines d'entre elles (cmath, thread, etc.) nécessitent d'être liées explicitement. Depuis C++20 le mot clé import peut servir à des fins similaires.Le langage C++ utilise les concepts de la programmation orientée objet et permet entre autres :la création de classes ;l'encapsulation ;des relations entre les classes :la composition de classes (composition dans un diagramme de classes),l'association de classes (en) (association dans un diagramme de classes),l'agrégation de classes (agrégation dans un diagramme de classes),la dépendance (dépendance dans un diagramme de classes),l'héritage simple et multiple (héritage dans un diagramme de classes) ;le polymorphisme ;l'abstraction ;la généricité ;la méta-programmation.L'encapsulation permet de faire abstraction du fonctionnement interne (c'est-à-dire la mise en œuvre) d'une classe et ainsi de ne se préoccuper que des services rendus par celle-ci. C++ met en œuvre l'encapsulation en permettant de déclarer les membres d'une classe avec le mot réservé public, private ou protected. Ainsi, lorsqu'un membre est déclaré :public, il sera accessible depuis n'importe quelle fonction ;private, il sera uniquement accessible d'une part, depuis les fonctions qui sont membres de la classe et, d'autre part, depuis les fonctions autorisées explicitement par la classe (par l'intermédiaire du mot réservé friend) ;protected, il aura les mêmes restrictions que s'il était déclaré private, mais il sera en revanche accessible par les classes filles.C++ n'impose pas l'encapsulation des membres dans leurs classes. On pourrait donc déclarer tous les membres publics, mais en perdant une partie des bénéfices apportés par la programmation orientée objet. Il est de bon usage de déclarer toutes les données privées, ou au moins protégées, et de rendre publiques les fonctions membres agissant sur ces données. Ceci permet de cacher les détails de la mise en œuvre de la classe.Voici l'exemple de Hello world donné dans The C++ Programming Language, Third Edition de Bjarne Stroustrup :Dans l'exemple ci-dessus, le code source std::cout << ""Hello, new world!\n"" envoie la chaîne de caractères ""Hello, new world!\n"" à l'objet global cout, défini dans l'espace de noms standard std, grâce à l'opérateur << de cout.En C++, le mot clef namespace permet de définir et de nommer des espaces de noms (namespaces), notion déjà présente en langage C ; en effet, le corps d'une routine, d'une structure de contrôle de flux d'exécution, d'une structure de données ou d'une section de code (délimitée par les accolades { et }) constitue un espace de noms. En C++, le corps d'une classe, à l'instar du corps d'une structure de données, constitue aussi un espace de noms.Dans différents espaces de noms, on peut ainsi définir des entités (routines, variables, etc.) ayant le même identificateur. L'ambiguïté est résolue en utilisant le nom de l'espace de nom devant l'opérateur de portée (::) pour indiquer l'espace de noms dans lequel on veut accéder. Notez que l'espace de noms global du programme n'a pas de nom. Pour accéder à une entité globale, cachée par une entité locale par exemple, on utilise l'opérateur de portée précédé d'aucun nom.Il est possible de spécifier un espace de noms précis à utiliser afin d'éviter d'avoir à recourir à l'opérateur de résolution de portée. Pour cela, le mot-clé using est utilisé avec cette syntaxe :Ainsi, pour utiliser la variable cout définie dans le namespace standard sans utiliser l'opérateur de résolution de portée, il est possible d'écrire using namespace std; ou using std::cout;. Cela est valable pour tous les espaces de noms. Cette instruction se place en général avant le début du code source proprement dit :Il est aussi possible, et conseillé, d'importer un symbole particulier, ou de placer cette instruction dans une fonction afin de limiter la portée :Le mot-clé using peut aussi être utilisé dans les classes. Si une classe B hérite d'une classe A, elle peut grâce à ce mot-clé passer des membres protected de A en public dans B, ou encore démasquer une fonction membre de A qui le serait par une fonction membre de B de même nom :Le programme ci-dessus affiche :Il est aussi possible de définir un nouveau nom pour un namespace :Il est d'usage de séparer prototype (déclaration) et implémentation (définition) de classe dans deux fichiers : la déclaration se fait dans un fichier d'en-tête (dont l'extension varie selon les préférences des développeurs : sans extension dans le standard, .h comme en C, .hh ou .hpp ou .hxx pour différencier le code source C++ du C) alors que la définition se fait dans un fichier source (d'extension également variable : .c comme en C, .cc ou .cpp ou .cxx pour différencier C++ du C).Exemple de la déclaration d'une classe comportant des attributs privés et des fonctions membres publiques :Le nom d'une fonction membre déclarée par une classe doit nécessairement être précédé du nom de la classe suivi de l'opérateur de résolution de portée ::.Exemple de définition des fonctions membres d'une classe (celle déclarée précédemment) :Les Modèles (ou templates) permettent d'écrire des variables, des fonctions et des classes en paramétrant le type de certains de leurs constituants (type des paramètres ou type de retour pour une fonction, type des éléments pour une classe collection par exemple). Les modèles permettent d'écrire du code générique, c'est-à-dire qui peut servir pour une famille de fonctions ou de classes qui ne diffèrent que par le type de leurs constituants.Les paramètres peuvent être de différentes sortes :types simples, tels que les classes ou les types élémentaires (int, double, etc.) ;tableaux de taille constante, dont la taille, déduite par le compilateur, peut être utilisée dans l'instanciation du modèle ;constantes scalaires, c'est-à-dire de type entier (int, char, bool), mais pas flottant (float, double) car leur représentation binaire ne fait pas partie de la norme du langage (jusqu'en C++20 où ils sont autorisés) ;templates, dont la définition doit être passée en paramètre, ce qui permet notamment de s'appuyer sur la définition abstraite, par exemple, d'une collection ;pointeurs ou références, à condition que leur valeur soit définie à l'édition de liens ;fonction membre d'une classe, dont la signature et la classe doivent être aussi passées en paramètres ;attribut d'une classe, dont le type et la classe doivent être aussi passés en paramètres.En programmation, il faut parfois écrire de nombreuses versions d'une même fonction ou classe suivant les types de données manipulées. Par exemple, un tableau de int ou un tableau de double sont très semblables, et les fonctions de tri ou de recherche dans ces tableaux sont identiques, la seule différence étant le type des données manipulées. En résumé, l'utilisation des templates permet de « paramétrer » le type des données manipulées.Les avantages des modèles sont :des écritures uniques pour les fonctions et les classes ;moins d'erreurs dues à la réécriture ;Dans la bibliothèque standard C++, on trouve de nombreux templates. On citera à titre d'exemple, les entrées/sorties, les chaînes de caractères ou les conteneurs. Les classes string, istream, ostream et iostream sont toutes des instanciations de type char.Les fonctions de recherche et de tri sont aussi des templates écrits et utilisables avec de nombreux types.Dans la ligne float f = max<float>(1, 2.2f);, on doit explicitement donner le type float pour le type paramétré T car le compilateur ne déduit pas le type de T lorsqu'on passe en même temps un int (1) et un float (2.2f).Un template donné peut avoir plusieurs instanciations possibles selon les types donnés en paramètres. Si un seul paramètre est spécialisé, on parle de spécialisation partielle. Ceci permet par exemple :de choisir un type de calcul selon qu'un type est un entier, un flottant, une chaîne de caractères, etc. Spécialisons l'exemple précédent pour le cas des pointeurs de chaînes de caractères :d'effectuer au moment de la compilation des calculs arithmétiques, si et seulement si tous les arguments sont connus à ce moment. Un exemple classique est le calcul de la fonction factorielle :À partir de C++14 pour arriver aux mêmes fins nous pourrions aussi utiliser les variables templates :Ainsi nous pouvons écrire factorielle<8>; à la place de Factorielle<8>::value;.Le mécanisme décrit par l'abréviation SFINAE (Substitution Failure Is Not an Error) permet de surcharger un template par plusieurs classes (ou fonctions), même si certaines spécialisations, par exemple, ne peuvent pas être utilisées pour tous les paramètres de templates. Le nom décrit précisément le fonctionnement du mécanisme, littéralement l’acronyme de « Un échec de substitution n'est pas une erreur », le compilateur, lors de la substitution, ignore alors les instanciations inapplicables, au lieu d'émettre une erreur de compilation. Par exemple :Ici f est définie deux fois, le type de retour est conditionné par le type donné en paramètre, il est du type du retour de f.foo() dans le premier cas et de celui de f.bar() dans le deuxième cas. Ainsi, si on appelle f avec un objet de la classe A, seule la première fonction fonctionne puisque la classe A n'a pas de fonction membre bar() et donc la substitution est possible avec cette première version mais pas pour la deuxième. Ainsi, f(a) appelle la première version de f, f(b) appelle la deuxième avec le même raisonnement, mais cette fois pour la fonction membre bar().Si lors d'un développement à venir, un développeur venait à écrire une nouvelle classe ayant une fonction membre publique foo ou bien (ou exclusif) bar, il pourrait également utiliser f avec.Le polymorphisme d'inclusion est mis en œuvre à l'aide du mécanisme des fonctions membres virtuelles en C++. Une fonction membre est rendue virtuelle par le placement du mot-clé virtual devant la déclaration de la fonction membre dans la classe. Lorsqu'une fonction membre virtuelle est appelée, l'implémentation de la fonction membre exécutée est choisie en fonction du type réel de l'objet. L'appel n'est donc résolu qu'à l'exécution, le type de l'objet ne pouvant pas a priori être connu à la compilation.Le mot-clé virtual indique au compilateur que la fonction membre déclarée virtuelle est susceptible d'être redéfinie dans une classe dérivée. Il suffit alors de dériver une classe et de définir une nouvelle fonction membre de même signature (même nom, paramètres compatibles — voir la notion de covariance). Ainsi l'appel de cette fonction membre sur un objet accédé en tant qu'objet de la classe de base mais appartenant en réalité à la classe dérivée donnera lieu à l'appel de la fonction membre définie dans la classe dérivée.En particulier, il est obligatoire d'utiliser le mot-clé virtual devant la déclaration du destructeur de la classe de base lorsque le programme souhaite pouvoir détruire un objet via un pointeur d'instance de la classe de base au lieu d'un pointeur d'instance de la classe dérivée.Ce type de polymorphisme (le polymorphisme d'inclusion) est dit dynamique. Le mécanisme de la surcharge de fonction qui est un polymorphisme ad hoc est de type statique. Dans les deux cas il faut appliquer une logique (par exemple : le nombre et le type des paramètres) pour résoudre l'appel. Dans le cas de la surcharge de fonction, la logique est entièrement calculée à la compilation. Ce calcul permet des optimisations rendant le polymorphisme statique plus rapide que sa version dynamique. La liaison dynamique de fonctions membres issues du mécanisme des fonctions membres virtuelles induit souvent une table cachée de résolution des appels, la table virtuelle. Cette table virtuelle augmente le temps nécessaire à l'appel de fonction membre à l'exécution par l'ajout d'une indirection supplémentaire.Le choix entre liaison dynamique et surcharge (polymorphisme dynamique et statique) est typiquement un problème de calculabilité des appels, ayant souvent pour conséquence finale un choix entre expressivité et performance.Malgré ce dynamisme, il est à noter que le compilateur est capable de « dévirtualiser » les appels de fonctions membres qui peuvent être résolus au moment de la compilation. Dans gcc par exemple, l'option -fdevirtualize lors de la compilation permet cette optimisation, s'il est possible de faire une telle résolution.Un programme C++ peut être produit avec des outils qui automatisent le processus de construction. Les plus utilisés sont :make ;Ant (génération portable en XML) ;SCons (génération portable en Python) ;CMake (génération de Makefile portable) ;Bazel.Anjuta DevStudio ;C++ Builder ;CLion (en) ;Code::Blocks (open-source) ;Dev-C++ et son extension RAD WxDev-C++ ;Eclipse avec le plugin CDT (open-source) ;Emacs (libre) ;KDevelop ;NetBeans (open-source) ;QtCreator (open-source) ;Sun Studio ;Vim ;Microsoft Visual C++ (a été intégré au framework Visual Studio) ;Xcode.GCC pour GNU Compiler Collection (libre, multilangage et multiplateforme : UNIX, Windows, DOS, etc.) ;Clang ;Microsoft Visual C++ (Windows) ;Borland C++ Builder (Windows) ;Intel C++ Compiler (Windows, Linux, MacOS) ;Open64 (en) compilateur opensource d'AMD (Linux) ;Digital Mars C/C++ compiler (Windows) ;Open Watcom ;Boost ;Qt ;Gtkmm ;wxWidgets ;SFML ;OpenCV ;SDLmm, surcouche C++ à la SDL ;LLVM.etc. Ouvrages en langue anglaise [Deitel et Deitel 2011] (en) P. Deitel et H. Deitel, C++ : How to Program, 20 Hall, 2011, 8e éd., 1104 p. (ISBN 978-0-13-266236-9).[Dawson 2010] (en) M. Dawson, Beginning C++ Through Game Programming, Course Technology PTR, 2010, 3e éd., 432 p. (ISBN 978-1-4354-5742-3).[Gregoire, Solter et Kleper 2011] (en) Marc Gregoire, Nicolas A. Solter et Scott J. Kleper, Professional C++, John Wiley, octobre 2011, 1104 p. (ISBN 978-0-470-93244-5, présentation en ligne).[Josuttis 2011] (en) Nicolaï Josuttis, The C++ Standard Library, A Tutorial and Reference, Addison-Wesley, 2011, 2e éd., 1099 p. (ISBN 978-0-321-62321-8, présentation en ligne).[Koenig et Moo 2000] (en) A. Koenig et B. Moo, Accelerated C++ : Practical Programming by Example, Addison-Wesley, 2000, 1re éd., 352 p. (ISBN 978-0-201-70353-5).[Lippman, Lajoie et Moo 2012] (en) Stanley B. Lippman, Josée Lajoie et Barbara E. Moo, C++ Primer : 5th Edition, août 2012, 5e éd., 1399 p. (ISBN 978-0-321-71411-4).[Lischner 2003] (en) R. Lischner, C++ in a nutshell, O'Reilly Media, 2003, 1re éd., 704 p. (ISBN 978-0-596-00298-5).[Meyers 2005] (en) S. Meyers, Effective C++ : 55 Specific Ways to Improve Your Programs and Designs, Addison-Wesley Professional, 2005, 3e éd., 320 p. (ISBN 978-0-321-33487-9, présentation en ligne).[Oualline 2003] (en) S. Oualline, Practical C++ programming, O'Reilly Media, 2003, 2e éd., 600 p. (ISBN 978-0-596-00419-4, présentation en ligne).[Lafore 2001] (en) R. Lafore, Object-oriented programming in C++, Sams, 2001, 4e éd., 1040 p. (ISBN 978-0-672-32308-9).[Prata 2011] (en) S. Prata, C++ Primer Plus (Developer's Library), Addison-Wesley Professional, 2011, 6e éd., 1200 p. (ISBN 978-0-321-77640-2, présentation en ligne).[Stroustrup 2009] (en) Bjarne Stroustrup, Programming : Principles and Practice using C++, Addison-Wesley, 2009, 1236 p. (ISBN 978-0-321-54372-1).[Stroustrup 2013] (en) Bjarne Stroustrup, The C++ Programming Language : 4th Edition, Addison-Wesley Professional, 2013, 4e éd., 1368 p. (ISBN 978-0-321-56384-2).[Stroustrup 1994] (en) Bjarne Stroustrup, The Design and Evolution of C++, Addison-Wesley professional, 1994, 1re éd., 480 p. (ISBN 978-0-201-54330-8).[Sutter 1999] (en) H. Sutter, Exceptional C++ : 47 Engineering Puzzles, Programming Problems, and Solutions, Addison-Wesley Professional, 1999, 240 p. (ISBN 978-0-201-61562-3, présentation en ligne).[Vandevoorde et Josuttis 2002] (en) David Vandevoorde et Nicolaï Josuttis, C++ Templates : the Complete Guide, Addison-Weslay, 2002, 528 p. (ISBN 978-0-201-73484-3).[Vandevoorde 1998] (en) David Vandevoorde, C++ Solutions : Companion to the C++ Programming Language, Addison-Wesley, 1998, 3e éd., 292 p. (ISBN 978-0-201-30965-2). Ouvrages en langue française [Benharrats et Vittupier 2021] Mehdi Benharrat et Benoît Vittupier, Le guide du C++ moderne : de débutant à développeur, D-Booker, 2021, 1re éd., 708 p. (ISBN 978-2-8227-0881-4, présentation en ligne).[Chappelier et Seydoux 2005] J-C. Chappelier et F. Seydoux, C++ par la pratique : Recueil d'exercices corrigés et aide-mémoire, PPUR, 2005, 2e éd., 412 p. (ISBN 978-2-88074-732-9, présentation en ligne).[Deitel et Deitel 2004] P. Deitel et H. Deitel, Comment programmer en C++, Reynald Goulet, 2004, 1178 p. (ISBN 978-2-89377-290-5).[Delannoy 2001] Claude Delannoy, Programmer en langage C++, Paris, Eyrolles, 2011, 8e éd., 822 p. (ISBN 978-2-212-12976-2, présentation en ligne).[Delannoy 2007] Claude Delannoy, Exercices en langage C++, Paris, Eyrolles, 2007, 3e éd., 336 p. (ISBN 978-2-212-12201-5, présentation en ligne).[Géron et Tawbi 2003] Aurélien Géron et Fatmé Tawbi (préf. Gilles Clavel), Pour mieux développer avec C++ : Design patterns, STL, RTTI et smart pointers, Paris, Dunod, 2003, 188 p. (ISBN 978-2-10-007348-1).[Guidet 2008] Alexandre Guidet, Programmation objet en langage C++, Paris, Ellipses, coll. « Cours et exercices. », 2008, 364 p. (ISBN 978-2-7298-3693-1, OCLC 221607125, BNF 41206426).[Hubbard 2002] J. R. Hubbard (trad. Virginie Maréchal), C++ [« Schaum's easy outline of programming with C++ »], Paris, EdiScience, coll. « Mini Schaum's », 2002, 192 p. (ISBN 978-2-10-006510-3).[Liberty et Jones 2005] Jesse Liberty et Bradley Jones (trad. Nathalie Le Guillou de Penanros), Le langage C++ [« Teach yourself C++ in 21 days »], Paris, CampusPress, 2005, 859 p. (ISBN 978-2-7440-1928-9).[Stephens, Diggins, Turkanis et al. 2006] D. Ryan Stephens, Christopher Diggins, Jonathan Turkanis et J. Cogswell (trad. Yves Baily & Dalil Djidel), C++ en action [« C++ Cookbook - Solutions and Examples for C++ Programmers »], Paris, O'Reilly, 2006, 555 p. (ISBN 978-2-84177-407-4, OCLC 717532188, BNF 40170870).[Stroustrup 2012] Bjarne Stroustrup (trad. Marie-Cécile Baland, Emmanuelle Burr, Christine Eberhardt), Programmation : principes et pratique avec C++ : Avec plus de 1000 exercices. [« Programming : principles and practice using C++ »], Paris, Pearson education, 2012, 944 p. (ISBN 978-2-7440-7718-0).[Stroustrup 2003] Bjarne Stroustrup (trad. Christine Eberhardt), Le langage C++ [« The C++ programming language »], Paris, Pearson education, 2003, 1098 p. (ISBN 978-2-7440-7003-7 et 2-744-07003-3).[Sutter et Alexandrescu 2005] Herb Sutter et Andrei Alexandrescu, Standards de programmation C [« C++ Coding Standards: 101 Rules, Guidelines, and Best Practices »], Paris, Pearson Education France, coll. « C++ », 2005, 243 p. (ISBN 978-2-7440-7144-7 et 2-744-07144-7).Langage C(en) Le Comité du Standard C++(fr) Documentation du C++ (wiki sous double licence CC-BY-SA et GFDL)(en) Documentation du C++ (le même wiki mais en anglais et plus complet)(en) Documentation du C++ (contenu non libre, édité par The C++ ressources network) Portail de la programmation informatique   Portail de l’informatique"
Informatique;
"on. Log""";
Informatique;Une page web dynamique est une page web générée à la demande, par opposition à une page web statique. Le contenu d'une page web dynamique peut donc varier en fonction d'informations (heure, nom de l'utilisateur, formulaire rempli par l'utilisateur, etc.) qui ne sont connues qu'au moment de sa consultation. À l'inverse, le contenu d'une page web statique est a priori identique à chaque consultation.Lors de la consultation d'une page web statique, un serveur HTTP renvoie le contenu du fichier où la page est enregistrée.Lors de la consultation d'une page web dynamique, un serveur HTTP transmet la requête au logiciel correspondant à la requête, et le logiciel se charge de générer et envoyer le contenu de la page. La programmation web est le domaine de l'ingénierie informatique consacré au développement de tels logiciels. Les logiciels générant des pages web dynamiques sont fréquemment écrits avec les langages PHP, JavaServer Pages (JSP) ou Active Server Pages (ASP).Un site web dynamique peut ainsi fournir des informations aux utilisateurs en fonction de leur navigation sur celui-ci. Deux utilisateurs peuvent accéder simultanément à la même page web sans pour autant avoir le même contenu affiché à l'écran.Avec un site web dynamique, des modifications effectuées, par exemple via un système de soumission de commentaire ou bien une interface privée de gestion du site, pourront être directement visibles sur le site.Un site web dynamique peut permettre la mise en œuvre de différentes fonctionnalités, par exemples :- un système de gestion des accès, granulaire ou non, à certaines parties d'un site (administration du site, comptes utilisateurs, etc),- un système de soumission de commentaires publiques.En 2004, Le Journal du Net consacrait un article comparant les avantages des technologies statiques et dynamiques.Web profondProgrammation webFeuilles de style dynamiques en cascade Portail de l’informatique   Portail d’Internet
Informatique;"PHP: Hypertext Preprocessor, plus connu sous son sigle PHP (sigle auto-référentiel), est un langage de programmation libre, principalement utilisé pour produire des pages Web dynamiques via un serveur HTTP, mais pouvant également fonctionner comme n'importe quel langage interprété de façon locale. PHP est un langage impératif orienté objet.PHP a permis de créer un grand nombre de sites web célèbres, comme Facebook et Wikipédia. Il est considéré comme une des bases de la création de sites web dits dynamiques mais également des applications web.PHP est un langage de script utilisé le plus souvent côté serveur : dans cette architecture, le serveur interprète le code PHP des pages web demandées et génère du code (HTML, XHTML, CSS par exemple) et des données (JPEG, GIF, PNG par exemple) pouvant être interprétés et rendus par un navigateur web. PHP peut également générer d'autres formats comme le WML, le SVG et le PDF.Il a été conçu pour permettre la création d'applications dynamiques, le plus souvent développées pour le Web. PHP est le plus souvent couplé à un serveur Apache bien qu'il puisse être installé sur la plupart des serveurs HTTP tels que IIS ou nginx. Ce couplage permet de récupérer des informations issues d'une base de données, d'un système de fichiers (contenu de fichiers et de l'arborescence) ou plus simplement des données envoyées par le navigateur afin d'être interprétées ou stockées pour une utilisation ultérieure.C'est un langage peu typé et souple et donc facile à apprendre par un débutant mais, de ce fait, des failles de sécurité peuvent rapidement apparaître dans les applications. Pragmatique, PHP ne s'encombre pas de théorie et a tendance à choisir le chemin le plus direct. Néanmoins, le nom des fonctions (ainsi que le passage des arguments) ne respecte pas toujours une logique uniforme, ce qui peut être préjudiciable à l'apprentissage.Son utilisation commence avec le traitement des formulaires puis par l'accès aux bases de données. L'accès aux bases de données est aisé une fois l'installation des modules correspondants effectuée sur le serveur. La force la plus évidente de ce langage est qu'il a permis au fil du temps la résolution aisée de problèmes autrefois compliqués et est devenu par conséquent un composant incontournable des offres d'hébergements.Il est multi-plateforme : autant sur Linux qu'avec Windows il permet aisément de reconduire le même code sur un environnement à peu près semblable (quoiqu'il faille prendre en compte les règles d'arborescences de répertoires, qui peuvent changer).Libre, gratuit, simple d'utilisation et d'installation, ce langage nécessite comme tout langage de programmation une bonne compréhension des principales fonctions usuelles ainsi qu'une connaissance aiguë des problèmes de sécurité liés à ce langage.La version 5.3 a introduit de nombreuses fonctions nouvelles : les espaces de noms (Namespace) — un élément fondamental de l'élaboration d'extensions, de bibliothèques et de frameworks structurés, les fonctions anonymes, les fermetures, etc.En 2018, près de 80 % des sites web utilisent le langage PHP sous ses différentes versions.Le langage PHP fait l'objet, depuis plusieurs années maintenant, de rassemblements nationaux organisés par l'AFUP (l'Association Française des Utilisateurs de PHP), où experts de la programmation et du milieu se retrouvent pour échanger autour du PHP et de ses développeurs. L'association organise ainsi deux évènements majeurs : le « Forum PHP », habituellement en fin d'année, et les « AFUP Day », qui ont lieu au cours du premier semestre, simultanément dans plusieurs villes.Le langage PHP a été créé en 1994 par Rasmus Lerdorf pour son site web. C'était à l'origine une bibliothèque logicielle en C dont il se servait pour conserver une trace des visiteurs qui venaient consulter son CV. Au fur et à mesure qu'il ajoutait de nouvelles fonctionnalités, Rasmus a transformé la bibliothèque en une implémentation capable de communiquer avec des bases de données et de créer des applications dynamiques et simples pour le Web. Rasmus a alors décidé, en 1995, de publier son code, pour que tout le monde puisse l'utiliser et en profiter. PHP s'appelait alors PHP/FI (pour Personal Home Page Tools/Form Interpreter). En 1997, deux étudiants, Andi Gutmans et Zeev Suraski, ont redéveloppé le cœur de PHP/FI. Ce travail a abouti un an plus tard à la version 3 de PHP, devenu alors PHP: Hypertext Preprocessor. Peu de temps après, Andi Gutmans et Zeev Suraski ont commencé la réécriture du moteur interne de PHP. C’est ce nouveau moteur, appelé Zend Engine — le mot Zend est la contraction de Zeev et Andi — qui a servi de base à la version 4 de PHP.En 2002, PHP est utilisé par plus de 8 millions de sites Web à travers le monde, en 2007 par plus de 20 millions et en 2013 par plus de 244 millions.De plus, PHP est devenu le langage de programmation web côté serveur le plus utilisé depuis plusieurs années :Enfin en 2010, PHP est le langage dont les logiciels open source sont les plus utilisés dans les entreprises, avec 57 % de taux de pénétration.Depuis juin 2011 et le nouveau processus de livraison de PHP, le cycle de livraison de PHP se résume à une mise à jour annuelle comportant des changements fonctionnels importants.La durée de vie d'une branche est de 3 ans, laissant trois branches stables et maintenues (cela signifie que lorsqu'une nouvelle version de PHP 5.x sort, la version 5.x-3 n'est plus supportée). Version 8.1 La version 8.1, sortie le 25 novembre 2021, introduit de nouvelles fonctionnalités comme : les énumérations ;les fibers ;la propriété Readonly. Version 8 Sortie le 26 novembre 2020, cette version majeure se démarque principalement par la fonctionnalité de « compilation à la volée » (Just-in-time compilation) qui permet un gain de vitesse d'exécution de plus de 45 % pour certaines applications Web. D'autres nouveautés sont également introduites comme : les weakmaps ;la Stringable Interface ;l'expression throw. Version 7.4 La version 7.4 est sortie le 20 février 2020. Elle vise à être maintenue jusqu'en novembre 2022.La version 7.4 se démarque de ses précédentes versions par :les propriétés typées 2.0 ;le pré-chargement ;l'opérateur d'affectation de coalescence nulle  ;improve openssl_random_pseudo_bytes ;les références faibles ;FFI (Foreign Function Interface) ;l'extension de hachage omniprésente ;le registre de hachage de mot de passe ;le fractionnement des chaînes multi-octets ;la réflexion sur les références ;le retrait de ext/wddx ;un nouveau mécanisme de sérialisation d'objets personnalisés. Version 7.3 Le 6 décembre 2018, la sortie de la version 7.3 mettait l'accent sur :l'évolution de la syntaxe Heredoc et Nowdoc ;la prise en charge de l'affectation de référence et de la déconstruction de tableau avec `list()` ;la prise en charge de PCRE2 ;l'introduction de la fonction High Resolution Time `hrtime()` function. Version 7.2 Le 30 novembre 2017, la version de PHP 7.2, qui utilise Zend Engine 2, a introduit une modélisation objet plus performante, une gestion des erreurs fondée sur le modèle des exceptions, ainsi que des fonctionnalités de gestion pour les entreprises. PHP 5 apporte beaucoup de nouveautés, telles que le support de SQLite ainsi que des moyens de manipuler des fichiers et des structures XML basés sur libxml2 :une API simple nommée SimpleXML ;une API Document Object Model assez complète ;une interface XPath utilisant les objets DOM et SimpleXML ;l'intégration de libxslt pour les transformations XSLT via l'extension XSL ;une bien meilleure gestion des objets par rapport à PHP 4, avec des possibilités qui tendent à se rapprocher de celles de Java. Version 7 (PHP7) Au vu des orientations différentes prises par le langage de celles prévues par PHP 6, une partie des développeurs propose de nommer la version succédant à PHP 5 « PHP 7 » au lieu de « PHP 6 ». Un vote parmi les développeurs valide cette proposition par 58 voix contre 24.PHP 7.0.0 est sorti en décembre 2015.La nouvelle version propose une optimisation du code et, d'après la société Zend, offre des performances dépassant celles de machines virtuelles comme HHVM,. Les benchmarks externes montrent des performances similaires pour HHVM et PHP 7, avec un léger avantage d'HHVM dans la plupart des scénarios. PHP 6 et Unicode En 2005, le projet de faire de PHP un langage fonctionnant d'origine en Unicode a été lancé par Andrei Zmievski, ceci en s'appuyant sur la bibliothèque International Components for Unicode (ICU) et en utilisant UTF-16 pour représenter les chaînes de caractères dans le moteur.Étant donné que cela représentait un changement majeur tant dans le fonctionnement du langage que dans le code PHP créé par ses utilisateurs, il fut décidé d'intégrer cela dans une nouvelle version 6.0 avec d'autres fonctionnalités importantes alors en développement. Toutefois, le manque de développeurs experts en Unicode ainsi que les problèmes de performance résultant de la conversion des chaînes de et vers UTF-16 (rarement utilisé dans un contexte web), ont conduit au report récurrent de la livraison de cette version. Par conséquent, une version 5.3 fut créée en 2009 intégrant de nombreuses fonctionnalités non liées à Unicode qui était initialement prévues pour la version 6.0, notamment le support des espaces de nommage (namespaces) et des fonctions anonymes. En mars 2010, le projet 6.0 intégrant unicode fut abandonné et la version 5.4 fut préparée afin d'intégrer la plupart des fonctionnalités non liées à l'unicode encore dans la branche 6.0, telles que les traits ou l'extension des fermetures au modèle objet.Le projet est depuis passé à un cycle de livraison prévisible (annuel) contenant des avancées significatives mais contenues tout en préservant au maximum la rétro-compatibilité avec le code PHP existant (5.4 en 2012, 5.5 en 2013, 5.6 prévue pour l'été 2014). Depuis janvier 2014, l'idée d'une nouvelle version majeure introduisant Unicode mais se basant sur UTF-8 (largement devenu depuis le standard du Web pour l'Unicode) et permettant certains changements pouvant casser la rétro-compatibilité avec du code PHP ancien est de nouveau discutée et les RFC sont maintenant triées selon leur implémentation en 5.x (évolutions ne causant pas ou marginalement de cassure de la rétro-compatibilité) ou dans la future version majeure (évolutions majeures du moteur et évolutions impliquant une non-compatibilité ascendante). À noter Il est à noter qu'historiquement, PHP disposait d'une configuration par défaut privilégiant la souplesse à la sécurité (par exemple register globals, qui a été activé par défaut jusqu'à PHP 4.2). Cette souplesse a permis à de nombreux développeurs d'apprendre PHP mais le revers de la médaille a été que de nombreuses applications PHP étaient mal sécurisées. Le sujet a bien été pris en main par le PHPGroup qui a mis en place des configurations par défaut mettant l'accent sur la sécurité. Il en résultait une réputation de langage peu sécurisé, réputation d'insécurité qui n'a plus de raison d'être[réf. nécessaire]. Détail de l'historique complet des versions PHP appartient à la grande famille des descendants du C, dont la syntaxe est très proche. En particulier, sa syntaxe et sa construction ressemblent à celles des langages Java et Perl, à ceci près que du code PHP peut facilement être mélangé avec du code HTML au sein d'un fichier PHP.Dans une utilisation destinée à l'internet, l'exécution du code PHP se déroule ainsi : lorsqu'un visiteur demande à consulter une page de site web, son navigateur envoie une requête au serveur HTTP correspondant. Si la page est identifiée comme un script PHP (généralement grâce à l'extension .php), le serveur appelle l'interprète PHP qui va traiter et générer le code final de la page (constitué généralement d'HTML ou de XHTML, mais aussi souvent de feuilles de style en cascade et de JS). Ce contenu est renvoyé au serveur HTTP, qui l'envoie finalement au client.Ce schéma explique ce fonctionnement :Une étape supplémentaire est souvent ajoutée : celle du dialogue entre PHP et la base de données. Classiquement, PHP ouvre une connexion au serveur de SGBD voulu, lui transmet des requêtes et en récupère le résultat, avant de fermer la connexion.L'utilisation de PHP en tant que générateur de pages Web dynamiques est la plus répandue, mais il peut aussi être utilisé comme langage de programmation ou de script en ligne de commande sans utiliser de serveur HTTP ni de navigateur. Il permet alors d'utiliser de nombreuses fonctions du langage C et plusieurs autres sans nécessiter de compilation à chaque changement du code source.Pour réaliser en Linux/UNIX un script PHP exécutable en ligne de commande, il suffit comme en Perl ou en Bash d'insérer dans le code en première ligne le shebang : #! /usr/bin/php. Sous un éditeur de développement comme SciTE, même en Windows, une première ligne <?php suffit, si le fichier possède un type .php.Il existe aussi une extension appelée PHP-GTK permettant de créer des applications clientes graphiques sur un ordinateur disposant de la bibliothèque graphique GTK+, ou encore son alternative WinBinder.PHP possède un grand nombre de fonctions permettant des opérations sur le système de fichiers, exécuter des commandes dans le terminal, la gestion des bases de données, des fonctions de tri et hachage, le traitement de chaînes de caractères, la génération et la modification d'images, des algorithmes de compression...Le moteur de Wikipédia, MediaWiki, est écrit en PHP et interagit avec une base MySQL ou PostgreSQLQuelques exemples du traditionnel Hello world :echo étant une structure du langage, il est possible – et même recommandé – de ne pas mettre de parenthèses.Il est aussi possible d'utiliser la version raccourcie :Résultat affiché :Le code PHP doit être inséré entre les balises <?php et ?> (la balise de fermeture est facultative en fin de fichier).Il y existe d'autres notations pour les balises :<?= et ?> (notation courte avec affichage) ;<? et ?> (notation courte sans affichage non disponible en PHP 8) ;<% et %> (notation ASP) ;<script language=""php""> et </script> (notation script).Les notations autres que la standard (<?php et ?>) et la notation courte avec affichage (<?= et ?>) sont déconseillées, car elles peuvent être désactivées dans la configuration du serveur (php.ini ou .htaccess) : la portabilité du code est ainsi réduite.Depuis PHP 7, les notations ASP et script ont été supprimées. La notation courte sans affichage reste déconseillée.Les instructions sont séparées par des ; (il n'est pas obligatoire après la dernière instruction) et les sauts de ligne ne modifient pas le fonctionnement du programme. Il serait donc possible d'écrire :Pour des raisons de lisibilité, il est néanmoins recommandé d'écrire une seule instruction par ligne. Il est aussi préférable d'écrire le dernier ;.Le code PHP est composé par des appels à des fonctions, dans le but d'attribuer des valeurs à des variables, le tout encadré dans des conditions, des boucles. Exemple :Une condition est appliquée quand l'expression entre parenthèses est évaluée à true, et elle ne l'est pas dans le cas de false. Sous forme numérique, 0 représente le false, et 1 (et tous les autres nombres) représentent le true.Le code précédent pourrait aussi être écrit de cette manière :Ici on teste l'égalité entre $lang et 'fr', mais pas directement dans le if : le test retourne un boolean (c'est-à-dire soit true, soit false) qui est stocké dans la variable $is_lang_fr. On entre ensuite cette variable dans le if et celui-ci, selon la valeur de la variable, effectuera ou non le traitement.Les blocs if, elseif et else sont généralement délimités par les caractères { et }, qui peuvent être omis, comme dans les codes précédents, lorsque ces blocs ne contiennent qu'une instruction.Il est également possible d'écrire else if en deux mots, comme en C/C++.On peut générer du code HTML avec le script PHP, par exemple :Il est également possible d'utiliser une syntaxe alternative pour la structure if/else :Une autre approche consiste à concaténer l'intégralité du code HTML dans une variable et de réaliser un echo de la variable en fin de fichier :Dans le cas où l'utilisateur aura préféré l'utilisation de la commande echo à la concaténation, il lui sera possible de capturer le flux en utilisant les fonctions ob_start() et ob_get_clean() :PHP, tout comme JavaScript, permet aussi de construire un modèle objet de document (DOM), ce qui permet de créer ou modifier un document (X)HTML sans écrire de HTML, comme le montre l'exemple suivant :Qui crée le code HTML suivant :Cette méthode est cependant peu utilisée pour générer un document complet, on l'utilise généralement pour générer un fichier XML.La commande phpinfo() est aussi utilisée pour générer un code HTML décrivant les paramètres du serveur ; elle est aussi très utilisée pour tester la bonne exécution du moteur d’exécution PHP.Comme en C++ et en Java, PHP permet de programmer en orienté objet, en créant des classes contenant des attributs et des méthodes, qui peuvent être instanciées ou utilisées en statique.Toutefois, PHP est un langage à héritage simple, c'est-à-dire qu'une classe ne peut hériter que d'au plus une seule autre classe (sinon il faut utiliser un trait pour simuler l'héritage multiple par composition). Cependant les interfaces peuvent en étendre plusieurs autres.Voici un exemple de création d'une classe :Comme de nombreux projets Open Source, PHP possède une mascotte. Il s'agit de l'éléPHPant, dessiné en 1998 par El Roubio.El Roubio s'est inspiré de la ressemblance des lettres PHP avec un éléphant et du fait que deux des lettres du langage soient déjà présentes dans ce mot, ce qui a permis de créer le néologisme éléPHPant. Toutes les œuvres d'El Roubio sont distribuées sous licence GNU GPL. Une peluche de l'ÉléPHPant bleu existe. D'autres versions ont vu le jour ces dernières années (rose, jaune, rouge, violet et orange) sous l'impulsion de sociétés (PHP Architect ou Zend Technologies) ou de groupes utilisateurs comme PHP Women ou PHP Amsterdam. Le site afieldguidetoelephpant.net recense tous les éléphpants existants.Wiki (MediaWiki, DokuWiki...)forum (phpBB, Vanilla, IPB, punBB...)FacebookSystèmes de gestion de blog (Dotclear)Systèmes de gestion de contenu (appelés aussi CMS) (WordPress, SPIP, ExpressionEngine, Drupal, Xoops, Joomla, K-Box...)Administration de bases de données (phpMyAdmin, phpPgAdmin, Adminer...)Frameworks (Laravel, Symfony, Zend Framework, CodeIgniter, CakePHP, etc.)Logiciel ECMLogiciel BPM, CRM et ou ERP (Dolibarr...)E-commerce (PrestaShop, WooCommerce, Magento, osCommerce, Sylius, etc.)Partis politiques (Parti chrétien-démocrate (France), etc.)Universités et formations supérieures alliant art et sciences (Ingénieur IMAC, UPEM, etc.)Un serveur Web en architecture trois tiers est composé d'un système d'exploitation, un serveur HTTP, un langage serveur et enfin un système de gestion de base de données (SGBD), cela constituant une plate-forme.Dans le cas de PHP comme langage serveur, les combinaisons les plus courantes sont celles d'une plateforme LAMP (pour Linux Apache MySQL PHP) et WAMP (Windows Apache MySQL PHP). Une plate-forme WAMP s'installe généralement par le biais d'un seul logiciel qui intègre Apache, MySQL et PHP, par exemple EasyPHP, VertrigoServ, WampServer ou UwAmp. Il existe le même type de logiciels pour les plates-formes MAMP (Mac OS Apache MySQL PHP), à l'exemple du logiciel MAMP.Il existe d'autres variantes, par exemple les plates-formes LAPP (le M de MySQL est remplacé par le P de PostgreSQL) ou encore le logiciel XAMPP (Apache MySQL Perl PHP ; le X indique que le logiciel est multiplate-forme), un kit de développement multiplate-forme.On peut décliner une grande variété d'acronymes sous cette forme. Des confusions peuvent parfois exister entre la plate-forme en elle-même et le logiciel permettant de l'installer, si elles ont le même nom. Il faut également remarquer que la grande majorité des logiciels « tout en un » sont destinés au développement d'applications Web en local, et non à être installés sur des serveurs Web. Une exception à cette règle est peut-être Zend Server, le serveur distribué par Zend Technologies, qui est prévu pour fonctionner aussi bien en environnement de développement que de production.PHP est à la base un langage interprété, ce qui est au détriment de la vitesse d'exécution du code. Sa forte popularité associée à son utilisation sur des sites Web à très fort trafic (Yahoo, Facebook) ont amené un certain nombre de personnes à chercher à améliorer ses performances pour pouvoir servir un plus grand nombre d'utilisateurs de ces sites Web sans nécessiter l'achat de nouveaux serveurs.La réécriture du cœur de PHP, qui a abouti au Zend Engine pour PHP 4 puis au Zend Engine 2 pour PHP 5, est une optimisation. Le Zend Engine compile en interne le code PHP en bytecode exécuté par une machine virtuelle. Les projets open source APC et eAccelerator fonctionnent en mettant le bytecode produit par Zend Engine en cache afin d'éviter à PHP de charger et d'analyser les scripts à chaque requête. À partir de la version 5.5 de PHP, le langage dispose d'un cache d'opcode natif (appelé OpCache) rendant obsolète le module APC.Il existe également des projets pour compiler du code PHP :Roadsend et phc compilent du PHP en C ;Quercus compile du PHP en bytecode Java exécutable sur une machine virtuelle Java ;Phalanger compile du PHP en Common Intermediate Language exécutable sur le Common Language Runtime du framework .NET ;HipHop for PHP transforme du PHP en C++ qui est ensuite compilé en code natif. Ce projet open source a été démarré par Facebook.(en) Luke Welling et Laura Thomson, PHP and MySQL Web development, Sams Publishing, 2008, 4e éd. (ISBN 978-0-672-32916-6 et 0-672-32916-6, OCLC 854795897)Damien Seguy et Philippe Gamache, Sécurité PHP 5 et MySQL, 3e édition, Eyrolles, 1er décembre 2011, 277 p. (ISBN 978-2-212-13339-4 et 2-212-13339-1, lire en ligne)Jean Engels PHP 5 Cours et Exercices, 3e édition, Eyrolles 2013, 631 pages  (ISBN 978-2-212-13725-5)Paamayim Nekudotayim : nom de l'opérateur :: en PHPListe de frameworks PHP : liste des cadres de développement (Frameworks) en PHPSuhosin: module de durcissement de PHP5(en) Site officiel Portail des logiciels libres   Portail de la programmation informatique"
Informatique;"La programmation orientée objet (POO), ou programmation par objet, est un paradigme de programmation informatique. Elle consiste en la définition et l'interaction de briques logicielles appelées objets ; un objet représente un concept, une idée ou toute entité du monde physique, comme une voiture, une personne ou encore une page d'un livre. Il possède une structure interne et un comportement, et il sait interagir avec ses pairs. Il s'agit donc de représenter ces objets et leurs relations ; l'interaction entre les objets via leurs relations permet de concevoir et réaliser les fonctionnalités attendues, de mieux résoudre le ou les problèmes. Dès lors, l'étape de modélisation revêt une importance majeure et nécessaire pour la POO. C'est elle qui permet de transcrire les éléments du réel sous forme virtuelle.La programmation par objet consiste à utiliser des techniques de programmation pour mettre en œuvre une conception basée sur les objets. Celle-ci peut être élaborée en utilisant des méthodologies de développement logiciel objet, dont la plus connue est le processus unifié (« Unified Software Development Process » en anglais), et exprimée à l'aide de langages de modélisation tels que le Unified Modeling Language (UML).La programmation orientée objet est facilitée par un ensemble de technologies dédiées :les langages de programmation (chronologiquement : Simula, LOGO, Smalltalk, Ada, C++, Objective C, Eiffel, Python, PHP, Java, Ruby, AS3, C#, VB.NET, Fortran 2003, Vala, Haxe, Swift) ;les outils de modélisation qui permettent de concevoir sous forme de schémas semi-formels la structure d'un programme (Objecteering, UMLDraw, Rhapsody, DBDesigner…) ;les bus distribués (DCOM, CORBA, RMI, Pyro…) ;les ateliers de génie logiciel ou AGL (Visual Studio pour des langages Dotnet, NetBeans ou Eclipse pour le langage Java).Il existe actuellement deux grandes catégories de langages à objets : les langages à classes, que ceux-ci soient sous forme fonctionnelle (Common Lisp Object System), impérative (C++, Java) ou les deux (Python, OCaml) ;les langages à prototypes (JavaScript, Lua).En implantant les Record Class de Hoare, le langage Simula 67 pose les constructions qui seront celles des langages orientés objet à classes : classe, polymorphisme, héritage, etc. Mais c'est réellement par et avec Smalltalk 71 puis Smalltalk 80, inspiré en grande partie par Simula 67 et Lisp, que les principes de la programmation par objets, résultat des travaux d'Alan Kay, sont véhiculés : objet, encapsulation, messages, typage et polymorphisme (via la sous-classification) ; les autres principes, comme l'héritage, sont soit dérivés de ceux-ci ou une implantation. Dans Smalltalk, tout est objet, même les classes. Il est aussi plus qu'un langage à objets, c'est un environnement graphique interactif complet.À partir des années 1980, commence l'effervescence des langages à objets : C++ (1983), Objective-C (1984), Eiffel (1986), Common Lisp Object System (1988), etc. Les années 1990 voient l'âge d'or de l'extension de la programmation par objets dans les différents secteurs du développement logiciel.Depuis, la programmation par objets n'a cessé d'évoluer aussi bien dans son aspect théorique que pratique et différents métiers et discours mercatiques à son sujet ont vu le jour :l'analyse objet (AOO ou OOA en anglais) ;la conception objet (COO ou OOD en anglais) ;les bases de données objet (SGBDOO) ;les langages objets avec les langages à prototypes ;ou encore la méthodologie avec MDA (Model Driven Architecture).Aujourd'hui, la programmation par objets est vue davantage comme un paradigme, le paradigme objet, que comme une simple technique de programmation. C'est pourquoi, lorsque l'on parle de nos jours de programmation par objets, on désigne avant tout la partie codage d'un modèle à objets obtenu par AOO et COO.La programmation orientée objet a été introduite par Alan Kay avec Smalltalk. Toutefois, ses principes n'ont été formalisés que pendant les années 1980 et, surtout, 1990. Par exemple le typage de second ordre, qui qualifie le typage de la programmation orientée objet (appelé aussi duck typing), n'a été formulé qu'en 1995 par Cook.Concrètement, un objet est une structure de données qui répond à un ensemble de messages. Cette structure de données définit son état tandis que l'ensemble des messages qu'il comprend décrit son comportement :les données, ou champs, qui décrivent sa structure interne sont appelées ses attributs ;l'ensemble des messages forme ce que l'on appelle l'interface de l'objet ; c'est seulement au travers de celle-ci que les objets interagissent entre eux. La réponse à la réception d'un message par un objet est appelée une méthode (méthode de mise en œuvre du message) ; elle décrit quelle réponse doit être donnée au message.Certains attributs et/ou méthodes (ou plus exactement leur représentation informatique) sont cachés : c'est le principe d'encapsulation. Ainsi, le programme peut modifier la structure interne des objets ou leurs méthodes associées sans avoir d'impact sur les utilisateurs de l'objet.Un exemple avec un objet représentant un nombre complexe : celui-ci peut être représenté sous différentes formes (cartésienne (réel, imaginaire), trigonométrique, exponentielle (module, angle)). Cette représentation reste cachée et est interne à l'objet. L'objet propose des messages permettant de lire une représentation différente du nombre complexe. En utilisant les seuls messages que comprend notre nombre complexe, les objets appelants sont assurés de ne pas être affectés lors d'un changement de sa structure interne. Cette dernière n'est accessible que par les méthodes des messages.Dans la programmation par objets, chaque objet est typé. Le type définit la syntaxe (« Comment l'appeler ? ») et la sémantique des messages (« Que fait-il ? ») auxquels peut répondre un objet. Il correspond donc, à peu de chose près, à l'interface de l'objet. Toutefois, la plupart des langages objets ne proposent que la définition syntaxique d'un type (C++, Java, C#…) et rares sont ceux qui fournissent aussi la possibilité de définir formellement sa sémantique (comme dans le langage Eiffel avec sa conception par contrats).Un objet peut appartenir à plus d'un type : c'est le polymorphisme ; cela permet d'utiliser des objets de types différents là où est attendu un objet d'un certain type. Une façon de réaliser le polymorphisme est le sous-typage (appelé aussi héritage de type) : on raffine un type-parent en un autre type (le sous-type) par des restrictions sur les valeurs possibles des attributs. Ainsi, les objets de ce sous-type sont conformes au type parent. De ceci découle le principe de substitution de Liskov. Toutefois, le sous-typage est limité et ne permet pas de résoudre le problème des types récursifs (un message qui prend comme paramètre un objet du type de l'appelant). Pour résoudre ce problème, Cook définit en 1995 la sous-classification et le typage du second ordre qui régit la programmation orientée objet : le type est membre d'une famille polymorphique à point fixe de types (appelée classe). Les traits sont une façon de représenter explicitement les classes de types. (La représentation peut aussi être implicite comme avec Smalltalk, Ruby, etc.).On distingue dans les langages objets deux mécanismes du typage :le typage dynamique : le type des objets est déterminé à l'exécution lors de la création desdits objets (Smalltalk, Common Lisp, Python, PHP…) ;le typage statique : le type des objets est vérifié à la compilation et est soit explicitement indiqué par le développeur lors de leur déclaration (C++, Java, C#, Pascal…), soit déterminé par le compilateur à partir du contexte (Scala, OCaml…).De même, deux mécanismes de sous-typage existent : l'héritage simple (Smalltalk, Java, C#) et l'héritage multiple (C++, Python, Common Lisp, Eiffel, WLangage).Le polymorphisme ne doit pas être confondu avec le sous-typage ou avec l'attachement dynamique (dynamic binding en anglais).La programmation objet permet à un objet de raffiner la mise en œuvre d'un message défini pour des objets d'un type parent, autrement dit de redéfinir la méthode associée au message : c'est le principe de redéfinition des messages (ou overriding en anglais).Or, dans une définition stricte du typage (typage du premier ordre), l'opération résultant d'un appel de message doit être la même quel que soit le type exact de l'objet référé. Ceci signifie donc que, dans le cas où l'objet référé est de type exact un sous-type du type considéré dans l'appel, seule la méthode du type père est exécutée :Soit un type Reel contenant une méthode * faisant la multiplication de deux nombres réels, soient Entier un sous-type de Reel, i un Entier et r un Reel, alors l'instruction i * r va exécuter la méthode * de Reel. On pourrait appeler celle de Entier grâce à une redéfinition.Pour réaliser alors la redéfinition, deux solutions existent :le typage du premier ordre associé à l'attachement dynamique (c'est le cas de C++, Java, C#…). Cette solution induit une faiblesse dans le typage et peut conduire à des erreurs. Les relations entre type sont définies par le sous-typage (théorie de Liskov) ;le typage du second ordre (duquel découlent naturellement le polymorphisme et l'appel de la bonne méthode en fonction du type exact de l'objet). Ceci est possible avec Smalltalk et Eiffel. Les relations entre types sont définies par la sous-classification (théorie F-Bound de Cook).La structure interne des objets et les messages auxquels ils répondent sont définis par des modules logiciels. Ces mêmes modules créent les objets via des opérations dédiées. Deux représentations existent de ces modules : la classe et le prototype.La classe est une structure informatique particulière dans le langage objet. Elle décrit la structure interne des données et elle définit les méthodes qui s'appliqueront aux objets de même famille (même classe) ou type. Elle propose des méthodes de création des objets dont la représentation sera donc celle donnée par la classe génératrice. Les objets sont dits alors instances de la classe. C'est pourquoi les attributs d'un objet sont aussi appelés variables d'instance et les messages opérations d'instance ou encore méthodes d'instance. L'interface de la classe (l'ensemble des opérations visibles) forme les types des objets. Selon le langage de programmation, une classe est soit considérée comme une structure particulière du langage, soit elle-même comme un objet (objet non-terminal). Dans le premier cas, la classe est définie dans le runtime ; dans l'autre, la classe a besoin elle aussi d'être créée et définie par une classe : ce sont les méta-classes. L'introspection des objets (ou « méta-programmation ») est définie dans ces méta-classes.La classe peut être décrite par des attributs et des messages. Ces derniers sont alors appelés, par opposition aux attributs et messages d'un objet, variables de classe et opérations de classe ou méthodes de classe. Parmi les langages à classes on retrouve Smalltalk, C++, C#, Java, etc.Le prototype est un objet à part entière qui sert de prototype de définition de la structure interne et des messages. Les autres objets de mêmes types sont créés par clonage. Dans le prototype, il n'y a plus de distinction entre attributs et messages : ce sont tous des slots. Un slot est un label de l'objet, privé ou public, auquel est attachée une définition (ce peut être une valeur ou une opération). Cet attachement peut être modifié à l'exécution. Chaque ajout d'un slot influence l'objet et l'ensemble de ses clones. Chaque modification d'un slot est locale à l'objet concerné et n'affecte pas ses clones.Le concept de trait permet de modifier un slot sur un ensemble de clones. Un trait est un ensemble d'opérations de même catégorie (clonage, persistance, etc.) transverse aux objets. Il peut être représenté soit comme une structure particulière du langage, comme un slot dédié ou encore comme un prototype. L'association d'un objet à un trait fait que l'objet et ses clones sont capables de répondre à toutes les opérations du trait. Un objet est toujours associé à au moins un trait, et les traits sont les parents des objets (selon une relation d'héritage). Un trait est donc un mixin doté d'une parenté. Parmi les langages à prototype on trouve Javascript, Self, Io, Slater, Lisaac, etc.Différents langages utilisent la programmation orientée objet, par exemple PHP, Python, etc.En PHP la programmation orientée objet est souvent utilisée pour mettre en place une architecture MVC (Modèle Vue Contrôleur), où les modèles représentent des objets.La modélisation objet consiste à créer un modèle du système informatique à réaliser. Ce modèle représente aussi bien des objets du monde réel que des concepts abstraits propres au métier ou au domaine dans lequel le système sera utilisé.La modélisation objet commence par la qualification de ces objets sous forme de types ou de classes sous l'angle de la compréhension des besoins et indépendamment de la manière dont ces classes seront mises en œuvre. C'est ce que l'on appelle l'analyse orientée objet ou OOA (acronyme de « Object-Oriented Analysis »). Ces éléments sont alors enrichis et adaptés pour représenter les éléments de la solution technique nécessaires à la réalisation du système informatique. C'est ce que l'on appelle la conception orientée objet ou OOD (acronyme de « Object-Oriented Design »). À un modèle d'analyse peuvent correspondre plusieurs modèles de conception. L'analyse et la conception étant fortement interdépendants, on parle également d'analyse et de conception orientée objet (OOAD). Une fois un modèle de conception établi, il est possible aux développeurs de lui donner corps dans un langage de programmation. C'est ce que l'on appelle la programmation orientée objet ou OOP (en anglais « Object-Oriented Programming »). Pour écrire ces différents modèles, plusieurs langages et méthodes ont été mis au point. Ces langages sont pour la plupart graphiques. Les trois principaux à s'imposer sont OMT de James Rumbaugh, la méthode Booch de Grady Booch et OOSE de Ivar Jacobson. Toutefois, ces méthodes ont des sémantiques différentes et ont chacune des particularités qui les rendent particulièrement aptes à certains types de problèmes. OMT offre ainsi une modélisation de la structure de classes très élaborée. Booch a des facilités pour la représentation des interactions entre les objects. OOSE innove avec les cas d'utilisation pour représenter le système dans son environnement. La méthode OMT prévaut sur l'ensemble des autres méthodes au cours de la première partie de la décennie 1990.À partir de 1994, Booch et Jacobson, rapidement rejoints par Rumbaugh, décident d'unifier leurs approches au sein d'une nouvelle méthode qui soit suffisamment générique pour pouvoir s'appliquer à la plupart des contextes applicatifs. Ils commencent par définir le langage de modélisation UML (Unified Modeling Language) appelé à devenir un standard de l'industrie. Le processus de normalisation est confié à l'Object Management Group (OMG), un organisme destiné à standardiser des technologies orientées objet comme CORBA (acronyme de « Common Object Request Broker Architecture »), un intergiciel (« middleware » en anglais) objet réparti. Rumbaugh, Booch et Jacobson s'affairent également à mettre au point une méthode permettant d'une manière systématique et répétable d'analyser les exigences et de concevoir et mettre en œuvre une solution logicielle à l'aide de modèles UML. Cette méthode générique de développement orienté objet devient le processus unifié (également connu sous l'appellation anglo-saxonne de « Unified Software Development Process »). Elle est itérative et incrémentale, centrée sur l'architecture et guidée par les cas d'utilisation et la réduction des risques. Le processus unifié est de plus adaptable par les équipes de développement pour prendre en compte au mieux les particularités du contexte.Néanmoins pour un certain nombre de concepteurs objet, dont Bertrand Meyer, l'inventeur du langage orienté objet Eiffel, guider une modélisation objet par des cas d'utilisations est une erreur de méthode qui n'a rien d'objet et qui est plus proche d'une méthode fonctionnelle. Pour eux, les cas d'utilisations sont relégués à des utilisations plutôt annexes comme la validation d'un modèle par exemple[réf. nécessaire].(en) Brad J. Cox et Andrew J. Novobilski, Object-Oriented Programming : An Evolutionary Approach, Addison-Wesley, 1986 (ISBN 0-201-54834-8).Grady Booch, James Rumbaugh et Ivar Jacobson, Le guide de l'utilisateur UML, EYROLLES, 2000 (ISBN 2-212-09103-6).Erich Gamma, Richard Helm, Ralph Johnson et John Vlissides (trad. de l'anglais par Jean-Marie Lasvergères), Design Patterns : Catalogue des modèles de conception réutilisables, Vuibert, 1999 (ISBN 2-7117-8644-7).Bertrand Meyer (2000). Conception et programmation orientées objet,  (ISBN 2-212-09111-7).De Hugues Bersini (2007). L'Orienté Objet,  (ISBN 978-2-212-12084-4).Francisco Bonito (2000). La programmation : l'orienté objet.Introduction à la POO Apprendre simplement la Programmation Orientée ObjetDes paradigmes « classiques » à l'orienté objetAnalyse et conception orientée objet avec UML et RUP, un survol rapide(en) The Theory of Classification de Anthony J.H. Simons sur le JOT (Journal of Object Technology) Portail de la programmation informatique"
Informatique;"En informatique, la programmation procédurale est un paradigme  qui se fonde sur le concept d'appel procédural. Une procédure, aussi appelée routine, sous-routine ou fonction (à ne pas confondre avec les fonctions de la programmation fonctionnelle reposant sur des fonctions mathématiques), contient simplement une série d'étapes à réaliser. N'importe quelle procédure peut être appelée à n'importe quelle étape de l'exécution du programme, y compris à l'intérieur d'autres procédures, voire dans la procédure elle-même (récursivité).La programmation procédurale est un meilleur choix qu'une simple programmation séquentielle. Les avantages sont en effet les suivants :la possibilité de réutiliser le même code à différents emplacements dans le programme sans avoir à le dupliquer (principe « DRY »), ce qui a pour effet la réduction de la taille du code source et un gain en localité des modifications, donc une amélioration de la maintenabilité (compréhension plus rapide, réduction du risque de régression) ;une façon plus simple de suivre l'exécution du programme : la programmation procédurale permet de se passer d'instructions telles que goto, évitant ainsi bien souvent de se retrouver avec un programme compliqué qui part dans toutes les directions (appelé souvent « programmation spaghetti[réf. nécessaire] ») ; cependant, la programmation procédurale permet les « effets de bord », c'est-à-dire la possibilité pour une procédure qui prend des arguments de modifier des variables extérieures à la procédure auxquelles elle a accès (variables de contexte plus global que la procédure).La modularité est une caractéristique souhaitable pour un programme ou une application informatique, et consiste enle découpage du programme ou de l'application en unités sans effet de bord entre elles, c'est-à-dire dont le fonctionnement et le résultat renvoyé au module appelant ne dépend que des paramètres explicitement passés en argument (unités fonctionnelles). Un module est un ensemble de structure de données et de procédures, dont l'effet de bord est confiné à cet ensemble de données.De ce fait, un module offre un service. Un module peut avoir un contexte d'exécution différent de celui du code appelant : on parle alors de RPC (Remote Procedure Call) si ce contexte est un autre ordinateur, ou de communication inter-processus (légers ou système) s'il s'agit du même ordinateur. Lorsque la transmission des données ne se fait pas en mémoire mais par fichiers, les modules qui communiquent peuvent être compilés séparément et un script doit assurer l'enchainement des appels.On constate qu'il n'est pas contre-indiqué pour une procédure d'accéder en lecture et en écriture à des variables de contexte plus global (celui d'un module) : cela permet une réduction essentielle du nombre d'arguments passés, mais au détriment de la réutilisation telle quelle dans d'autres contextes d'une procédure. C'est le module en entier qui est réutilisable.Du fait de leur comportement sans effet de bord, chaque module peut être développé par une personne ou un groupe de personnes distinct de ceux qui développent d'autres modules. Les bibliothèques sont des modules. À noter que pour qu'une procédure puisse être considérée comme se comportant comme une « fonction pure » mathématique, il faut que la valeur de son résultat renvoyé au programme appelant prenne toujours la même valeur pour chaque valeur des arguments. Il faut donc qu'elle ne dépende pas d'une variable globale statique éventuelle du module, statique au sens qu'elle garde sa valeur après la fin de l'invocation du module (par une de ses procédures).La programmation objet et générique permet une mutualisation et une unicité de l'information et des traitements/procédures/méthodes (en théorie). En identifiant les variables globales à un module à une structure au sens C ou Pascal et à un type utilisateur, ces modules deviennent par définition des « classes » dont l'instanciation correspond à l'instanciation d'un type composé (une structure C ou Pascal).De plus, par le jeu du polymorphisme et de la généricité, les méthodes d'une classe, qui correspondent exactement aux procédures du module correspondant, en confiant l'effet de bord aux attributs de cette classe, peuvent accepter des arguments dont le type est variable (d'une manière contrôlée par le graphe d'héritage). De ce fait, la programmation objet va plus loin dans la factorisation des traitements que la programmation procédurale (en prolongeant celle-ci), et permet de répondre bien mieux à des besoins où des traitements similaires sont attendus dans des endroits différents d'une solution. Dans une programmation objet aboutie, les « procédures d'aiguillage » (routage de traitements en fonction du type d'une variable passée en argument) sont reléguées au compilateur par utilisation de la liaison dynamique. Le code source s'en trouve aussi réduit.Le plus vieil exemple de ce type de langage est l'ALGOL. D'autres exemples sont Fortran, PL/I, Modula-2 et Ada (dans sa première version). Portail de la programmation informatique"
Informatique;"Un serveur web est soit un logiciel de service de ressources web (serveur HTTP), soit un serveur informatique (ordinateur) qui répond à des requêtes du World Wide Web sur un réseau public (Internet) ou privé (intranet),,, en utilisant principalement le protocole HTTP.Un serveur informatique peut être utilisé à la fois pour servir des ressources du Web et pour faire fonctionner en parallèle d'autres services liés, comme l'envoi d'e-mails, l'émission de flux en streaming, le stockage de données dans des bases de données, le transfert de fichiers par FTP.Les serveurs web publics sont reliés à Internet et hébergent des ressources (pages web, images, vidéos, etc.) du Web. Ces ressources peuvent être statiques (servies telle quelles) ou dynamiques (construites à la demande par le serveur).Certains serveurs sont seulement accessibles sur des réseaux privés (intranets) et hébergent des sites utilisateurs, des documents, ou des logiciels, internes à une entreprise, une administration, etc.Techniquement il serait possible qu'un même ordinateur remplisse ces deux fonctions, mais c'est rarement le cas pour des raisons de sécurité[réf. nécessaire]. La fonction principale d'un serveur Web est de stocker et délivrer des pages web qui sont généralement rendues en HTML. Le protocole de communication Hypertext Transfer Protocol (HTTP) permet le dialogue via le réseau avec le logiciel client, généralement un navigateur web.Les deux termes sont utilisés pour le logiciel car le protocole HTTP a été développé pour le Web, et les pages Web sont en pratique toujours servies avec ce protocole. Cependant d'autres ressources du Web comme les fichiers à télécharger ou les flux audio ou vidéo sont parfois servis avec d'autres protocoles, telle que, par exemple, le protocole de transport Temps Réel (Real-time Transport Protocol), ainsi que son pendant sécurisé, le protocole de transport sécurisé Temps Réel (Secure Real-time Transport Protocol).CERN httpd est le premier serveur HTTP, inventé en même temps que le World Wide Web, en 1990 au CERN de Genève. Il est rapidement devenu obsolète en raison de l'évolution exponentielle des fonctionnalités du protocole.Quelques serveurs HTTP :Apache HTTP Server de la Apache Software Foundation, successeur du NCSA HTTPd ;Apache Tomcat de la Apache Software Foundation, évolution de Apache pour J2EE ;BusyBox httpd, utilisé dans le domaine de l'informatique embarquée, et notamment avec OpenWRT ;Google Web Server de Google ;Internet Information Services (IIS) de Microsoft ;lighttpd de Jan Kneschke ;Monkey web server de Eduardo Silva Pereira, dédié au noyau Linux, permettant d'utiliser pleinement ses fonctionnalités ;nginx d'Igor Sysoev ;Hiawatha de Hugo LeisinkNodeJS sous MIT Licence conçu par Ryan Lienhart Dahl en lignes de programmation en JavaScript ;Sun Java System Web Server de Sun Microsystems (anciennement iPlanet de Netscape, puis Sun ONE de Sun Microsystems) ;Tengine, fork de nginx, de Taobao (9e rang mondial Alexa en juillet 2014) ;Zeus Web Server de Zeus Technology ;Gunicorn est un serveur web HTTP WSGI écrit en Python pour Unix ;Zazouminiwebserver, serveur extrêmement léger (approx. 500 kilooctets), sous environnement Microsoft Windows.Abyss Web Server, un serveur gratuit, multi-plateforme (Linux, Windows, MacOS, BSD), permettant un paramétrage très facile via une interface graphique multilingue.Le serveur HTTP le plus utilisé est Apache HTTP Server qui sert environ 55 % des sites web en janvier 2013 selon Netcraft.Le serveur HTTP le plus utilisé dans les 1 000 sites les plus actifs est en revanche Nginx avec 38,2 % de parts de marché en 2016  selon w3techs et 53,9 % en avril 2017Historiquement, d'autres serveurs HTTP importants furent CERN httpd, développé par les inventeurs du Web, abandonné le 15 juillet 1996 et NCSA HTTPd, développé au NCSA en même temps que NCSA Mosaic, abandonné mi-1994, ainsi que WebObjects.Il existe aussi des serveurs HTTP qui sont des serveurs d'applications capables de faire serveur HTTP, comme Caudium et GlassFish. À l'inverse, on peut trouver des serveurs HTTP spécialisés dans un service distinct comme : HTTP File Server qui est uniquement destiné au partage de fichiersLe logiciel serveur HTTP ou daemon HTTP est le logiciel prenant en charge les requêtes client-serveur du protocole HTTP développé pour le World Wide Web. Ces logiciels intègrent généralement des modules permettant d'exécuter un langage serveur comme PHP pour générer des pages web dynamiques. Les plus connus sont Apache, Nginx, IIS, et Lighttpd.Le plus souvent, un serveur Web exécute continuellement d'autres logiciels qui fonctionnent en collaboration avec le logiciel de serveur HTTP. Selon les besoins, certains services gourmands en ressources, comme le serveur de base de données, peuvent être situés sur la même machine ou un serveur spécialisé.Certaines combinaisons de logiciels de base sont connues sous différents acronymes, notamment celle d'Apache (serveur HTTP) logiciel installé et exécuté sur le serveur web en parallèle de MySQL (serveur de base de données) et le script d'interprétation et d'exécution de PHP (voire PHP-FPM).Voir en PDF l'introduction « Qu'entend-t-on par serveur HTTP et serveur Web ? » d'Anthony Garcia (2008) - IBISC[source insuffisante] :LAMP pour « Linux, Apache, MySQL, PHP » ;WAMP pour « Windows, Apache, MySQL, PHP » ;MAMP pour « Macintosh, Apache, MySQL, PHP ».Il existe aussi la distribution de Microsoft nommée IIS pour « Internet Information Services » qui comprend plusieurs services : HTTP, FTP, SMTP et NNTP.L’équilibrage de charge des serveurs web, ou répartition de charge des serveurs Web, regroupe l’ensemble des mécanismes utilisés pour distribuer les requêtes sur de multiples serveurs Web. Cette pratique est devenue indispensable depuis l’explosion du trafic du Web qui a pour conséquence un accroissement important de la charge demandé au serveur. Cela a entraîné une évolution des architectures, destinée à apporter plus de scalabilité, de disponibilité et de performances. Portail des réseaux informatiques   Portail d’Internet   Portail de l’informatique"
Informatique;"Un superordinateur ou supercalculateur est un ordinateur conçu pour atteindre les plus hautes performances possibles avec les techniques connues lors de sa conception, en particulier en ce qui concerne la vitesse de calcul. Pour des raisons de performance, c'est presque toujours un ordinateur central, dont les tâches sont fournies en traitement par lots.La science des superordinateurs est appelée « calcul haute performance » (en anglais : high-performance computing ou HPC). Cette discipline se divise en deux : la partie matérielle (conception électronique de l'outil de calcul) et la partie logicielle (adaptation logicielle du calcul à l'outil). Ces deux parties font appel à des champs de connaissances différents.Les premiers superordinateurs (ou supercalculateurs) apparaissent dans les années 1960.En 1961, IBM développe l'IBM Stretch ou IBM 7030, dont une unité est exploitée en France en 1963.À cette époque, et jusque dans les années 1970, le plus important constructeur mondial de superordinateurs est la société Control Data Corporation (CDC), avec son concepteur Seymour Cray. Par la suite, Cray Research, fondée par Seymour Cray après son départ de CDC, prend l’avantage sur ses autres concurrents, jusqu’aux alentours de l'année 1990. Dans les années 1980, à l’image de ce qui s’était produit sur le marché des micro-ordinateurs des années 1970, de nombreuses petites sociétés se lancèrent sur ce marché, mais la plupart disparaissent dans le « crash » du marché des superordinateurs, au milieu des années 1990.Ce que désigne le terme superordinateur varie avec le temps, car les ordinateurs les plus puissants du monde à un moment donné tendent à être égalés, puis dépassés, par des machines d’utilisation courante plusieurs années après. Les premiers superordinateurs CDC étaient de simples ordinateurs mono-processeurs (mais possédant parfois jusqu’à dix processeurs périphériques pour les entrées-sorties) environ dix fois plus rapides que la concurrence. Dans les années 1970, la plupart des superordinateurs adoptent un processeur vectoriel, qui effectue le décodage d’une instruction une seule fois pour l’appliquer à toute une série d’opérandes.C’est seulement vers la fin des années 1980 que la technique des systèmes massivement parallèles est adoptée, avec l’utilisation dans un même superordinateur de milliers de processeurs. De nos jours, certains de ces superordinateurs parallèles utilisent des microprocesseurs de type « RISC », conçus pour des ordinateurs de série, comme les PowerPC ou les PA-RISC. D’autres supercalculateurs utilisent des processeurs de moindre coût, de type « CISC », microprogrammés en RISC dans la puce électronique (AMD ou Intel) : le rendement en est un peu moins élevé, mais le canal d’accès à la mémoire — souvent un goulet d’étranglement — est bien moins sollicité.Au XXIe siècle, les superordinateurs sont le plus souvent conçus comme des modèles uniques par des constructeurs informatiques « traditionnels » comme International Business Machines (IBM), Hewlett-Packard (HP), ou Bull, qu’ils aient derrière eux une longue tradition en la matière (IBM) ou qu’ils aient racheté dans les années 1990 des entreprises spécialisées, alors en difficulté, pour acquérir de l’expérience dans ce domaine.Les superordinateurs sont utilisés pour toutes les tâches qui nécessitent une très forte puissance de calcul, comme les prévisions météorologiques, l’étude du climat (à ce sujet, voir les programmes financés par le G8-HORCs), la modélisation d'objets chimiques (calcul de structures et de propriétés, modélisation moléculaire, etc.), les simulations physiques (simulations aérodynamiques, calculs de résistance des matériaux, simulation d'explosion d'arme nucléaire, étude de la fusion nucléaire, etc.), la cryptanalyse ou les simulations en finance et en assurance (calcul stochastique).Les institutions de recherche civiles et militaires comptent parmi les plus gros utilisateurs de superordinateurs.En France, on trouve ces machines dans les centres nationaux de calculs universitaires, tels que l'Institut du développement et des ressources en informatique scientifique (IDRIS), le Centre informatique national de l'enseignement supérieur (CINES), mais aussi au Commissariat à l'énergie atomique et aux énergies alternatives (CEA) ou dans certaines grandes entreprises, comme Total, EDF ou encore Météo-France.Les superordinateurs tirent leur supériorité sur les ordinateurs conventionnels à la fois grâce à :leur architecture, en « pipeline » (exécution d’une instruction identique sur une longue série de données) ou parallèle (nombre très élevé de processeurs fonctionnant chacun sur une partie du calcul), qui leur permet d’exécuter plusieurs tâches simultanément ;des composants électroniques rapides (structure de type serveurs lame utilisant des processeurs multi-cœur ou des cartes graphiques dédiées au calcul scientifique de dernière génération, de la mémoire vive et des équipements de stockage de masse — disque dur — reliés à la fibre optique en grande quantité, etc.) associés à un système d'exploitation dédié (comme Linux, majoritairement utilisé actuellement).Ils sont presque toujours conçus spécifiquement pour un certain type de tâches (le plus souvent des calculs numériques scientifiques : calcul matriciel ou vectoriel) et ne cherchent pas de performance particulière dans d'autres domaines.L’architecture mémorielle des supercalculateurs est étudiée pour fournir en continu les données à chaque processeur afin d’exploiter au maximum sa puissance de calcul. Les performances supérieures de la mémoire (meilleurs composants et meilleure architecture) expliquent pour une large part l’avantage des superordinateurs sur les ordinateurs classiques.Leur système d’entrée/sortie (bus) est conçu pour fournir une large bande passante, la latence étant moins importante puisque ce type d’ordinateur n’est pas conçu pour traiter des transactions.Comme pour tout système parallèle, la loi d’Amdahl s’applique, les concepteurs de superordinateurs consacrant une partie de leurs efforts à éliminer les parties non parallélisables du logiciel et à développer des améliorations matérielles pour supprimer les goulots d'étranglement restants.D'une part, les superordinateurs ont souvent besoin de plusieurs mégawatts de puissance électrique. Cette alimentation doit aussi être de qualité. En conséquence, ils produisent une grande quantité de chaleur et doivent donc être refroidis pour fonctionner normalement. Le refroidissement (par exemple à air) de ces ordinateurs pose souvent un problème important de climatisation.D'autre part, les données ne peuvent circuler plus vite que la vitesse de la lumière entre deux parties d'un ordinateur. Lorsque la taille d’un superordinateur dépasse plusieurs mètres, le temps de latence entre certains composants se compte en dizaines de nanosecondes. Les éléments sont donc disposés pour limiter la longueur des câbles qui relient les composants. Sur le Cray-1 ou le Cray-II, par exemple, ils étaient disposés en cercle.De nos jours, ces ordinateurs sont capables de traiter et de communiquer de très importants volumes de données en très peu de temps. La conception doit assurer que ces données puissent être lues, transférées et stockées rapidement. Dans le cas contraire, la puissance de calcul des processeurs serait sous-exploitée (goulot d’étranglement).           En 1993, l'Institut de Physique du Globe de Paris (IPGP) opère un ordinateur CM-5/128 qui utilise des processeurs SuperSPARC, il est classé 25e au TOP500. Trois ans plus tard, en 1996, l'Institut du développement et des ressources en informatique scientifique (IDRIS) parvient à atteindre la 12e place mondiale avec le T3E construit par Cray.À la mi-2002, le plus puissant des supercalculateurs français se classe 4e au TOP500, c'est le TERA basé sur des processeurs Alpha à 1 GHz (AlphaServer SC45) et développé par Hewlett-Packard ; il appartenait au Commissariat à l'énergie atomique (CEA). En janvier 2006, le TERA-10 de Bull lui succède, il génère une puissance de calcul de 60 téraFLOPS et se placera au 5e rang mondial du TOP500.En juin 2008, l'IDRIS et son Blue Gene/P Solution d'IBM affiche, selon le test LINPACK, une puissance de 120 téraflops et remporte la 10e place.En novembre 2009, la première machine française a pour nom Jade. De type « SGI Altix (en) » elle est basée au Centre informatique national de l'enseignement supérieur (CINES) de Montpellier. Ce supercalculateur se classe au 28e rang mondial avec 128 téraflops au test LINPACK. Peu après, la configuration de la machine Jade est complétée pour atteindre une performance de 237 téraflops. La machine passe en juin 2010 au 18e rang du TOP500. C’est alors le troisième système informatique européen et le premier français, il est destiné à la recherche publique.En novembre 2010, le record français est détenu par le TERA-100 de Bull. Installé au CEA à Bruyères-le-Châtel pour les besoins de la simulation militaire nucléaire française, avec une performance de 1 050 téraflops, cette machine se hisse au 6e rang mondial et gagne le 1er rang européen. Elle est constituée de 17 296 processeurs Intel Xeon 7500 dotés chacun de huit cœurs et connectés par un réseau de type InfiniBand.En mars 2012, Curie, un système conçu par Bull pour le GENCI, installé sur le site du Très Grand Centre de Calcul (TGCC) à Bruyères-le-Châtel, dispose d'une puissance de 1,359 pétaflops. Il sera le supercalculateur le plus puissant de France en prenant la 9e place du classement mondial. Il est conçu pour délivrer 2 pétaflops.En janvier 2013, les systèmes Ada et Turing construits par IBM sont installés à l'IDRIS d'Orsay. La somme de leur puissance dépasse le pétaflops. Ces deux machines sont à la disposition des chercheurs. En mars 2013, le supercalculateur Pangea détenu par la société Total est inauguré, il devient le système le plus performant jamais installé en France. Sa puissance de calcul s'élève à 2,3 pétaflops. Équivalant à 27 000 ordinateurs de bureau réunis, il obtient la 11e place mondiale.En janvier 2015, le système Occigen, conçu par Bull, Atos technologies, pour le GENCI est installé sur le site du CINES ; il est doté d'une puissance de 2,1 pétaflops. Il se situait en 26e position au classement mondial du TOP500 de novembre 2014.En mars 2016, Total annonce avoir triplé la capacité de calcul de son supercalculateur Pangea, passant à une puissance de calculs de 6,7 pétaflops en pics de performance et de 5,28 pétaflops en puissance utilisable. Cela lui permet de retrouver le 11e rang au TOP500 et le place ainsi en tête du secteur industriel mondial.En juin 2022, le GENCI met en service Adastra, un superordinateur fourni par HPE-Cray hébergé au CINES. Ses 46,10 pétaflops lui permettent de gagner le 10e rang mondial en termes de performances de calcul.L'essor des supercalculateurs a vu Linux devenir le système d'exploitation équipant la majorité des 500 supercalculateurs les plus puissants de la planète,, Unix perdant progressivement du terrain face à Linux, mais occupant pendant un temps une place de choix sur le marché des supercalculateurs (5 %).[réf. souhaitée]Windows ne fut exécuté que par deux des 500 supercalculateurs les plus puissants de la planète, soit 0,4 %, tandis que BSD n'était présent que sur une seule machine du top 500, soit 0,2 %. Enfin, les autres configurations (« Mixed », soit un ensemble de plusieurs types de systèmes d'exploitation) représentaient 4,6 %.[réf. souhaitée]En novembre 2017, Linux équipe la totalité des 500 superordinateurs les plus puissants au monde.Georges Karadimas (Snecma), « Les superordinateurs dans le secteur aérospatial français », dans Nouvelle revue Aéronautique et Astronautique, no 2, juin 1994  (ISSN 1247-5793).Site HPC du commissariat à l'énergie atomique (CEA)Site officiel du centre informatique national de l'enseignement supérieur (CINES)Site officiel de l'institut du développement et des ressources en informatique scientifique (IDRIS) Portail de l’informatique"
Informatique;"Le Langage de Modélisation Unifié, de l'anglais Unified Modeling Language (UML), est un langage de modélisation graphique à base de pictogrammes conçu comme une méthode normalisée de visualisation  dans les domaines du développement logiciel et en conception orientée objet.L'UML est une synthèse de langages de modélisation objet antérieurs : Booch, OMT, OOSE. Principalement issu des travaux de Grady Booch, James Rumbaugh et Ivar Jacobson, UML est à présent un standard adopté par l'Object Management Group (OMG). UML 1.0 a été normalisé en janvier 1997; UML 2.0 a été adopté par l'OMG en juillet 2005. La dernière version de la spécification validée par l'OMG est UML 2.5.1 (2017).UML est destiné à faciliter la conception des documents nécessaires au développement d'un logiciel orienté objet, comme standard de modélisation de l'architecture logicielle. Les différents éléments représentables sont :Activité d'un objet/logicielActeursProcessusSchéma de base de donnéesComposants logicielsRéutilisation de composants.Il est également possible de générer automatiquement tout ou partie du code, par exemple en langage Java, à partir des documents réalisés.UML est un langage de modélisation. La version actuelle, UML 2.5, propose 14 types de diagrammes dont sept structurels et sept comportementaux. À titre de comparaison, UML 1.3 comportait 25 types de diagrammes.UML n'étant pas une méthode, l'utilisation des diagrammes est laissée à l'appréciation de chacun. Le diagramme de classes est généralement considéré comme l'élément central d'UML. Des méthodes, telles que le processus unifié proposé par les créateurs originels de UML, utilisent plus systématiquement l'ensemble des diagrammes et axent l'analyse sur les cas d'utilisation (« use case ») pour développer par itérations successives un modèle d'analyse, un modèle de conception, et d'autres modèles. D'autres approches se contentent de modéliser seulement partiellement un système, par exemple certaines parties critiques qui sont difficiles à déduire du code.UML se décompose en plusieurs parties :Les vues : ce sont les observables du système. Elles décrivent le système d'un point de vue donné, qui peut être organisationnel, dynamique, temporel, architectural, géographique, logique, etc. En combinant toutes ces vues, il est possible de définir (ou retrouver) le système complet.Les diagrammes : ce sont des ensembles d'éléments graphiques. Ils décrivent le contenu des vues, qui sont des notions abstraites. Ils peuvent faire partie de plusieurs vues.Les modèles d'élément : ce sont les éléments graphiques des diagrammes.Une façon de mettre en œuvre UML est de considérer différentes vues qui peuvent se superposer pour collaborer à la définition du système :Vue des cas d'utilisation (use-case view) : c'est la description du modèle vu par les acteurs du système. Elle correspond aux besoins attendus par chaque acteur (c'est le quoi et le qui).Vue logique (logical view): c'est la définition du système vu de l'intérieur. Elle explique comment peuvent être satisfaits les besoins des acteurs (c'est le comment).Vue d'implémentation (implementation view) : cette vue définit les dépendances entre les modules.Vue des processus  (process view) : c'est la vue temporelle et technique, qui met en œuvre les notions de tâches concurrentes, stimuli, contrôle, synchronisation…Vue de déploiement (deployment view) : cette vue décrit la position géographique et l'architecture physique de chaque élément du système (c'est le où).Le pourquoi n'est pas défini dans UML.En UML 2.5, les diagrammes sont représentés sous deux types de vue : d'un point de vue statique ou structurelle du domaine avec les diagramme de structure (Structure Diagrams).D'un point de vue dynamique avec les diagrammes de comportement (Behavior Diagrams) et les diagrammes d’interactions (Interaction Diagrams).Les diagrammes sont dépendants hiérarchiquement et se complètent, de façon à permettre la modélisation d'un projet tout au long de son cycle de vie. Il en existe quatorze depuis UML 2.3. Diagrammes de structure ou diagrammes statiques Les diagrammes de structure (structure diagrams) ou diagrammes statiques (static diagrams) rassemblent :Diagramme de classes (class diagram) : représentation des classes intervenant dans le système.Diagramme d'objets (object diagram) : représentation des instances de classes (objets) utilisées dans le système.Diagramme de composants (component diagram) : représentation des composants du système d'un point de vue physique, tels qu'ils sont mis en œuvre (fichiers, bibliothèques, bases de données…)Diagramme de déploiement (deployment diagram) : représentation des éléments matériels (ordinateurs, périphériques, réseaux, systèmes de stockage…) et la manière dont les composants du système sont répartis sur ces éléments matériels et interagissent entre eux.Diagramme des paquets (package diagram) : représentation des dépendances entre les paquets (un paquet étant un conteneur logique permettant de regrouper et d'organiser les éléments dans le modèle UML), c'est-à-dire entre les ensembles de définitions.Diagramme de structure composite (composite structure diagram) : représentation sous forme de boîte blanche des relations entre composants d'une classe (depuis UML 2.x).Diagramme de profils (profile diagram) : spécialisation et personnalisation pour un domaine particulier d'un meta-modèle de référence d'UML (depuis UML 2.2). Diagrammes de comportement Les diagrammes de comportement (behavior diagrams) rassemblent :Diagramme des cas d'utilisation (use-case diagram) : représentation des possibilités d'interaction entre le système et les acteurs (intervenants extérieurs au système), c'est-à-dire de toutes les fonctionnalités que doit fournir le système.Diagramme états-transitions (state machine diagram) : représentation sous forme de machine à états finis du comportement du système ou de ses composants.Diagramme d'activité (activity diagram) : représentation sous forme de flux ou d'enchaînement d'activités du comportement du système ou de ses composants. Diagrammes d'interaction ou diagrammes dynamiques Les diagrammes d'interaction (interaction diagrams) ou diagrammes dynamiques (dynamic diagrams) rassemblent :Diagramme de séquence (sequence diagram) : représentation de façon séquentielle du déroulement des traitements et des interactions entre les éléments du système et/ou de ses acteurs.Diagramme de communication (communication diagram) : représentation de façon simplifiée d'un diagramme de séquence se concentrant sur les échanges de messages entre les objets (depuis UML 2.x).Diagramme global d'interaction (interaction overview diagram) : représentation des enchaînements possibles entre les scénarios préalablement identifiés sous forme de diagrammes de séquences (variante du diagramme d'activité) (depuis UML 2.x).Diagramme de temps (timing diagram) : représentation des variations d'une donnée au cours du temps (depuis UML 2.3).Un stéréotype est une marque de généralisation notée par des guillemets, montrant que l'objet est une variété d'un modèle.Un classeur est une annotation qui permet de regrouper des unités ayant le même comportement ou structure. Un classeur se représente par un rectangle conteneur, en traits pleins.Un paquet regroupe des diagrammes ou des unités.Chaque classe ou objet se définit précisément avec le signe « :: ». Ainsi l'identification d'une classe X en dehors de son paquet ou de son classeur sera définie par « Paquet A::Classeur B::Classe X ». Modèles d'éléments de type commun Symbolique des modèles d'éléments :                  Fourche (fork).             État initial (initial state).État final (final state).Interface (interface).O?--- sens du flux de l'interface.O)----- est un raccourci pour la superposition de ---?O et O?---. Modèles d'éléments de type relation Généralisation (generalisation).Association (association).    Réalisation.Utilisation. Autres modèles d'éléments Les stéréotypes peuvent dépendre du langage utilisé.Les archétypes.Les profils.UML n'est pas une norme en droit mais un simple standard « industriel » (ou norme de fait), parce que promu par l'OMG (novembre 1997) au même titre que CORBA et en raison de son succès. Depuis juillet 2005, la première version 2.x de UML est validée par l'OMG.Par ailleurs, depuis 2003, l'OMG a mis en place un programme de certification à la pratique et la connaissance d'UML OCUP qui recouvre trois niveaux de maîtrise.S'il existe de nombreux logiciels de modélisation UML, aucun ne respecte entièrement chacune des versions de UML, particulièrement UML 2, et beaucoup introduisent des notations non conformes. En revanche, de nombreux logiciels comportent des modules de génération de code, particulièrement à partir du diagramme de classes, qui est celui qui se prête le mieux à une telle automatisation.Grady Booch, James Rumbaugh, Ivar Jacobson, Le guide de l'utilisateur UML, 2000 (ISBN 2-212-09103-6)Laurent Audibert, UML 2, De l'apprentissage à la pratique (cours et exercices), Ellipses, 2009 (ISBN 978-2729852696)Franck Barbier, UML 2 et MDE, Ingénierie des modèles avec études de cas, 2009 (ISBN 978-2-10-049526-9)Craig Larman, UML 2 et les design patterns, Analyse et conception orientées objet et développement itératif (3e édition), Pearson Education, 2005  (ISBN 2-7440-7090-4)Martin Fowler et al., UML 2.0, Initiation aux aspects essentiels de la notation, 2004 (ISBN 2-7440-1713-2)Pascal Roques, UML 2, Modéliser une application Web, Eyrolles, 2007 (ISBN 2-212-12136-9)Pascal Roques, UML 2 par la pratique, Études de cas et exercices corrigés, Eyrolles, 2006 (ISBN 2-212-12014-1)Jim Conallen, Concevoir des applications web avec UML, Eyrolles, 2000, 288 p. (ISBN 978-2-212-09172-4)Unified ProcessIngénierie dirigée par les modèlesModel Driven ArchitectureATLAS Transformation LanguageObject Constraint LanguageTransformation de modèlesModeling and Analysis of Real Time and Embedded systems(en) UML.org(en) Dernière version de la spécification UML(en) OMG (Object Management Group)(en) Profil UML standardisé par l'ITU-T basé sur le Specification and Description Language Portail de l’informatique   Portail de la programmation informatique"
