médecine;La thyroïdite de Hashimoto ou thyroïdite chronique lymphocytaire est une thyroïdite chronique auto-immune particulièrement fréquente caractérisée notamment par la présence d'anticorps anti-thyroperoxydase et par une infiltration lymphoïde de la glande thyroïde. Généralement évoqué à l'examen clinique devant un goitre et une hypothyroïdie, le diagnostic de la maladie nécessite la réalisation d'examens complémentaires biologiques et morphologiques. Le traitement de la maladie fait généralement appel à une hormonothérapie substitutive.C’est en examinant des pièces de thyroïdectomie obtenues chez quatre femmes d’âge moyen dans un contexte de goitre compressif que le médecin japonais Hakaru Hashimoto (1881?1934) découvre la maladie en 1912. Il publie sa découverte avec l’article K?j?sen rinpa-setsu shush?-teki henka ni kansuru kenky? h?koku ou Zur Kenntnis der lymphomatösen Veränderung der Schilddrüse (Struma lymphomatosa) dans « Archiv für klinische Chirurgie », y décrivant alors l’infiltration lymphocytaire de la glande thyroïde.En 1957, la thyroïdite de Hashimoto devient la première maladie auto-immune spécifique d’organe à être reconnue. Initialement sous-diagnostiquée et considérée comme une maladie rare,, la thyroïdite de Hashimoto est aujourd'hui reconnue comme une des pathologies thyroïdiennes les plus fréquentes.Les estimations de l’incidence et de la prévalence de la thyroïdite de Hashimoto sont variables. Une incidence de 1/1000 a été proposée ainsi qu’une prévalence de 8/1000. La maladie est ainsi la première cause d’hypothyroïdie dans les pays où les apports en iode sont satisfaisants,.Il existe une nette prédominance féminine de la maladie de Hashimoto avec un rapport estimé entre 8 et 20 femmes pour 1 homme. Cette thyroïdite survient généralement aux alentours de l’âge de 40 ans mais peut se voir à tout âge y compris en population pédiatrique. Elle serait également plus fréquente chez les populations caucasiennes et asiatiques.Une histoire familiale de maladies de la thyroïde est fréquente (20 % des cas), en faveur d'une prédisposition génétique. Celle-ci est le plus souvent associée à des haplotypes particuliers tels que HLA-DR3, HLA-DR4 et HLA-DR5. Par ailleurs, la thyroïdite de Hashimoto pourrait être liée, avec un niveau de risque plus faible, avec le polymorphisme de gènes impliqués dans la réponse immunitaire comme le gène CTLA-4 (Cytotoxic T-lymphocyte Associated-4) (en) qui entraîne une diminution du fonctionnement des produits du gène et une régulation négative de l'activité des lymphocytes T,. Ce mécanisme est également retrouvé dans le diabète de type 1.La carence iodée serait un facteur de protection contre le risque de thyroïdite de Hashimoto, et une correction excessive en iode ,, une carence en sélénium, des maladies infectieuses et quelques médicaments ont été impliqués comme facteurs de risque chez les personnes avec un risque génétique déjà prédéterminé.L'incidence est augmentée chez les patients avec des anomalies chromosomiques, comme dans le syndrome de Turner,, le syndrome de Down (ou trisomie 21) et le syndrome de Klinefelter.Des recherches récentes suggèrent un potentiel rôle du virus HHV-6 (possiblement variant A) dans le développement ou la stimulation de la thyroïdite de Hashimoto.Le rôle du tabac est discuté. Des études indiquent un risque plus élevé chez les fumeurs, d'autres tendent à montrer un effet protecteur du tabac avec une réduction des taux sériques en auto-anticorps ainsi qu'une évolution moins fréquente vers l'hypothyroïdie. Le mécanisme physiopathologique de cet effet protecteur n'est cependant pas compris.La thyroïdite de Hashimoto est associée à la survenue d'autres maladies auto-immunes : diabète de type 1,, maladie cœliaque,, vitiligo, maladie de Biermer, insuffisance surrénale (notamment dans le cadre d'un syndrome de Schmidt) et polyarthrite rhumatoïde.La maladie de Basedow peut être associée à la thyroïdite de Hashimoto et il existe des formes de passage entre ces deux maladies. On parle ainsi de « Hashitoxicose », entité décrite pour la première fois en 1971.Sur le plan physiopathologique, les anticorps dirigés contre la thyroperoxydase et/ou la thyroglobuline causent une destruction progressive des follicules thyroïdiens de la glande thyroïde.Macroscopiquement, le goitre est symétrique, non adhérent aux éléments péri-thyroïdiens et présente une surface capsulaire discrètement bosselée.En microscopie les lésions consistent en une association de fibrose interstitielle, d'infiltration lymphoïde et de destruction épithéliale,. Le degré de fibrose est très variable. L'infiltration lymphoïde présente une organisation en follicules avec des lymphocytes B au centre et des lymphocytes T dans le cortex. Les cellules épithéliales thyroïdiennes sont également modifiées, apparaissant élargies et acidophiles (cellules de Hürthle).Ces signes sont liés à la présence du goître induit par la thyroïdite. Celui-ci est diffus et sa surface est le plus souvent régulière. Sa consistance est très particulière : ferme, « suiffée » ou « caoutchoutée ». Parfois ces changements peuvent ne pas être palpables. Le goitre peut éventuellement être responsable de signes compressifs (dysphonie, dysphagie et dyspnée). La palpation cervicale ne retrouve généralement pas d'adénomégalie. Il n'est pas mis en évidence non plus de douleur ou de signes inflammatoires locaux.Ces derniers sont liés à la dysthyroïdie avec au premier plan l'hypothyroïdie. L'hyperthyroïdie peut également être présente, en particulier au début de l'évolution de la maladie. On soulignera que les signes de dysthyroïdie peuvent être absents initialement, la fonction thyroïdienne n'étant pas nécessairement perturbée. En revanche plus la maladie évolue et plus l'hypothyroïdie devient fréquente.Parmi les signes cliniques induits par l'hypothyroïdie on retrouve notamment : constipation, bradycardie, myxœdème, anémie, règles irrégulières, asthénie, troubles de la concentration et de la mémoire, dépression, peau sèche et épaissie, perte de cheveux.La positivité à un taux élevé des anticorps anti-TPO, retrouvée dans 95 % des cas, est le meilleur signe biologique pour diagnostiquer la thyroïdite de Hashimoto. Elle survient préférentiellement chez des sujets HLA B8-DR3 (en). Le titre en anticorps est de plus associé au degré d'infiltration lymphoïde de la glande. On notera que la positivité des anticorps anti-TPO est par ailleurs très rare chez les sujets sains.En cas de négativité des anticorps anti-TPO, on peut retrouver une augmentation des anticorps anti-thyroglobuline.Il n'y a pas nécessairement, au début, de trouble de la fonction hormonale, mais la maladie évoluera toujours vers une hypothyroïdie avec des taux de T4 anormalement bas et secondairement des taux de TSH élevés.Enfin, la thyroïdite de Hashimoto n'est pas associée à la présence de marqueurs sériques de l'inflammation.L'échographie de la thyroïde montre un goitre hypoéchogène,. Le parenchyme thyroïdien devient plus hétérogène au cours de l'évolution. On peut notamment mettre en évidence des pseudo-nodules et des nodules de régénérations hyperéchogènes (white knight). Des ganglions récurrentiels peuvent être visualisés. La vascularisation est hétérogène en Doppler couleur. L'étude en Doppler pulsé retrouve une élévation des vitesses systoliques, toutefois moindre que dans la maladie de Basedow.La scintigraphie est inutile dans ce contexte d'hypothyroïdie. Lorsqu'elle est réalisée elle montre des résultats très variables ne contribuant donc pas au diagnostic.La thyroïdite de Hashimoto peut être associée à toute autre maladie auto-immune : collagénose, insuffisance surrénalienne, à un cancer de la thyroïde ou entraîner des complications cardio-vasculaires. Elle peut aussi entraîner des symptômes laissant penser à tort à un virage maniaque (manie) caractéristique d'un trouble bipolaire.La thyroïdite de Hashimoto peut se compliquer d'une encéphalopathie (encéphalopathie de Hashimoto). Cette entité a été décrite pour la première fois en 1966 et seuls une centaine de cas ont été rapportés dans la littérature depuis.Le lymphome thyroïdien complique moins de 1 % des thyroïdites auto-immunes,. Toutefois celui-ci doit être évoqué devant toute augmentation de volume du goitre ou en cas de survenue d'adénopathie.Le traitement chirurgical n'a aujourd'hui que rarement sa place dans la thyroïdite de Hashimoto. On le réservera essentiellement aux goitres compressifs.La prise en charge est avant tout médicale, consistant en une hormonothérapie thyroïdienne substitutive. Les hormones thyroïdiennes permettraient une diminution du volume du goitre tout en corrigeant l'hypothyroïdie latente ou évidente.Endocrionologie, diabète et maladies métaboliques. Collège des enseignants d'endocrinologie, diabète et maladies métaboliques p361InfoThyroAFMT : site officiel de l'Association française des malades de la thyroïde Portail de la médecine
médecine;"La glande thyroïde ou thyroïde est une glande endocrine régulant, chez les vertébrés, de nombreux systèmes hormonaux par la sécrétion de triiodothyronine (T3), de thyroxine (T4) et de calcitonine. Dans l'espèce humaine, elle est située à la face antérieure du cou, superficiellement.Ses déformations (on parle de goitre quand le volume de la thyroïde est augmenté) sont visibles sous la peau. Elle peut être le siège de diverses affections : hyperthyroïdie, hypothyroïdie, tumeur maligne ou tumeur bénigne. On peut l'étudier grâce à l'échographie et à la scintigraphie.La thyroïde, moulée sur l'axe trachéo-laryngé, est de consistance ferme, de couleur rosée, et pèse de 25 à 30 grammes généralement mais en cas de goitre sa masse peut augmenter jusqu'à 100-150 grammes. Elle est entourée d'une capsule avasculaire (ou gaine viscérale péri-thyroïdienne) qui lui est propre et qui est différente de la loge thyroïdienne.La thyroïde se compose de deux lobes droit et gauche situés verticalement de part et d'autre du larynx. Une partie intermédiaire horizontale, l'isthme thyroïdien, forme un pont entre les deux lobes. Généralement, la glande thyroïde répond aux 2e et 3e anneaux trachéaux ; mais elle peut avoir une position haute : 1er et 2e anneaux trachéaux, ou une position basse : 3e et 4e anneaux trachéaux. Les deux lobes ont un sommet supérieur, ainsi qu'une grande base inférieure. On leur décrit trois faces : médiale, postérieure et antéro-latérale. Sa hauteur est d'environ 6 cm pour une longueur de 6  à   8 cm. On trouve souvent entre les deux lobes, donc au niveau de l'isthme, le lobe pyramidal de Lalouette, souvent déporté vers la gauche : c'est un reliquat du canal thyréoglosse.Il existe des variations morphologiques, s'expliquant par l'embryologie : en effet les deux lobes sont parfois éloignés l'un de l'autre sans qu'il n'y ait d'isthme, ou au contraire peuvent être soudés donnant une thyroïde en forme de V. Provenant d'un bourgeon de cellules endodermiques naissant près de la racine de la langue, différentes positions de la glande thyroïde peuvent cependant survenir durant l'ontogenèse : une mauvaise migration de cette ébauche conduit alors à la détection de cette glande (fonctionnelle ou non fonctionnelle) dans la région linguale, cervicale, voire endo-thoracique.La thyroïde présente les rapports anatomiques suivants :ventralement : muscles cervicaux superficielslatéralement : nerfs récurrents et axes vasculaires jugulo-carotidiensdorsalement : larynx au pôle supérieur et trachée cervicale au pôle inférieurLes quatre parathyroïdes ont des positions variables, mais se situent généralement aux quatre pôles thyroïdiens.La thyroïde est un organe richement vascularisé. En effet on retrouve :Deux artères principales :artère thyroïdienne supérieure, première branche de l'artère carotide externe ; elle se divise en 3 branches (latérale, médiale et postérieure) une fois la glande atteinte.artère thyroïdienne inférieure, naissant du tronc thyro-cervical, branche collatérale de l'artère subclavière. Se divise également en trois branches (mêmes situations) dans la thyroïde.Dans de très rares cas il est possible qu'une 3e artère vienne vasculariser la thyroïde dans sa portion basse appelée artère de Neubauer qui est une branche de la crosse de l'aorte.Les deux artères principales de la thyroïde sont anastomosées ; l'ATS droite avec l'ATS gauche et l'ATI droite, et l'ATI droite avec l'ATS droite et l'ATI gauche.Il existe néanmoins d'autres artères, moins volumineuses, inconstantes, naissant directement de l'arc aortique. Par exemple l'artère thyroïdea ima vascularisant la partie isthmique. Celle-ci est présente chez environ 5 à 10 pour cent des sujets et peut provoquer une hémorragie en cas de trachéotomie.Trois veines principales :veine thyroïdienne supérieure, résultant de la confluence de trois veines dans la glande, et formant avec les veines linguale et faciale le tronc thyro-lingo-facial qui se jette dans la veine jugulaire interne.veine thyroïdienne moyenne, réunion de plusieurs branches pas très volumineuses se jetant dans la veine jugulaire interne.veine thyroïdienne inférieure, formée par la confluence de trois veines dans la glande et se jetant dans le tronc veineux brachio-céphalique.De même que pour les artères, certaines veines accessoires vascularisant préférentiellement l'isthme vont rejoindre les troncs veineux brachio-céphaliques droit et gauche.Chez nos mammifères domestiques, on parle de 2 glandes thyroïdes (droite et gauche), car contrairement à l'homme, les 2 lobes thyroïdiens ne sont pas réunis par un isthme.  Chez les autres vertébrés, la glande thyroïde est diffuse, formée de groupes dispersés de follicules, et situés latéralement à des distances variables de l'œsophage.Chez les agnathes et la plupart des téléostéens (poissons), les groupes de follicules se distribuent sur toute la partie ventrale de la tête.Malgré cette diversité morphologique, la structure histologique folliculaire de la thyroïde est hautement conservée chez tous les vertébrés, ce qui témoigne d'un processus original et commun de production hormonale.L'unité morpho-fonctionnelle de la glande thyroïde est le follicule thyroïdien (ou vésicule thyroïdienne), composé d'un épithélium unistratifié de cellules folliculaires (les thyréocytes), produisant les hormones thyroïdiennes, disposées autour d'une lumière centrale contenant la colloïde : la colloïde est principalement constituée du précurseur des hormones thyroïdiennes, la thyroglobuline. Le follicule thyroïdien est un véritable piège à iode (ion iodure), élément rare à la surface de la terre, et indispensable au fonctionnement de l'organisme; l'iode sera ainsi capté et stocké dans la colloïde : la biosynthèse des hormones thyroïdiennes pourra alors  se dérouler, l'iode venant se coupler à la thyroglobuline ; la thyroglobuline iodée est ensuite réintégrée dans le follicule thyroïdien, et sécrétée dans le courant sanguin.Le follicule thyroïdien, en dehors d'une majorité de cellules folliculaires, contient 1 à 2 % de cellules dites parafolliculaires (ou cellules C, ou cellules claires), produisant la calcitonine : elles n'ont cependant jamais de contact avec la colloïde.On trouve aussi des amas de cellules (ilots de Woffler), cellules jointives pouvant se transformer en vésicule thyroïdien.La thyroïde est issue de trois ébauches :deux ébauches latérales issues du 4e sillon branchial interne et qui forme une partie des lobes latérauxet une ébauche centrale issue de l'évagination du pharynx buccal constituant ainsi le tractus thyréoglosse qui forme l'isthme ainsi  que la majeure partie des lobes latéraux. Rappelons que le tractus thyréoglosse est l'axe de migration de la thyroïde chez l'embryon.La thyroïde sécrète :la T3 ou triiodothyronine en très faible quantité ;la T4 ou thyroxine ;la calcitonine intervenant dans le métabolisme du calcium.La production de ces hormones est régie par la thyréostimuline (TSH, « thyroid-stimulating hormone »), produite par l'hypophyse et nécessite un apport en iode. La plus grande production de la T3 est obtenue par la conversion de la T4 au niveau du foie, pour la plus grosse quantité et les intestins pour le reste. La thyroïde ne produit, elle, de la T3 directement que pour à peine 10 à 20 %.De par sa position superficielle, la thyroïde est explorée en premier lieu par une échographie cervicale, qui recherchera des nodules ou un goitre. L'image ultrasonore permet d'évaluer le volume de la thyroïde ; à l'échelle du diagnostic individuel ou de population (suivi épidémiologique).La tomodensitométrie avec injection de produit de contraste iodé est peu utilisée, généralement dans le cadre du bilan pré-opératoire des goitres volumineux, notamment des goitres plongeants.La scintigraphie thyroïdienne à l'iode 123 est un examen fonctionnel. L'injection d'un traceur d'iode radioactif mettra en évidence des zones du parenchyme plus ou moins actives, et permettra la distinction entre un nodule hypersécrétant et un nodule « froid ».Un bilan thyroïdien standard comporte le dosage de la TSH et de la T3 ou de la T4. La calcitonine n'est pas systématiquement dosée.Les dysthyroïdies peuvent avoir des origines génétiques, être liées à des carences nutritionnelles en iode, mais aussi être induites par des toxiques (plomb, ou iode radioactif par exemple - on parle alors de « thyrotoxicoses »). Hypothyroïdie Situation d'imprégnation insuffisante de l'organisme en hormones thyroïdiennes, le plus souvent à cause d'un mauvais fonctionnement de la glande thyroïde.Les symptômes de l'hypothyroïdie découlent d'un ralentissement métabolique général : fatigue, difficultés de concentration, troubles de la mémoire, frilosité, myxœdème, prise de poids malgré un appétit stable voire diminué, diminution de la pilosité avec perte de cheveux ou cheveux devenant cassants, éclaircissement des sourcils, sécheresse ou épaississement cutané, pâleur, crampes musculaires, fourmillement ou engourdissement des extrémités, inappétence, tendance à la dépression, insomnies, tendance à la constipation.L'examen clinique recherche une augmentation de la taille de la thyroïde qui peut être importante (goitre), un ralentissement de la fréquence cardiaque, la bradycardie, et parfois, de la tachycardie ou des symptômes ressemblant à ceux de l'hyperthyroïdie.Le traitement est une substitution journalière à vie en hormones thyroïdiennes, par voie orale. Hyperthyroïdie Symptomatologie due à un excès de production d'hormones thyroïdiennes :cardio-vasculaire : tachycardie, éréthisme cardio-vasculaire (frémissement du choc de la pointe du cœur) ;digestif : syndrome polyuro-polydipsique (boit et urine en grande quantité), amaigrissement, diarrhée, flatulences ;neuro-psy : tremblement, agitation, trouble de l'humeur (irritabilité allant à la dépression), trouble du sommeil, trouble du comportement alimentaire (mange en quantité excessive, perte de poids) ;généraux : hypersudation (mains souvent moites, transpiration), hyperthermie, thermophobie (température élevée et n'apprécie pas les températures élevées) ;musculaire et articulaire : douleur et fatigue musculaire, ostéoporose, augmentations des glandes lactogènes.Le tabac multiplie par dix le risque de survenance de la maladie de Basedow, la forme la plus fréquente de l'hyperthyroïdie, et augmente les risques de complications. Thyroïdites La thyroïdite est une inflammation de la glande thyroïde.Il existe plusieurs types de thyroïdites: Maladie d'Hashimoto La cause la plus fréquente d’hypothyroïdie.Elle consiste en une destruction de la glande thyroïde causée par des taux d’anticorps antithyroïdiens anormalement élevés dans le sang et des globules blancs. La thyroïde ne sécrète alors plus suffisamment d’hormones thyroïdiennes.Cette maladie nécessite donc très souvent un supplément hormonal.Pour confirmer le diagnostic, il faut réaliser une prise de sang qui dosera les hormones thyroïdiennes et la capacité du corps à les gérer (T4, T3, TSH), ainsi que les auto anticorps thyroïdiens (AC anti TPO, et AC anti thyroglobuline).  Thyroïdite du post-partum La thyroïdite du post-partum peut survenir dans l'année qui suit un accouchement. Dans ce cas, la glande a tendance à récupérer, et le traitement de remplacement des hormones thyroïdiennes n’a besoin d’être administré que durant quelques semaines. Une évolution vers une hypothyroïdie permanente est possible. Thyroïdite silencieuse Elle porte ce nom, car elle n’entraîne aucun signe ni symptôme d’inflammation de la thyroïde. De prime abord, le patient présente une hyperthyroïdie pouvant donner lieu aux mêmes symptômes que ceux de la maladie de Basedow-Graves, qui laisse place à une phase d’hypothyroïdie aboutissant à une guérison complète. La présence d’anticorps antithyroïdiens comparables à ceux décelés au cours de la maladie de Hashimoto est un facteur de risque de persistance de l’hypothyroïdie. Thyroïdite subaiguë dite de Quervain Il s’agit d’une forme passagère de thyroïdite provoquant une hypothyroïdie. La thyroïdite subaiguë serait causée par une infection virale, car la majorité des patients atteints ont présenté une infection de la gorge dans les semaines précédant son apparition. Cette affection se manifeste sous forme d’épidémies de faible envergure et est généralement associée à des infections virales connues. Dépression et thyroïde L'hypothyroïdie peut parfois être confondue avec un état de dépression et l'hyperthyroïdie pour un état d'excitation. Le diagnostic thyroïdien permettra d'éliminer ces faux diagnostics.Cependant des travaux récents ont montré qu'une hypothyroïdie traitée uniquement avec de la thyroxine peut davantage encore se rapprocher d'un état dépressif. Plutôt que de prescrire un antidépresseur, le médecin peut parfois proposer une simple substitution d'une partie de la dose de thyroxine (T4) par de la tri-iodo-thyronine (T3) ou de flavinine (substitut générique du B52).Un goitre est une thyroïde globalement augmentée de volume. Il est dit toxique lorsqu'il sécrète des hormones thyroïdiennes de façon excessive, entraînant une hyperthyroïdie. Les goitres sont rarement homogènes et le plus souvent multinodulaires ; chaque lobe thyroïdien présente un nombre important de nodules bénins de volume variable.Les goitres peuvent être en situation cervicale normale ; ils sont dits « plongeants » lorsque le pôle inférieur d'au moins un lobe pénètre dans le médiastin à travers l'orifice supérieur du thorax. Plongeants ou non, les goitres peuvent être (rarement) compressifs, lorsque le volume trop important de la thyroïde comprime les organes de voisinage, principalement la trachée, mais aussi l'œsophage et parfois étirant les nerfs récurrents, entraînant alors une paralysie de la corde vocale homolatérale.Les tumeurs de la thyroïde se présentent sous la forme d'un nodule thyroïdien, qui peut être bénigne (adénome de la thyroïde) ou maligne (carcinome de la thyroïde). Tumeurs bénignes Adénome vésiculaire de la thyroïdeAdénome oncocytaire de la thyroïdeLipoadénome de la thyroïdeAdénome à cellules claires de la thyroïdeAdénome vésiculaire à cellules en bague à chaton de la thyroïdeAdénome vésiculaire mucosécrétant de la thyroïdeAdénome vésiculaire à noyaux bizarres de la thyroïdeAdénome vésiculaire atypique de la thyroïde Tumeurs malignes Il existe deux types histologiques principaux de cancers thyroïdiens :carcinomes différenciés folliculairescarcinomes différenciés papillaires (le plus fréquent, 80 % des cas)La thyroïde est généralement abordée par une cervicotomie médiane, qui peut être élargie latéralement en cervicotomie en U s'il existe une nécessité de curage ganglionnaire cervical. En cas de volumineux goitre plongeant, un refend cutané en Y en regard de l'extrémité crâniale du sternum sera souvent pratiqué. Au maximum, une simple manubriotomie (on parle alors de cervicomanubriotomie) ou une sternotomie médiane pourra être pratiquée.livret d'information ""Cancer de la thyroïde"" ; Institut Gustave Roussy.ARC, Monographie Volume 79 (2001) Some Thyrotropic AgentsFini J.B et Demeneix B (2019) Les perturbateurs thyroïdiens et leurs conséquences sur le développement cérébral. Biologie Aujourd’hui, 213(1-2), 17-26.Remaud S et Demeneix B (2019) Les hormones thyroïdiennes régulent le destin des cellules souches neurales. Biologie Aujourd’hui, 213(1-2), 7-16 (résumé). Portail de l’anatomie   Portail de la physiologie"
médecine;"L'appareil cardiovasculaire, appareil circulatoire ou système sanguin, est un système circulatoire en circuit fermé qui assure le transport du sang du cœur vers les extrémités et les divers organes et, en retour, de ceux-ci vers le cœur. Il est constitué du cœur et des vaisseaux sanguins qui forment le système vasculaire, les vaisseaux lymphatiques qui composent le système lymphatique lui étant parfois associé.La circulation du sang permet le transport et l'échange interne d'une grande variété de substances biochimiques. Elle permet d'acheminer des nutriments, du dioxygène et des hormones aux cellules de l'organisme. Ces éléments proviennent du tube digestif, des poumons et des glandes endocrines. Le système cardiovasculaire assure également la collecte des déchets métaboliques des cellules, comme le dioxyde de carbone ou l'urée, acheminés vers les poumons, le foie et les reins. Enfin, il participe à la régulation de nombreux facteurs, tels que le taux de sucre.La relation entre le saignement et la mort a sans doute été mise en évidence très tôt dans l'histoire de l'humanité.Les Égyptiens avaient identifié le sang comme source de vie et siège de l'âme.Les dissections pratiquées par les médecins grecs de Cos au Ve siècle av. J.-C., dans la lignée d'Hippocrate, sur des animaux égorgés induisent des erreurs de représentation : les artères sont retrouvées vides, on pense donc qu'elles transportent de l'air, tandis que le foie et la rate sont gorgés de sang, ces deux organes sont donc considérés comme des éléments importants du transport du sang. Hérophile, médecin d'Alexandrie du IVe siècle av. J.-C., décrivit le premier la palpation du pouls. C'est à Erasistrate de Keos (320–250 av. J.-C.) que l'on doit la première description des valves veineuses.Galien (131-201) fait une description précise du réseau de veines et d'artères à partir de dissection de porcs, mais interprète faussement le rôle des organes. Selon lui, le sang est créé dans le foie à partir des aliments, il circule par les veines et va d'une part vers les poumons pour se mélanger à de l'air, d'autre part passe du ventricule droit au ventricule gauche par la paroi poreuse où il prélève la chaleur qu'il redistribue dans le corps ; arrivé aux extrémités du corps, le sang est consommé et ressort sous forme de transpiration.Les médecins musulmans traduisent les traités de médecine égyptiens découverts lors de l'invasion de l'Égypte au VIIe siècle, dont le traité de Galien sur la circulation (traduit par Averroès). À partir du Xe siècle, ils décrivent de nombreuses maladies cardiovasculaires (thrombose et collapsus pour Avicenne, péricardite pour Avenzoar). Ibn Al-Nafis, le père de la physiologie fait partie des autres précurseurs de la dissection humaine. En 1242 il a été le premier à décrire la circulation pulmonaire, les artères coronaires et la circulation capillaire qui forment la base du système circulatoire. L'œuvre d'Ibn Al-Nafis restera ignorée jusqu'en 1924 lorsque le Dr Al-Tatawi, médecin égyptien résidant en Allemagne, retrouve la traduction d'Andrea Alpago dans la Bibliothèque nationale de Berlin.En 1543, Andreas Vesalius publie ses travaux De humani corporis fabrica, dans lesquels la théorie physiologique de Galien fut adaptée à ses nouvelles observations.Cette théorie de Galien sur la Physiologie du Système circulatoire est bouleversée en 1551 quand Amato Lusitano (João Rodrigues de Castelo Branco, 1511-1568), médecin marrane portugais fuyant l'Inquisition en Italie, décrit la circulation du sang dans son ouvrage de sept volumes Curationum Medicinalium Centuriæ Septem en 1551 (la 1re édition) et pour la première fois, constate que les veines ont des valvules qui obligent le sang à retourner vers le cœur,. Cette découverte renverse ce qui était admis depuis Galien qui disait que le sang sortait du cœur par les artères et les veines et n'y retournait pas.Au XVIe siècle, Michel Servet (espagnol) décrit la circulation pulmonaire, d'abord en 1546,, puis publié en 1553. Ce fait est resté ignoré, car il a été publié dans un traité de théologie considéré comme hérétique, et dont l'édition a été détruite. On ne connait que trois exemplaires, conservés respectivement à Édimbourg, Paris et Vienne.L'italien Realdo Colombo est un des premiers à décrire parfaitement la circulation pulmonaire.C'est Andrea Cesalpino (1519-1603) qui utilise le premier le terme de « circulation » et qui en attribue le rôle au cœur, alors que l'on pensait jusqu'ici que le mouvement du sang dépendait de la pulsation des artères. Finalement William Harvey (1578-1657), élève de Fabrice d'Acquapendente (1537-1619), fait la première description complète du système circulatoire, dans son ouvrage Exercitatio Anatomica de Motu Cordis et Sanguinis in Animalibus de 1628. Il décrit notamment le sens de circulation et le rôle exact des valvules veineuses, et établit que la circulation est importante (plusieurs litres par minute) alors qu'on la croyait au goutte-à-goutte. L'idée première en reviendrait à Walter Warner. Marcello Malpighi identifie pour la première fois les capillaires au microscope en 1661.L'idée du cœur comme pompe et moteur de la circulation sanguine a encore été mise en doute à l'époque contemporaine, notamment par le cardiologue Leon Manteuffel-Szoege.L'appareil cardiovasculaire a pour fonction d'apporter aux différentes cellules les nutriments et le dioxygène dont elles ont besoin et d'éliminer leurs déchets comme le dioxyde de carbone. Cette circulation continue fonctionne grâce à des vaisseaux sanguins et une pompe : le cœur. On appelle artères les vaisseaux apportant les du cœur vers les tissus tandis que les veines apportent le sang des tissus vers le cœur.On distingue la circulation systémique (grande circulation), dont le rôle est de recharger les muscles et organes en dioxygène et en nutriments et la circulation pulmonaire (petite circulation) dont le rôle est d'assurer la ré-oxygénation du sang par les poumons et l'élimination par ceux-ci du dioxyde de carbone (hématose).Dans la grande circulation, le ventricule gauche du cœur expulse le sang via l'artère aorte vers les capillaires des différents organes où s'effectuent divers échanges. L'aorte est une artère élastique et épaisse capable de résister aux hautes pressions lors de la contraction cardiaque. Son élasticité contribue à la restitution d'un débit continu alors que les contractions cardiaques sont discontinues. Le sang est ensuite ramené au cœur droit via les veines caves supérieure et inférieure.Dans la petite circulation, le ventricule droit du cœur propulse le sang via l'artère pulmonaire vers les poumons. Le ventricule droit est moins épais que le gauche car il doit seulement assurer la vascularisation d'une partie restreinte du corps.Ainsi, dans la circulation systémique, les artères apportent du sang oxygéné aux tissus et les veines ramènent du sang appauvri en oxygène vers le cœur ; dans la circulation pulmonaire, les artères pulmonaires transportent du sang pauvre en dioxygène et les veines pulmonaires du sang riche en dioxygène.Les veines profondes et superficielles sont équipées de valvules. Ces « clapets », disposés tous les quatre à cinq centimètres, imposent un sens unique de circulation du sang, et empêchent le reflux.L'aspiration du sang des pieds vers le cœur est le résultat de plusieurs mécanismes. Ainsi, la compression de la voûte plantaire, la contraction des muscles des mollets et des cuisses chassent le sang vers le haut. Les mouvements respiratoires facilitent également le travail en diminuant la pression au sein du thorax lors de chaque inspiration. C'est pourquoi la marche et l'exercice physique permettent de limiter les risques d'insuffisance veineuse.Les éponges, qui appartiennent à l'embranchement des spongiaires ou porifères, n'ont pas de véritables tissus ni d'organes, elles sont également dépourvues de système vasculaire. Le corps d'une éponge ressemble à un sac percé de pores, d'où le nom Porifera (« qui portent des pores », du latin pori, « pores », et -fera, « qui portent »). Grâce à ces pores inhalants, l'eau peut pénétrer à l'intérieur d'une cavité centrale nommé spongocèle. L'eau ressort ensuite par une ouverture plus grande appelée oscule. Les éponges sont caractérisées par la présence des choanocytes. Le choanocyte est une cellule qui est dotée d'une collerette et d'un flagelle qui assure la circulation de l'eau et l'absorption des particules nutritives dans la cavité centrale des spongiaires.Les cnidaires sont dépourvus de système vasculaire. Ils ont l'aspect d'un sac renfermant un compartiment digestif central appelé la cavité gastrovasculaire (qui a rapport aux vaisseaux et à l'estomac). La cavité gastrovasculaire est une innovation évolutive par rapport aux éponges. Cette cavité a une seule ouverture servant à la fois de bouche et d'anus. En fait les cnidaires dépendent principalement de la diffusion pour obtenir l'oxygène dont ils ont besoin, car leur corps est formé de deux couches de cellules, l'une étant à l'extérieur appelé épiderme et l'autre étant enveloppé sur la cavité gastrovasculaire appelé gastroderme. Le contenu de la cavité gastrovasculaire doit être renouvelé en gaz et en nutriments. Pour ce faire l'organisme va « aspirer » les éléments puis les « recracher » à travers la membrane.Les plathelminthes sont dépourvus de système circulatoire. Contrairement aux animaux radiaires, les vers plats et les autres bilatériens sont triploblastiques. L'un des feuillets embryonnaires, le mésoderme permet la formation d'organes plus complexes, de systèmes d'organes et de vrais muscles. Cependant ils n'ont pas de cavité générale interne, autrement dit de cœlome. Les échanges se font par diffusion. Cette diffusion se fait grâce aux déplacements de l'animal qui agite les fluides interstitiels. Par contre, il existe des cellules spécialisées qui forment l'appareil excréteur.Le corps cylindrique des nématodes n'est pas segmenté. Les vers ronds sont revêtus d'un exosquelette appelé cuticule. Ils possèdent un tube digestif complet mais pas de système cardiovasculaire. Le liquide qui circule dans leur pseudocélome (cavité corporelle recouverte de mésoderme) apporte des nutriments à toutes les cellules du corps. La respiration se fait par diffusion au travers de pores qui percent la cuticule imperméable.Les annélides tels le lombric possèdent un système circulatoire fermé ; la circulation sanguine est assurée par la contractilité de certains vaisseaux et par des « cœurs » comportant des valvules et une seule cavité.Les mollusques ont généralement un appareil circulatoire ouvert avec un cœur constitué de deux oreillettes et d'un ventricule. Le liquide permettant le transport des gaz respiratoires est appelé hémolymphe. Le pigment respiratoire qu'il contient, l'hémocyanine n'est pas contenu dans des cellules sanguines mais circule librement dans le liquide. Il porte deux cations cuivreux Cu+ permettant le transport de l'O2. L'hémocyanine libre est incolore tandis que la forme lié à l'O2 (Cu2+) est bleue.Parmi les mollusques, les céphalopodes ont acquis un système circulatoire clos. Poissons Le cœur des poissons est constitué d'une oreillette et d'un ventricule disposés en série. Le sang traverse 2 lits capillaires, un dans les branchies pour échanger les gaz (capillaires branchiaux) et l'autre dans le restant du corps (capillaires systémiques). Amphibiens La circulation sanguine des amphibiens est close, avec un cœur possédant deux oreillettes et un ventricule. De ce fait, il se produit un mélange entre le sang oxygéné et le sang désoxygéné. Reptiles La circulation sanguine de beaucoup de reptiles est double, composée d'un petit circuit capturant l'oxygène au niveau pulmonaire, puis d'un grand circuit le distribuant à l'ensemble des organes. Elle est incomplète ; le cœur, n'étant pas totalement cloisonné, permet un mélange entre le sang artériel oxygéné et le sang veineux riche en dioxyde de carbone (sauf chez les crocodiles qui possèdent une circulation complète). Les reptiles ont une circulation double, incomplète et fermée. Ils ont aussi deux oreillettes et un ventricule. Mammifères et oiseaux La circulation sanguine chez les oiseaux et les mammifères (dont l'humain) est assurée par un cœur ayant deux oreillettes et deux ventricules, ce qui permet alors une séparation entre le sang oxygéné et le sang désoxygéné. La circulation des mammifères est identique à celle de l'humain.VasculariteInsuffisance veineuseL'accident vasculaire cérébral, plus communément appelé AVC, est une affection due à un problème vasculaire qui est à l'origine d'une mauvaise alimentation en sang du cerveau. Il existe deux types d'AVC :l'AVC ischémique provoqué par un caillot qui obstrue une artère du cerveau stoppant ainsi son approvisionnement sanguin ;l'AVC hémorragique provoqué par une rupture de l’artère due à une hypertension artérielle.François Boustani, La Circulation du sang: entre Orient et Occident : l’histoire d’une découverte, Philippe Rey, 2007.Ressource relative à la santé : (en + es) MedlinePlus Modélisation de la circulation sanguine générale et lymphatique (animation flash).Visualisation de la circulation du sang et de sa teneur en O2 et CO2 (animation flash).« Circulation sanguine : un petit tour, et puis sang va », Eurêka ! , France Culture, 27 juillet 2022. Portail de l’anatomie   Portail de l’hématologie   Portail de la médecine   Portail de la physiologie"
médecine;"Le système digestif, appelé aussi appareil digestif, est l'ensemble des organes qui chez les animaux a pour rôle d'assurer l'ingestion et la digestion des aliments pour en extraire l'énergie et les nutriments nécessaires à la survie de l'organisme qui sont ensuite absorbés par l'organisme. Ce système est essentiel à la vie des animaux et se retrouve nécessairement pour toutes les espèces. Le rôle de ce système biologique est également d'assurer l'excrétion des matières alimentaires qui n'ont pu être absorbées par l'organisme.Le système digestif varie de manière plus ou moins importante d'un animal à un autre. Ainsi il peut être très simple pour les organismes unicellulaires et les éponges où il n'y a pas de système digestif différencié et où la digestion des aliments se fait au sein même de leurs cellules, ou bien être différencié mais sans spécialisation comme chez les cnidaires et les vers plats où la digestion est assurée au sein de la cavité gastrovasculaire et où la bouche et l'anus sont indifférenciés. La spécialisation commence lorsque la bouche et l'anus sont différenciés et qu'ainsi les aliments passent successivement dans une série d'organes plus ou moins nombreux selon les espèces. Les nématodes sont un exemple de système digestif spécialisé très simple ne disposant que d'une bouche, d'un pharynx, d'un intestin et d'un anus. Mais le nombre d'organes et leur complexité peut augmenter selon le régime alimentaire qui nécessite plus ou moins de traitements pour que la digestion soit assurée. C'est ainsi que les ruminants disposent de plusieurs organes supplémentaires et d'un estomac à plusieurs chambres.L'étude du système digestif d'un animal permet de déterminer de nombreuses choses sur son régime et comportement alimentaire. C'est ainsi qu'avec les seules dents, les anthropologues sont capables de déterminer si un hominidé était en majorité herbivore ou omnivore.Les plantes sont capables de produire des composés organiques (elles sont autotrophes), contrairement aux animaux qui en sont incapables et sont ainsi obligés de se nourrir d'êtres vivants (ils sont hétérotrophes). Les composés organiques ingérés servent à deux choses :comme source d'énergie ;comme éléments que l'organisme est incapable de synthétiser, ce sont les nutriments et minéraux essentiels.Les molécules organiques ingérées sont relativement complexes et nécessitent d'être digérées par l'organisme pour être réduites en plus petites molécules. Une fois réduites, elles sont absorbées, tous les autres éléments sont évacués du système digestif.Bien que les éponges soient des animaux, la digestion de leur nourriture est intracellulaire, comme chez la bactérie. Mais des animaux plus complexes, comme l'hydre, ont développé une cavité gastrovasculaire où la nourriture est acheminée pour y être digérée sous l'effet de diverses enzymes.Le système digestif se spécialise lorsqu'il y a une séparation de la bouche (par où pénètre la nourriture) et l'anus (par où sont expulsés les déchets). Dans le cas de l'hydre, la bouche et l'anus sont indifférenciés, ce qui n'est pas le cas de la nématode qui a développé un tractus digestif unidirectionnel où la nourriture passe successivement par la bouche, le pharynx, l'intestin et l'anus.Selon leurs régimes alimentaires, les animaux sont classés en trois groupes :les herbivores, qui se nourrissent exclusivement de plantes ;les carnivores, qui se nourrissent exclusivement d'autres animaux ;les omnivores, qui se nourrissent aussi bien de plantes que d'animaux.Ces régimes alimentaires ont nécessité la spécialisation de tous les organes composant le système digestif.La bouche est le point d'entrée de la nourriture. Chez certains animaux, comme l'Homme, des glandes salivaires produisent ce liquide qui sert à mouiller les aliments pour faciliter la déglutition, mais aussi pour éviter l'abrasion des tissus composants le pharynx et l'œsophage. La salive contient des enzymes servant à prédigérer certains composés organiques, comme c'est le cas de l'amidon. La digestion salivaire est généralement minimale chez les carnivores, car ils ingurgitent leur nourriture sans véritablement la mâcher, contrairement aux herbivores.La bouche a des formes très différentes selon les espèces. Un exemple de cette diversité est présent chez les insectes où les formes de leurs pièces buccales sont aussi différentes que leur régimes alimentaires.Selon le régime alimentaire, les dents ont différentes utilités. Chez les carnivores elles servent à arracher la nourriture et sont à cet effet pointues et sans surface plate. contrairement aux herbivores qui mâchent les plantes pour en extraire la cellulose, leurs dents sont donc plates et crénelés.Chez les omnivores, comme l'Homme, on retrouve ces deux types de dents. La partie antérieure correspond à un carnivore avec les canines, alors que la partie postérieure est de type herbivore avec les molaires.Chez les animaux dépourvus de dents (comme les oiseaux), la mastication est assurée par un organe spécialisé, le gésier.La langue assure plusieurs rôles et pas nécessairement liés à la nutrition comme c'est le cas de la phonation. Elle assure le mélange de la nourriture avec la salive et une fois les aliments correctement mâchés, la langue déplace ces derniers vers l'arrière de la bouche où la déglutition commence.Lorsque les aliments arrivent dans le pharynx, un réflexe déclenche plusieurs mouvements différents selon les espèces. Ainsi chez les vertébrés le larynx pousse la glotte contre l'épiglotte évitant ainsi aux aliments de se retrouver dans le tractus respiratoire.Le réflexe déclenché par la pression dans le pharynx déclenche également dans l'œsophage, qui est un tube musculeux, un péristaltisme, qui est une série de contractions musculaires permettant aux aliments mâchés de se déplacer vers l'estomac. L'entrée dans l'estomac est, selon les espèces, contrôlés par un sphincter et évitant ainsi que les aliments dans l'estomac, qui baignent dans des sécrétions acides, ne remontent dans l'œsophage et n'abîment ce dernier.L'estomac est un organe creux situé sous le diaphragme dont une poche qui sécrète de l'acide chlorhydrique concentré qui est essentiel dans la digestion. Selon le régime alimentaire des animaux, l'estomac peut avoir une structure très différente. Ainsi certains herbivores, tous les carnivores et omnivore ont une seule poche et sont à ce titre des animaux mono gastriques. Les ruminants par contre sont plurigastriques car il est nécessaire pour eux en raison de leur régime alimentaire riche en cellulose de ruminer leur nourriture pour la remastiquer.Le gésier est une partie musculaire (muscle fibreux qui forme une poche) de l'estomac des archosauriens, groupe qui comprend les oiseaux et les crocodiliens. Il a aussi des fonctions glandulaires.C'est aussi la partie située autour des cloaques, notamment chez la poule.Le gésier fait partie des structures anatomiques dites « pro-ventricule » (poches précédant l'estomac dans le tractus digestif), structure également présente chez certains poissons et invertébrés. Des évidences fossiles nous permettent de croire qu'il était présent chez certains dinosaures. Cet organe leur permet de broyer les aliments durs, en avalant des cailloux dits grit (diminutif de gastrolithes).Il n'y a pas d'équivalence au gésier chez les mammifères. La panse des ruminants est un autre type de structure.L'intestin est un tube digestif comptant 3 différentes parties principales (Duodénum-jéjunum-ileum) qui servent à absorber les nutriments comme les Acides Aminés (présents dans les protéines), les Acides Gras et les Oses (sucres). Son pH est plutôt basique à cause de la bile, qui est sécrétée par le foie et concentrée dans la vésicule biliaire. Elle est relarguée dans le duodénum (avec le HCl qui provient de l'estomac).Son rôle est d'absorber l'eau, récupérer quelques nutriments (composés organiques et minéraux) et de préparer les selles.Le rôle du côlon est principalement de stocker les déchets, de récupérer les liquides (H2O) et semi-liquide, de maintenir l'équilibre hydriqueC’est la dernière partie du rectum. La matière fécale y est éjectée lorsqu'elle s'est accumulée dans le côlon et passe le sphincter anal .L'appendiceLe foie s'occupe du stockage des glycogènes dans l'organisme au niveau des muscles et de la sécrétion de la bile pour la digestion.La vésicule biliaire est un organe piriforme, en forme de petite bourse, appendue à la face inférieure du foie. Ce réservoir membraneux (de 8 à 10 cm de long sur 3 à 4 cm de large) stocke la bile entre les phases de sécrétion. On distingue trois parties : le fond, le corps et le col qui se termine par le canal cystique. Ce canal cystique de 3 centimètres de long fait communiquer la vésicule biliaire au canal hépatique commun pour former le canal cholédoque qui s'abouche dans le duodénum.La vésicule peut être l'objet d'anomalie de position, de nombre sans conséquence médicale. Elle peut contenir des calculs biliaires: c'est la lithiase vésiculaire à l'origine de la cholécystite aiguë.Le pancréas est un organe abdominal, une glande annexée au tube digestif appartenant à la cavité péritonéale située derrière l'estomac, devant et au-dessus des reins. Ses fonctions dichotomiques de glandes à sécrétions exocrine comme les enzymes digestifs et endocrine comme le fameux Insuline font du pancréas une glande amphicrine. Chez l'Homme, le pancréas avoisine les 15 cm de long pour une masse allant de 70 à 100 g.Système digestif humainSystème digestif aviaireRessource relative à la santé : (en) Medical Subject Headings  Portail de la physiologie   Portail de l’anatomie"
médecine;"Une glande endocrine est une glande interne qui sécrète des hormones dans la circulation sanguine directement, plutôt que via un canal comme une glande exocrine. Ces hormones exercent alors leur action spécifique sur des organes cellules ou récepteur distants. On trouve des glandes endocrines chez la plupart des animaux, y compris chez les invertébrés.Les hormones agissent comme des sortes de messagers biochimiques, régulant de nombreuses fonctions de l'organisme telles que la croissance et le développement, la différenciation sexuelle, la reproduction, le métabolisme, la pression artérielle, la glycémie et assure l'homéostasie de nombreux paramètres corporelsLes animaux possèdent, en plus du système immunitaire, deux grands réseaux de communications internes : le système nerveux et le système endocrinien. L'appareil endocrinien transmet ses messages grâce à la sécrétion des hormones, généralement des peptides ou des protéines, tandis que le système nerveux utilise les neurones, qui libèrent des neurotransmetteurs dans les synapses pour transmettre l'influx nerveux à d'autres neurones. Mais ces deux systèmes ont des inter-relations profondes, puisque certains neurones synthétisent également des peptides, appelés neuropeptides, qui sont alors libérés dans la circulation sanguine : par exemple, chez les mammifères, les fibres nerveuses hypothalamiques à somatostatine ou à hormone thyréotrope (TRH) libèrent dans l'éminence médiane leurs produits de sécrétion, qui atteignent l'hypophyse antérieure par l'intermédiaire des vaisseaux du système porte hypothalamo-hypophysaire.Les épithéliums glandulaires endocrines peuvent être sous forme :de glandes endocrines bien individualisées (exemple : thyroïde, hypophyse, testicules, ovaires…) ;d'amas de cellules endocrines (exemple : îlots de Langerhans du pancréas, cellules de Leydig des testicules) ;dispersée, diffuse, au sein d'autres organes (exemple : cellules endocrines du tube digestif, comme les cellules à gastrine de l'estomac, les cellules à sécrétine du duodénum, ou les cellules à glicentine du côlon).Certaines glandes sont amphicrines, c'est-à-dire qu'elles sont à la fois endocrines et exocrines, comme les gonades (testicules et ovaires) ou le pancréas, sécrétant vers le milieu extérieur, généralement par l'intermédiaire de canaux excréteurs.Chez les mammifères, les glandes endocrines pures sont la thyroïde, les parathyroïdes, les surrénales, l'hypophyse, la glande pinéale (ou épiphyse). Chez les arthropodes, on peut citer la glande de mue.Enfin, d'autres organes peuvent également jouer un certain rôle endocrine : par exemple, les cellules de la graisse (ou adipocytes) sécrètent de la leptine ; les ovaires et les testicules produisent naturellement les gamètes, mais ils ont également une fonction endocrine.Pendant la grossesse, le placenta joue également le rôle d'une glande endocrine ; il devient le principal producteur d'hormones stéroïdes. Les hormones produites par ces glandes peuvent être des peptides (par exemple la TRH) ou des protéines (par exemple l'insuline) ; des stéroïdes comme l'œstrogène, la progestérone et la testostérone ; ou des dérivés d'acides aminés comme les hormones thyroïdiennes.HypothalamusHypophyseSurrénaleThyroïdeOvaire ou testiculeParathyroïdeThymusPancréasGlande pinéaleGlande de mueOvaire ou testiculeComplexe allato-cardiaqueEndocrinologie Portail de l’anatomie   Portail de la médecine   Portail de la physiologie"
médecine;"Une hormone est une substance produite de façon naturelle par un organe du corps, qui est transportée par le sang et agit sur d’autres organes. C'est une substance chimique biologiquement active, synthétisée par une cellule glandulaire et sécrétée dans le milieu intérieur où elle circule, agissant à distance et par voie sanguine sur des récepteurs spécifiques d'une cellule cible. Elle transmet un message sous forme chimique et joue donc un rôle de messager dans l'organisme. Le terme « hormone » (du grec ?????, mettre en mouvement) a été adopté par Starling en 1905 pour désigner les substances qui assurent la liaison entre les divers organes.Une hormone est une molécule messagère produite par le système endocrinien (une glande endocrine ou un tissu endocrinien) en réponse à une stimulation et capable d'agir à très faible dose.Elle est ensuite diffusée dans l'ensemble de l'organisme. Les hormones animales sont sécrétées par des glandes spécialisées et diffusées par le sang ou la lymphe. Les hormones, comme les autres molécules circulantes, sont excrétées dans les excréments et l'urine, parfois après conjugaison et/ou dégradation. Des molécules apparentées, les phéromones, sont produites par des glandes externes, qui servent par exemple chez l'animal à marquer le territoire, la dominance dans le groupe ou les dispositions sexuelles. Chez les végétaux, les hormones sont soit véhiculées par la sève, soit transportées activement par les cellules, soit diffusées entre les cellules dans la paroi ou vers l'extérieur, avec émission éventuelle dans l'atmosphère sous forme gazeuse (éthylène par exemple) ou dans la rhizosphère dans le sol.L'organe émetteur agit ainsi à distance sur l'ensemble des organes cibles de l'organisme ou d'organismes voisins de la même espèce, voire d'organismes symbiotes dont les récepteurs sont activés au contact des hormones spécifiques (interactions durables).Les hormones ont une fonction de communication qui, en comparaison avec celle du système nerveux, peut être qualifiée de lente, continue et diffuse. Les concentrations hormonales, étudiées en endocrinologie, contiennent donc des informations représentatives de différents états. Elles régulent ainsi l'activité d'un ou plusieurs organes ou organismes dont elles modifient le comportement et les interactions. Exemple : la mélatonine a une concentration dans le sang qui est régulée par les variations de la lumière (alternances jour/nuit) : comme elle est produite pendant la nuit, sa concentration circulante est plus élevée en hiver (nuits plus longues) qu'en été. C'est elle qui est responsable des variations saisonnières de la reproduction chez les petits ruminants (ovins, caprins), les chevaux et de très nombreuses espèces sauvages.La régulation de la sécrétion hormonale se fait par l'intermédiaire de rétrocontrôle, dit « positif » en cas d'augmentation de sécrétion de l'hormone, et « négatif » s'il induit une diminution de la sécrétion hormonale. Cette régulation est également influencée par de nombreux cycles hormonaux ou systèmes en cascade où la concentration en une première hormone commande la libération de la (ou des) suivante(s), ou au contraire l'inhibition de leur sécrétion. Exemple : la GnRH contrôle la libération de FSH et LH. (Elle agit donc sur leurs propres cycles de rétrocontrôle pour influer sur leurs concentrations). FSH et LH jouent un rôle majeur dans la libération d'hormones sexuelles dans le sang. C'est en fonction de cette concentration en hormones sexuelles qu'est libérée la GnRH. Les hormones interviennent dans de nombreux processus, dont la reproduction, la différenciation cellulaire, l'homéostasie, ou encore la régulation des rythmes chronobiologiques…Le rôle des hormones sexuelles externes est encore très discuté chez l'Homme qui a, par rapport aux autres mammifères, un odorat faible et une sexualité plus complexe, mais certaines études laissent penser qu'il existe. Les poils des aisselles et de la zone pubienne, du scrotum et du périnée pourraient ainsi jouer un rôle de « diffuseur hormonal », par exemple d'androstadienone (dérivé de la testostérone présent dans la sueur et d'autres sécrétions masculines, qui influe sur l'humeur des femmes et affecte la sécrétion de l'hormone lutéinisante stimulant l'ovulation). Il a été montré que des extraits de sueur féminine placés sur la lèvre supérieure, sous les narines d'autres femmes pouvaient modifier leurs taux d'hormones et synchroniser leurs cycles menstruels avec le cycle de la femme ayant fourni l'échantillon de sueur. On a aussi montré que des extraits de sueur masculine, déposé sur la lèvre supérieure d'une femme élèvent le taux de cortisol de cette femme dans les 15 minutes qui suivent, avec des effets persistants une heure (on ignore encore si c'est le taux de cortisol qui affecte l'humeur des femmes ou l'inverse).Les endocrinologues sont amenés à travailler sur de nouvelles questions telles que :relations entre hormones ou perturbateurs endocriniens et cancers ;conséquences du traitement substitutif de la ménopause ;effets de la contraception hormonale au long terme sur la santé, et aussi sur l'environnement (hormones diffusées via les urines et non traitées par les stations d'épuration) ;influence des hormones sur le développement de l’obésité et du diabète de type 2 ;impacts de l'usage d'hormones dans l'alimentation animale.Chez les vertébrés, on distingue les classes chimiques suivantes :Les hormones dérivées d'amines, qui sont constituées d'un seul acide aminé (la tyrosine ou le tryptophane) mais sous une forme dérivée. Exemples : les catécholamines et la thyroxine ;Les hormones peptidiques ; qui sont des chaînes d'acides aminés, donc des protéines, appelées peptides pour les plus courtes. Exemples d'hormones à base d'oligopeptides : le TRH et la vasopressine. Exemples d'hormones de type protéines : l'insuline et l'hormone de croissance. Ou encore l'ocytocine, découverte en 1954, fabriquée dans l’hypothalamus et la post-hypophyse, qui stimule les contractions de l'utérus chez la femme enceinte et accélère le travail de l'accouchement ;Les hormones stéroïdes, qui sont des stéroïdes dérivés du cholestérol. Les principales sources sont la cortico-surrénale et les gonades. Exemples d'hormones stéroïdes : les œstrogènes, la testostérone et le cortisol. Les hormones du type stérol tel le calcitriol sont un système homologue ;Les hormones à base de lipides et de phospholipides sont dérivées de lipides comme l'acide linoléique et de phospholipides comme l'acide arachidonique. Les eicosanoïdes forment la classe principale, parmi laquelle les plus étudiées sont les prostaglandines.Les hormones végétales sont plus rigoureusement appelées phytohormones ou facteurs de croissance car ce ne sont pas à proprement parler des hormones. Elles ont souvent comme fonction d'assurer la croissance de la plante ou sa morphogenèse.C'est le cas notamment de l'auxine qui contribue à la formation des organes de la plante (les racines par exemple) et à sa croissance mais intervient aussi dans les phénomènes de tropisme.Elles se distinguent des hormones animales en plusieurs points :leur sécrétion n'est pas assurée par des organes spécifiques de la plante (tout juste existe-t-il des zones de synthèse privilégiées) ;leur effet varie en fonction de leur concentration (exemple : à faible concentration 10?10 g/mL, l'auxine a un effet discret positif sur la croissance racinaire. À de plus fortes concentrations, 10?8 g/mL, elle inhibe l'élongation et induit la rhizogenèse) ;elles agissent rarement seules : leurs effets résultent bien souvent d'une action coordonnée de plusieurs hormones (exemple : stimulation de la division cellulaire grâce à l'action conjuguée de l'auxine et des cytokinines).Sur le même mode d'action chimique :dans le cas de diffusions limitées à une zone restreinte, on parle d'hormone paracrine ou substance paracrine. Il existe aussi un cas particulier où la substance agit sur la cellule productrice, on parle alors d'hormone autocrine ;les hormones libérées par des neurones sont appelées des neurohormones (à ne pas confondre avec les neuromédiateurs). Elles sont sécrétées de la même manière que les neurotransmetteurs dont le mode d'action est identique, mais dans le sang et non dans la synapse. Il arrive d'ailleurs qu'une même molécule soit appelée neurotransmetteur ou hormone suivant son utilisation ou le contexte dans lequel elle est étudiée.Certaines substances ont des effets chez les êtres humains et les animaux qui sont similaires à ceux des hormones naturelles. Parmi les androgènes naturellement présents dans certaines huiles, on trouve le campestérol et le stigmastérol. Ces substances sont appelées phyto-androgènes. Les phyto-androgènes sont beaucoup plus communs. On les retrouve dans des aliments de base comme le soja. Plusieurs substances chimiques entraînent également des effets hormonaux chez les mammifères et perturbent l'action des hormones naturelles. Ces substances sont classées dans la catégorie des perturbateurs endocriniens. Le terme perturbateur endocrinien est souvent utilisé comme synonyme de xénohormone (xéno-androgène s'il entraîne des effets androgéniques et xénoestrogène pour les effets œstrogéniques) même si ce dernier terme peut désigner tout composé naturel ou de synthèse présentant des propriétés similaires à celles des hormones (se liant généralement à certains récepteurs hormonaux).HormonothérapieHormone sexuelleSystème endocrinienPerturbateur endocrinienListe d'hormonesPhéromonesHormone gastro-intestinaleLes Hormones, PUF, coll. Que sais-je ?, n°63 : [1] Portail de la biochimie   Portail de la médecine   Portail de la physiologie"
médecine;"Un organe est un groupe de tissus collaborant à une même fonction physiologique. Certains organes assurent simultanément ou alternativement plusieurs fonctions, mais dans ce cas, chaque fonction est généralement assurée par un sous-ensemble de cellules.Le niveau d'organisation supérieur à l'organe sont les appareils et les systèmes, qui remplissent un ensemble de fonctions complémentaires. Le niveau d'organisation inférieur à l'organe est le tissu.L'étude des organes relève de l'anatomie, qui fait partie du domaine de la biologie.Les organes peuvent être décrits par des planches anatomiques, des préparations anatomiques, des représentations en cire ou des modèles informatiques.Le mot organe a une étymologie latine d'origine grecque :du latin « organum », « instrument, outil » ;du grec « ??????? » (organon) de même sens, « instrument, outil ».Dans le domaine de la recherche médicale et de la chirurgie reconstructrice, on parle aussi :d'organes artificiels (exemple : cœur artificiel) ;d'organes bio-artificiels, qui jouent le rôle de microcosme, par exemple pour l'évaluation toxicologiques de produits, en alternative à l'expérimentation animale (alternatives demandées en Europe par la directive REACH). Ils évoluent parfois vers des systèmes de tests miniaturisés, avec par exemple des cellules fixées sur une puce électronique qui réagissent à des toxiques ou médicaments, éventuellement dilués dans un fluide circulant. En France, l'INERIS, avec l'UTC produit des modèles mathématiques pour évaluer la cinétique et les effets des contaminants in vivo à partir de ce type de puces et d'organes bio-artificiels. À titre d'exemple, cette approche doit aider à étudier de manière non invasive et moins coûteuse les effets toxicologiques de l'exposition chroniques à de faible dose de résidus chimiques présent dans les aliments (dont néoformés via la cuisson), pour mieux comprendre et traiter les maladies inflammatoires chroniques intestinales (MICI) ;sous-ensemble d'un appareillage, composé de plusieurs pièces assemblées, destiné à effectuer une opération particulière ou un travail spécifique :mécanique (« transmission », organe de mécanique automobile),électrique (organe de commande) interrupteur, régulateur.La communauté des microbes (« flore intestinale ») qui habitent symbiotiquement l'intestin de l'humain (environ 100 000 milliards de bactéries par être humain) ou des animaux est parfois considérée comme une sorte d'organe virtuel. Le mot microbiote désigne cet organe virtuel.Par exemple, certaines vitamines indispensables ne peuvent être fabriquées par le microbiote. Le « Metagenomics of the Human Intestinal Tract », un programme initié en 2008 pour identifier le métagénome de ces microbes a montré qu'il existe chez l'humain trois groupes de composition bactérienne intestinale spécifiques (dit entérotypes) que l'on conserve toute sa vie, et qui est caractéristique à l'individu (comme le groupe sanguin).Par extension, on parlera d'organe dans le cadre des organisations humaines (exemple : organe de presse).Une pièce mécanique peut être désigné par le mot organe (exemple : organe de transmission dans un véhicule).On classe les organes selon leur fonctions, leur disposition leur nombre (un organe peut appartenir à plusieurs classes).Classement partiel : « organes vitaux », qui sont ceux dont l'organisme ne peut se passer ou qui le mettent en danger de mort en cas de blessure ou amputation. Ce sont généralement aussi des organes internes tels que le cerveau ou des viscères ;organes creux (cœur ou vessie) ;organes multiples (ex. : ganglions lymphatiques), doubles (ex. : poumons, reins), uniques (ex. : foie, cœur, cerveau) ;organe lymphoïde ;organes génitaux (de la reproduction) ;organes préhensiles (mains, pieds, bec, tentacules, queue chez certains vertébrés) ;organes embryonnaires (par exemple, chez l'humain : les branchies ou la queue qui apparaissent puis disparaissent au cours de l’embryogenèse) et organes-reliques (comme chez l'humain le coccyx, reste d'une queue, ou l'appendice, relique de cæcum, bien que ce cas soit discuté, l'appendice pouvant avoir un rôle immunitaire).Selon Ginet et Roux, les premiers organes seraient apparus chez les Plathelminthes ou vers plats. « C'est en effet avec les plathelminthes que le niveau correspondant aux organes est atteint, les précédents embranchements ne dépassaient pas le stade cellulaire ou le stade tissulaire ».Crâne, contenant l’encéphale (cerveau, cervelet et tronc cérébral)Œil, oreillesLangue, dentsCuir cheveluLarynx, pharynxGlandes salivairesGlande thyroïde et glandes parathyroïdesVertèbresMoelle spinale Organes féminins du bassin OvairesTrompe de FallopeUtérusVaginVulveClitoris Organes masculins du bassin TesticulesPénisProstateVésicules séminalesLes fonctions physiologiques sont :PoumonsCoeurCerveauMusclesFoie et intestinsReins et vessiePeauPoumonsRacinesRhizomeTroncTigeBrancheLa structure des organismes biologiques qui constituent la biosphère peut être décomposée en plusieurs niveaux d'organisation : atomique, moléculaire, cellulaire, tissulaire, des organes, des systèmes, et enfin celui de l'organisme dans sa totalité fonctionnelle, et éventuellement de supers-organismes (essaim d'abeilles, récif corallien, etc.). Pour la description de cette structure en niveaux emboîtés, Georges Chapouthier a proposé l'utilisation du concept de mosaïque. Comme dans une mosaïque au sens artistique du terme, qui laisse à ses tesselles l'autonomie de leur couleur ou de leur forme, chaque niveau de complexité du vivant intègre les niveaux inférieurs comme des parties, en leur laissant une autonomie de fonctionnement.L'étude scientifique du vivant se fait par des recherches sur les éléments de chacun de ces niveaux, puis par la compréhension des interactions entre ces différents niveaux (voir l'article « Méthode scientifique »).L'étude du niveau des organes permet de comprendre la structure, la fonction et le fonctionnement des organes, qui constituent les différents systèmes fonctionnels de l'organisme (système nerveux, système digestif, système immunitaire…).AnatomieOrgane ou structure vestigialeClassement thématique des neurosciencesGreffe d'organeDon et vente d'organeOrganismeOrganicisme Portail de la biologie   Portail de la physiologie   Portail de l’anatomie"
médecine;"L’appareil urinaire du système excréteur est l’appareil permettant l’évacuation des produits du catabolisme d'un vertébré sous une forme liquide, l'urine, et assure par conséquent l'épuration du sang ainsi que le maintien de l'homéostasie au sein de l'organisme. Aussi, il maintient l'équilibre sanguin, soit le volume et la composition chimique du sang. Pour ce faire il élimine entre autres les surplus de certains minéraux, nommés électrolytes, et renvoie dans le sang les substances utiles au bon fonctionnement de l'organisme. Chaque jour, un être humain produit 800 à 2000 millilitres d'urine. L'appareil urinaire fait partie du système excréteur.Le système excréteur est constitué des organes excréteurs (néphridies, tubes de Malpighi chez les invertébrés, reins chez les vertébrés, cellules à chlorures des branchies des téléostéens, glandes à sel des oiseaux, et les canaux excréteurs associés).Le système urinaire ou appareil urinaire est un ensemble d'organes dont le rôle est de filtrer puis d'évacuer les déchets de l'organisme sous forme liquide. Il comprend chez les vertébrés une succession d'organes rétro- puis sous-péritonéaux : les reins, qui fabriquent l'urine, et les voies excrétrices urinaires extrarénales constituées par les deux uretères qui transportent l'urine, la vessie qui la stocke et l'urètre qui permet de l'évacuer (phénomène de miction qui aboutit à l'évacuation périodique, complète et contrôlée des urines)L'appareil urinaire se compose des reins, des uretères, de la vessie, de l'urètre et du méat urinaire. Il se forme et commence à fonctionner avant la naissance. Quand la vessie contient 250 mL d'urine, l'envie d'uriner se fait sentir.Le rôle de cet appareil est de former l'urine qui sera évacuée. L'urée est excrétée par les reins qui fabriquent l'urine ; cette urine est acheminée par l'uretère jusqu'à la vessie, une poche retenant l'urine, ensuite rejetée à l'extérieur de l'organisme lors de la miction par l'urètre s'abouchant au méat urinaire.Le corps humain possède deux reins. Toutefois, un seul rein peut suffire à l'accomplissement des fonctions d'épuration et d'élimination.Ils ont la taille d'un poing, la forme d'un haricot et sont de couleur bordeaux. Les reins sont fixés sous les côtes de part et d'autre de la colonne vertébrale, ils sont en liaison avec l'artère rénale, par laquelle arrive le sang à filtrer.Le rein possède une fonction sécrétoire (filtration du sang au niveau des glomérules) puis excrétoire à partir du pyelon (triangle à base issue du hile rénal) origine de l'uretère. On parle de jonction pyelo-urétérale. Chaque rein contient environ 1 million de néphrons. Sur chaque rein, on retrouve des glandes surrénales. Elles sécrètent des hormones qui modifient la quantité des urines produites. Le sang est épuré au niveau du néphron, dans lequel certains éléments sont réabsorbés (ions minéraux, glucose, eau, acides aminés) et retourneront à la circulation sanguine par la veine rénale.Les déchets récupérés constituent une urine primitive qui sera déversée dans le bassinet, puis dans l'uretère attenant au rein dont elle est issue.Ils sont le prolongement des reins. Leur rôle est de collecter l'urine au niveau du bassinet. Ils se présentent comme des tubes dont l'extrémité supérieure prend une forme d'entonnoir, composée de fibres musculaires lisses évitant les reflux d'urine. L'uretère se dirige vers le bas, en avant et  dedans pour rejoindre la partie postéro-supérieure de la vessie. On distingue ainsi à l'uretère  quatre parties :l'uretère lombaire (12 cm) ;l'uretère iliaque (3 cm) ;l'uretère pelvien (12 cm) ;l’uretère mural ou vésical (correspond à la traversée de la paroi vésicale par l’uretère).La vessie se présente sous la forme d'une poche dont les parois sont faites de muscles lisses (le détrusor) et de tissu épithélial et voit s'aboucher à sa partie inférieure l'urètre : on parle de col vésico-urétral.Elle recueille l'urine qui lui parvient par les uretères. Sa capacité est d'environ 300 à 600 ml. L'urine est évacuée au niveau de l'urètre lors de la miction.Le contrôle de la miction est réalisé par un sphincter lisse à commande involontaire et par un sphincter strié volontaire utilisé en cas de retenue forcée (ou en période post-opératoire).Constituée en majeure partie d'eau 95 %, de sels minéraux 2 % (chlorures, phosphates, sulfates, sels ammoniacaux) et des matières organiques 3 % (urée, créatine, acide urique, acide hippurique).Son nom vient d'une molécule issue de la dégradation des protéines : l'urée. Celle-ci est en partie responsable de la couleur jaunâtre de l'urine.En moyenne, les reins produisent 800 à 2000 mL d'urine chaque jour.La couleur de l'urine provient de deux pigments : l'urochrome et l'urobiline. La couleur de l'urine peut beaucoup varier sur 24 heures, car le taux d'urobiline varie énormément en fonction de la sécrétion biliaire.La cystite, la néphrite, la pyélonéphrite ainsi que l'urétrite sont des inflammations des organes de l'appareil urinaire.La glycosurie représente le taux de glucose dans l'urine. Sa valeur normale est nulle.Le glucose est en principe filtré et réabsorbé par le rein. Au-delà d'une certaine glycémie (taux de glucose sanguin), les capacités de réabsorption du rein, qui sont d'environ 9mmol/L, sont saturées : l'excédant est donc évacué par l'urine.Une glycosurie peut être signe de diabète.La rétention aiguë d'urine est l'incapacité d'uriner malgré le remplissage de la vessie. Elle peut avoir plusieurs origines.MictionReinUretèreUrologieVessieRessources relatives à la santé : FMA TA2 Uberon Xenopus Anatomy Ontology (en) Medical Subject Headings (en) NCI Thesaurus (no + nn + nb) Store medisinske leksikon (la + en) TA98 (cs + sk) WikiSkripta (en) Excretory products and their elimination Portail de l’anatomie   Portail de la médecine   Portail de la physiologie"
médecine;"Le métabolisme est l'ensemble des réactions chimiques qui se déroulent à l'intérieur de chaque cellule d'un être vivant et lui permettant notamment de se maintenir en vie, de se reproduire (se diviser), de se développer et de répondre aux stimuli de son environnement (échanges par exemple). Certaines de ces réactions chimiques se déroulent en dehors des cellules de l'organisme, comme la digestion ou le transport de substances entre cellules. Cependant, la plupart de ces réactions ont lieu dans les cellules elles-mêmes et constituent le métabolisme intermédiaire.La biochimie cellulaire repose sur des réactions chimiques catalysées par des enzymes, c'est-à-dire des protéines possédant chacune la faculté de faciliter une réaction chimique spécifique. Ces réactions sont régies par les principes de la thermodynamique et s'organisent en voies métaboliques. Ces dernières sont constituées d'un ensemble de transformations permettant de convertir un composé chimique en un autre à travers des transformations successives, parallèles ou cycliques, catalysées par des enzymes. Certaines de ces enzymes sont soumises à une régulation par des métabolites cellulaires ou par des signaux extracellulaires. Ces facteurs de régulation modifient la cinétique enzymatique, accélérant ou ralentissant certaines réactions déterminantes, et aboutissant à l'autorégulation du système par l'ouverture et la fermeture des différentes voies métaboliques selon les circonstances.Dans l'ensemble des réactions constituant le métabolisme, on distingue d'une part l'anabolisme, qui représente l'ensemble des voies de biosynthèse des constituants cellulaires(et/ou autres constituants), et d'autre part le catabolisme, qui représente l'ensemble des voies de dégradation de ces constituants cellulaires en petites molécules pour en libérer l'énergie par oxydation ou pour rebâtir d'autres constituants cellulaires. Les réactions de l'anabolisme et du catabolisme sont interconnectées à travers des molécules spécialisées jouant le rôle de cofacteurs enzymatiques. C'est par exemple le cas de l'adénosine triphosphate (ATP), dont l'hydrolyse en adénosine diphosphate (ADP) et en phosphate inorganique (Pi) est souvent couplée aux réactions d'anabolisme pour les rendre thermodynamiquement favorables. Le nicotinamide adénine dinucléotide (NAD+ à l'état oxydé) et le nicotinamide adénine dinucléotide phosphate (NADPH à l'état réduit), quant à eux, sont des transporteurs d'électrons utilisés dans les réactions d'oxydoréduction cellulaires, le NAD+ plutôt dans le catabolisme et le NADPH dans l'anabolisme. Des coenzymes permettent également d'échanger de la matière entre les différentes voies métaboliques. Ainsi, la coenzyme A permet d'activer des groupes acyle pour former une acyl-CoA, dont la plus importante est l'acétyl-CoA : cette dernière se trouve au carrefour de plusieurs voies métaboliques majeures, telles que la dégradation des glucides et des lipides, la production d'énergie métabolique, ou encore la biosynthèse des acides gras et des oses.Le métabolisme d'un être vivant définit les types de substances chimiques qui sont des nutriments pour cet organisme et lesquels sont au contraire des poisons : ainsi, le sulfure d'hydrogène H2S est indispensable au développement de certains procaryotes, alors que ce gaz est toxique pour les animaux en général. L'intensité du métabolisme de base détermine également la quantité de nourriture nécessaire à l'organisme.Il est frappant d'observer la similitude des voies métaboliques fondamentales et des composés biochimiques à travers les organismes les plus divers. Ainsi, les acides carboxyliques constituant les intermédiaires du cycle de Krebs se retrouvent chez tous les êtres vivants connus de nos jours, allant d'un procaryote tel qu'E. coli jusqu'à un métazoaire tel que l'éléphant. Ces similitudes remarquables sont très certainement dues à l'apparition précoce de ces voies métaboliques au cours de l'évolution des formes de vie sur Terre et à leur conservation en raison de leur efficacité,.« On peut considérer, de façon arbitraire, trois périodes dans l'évolution des modes de raisonnement sur le fonctionnement des organismes vivants, depuis l'Antiquité grecque jusqu'à la Renaissance : la période des philosophes grecs riche d'idées audacieuses, souvent spéculatives, le Moyen Âge dominé par le pouvoir ecclésiastique qui prend de l'héritage grec ce qui est en accord avec le finalisme de la tradition biblique, enfin la période ou fleurit l'alchimie qui marque un renouveau dans la pratique expérimentale et annonce le nouvel esprit de la Renaissance. »Certains des philosophes grecs méditent sur la structure et la dynamique du vivant. Leur théorie des quatre éléments, enseignée jusqu'au XVIIIe siècle, considère que le monde (et donc les organismes vivants, leurs organes et tissus) résultent de l'association de la terre, du feu, de l'air et de l'eau et qu'elle devrait faire mieux comprendre le métabolisme des humeurs (chaque humeur étant associée à un organe). Dans son traité Parties des animaux, Aristote décrit le processus métabolique à partir d'un principe vital, le pneuma (pneuma inné apporté par le sperme ou pneuma inspiré, souffle vital produit par l'évaporation du sang qui a lieu dans le cœur, siège immédiat de l'âme). Ce souffle vital, fabriqué dans le cœur à partir de l'air inspiré « répartit dans le corps la chaleur qui donne la vie ; il permet la digestion et l'assimilation des aliments. Les aliments broyés par les dents sont désintégrés dans l'estomac, puis dans l'intestin pour être portés au cœur et transformés en sang. »La physiologie expérimentale plonge ses racines dans les alchimistes de l'Antiquité orientale, du Moyen Âge et de la Renaissance et dont les expérimentations sur les métaux ont préparé la méthode expérimentale. Dans ce contexte, Santorio Santorio fait figure de pionnier en inventant une balance reliée à un siège, afin de peser à la fois ce qu'il absorbe, et ce qu'il rejette à travers la transpiration et les excréments. Son expérience métabolique réalisée pendant une trentaine d'années donne les premiers résultats d'une étude à long terme sur le métabolisme humain, publiés dans son livre Ars de statica medicina en 1614.La nutrition humaine devient une discipline scientifique à la fin du XVIIIe siècle et se concentre au XIXe siècle sur le métabolisme de base et la valeur calorique des aliments. Les expériences pionnières pour vérifier les liens entre la nourriture et l'énergie sont en effet réalisées dans le contexte de la crise sociologique de la révolution industrielle où il devient « important pour les gestionnaires de fonder la hiérarchie du travail sur les capacités de chacun et, pour le travailleur, de connaître son rôle dans le système ». L'impact du « paradigme énergétiste » dans les recherches se traduit alors par « la mesure du rendement de l'activité corporelle [qui] devient l’élément central du processus d’expérimentalisation de la physiologie ».Au cours des années 1854 à 1864, Louis Pasteur réalise des expériences démontrant que la fermentation alcoolique n'est pas un processus purement chimique mais un processus physiologique résultant du métabolisme de microorganismes. En 1897, le chimiste Eduard Buchner et son frère Hans (de), bactériologiste, montrent que cette fermentation nécessite des médiateurs du métabolisme, les enzymes, biocatalyseurs qui permettent d'accélérer la plupart des réactions biochimiques se déroulant dans la cellule (anabolisme, catabolisme, oxydo-réduction, transferts d'énergie).Depuis les années 1950, les recherches biochimiques se multiplient. S'appuyant sur le développement de techniques telles que la chromatographie, la microscopie électronique, la cristallographie aux rayons X, le traçage isotopique, la spectroscopie RMN ou la dynamique moléculaire, elles aboutissent à une meilleure connaissance des voies métabolique et des molécules impliquées.Les animaux, les plantes et les microbes sont formés de trois grandes familles de molécules :Les lipides, qui jouent un rôle à la fois de réserve d'énergie, de constituant principal des membranes de leurs cellules, et de communication entre cellules par des mécanismes de signalisation lipidique ;Les peptides, qui jouent un rôle déterminant à la fois dans la structure des organismes (protéines), leur biochimie (enzymes) et l'intégration physiologique entre les organes (hormones peptidiques) ;Les glucides, qui servent à la fois à stocker de l'énergie, à stabiliser certaines protéines et à favoriser l'adhérence des cellules entre elles, par exemple dans les mécanismes de reconnaissance du système immunitaire à travers les lectines.Ces molécules étant essentielles à la vie, le métabolisme cellulaire consiste ou bien à les synthétiser pour produire de nouvelles cellules et faire croître les tissus, ou bien à les dégrader lors de la digestion pour les utiliser comme sources d'énergie et de constituants élémentaires qui peuvent être recyclés dans la biosynthèse de nouvelles biomolécules.Les macromolécules biologiques sont elles-mêmes des polymères appartenant à trois familles différentes :Les polypeptides, qui sont constitués d'acides aminés, au sein desquels on trouve les protéines et les enzymes ;Les polysaccharides, qui sont constitués d'oses — par exemple l'amidon, la cellulose, le glycogène ;Les polynucléotides, qui sont constitués de nucléotides, et dont les deux représentants sont les acides ribonucléiques (ARN) et les acides désoxyribonucléiques (ADN), lesquels portent le code génétique, qui détermine notamment la nature des protéines et des enzymes — et donc la physiologie — de chaque cellule.Les protéines sont constituées d'acides ?-aminés liés entre eux par une liaison peptidique pour former une chaîne linéaire. De nombreuses protéines sont des enzymes qui catalysent des réactions chimiques du métabolisme. D'autres protéines ont un rôle structurel ou mécanique, comme celles du cytosquelette, qui maintient la forme générale de la cellule. Les protéines jouent également un rôle clé dans la signalisation cellulaire, comme anticorps du système immunitaire, l'adhérence cellulaire, le transport actif à travers les membranes et le cycle cellulaire. Les acides aminés contribuent également à fournir de l'énergie au métabolisme cellulaire en alimentant le cycle de Krebs, en particulier lorsque les principales sources d'énergie, telles que le glucose, font défaut, ou lorsque la cellule est soumise à un stress métabolique.Les lipides sont le groupe de composés biochimiques le plus diversifié. Leur fonction structurelle principale est celle de constituant des membranes cellulaires, notamment de la membrane plasmique et du système endomembranaire des cellules eucaryotes, ainsi que de celles d'organites telles que les mitochondries et les chloroplastes, voire de sous-organites tels que les thylakoïdes. Ils sont également utilisés comme sources d'énergie. On les définit généralement comme des molécules biologiques hydrophobes et amphiphiles, solubles dans les solvants organiques tels que le benzène et le chloroforme. Les graisses sont, parmi les lipides, un grand groupe de composés solides essentiellement constitués d'acides gras et de glycérol. Une molécule formée de trois résidus d'acides gras estérifiant les trois hydroxyles d'un résidu de glycérol est appelée triglycéride. Il existe diverses variations autour de ce thème central, par exemple avec de la sphingosine dans le cas des sphingolipides, et des groupes hydrophiles tels que le groupe phosphate dans le cas des phospholipides. Les stéroïdes, tels que le cholestérol, sont une autre famille importante de lipides.Les glucides sont des aldéhydes ou des cétones ayant plusieurs groupes hydroxyle. Ces molécules peuvent exister sous forme linéaire ou cyclique. Ce sont les molécules biologiques les plus abondantes. Elles remplissent un grand nombre de fonctions, comme substances de stockage et le transport de l'énergie (amidon, glycogène) ou comme composants structurels (cellulose chez les plantes, chitine chez les animaux). Les monomères glucidiques sont appelés oses : ce sont par exemple le galactose, le fructose, et surtout le glucose. Ils peuvent polymériser en donnant des polysaccharides avec une variété de structures quasiment infinie.Les nucléosides résultent de la liaison d'une molécule de ribose ou de désoxyribose à une base nucléique. Ces dernières sont des composés hétérocycliques contenant des atomes d'azote ; elles se divisent en purines et pyrimidines. Les nucléotides sont formés d'un nucléoside et d'un ou plusieurs groupements phosphates liés au sucre.Les deux acides nucléiques, l'acide ribonucléique (ARN) et l'acide désoxyribonucléique (ADN), sont des polymères de nucléotides, ou polynucléotides. L'ARN est constitué de ribonucléotides (contenant un ribose) et l'ADN de désoxyribonucléotides (contenant un désoxyribose). Les acides nucléiques permettent le codage et l'expression de l'information génétique ainsi que son décodage à travers les processus successifs de transcription et de traduction génétique de la biosynthèse des protéines. Cette information est préservée par les mécanismes de réparation de l'ADN et transmise à travers le processus de réplication de l'ADN. De nombreux virus, dits virus à ARN, ont un génome constitué d'ARN et non d'ADN — c'est par exemple le cas du virus de l'immunodéficience humaine (VIH) ou du virus de la grippe— certains ont recours à des transcriptases inverses pour générer dans la cellule hôte une matrice d'ADN à partir du génome viral en ARN, d'autres sont directement répliqués d'ARN en ARN par une ARN polymérase-ARN dépendante (ou réplicase). L'ARN des ribozymes, tels que les splicéosomes (ou particules d'épissage) et les ribosomes, est semblable aux enzymes dans la mesure où il est capable de catalyser des réactions chimiques.Le métabolisme implique un très grand nombre de réactions chimiques différentes formant un réseau de transformations complexe, mais la plupart d'entre elles peuvent être rapprochées de quelques types de réactions basiques consistant en des transferts de groupes fonctionnels. Cela résulte du fait que la biochimie cellulaire fait appel à un nombre relativement restreint de molécules agissant comme des activateurs susceptibles de transporter des groupes d'atomes entre différentes réactions. De telles molécules sont appelées coenzymes. Chaque type de transfert de groupe fonctionnel fait appel à une coenzyme spécifique. Chacune de ces coenzymes est également spécifique d'un certain nombre d'enzymes qui catalysent les réactions de transfert, enzymes qui les altèrent et les régénèrent en permanence.L'adénosine triphosphate (ATP) est la coenzyme universelle des échanges d'énergie chez tous les organismes connus. Ce nucléotide permet de transférer de l'énergie métabolique entre les réactions qui libèrent de l'énergie et celles qui en absorbent. Il n'y a à chaque instant qu'une faible quantité d'ATP dans les cellules, mais, comme ce capital d'ATP est continuellement consommé et régénéré, le corps humain peut en réalité consommer chaque jour une masse d'ATP pratiquement équivalente à son poids total. L'ATP permet de coupler l'anabolisme au catabolisme, le premier consommant l'ATP produit par le second. Il sert également de transporteur de groupes phosphate dans les réactions de phosphorylation.Les vitamines sont des composés organiques indispensables en petite quantité au fonctionnement des cellules mais que ces dernières ne peuvent pas produire elles-mêmes. Chez l'homme, la plupart des vitamines deviennent des coenzymes après quelques transformations dans les cellules. Ainsi, les vitamines hydrosolubles (vitamines B) sont phosphorylées ou couplées à des nucléotides lorsqu'elles sont utilisées dans les cellules. Par exemple, la niacine (acide nicotinique) entre dans la composition du nicotinamide adénine dinucléotide (NAD+) et du nicotinamide adénine dinucléotide phosphate (NADP+), qui sont des coenzymes importantes impliquées dans les réactions d'oxydoréduction comme accepteurs d'hydrogène. Il existe des centaines de déshydrogénases, qui soustraient des électrons de leur substrat et réduisent le NAD+ en NADH et H+. Cette forme réduite de la coenzyme peut alors être utilisée par une réductase. Le couple NAD+ / NADH intervient davantage dans les réactions cataboliques tandis que le couple NADP+ / NADPH est spécifique à l'anabolisme.Les sels minéraux jouent un rôle déterminant dans le métabolisme. Certains sont abondants, comme le sodium et le potassium, tandis que d'autres ne sont actifs qu'à faible concentration. Environ 99 % de la masse des mammifères est constituée des éléments carbone, azote, calcium, sodium, chlore, potassium, hydrogène, phosphore, oxygène et soufre. Les composés organiques (protéines, lipides et glucides) contiennent l'essentiel du carbone et de l'azote, tandis que l'essentiel de l'oxygène et de l'hydrogène sont présents sous forme d'eau.Les sels minéraux les plus abondants agissent comme électrolytes. Les principaux ions sont le sodium Na+, le potassium K+, le calcium Ca2+, le magnésium Mg2+, le chlorure Cl?, le phosphate PO43? et l'ion organique bicarbonate HCO3?. Le maintien de gradients de concentration déterminés à travers les membranes cellulaires permet de maintenir l'équilibre osmotique et le pH du milieu intracellulaire. Les ions sont également essentiels au fonctionnement des nerfs et des muscles grâce au potentiel d'action issu de l'échange d'ions, à travers la membrane plasmique, entre le fluide extracellulaire (en) et le fluide intracellulaire, c'est-à-dire le cytosol. Les ions entrent et quittent les cellules en empruntant des protéines membranaires appelées canaux ioniques. Ainsi, la contraction musculaire dépend du passage des ions calcium, sodium et potassium à travers les canaux ioniques de la membrane cellulaire et les tubules T.Les métaux de transition sont généralement présents à l'état de trace chez les organismes vivants, le zinc et le fer étant les plus abondants d'entre eux,. Ces métaux interviennent comme cofacteurs de certaines protéines et enzymes et sont essentiels à leur bon fonctionnement. C'est par exemple le cas d'une enzyme telle que la catalase et d'une protéine transporteuse d'oxygène telle que l'hémoglobine. Les cofacteurs métalliques se lient spécifiquement à certains sites des protéines. Bien que les cofacteurs puissent être modifiés au cours de la réaction catalysée, ils reviennent toujours à leur état d'origine à la fin de la réaction. Ils sont absorbés par les organismes à l'aide de transporteurs spécifiques, par exemple les sidérophores pour absorber le fer, et sont liés à des protéines de stockage telles que la ferritine et les métallothionéines lorsqu'ils ne sont pas utilisés,.Le catabolisme est l'ensemble des processus métaboliques de dégradation des biomolécules. Cela comprend par exemple la dégradation et l'oxydation des nutriments. Le catabolisme a pour fonction de fournir l'énergie et les constituants élémentaires indispensables au métabolisme de la cellule. La nature exacte de ces réactions dépend de chaque organisme. Les êtres vivants peuvent être classés en fonction de leur sources d'énergie et de carbone, ce qu'on appelle leur type trophique : Les organotrophes utilisent des molécules organiques comme source d'énergie tandis que les lithotrophes utilisent des substrats inorganiques et que les phototrophes convertissent l'énergie solaire en énergie chimique. Ces différents métabolismes reposent cependant tous sur le transfert d'électrons de composés donneurs — tels que des molécules organiques, l'eau, l'ammoniac, le sulfure d'hydrogène ou encore des cations de fer(II) Fe2+ (fer ferreux) — vers des composés accepteurs d'électrons tels que l'oxygène, les nitrates ou encore les sulfates. Chez les animaux, ces réactions conduisent à dégrader des molécules organiques complexes en molécules plus simples telles que le dioxyde de carbone et l'eau. Chez les organismes photosynthétiques tels que les plantes et les cyanobactéries, ces réactions permettent de libérer l'énergie de la lumière solaire absorbée et stockée par l'organisme.Les principaux groupes de réactions cataboliques chez les animaux peuvent être classés en trois étapes principales. Dans la première, les grandes molécules organiques telles que les protéines, les polysaccharides ou les lipides sont digérés en leurs composants élémentaires à l'extérieur des cellules. Puis ces composants élémentaires sont absorbés par les cellules et convertis en métabolites encore plus petits, le plus souvent en acétyl-coenzyme A (acétyl-CoA), avec libération d'un peu d'énergie. Enfin, le résidu acétyle de l'acétyl-CoA est oxydé en eau et dioxyde de carbone par le cycle de Krebs et la chaîne respiratoire, cette dernière permettant de libérer l'énergie des électrons à haut potentiel transférés au NADH au cours du cycle de Krebs.Les macromolécules telles que l'amidon, la cellulose et les protéines, qui sont des biopolymères, ne peuvent être absorbées facilement par les cellules et doivent être clivées en oligomères, voire en monomères, afin de pouvoir être métabolisées. C'est ce qu'on appelle la digestion. Plusieurs classes d'enzymes communes réalisent ces transformations, par exemple les peptidases, qui clivent les protéines en oligopeptides et en acides aminés, ou encore les glycoside hydrolases (ou glycosidases), qui clivent les polysaccharides en oligosaccharides et en oses.Les microorganismes sécrètent leurs enzymes digestives dans leur voisinage, alors que les animaux sécrètent ces enzymes uniquement à partir de cellules spécialisées situées dans leur appareil digestif. Les acides aminés et les oses libérés par ces enzymes extracellulaires sont ensuite absorbées à travers la membrane plasmique des cellules par des protéines membranaires de transport actif,.Les glucides sont généralement absorbés par les cellules après avoir été préalablement digérés en oses. La principale voie de dégradation des oses à l'intérieur de la cellule est la glycolyse, qui produit quelques molécules d'ATP et deux molécules de pyruvate par molécule de glucose dégradée. Le pyruvate est un métabolite commun à plusieurs voies métaboliques, mais l'essentiel est converti en acétyl-CoA pour alimenter le cycle de Krebs. Ce dernier produit encore quelques molécules d'ATP, mais son produit essentiel est le NADH, issu de la réduction du NAD+ lors de l'oxydation de l'acétyl-CoA. Cette oxydation libère du dioxyde de carbone comme sous-produit. En conditions anaérobies, la glycolyse produit du lactate par transfert des électrons du NADH au pyruvate par une lactate déshydrogénase en vue de régénérer du NAD+ pour la glycolyse. Une voie alternative pour la dégradation du glucose est la voie des pentoses phosphates, qui a pour fonction première non pas de libérer de l'énergie, mais de produire des précurseurs de diverses biosynthèses tels que du NADPH, utilisé notamment pour la biosynthèse des acides gras, ainsi que du ribose-5-phosphate, utilisé pour la synthèse des nucléotides, et de l'érythrose-4-phosphate, précurseur d'acides aminés aromatiques.Les lipides sont dégradés par hydrolyse en glycérol et acides gras. Le glycérol est dégradé par la glycolyse tandis que les acides gras le sont par la ?-oxydation pour produire de l'acétyl-CoA, dégradé à son tour par le cycle de Krebs. L'oxydation des acides gras libère davantage d'énergie que les glucides car ces derniers contiennent plus d'oxygène et sont donc davantage oxydés que les acides gras.Les acides aminés sont utilisés ou bien pour produire des protéines et diverses autres biomolécules, ou bien oxydés en urée et dioxyde de carbone pour libérer de l'énergie. Leur oxydation commence par leur conversion en ?-cétoacide par une transaminase qui clive leur groupe amine, ce dernier alimentant le cycle de l'urée. Plusieurs de ces ?-cétoacides sont des intermédiaires du cycle de Krebs : la désamination du glutamate donne ainsi de l'?-cétoglutarate. Les acides aminés glucoformateurs peuvent également être convertis en glucose à travers la néoglucogenèse.Au cours de la phosphorylation oxydative — qu'il faudrait appeler plus correctement en français oxydation phosphorylante — les électrons à haut potentiel, issus des réactions d'oxydation du métabolisme, sont transférés à de l'oxygène avec libération d'énergie, cette énergie étant récupérée pour synthétiser de l'ATP. Ceci est réalisé par les eucaryotes à travers une série de protéines membranaires des mitochondries formant la chaîne respiratoire. Chez les procaryotes, ces protéines se trouvent dans la membrane interne. Ces protéines membranaires utilisent l'énergie libérée par la circulation des électrons depuis les coenzymes réduites telles que le NADH et le FADH2 vers l'oxygène pour pomper des protons à travers la membrane mitochondriale interne (chez les eucaryotes) ou la membrane plasmique (chez les procaryotes).Le pompage des protons hors de la matrice mitochondriale ou du cytoplasme génère un gradient de concentration de protons à travers les membranes — c'est-à-dire une différence de pH. Il en découle un gradient électrochimique. Cette « force proton motrice » actionne une enzyme appelée ATP synthase qui fonctionne comme une turbine qui catalyse la phosphorylation de l'ADP en ATP au passage des protons qui retournent vers la matrice mitochondriale à travers la membrane mitochondriale interne.La chimiolithotrophie (en) est un type trophique définissant les procaryotes qui tirent leur énergie de composés inorganiques. Ces organismes peuvent utiliser l'hydrogène, les composés réduits du soufre — sulfure S2?, sulfure d'hydrogène H2S, thiosulfate S2O32? — le fer ferreux (Fe2+) et l'ammoniac (NH3) comme donneurs d'électrons qu'ils transfèrent à des accepteurs tels que l'oxygène O2 ou l'anion nitrite (NO2?). Ces processus microbiens sont importants du point des vue des cycles biogéochimiques planétaires tels que le cycle de l'azote, la nitrification et la dénitrification, et sont déterminants pour la fertilité des sols,.L'énergie lumineuse est absorbée par les plantes, les cyanobactéries, les bactéries pourpres, les bactéries vertes sulfureuses et certains protistes. Ce processus est souvent couplé à la conversion du dioxyde de carbone en composés organiques dans le cadre de la photosynthèse. Ces deux processus — absorption de l'énergie lumineuse et biosynthèse de composés organiques — peuvent néanmoins fonctionner séparément chez les procaryotes. Ainsi, les bactéries pourpres et les bactéries vertes sulfureuses peuvent utiliser la lumière du soleil comme source d'énergie et en même temps mettre en œuvre ou bien un processus de fixation du carbone ou bien un processus de fermentation des composés organiques,.Chez de nombreux organismes, l'absorption de l'énergie solaire repose sur des principes semblables à ceux de la phosphorylation oxydative dans la mesure où un phénomène physique — la récupération de l'énergie des électrons de coenzymes réduites — est couplé à un phénomène chimique — la phosphorylation de l'ADP en ATP — par chimiosmose au moyen d'un gradient de concentration de protons générant un gradient électrochimique à travers une membrane. Dans le cas de la photosynthèse, les électrons à haut potentiel sont issus de protéines d'absorption de l'énergie lumineuse appelées centres réactionnels photosynthétiques ou rhodopsines. Les centres réactionnels se déclinent en deux photosystèmes selon le pigment photosynthétique présent : la plupart des bactéries photosynthétiques n'en ont qu'un, tandis que les plantes et les cyanobactéries en ont deux.Chez les plantes, les algues et les cyanobactéries, le photosystème II transfère l'énergie lumineuse à deux électrons d'une molécule d'eau qui sont captés par le complexe cytochrome b6f tandis que de l'oxygène O2 est libéré. L'énergie des électrons à haut potentiel transférés au complexe cytochrome b6f est utilisée pour pomper des protons à travers les membranes des thylakoïdes dans les chloroplastes, protons dont le retour dans le lumen s'accompagne de la phosphorylation d'ADP en ATP par une ATP synthase, comme dans le cas de la phosphorylation oxydative. Les électrons passent ensuite à travers le photosystème I et peuvent réduire une coenzyme NADP+ en NADPH en vue de son utilisation par le cycle de Calvin, ou bien être utilisés pour produire encore davantage d'ATP.L'anabolisme comprend l'ensemble des voies métaboliques qui utilisent l'énergie (ATP) et le pouvoir réducteur (NADH) produits par le catabolisme pour synthétiser des biomolécules complexes. De manière générale, les molécules complexes qui contribuent aux structures cellulaires sont construites étape par étape à partir de précurseurs bien plus petits et plus simples.L'anabolisme comprend trois étapes principales :Tout d'abord la production de précurseurs tels que les acides aminés, les oses, les isoprénoïdes et les nucléotides ;Puis leur activation sous une forme réactive du point de vue biochimique en utilisant l'énergie de l'ATP ;Enfin l'assemblage de ces précurseurs activés pour construire des molécules complexes telles que les protéines, les polysaccharides, les lipides et les acides nucléiques.Les organismes diffèrent dans le nombre des constituants de leurs cellules qu'ils sont capables de produire eux-mêmes. Les autotrophes tels que les plantes peuvent synthétiser les molécules organiques complexes de leurs cellules tels que les polysaccharides et les protéines à partir de molécules très simples comme le dioxyde de carbone CO2 et l'eau H2O. En revanche, pour produire leurs biomolécules complexes, les hétérotrophes ont besoin de nutriments plus complexes comme des sucres et des acides aminés. Les organismes peuvent être classés plus finement en fonction de leur source d'énergie première : les photoautotrophes et les photohétérotrophes tirent leur énergie de la lumière du soleil tandis que les chimioautotrophes et les chimiohétérotrophes tirent leur énergie de réactions d'oxydoréduction.La photosynthèse est la biosynthèse de glucides à partir d'eau et de dioxyde de carbone en utilisant la lumière du soleil. Chez les plantes, les algues et les cyanobactéries, la molécule d'eau H2O est scindée en oxygène O2 et en électrons à haut potentiel dont l'énergie est utilisée pour phosphoryler de l'ADP en ATP et pour former du NADPH utilisé pour réduire le dioxyde de carbone en 3-phosphoglycérate, lui-même précurseur du glucose. Cette réaction de fixation du carbone est réalisée par la Rubisco, une enzyme essentielle du cycle de Calvin. Il existe trois types différents de photosynthèse chez les plantes : la fixation du carbone en C3, la fixation du carbone en C4 et le métabolisme acide crassulacéen (CAM). Ces types de réactions diffèrent par la voie empruntée par le dioxyde de carbone pour entrer dans le cycle de Calvin : les plantes en C3 le fixent directement, tandis que les plantes en C4 et à photosynthèse CAM fixent le CO2 préalablement sur un autre composé comme adaptation aux températures élevées et aux conditions arides.Chez les procaryotes photosynthétiques, les mécanismes de fixation du carbone sont plus diversifiés. Ce processus peut être réalisé par le cycle de Calvin, mais aussi par un cycle de Krebs inverse ou par carboxylation de l'acétyl-CoA,. Les organismes chimioautotrophes procaryotiques fixent également le carbone du CO2 en utilisant le cycle de Calvin mais avec de l'énergie provenant de l'oxydation de composés inorganiques.Au cours de l'anabolisme des glucides, des acides organiques simples peuvent être convertis en oses tels que le glucose, puis être polymérisés en polysaccharides tels que l'amidon. La biosynthèse du glucose à partir de composés tels que pyruvate, lactate, glycérol, 3-phosphoglycérate et acides aminés est appelée néoglucogenèse. La néoglucogenèse convertit le pyruvate en glucose-6-phosphate en passant par une série de métabolites dont de nombreux sont également des intermédiaires de la glycolyse. Cependant, cette voie métabolique ne doit pas être vue comme la glycolyse prise en sens inverse car plusieurs de ses étapes sont catalysées par des enzymes différentes de la glycolyse. Ce point est important car il permet de réguler la biosynthèse et la dégradation du glucose de façon distincte l'une de l'autre et donc d'empêcher de voir ces deux processus fonctionner en même temps, l'un détruisant l'autre en pure perte,.Bien que les organismes stockent couramment l'énergie sous forme de lipides, les vertébrés tels que les humains ne peuvent convertir les acides gras de leurs graisses en glucose au moyen de la néoglucogenèse car ils ne peuvent pas convertir l'acétyl-CoA en pyruvate : les plantes disposent de l'équipement enzymatique nécessaire pour ce faire, ma"
médecine;"En biologie, la notion de fonction renvoie à plusieurs significations. En biologie moléculaire et en biochimie, la notion de fonction décrit le rôle biologique d'un composant de la cellule : protéine, gène, organite cellulaire, etc. En physiologie, une fonction est un processus ou ensemble d'actions coordonnées concourant au fonctionnement d'un organisme, accomplies par un ou plusieurs organes : la fonction respiratoire, la fonction circulatoire, etc.Une fonction physiologique peut être définie comme ""l'ensemble des actes accomplis par une structure organique défini en vue d'un résultat déterminé"".ÉvolutionPhylogénie Portail de la biologie cellulaire et moléculaire   Portail de la biochimie   Portail de la physiologie"
médecine;"La maladie de Basedow ou Graves-Basedow est une hyperthyroïdie auto-immune (maladie de la thyroïde). La personne atteinte produit des anticorps anormaux (stimulant le recepteur de la TSH) dirigés contre les cellules folliculaires de la thyroïde. Plutôt que de détruire ces cellules, comme le ferait tout anticorps normal, ces anticorps reproduisent les effets de la TSH et stimulent continuellement la libération d'hormones thyroïdiennes, provoquant une hyperthyroïdie accompagnée de signes cliniques spécifiques. La maladie de Basedow ou de Graves, plus fréquente chez la femme que chez l'homme, se manifeste le plus souvent par une accélération du métabolisme basal, une diaphorèse, des pulsations cardiaques rapides et irrégulières, une augmentation de la nervosité et une perte pondérale. Il s'agit de sa forme la plus fréquente.Elle doit son nom à Carl von Basedow.La maladie de Basedow peut toucher tout le monde, mais essentiellement les individus entre 40 et 60 ans et plus rarement à l'adolescence. Elle est cinq à dix fois plus fréquente chez les femmes. Elle est la cause de près des trois quarts des hyperthyroïdies.Il existe un facteur génétique comme l'atteste une atteinte concomitante chez de vrais jumeaux ainsi que la présence d'antécédents familiaux.Comparativement au tabagisme actif persistant, la cessation de consommation de tabac diminue le risque de développer une maladie de Basedow, notamment dans sa forme oculaire, et plus particulièrement chez les femmes.Décrit à de nombreuses reprises, ce syndrome doit son nom à Carl von Basedow mais peut avoir plusieurs dénominations :goitre exophtalmique ;goitre toxique diffus ;hyperthyroïdie auto-immune ;maladie de Graves ;maladie de Graves-Basedow.La maladie est probablement de cause multifactorielle. Il existe une participation génétique et plusieurs gènes sont impliquées : CD40, CTLA4, PTPN22, FCRL3, gènes de la thyroglobuline et au récepteur à la TSH.Le stress peut avoir un rôle provocateur. Le tabagisme est un facteur de risque.La maladie est plus fréquente chez la femme, faisant suspecter une participation génétique et/ou hormonale. D'autres facteurs ont été identifiés : infection à Yersinia enterocolitica, déficit en sélénium ou en vitamine D.L'auto-immunité se développe à partir des anticorps anti-récepteurs de la TSH, dans lequel le corps fabrique des anticorps pour le récepteur de la thyroïde-stimulant hormone (TSH-R). Ces anticorps se lient aux récepteurs TSH qui se trouvent sur les cellules qui produisent des hormones thyroïdiennes, ce qui entraîne une production anormalement élevée de T3 et T4. C'est une hypersensibilité de type V.Ils comprennent l'association de signes d'hyperthyroïdie et de signes plus spécifiques de la maladie.Elle se caractérise par une asthénie, un amaigrissement contrastant avec un appétit conservé voire augmenté, une hypersudation, des attitudes d'évitement de la chaleur. Il peut exister des troubles psychologiques, une agitation, une nervosité, un tremblement, une soif permanente avec augmentation des mictions (polyurie-polydipsie)L'examen clinique montre une fréquence cardiaque accélérée (tachycardie), voire un rythme irrégulier.Les signes et symptômes sont les suivants :goitre (augmentation de volume de la thyroïde) ;exophtalmie (déplacement de l'œil hors de son orbite) ;myxœdème (infiltration cutanée) au niveau des tibias ;gynécomastie, (développement excessif des glandes mammaires chez l'homme) ;augmentation du rythme cardiaque ;augmentation de l'activité métabolique ;plus rarement, hippocratisme digital (déformation du doigt et des ongles).L'hyperthyroïdie est démontrée biologiquement par l'effondrement du taux de TSH dans le sang, couplé à un taux élevé de triiodothyronine (T3) et de thyroxine (T4). En cas de maladie de Basedow, il existe une élévation du taux d'anticorps anti-récepteur de la TSH (« TRAK »), hautement spécifique et sensible, et des TSI (Thyroid stimulating immunoglobulins).La scintigraphie thyroïdienne consiste en la visualisation du captage (fixation) par cet organe d'un composé radioactif. Elle montre typiquement une thyroïde augmentée de volume et hyperfixante. Elle permet de différencier la maladie de Basedow d'autres causes d'hyperthyroïdie, comme un nodule, dit « toxique », par exemple.La vascularisation du goitre est augmentée et peut être démontrée par un doppler de la thyroïde. L'échographie de la glande, couplée au doppler, a des résultats comparables à ceux de la scintigraphie. L'échographie permet par ailleurs de détecter les nodules et distinguer une thyroïdite d'un Basedow (baisse du débit dans le premier cas).L'imagerie peut ainsi mettre en évidence dans près de 35 % des cas de maladie de Basedow des nodules thyroïdiens associés, généralement non fonctionnels. Dans environ 1% des cas, il est retrouvé un ou des nodules fonctionnels en plus de la maladie de Basedow (syndrome de Marine-Lenhart).Insuffisance surrénalienneMyasthénieThyroïdite d'HashimotoDiabète de type 1Polyendocrinopathie auto-immuneSyndrome de MeansUne rémission spontanée se fait dans près d'un tiers des cas. Le risque de rechute à court terme après arrêt des antithyroïdiens de synthèse peut être prédit par la persistance d'un taux élevé d'anticorps anti-récepteur de la TSH. Il reste toutefois élevé, dépassant 50 %.Les trois options sont la prescription de médicaments antithyroïdiens de synthèse (méthimazole et propylthiouracile), l'ablation chirurgicale de la thyroïde et l'emploi d'iode radioactif (iode 131 : irathérapie), détruisant ainsi sélectivement la glande. Ces trois traitements ont une efficacité comparable mais le taux de rechute est plus élevé avec les médicaments (près de 40 %) qu'avec les deux autres méthodes. Le choix de l'une ou l'autre des options dépend d'un certain nombre de paramètres, dont font partie les habitudes locales (les États-Unis recourant de manière beaucoup plus fréquente que l'Europe à l'iode radioactif en première intention).Pour les antithyroïdiens de synthèse, deux techniques de prescription peuvent être utilisés : la titration (ou recherche de la dose minimale efficace) ou la prescription à doses importantes conduisant à une hypothyroïdie et à une substitution hormonale associée. dans tous les cas, un dosage des hormones thyroïdiennes doit être fait au premier mois, puis tous les trois mois si l'hyperthyroïdie est équilibrée. Le traitement est prolongé jusqu'à 18 mois, voire plus.La prise en charge de l'atteinte oculaire a fait l'objet de la publication de recommandations par l'European Group on Graves' Orbitopathy en 2008. L'exophtalmie nécessite une prévention des lésions oculaires, secondaire à une couverture insuffisante des paupières avec un risque de lésion de la cornée. Plus rarement, elle doit être traitée pour elle-même.Louis-Ferdinand Céline en 1933, met au point un produit, La Basedowine, enregistré au Laboratoire National de contrôle des médicaments sous le no 343-4 et commercialisé par les Laboratoires Gallier jusqu'en 1971.Glafira Ziegelmann, première femme interne de Montpellier et la première femme admissible à l'agrégation de médecine, consacre en 1898 sa thèse au traitement de la maladie de Basedow. Portail de la médecine"
médecine;"Le rein est un organe de l'appareil urinaire des vertébrés. Il a de multiples fonctions : hormonales, de régulation de la pression sanguine et d'élimination des toxines. Il assure ainsi, par filtration et excrétion d'urine, l'équilibre hydroélectrolytique (homéostasie) du sang et de l'organisme en général. Ses fonctions hormonales comprennent la synthèse de l'érythropoïétine, du calcitriol (forme active de la vitamine D) et de la rénine.Chez les amniotes, il est le plus souvent pair et situé dans l'abdomen, dans le rétropéritoine, suivant une symétrie plus ou moins bilatérale. Il est de taille et de conformation très variable en fonction des espèces : lisses chez les humains, lobulés chez les ruminants, diffus chez les oiseaux...Par abus de langage, le langage courant nomme souvent « reins » la zone des vertèbres lombaires (ex.: « tour de reins » pour parler de lombalgie). Les reins de certains animaux de production sont consommés par les humains, sous le nom de rognons.La fonction complexe de cet organe vital a peut-être suscité l'ambivalence de sa réputation : objet de culte (il est cité 25 fois dans la Bible où il est considéré, selon l'art divinatoire tiré de l'examen hiéroscopique des reins de l'animal sacrifié, comme le siège de la sagesse, de l'intelligence et des émotions, notamment dans le Livre des Psaumes), il a également longtemps été méprisé car jugé peu noble (assimilé à une passoire qui filtre les déchets pour constituer les urines).Chez l'être humain, les reins sont des organes aplatis, ovoïdes, dits « en haricot ». La face externe est convexe ; la face interne est concave, et accueille le hile qui se projette au niveau de la 1re vertèbre lombaire : il constitue la zone de transit des éléments vasculo-nerveux et des voies excrétrices urinaires. La surface des reins est lisse chez l'adulte, de couleur rouge-brun. En moyenne, ils ont pour hauteur 12 cm, largeur 6 cm, épaisseur 3 cm et chacun pèse environ 150 g. Ces mensurations sont très variables d'un individu à l'autre.Les reins se situent dans l'espace rétropéritonéal, où ils se projettent par leur face postérieure dans la région lombaire. Celle-ci constitue d'ailleurs la principale voie d'abord chirurgical du rein.Le rein gauche est placé entre la 11e vertèbre thoracique et la 3e lombaire. Le rein droit quant à lui se projette entre la 12e vertèbre thoracique et l'espace entre la 3e et la 4e lombaire. Ce décalage est dû à la pression du foie sus-jacent sur le rein droit. Ils s'orientent :dans le plan frontal, selon un angle d'environ 18° avec l'axe médian (orientés vers le bas et en dehors) ;dans le plan transversal, selon un angle de 40 à 60° avec l'axe sagittal (orientés vers l'avant et en dedans)Un seul rein suffit pour vivre ; 5 % des individus n'ont qu'un rein, mais dans ce cas il s'agit le plus souvent du rein droit, mieux vascularisé et grâce à la présence du quadrilatère de Rogie qui favorise la stase veineuse et a des répercussions au niveau génital gauche.Le rein est vascularisé par les artères et veines rénales et c'est par une échancrure dans la face concave que ces vaisseaux pénètrent dans le rein (hile du rein).Les artères rénales sont deux artères droite et gauche qui naissent de l’aorte abdominale au niveau de L1. L’artère rénale gauche est plus courte que la droite. Chaque artère rénale donne deux branches terminales : une branche antérieure et une branche postérieure. Les artères et les veines présentent les subdivisions suivantes, jusqu'au glomérule :  Les veines rénales croisent en avant les artères rénales et se jettent dans la veine cave inférieure au niveau de L2. La veine rénale gauche est plus longue et de gros calibre. Le parenchyme rénal est entouré d'une capsule dure, très résistante qui le protège. La partie périphérique du parenchyme est le cortex alors que la partie centrale est la médulla. Cette médulla n'est pas continue : elle est interrompue par des prolongements du cortex qui vont jusqu'au sinus rénal.Le rein est innervé par le plexus rénal qui accompagne et entoure l'artère rénale. Il est innervé par le système nerveux sympathique et parasympathique. L'innervation parasympathique est assurée par le nerf vague (X).L'innervation sympathique émerge des segments de la moelle spinale T10 à L1. Les fibres pré-synaptiques vont se réunir pour former les nerfs splanchniques, ceux-ci font synapse principalement dans le ganglion aortico-rénal. De là partent les fibres post-synaptiques qui vont innerver le rein. Accessoirement on peut retrouver une innervation du 1er nerf splanchnique lombaire et l'implication des ganglions mésentérique supérieur et rénal.Pour plus de détails sur l'innervation orthosympathique des viscères de l'abdomen, consulter l'article concernant les plexus prévertébraux.Le rein a aussi une fonction endocrine (érythropoïétine, système rénine-angiotensine-aldostérone, calcitriol). En raison de caractéristiques génétiques ou liées aux traits de vie, la capacité des reins varie significativement selon les individus et selon l'âge. Elle est médiocre chez le nouveau-né et décline chez l'adulte avec l'âge. Les capacités fonctionnelles du rein peuvent être dégradées par diverses maladies et par l'exposition à certains toxiques (fluor, plomb, cadmium, autres métaux lourds, alcool ou excès de sodium…). En cas de déficience grave, les derniers recours sont la filtration externe du sang dans un rein artificiel (dialyse), ou la greffe de rein.De l'extérieur vers l'intérieur :Elle comporte les glomérules, les tubes contournés proximaux et distaux et les tubes collecteurs.Les colonnes de Bertin, dans les espaces entre les pyramides de Malpighi.Pyramides rénales ou de Malpighi ; dont la base est sous-corticale et la pointe tournée vers l'intérieur, forment les papilles sur lesquelles viennent se ventouser les petits calices. Elles comportent les tubes droits proximaux et distaux ainsi que l'anse de Henle et les canaux de Bellini.Une pyramide et ses colonnes forment un lobe du rein.Les néphrons qui se déversent dans le même canal collecteur forment collectivement un lobule du rein.Les petits calices recueillent l'urine émise par les pyramides de Malpighi. L'union des petits calices forment les grands calices, il y a trois ou quatre grands calices par rein. Tube abouché à la pointe de la pyramide rénale, et qui en se rejoignant forment le bassinet.Tube en forme d'entonnoir qui se jette dans l'uretère. Il est également appelé pyélon.C'est l'endroit où va passer l'urine à sa sortie du néphron via le tube collecteur. Les bassinets tout comme les calices possèdent un tissu musculaire lisse qui se contracte et propulse l'urine par péristaltisme.Le rein est issu de la métamérisation (segmentation puis formation de tubules) du mésoblaste intermédiaire (tissu du disque embryonnaire) en cordon néphrogène au cours de la 3e semaine de développement. Ce cordon se divise en trois régions distinctes dans le temps et l'espace (selon un axe céphalo-caudal) qui vont évoluer successivement:- le cordon pronéphrogène (le plus céphalique) qui se métamérise en pronéphros. Ce premier rein ne fonctionne pas et dégénère à la fin de la 4e semaine de développement ;- le cordon mésonéphrogène qui se métamérise après la dégénérescence du pronéphros en mésonéphros. Ce rein fonctionne dès la fin de la 4e semaine de développement, mais il dégénérera également ;- le cordon métanéphrogène (le plus caudal) qui donne le métanéphros, rein fonctionnel, qui est le rein définitif. Il a la particularité de ne pas se métamériser.Voir aussi le paragraphe sur l'embryologie du néphron.Le néphron est l'unité structurelle et fonctionnelle de base du rein. C'est un tubule mince consistant en un amas de capillaires appelés glomérules, entourés d'un bulbe creux, la capsule glomérulaire. La capsule glomérulaire amène à un long tubule entortillé en deux sections : le tubule contourné proximal, l'anse du néphron, le tubule contourné distal, et le tubule rénal collecteur. Les tubules collecteurs se déversent dans les calices via les papilles, les calices se jettent dans le pelvis rénal (appelé également pyélon ou bassin), qui est connecté à l'uretère. Chaque rein humain compte environ un million de néphrons. Le nombre de néphrons, fixé à la naissance, est d'une grande variabilité. Il dépend de multiples facteurs dont l'âge gestationnel, le retard de croissance intra-utérin, l'état nutritionnel maternel.Le rôle essentiel et le plus connu des reins est la formation de l'urine. Ils éliminent du sang les déchets provenant de la destruction des cellules de l'organisme et de la disgestion des aliments .La formation de l'urine et le rejet de celle-ci, comprennent quelques étapes:L'artère rénale apporte le sang au rein - Les artères rénales droite et gauche nées de l'aorte apportent une grande quantité de sang aux reins, environ 1700 litres par jour,, soit un cinquième du débit cardiaque. Elles se divisent en de nombreuses branches pour aboutir à des artérioles microscopiques qui vont alimenter les néphrons;Le néphron filtre le sang et produit l'urine - Chaque rein est constitué d'un million de minuscules canaux juxtaposés appelés néphrons. Chaque néphron comprend un glomérule et un tubule. Le glomérule est un filtre très fin qui retient les globules rouges et les grosses molécules (protéines) mais laisse passer l'eau, les électrolyses (sodium, potassium, calcium...) et les petites molécules (glucose, urée, acide urique, créatinine...). Il en résulte une urine primitive qui va subir des transformations à l'intérieur du tubule. Certaines substances y sont évacuées, d'autres sont réabsorbées, aboutissant à l'urine définitive qui va s'écouler dans les tubes collecteurs;L'urine atteint le bassinet, sorte d'entonnoir - Les tubes collecteurs déversent l'urine dans 8 à 10 calices qui se vident dans le bassinet, sorte d'entonnoir dans lequel s'abouche l'uretère;L'urine est déversée dans deux conduits: les uretères - Les uretères sont des tuyaux de 2,5 mm de diamètre et de 30 cm de long qui, partant du bassinet, vont amener l'urine à la vessie;La vessie stocke puis évacue l'urine par l'urètre - La vessie est un réservoir qui peut contenir jusqu'à 800 ml d'urine. Elle se remplit progressivement et se vide, par un mécanisme déclenché volontairement, laissant échapper l'urine par l'urètre: c'est la miction.Hormis sa fonction principale de filtration et d'épuration du sang, le rein intervient à bien des niveaux, notamment dans la régulation de la pression artérielle. Par sa fonction de synthèse de substances spécifiques régulatrices, notamment :la rénine synthétisée par le rein et qui va provoquer, via l'angiotensine II (ATII), une stimulation de la sécrétion d'aldostérone, qui est une hormone qui va en cas de baisse de pression artérielle, stimuler la réabsorption de sodium ; or les mouvements d'eau suivent les mouvements de sodium, donc cela va entrainer une réabsorption accrue d'eau qui va faire augmenter la volémie au niveau plasmatique et ainsi faire augmenter la pression dans le sang. Ce n'est qu'un aspect schématique et non exhaustif de la rénine car elle a comme effet également de stimuler la sécrétion de noradrénaline, toujours via l'angiotensine II et ainsi provoquer une vasoconstriction ;En cas d'hypertension artérielle (HTA), le rein va synthétiser de la kallikréine pour donner en fin de réaction de la bradykinine (qui est une kinine) qui a des effets vasodilatateurs, donc de réduire la pression au niveau des vaisseaux.Ceci explique pourquoi l'apport excessif de sel fait augmenter la pression artérielle : les mouvements de sodium dans le rein se font également passivement, donc si on augmente notre apport en sodium (sel), cela entrainera une réabsorption accrue d'eau également provoquant une augmentation de volémie donc de pression car : PA = DC × Rp DC = FC × VES PA = pression artérielleDC = débit cardiaqueRp = résistance périphérique FC = fréquence cardiaqueVES = volume d'éjection systolique (volume éjecté par le ventricule cardiaque gauche à chaque contraction).On sait que les entrées et sorties en sodium sont équivalentes, la quantité absorbée est éliminée au début mais si l'apport en sel n'est plus ponctuel mais continu, alors une autre limite est fixée et l'élimination se fait moins bien ; ceci augmente avec l'âge.Un rein adulte reçoit normalement le quart du débit cardiaque à chaque minute. Son rein est irrigué en moyenne chaque jour par plus de 1 700 litres de sang (toutes les quatre minutes, la totalité du sang de l'organisme, soit près de 6 litres, est filtrée en traversant cet organe), soit environ 900 litres de plasma sanguin. Sur ces 900 litres de plasma, 20 % sont filtrés au niveau des glomérules rénaux pour former 180 litres d'urine primitive qui subit par les différents segments du tubule rénal des modifications, essentiellement des phénomènes de réabsorption (plus de 99 % de l'eau et des sels filtrés sont réabsorbés), aboutissant à la production d'1 à 2 litres d'urines définitives. La diurèse quotidienne normale est de 1 à 1,5 L dépendant des apports hydriques.[pas clair]Lors du sommeil, le taux d'ADH sécrété par l'hypophyse augmente, ce qui a pour effet d'augmenter la réabsorption d'eau par le rein, donc de diminuer la quantité d'urine excrétée.Le débit de filtration glomérulaire normal est de 120 à 130 mL/min (un débit anormal sert à diagnostiquer l'insuffisance rénale chronique) soit les 180 litres d'urine primitive quotidienne.L'insuffisance rénale chronique (IRC) semble en augmentation dans les pays riches, probablement secondairement à l'augmentation des cas de diabète (diabète sucré) ;l'augmentation des cas d'hypertension artérielle ;des néphropathies vasculaires liées au vieillissement de la population ;un régime trop riche en sel conduisant à une hypertension artérielle qui peut être fatale au rein ;l'alcoolisme ;l'obésité ;une exposition excessive à certains produits néphrotoxiques (toxiques rénaux) : plomb, cadmium en particulier ou médicaments tels que phénacétine ou ciclosporine… éventuellement exacerbée par des produits chélateurs très présents dans notre environnement tels que le glyphosate.Elles peuvent être dues à des anomalies génétiques ou à une malformation survenue lors du développement (polykystose rénale le plus souvent, ou reflux vésico-urétéral chez l'enfant), des infections (fréquemment à la suite d'angines, d'infection urinaire, de tuberculose dans les pays en développement) ou à des intoxications ou séquelles d'intoxications. Certains cancers du rein pourraient être précocement induits par des perturbateurs endocriniens. La défaillance du rein peut apparaître brutalement 10 à 40 ans après le début de l'affection.Malformation congénitale : l'exposition prénatale à l'alcool peut provoquer une diminution de la quantité de néphrons, des reins en fer à cheval. On classe généralement les maladies rénales en :maladies glomérulaires (glomérulonéphrites primitives dont la cause initiale n'est pas comprise, ou maladies glomérulaires connues telles que le diabète sucré ou lupus érythémateux, ou couramment dans les pays pauvres les amyloses) ;néphropathies interstitielles (infection urinaire à pyélonéphrite, intoxication par des toxiques tels que cadmium ou plomb). Chez la femme, une cystite aggravée est la cause première. Chez l'homme, un cancer ou un grossissement de la prostate freinant l'écoulement de l'urine peut faciliter une infection conduisant à une néphropathie interstitielle ;néphropathies vasculaires ; le cas le plus fréquent étant une néphroangiosclérose liée à une hypertension artérielle (près de 10 % de la population dans les pays riches).NéphrectomieSonde urinaire en double JUn rein artificiel (ou générateur de dialyse) est un dispositif médical permettant d'épurer le sang des patients dont les reins ne fonctionnent plus.La première réalisée en France eut lieu à Paris sur le jeune Marius Renard en 1952, par l'équipe du docteur Louis Michon à l'Hôpital Necker ; les suites néphrologiques ont été assurées par le professeur Jean Hamburger et Gabriel Richet, mais le jeune homme est rapidement décédé. La méthode aujourd'hui[Quand ?] utilisée est la méthode « de Kuss » (1913-2006).Le 23 décembre 1954, le chirurgien américain Joseph Murray réalise la première transplantation rénale réussie au monde, en la pratiquant sur des jumeaux monozygotes, les frères Ronald et Richard Herrick (en) au Peter Bent Brigham Hospital (en).Il se réalise environ 3 000 greffes de reins par an en France.Le bicarbonate de sodium s'avère efficace pour ralentir la progression de maladie rénale chronique - études ayant exclu les personnes souffrant d'obésité morbide associée, de troubles cognitifs, de septicémie chronique, d'insuffisance cardiaque manifeste ou d'hypertension non contrôlée,,.Le système excréteur chez les autres animaux est constitué d'organes excréteurs et des canaux excréteurs associés :organes excréteurs non spécialisés (élimination de différents types de solutés sous forme d'urine) : organes néphridiens chez les invertébrés, tubes de Malpighi chez les arthropodes, organe de Bojanus (en) des mollusques, rein des vertébrés ;organes excréteurs spécialisés (élimination d'un type de soluté), par exemple les cellules à chlorures des branchies des téléostéens, les glandes à sel des oiseaux.Reins de plusieurs animaux obtenus par la technique d'injection de vinyle et corrosion :                        Ressources relatives à la santé : FMA TA2 Uberon Xenopus Anatomy Ontology (en) Medical Subject Headings (en) NCI Thesaurus (no + nn + nb) Store medisinske leksikon (la + en) TA98 Renaloo, communauté de patients et de proches concernés par l'insuffisance rénale, la dialyse, la greffe. Portail de l’anatomie   Portail de la médecine   Portail de la physiologie"
médecine;"L'hyperthyroïdie (appelée aussi dans des cas très prononcés — graves et rares — thyréotoxicose ou thyrotoxicose) est le syndrome clinique causé par un excès de thyroxine libre circulante (FT4) ou de triïodothyronine libre (FT3), ou les deux. Chez les humains, les causes principales sont la maladie de Basedow (cause la plus fréquente : 70-80 % des cas), l'adénome toxique de la thyroïde, le goitre multinodulaire toxique, et la thyroïdite sub-aiguë.La glande thyroïde, stimulée par la TSH (thyroid-stimulating hormone), secrète deux hormones, la thyroxine (= tétraiodothyronine) ou T4 et la triiodothyronine (T3). La première est une prohormone, transformée en la seconde qui constitue la forme active.L'hyperthyroïdie consiste en l'augmentation des taux de T3 et de T4 dans le sang. Si cette hypersecrétion est secondaire à une maladie de la thyroïde (ce qui est vrai dans la quasi-totalité des cas), la TSH est effondrée (par rétrocontrôle)L'incidence annuelle est de 0,6 pour 1 000 femmes. Elle est quatre fois moindre chez les hommes. La prévalence aux États-Unis est de 1,3 %.La cause la plus fréquente chez le sujet jeune est la maladie de Basedow et chez le sujet âgé, le nodule toxique ou le goitre multinodulaire, surtout si l'apport iodé de la nourriture est pauvre. Les thyroïdites, entraînant le relargage d'hormones thyroïdiennes à la suite de la destruction cellulaire, comptent pour 10 % des hyperthyroïdies. Les autres causes sont rares.C'est la première cause d'hyperthyroïdie en termes de fréquence. Elle est plus fréquente chez la femme jeune. On retrouve de manière non constante un souffle à l'auscultation de la glande thyroïde qui est augmenté de volume, un discret gonflement des parties molles de la jambe (myxœdème prétibial) ou des globes oculaires légèrement proéminents (exophtalmie). Le diagnostic est fait en présence de TSI (Thyroid stimulating immunoglobulins) dans le sang des patients. La structure de cette TSI est proche de celle de la TSH et stimule ainsi la production d'hormones thyroïdiennes par la glande.Le nodule toxique de Plummer est évoqué devant le nodule isolé de la glande thyroïde qui peut parfois être palpé et surtout, par la fixation d'iode radioactif de ce dernier de manière exclusive à la scintigraphie thyroïdienne, le reste de la glande n'étant plus visualisé. Il devient une cause importante d'hyperthyroïdie chez la personne âgée. Son traitement demande l'éradication du nodule, que cela soit par chirurgie ou par iode radioactif.Elle peut être :infectieuse (thyroïdite de De Quervain dans un contexte grippal) ou post opératoire ;auto-immune comme lors de la thyroïdite de Hashimoto avec la présence d'anticorps anti-TPO ;survenir après un accouchement (assez fréquente puisqu'elle concerne jusqu'à 10 % des parturientes, le plus souvent très discrète et guérissant sans séquelle).Elle évolue parfois vers une hypothyroïdie (diminution des hormones thyroïdiennes) régressive.La scintigraphie montre alors l'absence totale de fixation de l'iode radioactif (scintigraphie blanche).Parmi les autres causes possible, on distingue :le goitre multinodulaire : le goitre est révélé à l'examen clinique de la glande, il peut être suffisamment important pour causer des compressions des structures adjacentes. La fonctionnalité des nodules est affirmée par la scintigraphie thyroïdienne. Le traitement est essentiellement chirurgical : l'utilisation d'iode radioactif peut faire disparaître l'hyperthyroïdie clinique mais ne parvient pas, en règle générale, à faire diminuer le goitre ;l'association d'une maladie de Basedow et de nodules fonctionnels (syndrome de Marine-Lenhart) ;le cancer de la thyroïde évolué ;l'adénome hypophysaire à TSH ;la prise d'hormone thyroïdienne en quantité trop élevée ;effet secondaire de la prise de certains médicaments, surtout du fait de leur richesse en iode dans le principe actif ou les excipients : antiseptiques contenant de l'iode (polyvidone), produits de contraste de radiologie, etc. L'amiodarone peut donner également des hypothyroïdies. L'hyperthyroïdie de l'amiodarone est plus fréquente dans les régions avec apports iodés insuffisants. Elle impose l'arrêt de ce médicament lorsque c'est possible, en sachant que sa demi-vie prolongée (plus de 100 jours) fait que l'imprégnation en médicament va persister très  longtemps.La plupart des signes restent non spécifiques ou peuvent être discrets. La sévérité des signes est corrélée avec les taux hormonaux. Ils sont toutefois plus frustes chez la personne âgée.L'hyperthyroïdie peut se manifester par tout ou partie des signes ci-dessous.Une perte de poids malgré un appétit conservé ou accru (polyphagie).Une prise de poids dans environ 10 % des cas.Une chaleur ressentie comme insupportable (thermophobie).Une polydipsie, soif excessive.Une asthénie, fatigue, à l'instar de l'hypothyroïdie, pouvant avoir comme conséquence des troubles de l'érection dans la moitié des cas, chez l'homme, réversible sous traitement.Une fréquence cardiaque élevée (tachycardie) avec des palpitations ou des extrasystoles auriculaires.Un essoufflement (dyspnée) ;Un pouls irrégulier pouvant correspondre à une fibrillation auriculaire, cette dernière pouvant être présente même en cas d'hyperthyroïdie dite sub-clinique.Des tremblements fins des extrémités, conséquence de l'excès de circulation sanguine rapide du sang (Attention, ce tremblement n'est pas d'origine neurologique !).Le tout peut se compliquer soit :d'une insuffisance cardiaque typiquement à haut débit, régressive le plus souvent après normalisation des hormones thyroïdiennes mais pouvant aboutir à des séquelles dans un tiers des cas ;de douleurs thoraciques pouvant évoquer une angine de poitrine.Diarrhée chronique.Nausées ou vomissements.Il existe une diminution de la force musculaire (myopathie endocrinienne) avec parfois diminution de la taille des muscles (atrophie musculaire).La maladie peut se présenter sous forme de dépression ou irritabilité.Dans les formes graves, l'hyperthyroïdie peut entraîner un coma, des mouvements anormaux sous forme de chorée, des troubles du comportement pouvant ressembler à une psychose.Peau luisante, chaude et humide.Démangeaison isolée.Plusieurs symptômes sont décrits :impuissance ;augmentation de la taille des seins (gynécomastie) ;infertilité ;absence totale ou partielle de menstruations.L'hyperthyroïdie, même modérée (dite sub-clinique) peut se compliquer d'une décalcification osseuse (ostéoporose secondaire).Certaines thyrotoxicoses peuvent ainsi faciliter un saturnisme inattendu, via une contamination de l'organisme par relargage du plomb antérieurement stocké dans les os. Dans ce dernier cas, l'augmentation de la plombémie est accompagnée d'une augmentation du taux sérique d'ostéocalcine qui reflète l'augmentation du remodelage osseux qui accompagne souvent l'hyperthyroïdie.Inversement ou en retour le plomb pourrait affecter la thyroïde en inhibant la captation d'iode, phénomène d'abord observé chez l'animal puis confirmé chez l'Homme dans les années 1960,, y compris dans un cas d'intoxication saturnine liée à la présence d'une balle en plomb non extraite de l'organisme.On observe une élévation de l'hormone TSH ou une chute de la thyroxine sérique et libre, en cas d'exposition chronique et plutôt quand la plombémie dépasse 60 µg/100 mL.Diminution de la concentration de cholestérol sanguin (hypocholestérolémie).Anémie (diminution de la concentration d'hémoglobine dans le sang).Neutropénie (diminution du nombre de polynucléaire neutrophiles sanguin).Selon la cause de l'hyperthyroïdie on observe un goitre, un nodule thyroïdien, une hypertrophie thyroïdienne...Le diagnostic est établi par un examen sanguin : mesure du taux de TSH dans le sang. Un taux effondré de TSH est spécifique d'une hyperthyroïdie périphérique (l'immense majorité des hyperthyroidies, secondaire à une atteinte de la thyroïde). Le diagnostic est confirmé par une mesure du taux de T3 libre et T4 libre sanguin que l'on retrouve augmenté. L'augmentation de ces deux hormones peut cependant être dissociée avec des cas rares d'hyperthyroïdie à T3, la T4 étant normale. Si la TSH est basse et la T4 et T3 sont normales sur des dosages répétées, on parle d'« hyperthyroïdie infraclinique ».Une fois le diagnostic fait, il reste à rechercher la cause. Il est indispensable de doser les anticorps spécifiques (Anticorps anti-récepteur de la TSH, anti-thyroglobuline, anti-thyropéroxydase « anti-TPO ») et de réaliser un examen d'imagerie de la thyroïde : échographie (par ultrasons) ou  scintigraphie (par injection d'un isotope radioactif qui se fixe sur la glande thyroïde et dont le rayonnement est détecté par une caméra à scintillations). Ces examens précisent l'aspect de la glande et la répartition géographique de son activité (fixation à la scintigraphie).La prise en charge de l'hyperthyroïdie a fait l'objet de la publication de recommandations par l'American Thyroid Association en 2011.Le choix du traitement dépend de la cause, de la sévérité et du terrain.Un traitement d'urgence est l'ingestion de solution saturée d'iodure de potassium (SSKI), un fort taux d'ion iodure permettant de stopper temporairement la sécrétion de thyroxine par la thyroïde.Elle consiste en l'ablation de la totalité ou d'une grande partie de la glande thyroïdienne. La chirurgie se doit de respecter les glandes parathyroïdes de petite taille et situées en arrière de la thyroïde. Elle doit également passer en dehors du nerf récurrent qui remonte en arrière de la glande. La section de ce nerf peut entraîner un changement de la voix (dysphonie) du fait qu'il innerve les cordes vocales.Dans le cas de l'hyperthyroïdie, une radiothérapie métabolique peut être prescrite. Il s'agit de l'ingestion d'iode 131 radioactif qui va se fixer sur la glande thyroïde et la détruire. Ce traitement n'est proposé qu'à certaines formes de maladie de Basedow et est naturellement inefficace en cas de non fixation de l'iode sur la glande (scintigraphie blanche). Elle expose à un risque d'hypothyroïdie (comme la chirurgie par ailleurs) qui est facilement traitée par la prise d'hormones thyroïdiennes.Il s'agit de  médicament inhibant la production d'hormones thyroïdiennes, comme le méthimazole (alias thiamazole), le carbimazole ou le propylthiouracile. Le délai d'efficacité peut être long. Deux techniques de prescription peuvent être utilisés : la titration (ou recherche de la dose minimale efficace) ou la prescription à doses importantes conduisant à une hypothyroïdie et à une substitution hormonale associée.En cas de douleurs, il est possible de donner un antalgique et un antipyrétique en cas de fièvre.Les bêta-bloquants ralentissent le cœur et diminuent les palpitations ainsi que les tremblements.Elle est définie par un taux bas de TSH et un taux normal de T4 libre et de T3 totale. Plus de la moitié des hyperthyroïdies sont sub cliniques et leur prévalence serait de 0,7 % aux États-Unis.Ce syndrome est associé avec un risque majoré d'ostéoporose chez la femme âgée mais aussi de maladies cardiovasculaires, de mortalité cardiaque et de fibrillation auriculaire.Un traitement systématique d'emblée n'est pas recommandé : une surveillance régulière du taux des hormones doit être faite et le traitement débuté à l'élévation de ces dernières.C'est l'une des maladies hormonales les plus fréquentes chez le chat, souvent provoquée par une tumeur bénigne (non cancéreuse) de la thyroïde. Cette maladie a été décrite pour la première fois dans les années 1970. Portail de la médecine"
médecine;Une injection intraveineuse (IV) est une injection d'un liquide dans une veine en général à l'aide d'une seringue et d'une aiguille. Elle est notamment utilisée lorsque l'effet de la substance administrée doit être rapide : il peut s'agir d'un médicament, d'un analgésique, d'un marqueur en imagerie médicale ou d'une drogue.  Portail de la médecine   Portail des soins infirmiers
médecine;"Un médicament est toute substance ou composition présentée comme possédant des propriétés curatives ou préventives à l'égard des maladies humaines ou animales. Par extension, un médicament comprend toute substance ou composition pouvant être utilisée chez l'être humain ou l'animal ou pouvant leur être administrée, en vue d'établir un diagnostic médical ou de restaurer, corriger ou modifier leurs fonctions physiologiques en exerçant une action pharmacologique, immunologique ou métabolique.L'ensemble de la chaîne des médicaments (recherche, production, contrôle qualité, distribution en gros, délivrance aux patients, pharmacovigilance) est sous la responsabilité de spécialistes diplômés des médicaments, les pharmaciens.La notion de médicament est précisément définie en France par l'article L5111-1 du Code de la santé publique.On peut distinguer différents types de médicaments selon leur utilisation, leurs composants, leur mode d'enregistrement réglementaire, etc. :médicament générique ;médicament biosimilaire ;médicament orphelin ;médicament biologique ;médicament à base de plantes ;médicament essentiel ;médicament stupéfiant.Posologie : c'est la dose usuelle du médicament utilisé. Elle dépend de la maladie, de l'âge du patient, de son poids et de certains facteurs propres : fonction rénale, fonction hépatique. Elle ne doit naturellement être en aucun cas modifiée sans un avis médical ou éventuellement du pharmacien.Pharmacocinétique : c'est la vitesse à laquelle la substance active du médicament va être absorbée, distribuée dans l'organisme, métabolisée (transformée), puis éliminée de l'organisme. Elle conditionne la méthode de prise : orale (par la bouche), intraveineuse ou autre, mais aussi le nombre quotidien de prises, leur horaire, la dose journalière. Schématiquement, la pharmacocinétique est l'étude de l'action de l'organisme sur le médicament.Pharmacodynamique : c'est le mode d'action de la substance active qui va entraîner les effets thérapeutiques. Schématiquement, la pharmacodynamie est l'étude de l'action du médicament sur l'organisme.Indication : c'est une maladie ou une situation pour laquelle un médicament est utilisé.Contre-indication : c'est la ou les situations où la prise du médicament peut se révéler dangereuse. Ce dernier ne doit, par conséquent, pas être donné. On distingue les contre-indications relatives où dans certains cas, le rapport bénéfice-risque de la prise de la molécule reste acceptable, et les contre-indications absolues où le médicament ne doit pas être pris, quel que soit le bénéfice escompté.Association déconseillée : à éviter, sauf après évaluation du rapport bénéfice/risque ; nécessité d'une surveillance étroite.Précaution d'emploi : c'est le cas le plus fréquent ; association possible en respectant les recommandations.A prendre en compte : signalement du risque ; au praticien d'évaluer l'opportunité de l'association ; pas de conduite spécifique à tenir.Synergie : cela correspond à l'interaction entre deux médicaments présentant une activité pharmaceutique identique. L'intensité de l'activité de l'association est supérieure à celle que l'on pourrait obtenir avec l'un des médicaments administré seul.Potentialisation : elle s'exerce entre deux médicaments dont l'activité pharmaceutique est différente.Antagonisme : il s'agit d'une interaction entre deux médicaments dont l'activité pharmaceutique est identique ou différente. L'administration simultanée de deux médicaments entraîne l'inhibition partielle ou complète de l'action de l'un d'entre eux.Un médicament peut avoir une ou plusieurs actions, décrites comme :Action substitutive : consiste à apporter à l'organisme l'élément nutritif ou physiologique déficient (par exemple : méthadone ou vitamine C).Action par reproduction directe ou indirecte des effets d'une substance naturelle : le médicament reproduit ou stimule une fonction cellulaire ou organique, ou encore la transmission d'un influx nerveux au niveau du SNC (système nerveux central) ou autonome (par exemple : sympathomimétique ou parasympathomimétique).Action par antagonisme direct ou indirect des effets d'une substance naturelle : le médicament exerce un blocage partiel ou complet d'une fonction cellulaire ou organique en fixant sur des récepteurs spécifiques (par exemple : sympatholytique).Action mécanique (par exemple : huile de paraffine favorisant le transit digestif).Action sur certains processus métaboliques : action sur la perméabilité cellulaire ou la réactivité de certaines cellules à leur excitant physiologique ou pathologique (par exemple : médicament anticalcique (modifiant la perméabilité des ions calcium)).Le médicament est composé de deux sortes de substances : d'une ou plusieurs substances actives (aussi désigné principe actif — c'est souvent la substance active qui est désignée dans le langage courant par médicament) et d'un ou plusieurs excipients.La ou les substances actives sont constituées d'une quantité de produit active (dose) ayant un effet pharmacologique démontré et un intérêt thérapeutique également démontré cliniquement. Il est à remarquer que toute substance pharmacologiquement active ne constitue pas nécessairement la base d'un médicament et encore moins d'une thérapie médicamenteuse.Les excipients sont des substances auxiliaires inertes servant à la formulation de la forme galénique ou destinée à créer une absorption par le corps. Ces excipients sont le plus souvent des substances inertes sur le plan pharmacologique. Les excipients permettent de formuler la ou les substances actives, c’est-à-dire de présenter la substance active sous une forme galénique déterminée. La formulation permet en plus de présenter le médicament sous la forme la plus adaptée pour la voie d'administration souhaitée et éventuellement, le cas échéant, de moduler la vitesse de libération de la substance active vers l'organisme. Comme exemple d'excipients on citera : l'eau et le saccharose sont les deux excipients constituant le sirop simple — ou encore, pour des formes sèches, le ou les amidons modifiés et la ou les celluloses modifiées sont des agents de délitement utilisés dans des formes sèches (comprimés, gélules, etc.) pour accélérer la désintégration (ou encore délitage) de celles-ci une fois arrivées dans l'estomac. Les excipients sont dans leur très grande majorité, des substances chimiquement inertes et pharmacologiquement inactives. Mais il s'avère qu'ils ne sont pas toujours exempts d'effets pharmacologiques sur certains patients. En effet, certains excipients sont connus pour être à l'origine d'effets secondaires (e.g. réactions allergiques ou d'intolérance) chez une minorité de patients particulièrement sensibles. On parle alors d'excipient à effet notoire. On citera en exemple le lactose chez des patients intolérants au lactose. Le prescripteur ou le pharmacien devra en tenir compte lors de la prescription et de la dispensation du médicament. Ceci est très important notamment lors de la substitution d'un produit princeps par une forme générique du produit original. Le produit générique n'étant pas nécessairement formulé avec les mêmes excipients que le produit princeps d'origine. Ceci est une des raisons pour lesquelles un patient peut ne pas tolérer les produits génériques de substitution.Il est à remarquer qu'une substance active peut être par exemple un produit de contraste (sulfate de baryum) qui n'est pas pharmacologiquement actif car il n'est pas destiné à traiter le patient mais à aider à poser le diagnostic (il est actif sur le rayonnement auquel sera exposé le patient).La galénique (de Galien, médecin de l'Antiquité) ou « art de formuler les médicaments », va permettre de présenter la substance active à des doses différentes et sous différentes formes galéniques (les formes d'administration de la substance active au patient). On parlera de comprimés, de gélules, de capsules molles, de suppositoires, d'ampoules, de gouttes (orales, oculaires ou nasales), de collutoires, de collyres, de pommades, de gels et crèmes, de solutions, d'ovules, d'emplâtre ou de dispositifs transdermiques, etc. On peut ainsi classer les formes galéniques selon la voie d'administration aux patients pour laquelle elles ont été conçues. On parlera alors d'injectables (ampoules de solution ou de suspension, implants…) destinées aux différentes voies parentérales (sous-cutanées, intraveineuse, intramusculaires, intra-articulaires…). Ces formes doivent être stériles, apyrogènes et, parfois, isotoniques. Les autres formes liquides non injectables sont destinées aux voies orales (à avaler per os ou sublinguales, à enrobage entérique ou à désintégration rapide), nasales, auriculaires et oculaires, dermiques mais aussi transdermiques (timbre ou patch). Il existe encore des formes pour la voie, rectale, oculaire, auriculaire, etc.Une spécialité pharmaceutique est un médicament qui a un nom commercial (qui fait l'objet d'une propriété commerciale, nom commercial dit aussi nom de fantaisie). Chaque spécialité fait l'objet d'un enregistrement auprès des autorités de santé, qui est préparé industriellement selon des normes très strictes (les bonnes pratiques de fabrication) et est vendu par un laboratoire pharmaceutique. Sous son même nom de marque, il existe différentes formes pharmaceutiques et différents conditionnements, chacun faisant l'objet d'un enregistrement spécifique. Une même spécialité pourra être commercialisée éventuellement sous un ou plusieurs noms de marque et restera protégée tant qu'elle fera l'objet d'une propriété intellectuelle et d'une protection des droits intellectuels et/ou commerciaux (brevet, exclusivité commerciale, licence). Une fois la propriété intellectuelle perdue (épuisement des droits du ou des brevets), le médicament peut être commercialisé sous des formes dites génériques (en plus des formes commerciales existantes). Les formes génériques devant être bioéquivalentes au premier produit de marque mis sur le marché appelé encore produit « princeps » ou spécialité originale.Ce n'est pas parce que l'on absorbe des doses équivalentes d'une même substance active sous des formes différentes (une solution au lieu d'un comprimé par exemple) que l'effet pharmacologique recherché sera équivalent. La prise à jeun ou après un repas change également l'effet pharmacologique de la substance active. On parle alors de disponibilité de la substance active ou encore de « biodisponibilité ». Deux formes offrant la même biodisponibilité seront dites bioéquivalentes.Le principe de bioéquivalence décrit deux médicaments contenant la même quantité de substance active. Les substances actives sont dites bioéquivalentes si, pour un même groupe d'individus, leurs effets thérapeutiques sont estimés biologiquement équivalents. Des différences au niveau des caractéristiques physiques des substances actives (structure cristalline ou polymorphisme, taille des cristaux) ou caractéristiques de formulation (présence de certains excipients, compression, délitement, enrobage,..) peuvent faire que deux formes galéniques qui contiennent la même quantité de substance active sont très différentes au niveau de leur mise à la disposition de cette substance active au niveau du système digestif. Il en est de même pour des formes injectables où l'on injecte des substances actives en suspension. Mais comme il est difficile et surtout très coûteux de tester une équivalence thérapeutique basée sur des tests cliniques et/ou biologiques, on teste en fait les variations de la concentration plasmatique de la substance active inchangée au cours du temps, variation consécutive à la prise du médicament au t=0. La courbe de biodisponibilité est représentée par la concentration plasmatique en inchangé Cp = f(t). C'est la mesure de l'aire sous la courbe qui donne la biodisponibilité de la substance active tel que présenté dans la forme galénique. Deux médicaments bioéquivalents donnent des moyennes d'aires sous la courbe (donc de concentrations plasmatiques en produit inchangé = f(t) qui sont équivalentes dans une population d'une vingtaine d'individus sains. Pour enregistrer un produit générique, il est nécessaire de démontrer par une étude de bioéquivalence que la forme générique est bioéquivalente à la forme princeps. Les problèmes de bioéquivalence existent cependant et se posent de façon importante pour des substances actives peu solubles (solubilité aqueuse inférieure à 1 mg/ml) lorsqu'elles sont administrées par voie orale ou pour des formes galéniques modifiées telles que les formes à libération prolongée, appelées encore formes retard. Pour des formes en solution vraie et présentées en injectable et injectées par voie IV en bolus, il n'existe par définition aucune différence de bioéquivalence entre formes puisque la biodisponibilité est totale (on dit alors que la biodisponibilité est absolue et égale à 1, quelle que soit la spécialité injectable utilisée. En revanche, pour des solutions orales, la biodisponibilité n'est plus absolue mais relative, car elle est relative à la vitesse de transit gastrique de chaque individu (à jeun, pendant ou après un repas, repas léger ou gras, etc.) et à une fenêtre d'absorption duodénale, si celle-ci existe. Par voie orale, on est donc toujours dans le relatif. Pour assurer une qualité de biodisponibilité des formes galéniques orales, dans la pratique industrielle, on teste la vitesse de dissolution des formes galéniques orales en laboratoire (test de dissolution) et ceci sur chaque lot avant de le libérer vers la distribution.Les injectables à libération prolongée (formes intra-musculaire, intra-articulaire, implants, etc.) peuvent, en revanche, montrer des biodisponibilités fort différentes entre elles et par rapport à la forme IV bolus. Ceci ne signifiant pas nécessairement des effets thérapeutiques essentiellement différents ou nécessairement supérieurs ou inférieurs. Là commence le domaine de la pharmacocinétique en liaison avec la toxicité (animale et humaine) et les études cliniques (animale et humaine).Au début du XXe siècle n'étaient considérés comme médicaments qu'une douzaine de produits de synthèse et une centaine de produits naturels. Au début du XXIe siècle, nous utilisons des centaines de substances synthétiques et il ne reste que très peu de remèdes courants d'origine exclusivement naturelle. Le XXe siècle a vu l'essor des médicaments de synthèse produits par des laboratoires pharmaceutiques. Depuis peu, les protéines, molécules du vivant, sont de plus en plus utilisées comme médicament.Actuellement, pour une utilisation en santé humaine et animale, de la découverte d'une nouvelle substance active à l'Autorisation de mise sur le marché (AMM) en passant par la mise au point de(s) (la) forme(s) galénique(s) (le médicament délivré en officine), généralement une période de 10 à 15 ans se sera écoulée et plusieurs centaines de millions d'euros auront été investies.Le processus de développement peut être décrit selon les étapes suivantes :recherche d'une substance originale candidate au statut de candidat médicament selon plusieurs méthodes : modélisation informatique, criblage (screening), observation de médecines traditionnelles (medicine man (en)), étude des caractéristiques des plantes ou substances naturelles (pharmacognosie), et parfois par les faveurs du hasard (sérendipité) lors d'observations cliniques ;les substances candidates sont alors le plus souvent brevetées ce qui confère, dans ce cas, à l'inventeur un droit de propriété intellectuelle permettant l'exploitation commerciale exclusive de la molécule pour une durée maximale de 20 ans. Compte tenu du fait que la protection court à compter du dépôt du brevet et non celle de la mise sur le marché, pour le médicament, en France, en Europe et aux États-Unis un certificat complémentaire de protection (CCP ou SPC, en anglais) peut être obtenu. Par ailleurs, pour encourager des développements complémentaires, les autorités de santé peuvent accorder une exclusivité commerciale supplémentaires de quelques années dans des conditions particulières, par exemple indications orphelines, médicaments pédiatriques, etc.depuis 2009, commencent à se développer aux États-Unis des coopératives de conception libre de médicaments, notamment génétique ;étude de l'effet de la substance in vitro sur des micro-organismes en culture, ex vivo sur des organes isolés ou sur des récepteurs biologiques purifiés, puis in vivo, c'est-à-dire sur l'animal de laboratoire vivant ;recherche d'une forme galénique la mieux adaptée. On cherche tant que possible à obtenir une forme orale biodisponible et stable. Celle-ci étant la plus simple à prendre par le futur patient (compliance).Les dernières phases de recherche enclenchées dans le développement d'un nouveau médicament sont les études cliniques : depuis près de vingt ans, les différentes études cliniques qui doivent être réalisées à l'appui d'une demande d'enregistrement (demande d'AMM) font l'objet d'une standardisation internationale (harmonisation ICH) reconnue par tous les pays de l'OCDE. Elles sont structurées en trois phases avant la mises sur le marché et une, la phase IV, après cette mise sur le marché. Pour chaque nouvelle indication thérapeutique et parfois aussi par catégorie de formes galéniques (injectable, orale, topique…), il sera nécessaire de reconsidérer le plan clinique existant et de voir si les études cliniques existantes peuvent être utilisées à l'appui de la nouvelle indication / forme pharmaceutique ou si de nouvelles études sont nécessaires et doivent être entreprises avant d'aller plus avant. Lors de la mise sur le marché de copies génériques les études de bioéquivalence seront entreprises. Une substance active va donc faire l'objet d'études cliniques quasiment de façon continue pendant toutes les années de sa présence sur le marché.Les différentes études cliniques se font en quatre phases. Phase I La phase I est dite d'innocuité (ou encore de tolérance) du produit. Elle est généralement menée sur des volontaires sains. Elle vise à établir la dose minimale active (si son activité peut être mise en évidence sur le volontaire sain) et surtout pour établir la dose maximale tolérable, en doses uniques croissantes et/ou répétées. Pour des produits comme des antibiotiques, des anticancéreux, des hormones, etc., l'utilisation de volontaires sains est exclue. On cherche à connaître la pharmacocinétique ADME de la molécule (c'est-à-dire la vitesse d'absorption (A = la vitesse de passage dans le sang à partir d'une solution orale), M = la vitesse de métabolisation (transformation biologique par le foie et d'autres organes), D = la vitesse de distribution et de répartition dans les différents tissus à partir du compartiment plasmatique et E = la vitesse d'élimination de la molécule par l'organisme aussi appelée clearance). Les données ADME préalablement collectées sur les modèles animaux (rat, souris, chien et singe) servent d'encadrement et de comparaison pour les données ADME humaines. Comme il n'est pas éthique d'exposer des volontaires sains à des produits très actifs (anti-cancéreux, antithyroïdiens, hormones, antibiotiques, etc.), cette phase I est dans ce cas réalisée en phase II sur des patients qui eux peuvent bénéficier de l'effet thérapeutique supposé du produit testé. Dans tous les cas, l'accord du patient, après une information éclairée, est indispensable. Aucune expérimentation ne peut se faire à l'insu du patient et sans son accord « éclairé » par les explications du responsable de l'étude. Phase II Elle consiste en des tests dits de biodisponibilité sur patients volontaires et d'efficacité sur patients volontaires. Elle vise à établir la relation entre dose et effet. On établit le domaine (range) des doses actives à partir des données obtenues sur animaux en toxicologie préclinique. On établit le « range » des doses actives tolérées sans chercher à atteindre une dose maximale qui serait toxique. Ce range deviendra progressivement la posologie du produit pour telle indication. C'est lors de ces tests que l'on détecte les premiers effets secondaires, qui une fois confirmés en phase II et IV seront souvent les effets secondaires principaux du produit. Si ces effets sont trop importants par rapport à l'intérêt de l'effet thérapeutique apporté, le développement du produit sera arrêté. Phase III Le médicament dont l'activité pharmacologique a été confirmée en phase II doit être testé pour évaluer son intérêt clinique réel. Cette phase vise à établir le rapport entre bénéfice et risques. Le candidat médicament est comparé à un médicament de référence et toujours à un placebo (lorsqu'il n'existe pas d'opposition éthique à ne pas administrer de substance active au patient volontaire) dans une plus large étude clinique. Une randomisation (tirage au sort) est effectuée pour déterminer quel bras de traitement sera le patient. L'expérimentation dite « double aveugle » est un standard actuellement (ni le patient, ni le médecin ne savent si c'est un médicament, le placebo ou la référence qui est administrée). Ces méthodes statistiques sont un gage de rigueur et de qualité des données générées dans l'étude.Les données de toxicologie animale et d'innocuité clinique (innocuité = phase 2), les données cliniques (efficacité) et les données pharmaceutiques (qualité) sont rassemblées en un dossier dit de demande d'enregistrement qui est déposé pour obtenir une autorisation de mise sur le marché (AMM) à l'Agence européenne (EMEA). Si l'autorité estime (évaluation sur dossier uniquement) que les informations déposées à l'appui de la demande d'enregistrement sont suffisantes, elle autorise la commercialisation du médicament mais uniquement dans les indications cliniques approuvées. Si l'autorité estime qu'un complément d'information est nécessaire, elle exigera des compléments d'information à déposer avant de commercialiser la spécialité ou à remettre dans un délai assez court un an deux ans, mais sans empêcher la mise sur le marché du médicament.Le plus souvent, lorsqu'il s'agit d'un médicament contenant une nouvelle molécule (NCE = New Compound Entity), celle-ci est couverte par des droits de propriété intellectuelle (brevet ou patent). Cette propriété s'obtient par le dépôt d'une demande de brevet. Cette propriété intellectuelle une fois accordée, court sur une période maximale de 18 à 20 ans depuis le dépôt de la demande de brevet. Au bout d'un certain nombre d'années, le brevet de la substance active tombe dans le domaine public, et ainsi ouvre la possibilité de copie par des laboratoires spécialisés dans la production de médicaments génériques. Ces « génériques » doivent aussi faire l'objet d'enregistrement auprès des autorités de santé. Ces produits étant (on ne dit plus équivalents mais) essentiellement similaires aux produits originaux qualifiés de princeps, seule la partie pharmaceutique du dossier d'AMM est déposée pour obtenir un enregistrement. Une période dite de protection des données de 5 ans peut être obtenue auprès des autorités de santé pour empêcher les copies génériques d'un produit original, innovateur qui a mis longtemps pour être développé, plus que sa période de protection du brevet.Ainsi, il ne faut faire pas de confusion pour le médicament entre la protection des droits de propriété industrielle (brevet, CCP) qui est accordée par les agences de propriété industrielle (INPI au niveau national, Office Européen des Brevets au niveau européen) et les protections dites réglementaires auxquelles s'engagent les agences de santé (ANSM au niveau national ou EMA Agence Européenne du Médicament au niveau européen). La protection offerte par les agences de santé porte ainsi sur les données cliniques de développement dans l'indication considérée du médicament princeps.En France, un médicament expérimental est produit selon des critères de qualité équivalent au produit mis sur le marché. La loi dit que (annexe I de la décision du 26 mai 2006 modifiant l'arrêté du 10 mai 1995 modifié relatif aux bonnes pratiques de fabrication (industrie pharmaceutique)) tout principe actif sous une forme pharmaceutique ou placebo expérimenté ou utilisé comme référence dans une recherche biomédicale, y compris les médicaments bénéficiant déjà d'une autorisation de mise sur le marché, mais utilisés ou présentés ou conditionnés différemment de la spécialité autorisée, ou utilisés pour une indication non autorisée ou en vue d'obtenir de plus amples informations sur la forme de la spécialité autorisée. Phase IV La phase IV (ou post-marketing) est le suivi à long terme d'un traitement alors que le traitement est autorisé sur le marché. Elle doit permettre de dépister des effets secondaires rares ou des complications tardives. Cette phase est à la charge des laboratoires.Parmi les médicaments, des familles thérapeutiques sont notamment retrouvées :qu'on pourrait regrouper en 6 catégories plus vastes :Les hypnotiques (somnifères) et les anxiolytiques sont quelquefois rassemblées sous le nom de « psycholeptiques », terme qui est en fait assez vaste. Cette classification selon Delay et Deniker (1957) a été modernisée plus tard par Pelicier et Thuillier (1991).Traditionnellement, les médicaments sont prescrits par les médecins à leurs patients qui vont se les procurer chez leur pharmacien.Certains médicaments peuvent être obtenus sans ordonnance (automédication ou médication officinale) ; en France, lorsqu'un médicament est acheté sans être prescrit, il n'est pas remboursé par l'assurance maladie, mais il peut l'être par certaines mutuelles. Dans la plupart des pays, un médicament doit avoir obtenu une autorisation de mise sur le marché (AMM) pour être vendu. L'AMM est connue sous l'appellation « NDA » (new drug application) aux États-Unis et sous « NDS » (new drug submission) au Canada.Les organismes de régulation de la santé dressent des listes de médicaments en fonction des risques que représentent leur prise. Par exemple, en France, il existe plusieurs listes de substances vénéneuses : les médicaments qui renferment ces substances ne peuvent être acquis que sur ordonnance (sauf cas limités d'exonération) :liste I : médicaments toxiques (dans les conditions normales d'emploi) ;liste II : médicaments dangereux, moins toxiques que ceux de la liste I (dangereux en conditions anormales d'emploi) ;stupéfiants : substances psychoactives fortes capables de provoquer une dépendance et des effets délétères sur la santé psychique et physique, tout en représentant un danger particulier pour la santé publique.psychotropes : complémentaires aux stupéfiants, mais représentant un risque pour la santé publique jugé moindre et dont les conditions de prescription sont plus souples. Regroupe globalement les benzodiazépines et les barbituriques.Les différences entre ces listes sont surtout théoriques et ne garantissent pas forcément une description précise du caractère dangereux du médicament. Par exemple, le sécobarbital est classé comme stupéfiant tandis que le phénobarbital, dont le profil addictogène et nocif est comparable, ne figure que sur la Liste II ainsi que la liste des psychotropes. De ce fait, ces classements ne sont pas des garanties et de nombreux produits figurent d'ailleurs sur deux registres complémentaires.Du fait de l'émergence régulière de nouvelles substances l'Arrêté du 22 février 1990 fixant la liste des substances classées comme stupéfiants ainsi que celui fixant la liste des psychotropes sont fréquemment consolidés, et ont uniquement cours jusqu'à ce qu'une version ultérieure ne les remplace.Selon leurs particularités et leurs conditions d'utilisation ou de manipulation, certains médicaments en France sont soumis à des « conditions de prescription » telles que : médicaments à prescription restreinte :ceux qui sont réservés à l'usage hospitalier,ceux qui ne peuvent être prescrits que par un médecin hospitalier,ceux nécessitant une surveillance spécifique et une prescription par un médecin spécialisé,médicaments d'exception : médicaments particulièrement onéreux, ils doivent faire l'objet d'un suivi spécifique et de justifications médicales pour la prise en charge ;médicaments restreints et d'exception.Le médicament peut s'administrer, selon sa forme galénique, par plusieurs voies d'administration :de manière globale (systémique) : la substance active passe dans le sang et est transportée partout dans l'organisme, afin d'atteindre sa cible :administration orale, dite per os : comprimé, sirop, gélule, solution buvable, granulé,suppositoire,administration par voie pulmonaire (inhalation ou instillation), avec absorption par les muqueuses des voies respiratoirespar timbre transdermique (à travers la peau) : par exemple pour pallier l'envie de fumer, ou comme anti-inflammatoire ou antidouleur (morphinique),L'administration par voie parentérale est faite au moyen d'une injection. Elle peut être :intraveineuse, en une fois on dira en bolus ou par une perfusion lente. La veine pouvant être superficielle, habituellement au bras (voie veineuse périphérique) ou profonde (voie veineuse centrale), le plus souvent au niveau du cou (veine jugulaire) ou sous la clavicule (veine sous clavière). La voie intraveineuse permet d'administrer un produit qui doit agir très rapidement (urgence) ou un produit mal toléré avec le risque d'irriter la veine (phlébite),sous-cutanée : sous la peau, fréquemment au niveau du ventre ou des cuisses (insuline),intradermique : dans le derme,intramusculaire : dans un muscle (cuisse) pour un produit qui doit agir lentement.de manière locale, directement sur le site d'action désiré:administration par voie oculaireadministration par voie vaginale / en intra-utérin, respectivement par le vagin et l'utérusde manière dermique (topique) : la substance active est amenée directement à l'endroit où il doit agir au niveau de la peau: pommade, crème dermique, gel dermique, etc. (action cutanée ou topique),L'efficacité et l'évaluation du médicament tiennent compte de la balance bénéfice/risque, des effets secondaires et paradoxaux, des interactions et contre-indications. Le profil de risque est surtout lié à la relation entre les effets secondaires et la maladie soignée.Le rapport bénéfice/risque est pris en compte - ainsi des effets secondaires sévères seront indéniablement mieux acceptés pour échapper à un cancer que pour éviter la douleur ou l'obésité. Du côté du médecin, celui-ci doit prendre en compte dans ce rapport au risque la durée du traitement (effet cumulatif), et ne pas négliger le risque sur le fœtus quand il s'agit d'une femme enceinte (exemple : thalidomide, mieux connu sous le nom Softenon). La posologie et les effets secondaires connus doivent être inscrits sur la notice accompagnant le médicament.De plus, certains médicaments sont strictement réglementés et ne peuvent être prescrits que sous certaines conditions (voir prescription, distribution). Les données récoltées, touchant un grand nombre de patients, sont transmises aux autorités de santé qui réévaluent la balance bénéfice/risque du médicament. Il peut en ressortir des effets graves qui n'étaient pas apparus lors des études cliniques et ainsi mener le laboratoire ou l'autorité à retirer le médicament.Les événements indésirables médicamenteux concernent des effets indésirables iatrogènes qui peuvent être graves (Évènement indésirable grave (EIG)), qu'ils soient le fait d'une erreur médicamenteuse ou non.Les médicaments peuvent provoquer une réaction anaphylactique ou anaphylactoïde. C'est le cas par exemple de l'acide acétylsalicylique, des inhibiteurs de l'enzyme de conversion de l'angiotensine et des sartans, des pénicillines, des céphalosporines, des produits de contraste, des anesthésiques locaux, des anti-inflammatoires non stéroïdiens. Des réactions croisées sont possibles, par exemple entre les pénicillines et les céphalosporines. Les bêtabloquants peuvent aggraver l'évolution d'une réaction anaphylactique et contrecarrer la réponse à l'adrénaline.La prescription d'un médicament n'est pas neutre - les effets induits ne sont pas toujours légers, ils peuvent être graves. Les effets secondaires peuvent à leur tour être mal interprétés, comme symptômes d'autre chose, ou d'une aggravation de l'état de la personne… ce qui complique singulièrement la situation et peut conduire à des prescriptions supplémentaires (inadaptées) à d'autres effets secondaires, et aussi à une dépendance.Les Français sont les plus gros consommateurs au monde de somnifères (3 fois plus que les Britanniques, ou que les Allemands) dont les effets secondaires peuvent être la dépression, avec ou sans tendances suicidaires, des états phobiques, l'agressivité et un comportement violent[réf. nécessaire].Dans le domaine des antibiotiques, une sur-prescript"
médecine;"La pharmacologie est une discipline scientifique du vivant, subdivision de la biologie, qui étudie les mécanismes d'interaction entre une substance active et l'organisme dans lequel elle évolue, de façon à pouvoir ensuite utiliser ces résultats à des fins thérapeutiques, comme l'élaboration d'un médicament (principalement) ou son amélioration.Pour ce faire, la pharmacologie intègre des concepts et données issus de la physiologie, physio-pathologie, biochimie, génétique et biologie moléculaire.Le champ de la pharmacologie peut être étendu puisqu'elle étudie également les moyens d'administration des médicaments, les interactions médicamenteuses et les effets néfastes de ces médicaments (effets latéraux, effets secondaires).Cette discipline pharmaceutique est fortement liée à la recherche fondamentale, à la recherche clinique et à la santé publique (pharmaco-épidémiologie), mais aussi à la toxicologie et la chronopharmacologie.Durant l'Antiquité, Hippocrate de Cos fut le premier médecin à rejeter les superstitions et les croyances qui attribuaient la cause d'une maladie à des forces surnaturelles ou divines. Dans son livre Sur la maladie sacrée, il évoque sa « théorie des humeurs » qui prouva qu'une maladie n'est pas une punition infligée par les dieux, mais plutôt le résultat de facteurs environnementaux, de l'alimentation et des habitudes de vie. Ce constat permit à la médecine de se dissocier de la religion et d'exister en tant que discipline à part entière, mais aussi de concevoir l'interaction de l'organisme, entité matérielle, avec son environnement qui est lui aussi matériel. Héraclide de Cumes, contemporain d’Asclépiade de Bithynie, il fut un des fondateurs de la pharmacologie et de la toxicologie. Galien, médecin grec considéré comme l'un des fondateurs de la pharmacie, reprit et précisa la théorie d'Hippocrate. Selon lui, la maladie résulte d'un déséquilibre entre les humeurs et la thérapie consiste donc à en rétablir l'équilibre, souvent à l'aide de remèdes ayant l'effet inverse aux symptômes identifiés. Toutefois, la dissection de cadavres étant interdite par le droit romain, il pratiqua la dissection sur des d'animaux ce qui l'amena à développer un grand nombre d'idées erronées sur l'anatomie humaine. Ainsi, Galien proposa la saignée comme remède à presque tous les maux. L'utilisation abusive de purgatifs, de sudorifiques et d'émétiques (vomitifs), visant à purifier le corps pour en chasser l'excès maladif de l'une des humeurs, fut l'un des premiers balbutiements de la pharmacie et de la pharmacologie occidentale.Au Moyen Âge, on ne constate pas d'avancement significatif : la théorie galénique est conservée jusqu'au XVIe siècle et on fait une distinction entre la pharmacie et la médecine. Néanmoins, les concoctions ayant des propriétés miraculeuses sur l'organisme se multipliaient. Des épiciers connaissant les propriétés médicamenteuses de certaines épices se spécialisèrent en apothicairerie. En effet, la profession d'apothicaire devint popularisée aux XIIIe et XIVe siècles. Le terme « apothicaire » désignait alors les boutiquiers qui vendaient des drogues et des médicaments pour les malades. Évidemment, la distinction entre l'apothicaire et le charlatan était presque imperceptible. Des rapports houleux s'établirent alors entre les apothicaires et les médecins du XIIe siècle. Ces derniers considéraient l'acte médical des autres comme moins noble. En 1241, l'édit de Salerne édicté par Frédéric II sépara juridiquement la médecine et l'apothicairerie, ce qui marqua l'origine officielle de la profession d'apothicaire.Au XVIe siècle, Paracelse, alchimiste et médecin parfois considéré comme le père de la toxicologie et de la pharmacologie, écrit un livre sur le corps humain qui réfute Galien. À l'époque, on se purgeait encore dans l'intention de se « nettoyer des humeurs putrides ou malsaines » qui attaquaient le corps et provoquaient un déséquilibre physiologique. Paracelse énonça la théorie selon laquelle le fonctionnement de l'organisme s'explique par un ensemble de réactions chimiques. Selon lui, les maladies sont provoquées par des désordres chimiques, provenant d'organes spécifiques, à l'intérieur du corps et elles ne peuvent donc être soignées que par des moyens chimiques. Le remède est donc formé par l'extraction d'un élément particulier pour chaque maladie, la « quintessence », puis donné au malade. À ce titre, il introduit notamment l'utilisation du mercure pour le traitement de la syphilis, ce qui provoqua un évident intérêt pour les gens de l'époque. L'une des causes de ce succès est dû au fait qu'il reconnaissait la relation dose-effet d'un médicament, l'un des principes fondamentaux de la pharmacologie. Dans ses mots, Paracelse écrit :« Toutes les choses sont poison, et rien n’est poison ; seule la dose détermine ce qui n’est pas un poison. »L'apparition des premières pharmacopées, recueils officiels de médicaments, vit le jour aux XVe et XVIe siècles. On parle entre autres du Codex Medicamentarius Parisiensis paru en 1638.La pharmacie galénique.La pharmacocinétique, qui étudie le devenir d'une molécule bioactive dans l'organisme, de son absorption à son excrétion en passant par son métabolisme. En résumé, elle étudie les effets de l’organisme sur la molécule.La pharmacodynamique, qui étudie comment une molécule produit un effet sur un organisme.La pharmacogénétique, qui étudie l'influence des gènes sur l'activité des médicaments sur l'organisme.La pharmacogénomique.La toxicologie s'intéresse spécifiquement aux molécules ayant un effet nocif sur un organisme.La pharmaco-épidémiologie.La pharmacovigilance, qui étudie les effets indésirables des médicaments.Les étudiants en pharmacologie doivent acquérir un large éventail de connaissances, notamment en physiologie, biochimie, chimie, génétique, ainsi qu'en pharmacologie moléculaire et clinique.À Sherbrooke, Québec au Canada, l'Université de Sherbrooke offre son baccalauréat en pharmacologie depuis 2001.Michael Neal, Pharmacologie Médicale , Deboeck, 2017  (ISBN 978-2-807-306110)PharmacologueRépertoire de la pharmacologieEthnopharmacologieInstitut de pharmacologie de SherbrookePharmaciePharmacothérapieToxicologieSociété Française de Pharmacologie et de ThérapeutiquePharmacomedicale.org, site officiel du collège national (France) de pharmacologie médicale (CNPM), et site d'information sur le médicament Portail de la pharmacie   Portail de la chimie   Portail de la biochimie   Portail de la médecine"
médecine;La thyroxine ou T4 est une hormone thyroïdienne agissant comme une prohormone devant être désiodée en triiodothyronine, ou T3, par la thyroxine 5'-désiodase pour être pleinement active. Elle est biosynthétisée chez les mammifères dans la thyroïde par iodation de la thyroglobuline sous l'action de l'iode introduit dans les cellules par la pendrine et oxydé en iode atomique par la thyroperoxydase, une enzyme dont l'expression est accrue par la thyréostimuline (TSH). Elle est inactivée par la thyroxine 5-désiodase, qui la convertit en 3,3',5'-triiodothyronine ou « T3 inverse », isomère inactif de la T3.Les hormones thyroïdiennes jouent un rôle important dans le métabolisme énergétique et agissent en relation avec d'autres hormones, telles que l'insuline, le glucagon, l'adrénaline ou encore l'hormone de croissance.La L-thyroxine (énantiomère S-(–), ou lévothyroxine) est synthétisée en laboratoire comme médicament contre l'hypothyroïdie ou comme traitement à vie en cas de thyroïdectomie. Elle est prise sous forme de comprimés à raison de 12,5 ?g à 200 ?g par jour, habituellement une demi-heure avant le petit déjeuner afin d'en maximiser l'absorption dans la mesure où elle est mal absorbée par l'intestin. Elle peut également être administrée par intraveineuse dans les cas d'hypothyroïdie sévère.Liste d'hormones Portail de la chimie   Portail de la biochimie   Portail de la médecine
médecine;"Un antibiotique (du grec anti : « contre », et bios : « la vie ») est une substance naturelle ou synthétique qui détruit ou bloque la croissance des bactéries. Dans le premier cas, on parle d'antibiotique bactéricide et dans le second cas d'antibiotique bactériostatique. Lorsque la substance est utilisée de manière externe pour tuer la bactérie par contact, on ne parle pas d'antibiotique mais d'antiseptique.Un antibiotique peut être à la fois bactéricide et bactériostatique, tout dépendant de sa dose.Un grand nombre des antibiotiques existants sont constitués de molécules naturelles, fabriquées par des micro-organismes : des champignons ou d'autres bactéries. Ces dernières les produisent pour éliminer les bactéries concurrentes avec lesquelles elles sont en compétition dans leur biotope. Cependant, seul un petit nombre des antibiotiques naturels est utilisable en thérapeutique humaine, pour des raisons de disponibilité dans l'organisme ou d'effets indésirables. Un grand nombre de molécules aujourd'hui sur le marché sont des molécules de synthèse, dérivées ou non d'antibiotiques naturels, en particulier pour contourner les problèmes de résistance.Les antibiotiques agissent de manière spécifique sur les bactéries, en bloquant une étape essentielle de leur développement : synthèse de leur paroi, de l'ADN, des protéines, ou la production d'énergie, etc. Ce blocage se produit lorsque l'antibiotique se fixe sur sa cible, une molécule de la bactérie qui participe à l'un de ces processus métaboliques essentiels. Cette interaction entre l'antibiotique et sa cible est très sélective, spécifique des bactéries et ces composés ne sont en général actifs ni sur les champignons ni sur les virus. Il existe d'autres molécules actives sur ces autres types d'agents infectieux que l'on appelle des antimycosiques ou des antiviraux, distincts des antibiotiques.L'introduction généralisée des antibiotiques après la Seconde Guerre mondiale a été l'un des progrès thérapeutiques les plus importants du XXe siècle. Les traitements antibiotiques ont fait progresser l'espérance de vie de plus de dix ans, soit plus qu'aucun autre traitement médical. Cependant, l'usage généralisé, voire abusif de certains antibiotiques, y compris en traitement préventif, curatif ou en complément alimentaire dans l'alimentation animale, dans les piscicultures, en médecine vétérinaire et humaine, ou encore comme pesticides pour le traitement des végétaux (contre le feu bactérien par exemple) a introduit une pression de sélection qui a conduit au développement de populations de micro-organismes antibiorésistants et à une baisse générale de l'efficacité thérapeutique. En milieu hospitalier, ceci conduit à une augmentation du risque nosocomial, faute de traitement adapté contre certains germes particulièrement résistants.De manière simplifiée un antibiotique est, dans le domaine médical, « une substance chimique organique d’origine naturelle ou synthétique inhibant ou tuant les bactéries pathogènes à faible concentration et possédant une toxicité sélective ». Par toxicité sélective, on entend que celle-ci est spécifique des bactéries et que la molécule antibiotique n'affecte pas l'hôte infecté, au moins aux doses utilisées pour le traitement.Plus généralement, pour les microbiologistes et les chimistes, un antibiotique est une substance anti-bactérienne.Il a existé des variantes dans cette définition qui diffèrent par la présence ou non des concepts de toxicité sélective, d’origine microbienne et de limitation de cible aux seules bactéries.Les antiseptiques ne sont pas des antibiotiques. Leur fonction est de tuer un maximum de germes (bactéries, champignons, virus), leur mode d'action n'est pas spécifique, ils ne s'utilisent que localement en application externe et mal employés (trop concentrés par exemple) ils peuvent provoquer des lésions et/ou retarder la cicatrisation.Les antibiotiques ne sont généralement pas actifs contre les virus. Un produit luttant contre les virus est un antiviral. Toutefois, des études en cours tendent à démontrer une certaine efficacité de quelques antibiotiques dans des cas particuliers comme l'effet de la teicoplanine sur la maladie à virus Ebola.Le premier antibiotique identifié fut la pénicilline. Si dès la fin du XIXe siècle Ernest Duchesne découvrit les propriétés curatives de Penicillium glaucum, la découverte de la pénicilline est à mettre au crédit de Sir Alexander Fleming qui s’aperçut en 1928 que certaines de ses cultures bactériennes dans des boîtes oubliées avaient été contaminées par les expériences de son voisin de paillasse étudiant le champignon Penicillium notatum et que celui-ci inhibait leur reproduction. Mais l’importance de cette découverte, ses implications et ses utilisations médicales ne furent comprises et élaborées qu’après sa redécouverte, entre les deux grandes guerres notamment à la suite des travaux de Howard Walter Florey, Ernst Chain, et Norman Heatley (en) en 1939.En 1932, Gerhard Domagk met au point chez Bayer AG le Prontosil, un sulfamidé, le premier antibiotique de synthèse. C’est toutefois la découverte subséquente, à l'Institut Pasteur, dans le laboratoire de chimie thérapeutique dirigé par Ernest Fourneau, des propriétés antibiotiques du sulfanilamide, agent actif du Prontosil, (découverte publiée en 1935 par Jacques et Thérèse Tréfouel, Federico Nitti et Daniel Bovet) qui ouvrira effectivement la voie à la sulfamidothérapie. Ce premier antibiotique de synthèse a ouvert une voie nouvelle dans la lutte contre de nombreuses maladies qui étaient considérées comme incurables auparavant.René Dubos isole en 1939 la tyrothricine (un mélange de tyrocidine et de gramicidine) à partir du Bacillus brevis dont il avait observé l’action antibactérienne. L’importance de cette découverte ne fut pas tant d’ordre thérapeutique que théorique : si la gramicidine fut effectivement le premier antibiotique commercialisé, son utilisation fut limitée à une application locale — en topique — ; toxique en intraveineuse, la gramicidine s’avéra en revanche très efficace pendant le second conflit mondial pour guérir les blessures et les ulcères. Comme Howard Florey lui-même devait le rappeler plus tard, la découverte de la gramicidine fut une étape déterminante en cela qu’elle encouragea les recherches autour des applications thérapeutiques de la pénicilline qui avaient souffert jusque-là de plusieurs déconvenues.En 1944, Selman A. Waksman, Albert Schatz et E. Bugie découvrent la streptomycine, le premier antibiotique ayant un effet sur le bacille de Koch, rendant ainsi possible le traitement de la tuberculose. En 1952, commercialisation sous la marque Ilosone de l’érythromycine, premier macrolide connu, nouvellement isolée par J. M. McGuire, de la firme Eli Lilly. En 1956 est découverte la vancomycine. Suivent alors le développement des quinolones à partir de 1962 et leurs dérivés, les fluoroquinolones dans les années 1980.Au début des années 1970, la recherche sur les antibiotiques se ralentit fortement, l'arsenal thérapeutique de l'époque permettant alors de traiter efficacement la plupart des infections bactériennes.L'émergence des résistances de plus en plus nombreuses va modifier ce tableau et stimuler la reprise des travaux. En 2000, le linézolide (approuvé par la FDA le 18 avril 2000) est mis sur le marché américain. Le linézolide correspond à une nouvelle classe de composés, les oxazolidinones. C'est la première fois en 20 ans qu'une nouvelle classe de composés antibiotiques est introduite dans la pharmacopée.Globalement, en un demi-siècle, les antibiotiques ont augmenté de plus de dix ans l’espérance de vie de ceux qui y ont accès, soit plus qu'aucun autre traitement. Comparativement, un médicament qui guérirait 100 % des cancers n’augmenterait l’espérance de vie que de cinq ans[réf. nécessaire].Les antibiotiques ont en particulier fourni des traitements efficaces pour la plupart des grandes maladies infectieuses bactériennes. Combinés à la vaccination ils ont contribué à faire largement disparaître les grandes maladies épidémiques, au moins dans les pays développés : tuberculose, peste, lèpre, typhus, fièvre typhoïde... Ils sont également utilisés dans les cas de choléra en complément de la réhydratation des malades.Il existe plus de 10 000 molécules antibiotiques connues (voir liste d'antibiotiques), la plupart d'entre elles sont des produits naturels, synthétisés par des procaryotes, des champignons, des végétaux supérieurs, des animaux ou des lichens.Le principe d'action des antibiotiques consiste à bloquer sélectivement une étape d'un mécanisme essentiel à la survie ou à la multiplication des micro-organismes. Le mécanisme ciblé par l'antibiotique est le plus souvent spécifique des bactéries et n'a pas d'équivalent chez les eucaryotes et en particulier chez l'humain. Ainsi, idéalement, l'antibiotique tue ou bloque la multiplication des bactéries mais n'a pas d'impact sur les cellules du patient traité. Il existe ainsi quelques grandes familles de mécanisme d'action pour les antibiotiques, ce qui permet de les regrouper en grandes classes décrites ci-après.Certaines bactéries sont protégées de l'environnement extérieur par une paroi, qui doit croitre quand la bactérie se divise. Cette paroi contient en particulier une couche de peptidoglycane plus ou moins épaisse, un polymère spécifique comportant des acides aminés et des sucres. Il existe une machinerie de synthèse qui fabrique les composants de cette paroi et qui est composée d'enzymes et de systèmes de transport acheminant les composants à la surface cellulaire.Il existe un ensemble d'antibiotiques qui bloquent différentes étapes de cette machinerie. Le blocage de la synthèse de la paroi fragilise fortement l'enveloppe externe des bactéries, qui deviennent très sensibles à des stress extérieurs (pression osmotique, température, stress mécanique) provoquant la lyse cellulaire. In vitro, on peut maintenir ces cellules sans paroi avec un stabilisant osmotique, on obtient alors un protoplaste.Ces antibiotiques agissent sur des cibles extracellulaires. Ils n'ont donc pas besoin de pénétrer dans la cellule, ce qui les rend insensible aux mécanismes de résistance liés à la perméabilité ou à l'efflux (voir plus bas). En revanche, ils ne sont en général actifs que sur les germes en croissance. Les bactéries quiescentes (qui ne se divisent pas) ne sont pas perturbées par l’action de ces molécules, parce que le peptidoglycane n'est produit que lors de la croissance cellulaire, pour s'adapter à l'augmentation du volume précédant la division cellulaire.Les principaux antibiotiques ayant ce mode d'action correspondant à la famille appelée les béta-lactames (pénicillines et céphalosporines). Ceux-ci agissent sur les enzymes de la machinerie de synthèse du peptidoglycane que l'on appelle pour cette raison les « protéines fixant la pénicilline » (penicillin binding proteins ou PBP).La catégorie des antibiotiques inhibant la synthèse de la paroi bactérienne comprend entre autres :la bacitracinela teixobactineles pénicillines: amoxicillineles céphalosporinesles glycopeptides comme la vancomycineL'existence d'une membrane plasmique intacte est nécessaire à la survie bactérienne. Son rôle est double, d'une part elle permet de séquestrer métabolites et ions nécessaires à l'intérieur du cytoplasme, d'autre part, elle permet de maintenir un gradient de protons entre l'intérieur et l'extérieur de la cellule, généré par la chaîne respiratoire et le cycle de Krebs et qui permet le stockage de l'énergie cellulaire. Ce gradient de protons alimente l'ATP synthase qui fabrique l'ATP. Toute perturbation de l'imperméabilité de la membrane rompt ces confinements, l'énergie chimiosmotique est dissipée et le contenu du cytoplasme fuit dans le milieu extracellulaire. Il existe un certain nombre de molécules antibiotiques qui agissent sur la membrane des cellules, soit en agissant comme des détergents qui désorganisent les lipides, soit en formant un pore (un trou) dans la membrane qui va permettre la fuite des composés cellulaires.Parmi ces composés attaquant la membrane des cellules bactériennes, on trouve :la polymyxine qui est un surfactant (détergent) interagissant avec les lipides membranaires et qui désorganise la bicouche phospholipidique membranaire. Ceci détruit l’intégrité de la membrane, les éléments hydrosolubles sortent de la cellule. Cette molécule est efficace sur les cellules en croissance et au repos ;la gramicidine, un peptide qui s'insère dans la membrane en formant un pore cylindrique permettant la fuite des cations.La synthèse des acides nucléiques, ADN et ARN est absolument vitale pour les cellules, sans elle, la division cellulaire et la fabrication des protéines est impossible. Un certain nombre de composés peuvent bloquer de manière directe ou indirecte ces voies de biosynthèse des acides nucléiques et ont en conséquence une activité antibiotique.Chez les bactéries, le ou les chromosomes sont souvent circulaires et se trouvent dans un état topologique particulier caractérisé par un surenroulement négatif. Ce surenroulement négatif est essentiel à la réplication de l'ADN (et aussi à la transcription de l'ARN) et constitue une caractéristique de l'ADN bactérien. C'est l'ADN gyrase qui introduit ce surenroulement négatif dans l'ADN. Cette enzyme, de la famille des topoisomérases est essentielle à la survie des bactéries, mais n'a pas d'équivalent chez les eucaryotes. Il existe des antibiotiques qui bloquent l'action de l'ADN gyrase, il s'agit des aminocoumarines et des quinolones. Plus récemment, ces dernières ont été supplantées par les fluoroquinolones, molécules de synthèse permettant de contourner les mécanismes de résistance aux quinolones.D'autres molécules bloquent la réplication de l'ADN en introduisant des pontages covalents entre des bases voisines, soit sur le même brin soit entre les deux brins de l'ADN. Ces pontages déforment l'ADN, peuvent empêcher l'ouverture des brins et bloquent l'action de différentes enzymes agissant sur l'ADN. Ceci a en particulier pour conséquence d'empêcher la progression de la fourche de réplication et du réplisome et rendent donc la réplication impossible.Ces molécules, comme la mitomycine ou l'actinomycine, si elles ont bien une activité antibiotique sur les bactéries, ne sont pas utilisées comme telles chez l'humain car elles ne sont pas sélectives et agissent aussi sur l'ADN des cellules eucaryotes. Leur capacité à ponter également notre ADN bloque aussi la réplication de nos propres cellules, ce qui leur confère en plus des propriétés antimitotiques chez l'humain. Pour cette raison, on les a utilisées en chimiothérapie anticancéreuse.Il existe enfin des inhibiteurs spécifiques de l'ARN polymérase bactérienne qui bloquent la transcription des gènes et la synthèse des ARN messagers. Parmi ces antibiotiques, on trouve en particulier la rifampicine qui est aujourd'hui utilisée en association avec d'autres antibiotiques pour le traitement de la tuberculose.La synthèse des protéines est un processus essentiel des cellules vivantes. L'acteur central de ce processus dans lequel l'ARN messager est traduit en protéine est le ribosome, l'organite cellulaire qui est responsable de cette étape. Les détails du mécanisme de traduction et les ribosomes des bactéries sont sensiblement différents de ceux des eucaryotes. Il existe un grand nombre de molécules antibiotiques qui exploitent ces différences et sont capables de bloquer sélectivement la traduction des protéines chez les bactéries, mais pas chez l'humain ou l'animal.De fait, approximativement la moitié des antibiotiques utilisés en thérapeutique (disposant de l'AMM) ont pour cible le ribosome bactérien. Ces antibiotiques se répartissent en plusieurs classes, de nature chimique et de mode d'action différents. La plupart interagissent avec l'ARN ribosomique. Enfin, certains antibiotiques bloquent la traduction en inhibant l'action des facteurs de traduction associés au ribosome.Les aminoglycosides ou aminosides (exemples : streptomycine, gentamicine, amikacine) se fixent sur la petite sous-unité des ribosomes (30 Svedberg) au niveau du site du décodage des codons, empêchent la traduction de l’ARNm et conduisent à des erreurs de lecture.Les phénicols (exemples : chloramphénicol, thiamphénicol) bloquent la formation de la liaison peptidique. Ils se fixent sur la grande sous-unité du ribosome bactérien (50 Svedberg) mais pas sur celle des ribosomes eucaryotes.Les cyclines (exemples : tétracycline, doxycycline, auréomycine) : en se fixant sur la sous-unité (30 S), elles bloquent l’élongation de la chaîne polypeptidique.Les macrolides et kétolides (exemples : érythromycine, azithromycine) agissent sur la partie 50 S du ribosome et bloquent l’élongation de la chaîne polypeptidique.La puromycine mime l’extrémité d’un ARNt, prend sa place dans le ribosome et bloque l’élongation de la chaîne polypeptidique.L'acide fusidique bloque l'action du facteur de traduction EF-G qui permet la translocation (progression) du ribosome sur l'ARN messager.Une autre classe importante d'antibiotiques interfère avec la production de métabolites essentiels, bloquant la synthèse de différents constituants essentiels de la cellules : lipides, acides aminés, nucléotides.Une voie particulièrement importante qui est fréquemment ciblée est celle de la synthèse des folates (vitamine B9). Ses dérivés, notamment le dihydrofolate et le tétrahydrofolate, interviennent dans des réactions de transfert de groupements à un atome de carbone (méthyle, formyle) et en particulier dans des réactions de méthylation. Ces réactions sont essentielles à la synthèse de la thymine et par voie de conséquence, celle de l'ADN. Ces transferts de carbone dépendant du folate interviennent également de façon centrale dans le métabolisme de certains acides aminés : méthionine, glycine, sérine et donc indirectement dans la synthèse des protéines.Plusieurs classes de composés antibiotiques ciblent différentes étapes de cette voie des folates :Les sulfamidés et le sulfanilamide sont des analogues structurels de l'acide p-aminobenzoïque ou PABA. Ce dernier composé est l'un des éléments qui interviennent dans la synthèse des folates chez les bactéries et chez les plantes. Le sulfanilamide est un inhibiteur de la dihydroptéroate synthase et bloque la synthèse du dihydrofolate. Le triméthoprime intervient en aval dans la voie, en inhibant la synthèse de tétrahydrofolate par la dihydrofolate réductase. Le triméthoprime est sélectif de la dihyrdrofolate réducate des bactéries et n'inhibe pas l'enzyme humaine, ce qui rend possible son utilisation thérapeutique comme antibiotique (contrairement au méthotrexate, qui inhibe la dihydrofolate réductase humaine et est utilisé comme anticancéreux)Sur les milliers d'antibiotiques connus, seulement un peu plus d'une centaine sont efficaces et utilisables pour des applications médicales et font donc partie de la pharmacopée moderne. Les autres sont trop toxiques, trop instables ou ont une biodisponibilité insuffisante chez l'humain. Les antibiotiques actuellement utilisés sont le plus souvent des molécules dérivées de produits naturels, dont on a légèrement modifié la structure pour améliorer leurs propriétés thérapeutiques ou contourner les problèmes de résistance. D'autres enfin ne sont plus utilisés parce que les bactéries pathogènes y sont devenues résistantes, c'est par exemple le cas de la streptomycine qui était autrefois utilisée pour traiter la tuberculose.Près de la moitié des antibiotiques utilisés en thérapeutique ciblent le ribosome bactérien et environ un quart d'entre eux sont des bêta-lactames, qui ciblent la synthèse de la paroi bactérienne. Si on regarde les prescriptions, on constate que ce sont les bêta-lactames (pénicillines et céphalosporines) qui sont les antibiotiques les plus utilisés, en particulier par les médecins généralistes. En France, ils représentent près des deux tiers des doses définies journalières utilisés, devant les macrolides (~15 %).Face à une infection bactérienne, le choix d'un antibiotique à utiliser dépend d'un ensemble de paramètres. Deux types de critères doivent être pris en compte : ceux qui dépendent du germe responsable lui-même et ceux qui dépendent du patient et du site de l'infection. Les premiers sont liés au spectre d'activité des différents antibiotiques, il est en effet nécessaire d'utiliser une molécule qui soit efficace sur le germe responsable de l'infection, en particulier lorsqu'on a affaire à des bactéries multirésistantes. Les seconds concernent l'interaction du médicament antibiotique avec le patient. Ceci concerne par exemple la capacité de l'antibiotique à atteindre efficacement le site de l'infection, l'existence d'un terrain allergique à certains antibiotiques ou encore la toxicité du composé utilisé (voir plus bas).Les différents antibiotiques disponibles possèdent des spectres d'activité variés, certains étant plus actifs sur les bactéries gram positives ou gram négatives, sur les germes aérobies ou anaérobies ou encore sur les bactéries capable de pénétrer à l'intérieur des cellules infectées. Pour déterminer les antibiotiques efficaces, en particulier en cas d'échec du traitement de première intention, on réalise souvent un test antibiogramme : le germe responsable est mis en culture dans une boîte de gélose Müller-Hinton contenant plusieurs pastilles d’antibiotique qui vont inhiber plus ou moins le développement du micro-organisme, ce qui permet de comparer la sensibilité des bactéries à tel ou tel antibiotique.Pour les infections sévères ou difficiles, on peut être amené à utiliser des combinaisons d'antibiotiques (poly-antibiothérapies). C'est en particulier le cas actuellement pour le traitement de la tuberculose, en raison des résistances acquises par le bacille de Koch pour lequel on utilise dans la phase initiale une quadrithérapie : isoniazide, rifampicine, pyrazinamide et éthambutol.Enfin, en raison de l'émergence progressive de germes multirésistants, certaines molécules de génération récente sont réservées au traitement d'infections « difficiles », résistantes aux traitements traditionnels utilisés en première intention. C'est en particulier le cas des carbapenems comme l'imipenem ou des oxazolidinones comme le linezolide. Cette restriction a pour objectif de retarder la propagation de résistance à ces nouveaux composés et donc de prolonger l'efficacité de l'arsenal thérapeutique disponible.L'analyse de l'activité d'un antibiotique donné sur une bactérie a conduit à définir un certain nombre de paramètres qualitatifs et quantitatifs. Le premier d'entre eux est le spectre d'activité qui définit la liste des espèces bactériennes sur lesquelles un antibiotique agit. Le spectre est propre à chaque antibiotique, et peut varier dans le temps à la suite de l'apparition de nouvelles résistances chez les différentes espèces bactériennes. L'autre concept majeur en antibiothérapie est celui de concentration minimale inhibitrice ou CMI (en anglais MIC, pour Minimal inhibitory concentration). Dans la pratique, on définit la CMI comme la concentration minimale d'antibiotique permettant d'inhiber (bactériostase) totalement la multiplication bactérienne, après 18 à 24 heures de contact à 37 °C. Ceci se décline en plusieurs variantes :la CMI50 est la plus faible concentration inhibant, en 18 à 24 heures, la multiplication de 50 % des bactéries ;la CMI90 est la plus faible concentration inhibant, en 18 à 24 heures, la multiplication de 90 % des bactéries.On définit également la concentration minimale bactéricide (CMB), qui est la plus faible concentration permettant de détruire ou de tuer (bactéricidie) 99,99 % des bactéries après 18 à 24 heures de contact avec l'antibiotique. La CMI et la CMB sont caractéristiques d'un antibiotique pour une souche donnée. L'analyse de la concentration minimale bactéricide et de la concentration minimale inhibitrice (CMB/CMI) permet de caractériser l'effet de l'antibiotique étudié sur une souche bactérienne donnée. Lorsque le rapport CMB / CMI = 1, l'antibiotique est dit « bactéricide absolu », s'il est proche de 1, l'antibiotique est dit « bactéricide », s'il est supérieur à 2, l'antibiotique est dit simplement « bactériostatique ». En dépit d'efforts de standardisation des méthodes de détermination des CMI, il subsiste des différences d'un auteur à l'autre qui sont liées à la variabilité des conditions expérimentales utilisées : divers facteurs peuvent jouer : Composition des milieux, taille de l'inoculum, souches de phénotypes différents, etc.Il existe également d'autres paramètres qui servent à caractériser le mode d'action d'un antibiotique et en particulier sa pharmacologie chez le patient :la concentration sérique maximale après l'administration de l'antibiotique, ou Cmax ;l'index thérapeutique sérique est égal à Cmax / CMI50. Plus ce paramètre est élevé, plus l'antibiotique sera efficace ;le temps d'antibiotique utile est la durée pendant laquelle la concentration sérique d'un antibiotique donné est supérieure à sa CMI pour un germe donné. Il correspond à la durée pendant laquelle il va avoir une activité sur les bactéries infectieuses dans l'organisme du patient ;le Tmax est le temps qu'il faut attendre pour atteindre la concentration sérique maximale Cmax ;la surface sous la courbe est un très bon indicateur de l'activité d'un antibiotique. On trace la courbe de l'évolution de la concentration de l'antibiotique dans le temps ; on place la ligne horizontale qui correspond à la CMI90. La surface sous la courbe est celle comprise entre la courbe des concentrations et la ligne de la CMI90. La surface sous la courbe est propre à un antibiotique pour un germe donné. Mécanismes Plusieurs mécanismes peuvent expliquer les effets indésirables liés à la prise d'antibiotique, parmi lesquels on trouve :Immunotoxicité : le médicament joue alors le rôle d'élément sensibilisateur. Les principales classes d'antibiotiques concernées sont les bêta-lactamines et les sulfamides. On distingue selon la classification de Gell et Coombs, les réactionsde type 1 provoquant des phénomènes anaphylactiques, asthme, choc anaphylactique). Les pénicillines peuvent induire des réactions anaphylactiques avec une prévalence de 0,01 % (réf. 1968 mais largement citée dans la littérature)de type 2 (cytopénie, anémie hémolytique...),de type 3 (vascularite immuno-allergique...),de type 4 (hypersensibilité retardée).Réactions anaphylactoïdes : la perfusion trop rapide de vancomycine peut entraîner un syndrome dit de l'Homme Rouge (Red Man Syndrome), lié à un phénomène d'histamino-libération.Troubles métaboliques :L'acide fusidique ou la rifampicine peuvent entraîner une hyperbilirubinémie.L'amphotéricine B augmente le risque d'hypokaliémie.Interactions médicamenteuses (induction et inhibition enzymatique) :Certains antibiotiques sont des inhibiteurs enzymatiques. C'est le cas des macrolides, de certaines cyclines telle la doxycycline, du métronidazole, de certains anti-fongiques azolés (fluconazole, itraconazole, kétoconazole, miconazole). Ils peuvent alors entraîner un surdosage des antivitaminiques K (AVK), de la théophyline, des dérives de l'ergot de seigle (ergotisme, nécrose des extrémités), risque majorée d'hypoglycémie (avec les azolés).Certains antibiotiques sont des inducteurs enzymatiques. C'est le cas de la rifampicine, de la rifabutine. Ils peuvent alors diminuer le taux des œstroprogestatifs, majorer l'hépatotoxicité de l'isoniazide..Phototoxicité des fluoroquinolones, des cyclines ou du voriconazole. Toxicité Certains antibiotiques ne sont pas totalement spécifiques des bactéries et ont une certaine toxicité sur les cellules humaines, en particulier en cas de surdosage. C'est en particulier le cas pour certains antibiotiques qui ciblent la synthèse des protéines et le ribosome, comme les aminoglycosides. Il existe en effet une assez grande similarité de fonctionnement entre le ribosome des bactéries et celui qui est présent dans les mitochondries des animaux, ce qui, à forte dose, peut conduire à une inhibition des ribosomes mitochondriaux et donc à un effet toxique. Ce mécanisme est responsable de la nephrotoxicité des aminoglycosides à trop forte dose (voir plus bas).Certains antibiotiques peuvent aussi réduire l'efficacité du système immunitaire chez la souris et altérer les cellules épithéliales chez l'homme. Manifestations pulmonaires Des bronchospames, des insuffisances respiratoires aigües[Combien ?] peuvent être liés à des réactions anaphylactoïdes.Des pneumopathies interstitielles immuno-allergiques peuvent être provoquées par les bêta-lactamines, les sulfamides ou les cyclines. On décrit aussi des pneumopathies alvéolaire aigües liée à la prise de bêta-lactamines ou de cyclines. Les nitrofurantoïnes peuvent rarement provoquer des pneumopathies interstitielles desquamations. Les sulfamides et les bêta-lactamines peuvent induire des angéites leucocytoclasiques. Manifestations rénales Néphropathie toxique : on observe des nécroses tubulaires aigües notamment provoquées par les aminosides (prévalence entre 7 % et 25 %, la gentamicin (26 %) étant plus néphrotoxique que la tobramycin (12 %)), les céphalosporines, l'amphotéricine B (une dysfonction rénale est observée dans 60 % à 80 % des cas, mais dans une large mesure, ces perturbations sont transitoires) et les polymyxines,, des néphropathies tubulo-interstitielles provoquées par les bêta-lactamines, les sulfamides, la rifampicine, les fluoroquinolones, les glycopeptides et les nitrofurantoïnes.Des néphropathies immuno-allergiques peuvent être provoquées par de très nombreuses classes d'antibiotiques. Les bêta-lactamines et la rifampicine sont les plus souvent incriminées.À noter les cristalluries provoquées par les fluoroquinolones ou les sulfamides, les nitrofurantoïnes, et les lithiases rénales provoquées par les pénicillines, les céphalosporines, les nitrofurantoïnes ou la sulfadiazine. On décrit aussi des glomérulonéphrites induite par la rifampicine ou l'isoniazide ou par la prise de cyclines. Manifestations neurologiques Les aminosides présentent une toxicité cochléo-vestibulaire. Celle-ci est cumulative et irréversible en cas de traitement prolongé. Précisément, on note une vestibulo-toxicité de l'ordre de 15 % des patients traités (certaines études rapportent une vestibulotoxicité de 10 % pour les streptomycine, et de 20 % pour la gentamicine).Une hypertension intra-crânienne peut être liée à la prise de fluoroquinolones, de tétracyclines, des nitrofurantoïnes, du sulfaméthoxazole.Des troubles neuro-sensoriels sont possibles après la prise de fluoroquinolones. Indiquons que rarement l'hydroxyquinoléine peut entraîner des neuropathies sensitives ainsi que des névrites optiques, et l'imidazolé (auquel les quinoléines sont associées dans le traitement des amoeboses intestinales), peut entrainer, à forte dose, une neuropathie sensitive.De façon assez peu spécifique, insomnies et vertiges peuvent être observés après la prise de nombreuses classes d'antibiotiques. Manifestations hématologiques Comme d'autres médicaments, les antibiotiques peuvent parfois induire des syndromes hématologiques :Cytopénies observées avec les bêta-lactamines, les sulfamides et sont en général réversibles. On estime à 1 % la prévalence d'une neutropénie induite par la prise de bêta-lactamines, chez les patients ayant une fonction hépatique normale et pour une prise de moins de 10 jours. À noter, les effets myélotoxiques des oxazolidinones, notamment pour les traitements au long coursDes anémies hémolytiques (immune, déficit en G6PD) sont décrites après la prise de sulfamides, de céphalosporines. La sulfasalazine peut provoquer une anémie mégaloblastiqueLes sulfamides peuvent entraîner une agranulocytose, une anémie hémolytique, une aplasie médullaire. Le chloramphénicol provoque rarement une aplasie médullaire ou une anémie sidéroblastique.Des coagulopathies sont décrites, en particulier des hypoprothrombinémies à la suite de la prise de bêta-lactamines, de cyclines mais aussi la prise de sulfamides, de chloramphénicol.La dapsone peut provoquer une méthémoglobinémie.Par ailleurs, selon un mécanisme allergique, les antibiotiques peuvent induire un syndrome d'hypersensibilité médicamenteuse, lequel s'accompagne en règle générale d'une hyperéosinophilie. Manifestations hépatiques Parmi les effets secondaires documentés au niveau hépatique, on peut citer :Hépatites cytolytiques avec les kétolides, l'isoniazide, la rifampicine,Hépatites cholestatiques avec l'amoxicilline-acide clavulanique, l'érythromicineL'acide fusidique ou la rifampicine peuvent entraîner une hyperbilirubinémie. Allergies Les antibiotiques sont parmi les médi"
médecine;Le corps humain est la  structure culturelle et physique d'un être humain. Le corps humain est constitué de plusieurs systèmes (nerveux, digestif, etc.), ainsi que de 206 os et 639 muscles dont 570 sont des muscles squelettiques. La science et la pratique visant à décrire l'organisation et le fonctionnement du corps humain est l'anatomie humaine, qui est une spécialité de la médecine. La médecine vise plus généralement à préserver la santé, c'est-à-dire le fonctionnement normal du corps humain.L'anatomie est la discipline consacrée à la description du corps humain et de ses différentes parties. L'anatomie macroscopique s'intéresse à sa surface et aux éléments visibles à l’œil nu, tandis que l'anatomie microscopique, ou histologie, suppose l'utilisation d'outils grossissants comme le microscope. L'anatomopathologie se focalise sur la morphologie du corps malade. La physiologie se consacre elle aux fonctions des différents éléments corporels. La position anatomique de référence utilisée afin d'uniformiser les descriptions est celle d'un corps en position debout, la tête droit, le regard fixe, les bras sur les côtés avec les paumes tournées vers l'avant et les pieds joints. La terminologie comprend des termes directionnels, servant à localiser les parties du corps les unes entre les autres, ainsi que des termes régionaux, utiles pour désigner ces parties. Le corps peut être divisé selon un plan médian, coronal ou transverse.Le corps se compose de différents niveaux de stratification et de complexité structurales. Le niveau fondamental est chimique, donc atomique et moléculaire. La plupart des biomolécules se composent de quatre éléments (le carbone, l’hydrogène, l'oxygène et l'azote). 4 % du poids corporel correspond à d'autres éléments, dont le potassium, le sodium, le calcium et le phosphore. Le corps d'un homme adulte contient 60% d'eau, ce qui représente environ 42 litres. Les cellules sont les plus petites unités indépendantes de matière vivante. La plupart sont spécialisés afin de répondre à une fonction précise. Il en existe environ 30 000 milliards de 300 types.Les tissus forment le niveau d'organisation intermédiaire entre la cellule et l'organe. Ils se composent d'un ensemble de cellules semblables et de même origine, sont regroupés en amas, réseau ou faisceau (fibre), se régénèrent et assurent la même fonction. Les tissus conjonctifs, épithéliaux, musculaires et nerveux sont les quatre types de tissus principaux, avec chacun leurs subdivisions. Constitués de plusieurs tissus, les organes, au nombre d'environ 80, réalisent une fonction physiologique unique et sont associés à un ou plusieurs systèmes. Par différence avec les appareils, qui remplissent un ensemble de fonctions complémentaires, les systèmes sont répartis dans l'ensemble de l'organisme. Ils sont constitués d'organes et de tissus assurant une même fonction vitale, comme la digestion ou la respiration.Anatomie humaineDroit à l'intégrité physiquePlanète corps, de Pierre-François Gaudry, Collection : Odyssée des sciences, 88 minutes, Mona Lisa, Universcience, l’Inserm, Smith&Nasht(en) Cet article est partiellement ou en totalité issu de l’article de Wikipédia en anglais intitulé « Composition of the human body » (voir la liste des auteurs).Janet S. Ross et Kathleen Jean Wilson Wallace (trad. de l'anglais, ill. Richard Tibbitts, édité par Anne Waugh & Allison Grant, coordination scientifique de l’édition française par Julie Cosserat), Anatomie et physiologie normales et pathologiques, Issy-les-Moulineaux, Elsevier Masson, 2019, 13e éd., XV-575 p. (ISBN 978-2-294-76408-0, BNF 45760534).Elaine N. Marieb et Katja Hoehn (trad. de l'anglais par Sophie Dubé), Anatomie et physiologie humaines, Montreuil, Pearsons, 2019 (ISBN 978-2-7661-0122-1, BNF 45798350), p. XXVI-1310.(en) Yoram Yom-Tov et Eli Geffen, « Geographic variation in body size : the effects of ambient temperature and precipitation », Oecologia, vol. 148, no 2,? juin 2006, p. 213-218 (PMID 16525785, DOI 10.1007/s00442-006-0364-9, lire en ligne, consulté le 30 mars 2021).Exploration d'un corps humain virtuel (ikonet)Portail de l'explorateur du corps humain de Google (Body Browse) Portail de l’anatomie   Portail de la biologie
médecine;"L'embryologie est une discipline scientifique qui englobe la description morphologique des transformations de l'œuf fécondé, le zygote, en organisme (embryologie morphologique) et l'étude de leur déterminisme (embryologie causale). « L'embryologie causale » est plus couramment désignée, depuis les années 1990 et l'avènement de la génétique moléculaire, par le terme de « biologie du développement » qui inclut aussi l'étude du développement post-embryonnaire.On peut diviser la formation embryonnaire (ou embryogénèse) en trois stades :le clivage, ou segmentation ;la gastrulation ;l'organogénèse.L'organogénèse, commune à tous les organismes, comporte plusieurs sous-étapes comme la neurulation ou la métamérisation, qui se déroulent parfois en même temps.La tératologie est l'étude des anomalies de l'embryon et du fœtus.Le modèle actuel du développement de l'embryon repose sur l'épigenèse qui stipule que celui-ci se développe de manière de plus en plus complexe en rapport direct avec son environnement. Au XVIIIe siècle, elle a été opposée à la théorie de la préformation qui voit l'embryon comme un être vivant « miniature » où tous les organes sont déjà présents.Il faut attendre le début du XIXe siècle et Karl Ernst von Baer (1792-1876) pour entrer véritablement dans l’embryologie moderne. Alors professeur à l’université de Königsberg, von Baer élabore entre 1819 et 1834 les travaux d'embryologie qui établiront sa notoriété à partir de l'étude de mammifères.Sa découverte la plus célèbre est celle de l'ovule, jusque-là confondu avec le follicule ovarien, chez les mammifères en 1827, venant après la mise en évidence, en 1824, du rôle fécondant des spermatozoïdes par Prevost (1790-1850) et Dumas (1800-1884).Le passage d'une embryologie descriptive à une véritable embryologie comparée est effectuée en 1828 par Von Baer. L’embryon est formé de trois feuillets à partir desquels se forment ultérieurement les organes ; les premiers stades sont semblables chez tous les animaux.Les feuillets embryonnaires sont l'objet de nouvelles découvertes que Von Baer décrit dans son ouvrage Über Entwickelungsgeschichte der Thiere (1828-1837).En 1866, Ernst Haeckel introduit la théorie de la récapitulation, aujourd'hui réfutée en grande partie, qui fait le parallèle entre la croissance d'un embryon et l'évolution de son espèce : selon lui, l'ontogenèse suit la phylogenèse. Par exemple, à un certain stade de leur développement, les organes qui formeront les nageoires de l'embryon d'un mammifère marin comme le dauphin présentent une conformation qui rappelle les pattes des animaux terrestres. Cette observation coïncide avec l'existence d'un ancêtre terrestre.Lorsque le spermatozoïde féconde l'ovocyte, le développement de l'œuf en animal commence.Ce développement se déroule en cinq grandes étapes : la segmentation, la gastrulation, l'organogenèse (comportant les phénomènes de délimitation, neurulation et métamérisation) et l'histogenèse.La segmentation : première phase du développement embryonnaire caractérisée par une suite de divisions rapides et rapprochées, à interphases très courtes. Ces divisions, qui sont en fait des mitoses singulières, fragmentent l'œuf en un ensemble de cellules nommées blastocytes - ou blastomères, dont la taille diminue à mesure des clivages. La segmentation n'engendre pas l'accroissement du diamètre de l'œuf, qui garde ainsi le même volume, et s'achève au stade morula.La gastrulation : mise en place dans l'embryon du disque embryologique tridermique équivalant aux trois feuillets fondamentaux (ectoderme, mésoderme et endoderme qui chez l'homme apparaissent dès la 3e semaine de développement), et de la chorde (structure médiale de l'embryon, inductrice primaire de beaucoup d'éléments, comme le tube neural, ou le gril costal. Elle est transitoire et donnera un vestige chez l'adulte : le nucleus pulposus). Ils dérivent des deux feuillets éphémères (épiblaste et hypoblaste) dont vont dériver les futurs organes.La neurulation : mise en place des ébauches neurales. L'embryon qui en est le siège est la neurula. C'est une phase caractéristique des chordés qui correspond à l'enroulement et à la soudure des bords externes de la gouttière neurale. La neurulation primaire aboutit à la formation du tube neural et de l'ampoule neurale, premières ébauches du système nerveux central. La neurulation secondaire aboutit à la formation du reste du système nerveux (filum terminale, etc.)La métamérisation : fragmentation ou bourgeonnement du mésoblaste. Dans la jeune neurula, les lames mésodermiques gauche et droite s'étendent de manière continue d'un bout à l'autre de l'embryon. Ces lames vont se découper en une série de segments successifs.ex. : chez les vertébrés la métamérisation engendre les somites et les vertèbres.L'histogenèse : formation des tissus par différenciation des cellules embryonnaires.Il y a donc formation de tissus par transformation des ébauches embryonnaires.Les grands types de tissus sont : les épithéliums (avec tissus glandulaires), les tissus musculaires, les tissus nerveux, les tissus conjonctifs (dont les tissus osseux, le tissu sanguin, les tissus conjonctifs communs, etc.).L’œuf est libéré par la femelle et fécondé au stade de deuxième division de méiose.La fécondation est le stade qui précède l’ontogenèse qui elle-même est le phénomène étudié en biologie du développement. La fécondation est par définition la fusion des gamètes qui sont chez cette espèce libérés dans le milieu aquatique. Suit l'amphimixie qui est l'accolement des pronoyaux sans fusion sous le contrôle des centrioles distal et proximal (originaires du spermatozoïde). Dès lors commence la segmentation de l'animal.Le clivage est qualifiée de pseudoholoblastique car la division de l'embryon se fait de façon proportionnée durant les premières phases de segmentation de l'animal puis de façon inégale à partir du stade 32 blastocystes. Dès lors on peut différencier les différents blastocystes par leurs tailles; ils sont nommés macromères au pôle végétatif et micromères, de plus petite taille, au pôle animal. L'embryon passe ensuite par le stade de morula (qui est ici une cœloblastula).Cette phase de l'ontogenèse aboutit à la formation des trois feuillets embryonnaires (chez les organismes triploblastiques comme l'échinoderme). Elle aboutit à la formation d'une gastrula qui possède par définition un archentéron permettant le commencement de la phase suivante de développement embryonnaire.L'organogenèse est le processus de formation des organes à partir des trois feuillets embryonnaires fondamentaux (ectoderme, endoderme et mésoderme). Elle se fait chez l’échinoderme par invagination de la plaque primaire afin de former l'archentéron. Étant un être deutérostomienCe premier orifice formé deviendra l'anus chez l'adulte. Lors de cette embolie est formé le squelette de l'animal (formé de spicules) à partir de mésenchyme primaire, ainsi que du mésenchyme secondaire. Puis le fond de l'archentéron va fusionner avec la plaque stomodéale du côté du pôle buccal de l'animal ce qui formera plus tard la bouche. Ainsi le tube digestif (qui s'étend de la bouche à l'anus) est formé. L'Organisme obtenu est une larve appelée pluteus qui subira la métamorphose, processus de transformation post-embryonnaire, pour former un organisme épithélioneurien comme l'étoile de mer ou encore l'oursin.Le terme d'Addendum Mammifères (ou Annexes embryonnaires) désigne les organes utilisés lors de l'embryogenèse et ne perdurant pas à l'âge adulte. Parmi ces annexes embryonnaires on peut citer le placenta (qui reste sans doute la plus communément connue) ou encore le diverticule allantoïdien. Elles ont un rôle prépondérant dans la distribution ou le stockage de réserves nutritives ou encore dans le recyclage des urines comme chez Gallus domesticus.On divise les placentas en deux ordres qui sont les décidués et les indécidués. Ces deux ordres sont divisés en deux types de placentation. On peut les lister ainsi : Indécidués  Epithélio-chorial Dans ce cas le placenta est qualifié de placenta diffus. Comme son nom l'indique les barrières histologiques entre mère et embryon sont au nombre de trois :Épithélium ;Conjonctif ;Endothélium.Les pachydermes, cétacés, équidés et suidés possèdent ce type de placenta. Conjonctivo-chorial Dans ce cas-ci le placenta traverse l'épithélium et se retrouve au sein du conjonctif maternel. On qualifie la placentation de cotylédonaire du fait de sa morphologie. Les ruminants possèdent ce type de placentation. Décidués  Endothélio-chorial Le capillaire embryonnaire est en contact avec l'endothélium maternel (qui constitue les vaisseaux sanguins situés à la périphérie du placenta). On qualifie ce placenta de zonaire. Il est présent chez tous les carnivores. Hémo-chorial Les contacts entre l'embryon et la mère est fait par des lacs sanguins qui permettent l'alimentation de l'embryon. Le placenta est alors qualifié de discoïdal. Les insectivores, chiroptères, rongeurs et primates (et donc humains) possèdent ce type de placentation.Raphaël Franquinet et Jean Foucrier, Embryologie descriptive, 2e édition.VitellusEmbryogenèse humaineCours d'embryologie en ligne à l'usage des étudiants et étudiantes en médecine (développé par les universités de Fribourg, Lausanne et Berne - Suisse)Site sur l'embryologie humaine (université Paris 5)Court-métrage avec prises de vue accélérées de la division et de la différenciation cellulaire chez un amphibien (Jan Van Ikjen) Portail de la biologie"
médecine;"La maladie est une altération des fonctions ou de la santé d'un organisme vivant.On parle aussi bien de la maladie, se référant à l'ensemble des altérations de santé, que d'une maladie, qui désigne alors une entité particulière caractérisée par des causes, des symptômes, une évolution et des possibilités thérapeutiques propres.Un ou une malade est une personne souffrant d'une maladie, qu'elle soit déterminée ou non. Lorsqu'elle fait l’objet d'une prise en charge médicale, on parle alors de patient(e).La santé et la maladie sont liées aux processus biologiques et aux interactions avec le milieu social et environnemental. Généralement, la maladie se définit comme une entité opposée à la santé, dont l'effet négatif est dû à une altération ou à une désharmonisation d'un système à un niveau quelconque (moléculaire, corporel, mental, émotionnel…) de l'état physiologique ou morphologique considérés comme normal, équilibré ou harmonieux. On peut parler de mise en défaut de l'homéostasie.Les termes maladie et malade proviennent du latin male habitus signifiant qui est en mauvais état.Ce terme est unique en français, italien et espagnol, alors que l'anglais et l'allemand disposent de doublons tels que illness et disease, Erkrankung et Krankheit qui expriment des distinctions particulières de sens.Il n'existe pas de terme commun désignant la maladie dans le groupe des langues indo-européennes, on note l'existence de nombreux synonymes dont la signification étymologique appartient à quatre champs sémantiques :la faiblesse, la perte de force, l'incapacité à travailler ;la difformité et la laideur ;la gêne, le trouble, le malaise ;la souffrance et la douleur.Le concept initial d'état morbide ou de maladie s'appuie sur un critère objectif (incapacité de fournir un travail pour soi ou pour la société), et un critère subjectif (de la gêne ou indisposition à la douleur aiguë).Ce concept n'est pas socialement neutre, car il implique un jugement moral et esthétique : il y a la maladie, mais aussi le mal, le mauvais, et le laid. Disease, illness, sickness En français, les termes « maladie » et « malade » sont utilisés de façon indistincte pour signifier « avoir une maladie » (reconnue par un médecin), « être malade » (se sentir mal), « être un malade » (être reconnu comme tel par l'entourage ou la société).L'anglais utilise trois termes, plus ou moins interchangeables, mais en principe utilisés le plus souvent dans un contexte spécifique. Disease se rapporte à une perturbation biomédicale, objectivée par une maladie reconnue par un médecin, dans le cadre d'une pathologie référencée (nosologie).Illness se rapporte à l'expérience vécue, personnelle et intime, de la maladie : « je me sens, ou je suis, malade ».Sickness se rapporte à la perception de la maladie dans le cadre de l'entourage non-médical (social ou culturel) : «je suis un malade» (reconnu comme tel). Limites et extensions Il a été montré en 1989 que plus les étudiants en médecine étaient avancés dans leur cursus plus ils avaient tendance à qualifier de maladie les conditions parmi 38 qui leur étaient présentées, sans que cette qualification n'ait de lien fort avec les propriétés de gravité, curabilité, responsabilité du patient ou causalité externe. L'idée de maladie, plutôt qu'être parfaitement définie, évolue donc chez l'étudiant en fonction de son avancement dans le cursus.Classifier un certain état comme une maladie est aussi un fait social d'évaluation. Ainsi, certains états ne sont reconnus comme des maladies que dans certaines cultures, ou à certaines époques, et pas dans d'autres. On parle alors de syndromes culturels. Parfois la catégorisation d'un état comme une maladie est controversé au sein d'une même société. L'hyperactivité et l'obésité sont par exemple des états de plus en plus considérés comme des maladies par l'opinion publique dans les pays occidentaux mais n'étaient pas ainsi considérés il y a encore quelques décennies, et ne le sont toujours pas dans certains pays.La maladie est à différencier des blessures, handicaps, syndromes et affections.Une blessure est une lésion, physique ou psychique.Un handicap est une déficience qui peut aussi bien être due à une maladie qu'à une blessure.Un syndrome est un ensemble de signes ou de symptômes qui apparaissent simultanément. Ainsi l'usage médical distingue une maladie, qui a une cause spécifique connue, d'un syndrome, qui ne se préoccupe pas des causes.Une affection désigne une altération de fonctions qui est rattachée à un organe spécifique et qui ne prend en compte ni les causes, ni les symptômes, ni le traitement. Tout comme les syndromes, elle est parfois distinguée d'une maladie.Par extension, on peut associer la maladie à des entités non biologiques pour signifier qu'elles sont altérées ou que leur fonctionnement n'est plus considéré comme bon. Il est ainsi habituel d'entendre les termes « société malade » ou « entreprise malade » par exemple.La maladie humaine est le noyau fondateur de la médecine, une grande partie de la connaissance médicale étant orientée vers la maladie et ses solutions.La science vétérinaire concerne les maladies qui affectent les animaux, dont les zoonoses.La phytopathologie est la science qui concerne les maladies qui affectent les plantes et autres sujets botaniques.La pathologie est la branche de la médecine traitant des causes et des symptômes des maladies dans leur ensemble. Le terme est souvent utilisé fautivement pour désigner la maladie elle-même, ou ses manifestations, y compris par des médecins.La pathogénie est l'étude des mécanismes responsables du déclenchement et du développement d'une maladie.L'ontologie est l'étude de la genèse des entités médicales telles que les maladies, les signes cliniques et les syndromes.L'étiologie est l'étude spécifique des causes et des facteurs d'une maladie.La séméiologie, ou sémiologie médicale, est la branche de la médecine qui traite des signes cliniques et des symptômes des maladies et de la façon de les présenter.Le diagnostic est la réflexion menant à l'identification de la nature d'une maladie à partir des symptômes relevés par les observations.Le pronostic est la prévision de la progression de la maladie et des chances éventuelles de guérison.La prophylaxie désigne le processus ou l'ensemble de mesures visant à prévenir la propagation ou l'apparition d'une maladie.La thérapeutique est la section de la médecine s'occupant de l'étude des traitements.La nosologie est la branche de la médecine qui étudie les critères de classification systématique des maladies.Les facteurs des maladies sont le domaine d'étude de l'étiologie et physiologie. Catégorisation des facteurs  Facteurs intrinsèques et extrinsèques Il existe de nombreux facteurs différents pouvant entraîner l'apparition d'une maladie.Ces facteurs peuvent être aussi bien intrinsèques qu'extrinsèques à l'organisme concerné par la maladie.La présence d'un facteur intrinsèque n'exclut pas celle d'un facteur extrinsèque, et inversement. Ainsi, de nombreuses maladies résultent d'une combinaison de facteurs intrinsèques et extrinsèques. Liste Les facteurs peuvent être répartis dans les catégories suivantes :Facteurs chimiquesFacteurs économiquesFacteurs sociauxFacteurs psychologiquesFacteurs biologiquesFacteurs environnementauxLes facteurs environnementaux incluent les produits chimiques toxiques (par exemple les acétaldéhydes dans la fumée de cigarette et les dioxines relâchées lors de l'utilisation d'Agent orange) et les agents infectieux (par exemple les virus de la varicelle ou de la polio).Certains facteurs peuvent faire partie de plus d'une catégorie. Facteurs biochimiques C'est le cas des causes biochimiques de maladies qui peuvent être considérées comme un spectre où à l'une des extrémités la maladie est causée exclusivement par des facteurs génétiques (par exemple les répétitions CAG dans le gène HD (ou gène huntingtine ou encore gène IT15) qui cause la maladie de Huntington) et à l'autre causée entièrement par des facteurs environnementaux.Entre ces deux extrêmes, gènes et facteurs environnementaux interagissent pour causer la maladie comme c'est le cas pour la maladie inflammatoire appelée maladie de Crohn où les gènes NOD2/CARD15 et la flore intestinale jouent chacun un rôle. L'absence de facteur génétique ou environnemental dans ce cas a pour résultat l'absence de manifestation de la maladie. Étude des facteurs environnementaux Les postulats de Koch peuvent être utilisés pour déterminer si une maladie est causée par un agent infectieux. L'émergence de nouvelles maladies infectieuses est liée aux activités humaines perturbant l'équilibre des écosystèmes.Par exemple, l'Institut de recherche pour le développement indique que « le déboisement des forêts primaires reste l'une des causes principales de l'apparition de nouveaux agents infectieux et de leur circulation épidémique dans les populations humaines ». En effet, les forêts jouent un rôle essentiel pour la biodiversité terrestre, élément stabilisateur des agents pathogènes. Étude des facteurs génétiques Pour déterminer si une maladie est causée par un facteur génétique, les chercheurs étudient la présence de la maladie dans l'arbre généalogique familial.Cela fournit des informations qualitatives à propos de la maladie, c'est-à-dire comment elle est héritée.Un exemple classique de cette méthode de recherche est l'héritage de l'hémophilie dans la famille royale britannique. Plus récemment cette méthode a été utilisée pour identifier le gène Apoliprotéine E (ApoE) comme un gène susceptible d'être lié à la maladie d'Alzheimer, bien que certaines formes de ce gène (ApoE2) en soient moins susceptibles.Pour déterminer jusqu'à quel point une maladie est causée par des facteurs génétiques, c'est-à-dire pour obtenir des informations quantitatives, des études sur des jumeaux sont effectuées. Les jumeaux monozygotes sont génétiquement identiques alors que les jumeaux dizygotes sont seulement génétiquement similaires. De plus des jumeaux, qu'ils soient monozygotes ou dizygotes, partagent souvent un environnement similaire. Ainsi en comparant l'incidence de la maladie (nommée taux de concordance) chez des jumeaux monozygotes avec l'incidence de la maladie chez des jumeaux dizygotes, la contribution de chaque gène à la maladie peut être déterminée.Les gènes suspects peuvent être identifiés grâce à plusieurs méthodes. L'une d'entre elles est la recherche de mutation d'un organisme modèle (par exemple les organismes Mus musculus, Drosophila melanogaster, Caenhorhabditis elegans, Brachydanio rerio et Xenopus tropicalis) qui possèdent un phénotype similaire à la maladie étudiée. Une autre approche est la recherche de ségrégation de gènes ou l'utilisation de marqueurs génétiques (par exemple les polymorphismes nucléotidiques et marqueurs de séquences exprimées). Maladies complexes Les maladies complexes sont dues à l'interaction entre un profil génétique particulier et un environnement particulier. Quelques exemples :ObésitéDiabète sucréHypertension artérielleAthérome et athéroscléroseAsthmeMaladies dysimmunitaires ou auto-immunesMaladies neurodégénératives (maladie d'Alzheimer, maladie de Parkinson, sclérose latérale amyotrophique)Un symptôme se distingue d'un signe. Le symptôme est l'expression subjective des effets ressentis par le malade alors que les signes en sont l'expression objective déduite par le médecin, ou plus généralement de la personne réalisant un diagnostic.Certaines maladies sont contagieuses ou infectieuses, comme c'est le cas par exemple de l'influenza (ou grippe). Les maladies infectieuses peuvent être transmises par un grand nombre de mécanismes, incluant l'expulsion de particules dans l'air lors d'un éternuement ou d'une toux, les fomites (objets contaminés par des pathogènes), les morsures et piqûres d'insectes ou autres animaux vecteurs porteurs de la maladie, et l'absorption d'eau ou de nourriture contaminée.Il existe également des infections ou maladies sexuellement transmissibles (MST ou IST). Ce sont des maladies infectieuses qui se transmettent au cours de rapports sexuels, ou de contacts sanguins. Au début du XXIe siècle, un des principaux représentants de ces maladies est le SIDA. Un représentant plus ancien est la syphilis.Certaines maladies sont dites non transmissibles, elle ne se transmettent pas directement. Il y a par exemple les maladies liées à l'environnement.Une des principales mesures permettant d'éviter la propagation d'une maladie parmi une population ou seulement le développement d'une maladie chez un individu est la prévention.Elle peut se décomposer en trois parties :La prévention, qui a pour but de réduire la probabilité d'apparition de la maladie (ex : vaccination).La prévision, qui doit prévoir des mesures pour combattre le sinistre si celui-ci survient.La protection, qui a pour but de limiter l’étendue et la gravité de la maladie ou de l'épidémie, lorsqu'elle est déjà présente (ex : amputation, quarantaine).En médecine, on parle plus particulièrement de prophylaxie, le processus qui vise à prévenir les épidémies et la propagation d'une maladie. La prophylaxie est, plutôt qu'un traitement médical, une promotion de la prise de conscience générale des bonnes conduites à adopter face à la maladie.Les principales mesures de prévention de la maladie sont l'amélioration de l'hygiène et la vaccination.La thérapeutique est la section de la médecine s'occupant de l'étude des traitements.Les traitements consistent souvent, suivant le niveau évolutif de la société humaine concernée, en la prise de médicaments à base de molécules de synthèse ou bien de remèdes produits à partir de l'environnement naturel. Il existe toutefois de nombreuses autres thérapies, telles la radiothérapie ou la kinésithérapie, n'ayant pas recours à l'ingestion et à l'injection de substances extérieures.L'identification d'un état comme une maladie, plutôt que comme une simple variation de la structure humaine ou de fonctions, peut avoir des implications sociales et économiques significatives et peut changer le statut social de l'être concerné.La maladie peut parfois entraîner l'exclusion sociale des personnes touchées. Un exemple est l'exclusion des lépreux, courante en Europe depuis le Moyen Âge, et leur regroupement dans des établissements appelés léproseries dans le but de limiter la propagation de la maladie par contagion.La peur de la maladie a été et est encore un phénomène social très répandu, bien que toutes les maladies, notamment les plus bénignes, n'aient pas ce genre de répercussions sociales.Dans certains pays, les maladies infectieuses les plus dangereuses, du point de vue du risque épidémique, sont des maladies à déclaration obligatoire, c'est-à-dire qu'elles doivent être déclarées aux autorités dès qu'elles sont diagnostiquées par le médecin ou le vétérinaire.Certains dispositifs ont également été mis en place dans de nombreux pays pour éviter ou compenser les effets néfastes de la maladie. C'est dans cette optique qu'est apparue l'assurance maladie, qui est un dispositif chargé d'apporter une compensation financière à un individu subissant ou ayant subi une maladie.Une dérive consiste à élargir les descriptions nosographiques des maladies tout en y sensibilisant le grand public afin d'augmenter le marché de certains fournisseurs de traitements contre ces mêmes maladie. Cette pratique est appelée le disease mongering.L'étude des différentes classifications de la maladie concerne la branche de la médecine appelée « nosologie ».Il existe différentes tentatives de classification des maladies. Toutefois, du fait de la constante évolution de la médecine, elles ne sont pas figées. Les maladies peuvent être catégorisées en fonction de leurs causes et facteurs, de leurs symptômes ou des fonctions et organes touchés. On parle alors respectivement de classification étiologique, nosographique et fonctionnelle. Classification étiologique Maladies par agents physiques (froid, chaleur, etc.)Maladies toxiques (produits chimiques, poisons, etc.)Maladies parasitaires (champignons, vers, etc.)Maladies infectieuses (virus, bactéries, etc.)Maladies traumatiques (chocs psychologiques ou physiques, brûlures, etc.)Maladies dyscrasiques (troubles des métabolismes, troubles génétiques, etc.)Maladies psychiques (facteurs psychiques, bien que ces maladies puissent aussi avoir les mêmes facteurs que les maladies précédentes) Classification fonctionnelle Dysfonctionnements moléculaires (au niveau de la molécule)Dysfonctionnements cellulaires (au niveau de la cellule)Dysfonctionnements organiques (au niveau de l'organe)Dysfonctionnements corporel (au niveau d'un système d'organes)Dysfonctionnements mental (au niveau psychologique)On peut également séparer les maladies en :maladies aiguës et maladies chroniques, suivant qu'elles aient un développement rapide ou étalé ;en maladies bénignes et maladies malignes, suivant leur gravité ;en maladies locales et maladies générales, suivant l'étendue de la zone touchée ;en maladies évitables et inévitables.L'Organisation mondiale de la santé publie et est responsable de l'évolution de la Classification internationale des maladies, poursuite des travaux de Jacques Bertillon. Cette classification permet le codage des maladies, des traumatismes et de l'ensemble des motifs de recours aux services de santé grâce aux codes CIM (ou ICD en anglais). Elle permet également l'analyse systématique et l'interprétation des causes de morbidité et de mortalité dans le monde entier. Son but est notamment l'organisation et le financement des services de santé.De nombreuses cultures ont tenté de donner une signification et une origine à la maladie.Dans la mythologie grecque, l'apparition de la maladie est expliquée par l'ouverture de la boîte de Pandore. Zeus, qui voulait se venger des hommes à la suite du vol du feu par Prométhée, ordonne la création de Pandore, femme qu'il envoie auprès du frère de ce dernier. Pandore apporte avec elle une boîte qu'il lui est interdit d'ouvrir. La curiosité la pousse à le faire tout de même et c'est ainsi qu'elle libère la maladie et les autres maux de l'humanité que la boîte contenait.Au Proche-Orient ancien, l'origine naturelle de la maladie est concevable, mais elle se rajoute à une origine surnaturelle, par exemple la colère des dieux, la première étant la conséquence de la seconde.À partir de 1860, la pensée tendait vers l'idée que les homosexuel(le)s souffraient plutôt d'une maladie. Cette position de la communauté médicale et scientifique a perduré jusque vers les années soixante, où plusieurs voix se sont manifestées pour remettre en question cette vision de l'homosexualité. En 1974, l'Association américaine de psychiatrie a éliminé l'homosexualité de sa liste des maladies mentales, le Manuel diagnostique et statistique des troubles mentaux. Le 17 mai 1990, c'était au tour de l'Organisation mondiale de la santé de prendre la même position et de retirer l'homosexualité de sa Classification internationale des maladies dans sa dixième version (CIM-10).La maladie a inspiré de nombreuses créations artistiques.Le personnage du malade tient par exemple la place centrale dans Le Malade imaginaire, la dernière comédie écrite par Molière.Mais aussiHôpital général, de Slaughter : Tous les aspects de la médecine y sont représentés : l’organisation hospitalière…L’Hôpital, d’Alfonse BoudardLe Pavillon des cancéreux de Soljenitsyne : le cancerLa Mort du pantin, de Pierre MoustiersUn cri, de Michèle LoriotLa Peste, de Camus : les épidémiesLe Hussard sur le toit de Giono : Le choléraOpération épidémie, de SlaughterLa Montagne magique de Thomas Mann : La tuberculoseUn grand patron, de Pierre Véry : la formation médicaleLe Destin de Robert Shanon, de CroninLe Médecin de campagne de Balzac : l’exercice de la médecineLe Docteur Pascal de ZolaVoyage au bout de la nuit de L.-F. CélineLes Hommes en blanc, d’André SoubiranLe Livre de San Michele, d’Axel MuntheSept morts sur ordonnance, de Georges Conchon : les problèmes morauxKnock, de Jules Romain : l’arrivisme, les tentations et dérives de la médecineLes Grandes Familles, de Maurice Druon : la tentation des honneurs avec le professeur LartoyLes Thibault, de Roger Martin du GardOscar et la Dame rose, d’Eric-Emmanuel Schmitt.Philippe Adam et Claudine Herzlich, Sociologie de la maladie et de la médecine (1994), Paris, Armand Colin, 2014.Marc Augé et Claudine Herzlich (dir.), Le Sens du mal. Anthropologie, histoire, sociologie de la maladie, Bruxelles, Éditions des archives contemporaines, coll. « Ordres sociaux », 1984.Philippe Batifoulier, Capital-Santé. Quand le patient devient client, Paris, La Découverte, 2014.Frédéric Bauduer, Histoires des maladies et de la médecine, Paris, Ellipses, coll. « Sciences humaines en médecine », 2017.Henri Bergeron et Patrick Castel, Sociologie politique de la santé, Paris, PUF, coll. « Quadrige », 2014.Max Blecher, Aventures dans l’irréalité immédiate, suivi de Cœurs cicatrisés, trad. d’Elena Guritanu, Paris, L’Ogre, 2015.Max Blecher, La Tanière éclairée, trad. par Georgeta Horodinca et Hélène Fleury, Paris, Maurice Nadeau, 1989.Norbert Elias, La Solitude des mourants (1982), trad. Sybille Muller, suivi de Vieillir et mourir : quelques problèmes sociologiques, trad. Claire Nancy, Paris, Christian Bourgois éditeur, 1987.Dr. Christophe Fauré, Vivre ensemble la maladie d'un proche, Albin Michel, 2002.Jean-Claude Fondras, Santé des philosophes, philosophes de la santé, Nantes, éditions nouvelles Cécile Defaut, 2014.Elodie Giroux et Maël Lemoine (dir.), Philosophie de la médecine. Santé, maladie, pathologie, Paris, Vrin, 2012.Xavier Guchet, La Médecine personnalisée. Un essai philosophique, Paris, Les Belles Lettres, 2016.Hervé Guibert, À l'ami qui ne m'a pas sauvé la vie, Paris, Gallimard, coll. « Folio », 1990.Céline Lefève, Lazare Benaroyo et Frédéric Worms (dir.), Les Classiques du soin, Paris, PUF, 2015.Thomas Mann, La Montagne magique (1924), trad. Maurice Betz, Paris, Le Livre de poche, 1991.Claire Marin, Violences de la maladie, violence de la vie, Paris, Armand Colin, 2008.Ruwen Ogien, Mes Mille et Une Nuits : la maladie comme drame et comme comédie, Paris, Albin Michel, 2017.Roselyne Rey, Histoire de la douleur, Paris, La Découverte, coll. « Histoire des sciences », 1993 ; nouvelle édition avec des postfaces de Jean Cambier et Jean-Louis Fischer, Paris, La Découverte, 2011.Susan Sontag, La Maladie comme métaphore (1977, 1978), trad. Marie-France de Paloméra, suivi de Le Sida et ses métaphores, trad. Bruce Matthieussent, Paris, Christian Bourgois éditeur, 1993.Virginia Woolf, De la maladie (1930), trad. Élise Argaud, Paris, Payot & Rivages, 2007.Ressources relatives à la santé : Orphanet (en) Classification internationale des soins primaires (en) Diseases Ontology (en) Medical Subject Headings (en) NCI Thesaurus (no + nn + nb) Store medisinske leksikon (cs + sk) WikiSkripta (fr) Site officiel de l'Organisation mondiale de la santé(fr) Site officiel du Ministère de la Santé, de la Jeunesse et des Sports français.(fr) (en) Site officiel de Santé Canada.(fr) Site officiel du Ministère de la Santé et des Services sociaux du Québec(fr) Site officiel du Service public fédéral de la santé belge.(fr) Site officiel du Ministère de la Santé du Congo-Kinshasa(fr) Site officiel du Ministère de la Santé du Luxembourg(fr) (en) (de) (it) Site officiel de l'Office fédéral de la santé publique suisse.(fr) Classification étionosographique des pathologies sur psychobiologie.ouvaton.org Portail de la médecine"
médecine;"La médecine (du latin : medicina, qui signifie « art de guérir, remède, potion »), au sens de pratique (art), est la science témoignant de l'organisation du corps (anatomie), son fonctionnement normal (physiologie), et cherchant à préserver la santé (physique comme mentale) par la prévention (prophylaxie) et le traitement (thérapie) des maladies. La médecine humaine est complémentaire et en synergie avec la médecine vétérinaire.La médecine contemporaine utilise l'examen clinique, les soins de santé, la recherche et les technologies biomédicales pour diagnostiquer et traiter les blessures et les maladies, habituellement à travers la prescription de médicaments, la chirurgie ou d'autres formes de thérapies.Il n'existe pas suffisamment de données fiables pour déterminer le début de l'usage des plantes à des fins médicinales (phytothérapie). Les données médicales contenues dans le Papyrus Edwin Smith peuvent être datées du XXXe siècle av. J.-C.. Les premiers exemples connus d’interventions chirurgicales ont été réalisés en Égypte aux alentours du XXVIIIe siècle av. J.-C. (voir chirurgie). Imhotep sous la troisième dynastie est parfois considéré comme le fondateur de la médecine en Égypte antique et comme l'auteur originel du papyrus d’Edwin Smith qui énumère des médicaments, des maladies et des observations anatomiques. Le papyrus gynécologique Kahun traite des maladies des femmes et des problèmes de conception. Nous sont parvenues trente-quatre observations détaillées avec le diagnostic et le traitement, certains d'entre eux étant fragmentaires. Datant de 1800 av. J.-C., il s’agit du plus ancien texte médical, toutes catégories confondues. On sait que des établissements médicaux, désignés par l’expression Maisons de vie ont été fondés dans l’Égypte antique dès la première dynastie.Les plus anciens textes babyloniens sur la médecine remontent à l’époque de l’ancien empire babylonien dans la première moitié du IIe millénaire av. J.-C. Cependant, le texte babylonien le plus complet dans le domaine de la médecine est le Manuel de diagnostic écrit par Esagil-kin-apli le médecin de Borsippa, sous le règne du roi babylonien Adad-ALPA-iddina (1069-1046 av. J.-C.).Hippocrate, est considéré comme le père fondateur de la médecine moderne et rationnelle,, et ses disciples ont été les premiers à décrire de nombreuses maladies. On lui attribue la première description des doigts en baguette de tambour, un signe important pour le diagnostic de la bronchopathie chronique obstructive, du cancer du poumon et des cardiopathies cyanogènes congénitales. Pour cette raison, le symptôme des doigts en baguette de tambour est parfois appelé hippocratisme digital . Hippocrate a également été le premier médecin à décrire la face hippocratique. Shakespeare fait une allusion célèbre à cette description dans sa relation de la mort de Falstaff dans Henry V, acte II, scène III,. Le Corpus hippocratique popularise la théorie des humeurs. La médecine rationnelle grecque et latine coexiste cependant pendant toute l'Antiquité avec les cultes des Dieux guérisseurs.Agnodice (Hagnodice) ou Hagnodikè (en grec ancien : ????????) fut, selon une légende grecque rapportée par Hygin (Caius Julius Hyginus) dans la 274e de ses Fabulae, l'une des premières femmes médecin et gynécologue. Issue de la haute société athénienne, elle se déguisa en homme pour suivre les cours de médecine du célèbre médecin Hérophile. Vers 350 av. J.-C., elle passa l'examen et devient gynécologue, mais sans révéler qu'elle était une femme.La médecine pratiquée et enseignée en occident a ses racines dans les connaissances acquises et protocolées de l'Antiquité au Ier millénaire av. J.-C. de l'Orient à l'Empire romain.Elles proviennent de la Torah, étonnement rationnelle en la matière, car tenant compte des conditions climatiques. En effet, les cinq livres de Moïse qui la constituent, contiennent diverses « lois » ayant des conséquences directes sur la santé à travers différents rituels, tels que l'isolement des personnes infectées (Lévitique 13:45-46), le lavage des mains après avoir manipulé un cadavre (Livre des Nombres 19:11-19) et l’enfouissement des excréments à l’extérieur du campement (Deutéronome 23:12-13).La traduction dans les années 830-870 de 129 œuvres du médecin grec Galien (1er siècle av J.C.) en arabe par Hunayn ibn Ishaq et ses élèves sert de modèle à la médecine des civilisations islamiques et se propage rapidement à travers l’Empire arabe, reprenant en particulier, l'insistance de Galien sur une approche rationnelle et systématique de la médecine. Qusta ibn Luqa joua aussi un rôle important dans la traduction et la transmission des textes grecs. Les médecins musulmans ont mis en place certains des premiers hôpitaux, institution qui importée en Europe à la suite des croisades.En Europe occidentale, l'effondrement de l'autorité de l’empire romain a conduit à l’interruption de toute pratique médicale organisée. La médecine était exercée localement, alors que le rôle de la médecine traditionnelle augmentait, avec ce qui restait des connaissances médicales de l'antiquité. Les connaissances médicales ont été préservées et mises en pratique dans de nombreuses institutions monastiques qui s’étaient souvent adjoint un hôpital et disposaient de carrés d'herbes médicinales. Une médecine professionnelle organisée est réapparue, avec la fondation de l’école de médecine de Salerne en Italie au XIe siècle qui, en coopération avec le monastère du Mont Cassin, a traduit de nombreux ouvrages byzantins et arabes.À partir du XIe siècle, l'Église veut dissocier la vocation de moine de la profession de médecin. La volonté d'encadrer le savoir aboutit à la formation d'universités aux mains des ecclésiastiques. Les médecins de l'université de médecine de Montpellier, dépositaires des doctrines des médecins juifs et arabes, privilégient les plantes, ceux de l'Ancienne université de Paris privilégient la purge et la saignée.Au XIXe siècle, Karl August Wunderlich publie Das Verhalten der Eigenwärme in Krankheiten, qui établit que la fièvre est seulement un symptôme et met fin au credo d'une maladie infectieuse jusqu'alors nommée « fièvre intermittente ». En 1881 Theodor Billroth réalise la première gastrectomie, il révolutionne la chirurgie du pharynx et de l'estomac. En utilisant l'analyse statistique, le médecin Pierre-Charles Alexandre Louis (1787-1872) montre que l'utilisation des saignées chez les malades atteints de pneumonie n'est pas bénéfique mais néfaste. Ceci esquisse la notion d'étude randomisée en double aveugle.Madeleine Brès (1842-1921) est la première femme de nationalité française à accéder aux études de médecine en 1868, mais sans avoir le droit d'accéder aux concours. Elle obtient son doctorat en médecine, en 1875 et devient gynécologue et pédiatre. Elle démontre dans sa thèse que le lait du nourrisson se modifie au cours de l'allaitement et crée une des premières crèches parisiennes. Elizabeth Garrett Anderson, britannique la devance de cinq ans en France dans l'obtention de son doctorat.En 1854, Florence Nightingale est la première à utiliser les statistiques pour réorganiser les soins aux blessés de la guerre de Crimée et faire baisser la mortalité des soldats,,.Le 25 novembre 1901, Aloïs Alzheimer décrit le tableau clinique de la maladie qui porte son nom, dont il n'existe toujours aucun traitement connu à ce jour. Les traitements médicaux font des progrès spectaculaires avec l'invention de nouvelles classes de médicaments. Felix Hoffmann dépose le brevet de l'aspirine le 6 mars 1899. En 1909, le Nobel de médecine Paul Ehrlich invente la première chimiothérapie en créant un traitement à base d'arsenic contre la syphilis. En 1921 Frederick Banting de l'université de Toronto isole l'insuline et invente un traitement du diabète sucré. Le premier antibiotique date de 1928 avec la découverte de la pénicilline par Alexander Fleming.Selon la psychanalyste argentine Raquel Capurro, la médecine a été le premier domaine influencé par le positivisme d'Auguste Comte, à partir du milieu du XIXe siècle, à travers des personnalités telles que le docteur Robinet parmi d'autres.La délimitation de ce qui est médecine et de ce qui ne l'est pas est source de débat.La plus grande partie de cet article traite de la médecine telle qu'elle s'est développée à partir de l'époque moderne, et pratiquée à partir du XIXe siècle. Les innovations majeures apportées par la médecine occidentale à partir du XIXe siècle (anesthésie et asepsie puis vaccination et antibiotiques au XXe siècle), ses succès, ainsi que sa diffusion à travers le monde par le biais notamment de la colonisation par l'Occident vont inciter à poser, dès la fin du XIXe siècle, la médecine scientifique occidentale comme modèle de médecine faisant autorité, lequel s'est diffusé au niveau mondial à travers son industrialisation au XXe siècle.Certains chercheurs réhabilitent de même certains aspects de la médecine médiévale occidentale. Ainsi l'historien de la médecine Roger Dachez qui met en valeur l'aspect préventif et la vision globale qu'avait de la médecine le Moyen Âge.De même, toujours à la fin du XXe siècle, notamment sous l'effet de la mondialisation, les médecines traditionnelles ou non occidentales ont vu leur place reconnue au sein de la médecine mondiale : en 2002, l'organisation mondiale de la santé a ainsi mis en place sa première stratégie globale en matière de médecine traditionnelle.On identifie ainsi, à côté de la médecine occidentale, d'autres types de médecines, dites « alternatives » incluant : médecine chinoise, médecine tibétaine traditionnelle, médecine ayurvédique, médecine traditionnelle, et médecine non conventionnelle.En Occident, l'usage de médecines alternatives et complémentaires est constaté dans certaines conditions où les traitements de biomédecine semblent inefficaces, notamment dans le cas de maladies chroniques.Les étapes de l'acte médical sont formées de :l'étiologie qui désigne l'étude des causes de la maladie ;la pathogénie ou pathogenèse qui désigne l'étude du mécanisme causal ;la physiopathologie qui désigne l'étude des modifications des grandes fonctions au cours des maladies ;la sémiologie qui désigne l'étude de l'ensemble des signes apparents. Elle est apparentée à ce qui est nommée la clinique, opposée à la para-clinique qui sont les résultats des examens complémentaires. Face à la complexité croissante des techniques d'imagerie, il s'est développé une sémiologie des examens complémentaires ;le diagnostic qui désigne l'identification de la maladie ;le diagnostic différentiel qui désigne la description des maladies comportant des signes proches et qui peuvent être confondues ;la thérapeutique qui désigne le traitement de la maladie ;le pronostic qui désigne l'anticipation de l'évolution de celle-ci ;la psychologie qui désigne la partie de la philosophie qui traite de l’âme, de ses facultés et de ses opérations. La psychologie du patient est un élément important de la réussite du processus médical. Comme le dit dès 1963 l'historien de la médecine Jean Starobinski, « une médecine vraiment complète ne se borne pas à cet aspect technique ; s'il accomplit pleinement son métier, le médecin établit avec son patient une relation qui satisfera les besoins affectifs de ce dernier. L'acte médical comporte donc un double aspect : d'une part les problèmes du corps et de la maladie font l'objet d'une connaissance qui n'est pas différente de celle que nous prenons du reste de la nature - et l'organisme du patient est alors considéré comme une « chose » vivante capable de réagir conformément à des lois générales ; d'autre part, le rapport thérapeutique s'établit entre deux personnes, dans le contexte d'une histoire personnelle - et la médecine devient alors cette fois un art du dialogue, où le patient s'offre comme un interlocuteur et comme une conscience alarmée ». Georges Canguilhem écrivait lui que « l’acte médicochirurgical n’est pas qu’un acte scientifique, car l’homme malade n’est pas seulement un problème physiologique à résoudre, il est surtout une détresse à secourir ». Une décision médicale doit tenir compte à la fois des données de la science, mais également des préférences des patients et de l’expérience du praticienEn travaillant ensemble comme une équipe interdisciplinaire, de nombreux professionnels de la santé hautement qualifiés sont impliqués dans la prestation des soins de santé modernes. Voici quelques exemples : les infirmiers, les techniciens médicaux d'urgence et les ambulanciers, les scientifiques de laboratoire, pharmaciens, podologues, physiothérapeutes, inhalothérapeutes, psychologues, orthophonistes, ergothérapeutes, radiologues, des diététiciens, des bioingénieurs, des chirurgiens et des vétérinaires.Un patient admis à l'hôpital est habituellement sous les soins d'une équipe spécifique en fonction de leur problème de présentation principale, par exemple, l'équipe de cardiologie, qui peut ensuite interagir avec d'autres spécialités, par exemple, la chirurgie, la radiologie, pour aider à diagnostiquer ou traiter le problème principal ou des complications ultérieures. Les médecins ont de nombreuses spécialisations et sous-spécialisations dans certaines branches de la médecine, qui sont énumérés ci-dessous. Il existe des variations d'un pays à l'autre en ce qui concerne les spécialités et les sous-spécialités.Les principales branches de la médecine sont :les sciences fondamentales ;les spécialités médicales ;les domaines interdisciplinaires, comme les humanités médicales.L'anatomie : étude de la structure physique des organismes. Contrairement à l'anatomie macroscopique ou brute, la cytologie et l'histologie sont concernés par des structures microscopiques.La biochimie : étude de la chimie qui se déroule dans les organismes vivants, en particulier la structure et la fonction de leurs composants chimiques.La biologie moléculaire : étude des mécanismes moléculaires des processus de réplication, de transcription et de traduction du matériel génétique.La biomécanique : étude de la structure et des mouvements des systèmes biologiques au moyen de la mécanique.La biophysique : science interdisciplinaire qui utilise les méthodes de la physique et de la chimie physique pour étudier les systèmes biologiques.La biostatistique : application des statistiques à des champs biologiques dans le sens le plus large. Une connaissance de la biostatistique est essentiel dans la planification, l'évaluation et l'interprétation de la recherche médicale. Il est également fondamental de l'épidémiologie et de la médecine fondée sur des preuves (EBM).La cytologie : étude des cellules.L'embryologie : étude du développement précoce des organismes.L'épidémiologie : étude de la démographie des processus de la maladie, et inclut, mais sans s'y limiter, l'étude des épidémies.La génétique : étude des gènes, et leur rôle dans l'héritage biologique.L'histologie : étude des structures des tissus biologiques par microscopie optique, la microscopie électronique et l'immunohistochimie.L'immunologie : étude du système immunitaire, qui comprend le système immunitaire inné et adaptatif.La microbiologie : étude des micro-organismes, y compris les protozoaires, les bactéries, les champignons, les virus et les prions.La neuroscience : étude du système nerveux.La nutrition (mise au point théorique) et la diététique (orientation pratique) : étude de la relation entre la nourriture et des boissons à la santé et à la maladie, en particulier dans la détermination d'une alimentation optimale. thérapie nutritionnelle médicale se fait par des diététistes et est prescrit pour le diabète, les maladies cardiovasculaires, le poids et les troubles alimentaires, les allergies, la malnutrition et les maladies néoplasiques.La pathologie en tant que science : étude des maladies, de leurs causes, progressions et traitements.La pharmacologie : étude des médicaments et de leurs actions.La physiologie : étude du fonctionnement normal de l'organisme et les mécanismes de régulation sous-jacents. La physiologie peut être subdivisée (physiologie cardiaque, endocrinienne…).La physique médicale : étude des applications des principes de physique en médecine.La toxicologie : étude des effets nocifs des médicaments et des poisons. Par pratique l'anatomopathologie : étude microscopique des tissus malades ;l'anesthésie-réanimation : l'anesthésie qui est la médecine péri-opératoire, la réanimation qui est la prise en charge des malades présentant au moins deux défaillances d'organe ou une nécessitant une technique de suppléance ;la biologie médicale ;la chirurgie : thérapeutique médicale qui comporte une intervention mécanique au sein même des tissus ;l'éducation de la santé ;la médecine esthétique : type de soins visant à améliorer l'aspect plastique du patient ;la médecine générale (médecine de famille) ;la médecine du travail : médecine préventive consistant à éviter toute altération de la santé des travailleurs du fait de leur travail, notamment en surveillant les conditions d'hygiène du travail, les risques de contagion et l'état de santé des travailleurs ;la médecine d'urgence : médecine hospitalière (service des urgences) et extrahospitalière (Samu), traitement des urgences vitales ;la nutrition : prise en charge du métabolisme et de l'alimentation ;la pharmacie : dispensation des médicaments et prise en charge pharmaco-thérapeutique ;la radiologie, spécialité de l'imagerie médicale. Par type de patient L'andrologie : médecine de l'homme, prise en charge des maladies spécifiques du sexe masculin ;la gynécologie : spécialité médicochirurgicale, dont l'activité variée inclut notamment la médecine de la femme, le suivi gynéco-obstétrical et les cancers des organes génitaux féminins ainsi que des seins ;l'obstétrique : médecine de la femme enceinte. À noter la pratique médicale à part entière des sages-femmes, qui se consacrent à la surveillance de la grossesse normale ;la médecine fœtale : médecine du fœtus grâce à l'apparition de méthodes d'explorations de la vie intra-utérine (échographie, Doppler, amniocentèse) ;la médecine légale : recherche des causes de la mort sur un cadavre (nécropsie) et rédaction d'un rapport pour la Justice ;la pédiatrie : médecine des enfants, domaine très large et englobant généralement la génétique clinique ;la néonatologie : médecine et réanimation des nouveau-nés et des prématurés ;la gériatrie : médecine des personnes âgées ;la médecine des gens de mer : médecine des marins et travailleurs de la mer.la médecine vétérinaire : médecine des animaux. Par organe L'angiologie : médecine des vaisseaux ;la cardiologie : médecine des maladies du cœur et du système vasculaire ;la dermatologie : médecine des maladies de la peau ;l'endocrinologie : médecine des maladies des glandes, des anomalies hormonales, des troubles de la nutrition et des métabolismes ;l'hématologie : médecine des maladies du sang ;l'hépato-gastro-entérologie : aussi appelée gastroentérologie, médecine des maladies de l'appareil digestif dans son ensemble, incluant celles du tube digestif et celles du foie, du pancréas, ainsi que de la paroi abdominale. La gastroentérologie comprend également les activités d'endoscopie digestives, soit haute (endoscopie œsogastroduodénale), soit basse (iléocoloscopie) ;l'immunologie : médecine des maladies ou des troubles du système immunitaire ;la néphrologie : médecine des maladies des reins ;la neurologie : médecine des maladies du système nerveux ;l'odontologie : soins des dents ;l'ophtalmologie : médecine des maladies des yeux, de l'orbite et des paupières ;l'orthopédie : discipline chirurgicale traitant les affections de l'appareil locomoteur ;l'oto-rhino-laryngologie (ORL) : médecine des maladies des oreilles, du nez et de la gorge ;la pneumologie : médecine des maladies de la plèvre, des bronches et des poumons ;la proctologie : médecine des maladies du rectum et de l'anus ;la rhumatologie : discipline médicale traitant les affections de l'appareil locomoteur ;la stomatologie : médecine des maladies de la bouche ;l'urologie : médecine de l'appareil urinaire. Par affection L'addictologie : médecine des dépendances, regroupant l'alcoolisme, le tabagisme et la toxicomanie (branche de la psychiatrie selon certains) ;l'alcoologie : médecine des troubles liés à l'alcool ;l'allergologie : médecine des allergies ;la cancérologie ou oncologie : médecine des cancers (comprenant la chimiothérapie des tumeurs) associée avec la radiothérapie : traitement des tumeurs par radiations ionisantes ;la diabétologie : médecine des diabètes ;l'infectiologie : médecine des maladies infectieuses ;la psychiatrie : médecine des troubles comportementaux, psychiques et des maladies mentales ;la toxicologie : traitement des empoisonnements et intoxications ;la traumatologie : traitement des patients ayant subi de graves blessures, généralement accidentelles ;la vénérologie : médecine faisant l'étude des maladies transmises par l'acte sexuel. Types de chirurgie Chirurgie cardiaqueChirurgie digestiveChirurgie de la face et du cou (cervico-faciale)Chirurgie généraleChirurgie pédiatriqueChirurgie orthopédiqueChirurgie dentaireChirurgie plastique, reconstructrice et esthétiqueChirurgie thoraciqueChirurgie urologique (Urologie)Chirurgie vasculaireChirurgie viscéraleNeurochirurgieTechniques chirurgicales Divers Anatomie et cytologie pathologiques (voir anatomopathologie)Anesthésie-réanimationBiologie médicaleGénétiqueGynécologie obstétriqueInformatique Médicale et Technologies de l'InformationMédecine généraleMédecine interneMédecine hyperbareMédecine nucléaireMédecine nutritionnelle (voir nutrition)Pathologie (pays anglophones)PédopsychiatrieMédecine physique et de réadaptationSanté publiqueLes académies de médecineles Centers for Disease Control and Prevention, soit « centres de contrôle et de prévention des maladies »les hôpitauxles organismes de recherche médicaleles organismes publicsles Conseils de l'Ordre de médecinsl'Agence européenne des médicamentsUne profession de la santé est une profession dans laquelle une personne exerce ses compétences ou son jugement ou fournit un service lié au maintien ou l'amélioration de la santé des individus, ou au traitement ou soins des individus blessés, malades, souffrant d'un handicap ou d'une infirmité. Des exemples de profession peuvent notamment inclure : médecin, pharmacien, chirurgien-dentiste, sage-femme, masseur-kinésithérapeute, physiothérapeute, ergothérapeute, psychomotricien, infirmier, podologue, aide-soignant, ambulancier, et attaché de recherche clinique.Chaque profession possède son propre cursus de formation. En plus des études permettant d'exercer la profession de médecin dont l'organisation varie selon les pays, on trouve donc notamment les études en soins infirmiers, et les études de pharmacie.L'étudiant en médecine s'appelle carabin.Les apports de la médecine, particulièrement de la médecine occidentale depuis le XIXe siècle, se mesure notamment par l'allongement de la durée de la vie, l'espérance de vie en bonne santé, la réduction de la mortalité infantile, et l'éradication ou la capacité technique d'éradication de très anciennes épidémies (tuberculose, peste, lèpre, etc.). Ces progrès se poursuivent comme avec les succès de nouvelles thérapies (ou actes chirurgicaux) sur des pathologies considérées encore incurables il y a une quinzaine d'années (comme certains cancers et maladies auto-immunes).La médecine n'est pas une science exacte, et l'acte médical peut parfois affecter la personne humaine de manière négative, par exemple via :des « effets secondaires » ou indésirables de médicaments ou traitements, qui devront pour certains (Distilbène par exemple) être supportés par plusieurs générations. La recherche de ces effets se fait par pharmacovigilance ;l'antibiorésistance est due à la sélection de souches bactériennes résistantes à divers antibiotiques à cause d'un usage non raisonné de ces derniers ;les maladies nosocomiales peuvent apparaître en hôpital à cause de la concentration de malades. La forte pression exercée par les traitements ainsi que par les désinfectants et antiseptiques sur ce « pot pourri » de germes amène à long terme à l’émergence d'agents infectieux résistants qui pourront infecter facilement les malades déjà affaiblis ;les résultats de maladresses, d'erreurs médicales, de défauts d'organisation, de prises excessives de médicaments ou de traitements inadaptés. Un trouble ou une maladie est dite iatrogène lorsqu'elle est provoquée par un acte médical ou par les médicaments, même en l’absence d’erreur du médecin, du soignant, du pharmacien ou tout autre personne intervenant dans le soin. En France, 4 % des hospitalisations sont consécutives à des soins, et 40 % de ces cas seraient évitables. Ces problèmes comprennent une partie des maladies nosocomiales dont les plus fréquentes sont les infections nosocomiales.De nombreux progrès sont annoncés ou espérés dans les années à venir, en matière de santé-environnement, d'épidémiologie, d'allongement de la durée de vie, si ce n'est de la durée de vie en bonne santé. La médecine prédictive, le clonage, les cellules-souches posent des questions nouvelles en termes de bioéthique.Des défauts d'anticipation font que, par exemple en France, en 2025, alors que la population aura augmenté (et la population âgée plus encore), le nombre de médecins aura diminué de 10 % et la densité médicale de 15 %, à la suite du non-remplacement des médecins baby-boomers induit par les quotas d’accès aux études de médecine dans les années 1970 à 1990. La médecine libérale devrait perdre 17 % de ses effectifs, et le secteur salarié 8 %, sauf en milieu hospitalier où le ministère envisage une hausse de 4 % ; 13 % des généralistes auront disparu, contre 7 % pour les spécialistes (ophtalmologistes, oto-rhino-laryngologistes et psychiatres surtout). La faible « densité médicale » augmentera aussi le coût des soins, l’impact des déplacements en termes de pollution (et secondairement de santé) et pourrait diminuer l'efficience médicale (une moindre densité médicale augmente la mortalité), d'autant plus que les patients sont plus pauvres.Cet article est partiellement ou en totalité issu de l'article intitulé « Histoire de la médecine » (voir la liste des auteurs).(en) Charles Singer et E. Ashworth Underwood, A Short History of Medicine, New York et Oxford, Oxford University Press, 1962(en) Roberto Margotta, The Story of Medicine, New York, Golden Press, 1968.(en) Roberta Bivins, Alternative Medicine? : A History, Oxford University Press, 5 octobre 2007, 264 p. (ISBN 978-0-19-156881-7, lire en ligne)(en) Robert A. Schwartz, Gregory M Richards et Supriya Goyal, « Clubbing of the Nails », Medscape Reference,? 28 février 2012 (lire en ligne, consulté le 11 juin 2012).Stanis Perez, Histoire des médecins. Artisans et artistes de la santé de l'Antiquité à nos jours, Perrin, 2015, 470 pages.Ressource relative à la littérature : (en) The Encyclopedia of Science Fiction Ressource relative à la santé : (en) Medical Subject Headings Haute Autorité de Santé : recommandations, conférences de consensus, etc. (France)Code de la santé publique (France)Service Public Fédéral (SPF) Santé publique, Sécurité de la Chaîne alimentaire et Environnement (Belgique)CISMeF : annuaire de sites médicaux Internet francophoneBase de données de publications médicales(en) Medline, base de données de publications médicales Portail de la médecine"
médecine;"La physiologie (du grec ?????, phusis, la nature, et ?????, logos, l'étude, la science) étudie le rôle, le fonctionnement et l'organisation mécanique, physique et biochimique des organismes vivants et de leurs composants (organes, tissus, cellules et organites cellulaires). La physiologie étudie également les interactions entre un organisme vivant et son environnement. Dans l'ensemble des disciplines biologiques, en définissant schématiquement des niveaux d'organisation, la physiologie est une discipline voisine de l'histologie, de la morphologie et de l'anatomie.La physiologie regroupe des processus qu'elle étudie en grandes fonctions qui sont :les fonctions de nutrition ;la fonction de reproduction ;les fonctions de relation : la locomotion et les fonctions sensorielles (voir les articles détaillés dans la liste ci-dessous).Le terme physiologie a aussi été utilisé au XIXe siècle par les écrivains réalistes pour qualifier de petites études de mœurs de personnage typiques comme les concierges, les curés de campagne, le bagnard ou la femme de trente ans dont certains sont regroupés dans l’ouvrage Les Français peints par eux-mêmes. Balzac a publié Physiologie du mariage en 1829.L’étude de la physiologie humaine remonte à au moins 420 av. J.-C. avec Hippocrate. La pensée critique d'Aristote et son accent sur la relation entre la structure et la fonction a marqué le début de la physiologie dans la Grèce antique, tandis que Claude Galien est le premier à réaliser des expériences pour étudier le fonctionnement de l'organisme, faisant de lui le fondateur de la physiologie expérimentale.Au XVIIe siècle naît la « première révolution biologique » : le cabinet d'études du physiologiste s'équipe de nombreux instruments de mesure (balance thermomètre, baromètre) qui permettent de mesurer les paramètres biologiques des animaux sacrifiés mais les résultats de ces études ne sont pas mis à profit par les médecins qui appliquent toujours le Primo saignare, deinde purgare, postea clysterium donare (« d'abord saigner, ensuite purger, postérieurement seringuer »). Lui succède au XIXe siècle une seconde révolution, la médecine expérimentale dont les bases ont été formulées et théorisées par le physiologiste français Pierre Rayer puis par son élève Claude Bernard.La physiologie comporte plusieurs subdivisions regroupées en divers articles :L'électrophysiologie est la partie de la physiologie qui mesure les courants électriques des cellules. Les phénomènes électriques sont nombreux et variés dans l'organisme, en particulier dans les tissus excitables (muscle, système nerveux central), le cœur, le rein ainsi que certaines glandes.Le système nerveux autonome est un système en réseau formé des organes des sens, des nerfs, du cerveau, de la moelle épinière, etc. Avec le système endocrinien (qui est l'ensemble des glandes sécrétant des hormones), il assure l'homéostasie de l'organisme en agissant par des impulsions électriques exerçant une action sur les muscles ou les organes. Neurophysiologie La neurophysiologie, physiologie du cerveau et des cellules nerveuses (neurone et cellule gliale), est la partie de la physiologie qui traite du système nerveux pouvant être séparé en deux parties :système nerveux central ;système nerveux périphérique. Physiologie sensorielle PerceptionGoûtOdoratOuïeAudition humaineOreilleVueŒilSomesthésieLe système reproducteur chez les humains est l'ensemble des organes qui concourent à la reproduction d'un organisme. Le développement du système reproducteur et son bon fonctionnement dépendent de glandes sécrétant des hormones endocrines.Appareil reproducteurReproduction (biologie)Physiologie de la reproductionMenstruationLe système circulatoire, dont l'organe moteur est le cœur, transporte les matières chimiques, les gaz respiratoires et la chaleur dont l'organisme a besoin. Il sert donc au maintien de l’homéostasie. Il est composé de deux sous-systèmes :l'appareil cardiovasculaire :cœursangcirculation sanguinele système lymphatique :lympheLe système circulatoire est essentiel au fonctionnement des autres systèmes, respiratoire, nutritif, immunitaire, endocrinien et thermorégulateur.Pour un organisme animal, le système respiratoire permet l’approvisionnement des cellules en oxygène et le rejet du CO2. Le système respiratoire assure ces échanges de gaz vitaux au niveau des poumons ; tandis que le système circulatoire les transporte des cellules aux poumons.PoumonBroncheLobe pulmonaireRespirationRespiration humaineVentilation pulmonaireRéflexe (réaction motrice)Activités posturalesMouvement volontaireMuscleSqueletteLe système digestif a pour fonction de transformer les aliments en des formes physiques et chimiques capables d'être absorbées et transportées dans le système circulatoire (sang et lymphe) pour répondre aux besoins en glucides, lipides, protéines, vitamines, sels minéraux et eau des cellules d'un organisme.NutritionDigestionRéserves énergétiquesExcrétionLa thermorégulation permet à un organisme de conserver une température constante. Elle est le résultat de productions et de déperditions de chaleur. On distingue les organismes homéothermes des poïkilothermes. Les poïkilothermes sont les animaux dont la température interne varie en fonction de la température externe.La thermorégulation comprend deux phénomènes :thermolyse (biologie) (perte de chaleur),thermogenèse (production de chaleur).La physiologie végétale, ou phytobiologie, est la science qui étudie le fonctionnement des organes et des tissus végétaux et cherche à préciser la nature des mécanismes grâce auxquels les organes remplissent leurs fonctions. Elle cherche en somme à percer les secrets de la vie chez les plantes.Les domaines d'étude de la physiologie végétale sont très diversifiés et concernent notamment :La nutrition, en particulier l'absorption des éléments minéraux et les fonctions de synthèse :Nutrition carbonée ;Nutrition azotée ;Nutrition minérale ;Photosynthèse.La respiration et les échanges gazeux chez les plantes.La transpiration est affectée par la chaleur et par une circulation d'air sec et chaud, donc perte de H2O chez les plantes.Les relations des végétaux avec leur environnement.La croissance et le développement.La reproduction, végétative ou sexuée.Cette discipline s'intéresse aux mécanismes de fonctionnement des diverses fonctions vitales des organismes vivants du règne animal, ainsi qu'à ses liens avec les structures organiques présentes à différents niveaux d'organisation : organes, tissus, cellules, molécules.La physiologie animale tente de brosser un panorama des adaptations des animaux à leur environnement, dans leur diversité.Sylvie Meyer, Catherine Reeb et Robin Bosdeveix, Botanique, biologie et physiologie végétales, Paris, éditions Maloine, collection « Sciences fondamentales », 2004  (ISBN 2-224-02767-2). XII, 461, L pages.Jack Baillet et Erik Nortier, préface de Roger Guillemin, Précis de Physiologie Humaine, 1998.A. Calas, J.-F. Perrin, C. Plas et P. Vanneste, Précis de physiologie, éditions Doin, 1997.Knut Schmidt-Nielsen, Physiologie animale ; adaptation et milieux de vie, éditions Dunod, 1998.(en) Gilbert Chauvet, Theoretical Systems in Biology : Hierarchical and Functional Integration, vol I, II et III, Elsevier, 1996.Bernard Calvino, Introduction à la physiologie, éditions Belin, 2003  (ISBN 2-7011-3079-4).Lauralee Sherwood, Hillar Klandorf et Paul Yancey, Physiologie animale, De Boeck Superieur, 2016 (lire en ligne)Pour une biologie et une physiologie intégrative sur le site de Gilbert ChauvetUne modélisation piagétienne du système nerveux humain Portail de la physiologie   Portail de la biologie   Portail de la médecine"
médecine;"La santé est « un état de complet bien-être physique, mental et social, et ne consiste pas seulement en une absence de maladie ou d'infirmité ». Dans cette définition par l'Organisation mondiale de la santé, OMS, depuis 1946, la santé représente « l’un des droits fondamentaux de tout être humain, quelles que soient sa race, sa religion, ses opinions politiques, sa condition économique ou sociale »,. Elle implique la satisfaction de tous les besoins fondamentaux de la personne, qu'ils soient affectifs, sanitaires, nutritionnels, sociaux ou culturels.. Mais cette définition confond les notions de santé et de bien-être.Par ailleurs, « la santé résulte d’une interaction constante entre l’individu et son milieu » et représente donc « cette « capacité physique, psychique et sociale des personnes d’agir dans leur milieu et d’accomplir les rôles qu’elles entendent assumer d’une manière acceptable pour elles-mêmes et pour les groupes dont elles font partie ». René Dubos présente en 1973 la santé comme « la situation dans laquelle l'organisme réagit par une adaptation tout en préservant son intégrité individuelle. C'est l'état physique et mental relativement exempt de gênes et de souffrances qui permet à l'individu de fonctionner aussi longtemps que possible dans le milieu où le hasard ou le choix l'ont placé. »,Pour René Leriche en 1936, « la santé c'est la vie dans le silence des organes. »,Dans les sociétés traditionnelles (« primitives »), la santé relève généralement autant de l'individu que du groupe. Elle est imbriquée avec les croyances animistes et religieuses, et le rôle des guérisseurs (chamans, sorciers, etc.) qui utilisent à la fois la pharmacopée locale, le toucher et des pratiques relevant de la magie, de la divination, ou de la psychologie.À partir du XVIIIe siècle, la maladie cesse progressivement d'être considérée comme une fatalité et le corps redevient un sujet de préoccupation. Ce mouvement concerne d'abord les élites, puis s'étend progressivement à l'ensemble de la société. La santé devient alors un droit que les États se doivent de garantir.L'état de santé se recherche à la fois pour chaque individu, avec la médecine clinique, ou pour une population, avec la santé publique.La santé d'une population est classiquement évaluée d'abord par les taux de mortalité et de morbidité, avec l’espérance de vie. La santé est une notion relative, « parfois non présentée comme corollaire de l'absence de maladie : des personnes porteuses d'affections diverses sont parfois jugées « en bonne santé » si leur maladie est contrôlée par un traitement. A contrario, certaines maladies peuvent être longtemps asymptomatiques, ce qui fait que des personnes qui se sentent en bonne santé peuvent ne pas l'être réellement. »« État de santé ressentie » : c'est l'un des indicateurs d'état de santé. Il est publié tous les deux ans depuis 2002, pour les pays de l'OCDE. Après une tendance à la hausse de 2002 à 2008, il a chuté de plusieurs points en 2010 « Quelles que soient les tranches d’âge, le pourcentage des femmes et des hommes s’estimant en bonne ou très bonne santé baisse en 2010. Et lorsque l’on considère l’ensemble des sexes, il en est de même pour le quintile de revenu le plus élevé ». En 2008, 74,9 % des hommes se jugeaient en bonne ou très bonne santé, contre 70,6 % en 2010. Pour les femmes ce taux est passé de 70,1 % à 66,5 %.La santé mentale peut être considérée comme un facteur très important de la santé physique pour les effets qu'elle produit sur les fonctions corporelles. Ce type de santé concerne le bien-être émotionnel et cognitif ou une absence de trouble mental. L'Organisation mondiale de la santé (OMS) définit la santé mentale en tant qu'« état de bien être dans lequel l'individu réalise ses propres capacités, peut faire face aux tensions ordinaires de la vie, et est capable de contribuer à sa communauté ». Il n'existe aucune définition officielle de la santé mentale. Il existe différents types de problèmes sur la santé mentale, dont certains sont communément partageables, comme la dépression et les troubles de l'anxiété, et d'autres non communs, comme la schizophrénie ou le trouble bipolaire.Pour l'Organisation mondiale de la santé (OMS), la santé reproductive est une composante du droit à la santé.Cette notion récente évoque la bonne transmission du patrimoine génétique d'une génération à l'autre. Elle passe par la qualité du génome, des spermatozoïdes et des ovules, mais aussi par une maternité sans risque, l'absence de violences sexuelles et sexistes, l'absence de maladies sexuellement transmissibles (MST), la planification familiale, l'éducation sexuelle, l'accès aux soins, la diminution de l'exposition aux perturbateurs endocriniens, etc.Un certain nombre de polluants (dioxines, pesticides, radiations, leurres hormonaux, etc.) sont suspectés d'être, éventuellement à faibles ou très faibles doses, responsables d'une délétion de la spermatogenèse ou d'altération des ovaires ou des processus de fécondation puis de développement de l'embryon. Certains sont également cancérigènes ou mutagènes (ils contribuent à l'augmentation du risque de malformation et d'avortement spontané).Les soins de santé reproductive recouvrent un ensemble de services, définis dans le Programme d’action de la Conférence internationale sur la population et le développement (CIPD) tenue au Caire (Égypte) en septembre 1994 : conseils, information, éducation, communication et services de planification familiale ; consultations pré et postnatales, accouchements en toute sécurité et soins prodigués à la mère et à l’enfant; prévention et traitement approprié de la stérilité ; prévention de l’avortement et prise en charge de ses conséquences ; traitement des infections génitales, maladies sexuellement transmissibles y compris le VIH/SIDA ; le cancer du sein et les cancers génitaux, ainsi que tout autre trouble de santé reproductive ; et dissuasion active de pratiques dangereuses telles que les mutilations sexuelles féminines.La santé au travail fait partie des principaux thèmes de santé identifiés par l'OMS.Un déterminant de santé est un facteur qui influence l’état de santé d'une population soit isolément, soit en association avec d’autres facteurs.L'hygiène est l'ensemble des comportements concourant à maintenir les individus en bonne santé. Ils demandent de pouvoir notamment faire la part entre les « bons microbes » et ceux qui sont pathogènes ou peuvent le devenir dans certaines circonstances. Ces circonstances l'hygiène cherche à les rendre moins probables, moins fréquentes ou supprimées. Après une phase hygiéniste, dont l'efficacité de court terme est indiscutable, sont apparus une augmentation des allergies, des maladies auto-immunes, des antibiorésistances et des maladies nosocomiales jugées préoccupantes. La recherche de juste équilibre entre exposition au risque et solution médicale usuelle est rendue difficile dans un contexte d'exposition accrue à des cocktails de polluants complexes (pesticides en particulier) et perturbateurs hormonaux, de modifications sociétales et climatiques planétaires (cf. maladies émergentes, risque pandémique, zoonoses, risque de bioterrorisme, etc.).La lutte contre les infections nosocomiales à l'hôpital, ou contre les toxi-infections alimentaires par exemple, est née après la découverte de l'asepsie sous l'influence par exemple de Ignace Semmelweis ou Louis Pasteur. Les comportements individuels et collectifs sont de toute première importance dans la lutte contre les épidémies ou les pandémies.Cette discipline de l'hygiène vise donc à maîtriser les facteurs environnementaux pouvant contribuer à une altération de la santé, comme la pollution par exemple, avec des problèmes paradoxaux à gérer : par exemple, l'amélioration des conditions d'hygiène semble avoir paradoxalement pu favoriser la réapparition de maladies comme la poliomyélite et diverses maladies auto-immunes et allergies.De nombreux facteurs de risque sont intrinsèquement liés au mode de vie. Les soins corporels, l'activité physique, l'alimentation, le travail, les problèmes de toxicomanie, notamment, ont un impact global sur la santé des individus.Nutrition : Aliments - Oligo-élément - AlicamentProduits d'hygiène : Crème solaire - Dentifrice - Préservatif - SavonToxicomanies & dépendances : Alcool - Cannabis - Cocaïne - Tabac - Jeu pathologiqueDe nombreux risques et dangers sont liés au domaine de la santé, l'évolution humaine et également les changements de son mode de vie ne sont pas sans conséquences. L'alimentation et les nouvelles technologies sont également des facteurs de risques en France et dans le reste du monde. Les rythmes, les cadences de travail ; les gestes inadaptés sont des facteurs très importants sur la santé. Ils entraînent des troubles psychosomatiques et parfois des handicaps pour la vie.Quatre facteurs permettraient d'allonger considérablement la durée de la vie : absence de tabac, consommation d'alcool égale ou inférieure à un demi verre par jour, consommation de 5 fruits et légumes par jour, exercice physique d'une demi-heure par jour. Le tout donnerait une majoration de l'espérance de vie de 14 ans par rapport au non-respect de ces facteurs.Du strict point de vue de l'alimentation, de nombreuses études concordantes concluent qu'une alimentation exclusivement végétarienne permet de limiter les risques de cancer et de maladies cardio-vasculaires, et donc d'avoir une espérance de vie en bonne santé plus longue,. Les études mettent à la fois en évidence les bénéfices d'une alimentation riche en légumes et fruits et les risques relatifs liés à la consommation de viande, poisson et produits laitiers,,,. Les compléments alimentaires synthétiques ne seraient absolument pas nécessaires,.D'autres pistes sont explorées pour allonger la durée de vie en bonne santé : le jeûne, le jeûne intermittent et la restriction calorique.Par ailleurs, l'« hygiénisme moral » trans-national débuté au XIXe siècle (à ne pas confondre avec la médecine alternative créée par Herbert Shelton) est une doctrine contre le « relâchement des mœurs », ce qui serait le meilleur moyen de garantir la santé. C'est ce courant qui a par exemple déclaré la lutte contre la syphilis ou l'alcoolisme comme priorité nationale. C'est également lui qui déclare que si les obèses sont gros, c'est qu'ils sont gourmands et paresseux, ou encore que les fumeurs n'ont pas de volonté; Il semble persister dans certaines politiques et campagnes d'information et d'éducation des citoyens à l'hygiène.C'est un domaine, parfois nommé « santé environnementale », qui se développe depuis la fin du XXe siècle, à la suite de la prise de conscience du fait que l'environnement, notamment lorsqu'il est pollué, est un déterminant majeur de la santé.La pollution aiguë ou chronique, qu'elle soit biologique, chimique, due aux radiations ionisantes, ou due aux sons ou la lumière (ces facteurs pouvant additionner ou multiplier leurs effets) est une source importante de maladies.Dans l'Union européenne, la Commission a adopté (11 juin 2003) une « stratégie communautaire en matière de santé et d'environnement », traduite le 9 juin 2004, en un « Plan d'action » (2004-2010), qui vise notamment les maladies dites « environnementales ». Cela concerne l'asthme et les allergies respiratoires, en cherchant plus généralement à « mieux prévenir les altérations de la santé dues aux risques environnementaux » (dont l'exposition aux pesticides et à leurs résidus). Des systèmes de veille sanitaire permanente doivent identifier les menaces émergentes (dont nanotechnologies, OGM, maladies émergentes, impacts des modifications climatiques, etc.) et en évaluer l'impact sanitaire selon des actions réalisées au niveau communautaire mais aussi national. Un « plan d'action environnement et santé » va être développé afin de mettre en œuvre cette stratégie ; de plus un processus de consultation a été lancé. Le plan d'action vise à faire le point sur les connaissances scientifiques existantes et à évaluer la cohérence et les progrès réalisés dans l'installation du cadre législatif communautaire en matière de santé et d'environnement. Un nouveau système d'information sur la santé est prévu « qui fonctionnera également dans le domaine de l'environnement » et veut devenir « la plus importante source de données fiables pour l'évaluation de l'impact des facteurs environnementaux sur la santé ». Ces aspects seront coordonnés avec les systèmes de réaction rapide et une approche intégrée « visant à juguler les déterminants environnementaux de la santé ».En ce qui concerne plus spécifiquement la France, un premier Plan national santé-environnement a été lancé en 2004 et un second en 2009, à la suite du Grenelle de l'environnement. Le bilan des actions menées devrait être fait en 2013.La santé publique désigne à la fois l'état sanitaire d'une population apprécié via des indicateurs de santé (quantitatifs et qualitatifs, dont l'accès aux soins) et l'ensemble des moyens collectifs susceptibles de soigner, promouvoir la santé et d'améliorer les conditions de vie.La notion de santé publique regroupe plusieurs champs :la santé au travail incluant la médecine du travail et parfois des démarches épidémiologiques ;la gestion des campagnes de prévention, qui doivent influencer les autres secteurs de la société pour y promouvoir la santé (économie, écoles, trafic, habitation, environnement, style de vie, etc.), la vaccination... ;l'organisation des réseaux de soins : premiers secours, hôpitaux, médecine libérale, médecine d'urgence... ;la formation initiale et continue des professions médicales et paramédicales ;la sécurité sociale et l'assurance maladie (Sécurité sociale en France) ;la recherche médicale et pharmacologique.Les règles en matière de santé font l'objet de textes internationaux édictés par l'OMS ou la FAO (Codex alimentarius pour l'alimentation).L'Union européenne a produit de nombreuses directives, règlements ou décisions pour protéger la santé des consommateurs ou d'animaux consommés.La promotion de la santé telle que définie par l'OMS est le « processus qui confère aux populations les moyens d'assurer un plus grand contrôle sur leur propre santé, et d'améliorer celle-ci ». Cette démarche relève d'un concept définissant la « santé » comme la mesure dans laquelle un groupe ou un individu peut d'une part réaliser ses ambitions et satisfaire ses besoins, et d'autre part évoluer avec le milieu ou s'adapter à celui-ci.La santé est prise en compte par le droit, y compris du point de vue des Conditions de travail.Les crises sanitaires sont des pandémies importantes, qui touchent entre une dizaine de personnes (cas des crises très médiatisées qui touchent les pays développés, comme certaines crises alimentaires) et des millions de personnes. Elles peuvent avoir des coûts économiques, sociaux et politiques considérables.L'OMS a d'ailleurs été créée pour qu'une pandémie telle que celle produite par la grippe espagnole ne se reproduise pas avec les mêmes effets (30 à 100 millions de morts selon les sources).Les sommes en jeu dans le domaine de la santé sont considérables, tant pour les coûts induits par les maladies, les pollutions et l'absentéisme, que par le marché des soins et des médicaments (en 2002, le marché mondial du médicament a été évalué à 430,3 milliards de dollars, contre 220 milliards en 1992). Le marché pharmaceutique a augmenté de 203 milliards d'euros. Et la consommation médicale progresse plus rapidement que le PIB dans les pays développés.Des crises sanitaires telles qu'une pandémie peuvent avoir des coûts économiques, sociaux et politiques considérables.La santé comme concept peut être un objet d’étude anthropologique. Tel que rapporté par Roy, elle est souvent conceptualisée comme une construction sociale par les anthropologues puisque le rapport que les sociétés ont avec elle est très variable d’une à l’autre, et selon les époques également. Le travail anthropologique cherchera donc à mieux comprendre l’expérience que font les groupes sociaux et culturels de la santé. Cet objet d’étude, pour faire preuve de rigueur méthodologique, doit être replacé dans son contexte global, notamment à travers les changements sociaux. On cherche alors à comprendre les phénomènes de relation santé/maladie, bien que de plus en plus le schéma santé/vie prend place. Pour dire autrement, selon Massé, l’anthropologie médicale s’intéresse à comment les acteurs sociaux définissent la bonne ou la mauvaise santé, et comment les maladies sont soignées dans ce contexte.Quelques approches théoriques sont nées en anthropologie médicale, rapportées par Roy. Parmi elles, celle de la théorie médico-écologique, celle de la phénoménologie et celle de la critique de la médecine et de la santé internationale.La théorie médico-écologique est formulée par Alexander Alland au début des années 1970, mais est reprise par d’autres quelques années après. Elle part du principe que les groupes humains adaptent leur culture à l’environnement. Cette théorie propose l'idée que l’adaptation culturelle est intimement liée à l’adaptation biologique en fonction de l'environnement et du milieu dans lequel le groupe se trouve. Ainsi, la santé est liée à ces transformations externes.L’approche phénoménologique se développe en parallèle à cette dernière. Des auteurs comme Kleinman et Good en sont un point d’origine, en cherchant à redonner une subjectivité à l’expérience humaine de la santé, s’éloignant de l’objectivité préconisée par la médecine. Pour ce faire, des perspectives expérientielles et sémantiques sont mobilisées.L’approche critique de la médecine et de la santé internationale se développe dans les années 1960. Elle a pour objet les conditions notamment politiques et économiques, donc globales, dans lesquelles sont vécues la santé et la maladie : les inégalités sociales façonnent l’accès à l’information, aux ressources de maintien de la santé et aux traitements. Un texte clé pour comprendre ce mouvement est notamment celui de Baer, Singer et Johnsen.De nombreux médias et émissions sont spécialisés dans les thèmes de la santé. En voici une sélection :Le Magazine de la santé, sur France 536.9, sur la Radio télévision suisseQuoi de neuf doc ?, sur TV5 MondeRadio Public SantéRadio France internationale, émission Priorité santéRadio Canada première chaîne, émission RDI SantéPlace à la santéSanté MagazineAlternative santéEnvironnement, Risques et SantéHealth On the Net Foundation est fondation qui indique aux internautes dans quels sites internet, ils peuvent obtenir des informations justes et sérieuses dans le domaine de la santé.PubMedSantepratiquePortail Santé-UEFasosante.netCarenity (réseau social santé sur internet destiné aux malades et à leurs proches).André Rauch, Histoire de la santé, PUF, Que-sais-je ?, 1995Georges Canguilhem, La santé, concept vulgaire et question philosophique, Sables, Pin-Balma, 1990Ressources relatives à la santé : (en) Medical Subject Headings (en) NCI Thesaurus Ressource relative à la recherche : Horizon 2020 Liste des thèmes de santé, site de l'OMS(en) Global Health, site Our World in Data Portail des soins infirmiers   Portail de la médecine   Portail de la société"
médecine;"En médecine, un symptôme (du grec ????????, « rencontrer ») ou signe fonctionnel est un signe qui représente une manifestation d'une maladie, tel qu'il est observé chez un patient. En général, pour une pathologie donnée, les symptômes sont multiples, et parfois il peut ne pas y avoir de symptôme (la maladie ou le malade est dit dans ce cas asymptomatique) ou peu de symptômes (maladie ou malade paucisymptomatique). Inversement, un même symptôme peut très souvent être attribué à différentes maladies : on ne peut donc en général pas conclure automatiquement qu'un symptôme (par exemple, le mal de gorge) est dû à une maladie donnée (par exemple, la grippe) ; ce serait commettre le sophisme de l'affirmation du conséquent.Le mot ????????, en grec, signifie « accident », « coïncidence » ; il est constitué du préfixe ???, « avec » et de ?????, « arriver », « survenir ». Le symptôme est donc, à l'origine, « ce qui survient ensemble », ce qui « concourt » ou « co-incide », au sens littéral du terme.Les symptômes sont les signes cliniques dont le malade se plaint (comme la douleur, la toux, le vertige, la tristesse). Les symptômes sont les éléments d'alerte d'un processus pathologique en cours, motivant ainsi le recours à une consultation médicale permettant d'objectiver la plainte en retrouvant des signes, qui, rassemblés en syndrome, puis en maladie en établissant un diagnostic, permettront de guider l'attitude thérapeutique.Les symptômes sont donc à différencier :des autres signes cliniques :les signes physiques, découverts en examinant le malade : contracture abdominale, souffle cardiaque,certains signes généraux : fièvre, hypotension artérielle ;des signes paracliniques obtenus à l'aide d'examens complémentaires :les signes radiologiques à la suite de radiographies,les signes biologiques à la suite de prélèvements.Par exemple, dans l'arthrose de hanche, le patient peut se plaindre de douleur à la marche (symptôme), et le praticien pourra objectiver à l'examen une limitation de mobilité de la hanche (signe physique), et sur une radiographie du bassin (signe radiologique).En créant la psychanalyse, Sigmund Freud va donner un sens au symptôme. À la suite des Études sur l'hystérie (1895), il n'a plus cesse de l'interroger dans les manuscrits à une époque « où la psychiatrie le réduisait à un phénomène hétérogène et opaque de la vie psychique ».Le symptôme peut être une manifestation somatique : une paralysie, des troubles du langage.Il peut être aussi une manifestation psychique : angoisse, hydrophobie.En étudiant le cas d'Anna O. (Bertha Pappenheim), une hystérique soignée par Josef Breuer grâce à la méthode cathartique, Freud a d'abord vu dans le symptôme un résidu mnésique d'expériences émotives (c'est-à-dire de traumatismes psychiques).Ensuite, en formulant sa nouvelle compréhension du système psychique, il a interprété différemment le symptôme.L'appareil psychique est composé de différentes instances en conflit : le moi, le ça et le surmoi.Quand une représentation (pulsionnelle) tombe sous le coup d'un interdit, elle est refoulée dans l'Inconscient par la censure opérée par le moi, mais jamais anéantie. Un processus alors de tentative de réapparition des éléments refoulés se met en place : c'est le retour du refoulé. Il y a plusieurs façons de déjouer la censure : le rêve, les lapsus, les oublis et les actes manqués ou bien les symptômes. Ces formations substitutives sont des formes de déguisement de la représentation, rendus acceptables pour la conscience pour pouvoir réinvestir son champ. Ainsi, ils permettent la satisfaction du désir sans éveiller la censure en formant un compromis entre les désirs et les interdits. Ce sont tous ces déguisements qui sont investigués, interprétés dans la cure psychanalytique.Remarque : il y a des liaisons associatives entre le symptôme et ce à quoi il se substitue.Le symptôme est le substitut de représentations tombées sous le coup d'un interdit et refoulées dans l'Inconscient. Il est le déguisement de ces représentations pour qu'elles puissent réinvestir le champ de la conscience, en étant acceptable. Et, il apporte une satisfaction de remplacement au désir inconscient, sans éveiller la censure et même en satisfaisant les exigences défensives. Cette double-satisfaction explique la capacité de résistance du symptôme car il est maintenu des deux côtés.Récapitulatif :il est formation de compromis en tant qu'il est le produit du conflit défensif ;il est formation substitutive dans la mesure où c'est le désir qui cherche à se satisfaire ;il est formation réactionnelle dans la mesure où c'est le processus défensif qui prévaut.Le symptôme est satisfaction, décharge pulsionnelle, il offre un bénéfice primaire. On ne saurait chercher à retirer au malade mental son symptôme, en ce qu'il en jouit, et que le psychologue doit reconnaitre comme jouissance.Ce bénéfice primaire correspond à la signification que porte le symptôme, signification qui seule permet l'expression d'un désir inconscient - le symptôme se rattache donc à la représentation, voire au discours. Pour Jacques Lacan, le symptôme est donc métaphore (Le symptôme est une métaphore que l'on veuille ou non se le dire).Le symptôme peut également engendrer un bénéfice secondaire, plaisir supplémentaire qui ne se relie donc pas directement au sens que veut énoncer ce signe de la maladie, mais qui provient plus d'un hasard relatif cette fois à la nature même du symptôme. Ainsi, le procédurier paranoïaque ralliant à lui un mouvement de soutien.Du point de vue psychosociologique, le symptôme est la façon particulière dont un individu trouve sa place dans le monde et règle son rapport à celui-ci, en fonction des contraintes et des stimulations psychosociales qui lui parviennent. Le symptôme est un prolongement de la personnalité, qui permet à cette dernière d'appréhender le monde mais aussi de s'en distancier, par un ensemble de protections constitutives dudit symptôme.Ainsi le symptôme est-il, du point de vue du sujet :stratégie d'individualisation ;matériau de la personnalité ;interprétation continue du monde ;modalité comportementale dynamique ;dispositif protecteur du Moi (ou ego) ;routine pathologique sitôt qu'il étouffe la créativité du sujet ou porte atteinte à l'intégrité d'autrui.Cet article est partiellement ou en totalité issu de l'article intitulé « Symptôme fonctionnel » (voir la liste des auteurs). Médecine et biologie  Psychanalyse Augustin Jeanneau, « symptôme », dans Alain de Mijolla (dir.), Dictionnaire international de la psychanalyse 2. M/Z, Paris, Hachette, 2005 (ISBN 2-0127-9145-X), p. 1769-1770.Augustin Jeanneau et Roger Perron, « symptôme (formation de -) », dans Alain de Mijolla (dir.), Dictionnaire international de la psychanalyse 2. M/Z, Paris, Hachette, 2005 (ISBN 2-0127-9145-X), p. 1770-1772.Yves Morhain, « Permanence du corps et variations du symptôme hystérique et/ou psychosomatique », Psychothérapies, 2011/2 (Vol. 31), p. 131-141. DOI : 10.3917/psys.112.0131. [[ lire en ligne]]Valentin Nusinovici, « symptôme,sinthome », dans Alain de Mijolla (dir.), Dictionnaire international de la psychanalyse 2. M/Z, Paris, Hachette, 2005 (ISBN 2-0127-9145-X), p. 1772-1774.Marcel Scheidhauer, « Le symptôme, le symbole et l'identification dans l'hystérie dans les premières théories de Freud », in: Enfance, tome 40, n°1-2, thématique : « Identités, Processus d'identification. Nominations », 1987, p. 151-162, sur le site de Persée, consulté le 30 mars 2021 [lire en ligne].Liste des symptômes en médecine humaineSémiologie médicaleTableau cliniqueSigne physiquePathomimieSymptôme (pathologie végétale)Inhibition, symptôme et angoisseSinthome Portail de la médecine   Portail de l’agriculture et l’agronomie   Portail de la psychologie"
médecine;"Un antiseptique est un désinfectant à usage corporel ; c'est une substance qui détruit ou prévient le développement des agents infectieux (microorganismes ou virus) sur la peau ou les muqueuses. Les antiseptiques sont à distinguer des antibiotiques, qui agissent seulement contre les bactéries et sont administrés par injection ou par voie orale, et des bactériophages qui sont des produits contenant des virus prédateurs des bactéries. Au plan règlementaire, les antiseptiques sont des médicaments nécessitant une autorisation de mise sur le marché.L'antisepsie fut étudiée expérimentalement au XVIIIe siècle par John Pringle. Joseph Lister, inspiré par les travaux de Pasteur sur les fermentations, fut un des pionniers et le plus efficace vulgarisateur de l'application de l'antisepsie à la chirurgie.Il existe plusieurs classes de produits antiseptiques, déterminées selon leur structure chimique et leur efficacité. Les antiseptiques majeurs regroupent les biguanides (chlorhexidine), les dérivés iodés (povidone iodée), les dérivés chlorés (hypochlorite de sodium), et les alcools (éthanol),. Les différentes classes d'antiseptiques ne doivent pas être mélangées ni combinées, sous peine d'inactivation, voire d'entraîner la formation de produits irritants. Certains antiseptiques existent sous forme de solution aqueuse ou de solution alcoolique. En dehors d'une contre-indication occasionnelle, la forme alcoolique doit être préférée. En effet, l'action est plus rapide, et l'indice de pénétration de l'antiseptique meilleur. La concentration et la pénétration du di-iode est ainsi augmentée.Les antiseptiques mineurs regroupent les colorants, comme l'éosine, la solution de Milian, la fluorescéine, ou le Bleu de méthylène.D'autres classes, moins utilisées, existent. Les composés organomercuriels ne sont plus fabriqués. Les oxydants comprennent l'eau oxygénée et le permanganate de potassium. Les ammoniums quaternaires sont, eux, dominés par le chlorure de benzalkonium.La rémanence est le temps durant lequel persiste l'action antiseptique en absence de nouvelle application. L'éthanol, par exemple, ne possède pas de rémanence, tandis que celle de la povidone iodée est de trois heures.Les antiseptiques sont utilisés de plusieurs manières. Ils peuvent servir à désinfecter une plaie souillée, sur peau lésée, mais aussi sur peau saine à préparer le champ opératoire. À cette fin, il existe différents types de produits adaptés à l'utilisation cutanée ou muqueuse, orale comme gynécologique.Les antiseptiques permettent l'élimination de la flore cutanée transitoire, et la réduction de la flore résidente.Le choix du protocole doit être adapté au niveau de risque. On distingue les procédures en un temps (passage unique d'une solution alcoolique) pour les gestes à faible risque, en deux temps (deux passages avec un antiseptique majeur) pour les gestes à risque intermédiaire, et enfin en quatre temps pour les gestes à risque élevé de contamination. Cette dernière procédure comporte en premier lieu une détersion par un savon antiseptique, qui est rincé, puis la peau est séchée. Enfin, le dernier temps est un badigeon par un antiseptique majeur de la même classe.La durée de conservation des antiseptiques dépend à la fois du produit et du conditionnement. Les conditionnements en uni-dose, de même que les solutions diluées, doivent être jetés après utilisation. Les conditionnements multidoses se conservent, eux, pendant un mois. À cette fin, la date d'ouverture doit être écrite sur le flacon, qui doit être rebouché après utilisation. L'antiseptique ne doit pas être transvasé, ni reconditionné.Les différentes classes d'antiseptiques ne possèdent pas le même spectre antimicrobien. Pour toutes, l'activité létale est maximale envers les bactéries. Les biguanides sont peu actifs sur les champignons, et inactifs sur les spores, au contraire des dérivés iodés et chlorés et de l'éthanol. Les virus sont difficilement inactivés par les biguanides et l'éthanol, alors que les dérivés iodés et chlorés possèdent une activité létale modérée.AntisepsieAntiseptiques et désinfectants Portail de la microbiologie   Portail de la médecine   Portail de la pharmacie"
médecine;"Une maladie infectieuse (ou infection) est une maladie provoquée par l'invasion d'un ou plusieurs micro-organismes ou agent infectieux (bactéries, champignons, parasites, protozoaires, virus) dans un tissu où ils se multiplient, et par une réaction générale des cellules et des tissus infectés pour éliminer ces agents pathogènes ou leurs toxines (processus impliquant notamment le système immunitaire des plantes et des animaux).L'étude des agents infectieux relève de la biologie, de la microbiologie médicale, de l'épidémiologie et de l'écoépidémiologie. Dans la nature, des maladies infectieuses se développent chez tous les organismes vivants (animaux, végétaux, fongiques, micro-organismes… il existe également des virus de virus). En tant qu'interactions durables, les maladies infectieuses font partie des boucles de rétroaction qui entretiennent la stabilité relative (équilibre dynamique) des écosystèmes, la plupart des pathogènes coévoluant avec leur hôte depuis des millions d'années. Leur mode de transmission est variable et dépend de leur réservoir (humain, animal, environnemental) et parfois de vecteurs (maladies vectorielles).Elles sont plus ou moins contagieuses. Par exemple, le tétanos est une toxi-infection causée par Clostridium tetani, une bactérie qui se trouve dans la terre. Il n’y a pas de transmission interhumaine, l’infection se produit lorsque la bactérie entre dans l’organisme par une plaie souillée. Un vaccin existe contre cette affection et est obligatoire en France pour tous les enfants d’âge scolaire. Autre exemple, le paludisme est dû à un parasite, le Plasmodium falciparum (il existe d’autres Plasmodii), transmis d’homme à homme par l’intermédiaire d’un moustique, l’anophèle. Le réservoir du parasite est humain mais il n’y a pas de transmission interhumaine. Il n’existe à l'heure actuelle pas de vaccin. La tuberculose se transmet d’homme à homme par mécanisme aéroporté : le réservoir est humain et c’est une maladie contagieuse. Les infections sexuellement transmissibles (ou encore MST pour maladies sexuellement transmissibles) se transmettent à l’occasion de rapports sexuels ou par le sang.De nombreux microbes vivent normalement et nécessairement dans notre tube digestif et sur notre peau, et ne deviennent infectieux qu'à certaines occasions. Le contact avec les microbes est nécessaire à l'entretien et au bon fonctionnement de la digestion et du système immunitaire.L'infection est le terme désignant soit une maladie infectieuse en général, soit la contamination par un germe. C'est la conséquence pathologique au niveau d'un tissu ou d'un organisme de la présence anormale et/ou de la réplication d’un germe bactérien, viral ou mycosique. La contamination est la pénétration du germe dans un organisme.L'infectiologie est la branche de la médecine concernant les maladies infectieuses. Le médecin spécialiste est un infectiologue. Suivant le type de germe, il est également question de bactériologie, de virologie, de parasitologie ou de mycologie.Un sepsis est une infection grave. L'adjectif septique se rapporte à un organisme ou un objet contaminé par un germe (fosse septique par exemple). Une septicémie est la contamination grave et durable (sans traitement) du sang par un germe. Une bactériémie est une contamination transitoire du sang par un germe. Lorsque les cas se multiplient dans un lieu et une période limitée, il est question d’épidémie. Si la diffusion est beaucoup plus généralisée, il est alors question de pandémie. Lorsque l'épidémie concerne le milieu animal, il est question d'épizootie. Lorsque le germe se transmet de l’animal à l’homme, il est question d'anthropozoonose ou plus simplement de zoonose.Le contage désigne la contamination par le germe.La période d’incubation est le délai entre le contage et la première manifestation de la maladie. Le malade peut être contagieux durant ce temps.La période de contagion est le temps pendant lequel le patient excrète le germe et peut le transmettre. Elle dépend de chaque maladie infectieuse.Les infections nosocomiales (ou iatrogènes) sont des infections attrapées à l’hôpital. Elles sont particulièrement complexes et dangereuses car elles surviennent chez des sujets affaiblis et concernent souvent des germes résistants aux antibiotiques. Il s’agit d’un problème de santé publique majeur.Comme le résumait en 1935 le bactériologiste français Charles Nicolle : « Malheureusement, les signes des maladies infectieuses sont presque tous les mêmes : fièvre, maux de tête, agitation ou stupeur, éruption. Seuls leur groupement, leur succession, une observation minutieuse ont pu, après de longs tâtonnements, permettre d'établir des tableaux symptomatiques particuliers et les distinguer entre eux. »Les maladies infectieuses sont responsables dans le monde de 17 millions de décès par an, soit un tiers de la mortalité et 43 % des décès dans les pays en voie de développement (contre 1 % dans les pays industrialisés). Les six maladies suivantes représentent 90 % des décès par maladies infectieuses dans le monde.Depuis les années 2000, de nombreuses urgences sanitaires reliées à l’émergence de nouveaux agents étiologiques responsables de maladies respiratoires sévères sont survenues : le syndrome respiratoire aigu sévère (SRAS), les infections d’influenza aviaire A (H5N1) chez les humains dans plusieurs pays de l’Asie, la pandémie de grippe A (H1N1) et, plus récemment, le virus influenza aviaire A (H7N9) en Chine, le coronavirus du syndrome respiratoire du Moyen-Orient (MERS-CoV) et la pandémie de Covid19. La pathogénicité et la létalité élevées de la plupart de ces virus génèrent des répercussions sociales et une pression importante sur les services de santé.La population mondiale infectée par le VIH continue de croître : rien qu’en 2000, 5,3 millions de nouveaux cas se sont déclarés dans le monde, dont la moitié parmi les jeunes de plus de 25 ans.Après une phase de forte régression (époque pastorienne et hygiéniste), les maladies infectieuses sont revenues ou sont devenues plus résistantes (antibiorésistance). Des maladies infectieuses émergentes ou réémergentes inquiètent périodiquement les épidémiologistes et les autorités sanitaires en raison de leurs impacts sanitaires, économiques et socio-politiques actuels ou potentiels. Le Haut Conseil de la santé publique (HCSP) a récemment fait 25 recommandations (sur la recherche et l'enseignement, la surveillance sanitaire et la gestion raisonnée des crises sanitaires notamment).Les progrès de l'hygiène et de la vaccination ont fourni un espoir de pouvoir les éradiquer, mais elles sont encore en France, la troisième cause de mortalité :Il est également noté que certaines infections sont aussi à l’origine de maladies inflammatoires chroniques (telles que l’asthme) et de cancers.Les maladies infectieuses entravent la santé de base des individus et ont une influence négative sur chaque indice du développement humain et plus particulièrement sur l'espérance de vie à la naissance, l'éducation et le PIB réel. Elles sont responsables d'une forte mortalité dans les régions où l'hygiène connaît un déficit et où l’accès aux soins est difficile. La malnutrition ainsi qu'un accès limité à l'eau potable sont autant de facteurs aggravants qui diminuent les chances de survie des malades mais aussi des enfants en bas âge de même que leurs conditions de développement. Ces deux facteurs désarment le système immunitaire et peuvent être vecteurs de maladies infectieuses.Ces maladies ont des conséquences négatives importantes sur le développement cognitif et les performances scolaires chez l’enfant. La malaria, entre autres, peut causer de graves séquelles, dont des troubles comportementaux, des problèmes moteurs et un manque d’autonomie. Une telle infection est donc un frein à l’éducation. Dans le cas des épidémies, il peut arriver que les enseignants soient eux aussi touchés par la maladie. Un manque de corps enseignant réduirait de façon directe la qualité de l’éducation en affaiblissant le système scolaire. Par ailleurs, si dans une famille, les responsables de l'éducation des enfants (souvent la mère) sont touchés par la maladie, c’est l’éducation dans son ensemble qui va être affectée. Le coût du traitement réduit le budget qui aurait pu être accordé à la scolarisation mais également les conditions de vie de l’enfant. Ce qui crée un cercle vicieux : les couches les plus éduquées de la population sont de moins en moins atteintes par des maladies infectieuses telles que le sida. En effet ces personnes qui sont les plus éduquées sont les mieux informées sur les modes de transmission et de prévention. Or, plus de 80 % des personnes atteintes par ces maladies vivent dans les pays en développement.D'un point de vue macroéconomique, les maladies infectieuses ont un impact sur la croissance économique et le PIB. Dans les pays en développement, la main d’œuvre est le facteur-clé de la production et donc du PIB. Néanmoins, le bon fonctionnement des entreprises et la possibilité d'être concurrent sur le marché international nécessitent avant tout une bonne santé et une éducation de base. Lorsque la santé de la personne génératrice de revenu pour la famille est affectée, toute la famille en souffre. Les maladies infectieuses aggravent donc la pauvreté, réduisent la croissance économique, le capital humain et contribuent à l’augmentation des inégalités entre les pays en voie de développement et les pays riches.La prévention des maladies infectieuses vise à limiter le risque infectieux (y compris professionnel, notamment pour les métiers de la santé, de contact avec les animaux, des déchets, des cadavres, des eaux usées, des échantillons à analyser en laboratoires de biologie, etc.).Elle s’articule en trois volets : éviter l’infection, renforcer les défenses immunitaires et prendre des traitements préventifs (prophylaxie) en cas de risque d’exposition.La maladie infectieuse est provoquée par la pénétration dans l’organisme d’une bactérie ou d’un virus. La première précaution consiste donc à « fermer les portes d’entrée », à savoir :les voies respiratoires : tousser ou éternuer dans un mouchoir, dans le coude, ou dans les mains (en se les lavant immédiatement après) pour éviter de contaminer l’entourage ; porter un masque facial lorsque des personnes vulnérables sont rencontrées (par exemple dans certaines zones des milieux hospitaliers, personnes immunodéprimées) ou porteuses de virus très contagieux (comme le sras) ; pour la ventilation artificielle, utiliser un filtre antibactérien ;les voies digestives : se laver les mains avant de manger ou de préparer un repas, ou après une exposition à des liquides biologiques (par exemple en sortant des toilettes), voire les désinfecter lorsqu’il s’agit de liquides d’une autre personne (par exemple accident d'exposition au sang) ; porter des gants fins (latex, ou pour les personnes allergiques en PVC ou nitrile) lorsqu’une telle exposition est probable ; en général laver les mains régulièrement pendant la journée ;effraction cutanée : toute plaie grave devra être montrée à un médecin qui prendra les mesures nécessaires ; toute plaie simple doit être nettoyée, ou mieux désinfectée (voir l’article bobologie) ; mais la première précaution est bien sûr d’éviter de se faire une plaie, en respectant les règles de sécurité de certaines activités et en portant des protections adaptées (gants de travail…) ;voie oculaire : éviter de se frotter les yeux et se laver les mains avant au cas où cela arriverait ; en cas de risque d’exposition à des liquides biologiques, porter des lunettes de protection ;sexualité : utiliser un préservatif pour réduire les risques de transmission des maladies sexuellement transmissibles.Le port d'équipements de protection individuelle dépend de l’évaluation des risques. Au travail outre des gants de protection, un appareil de protection respiratoire et des lunettes masques ou une visière sont parfois nécessaires, voire un vêtement de protection intégral.Les gants fins sont recommandés en cas de risque d’exposition à des liquides biologiques ou chimiques, mais déconseillé pour les activités courantes : en effet, la peau est alors dans une atmosphère chaude et humide propice au développement de germes, et par ailleurs, il vaut mieux des mains propres que des gants sales. À noter qu’au bout d’une vingtaine de minutes, certains gants fins deviennent poreux ou sont incompatibles avec certaines substances.Il faut aussi limiter le développement de germes pathogènes sur et dans le corps et dans l’habitation, par une hygiène suffisante :hygiène corporelle : se laver, se brosser les dents ;hygiène ménagère : avoir un réfrigérateur créant un froid suffisant, décongelé et nettoyé régulièrement, laver les couverts, assiettes et verres après utilisation, stocker les ordures dans des poubelles dédiées et ramassées régulièrement par les services municipaux, évacuation des eaux usagées vers une fosse septique vidangée régulièrement ou vers les égouts, rangement et nettoyage de l’habitation, aérer pour limiter la pollution intérieure (acariens, composés organiques volatils) et donc les allergies et les maladies respiratoires ;surveiller et traiter les parasitoses (certaines facilitent les maladies infectieuses, virales ou bactériennes). Par exemple, chez le porc, l'ascaris augmente le risque de bronchopneumonie, la trichocéphalose l'entérite hémoragique, l'oesophagostomum les salmonelloses, les strongyloides le rouget, les metastrongylus la grippe porcine, etc.Les collectivités territoriales jouent un rôle important en ce qui concerne l’hygiène collective, avec la gestion des eaux pour fournir de l’eau potable, l’organisation de la collecte et du traitement des ordures, l’équarrissage des cadavres d’animaux et la police des funérailles et des lieux de sépulture (condition de transport et de conservation des corps avant crémation ou inhumation, gestion des cimetières et crématoriums).La première mesure consiste à avoir une bonne hygiène de vie : alimentation saine, exercice physique régulier, sommeil suffisant, éviter les comportements à risque (tabagisme, excès d’alcool), ce qui permet d’avoir un meilleur état de santé général donc de mieux résister aux infections.Par ailleurs, il convient de respecter les vaccinations préventives obligatoires, ou recommandées comme la vaccination des personnes âgées contre la grippe.Il faut aussi prendre précautionneusement les médicaments prescrits par un médecin, en lisant systématiquement les notices accompagnatrices, riches en informations (effets secondaires, interactions avec d’autres médicaments, recommandations…) et ne pas hésiter à questionner le médecin ou le pharmacien en cas de doute. Les effets peuvent ne pas être immédiats, et il faut continuer le traitement jusqu’à la fin même en cas d’amélioration et disparition des symptômes, notamment dans le cas des antibiotiques : la disparition des symptômes signifie la diminution du nombre de germes, mais pas leur disparition, si le traitement est interrompu trop tôt, ceux-ci peuvent se redévelopper, et devenir résistants à l’antibiotique.Il ne faut pas que le médecin prescrive systématiquement d’antibiotique : ils ne sont pas efficaces contre les maladies virales.Les mesures d’hygiènes simples sont les meilleurs traitement préventifs : lavage des mains, pour éviter la transmission des infections alimentaires, éternuer dans ses coudes lors d'un Éternuement et non pas dans ses mains afin de ne pas les « contaminer » par d'éventuels microbes… Il est parfois nécessaire de prendre des médicaments à titre préventif, comme les médicaments contre le paludisme lors d’un voyage dans un pays impaludé.La détection précoce d’une maladie permet de démarrer son traitement plus tôt et donc de réduire la mortalité ; il est recommandé de faire au moins une visite médicale par an. En cas de doute sur une infection (par exemple plaie souillée, accident d’exposition au sang, rapport sexuel non protégé), le médecin pourra mettre en place un traitement préventif pour diminuer les risques de développement d’une maladie. Pour les maladies sexuellement transmissibles, il existe en France des centres anonymes et gratuits de dépistage.Certains patients doivent être isolés (voire mis en quarantaine) pour éviter la dissémination du germe : ainsi, lors d’une varicelle, l’enfant ne doit pas aller à l’école pendant 15 jours à partir de la première éruption. Il s’agit de l'éviction scolaire. La prévention hospitalière des infections nosocomiales est un sujet complexe. Elle repose essentiellement sur l’hygiène des soignants et des soignés (lavage des mains), sur l’isolement des patients porteurs de germes résistants aux antibiotiques, mais aussi sur une antibiothérapie ciblée et adaptée.Une nouvelle approche en phase d'étude est d'utiliser la phagothérapie à des fins préventives pour la santé humaine comme cela se fait déjà dans l'agriculture et l'industrie alimentaire.Leur étude relève de l'épidémiologie et pour les zoonoses ainsi que de l'écoépidémiologie.Certaines situations (crises sanitaires ou alimentaires…) ou lieux (ports, aéroports) sont des facteurs de risques.Le traitement par antibiotiques est le traitement qui a permis de vaincre les maladies infectieuses jusqu'à l'apparition des bactéries multi-résistantes.Il présente de nombreux avantages dont la possibilité d'une fabrication en masse, rapide et bon marché des médicaments antibiotiques.Il trouve ses limites avec l'apparition de bactéries de plus en plus résistantes.La phagothérapie est apparue au début du XXe siècle avec le développement par le Français Félix d'Hérelle de médicaments bactériophagiques réalisés à partir de virus bactériophages (simplement appelés bactériophages ou même phages) lytiques afin de traiter certaines maladies infectieuses d’origine bactérienne. D'Hérelle a ainsi traité des épidémies de peste et de choléra avec succès.La phagothérapie a été largement utilisée dans le monde avant la découverte des antibiotiques. Si elle a été progressivement abandonnée par les pays occidentaux séduits par les avantages de l’antibiothérapie, la phagothérapie traditionnelle est toujours employée et développée dans les pays de l'ancienne Union Soviétique. Dans les pays occidentaux, des patients victimes d'infection par bactéries multi-résistantes se regroupent pour faciliter l'accès aux traitements bactériophagiques étrangers,,.Elle connaît un regain d'intérêt en Occident avec l'émergence de l'antibiorésistance. Elle fait l'objet de recherches à l'Institut Pasteur mais son utilisation demeure soumise à ATUn par l'ANSM.Antoine van Leeuwenhoek (1632-1723) voit pour la première fois des agents bactériens en microscopie.Louis Pasteur permet le rapprochement entre maladie et agents infectieux. Première vaccination contre la rage.Robert Koch est célèbre pour sa découverte du bacille de la tuberculose qui porte son nom : le bacille de Koch.Jonas Salk et Albert Sabin assurent le développement de la vaccination anti-polio.Charles Nicolle, Destin des maladies infectieuses, PUF 1939Brown L. (2010), ""Le plan B pour un pacte écologique mondial"", Paris, Calmann-Lévy Souffle Court Editions, 509 pages.Contrepois A. L'invention des maladies infectieuses. Édition des Archives Contemporaines. 2001. Naissance et développement institutionnel de la bactériologie médicale en France et en Allemagne au XIXe siècle.Flahaut A. et Zylberman P. Des épidémies et des hommes. Édition de la Martinière. 2008. Une bonne vulgarisation par deux experts de la question, avec nombreuses photos et illustrations.INSTITUT PASTEUR , ""Le défi des maladies infectieuses"", http://www.pasteur.fr/ip/easysite/pasteur/fr/presse/dossiers-de-presse/sante-en-voyage/le-defi-des-maladies-infectieuses, dernière visite le 8 mars 2014.Nicolle C. Le destin des maladies infectieuses. Édition France Lafayette. 1993. Réédition d'un grand classique de 1933. Conférences au Collège de France par Charles Nicolle, Prix Nobel de Médecine 1928. Toujours d'actualité.Orth G. et Sansonetti P. (sous la direction de). La maitrise des maladies infectieuses. Académie des Sciences. EDP Sciences. 2006. État des lieux et recommandations adressées aux pouvoirs publics et à l'ensemble des acteurs de santé. Un ouvrage collectif à l'aspect sévère, mais une actualisation pointue de tous les aspects (médico-scientifiques, socio-culturels, etc.) du problème.Raoult D. (1999), ""Les nouvelles maladies infectieuses, que sais-je ?"", Presses universitaires de France, 128 pages.Dossier documentaire Société Française de Santé PubliqueRessources relatives à la santé : ICD-10 Version:2016 ICD9Data.com (en) Diseases Ontology (en) Medical Subject Headings (en) NCI Thesaurus (no + nn + nb) Store medisinske leksikon (cs + sk) WikiSkripta Infectiologie.com Portail de la médecine   Portail de la microbiologie   Portail des maladies infectieuses"
médecine;"En médecine, un patient est une personne physique recevant une attention médicale ou à qui est prodigué un soin.Le mot patient est dérivé du mot latin patiens, participe présent du verbe déponent pati, signifiant « celui qui endure » ou « celui qui souffre ».Il existe plusieurs dénominations communes au terme patient, dont personne soignée, bénéficiaire de soins , ""usager"" ou encore client employé notamment dans la culture anglophone[réf. souhaitée]. Dans la recherche médicale, le patient est parfois appelé sujet. On commence même à utiliser le terme d’actient (patient qui agit) du fait de l'évolution des patients à se renseigner par eux-mêmes et à poser de plus en plus de questions au praticien.[réf. souhaitée].En médecine, le patient bénéficie d'examens médicaux, de traitements prodigués par un médecin ou un professionnel de la santé pour faire face à une maladie ou à des blessures. Le patient peut également bénéficier d'actes de prévention.Knock ou le Triomphe de la médecine, pièce de théâtre de Jules Romains, ayant fait l'objet de plusieurs adaptations cinématographiques, qui illustre de manière humoristique les relations entre médecins et patients.Luc Perino, Patients zéro. Histoires inversées de la médecine, La Découverte, 2020Jean-Philippe Pierron, « Une nouvelle figure du patient ? Les transformations contemporaines de la relation de soins », Sciences sociales et santé, vol. 25, no 2,? 2007, p. 43-66 (DOI 10.3406/sosan.2007.1858)Relation médecin-patientCharte du patient hospitaliséÉducation thérapeutique du patientDossier médical du patientPatient zéro Portail de la médecine"
médecine;"La prévention regroupe toutes les dispositions prises pour empêcher l'apparition, l'aggravation ou l'extension d'un danger, d'un risque, d'un accident, d'une maladie ou, plus généralement, de toute situation (sanitaire, sociale, environnementale, économique, etc.) dommageable comme une épidémie, un conflit, une catastrophe, une crise. Agir avec une attitude de prévention consiste donc à :mettre en œuvre des mesures pour réduire, parfois jusqu'à supprimer, les conditions et donc la probabilité de survenue d'un événement incertain et aléatoire ;augmenter la résilience économique ou communautaire en anticipant les mesures permettant de mieux combattre les conséquences d'un « sinistre » ;assurer une prestation, souvent financière, avant la survenance d'un événement accidentel auprès d'un individu, d'une association ou d'une entreprise, en échange de la perception d'une cotisation ou prime. Par extension, la prévention est alors le secteur économique assurantiel qui regroupe les activités de conception, de production et de commercialisation de ce type de service. On l'appelle aussi Prévention dommage.Les types de prévention sont nombreux : prévention des accidents (domestiques, au travail, de la route), des incendies, des blessures, prévention de la criminalité, du suicide, prévention des maladies, du cancer, prévention d'éducation spécialisée, prévention situationnelle, prévention des déchets, etc.La prévention est de plus en plus souvent au centre des politiques sociales et de santé. Elle s'organise parfois au travers de plans de prévention. Elle peut aussi apparaître comme une composante majeure de politiques publiques s'inspirant des théories de l'éthique de la sollicitude.La science qui étudie la prévention est la cindynique.La prévention se distingue de la protection qui prend elle des mesures visant à limiter l’étendue ou/et la gravité des conséquences d’un phénomène dangereux, sans en modifier la probabilité d'occurrence.Un accident est toujours la rencontre de plusieurs facteurs, dont souvent le hasard ou une négligence. En effet, s'il n'y a pas de négligence ou de hasard, il s'agit alors d'un crime, au sens large du terme : une action délibérément nuisible, malveillance, sabotage… Lorsqu'il n'y a pas de négligence mais seulement un hasard, on parle de calamité ou de catastrophe naturelle.La prévention consiste donc d'abord à essayer de prévoir les facteurs pouvant conduire à l'accident. Lorsqu'un accident se produit, il faut analyser ces facteurs (arbre des causes) afin d'éviter qu'un accident similaire ne se reproduise (capitalisation de l'expérience).Un des principaux moyens de prévenir les accidents consiste à informer les personnes soumises au risque. En effet, puisqu'il y a toujours au moins une négligence, il faut tenter d'influer sur les comportements afin de réduire ce facteur. La première étape de l'information est en général la sensibilisation : faire comprendre aux personnes quel est le risque et pourquoi il est nécessaire de changer (ou de maîtriser) son comportement. Définitions  Accident L'accident peut être défini comme un événement soudain, dommageable et non désiré, ayant pour conséquence des dégâts sur les personnes, les biens ou l'environnement. Danger L'accident étant la conséquence de plusieurs facteurs, si l'on supprime un seul facteur, on peut éviter l'accident, mais celui-ci reste toujours probable : on est en situation de danger. On pourrait définir un danger comme une situation dans laquelle il ne manque qu'un seul facteur pour qu'il y ait un accident.Par exemple, pour qu'il y ait une explosion de gaz, il faut un mélange explosif et une source d'énergie (étincelle, flamme, etc.). Si l'on est en présence d'un mélange explosif qui n'a pas encore explosé, il n'y a pas encore d'accident, mais il y a un danger. Risque Le risque pourrait être défini comme la prise en compte d'une exposition à un danger, c'est-à-dire que l'action est un facteur pouvant mener à l'accident. En effectuant l'action, on transforme une situation dangereuse en accident ; mais l'on n'est pas sûr que cela va déboucher sur l'accident : le comportement à risque ne débouche pas toujours sur un accident, on a donc fréquemment l'impression que ce comportement est inoffensif.Exemple d'une personne qui traverse une voie de chemin de fer en dehors d'un passage protégé. Il y a danger car le train est toujours intrinsèquement à même de blesser cette personne ; de même il y a un risque car la personne est exposée au danger. Toutefois, le risque n'implique pas toujours la réalisation d'un accident. Ainsi, la personne peut réussir à traverser de justesse ou bien avoir le réflexe de se jeter en arrière, auquel cas les conséquences de l'exposition au danger seront nulles. S'il n'y a pas de train à l'approche, le train est toujours dangereux mais la situation n'est pas dangereuse et le risque est nul car la personne n'est pas exposée au danger.Exemple d'un accident une route simple à deux voies en côte et une voiture doublant une autre voiture en montant la côte. Si une voiture circule en sens inverse, on a un danger accident (choc frontal), mais qui peut aussi se résoudre sans dégât (par exemple la voiture doublant réussit à se rabattre grâce au coup de frein de la voiture doublée et de la voiture venant de face). Si aucune voiture ne vient en face, il n'y a alors pas de danger, mais le conducteur doublant n'a aucun moyen de le savoir, il prend un risque.Le risque est donc un danger qui lui-même est un accident potentiel ; de ce fait, le risque n'est souvent pas perçu comme tel, mais il peut bel et bien mener à l'accident. C'est là qu'est toute la difficulté de la sensibilisation…que de danger » signifie donc « la probabilité de survenance d'un événement dommageable ». Analyse bénéfice/risque et protection/contrainte Les comportements humains font suite à une prise de décision (si l'on excepte les réflexes). Cette prise de décision s'appuie en général sur une analyse plus ou moins consciente de type « coût/gain » (dans le sens large et non pas financier), ou plutôt coût estimé/gain attendu, le coût estimé et le gain attendu étant plus ou moins éloignés des coût et gain réels.Hors comportements à risque volontaire, la personne qui décide de prendre le risque le fait car à son avis le bénéfice (gain) vaut le risque couru (le coût étant ici la probabilité et la gravité de l'éventuel accident). Si le comportement est à risque, c'est précisément que l'estimation faite par la personne est fausse, il ne s'agit pas d'une analyse rationnelle mais d’a priori, d'idées reçues, d'impressions.Exemple d’un conducteur qui a l'impression qu'en accélérant au-delà de la limite de vitesse, il arrivera plus tôt à sa destination tout en n'augmentant pas le risque d'accident. L'estimation du gain (de temps) et du coût (risque d'accident) sont tous les deux faussés.De même, le respect d'une mesure de sécurité est une contrainte (un coût) qui apporte une protection (gain). Si une personne ne respecte pas une consigne de sécurité, c'est qu'à son avis, la protection apportée ne justifie pas la contrainte.Exemple du cycliste qui a l'impression que le port du casque est contraignant (chaleur) et n'est pas nécessaire (la vitesse est faible comparé aux motocyclettes). Là encore, l'estimation du gain (protection contre les traumatismes crâniens) et du coût (casque supposé inconfortable) sont faussés.Pour amener les gens à respecter un règlement de sécurité, on peut donc :diminuer le coût de la mesure de prudence diminuer la contrainte, par exemple en soutenant financièrement l'acquisition du dispositif de sécurité, en étudiant un dispositif plus confortable (ergonomie) ;augmenter le bénéfice perçu de la mesure de prudence par exemple en valorisant l'attitude responsable (estime de soi), en accompagnant l'acquisition d'un « cadeau » (par exemple diminution de la prime d'assurance) ;augmenter le coût et diminuer le gain de l'attitude imprudente sanctionner les manquements aux obligations de prudence (par exemple amende et retrait de points sur le permis en cas de dépassement des limitations de vitesse ou oubli du port du casque, malus sur la prime d'assurance en cas de responsabilité dans un sinistre) ; rendre les produits néfastes moins disponibles ou plus chers (par exemple augmentation du prix des cigarettes, suppression des distributeurs de friandises dans les écoles) ;favoriser la réflexion sur l'analyse coût/gain sensibiliser, éduquer afin de montrer les erreurs d'analyse, et que la mesure proposée/imposée est justifiée. Estimation du risque L'estimation du risque est souvent faussée par des idées reçues. Ce décalage entre l'estimation et la réalité peut avoir plusieurs causes, notamment :la gravité perçue d'un accident ; l'horreur ressentie d'une situation va augmenter l'importance réelle du risque ; par exemple, les accidents d'avion apparaissent particulièrement mortels et d'autant plus choquants qu'on s'identifie aux victimes ; pourtant, le risque est faible (le nombre de morts dans le monde est dix fois inférieur au nombre de morts sur la route en France) et fumer conduit à une probabilité bien plus élevée d'agonie par cancer du poumon plus longue et plus douloureuse que celle-ci) ;S'ajoute à ceci un renforcement par la résonance des médias qui peuvent par exemple renforcer l'impression d'insécurité quand ils relatent des homicides, alors que ceux-ci ne représentent en France qu'environ 400 morts par an (0,7 décès pour 100 000 habitants contre 17,5 pour les suicides et 12,9 pour les accidents de la route).Par ailleurs, le risque estimé est comparé avec un « risque acceptable » : étant évident que le « risque zéro » n'existe pas d'une part, et d'autre part qu'un bénéfice ne peut s'acquérir qu'en courant un risque, chaque personne évalue implicitement un risque acceptable, qui est le danger qu'elle accepte de courir, l'accident qu'elle trouve normal de subir, par exemple comme sanction d'échec ou fatalité. Cette notion de risque acceptable comporte des dimensions sociales et psychologiques. Par exemple, pour un grand nombre de citoyens français, les accidents de la route sont acceptables alors qu'ils causent de nombreux morts et que l'on peut agir par un comportement individuel ; à l'inverse, une inondation sur laquelle on ne peut agir et qui fait peu de victimes paraît inacceptable.Cette double source d'irrationalité interfère avec la prise de risque : irrationalité de l'estimation du risque, et irrationalité de la référence (risque acceptable).Pour estimer de manière plus précise les risques sans a priori, il faut donc se reporter aux statistiques. En France, les décès sont principalement dus, :Note : les chiffres proviennent de plusieurs sources et peuvent correspondre à des années différentes, il ne faut donc les considérer que comme des ordres de grandeur ; se reporter aux références pour plus de détailsCette analyse dépend bien entendu de la manière dont on répartit les causes de décès. Si l'on considère par exemple non pas la pathologie menant au décès mais le comportement ayant favorisé la pathologie, on voit quel'obésité cause 178 000 morts par an (287 pour 100 000 hab.), ce qui en ferait donc la première cause de décès ;60 000 morts par an (97 pour 100 000 hab.) sont associés au tabagisme en France, dont 25 000 par cancer du poumon (soit 90 % des cancers du poumon), 15 000 à 40 000 par bronchite chronique (bronchopneumopathie chronique obstructive ou BPCO) ;l'alcoolisme cause environ 45 000 décès par an (73 pour 100 000 hab.) : 23 000 décès directs (11 000 cancers des lèvres, de la bouche, du pharynx et du larynx, 9 000 cirrhoses, 2 500 par alcoolo-dépendance), et 22 000 morts indirectes (troubles mentaux, maladies cardiovasculaires, accidents…) ; tous les ans, 5 000 à 7 000 bébés naissent en France avec des malformations graves ou retard mental en raison de l'alcoolisme de la mère. L'alcool est maintenant un cancérogène reconnu (plus encore s'il est associé au tabac), et en tant que désinhibiteur, il favorise de nombreuses conduites à risque (dont actes violents, rapports sexuels non protégés, conduite dangereuse…).La médecine préventive fait partie de la santé publique. La prévention des maladies est complexe pour différentes raisons :Les causes en sont moins évidentes : la relation entre tabac et maladies cardiovasculaires n'a pu être prouvée qu'après de nombreuses études épidémiologiques incluant plusieurs dizaines de milliers de personnes suivies pendant de nombreuses années[réf. nécessaire].L'efficacité de la prévention est moindre, s'agissant souvent uniquement d'une réduction du risque et non son abolition (mis à part, peut-être, la vaccination qui permet parfois d'exclure la maladie concernée).L'évaluation des résultats de la prévention est également plus difficile à réaliser (études d’intervention)[réf. nécessaire]. Elle est nécessaire et peut révéler parfois des surprises : dans les années 1980, certains médicaments anti-arythmiques, donnés en prévention de la mort subite de l'adulte (arrêt cardiorespiratoire), ont finalement entraîné plus de décès que chez un groupe de patients non traités[réf. nécessaire].Elle est parfois plus coûteuse, au point de poser des problèmes de santé publique : On sait[réf. nécessaire] que le défibrillateur implantable est efficace dans la prévention de la mort subite chez certains patients bien ciblés, mais le coût important de cette technique limite sa diffusion actuelle.L'information est parfois biaisée par des intérêts financiers ou de pouvoir : l'intérêt des alicaments, qui bénéficient d'une large publicité, n'est que pourtant rarement prouvé[réf. nécessaire].La prévention, à une « juste mesure et au bon moment » et donc bien définie, est fondamentale en médecine et en épidémiologie : l'action la plus efficace est a priori celle qui limite le risque qu'il y ait des victimes. Mais une vision à long terme est nécessaire, car une prévention qui semble pertinente et efficace à court terme peut avoir à long terme un effet inverse de celui qui était recherché. Ainsi « trop d'hygiène » ne permettant plus à l'individu d'entretenir une immunité normale face aux microbes et parasites, ou une utilisation préventive intensive d'antibiotique(s) (en médecine ou dans l'alimentation animale) peut conduire à des phénomènes d'antibiorésistance et à des maladies nosocomiales à grande échelle.Dans le cas d'actions ou d'inaction susceptibles d'avoir des conséquences en chaîne (« effet domino » ou « boule de neige »), le comportement individuel (hygiène de vie, réduction de la prise de risque) a parfois autant d'importance que la stratégie du groupe. C'est le cas pour les maladies contagieuses ou sexuellement transmissibles, mais aussi pour beaucoup d'autres facteurs de risque.On distingue classiquement la prévention primaire visant à éviter la maladie chez l'individu qui n'a jamais été malade, de la prévention secondaire destinée à diminuer les suites et la gravité de l'affection chez un patient ayant déclaré la maladie ou visant à dépister plus tôt la maladie dans les populations présentant un risque important de déclaration de cette dernière. La prévention tertiaire, enfin, cherche à empêcher les complications ou les rechutes.L'Organisation mondiale de la santé (OMS) définit trois niveaux de prévention :La prévention primaire désigne l'ensemble des actes destinés à diminuer l'incidence d'une maladie, donc à réduire l'apparition des nouveaux cas. En agissant en amont de toute manifestation clinique, cette prévention empêche l'apparition de la maladie. Elle utilise l'éducation et l'information auprès de la population.La prévention secondaire désigne l'ensemble des actes destinés à diminuer la prévalence d'une maladie, donc à réduire sa durée d'évolution. Intervient dans le dépistage de toutes les maladies et comprend le début des traitements de la maladie.La prévention tertiaire désigne l'ensemble des actes destinés à diminuer la prévalence des incapacités chroniques ou des récidives dans la population, donc à réduire les invalidités fonctionnelles dues à la maladie. Agit en aval de la maladie afin de limiter ou de diminuer les conséquences de la maladie et d'éviter les rechutes. Dans ce stade de prévention les professionnels s'occupent de la rééducation de la personne et de sa réinsertion professionnelle et sociale.La prévention quaternaire désigne initialement, en santé publique, l'ensemble des soins auprès de malades qui ont dépassé le stade des soins curatifs et qui se trouvent parfois aussi en phase terminale. La prévention quaternaire inclut donc l'accompagnement des personnes en fin de vie. Le terme soins palliatifs est cependant préférable et plus répandu.Une autre définition, plus récente, de la prévention quaternaire la considère comme l'ensemble des actions menées pour identifier un patient ou une population à risque de surmédicalisation, le protéger d'interventions médicales invasives, et lui proposer des procédures de soins éthiquement et médicalement acceptables. La prévention quaternaire devient alors l'ensemble des activités de santé pour atténuer ou éviter les conséquences de l'intervention inutile ou excessive du système de santé. Dans cette définition, la prévention quaternaire se rapproche de la prévention de la iatrogénèse.Une approche alternative se réfère aux groupes sur lesquels on peut agir, plutôt que sur la maladie à prévenir :Prévention universelle (pour l'ensemble de la population) ;Prévention spécifique (pour un groupe vulnérable) ;Prévention indiquée (pour un individu à risque).Le comportement à risque ne consiste pas uniquement à faire des actions dangereuses ; lorsque l'on considère les principales causes de décès, on voit que le comportement quotidien (l'alimentation, la sédentarité…) peut être considéré comme la première cause de décès.On peut réduire les risques de maladie cardiovasculaire et de cancers, de manière relativement simple et sans danger, en adoptant ces comportements :en mangeant des fruits et légumes, au moins 800 g par jour (soit environ cinq portions par jour) : ils contiennent des anti-oxydants qui réduisent les risques de cancers, ils « remplissent le ventre » sans apport excessif de graisses et de sucres ; l'Organisation mondiale de la santé estime qu'un apport suffisant en fruits et légumes permettrait de réduire de 3 millions les décès dus aux maladies cardiovasculaires et cancers,,. Chez les non-fumeurs, le cancer qui a le potentiel de prévention le plus important est le cancer colorectal : on estime que les 3/4 des cancer du côlon pourraient être évités par une meilleure hygiène de vie. Cela a été démontré en particulier par l'étude SU.VI.MAXen préparant soi-même ses repas : les industriels agroalimentaires ayant tendance à mettre des additifs pas chers pour augmenter le poids de leurs produits (sel, graisses, sucre), il est donc préférable de préparer soi-même ses repas à partir de produits frais[réf. nécessaire] ;en ayant une activité physique minimale équivalente à une demi-heure de marche par jour, ou mieux en pratiquant un sport régulièrement ;en ne fumant pas et en ne buvant que modérément ;en évitant les expositions excessives au soleil en été, surtout chez les jeunes enfants (risque de développement de cancers de la peau).Quatre facteurs de mode de vie peuvent faire varier de 14 ans l'espérance de vie. Une équipe de chercheurs de l'université de Cambridge (Royaume-Uni), en partenariat avec le Medical Research Council, a mené une enquête sur 20 244 individus pendant 14 ans (1993-2007), dont 1 987 sont décédés en cours d'enquête, afin de déterminer l'impact du mode de vie sur l'espérance de vie[source insuffisante]. L'étude conclut que « le mode de vie idéal » - absence de tabac, consommation d'alcool égale ou inférieure à un demi verre par jour, consommation de 5 fruits et légumes par jour, exercice physique d'une demi heure par jour - majore l'espérance de vie de 14 ans par rapport au cumul de quatre facteurs de risque. Le cumul des quatre facteurs de risque (tabac, alcool, manque de fruits et légumes et d'exercice physique) multiplie le risque de décès par 4,4, trois facteurs, de 2,5, deux facteurs de près de 2 et 1 facteur de 1,4. Selon le professeur Kay-Tee Khaw, premier signataire de l'étude, « C'est la première fois que l'on analyse l'effet cumulé des facteurs de risque sur la mortalité. »En France, les cancers sont la première cause de mortalité et les maladies infectieuses la troisième.Cette mortalité concerne surtout les personnes âgées, à l'exception du sida ; on est donc tenté de penser qu'il s'agit là d'un phénomène « normal » touchant les personnes en fin de vie. Cela serait oublier que les maladies infectieuses ont été pendant des millénaires la première cause de mortalité infantile comme adulte — c'est d'ailleurs toujours la première cause de mortalité dans le monde [1], essentiellement dans les pays en voie de développement [2] — et que leur régression est due certes aux progrès des soins médicaux, mais aussi à la prévention : hygiène et vaccinations. D'ailleurs, les populations jeunes des pays développés ayant perdu ces notions sont particulièrement exposées, comme les sans domicile fixe.Outre une bonne hygiène de vie (alimentation, exercice, éviter les comportements à risque, cf. section ci-dessus) qui permettent d'avoir un meilleur état de santé général (donc de mieux résister aux infections) et d'éviter les cancers, il faut également insister sur :l'hygiène corporelle : se laver, notamment se laver les mains avant un repas et après être allé aux toilettes, se brosser les dents deux fois par jour ;l'hygiène ménagère : stockage des ordures dans des poubelles dédiées et ramassées régulièrement par les services municipaux, évacuation des eaux usagées vers une fosse septique vidangée régulièrement ou vers les égouts, rangement et nettoyage de l'habitation, aération pour éviter les pollutions intérieures (acariens, produits organiques volatils) et donc les allergies et les maladies respiratoires ;les vaccinations : si certaines maladies ont quasiment disparu en France (comme le tétanos, ou la rougeole qui continue à causer 900 000 morts par an dans le monde), c'est grâce aux vaccinations ; la vaccination des personnes âgées contre la grippe est recommandée ;dépistage : la détection précoce d'une maladie permet de démarrer son traitement plus tôt et donc de réduire la mortalité ; il est recommandé de faire au moins une visite médicale par an ; pour les maladies sexuellement transmissibles, il existe en France des centres anonymes et gratuits de dépistage ;prophylaxie : dans certaines conditions, il est possible de prendre des mesures préalables pour réduire le risque de contacter une maladie, par exemple utiliser un préservatif pour réduire le risque de contracter une maladie sexuellement transmissible, prendre des médicaments contre le paludisme lors d'un voyage dans un pays impaludé…Il faut aussi prendre précautionneusement les médicaments prescrits par un médecin, en lisant systématiquement les notices accompagnatrices, riches en informations (effets secondaires, interactions avec d'autres médicaments, recommandations…) et ne pas hésiter à questionner le médecin ou le pharmacien en cas de doute. Les effets peuvent ne pas être immédiats, et il faut continuer le traitement jusqu'à la fin même en cas d'amélioration et disparition des symptômes, notamment dans le cas des antibiotiques : la disparition des symptômes signifie la diminution du nombre de germes, mais pas leur disparition, si l'on interrompt le traitement trop tôt, ceux-ci peuvent se re-développer, et devenir résistants à l'antibiotique.Il faut également limiter l'automédication aux seuls médicaments en vente libre, après conseil du pharmacien et lecture de la notice ; il ne faut pas reprendre un médicament prescrit par le médecin même si les symptômes semblent les mêmes, par exemple, une angine peut être virale ou bactérienne, les symptômes sont similaires mais le traitement différent. En particulier, les antibiotiques sont sans effet sur les virus. Notons également que les intoxications médicamenteuses causent 600 morts par an.Le rôle de la collectivité (l'État) est primordial, pour organiser l'hygiène collective, le suivi de la santé, l'information et la sensibilisation.On estime que six millions d'enfants de moins de cinq ans meurent par an. Selon Bryce et al., on pourrait facilement réduire la mortalité infantile dans les pays en voie de développement par un programme préventif à bas coût (estimé à 1,23 USD par enfant, 4,6 milliards d'USD par an pour les 42 pays concentrant 90 % des cas de mortalité infantile), comprenant douze mesures :traitement antipaludéen prénatal,vaccination anti-tétanos des femmes enceintes,délivrance d'antirétroviraux avant l'accouchement et programme d'alimentation artificielle pour les enfants nés de mères VIH positives,amélioration des soins néonataux (accouchement par un professionnel entraîné, surveillance de la température, antibiotiques en cas de rupture prématuré des membranes ou de sepsis, administration de corticoïdes si nécessaire),incitation à l'allaitement maternel,délivrance de moustiquaires traitées aux insecticides,vaccination contre l'Haemophilus influenzae de type b (Hib),supplémentation en zinc et en vitamine A,suppléments alimentaires entre 6 et 9 mois,assainissement de l'eauvaccination contre la rougeole.Cette démarche préventive se ferait au cours de 18 contacts dans les cinq premières années, alors que le traitement curatif de ces maladies et carences nécessite en moyenne 35 contacts sur cette période.Parmi les actions de l'Éducation spécialisée en France il existe une forme d'action nommée « Prévention Spécialisée ». Cette action est parfois, écrite et nommée, « la P.S ». On peut retrouver cette manière de la prévention dans de nombreux écrits et propos des travailleurs sociaux, des fonctionnaires, des élus locaux, des spécialistes en lien avec cette action.La Prévention Spécialisée est une forme d’intervention sociale placée sous la responsabilité du Conseil général depuis la loi de décentralisation du 6 janvier 1986. Inscrite dans le Code de l’action sociale et des familles, elle est une prestation de l’Aide Sociale à l’Enfance.Cette action vise à rompre avec l’isolement et restaurer le lien social des jeunes en voie de marginalisation. Dernier recours face à l’échec des autres démarches éducatives institutionnelles, elle vise à favoriser la reconstruction des liens sociaux, une meilleure intégration des jeunes en rupture, la lutte contre l’exclusion sous toutes ses formes.Elle se caractérise par :le non-mandat nominatif ;la libre adhésion ;le respect de l'anonymat.Principalement tournée vers les jeunes de 16 à 25 ans, elle peut, selon les départements, s'adresser à des plus jeunes. Les éducateurs de prévention, généralement des éducateurs spécialisés, vont à la rencontre des jeunes dans leurs lieux de rencontre. Ils sont de ce fait régulièrement appelés « Éducateurs de Rue ».Dans certains pays, la législation oblige tout employeur à effectuer une analyse de risques ; en France, cette analyse doit être synthétisée dans un document unique.Les risques sont évalués selon deux critères : probabilité de l’événement non souhaité et gravité du dommage causé dont les accidents qui font partie des dommages les plus graves. Il en va ainsi de la responsabilité de l'employeur de prendre des mesures de prévention ou de protection adéquates afin d'éviter ces accidents pour les premières ou de diminuer la grièveté de ceux-ci pour les deuxièmes.Chaque année en France métropolitaine, les accidents domestiques causent environ 20 000 morts (soit 3,6 % des décès), 80 000 morts dans l'Union européenne et plusieurs millions de morts dans le monde. On parle aussi d'accident de la vie courante (AcVC).Les principales causes sont (le nombre de morts indiqué est pour la France en 1999) :chutes (10 520 morts, dont 95 % sont des personnes de plus de 65 ans)suffocation (3 543 morts)intoxication (758 morts, 600 par médicaments, 158 par d'autres substances et gaz)noyade (547 morts)feu (460 morts)En France, les accidents de la route font environ 7 000 morts par an et plus de 100 000 blessés. En 2001, ils ont représenté 61,2 % des accidents du travail mortels (accidents de parcours compris), avec 836 accidents, selon les données de la Caisse primaire d'assurance maladie.En 1999 en France, le suicide a causé la mort de 12 000 personnes, soit plus que les accidents de la circulation.La prévention est complexe et délicate, les situations ne sont pas toujours évidentes à détecter. D'une manière générale, on peut dire que le suicidant (celui qui fait une tentative de suicide) est très fréquemment en état de dépression.Il faut donc veiller à soutenir un proche en situation difficile (séparation, perte d'un être proche, perte d'emploi, adolescence, échec scolaire, échec sentimental, perte d'autonomie…), c'est-à-dire de l'écouter sans le juger et sans faire d'analyses pseudo-psychanalytiques, voire de stimuler le dialogue avec une phrase du type « je ne te sens pas bien en ce moment ». On peut essayer de stimuler la personne à sortir et se divertir malgré une perte d'envie, mais éviter à tout prix les phrases du type « fais un effort ! » ou « prends un peu sur toi ! » : la personne fait déjà des efforts énormes pour essayer de se sortir de sa déprime. On peut enfin l'inviter à consulter un médecin généraliste.Lorsque l'on sent que le passage à l'acte est imminent, il faut prévenir les secours (112 dans l'Union européenne, 15 ou 18 en France).La formation aux premiers secours est une mesure de prévision (agir après la survenue de l'accident), mais c'est aussi une mesure de prévention : en effet, une personne formée est plus consciente des risques, elle fait plus attention aux autres personnes, elle adopte donc spontanément un comportement plus prudent. La sensibilisation aux risques est d'ailleurs un des buts explicites de ces formations.Aux États-Unis, la prévention néonatale (neonatal prevention) renvoie à un cas particulier de prévention. Celle de la déficience en fer du nouveau-né, que l'on peut prévenir par transfusion du sang du cordon clampé à son extrémité placentaire.(en) Sackett DL. « The arrogance of preventive medicine » CMAJ. 2004;167:363-4.Andrée Charles, Farid Baddache. Prévenir les risques. Agir en organisation responsable, éditions AFNOR, 2006.  (ISBN 2-1247-5519-6).(es) Gérvas J, Pérez Fernández M. « Los límites de la prevención clínica » AMF. 2007; 3(6):352-60.(es) Gérvas J, Pérez Fernández M, González de Dios J. « Problemas prácticos y éticos de la prevención secundaria. A propósito de dos ejemplos de pediatría » Rev Esp Salud Pública 2007;81:345-52.(en) Starfield B, Hyde J, Gérvas J, Heath I. « The concept of prevention: a good idea gone astray ? » J Epidemiol Community Health. 2008;62(7):580-3.(en) Gérvas J, Starfield B, Heath I. « Is clinical prevention better than cure ? » Lancet 2008;372:1997-9(es) Gérvas J, Pérez Fernández M. « Los daños provocados por la prevención y por las actividades preventivas » RISAI. 2009; 1(4).(pt) Gérvas J. « Abuso de la prevención clínica. El cribaje del cáncer de mama como ejemplo » Rev Espaço Saùde. 2009; 11(1):49-53.(en) Agence européenne de l'environnement, « Signaux précoces et leçons tardives, (Late Lessons from Early Warnings) » Environmental issue report no 22/2001, publié le 9 janvier 2002 et résumé(en) Agence européenne de l'environnement (2013), Late Lessons from Early Warnings no 2 ; Environmental issue report, 750 pagesDéfinition et évaluation des risques pour la santé, extrait du Rapport sur la Santé dans le Monde 2002 - Réduire les risques et promouvoir une vie saine, Organisation mondiale de la santé (fichier [PDF], 21 p.)Site officiel de l'Institut national de prévention et d'éducation pour la santé d'Ile-de-FranceSite officiel de l'instance régionale en éducation et promotion de la santé d'Ile-de-France Portail des risques majeurs   Portail de la médecine   Portail des premiers secours et du secourisme"
médecine;"Les soins palliatifs sont des soins qui ne visent qu'au confort du malade, souvent en phase de fin de vie. L'objectif des soins palliatifs est de prévenir et de soulager les douleurs physiques, les symptômes inconfortables (nausées, constipation, anxiété...) ou encore la souffrance psychologique. Un soin palliatif est une mesure visant à endiguer les conséquences d'un grave problème médical, en ne se préoccupant plus de sa cause.En parallèle, une aide psychologique, morale, spirituelle peut être offerte aux proches du patient.— Cicely Saunders (1918-2005)   Les soins palliatifs ont pour mission d'améliorer la qualité de vie des patients atteints d'une maladie évolutive grave ou mettant en jeu le pronostic vital ou en phase avancée et terminale.  Les soins palliatifs ne sont pas le synonyme de « soins de fin de vie », bien que les soins terminaux soient des soins palliatifs. Ainsi, les patients bénéficiant de ces soins sont aussi ceux qui ont l’espérance de vivre encore plusieurs mois ou quelques années avec une qualité de vie acceptable malgré la présence d'une maladie inéluctablement évolutive.On dit souvent des soins palliatifs qu'ils sont « tout ce qu'il reste à faire, quand il n'y a plus rien à faire ». Autrement dit, pour un malade recevant des soins palliatifs, l'objectif n'est plus la guérison de sa maladie causale mais la lutte contre tous les symptômes inconfortables qui découlent de cette maladie, dont la douleur, la fatigue et l'anorexie. La démarche de soins palliatifs peut donc parfois envisager la prise d'un traitement médical ou la réalisation d'un acte chirurgical si ce traitement permet de soulager un symptôme inconfortable. Elle vise aussi à éviter les investigations et certains traitements déraisonnables s'ils ne peuvent faire espérer une amélioration de confort. Ce qui prime avant tout est le confort et la qualité de vie définie de manière personnalisée avec le patient. Les soins palliatifs cherchent à limiter les ruptures de prise en charge en veillant à la bonne coordination entre les différents acteurs du soin. Les aspects sociaux, et éventuellement religieux et spirituels, sont pris en compte.Les proches sont aussi accompagnés dans la compréhension de la maladie de leur proche et in fine dans leur cheminement de deuil.Le patient a le choix de recevoir ses soins où il souhaite à partir du moment où le médecin traitant lui assure une accessibilité complète aux soins palliatifs compte tenu de la gravité de sa maladie. Les soins sont effectués en hôpital ou à domicile, les services à domicile sont plus complexes à obtenir car il faut un accord supplémentaire du médecin traitant, ainsi qu’une équipe spécialisée disponible pour apporter les soins nécessairesIl s'agit de l'ensemble des valeurs portées par ce qu'on appelle « le mouvement des soins palliatifs » dont l'origine remonte aux pionnières anglo-saxonnes du « Saint Christopher Hospice » autour de Cicely Saunders.  Le docteur Maurice Abiven (1924-2007), spécialiste de médecine interne, fut l'un des pionniers de la pratique des soins palliatifs en France et Charles-Henri Rapin (1947-2008), médecin gériatre suisse, l'est dans le monde francophone de la gériatrie. Ce mouvement s'appuie sur des concepts éthiques faisant une large part à l'autonomie du malade, au refus de l'obstination déraisonnable ainsi qu'au refus de vouloir hâter la survenue de la mort. Michel Castra explique : « Cherchant à s'affranchir du cadre traditionnel d'une médecine techniciste et scientifique, les promoteurs des soins palliatifs sont parvenus à affirmer les principes d'une médecine privilégiant une logique de confort et ayant pour objectif de lutter contre les conséquences d'une maladie devenue incurable... (Il s'agit) d'une redéfinition des conceptions du « bien » pour le malade qui est ici à l'œuvre et qui marque un changement de légitimité de l'action médicale fondée non plus sur une rationalité strictement biomédicale mais sur de nouvelles croyances dans la finalité de l'intervention soignante auprès des patients terminaux : il s'agit notamment de privilégier la qualité de la vie qui reste à vivre sur la durée de cette vie... (Elle implique le) refus de l'euthanasie et de l'acharnement thérapeutique ». Certains partisans des soins palliatifs en tant que concept de prise en charge sont donc opposés à l'euthanasie définie comme l'administration de substances à doses mortelles dans le but de provoquer la mort dans un objectif compassionnel. Un des points importants défendu par le mouvement des soins palliatifs est la place à reconnaitre dans notre société à « celui qui meurt ». Pour le mouvement des soins palliatifs il est important de se rappeler que la mort est un phénomène naturel de la vie. Les soins palliatifs sont des soins actifs délivrés dans une approche globale de la personne atteinte d'une maladie grave évolutive ou terminale. Il faut également évoquer l'importance accordée à la prise en compte de la souffrance globale du patient : physique, sociale, psychologique, spirituelle et de son entourage. Dans une approche interdisciplinaire, une place particulière est accordée aux bénévoles d'accompagnement dans la démarche de soins dans le cadre des soins de support.L'Église catholique condamne l'euthanasie mais soutient les pratiques palliatives. La Congrégation pour la doctrine de la foi a rappelé l’obligation d’alimenter et d’hydrater les malades en état végétatif, dans un document rendu public le 14 septembre 2007. Le Vatican répond à deux questions posées par les évêques américains à la suite de l'affaire Schiavo en 2005 : il dit oui à l’administration de nourriture et d’eau, « moralement obligatoire » et non à la possibilité d’interrompre la nourriture et l’hydratation fournies par voies artificielles à un patient en état végétatif permanent.Pour ce qui est des sédatifs, il est « licite de supprimer la douleur au moyen de narcotiques, même avec pour effet d'amoindrir la conscience et d'abréger la vie » (affirmation de Pie XII rappelée dans l'encyclique Evangelium Vitae, 65).Il existe de nombreuses définitions « officielles » des soins palliatifs : définition de la loi française (juin 1999), définition de l'Organisation mondiale de la santé, autres (Ordre des médecins, ANAES). L'introduction des soins palliatifs en France a été plus tardive qu'en Grande-Bretagne et aux États-Unis, d'où est parti le « hospice movement » après la Seconde Guerre mondiale, avec Cicely Sanders, pionnière des soins palliatifs. Elle a été officiellement reconnue par la circulaire Laroque de 1986 « relative à l'organisation et à l'accompagnement des malades en phase terminale ». Depuis 1991, « ces soins font partie des missions de l'hôpital et leur accès est présenté comme un droit des malades » (Comité consultatif national d'éthique, avis no 63 ).  La loi du 9 juin 1999 et la circulaire DHOS/O2/DGS/SD5D du 19 février 2002 tracent le droit à l'accès aux soins palliatifs. Ces derniers sont  également inscrits dans le Code de déontologie médicale de 1995 (art. 37-38), qui rejette aussi l'acharnement thérapeutique. À la suite de la mission Jean Leonetti « sur l’accompagnement de la fin de vie » menée en 2002, la « loi relative aux droits des malades et à la fin de vie » de 2005 a été promulguée, tandis qu'un Observatoire national de la fin de vie était inauguré en février 2010 par la ministre Roselyne Bachelot.La circulaire de 2002 édicte les modalités d'organisation des soins palliatifs, souligne l'accès inégal de ceux-ci sur le territoire national. Les objectifs législatifs sont :le respect du choix du malade sur les conditions et le lieu de leur fin de vie ;l'adaptation et la diversification de l'offre territoriale de SP et l'articulation entre les différents dispositifs, structures et instances concernées ;la promotion du bénévolat et des soins de support.D'autre part :chaque département devra être doté d'un réseau de soins palliatifs.chaque établissement de santé se doit d'organiser les soins palliatifs dans son projet d'établissement, avec mise en place de formations de personnels, organisation de soutien des soignants, réflexion sur l'accueil et l'accompagnement des familles.La circulaire définit les notions d'unités de soins palliatifs, de lits « identifiés soins palliatifs », d'équipes mobiles de soins palliatifs.L'Agence régionale de l'hospitalisation (ARH) a été chargée de cette mise en œuvre. Depuis 2009, l'Agence régionale de santé (ARS) se substitue à l'ARH.Le 2 février 2016, le Code de la santé publique  intègre les modifications intervenues à la suite de la « petite loi » adoptée par l'Assemblée Nationale à partir de la proposition de loi créant de nouveaux droits en faveur des malades et des personnes en fin de vie déposée par les députés Leonetti et Claeys. Soins palliatifs et tarification à l'activité Le codage des soins palliatifs (SP) regroupe trois groupes homogènes de séjours (GHS) selon le lieu du séjour, respectivement dans un lit sans autorisation spéciale (GHS 7956), dans un lit « dédié » aux SP (GHS 7958), ou si le séjour a lieu dans une unité de SP (GHS 7957). La tarification est différente selon ces 3 cas. Le code DP (diagnostic principal) de Soins palliatifs est le code Z51.5.Les soins palliatifs peuvent, être pratiqués par toutes les équipes soignantes spécialisées dans l'accompagnement des malades en fin de vie, aussi bien au domicile qu'en milieu hospitalier. Cependant, il existe des situations complexes nécessitant l'intervention d'équipes de soins palliatifs (à caractère pluridisciplinaire). En France, on distingue habituellement : Les unités de soins palliatifs où se gèrent des situations de phases terminales complexes ne pouvant se dérouler au domicile ou en milieu hospitalier traditionnel en raison notamment de la survenue de syndromes réfractaires, c’est-à-dire résistants aux traitements habituels, altérant la qualité de vie restante du malade.Les équipes mobiles de soins palliatifs qui interviennent soit au sein des services d'un même hôpital, soit au sein de plusieurs établissements, soit à domicile, pour venir appuyer et conseiller les équipes référentes dans la prise en charge de patients atteints de maladies graves et potentiellement mortelles. Elles n'ont pas vocation à se substituer à l'équipe soignante.Les réseaux de maintien à domicile, sont chargés de coordonner l'action des soignants et des équipes mobiles prenant en charge un patient atteint d'une maladie grave et potentiellement mortelle.D'autres structures comme l' hospitalisation à domicile (HAD), les services d'hospitalisation à domicile, ou des lits identifiés pour la pratique des soins palliatifs au sein d'un service, complètent l'ensemble de ces structures spécialisées « en soins palliatifs ».Les services de médecine, de chirurgie ou de soins de suites et réadaptation (SSR), sans avoir le titre d'unités de soins palliatifs peuvent également assurer cette mission, d'autant que les besoins de la population sont bien supérieurs au nombre de lits disponibles ou d'unités de SP.La mise en place de soins palliatifs et d’accompagnement en maternité et en néonatalogie est en cours d’élaboration en France depuis les années 2000. Comme toute démarche de soins palliatifs, elle repose sur  un accompagnement pluridisciplinaire qui :accorde une grande place à l’écoute de la souffrance des parents face à la maladie de leur enfant à naître ou à celle de leur nouveau-né ;assure un suivi médical rapproché de la maman et du bébé pendant tout le temps de la grossesse ;permet l’élaboration d’un projet de vie pour le bébé. En accord avec les parents, le pédiatre définit les soins que pourra recevoir le bébé après sa naissance : soins de confort et soins proportionnés qui excluent tout acharnement thérapeutique et qui contribuent au bien-être du bébé ;privilégie la présence des parents auprès du bébé et de la fratrie pendant tout le temps de vie de l’enfant malade. Et si l’état du bébé le permet, son  retour au domicile familial  peut être envisagé en liaison avec le médecin traitant de la famille et en collaboration avec un réseau de soins palliatifs à domicile.Cette démarche de soins palliatifs et d’accompagnement est possible en cas de diagnostic prénatal d’une maladie létale du bébé à naître, dans le cadre d’une poursuite de la grossesse mais aussi dans les situations où le pronostic vital du  nouveau-né est engagé après sa naissance.La mise en place de soins palliatifs en service de pédiatrie est apparue dans les services d'oncologie pédiatrique au cours des années 1980-1990 avec le développement de la discipline de psycho-oncologie pédiatrique, puis s'est développée dans d'autres spécialités.Une étude de l'INSERM de février 2002 indique que les médecins français « restent peu formés » en matière de soins palliatifs et pointe les difficultés d'accès des patients à ce type de prise en charge. Ce sont ainsi 57 % des patients habitant en zone rurale, et 67 % des patients âgés de plus de 65 ans qui n'ont pas eu accès aux soins palliatifs, ces deux statistiques n'étant pas indépendantes. En octobre 2014, le Comité consultatif national d'éthique (CCNE) publie un rapport de synthèse soulignant le « non-respect du droit d'accéder à des soins palliatifs pour l'immense majorité des personnes en fin de vie ».Le nombre de structures dédiées bien qu'en progression constante reste encore insuffisant et  inégalement réparti sur le territoire.Les moyens employés par les médecins pour procurer des soins de fin de vie incluent la sédation profonde et continue.À cet égard, l’Académie nationale de médecine a déploré le glissement sémantique entre « fin de vie » et « arrêt de vie » autour de la question de la sédation, abordée par le texte publié par le Conseil national de l'Ordre des médecins et intitulé « Fin de vie, Assistance à mourir », le 14 février 2013.Les bénévoles font partie d'une association d'accompagnement de la maladie grave et de la fin de la vie. Ils sont recrutés, formés et encadrés par leur association. Une période de formation initiale leur permet de débuter des accompagnements avec l'appui d'un tuteur (bénévole expérimenté), soit dans un service clinique (oncologie, réanimation, médecine interne, gastro-entérologie, gériatrie), soit dans les soins palliatifs (unité de soins palliatifs, lits identifiés de soins palliatifs, équipe mobile de soins palliatifs, réseau de santé) ou à domicile (EHPAD, maison de retraite). Il est recommandé d'accomplir ce bénévolat au sein d'une équipe et sans exclusive d'accompagnement.Ce bénévolat a été inscrit dans la loi du 9 juin 1999 (loi no 9-477, article 10) visant à garantir le droit à l'accès aux soins palliatifs pour tout citoyen, puis dans le code de santé publique (article L1112-5). Le rôle du bénévole d'accompagnement se situe dans la présence (même silencieuse) et l'écoute empathique de la personne malade et de ses proches au nom d'une société dont il est toujours membre. Il n'interfère jamais dans les soins. Il connaît et applique la charte de la personne hospitalisée  notamment la confidentialité, le respect des convictions philosophiques et religieuses, l'intimité de la personne.Il participe chaque mois à un groupe de paroles de deux heures animé par une psychologue clinicienne. Cela lui permet de déposer ses ressentis, ses difficultés, ses limites et son impuissance, de visiter son système de défense, ses croyances, sans jugement d'autrui et tolérance des autres participants qui œuvrent à cette dynamique collective.  Cet exercice libère les nœuds, les énergies mais ne constitue pas une thérapie personnelle ou de groupe.Ce cheminement personnel du bénévole est sous tendu par une éthique de convictions et de responsabilité (Max Weber) dont les attributs sont : la dignité intrinsèque et la vulnérabilité de tout homme, sa dimension spirituelle, de solidarité enfin, ciment d'une société démocratique et fraternelle.Ressources relatives à la santé : (en) Medical Subject Headings (en) NCI Thesaurus Suzanne Philips-Nootens « La personne en fin de vie : le regard du droit civil au Québec » RDUS 2009-2010, vol. 40, p. 327[PDF].Michèle-H. Salamagne et Emmanuel Hirsch, Accompagner jusqu'au bout de la vie. Manifeste pour les soins palliatifs, Coll. Recherches morales, 2e édit., Les éditions du Cerf, Paris 1993, 145 p.Aline Cheynet de Beaupré, Vivre et laisser mourir. (D.2003.2980)Serge Paugam (sous la direction de) Repenser la solidarité. PUF, 2007Isabelle de Mézerac, Un enfant pour l'éternité, Éditions du Rocher, 2004 Au Canada Palli-Science : portail conçu pour le rehaussement des soins palliatifs En Belgique Soins palliatifs : portail wallon (Belgique francophone) En France HELEBORCentre national de ressources soin palliatifSociété française d'accompagnement et de soins palliatifsDossier documentaire de la Société Française de Santé Publique En Suisse Palliative.ch : société suisse de médecine et de soins palliatifs Portail de la médecine   Portail des soins infirmiers   Portail de la bioéthique   Portail de la psychologie"
médecine;"En médecine, un traitement, appelé aussi traitement médical, traitement thérapeutique, thérapie ou plus généralement thérapeutique, est un ensemble de mesures appliquées par un professionnel de la santé à une personne vis-à-vis d'une maladie, afin de l'aider à en guérir, de soulager ses symptômes, ou encore d'en prévenir l'apparition. Lorsque ce professionnel décide de ne pas médicaliser une situation qu'il juge ne pas relever de traitement, il s'agit d'abstention thérapeutique (non-initiation d'un traitement ou interruption du traitement appelée retrait ou congé thérapeutique). L'observance thérapeutique désigne la capacité d'un patient à suivre correctement le traitement qui lui a été prescrit.Concernant la santé animale, il s'agit des mesures appliquées par un vétérinaire.Le traitement peut être théoriquement classé selon le but global poursuivi pour l'individu :curatif, dont l'objectif est d'obtenir la guérison d'une personne malade (exemple : fracture d'un fémur d'origine traumatique chez un individu sain) ;palliatif, dont l'objectif est de soulager les manifestations d'une maladie (exemple : fracture d'un fémur d'origine pathologique chez un individu en fin de vie) ;préventif, dont l'objectif est de prévenir l'apparition d'une maladie (exemple : accident de la voie publique, ostéoporose et risque de fracture).En outre, on peut théoriquement classer un traitement selon son mode d'action principal par rapport à une maladie :étiologique, dont l'objet est la cause de la maladie (exemple : un antibiotique pour une angine bactérienne) ;symptomatique, dont l'objet est la manifestation d'une maladie (exemple : un antalgique pour la douleur liée à une angine).On peut classer un traitement, selon le type d'acte dispensé, en traitement médical, chirurgical ou médicotechnique.Le traitement médical fait intervenir un pharmacien, un médecin ou un infirmier, le plus souvent à l'aide de mesures hygiénodiététiques (conseil sur le mode de vie et l'alimentation, éducation thérapeutique) et de médicaments par voie injectable ou non.D'autres procédés font partie du traitement médical. La rééducation fait intervenir un kinésithérapeute, un orthophoniste ou un ergothérapeute. La psychothérapie fait intervenir un psychiatrie ou un psychologue. Le pansement fait intervenir un infirmier. Le massage et la physiothérapie font intervenir un kinésithérapeute. Il existe également l'hydrothérapie en rhumatologie, l'électroconvulsivothérapie et la luminothérapie en psychiatrie, et l'asticothérapie en dermatologie. Dans le cadre de soins d'urgence ou de réanimation, le traitement médical peut également concerner la pratique d'acte technique ""simple"" tel que le sondage des voies naturelles (urinaire, digestive, respiratoire) ou le massage cardiaque.Il faut également citer ici le traitement non conventionnel qui regroupe différentes pratiques ayant en commun le fait de ne pas avoir de base scientifique théorique ni de preuve scientifique d'efficacité.Le traitement chirurgical fait intervenir un chirurgien qui va pratiquer une incision.Plusieurs traitements sont à la frontière de la chirurgie. Le traitement radio-interventionnel fait intervenir un radiologue. Le traitement endoscopique fait intervenir un médecin endoscopiste. La radiothérapie fait intervenir un radiothérapeute. La photothérapie fait intervenir un médecin spécialiste.On peut classer le traitement selon la méthode employée :entretien oral : mesures hygiénodiététiques, psychothérapie ;molécule chimique : traitement médicamenteux, pharmacothérapie ;intervention mécanique : chirurgie, endoscopie, radiologie interventionnelle, oncologie physique, massage ;rayonnement ionisant : radiothérapie ;rayonnement électromagnétique non ionisant : photothérapie, luminothérapie ;onde ultrasonore : ultrasonothérapie ;modification de température : thermothérapie, cryothérapie ;électricité : électrothérapie, électroconvulsivothérapie ;eau : hydrothérapie ;animal : asticothérapie.De nombreuses autres méthodes sont utilisées en médecine non conventionnelle.Parfois, la surveillance, qui peut être clinique (exemple : pression artérielle), biologique (exemple : protéine C réactive) ou radiologique (exemple : radiographie du thorax), est considérée comme partie intégrante du traitement, en particulier pour la surveillance clinique.En orthopédie, on différencie le traitement chirurgical (mise en place de matériel rigide dans le corps) ; le traitement orthopédique (mise en place d'un plâtre) et le traitement fonctionnel (mise en place d'une immobilisation relative non plâtrée : attelle ou bandage).En psychologie, la psychothérapie se décline en différents types. On retrouve par exemple les thérapies cognitivo-comportementales, psychanalytiques ou familiales.ChimiothérapieImmunothérapieÉchec thérapeutiqueProphylaxie""Chapitre neuf du livre de médecine dédié à Mansur, accompagné des commentaires de Sillanus de Nigris"" est un livre latin par Rhazès, de 1483, qui est connu pour son chapitre 9, qui est d'environ thérapeutiques Portail de la médecine   Portail de la psychologie"
économie;"La production est l'action d'un sujet qui transforme une matière première  pour faire exister un nouvel objet. On rencontre ce phénomène de production dans la société, mais aussi bien dans la nature. C'est pourquoi on peut l'étudier soit sous l'angle économique et sociologique, soit sous l'angle biologique.Le terme « production » dérive du latin classique qui signifie « prolonger, mettre en avant ». Dans l'Antiquité, il désigne aussi bien les créations de la nature (l'arbre producteur de fruits) que celles de l'homme (l'artisan producteur d'objets utiles). Ce n'est qu'au début de l'ère industrielle qu'il entre dans le discours économique.Selon John Stuart Mill, « l'économie décrit les lois des phénomènes de société qui se produisent du fait des opérations conjointes de l'humanité pour la production de richesses ». L'économie est donc la discipline scientifique qui étudie la production comme élément fondamental, mais aussi l'échange, la distribution et la consommation des biens et des services. C'est ainsi qu'on étudie la production selon les méthodes, les lieux et les marchés. On compare la production d'un même produit à partir de modèles différents d'organisation. On calcule le volume de production par pays et par époques. On sépare l'analyse par secteurs économiques. On distingue la production marchande de la production non marchande. La production marchande est celle qui est réalisée et vendue essentiellement par les entreprises sur le marché des biens de consommation achetés par les ménages ou sur celui des biens de production achetés par les entreprises. La période de référence est généralement l'année. Elle est différente de la production annuelle. Grâce à la variation des stocks (stockage lorsque la production annuelle n'est pas totalement vendue ou déstockage dans le cas où celle-ci est insuffisante), la production permet de répondre au besoin annuel du marché national. Cette production est celle réalisée sur le territoire national (par des entreprises nationales ou étrangères) et ne tient donc pas compte de la production réalisée par des entreprises nationales dans le reste du monde. Par contre, la production est dite non marchande lorsque le prix payé par l'utilisateur est inférieur à la moitié de son coût de production. La production non marchande est réalisée essentiellement par l'État et accessoirement par les administrations privées (syndicats et partis politiques, par exemple) et les ménages. Les services concernés sont, essentiellement, de défense nationale, de sécurité, de justice, religieux et de spectacle public. La production non marchande sous forme d'autoconsommation des ménages n'est pas prise en compte par la comptabilité nationale car elle n'est pas justifiée par des documents (factures ou bulletins de paie, par exemple) justifiant son existence. Par contre, les travaux domestiques (services de jardinage ou d'éducation des enfants) sont comptabilisés lorsque le paiement des domestiques est prouvé par des pièces justificatives et non effectué uniquement gratuitement ou par remise d'une somme d'argent de la main à la main.La première approche économique de la production fut celle des physiocrates au XVIIIe siècle, qui considéraient que seule l'agriculture était vraiment productrice puisque le végétal apporte plus de graines qu'il n'en consomme, les autres activités ne faisant que transformer les produits de la terre. Au siècle suivant, David Ricardo va mettre l'accent sur la théorie de la valeur fondée sur le travail, approfondissant la distinction entre valeur d'usage et valeur d'échange. Henry Charles Carey est un célèbre économiste américain qui s'est opposé à Ricardo et au libre-échange en faisant l'éloge du capitalisme protectionniste et interventionniste américain,.Aujourd'hui, la production est l'activité socialement organisée exercée par une unité institutionnelle qui combine des facteurs de production (facteur travail et facteur capital) afin de transformer les consommations intermédiaires en biens ou en services s'échangeant sur le marché.Depuis les travaux de Colin Clark, on regroupe les activités économiques de production  selon trois grands secteurs :le secteur primaire : l'ensemble des activités qui exploitent les ressources naturelles : agriculture, mines, pêche...le secteur secondaire : toutes les activités de transformation d'une matière première : industries manufacturières, construction...le secteur tertiaire : principalement marchand : commerce, transports, hébergement-restauration... ; ou non-marchand : administration publique, enseignement...Selon une enquête de 2016, en France, le secteur primaire représente 2,8 % des 26 millions de personnes possédant un emploi (au sens du Bureau international du travail) ; le secteur secondaire 20,6 % et le secteur tertiaire 75,7 %. La France est le pays européen où le poids du tertiaire est le plus élevé.Selon l'INSEE, l'industrie regroupe « les activités économiques qui combinent des facteurs de production (installation, approvisionnement, travail, savoir) pour produire des biens matériels destinés au marché. » En France, l'industrie représente 12,4 % du PIB (20,3 % en Allemagne, 8,7 % au Royaume-Uni). La part de l'industrie manufacturière dans l'économie française a diminué de moitié depuis 1970 (5,7 millions de salariés contre 2,7 millions aujourd'hui).On distingue la production marchande de la production tout court.La production marchande peut se subdiviser en deux catégories :la production marchande simple où le producteur vend son produit sur le marché ou rend un service marchand à titre individuel ;la production marchande capitaliste où le produit ou le service créé par des salariés est propriété du capitaliste. Il est ensuite vendu en tant que marchandise dans le but de réaliser un bénéfice.La production non-marchande se définit comme la production de biens ou services proposés gratuitement ou à un prix inférieur au coût de production, par des organisations publiques, ou des associations.La production réelle d'une entreprise ne correspond pas normalement à sa production vendue. Celle-ci comprend en effet, en plus de la production propre de l'entreprise, celle issue d'autres entreprises, qui correspond aux matières premières et aux autres produits achetés (appelés consommations intermédiaires) pour fabriquer le produit vendu. La production réelle de l'entreprise, appelée « valeur ajoutée », est donc sa production vendue, de laquelle il faut retrancher les consommations intermédiaires.Lorsque la production n'est pas vendue sur le marché (l'essentiel de la production des administrations), sa valeur correspond, par définition, à son coût de production. Comme dans le cas de la production marchande, la valeur ajoutée des administrations est obtenue après avoir retranché les consommations intermédiaires de la production.Différentes organisations permettent de produire un bien ou un service. Certaines sont des espaces où sont concentrés les moyens de production et les ressources humaines pour produire à grande échelle, en grande quantité et d'une manière répétitive avec une division des tâches poussée. D'autres sont des structures plus éclatées et plus mobiles comme l'entreprise en réseau, (l'entreprise étendue) mise en place dans le cadre de l'économie post-industrielle.Trois grands modes d’organisation de la production peuvent être observés : organisation de type « série unitaire »,  les industries process, la production manufacturière.La sociologie économique considère que la production est une activité de création, de rencontre, d'échange et de partage de nombreux éléments tels que le temps, l'espace, les biens, les idées et les émotions.Les économistes ont modélisé la production en identifiant les éléments qui contribuent à sa réalisation, à savoir les facteurs de production. L'un des facteurs de production est constitué par le travail, ce qui représente la dimension sociale de la production du point de vue des théories économiques.Depuis les années 1970 environ, où sont apparus et se sont développés les mouvements écologistes, on se rend compte que la production, surtout industrielle, est grosse consommatrice de ressources naturelles, ce qui pose le problème de la rareté ou de l'épuisement de ces ressources, et qu'elle peut engendrer d'importantes pollutions. C'est pourquoi est apparue la notion de développement durable, qui combine deux aspects : ne pas abuser des ressources naturelles ; régler la production pour qu'elle ne détruise ni ne pollue l'environnement.Du point de vue biologique, tous les êtres vivants, végétaux comme animaux, sont des producteurs : ils produisent de la matière vivante en prélevant des éléments dans leur milieu de vie. L'animal comme le végétal produit sa propre matière à partir des aliments qu'il consomme. On distingue deux types de producteurs :les producteurs primaires : ce sont les végétaux verts qui contiennent de la chlorophylle grâce à laquelle en présence de lumière et uniquement à partir de matières minérales, ils fabriquent de la matière organique carbonée ;les producteurs secondaires : ce sont tous les autres êtres vivants qui fabriquent leurs substances organiques à partir de la matière d'un autre être vivant végétal ou animal.L'histoire de la production est marquée par deux grandes ruptures. La première est la Révolution  néolithique caractérisée par la transition de tribus de chasseurs-cueilleurs vers des communautés  d'agriculteurs. La première émergence eut lieu au Proche-Orient, il y a 5000 ans environ, où les hommes passèrent graduellement de la cueillette de céréales sauvages, à la production de plantes et d'animaux domestiqués. Les hommes ne se contentent plus de prendre ce que la nature leur offre, ils modifient radicalement leur environnement par des techniques agricoles nouvelles pour obtenir d'importants surplus de production. Une société sédentaire remplace progressivement les groupes nomades.La seconde rupture majeure, à partir du XVIIe siècle, est la Révolution industrielle qui transforme une société à dominante agraire et artisanale en une société commerciale et industrielle. Le caractère dominant de cette mutation est le passage de l'outil (prolongement de la force musculaire de l'ouvrier) à la Machine (dispositif autonome mû par une énergie naturelle), ce qui permet la mise en place de la production en série, c'est-à-dire d'une production de masse, production d'objets tous identiques à  grande échelle.Dans le cadre du capitalisme, la production est généralement conçue comme l'activité destinée à satisfaire non plus les besoins du producteur (autoconsommation), mais à être vendue sur le marché. Cette dernière est appelée « production marchande ». De plus, la vente n'est pas effectuée pour satisfaire les besoins jugés nécessaires ou urgents. Ceux-ci doivent être armés d'un pouvoir d'achat ; autrement dit, la production est destinée aux consommateurs qui sont capables de payer.Dans le second Discours, Jean-Jacques Rousseau cherche à cerner l'origine de la civilisation, qui est aussi selon lui l'origine du malheur de l'homme. Il affirme : « La métallurgie et l'agriculture furent les deux arts dont l'invention produisit cette grande révolution ». Au XIXe siècle les archéologues et les historiens ont parlé de ""révolution néolithique"" pour caractériser ""la période de la préhistoire marquée par l'émergence des premières sociétés agricoles sédentaires (...) qui ont éliminé, en quelques millénaires les sociétés de chasseurs-cueilleurs"", et qui ont installé ""une économie de la production"".C'est aussi au XIXe siècle que Karl Marx a élaboré une philosophie qui donne une grande importance à la production :d'une part, il en fait la base de la compréhension de l'homme : les hommes ""commencent à se distinguer des animaux dès qu'ils commencent à produire leurs moyens d'existence, pas en avant qui est la conséquence même de leur organisation corporelle"". La production n'est donc pas seulement une action économique; elle a un sens plus profond car elle est ""la façon dont les individus manifestent leur vie"". ""Ce qu'ils sont coïncide donc avec leur production"" (ibid.). À la question philosophique : ""qu'est-ce que l'homme ?"", Marx répond donc : ""l'homme c'est le monde de l'homme, l'État, la société"". Il faut donc dire que l'homme se produit lui-même dans l'histoire : ""par son activité historique, l'homme se donne une valeur humaine, il produit ses propriétés d'homme"", il se met en valeur. Marx écrivait : « Tout ce qu'on appelle l'histoire universelle n'est rien d'autre que l'engendrement de l'homme par le travail humain. »d'autre part, l'étude de la production fournit la base scientifique qui permet de comprendre la structure et l'évolution des sociétés humaines. C'est la théorie du matérialisme historique selon laquelle les rapports de production (relations entre les classes sociales) sont liés  aux forces productives (techniques, outillage et machines): ""les rapports sociaux sont intimement liés aux forces de production. En acquérant de nouvelles forces productives, les hommes changent leur mode de production et en changeant leur mode de production,ils changent la manière de gagner leur vie, ils changent tous leurs rapports sociaux"". Dans un fameux raccourci, il écrit : « Prenez le moulin à bras et vous aurez la société féodale avec le suzerain; prenez le moulin à vapeur et vous aurez la société avec le capitaliste industriel». Les rapports de production  sont d'abord en accord avec l'état de développement des forces productives, mais l'évolution de ces dernières finit par créer le besoin de nouveaux rapports de production, ""alors commence une ère de révolution sociale"" qui se conclut par l'apparition d'un nouveau mode de production et donc d'un nouveau type de société. L'histoire de l'humanité se définit par celle des modes de production. Il distingue les suivants : asiatique, antique, féodal et capitaliste auxquels devrait succéder le mode de production communiste débarrassé de la lutte entre les classes sociales qui a caractérisé les précédents.D'une manière plus générale, Michel Henry crédite Marx d'avoir pensé ""l'activité productive des hommes"" comme une praxis: ""C'est dans la pratique qu'il faut que l'homme prouve la vérité"". Selon Adolfo Sanchez-Vasquez, le concept de praxis signifie : ""activité orientée vers la transformation d'un objet (nature ou société) en tant que fin tracée par la subjectivité consciente et agissante des hommes et, par conséquent, activité objective et subjective à la fois"", et en ce sens, il s'oppose à toutes les philosophies précédentes, car comme le dit la XIe thèse des thèses sur Feuerbach : « Les philosophes n'ont fait qu'interpréter le monde de diverses manières, il s'agit maintenant de le transformer ».Biens et services marchandsCapital productifConsommationÉconomie post-industrielleEntrepriseEntreprise étendueFacteur de productionMatérialisme historiqueMode de productionProduction audiovisuelleProductivismeProductivité Portail de l’économie   Portail des entreprises   Portail de la philosophie   Portail de la sociologie   Portail de la production industrielle"
économie;"Un agent économique est, en économie, une personne physique ou morale prenant des décisions qui participent à l'activité économique. Il est l'actant économique principal des modèles économiques. Le périmètre pertinent de définition de l'agent économique dépend des conceptions de l'économie : les courants de pensée économiques les définissent de manière différentes, ainsi que la comptabilité nationale. La question de la définition de l'agent économique est au centre des controverses économiques du XVIIIe siècle. Au sein de l'école physiocrate, François Quesnay crée un système de pensée où il définit l'économie nationale comme peuplée de trois agents économiques : les fermiers, les propriétaires fonciers et les artisans. Il qualifie cette dernière de « classe stérile ». Cette conception est plus tard critiquée par Adam Smith, qui considère les travailleurs et les commerçants comme les agents économiques majeurs.Le marxisme se fonde lui aussi à partir d'une remise en question de la définition des agents économiques. Karl Marx propose une analyse de l'économie avec une bipartition sociale, entre les capitalistes d'un côté, et les travailleurs de l'autre, représentatifs de la bourgeoisie et du prolétariat. Néanmoins, au sein de la classe des capitalistes, Marx distingue deux catégories d'agents économiques : ceux qui produisent des biens de production et ceux qui produisent des biens de consommation. Pour l'école du circuit, les agents économiques doivent être définis de manière proche de la définition de la comptabilité nationale, c'est-à-dire en étant regroupés en pôles fonctionnels. Cette école considère ainsi que les grands agents économiques sont les institutions financières (et notamment les banques), les entreprises, les ménages et les administrations publiques. Chez les keynésiens, chaque agent économique est classé par référence à sa fonction principale dans l'économie. Un entrepreneur appartement à l'agent "" entreprises "" en raison de sa fonction de base bien qu'il peut effectuer des opérations de consommation courante, à titre secondaire, qui concerne l'agent "" ménages "". Une banque effectue, à titre principal des opérations financières est classée donc dans l'agent "" banques "" bien qu'elle peut effectuer des opérations, à titre secondaire, relevant des agents "" ménages "" ou "" entreprises "". On peut faire un raisonnement analogue pour démontrer l'appartenance des individus à l'agent "" ménages "".La macroéconomie considère que l'agent économique pertinent peut être, ou bien un agent représentatif (le ménage moyen ou médian), ou bien une agrégation d'agents économiques, ou bien, dans le cadre d'une étude sectorielle, un groupe homogène. Dans tous les cas, les agents économiques sont agents car ils agissent dans le cadre d'échanges économiques avec d'autres agents,.L'objectif de l'étude menée par l'économiste oriente son choix dans la définition des agents économiques qu'il souhaite étudier.La microéconomie s'intéresse aux décisions prises par les agents économiques. Chaque agent possède des caractéristiques particulières qui permettent aux économistes de prévoir ses décisions. Plutôt qu'être un simple représentant de sa classe ou de son groupe d'appartenance, l'agent microéconomique arbitre entre les choix possibles, pour maximiser son utilité. L'hypothèse qui sous-tend cette conception est celle de la rationalité des agents, qui sont censés effectuer des choix optimaux en s'appuyant sur un calcul coût-avantage.Les néoclassiques considèrent qu'il faut s'intéresser à deux types d'agents économiques : le consommateur et le producteur. Le consommateur offre son travail en échange d'un salaire, qu'il va consommer sur le marché des biens et services. Le producteur achète la force de travail et les capitaux nécessaire à la production, et l'écoule ensuite sur le marché des biens et services. Comme le note la Direction générale du Trésor dans une note longue de 2021, « les modèles traitent les ménages et les entreprises de manière quasi-symétrique malgré leur profonde différence de nature, en les rassemblant dans le concept d'agents économiques ».Les organes de comptabilité nationale regroupent les agents économiques selon leurs fonctions, à l'instar de l'école du circuit. Les catégories les plus simples sont les ménages, les entreprises et le gouvernement. La fonction principale des entreprises et du gouvernement est de produire des biens et services, celle des ménages est de consommer.En France, l'INSEE catégorise les agents économiques en six catégories, aussi appelées unités institutionnelles. Chaque catégorie inclut des sous-divisions.Les modèles économiques théoriques sont le plus souvent basés sur des hypothèses comportementales homogènes, afin de décrire les ensembles économiques de la manière la plus simple. Ainsi les ménages consomment, les entreprises produisent.La théorie économique gagnant en complexité, les économistes affinent leur segmentation des agents économiques pour les différencier selon certains critères comme le revenu, le patrimoine ou l'âge.Agent (fonction publique)Unité institutionnelleÉconomie des institutions[PDF] http://www.newschooljournal.com/files/NSER01/82-94.pdf Duncan Foley, The strange history of the economic agent, 2002Les agents économiques et leurs opérations Portail de l’économie"
économie;Un bien de consommation est un produit fabriqué destiné au consommateur final. En économie, on le distingue d’un bien de production. Un service ne peut pas être considéré comme un bien.  Biens et services qui se consomment en une seule fois (pain, électricité du logement…)Biens semi-durables : ils durent quelque temps mais s'usent assez facilementBiens durables au plein sens du terme que l'on peut utiliser durant de nombreuses années (réfrigérateur, automobile…)Équipements (machines, moyens de transport…)Produits semi-finisMatières premièresÉnergie (électricité, pétrole…)Services rendus par une entreprise à une autre entrepriseBiens et services marchandsBien intermédiaireConsommation« Industrie des biens de consommation / Biens de consommation », sur le site de l'Insee Portail de l’économie
économie;"Un bien durable est un bien qui ne s'use pas rapidement ou, plus spécifiquement, dont l'utilité se maintient dans le temps au lieu d'être complètement consommé en une utilisation. Un objet comme la brique pourrait être considéré comme parfaitement durable, car en pratique on suppose qu'elle ne s'use pas. Les biens hautement durables comme les réfrigérateurs, les voitures ou les téléphones mobiles continuent à être utiles pendant un usage de trois ans ou plus, c'est pourquoi les biens durables sont habituellement caractérisés par de longues périodes entre des achats successifs.Les biens de consommation durables incluent les voitures, l'équipement de la maison (ameublement, électroménager, électronique grand public...), l'équipement sportif et les jouets.Les biens non durables (consommables) sont l'opposé des biens durables. Ils peuvent être définis comme des biens qui sont immédiatement consommés en une utilisation ou qui ont une durée de vie de moins de 3 ans.Les exemples de biens non durables incluent les biens de grande consommation tels que les cosmétiques, les produits de nettoyage, la nourriture, le carburant, les boissons alcoolisées, les cigarettes, les médicaments et les fournitures de bureau.Définir ce qu'est un bien durable est difficile, car la notion de développement durable est complexe et encore relativement nouvelle. Tous les domaines liés à l'économie n'ont pas encore pleinement intégré ces notions parfois floues.Une définition de bon sens serait donc un bien dont la conception, la production, l'utilisation, et la fin de vie respectent les principes fondamentaux du développement durable.Les biens durables sont généralement des biens d’équipement du foyer, de la personne, mais également des biens telle qu’une voiture.Parmi les domaines qui n'ont pas encore intégré complètement les notions de développement durable, on peut citer :le droit, qui n'en est qu'au stade de la réflexion sur ce qu'est la responsabilité envers les générations futures, en raison de la difficulté à définir sur un plan juridique ce que sont les besoins des générations futures ;la comptabilité nationale, qui connaît, pour le calcul de la formation brute de capital fixe (l'investissement), la notion d'actifs fixes, utilisables pendant plus d'un an dans un processus de production, parfois confondus par erreur avec des biens durables.(en) Cet article est partiellement ou en totalité issu de l’article de Wikipédia en anglais intitulé « Durable good » (voir la liste des auteurs).Développement durableMicroéconomiePrincipe de destination universelle des biensUtilisation durable Portail de l’économie   Portail de l’environnement"
économie;"Une entreprise, également appelée firme, compagnie ou société, ou encore familièrement boîte ou business, est une organisation ou une unité institutionnelle, mue par un projet décliné en stratégie, en politiques et en plans d'action, dont le but est de produire et de fournir des biens ou des services à destination d'un ensemble de clients, en réalisant un équilibre de ses comptes de charges et de produits.Pour ce faire, une entreprise fait appel, mobilise et consomme des ressources (matérielles, humaines, financières, immatérielles et informationnelles) ce qui la conduit à devoir coordonner des fonctions (fonction d'achat, fonction commerciale, fonction informatique, etc.). Elle exerce son activité dans le cadre d'un contexte précis auquel elle doit s'adapter : un environnement plus ou moins concurrentiel, une filière technico-économique caractérisée par un état de l'art, un cadre socio-culturel et réglementaire spécifique. Elle peut se donner comme objectif de dégager un certain niveau de rentabilité, plus ou moins élevé. Du point de vue légal, une entreprise est une personne morale.Depuis le début du XXIe siècle, les entreprises sont appelées à prendre en compte les exigences de développement durable, à travers la responsabilité sociétale des entreprises.L'entreprise est la plus petite combinaison d'unités légales qui constitue une unité organisationnelle de production de biens et de services jouissant d'une certaine autonomie de décision, notamment pour l'affectation de ses ressources courantes (définition consultée en novembre 2020).En droit français, il n'y a pas de reconnaissance de l'entreprise comme sujet, mais comme activité. Il y a plusieurs formes de sujets juridiques qui peuvent porter une entreprise. Les plus courantes sont :les sociétés : lorsque l'entreprise est portée par plusieurs associés (société anonyme, société par actions simplifiée, société à responsabilité limitée, société civile professionnelle) ;les associations ou coopératives : lorsque l'entreprise n'a pas de but lucratif ;les structures individuelles : lorsque l'entreprise est portée par un individu seul (auto-entrepreneur, profession libérale, artisan, entreprise individuelle, EURL).La forme juridique choisie doit faire l'objet d'un enregistrement auprès des autorités compétentes (registre du commerce et des sociétés ; répertoire des métiers pour les entreprises artisanales ; URSSAF pour les professions libérales). Cette forme juridique est associée à une identification distinctive et non ambiguë (en France par exemple, inscription au répertoire SIREN/SIRET).Lorsqu'il s'agit d'une société, cet enregistrement lui confère la personnalité morale et un statut juridique dont la forme dépend de l'objet social de la société, du nombre des apporteurs de capitaux, du montant des capitaux engagés, ainsi que du cadre législatif et réglementaire en vigueur. L'exercice de l'activité de l'entreprise peut également faire l'objet d'une autorisation préalable délivrée à titre permanent ou révisable, là encore dans le cadre des législations en vigueur (exemples des activités de banque, assurance, pharmacie, travail temporaire, etc.).La question de l'entreprise comme patrimoine juridique, comme propriété, est toujours débattue en doctrine. En l'état actuel du droit français seuls des aspects parcellaires de l'entreprise, comme le capital, la fidélité de la clientèle et les moyens de production, sont considérés comme des droits patrimoniaux qui reviennent à l'entité exploitante. Par contre, la liberté d'entreprendre est reconnue par le Conseil d’État comme principe général du droit à valeur constitutionnelle.Par le concept de société, le droit identifie donc l'entreprise avec ses dirigeants. Cependant, le droit encadre aussi la représentation des employés au sein de l'entreprise (voir Comité d'entreprise). La personnalité de l'entreprise en anthropologie La conception de l'entreprise comme une entité propre et capable d'agir par elle-même est une construction culturelle. L'attribution de décisions, de comportements, voire d'émotions, à une entreprise est une croyance qui l'assimile à une personne humaine. Cette personnalisation de l'entreprise se retrouve en droit des sociétés, qui utilise l'image de la personne morale. Elle se retrouve aussi en marketing avec le concept d'identité de l'entreprise auprès des clients.Cette assimilation culturelle a des effets juridiques et économiques. Ainsi, le concept de « responsabilité limitée » et sa mise en œuvre dans les lois au XIXe siècle (ex : en France, lois du 23 mai 1863 puis du 24 juillet 1867 ; en Angleterre lois de 1856 à 1862 sur les Joint-Stock Company limited) compte, d'après Y.N. Harari dans son ouvrage Sapiens, « parmi les inventions les plus ingénieuses de l’humanité » : « Peugeot est une création de notre imagination collective. Les juristes parlent de « fiction de droit ». Peugeot appartient à un genre particulier de fictions juridiques, celle des « sociétés anonymes à responsabilité limitée ». Harari explique : « Si une voiture tombait en panne, l’acheteur pouvait poursuivre Peugeot, mais pas Armand Peugeot. Si la société empruntait des millions avant de faire faillite, Armand Peugeot ne devait pas le moindre franc à ses créanciers. Après tout, le prêt avait été accordé à Peugeot, la société, non pas à Armand Peugeot, l’Homosapiens ».La « responsabilité limitée » est donc un transfert de la responsabilité pénale de l'actionnaire à la société-entreprise, et des risques économiques à son collectif de travail. Toutefois, ce transfert ne s'accompagne pas en retour d'un transfert de propriété du fait de la non-réalité juridique de l'entreprise : quel que soit le montant investi par l'actionnaire il a toujours le pouvoir et est propriétaire de fait (grâce à sa possession des actions) de tous les moyens de production (locaux, machines, moyens informatiques, etc.), y compris de ceux acquis grâce aux « millions » empruntés : c'est l'entreprise, qui acquiert en empruntant, qui rembourse, et qui entretient à ses frais les moyens de production en plus, bien entendu, de payer les salaires, charges et taxes.Grâce à cette « responsabilité limitée » conjuguée avec la non-réalité juridique de l'entreprise, plusieurs procédés permettent aux actionnaires d'accroître les moyens de production qu'ils contrôlent en minimisant au maximum leur mise (le capital social) : investissement par effet de levier, achat à effet de levier, rachat d'actions. Il est donc très compréhensible que les actionnaires recourent à ces procédés plutôt que d’émettre des actions supplémentaires provoquant l'arrivée d'autres actionnaires avec qui certes les risques sont partagés, mais également le pouvoir et la propriété. Si l'entreprise était comme une association 1901, sujet de droit, la « responsabilité limitée » serait remplacée par les « responsabilités et propriétés partagées » entre actionnaires et le collectif de travail de l'entreprise, chacun selon sa contribution.Le concept d'entrepreneur désigne celui qui entreprend, qui se trouve être à l'origine et concrétise un projet d'entreprise :sa démarche peut être innovatrice lorsqu'il anticipe un besoin, ou assemble et organise les outils et les compétences nécessaires pour satisfaire de manière inédite ce besoin. Ce type d'entrepreneur fait appel à des notions de création et d'innovation, et se distingue donc de celui de chef d'entreprise. Pourtant, ces deux termes bien que relevant de réalités différentes, caractérisent souvent les mêmes personnes : un entrepreneur est un chef d'entreprise s'il pilote lui-même son projet et un chef d'entreprise peut être qualifié d'entrepreneur à raison des objectifs intrinsèques de sa fonction ;la démarche peut être moins originale et plus conventionnelle lorsque l'entrepreneur considéré porte un projet qui s'inspire fortement, voire reproduit ou utilise des modèles d'activité ou d'entreprise déjà existants.Ce faisant, l'entrepreneur prend le risque que le besoin ne se matérialise pas ou que les moyens qu'il met en place pour le satisfaire se révèlent inadéquats.Historiquement, l'entrepreneur est un intermédiaire, un agent en travail : on lui passe des commandes fermes de biens ou de services, il recherche les ouvriers qui vont produire chacun une partie de cette commande et il s'assure de la bonne livraison. Ceci dans un contexte où la division du travail est trop peu marquée, où les ouvriers travaillent à domicile, et disposent de leurs outils et même de leurs machines (métier à tisser par exemple).Avant la révolution industrielle, un entrepreneur est surtout un « homme-orchestre » capable d'optimiser les besoins en capitaux et les ressources humaines pour mener une activité licite et rentable, les moyens de production et la force de travail n'étant pas encore regroupé au sein d'entreprise. On retrouve encore au XXIe siècle ce type d'organisation, par exemple, dans l'industrie du transport, les services (ingénierie, etc.) où à côté de grands groupes, des indépendants sont propriétaires de leur outil de travail (par exemple : camions, péniches ou barges) et trouvent leurs donneurs d'ordres par l'intermédiaire de courtiers.Avec la révolution industrielle, les entrepreneurs changent, ils regroupent des machines sur un même lieu de travail et conservent les mêmes ouvriers longtemps, ce qui donne naissance aux entreprises au sens traditionnel. On voit alors émerger la figure du chef d'entreprise (un exemple connu étant celui d'Henry Ford).Les prémisses de l'entreprise au sens moderne du terme n'apparaissent qu'au XVIIIe siècle, avant cela, les activités de production et d'échange sont presque exclusivement assurées au sein de familles ou de guildes. La place de l'entrepreneur y est alors essentielle, il dirige tous les maillons de la chaîne de valeur. Du fait principalement de l'industrialisation, au XIXe siècle, l'organisation des entreprises change considérablement. L'identité familiale de l'entreprise et l'exclusivité du pouvoir de l'entrepreneur-dirigeant s'affaiblissent progressivement. À partir de 1880 se développent les « grandes entreprises modernes » sous forme de sociétés anonymes où la contribution de chaque actionnaire aux pertes ne peut excéder sa part dans le capital social. Grâce à ce principe, l'offre de capitaux explose.Les entreprises peuvent être classées selon différents critères :La classification par secteur économique est déterminée par l'activité principale de l'entreprise :secteur primaire : il s'agit d'activités liées à l'extraction des ressources naturelles via l'agriculture, la pêche, l'exploitation forestière ou minière ;secteur secondaire : il s'agit d'activités liées à la transformation des ressources naturelles issues du secteur primaire (bâtiments et travaux publics, électroménager, aéronautique, etc.) ;secteur tertiaire : il regroupe toutes les activités économiques qui ne font pas partie du secteur primaire et secondaire. Il s'agit d'activités marchandes (vente de produit) et d'activités non marchandes (vente de services, non échangeables).Au-delà de ce découpage classique, un secteur quaternaire est parfois distingué, avec une définition variant selon les auteurs.Selon la définition de la Commission européenne en 2011, les entreprises sont classées comme :microentreprise : sous-catégorie des TPE définie en France par un chiffre d'affaires inférieur à 81 500 euros pour celles réalisant des opérations d'achat-vente et à 32 600 euros pour les autres ;très petite entreprise (TPE) : moins de 10 salariés avec soit un chiffre d'affaires inférieur à 2 millions d'euros par an, soit un total bilan inférieur à 2 millions d'euros ;petite et moyenne entreprise (PME), on distingue :petite entreprise (PE) : entre 10 salariés et 49 salariés avec soit un chiffre d'affaires inférieur à 10 millions d'euros par an, soit un total bilan inférieur à 10 millions d'euros,moyenne entreprise (ME) : entre 50 salariés et 250 salariés avec soit un chiffre d'affaires inférieur à 50 millions d'euros par an, soit un total bilan inférieur à 43 millions d'euros ;grande entreprise : plus de 250 salariés et à la fois un chiffre d'affaires supérieur ou égal à 50 millions d'euros par an et un total bilan supérieur ou égal à 43 millions d'euros ;groupe d'entreprises : comporte une société mère et des filiales ;entreprise étendue (ou en réseau, ou matricielle, ou virtuelle) : comprend une entreprise pilote travaillant avec de nombreuses entreprises partenaires.Le secteur : ensemble des entreprises ayant la même activité principale.La branche : ensemble d'unités de production fournissant un même produit ou service.Pour l'Insee, une entreprise est une unité économique, juridiquement autonome, organisée pour produire des biens ou des services pour le marché ; elle est identifiée par le numéro SIREN. Un établissement est une unité de production géographiquement individualisée mais juridiquement dépendante de l'entreprise, et où s'exerce tout ou partie de l'activité de celle-ci ; il est identifié par un numéro SIRET.Les entreprises individuelles (existence juridique à travers la personne physique de l'entrepreneur — EI, EIRL).Les sociétés civiles (exemple : société civile professionnelle).Les sociétés commerciales (de personnes ou de capitaux ; parfois unipersonnelles — EURL, SASU).Les groupements d'intérêt économique.Les associations, entreprises privées dont les bénéfices doivent être intégralement réinvestis.Les sociétés coopératives, dans lesquelles les associés coopérateurs n'ont chacun qu'une voix quel que soit le montant de leurs apports (salariés, consommateurs, habitants, bénéficiaires du service, etc.).Les sociétés mutuelles à but non lucratif, immatriculées au registre national des mutuelles et soumises aux dispositions du code de la mutualité.Une autre forme de classement distingue trois grands types d'entreprises[réf. nécessaire] existant dans tous les pays :les entreprises privées à but lucratif (exemple : TPE, PME, groupe d'entreprises) ;les entreprises privées à but non lucratif (sociétés coopératives, associations et sociétés mutuelles relevant de l'économie sociale) ;les entreprises chargées d'une mission de service public (exemple : régie des transports urbains, régie des eaux, établissements publics industriels et commerciaux).L'activité économique est, dans tous les pays, encadrée par une réglementation. La plupart des entreprises fonctionnent donc dans un cadre prédéterminé par la loi : le droit des sociétés. Entreprise individuelle Dans le contexte de l'économie capitaliste, il est possible d'avoir une entreprise à titre personnel. Il s'agit alors d'une entreprise individuelle, c'est-à-dire que l'entrepreneur exerce directement et en son propre nom l'activité économique. L'exercice d'une activité sous forme d'entreprise individuelle concerne en général les TPE. Entreprise personne morale Il est aussi possible de constituer une personne morale sous forme de société. Celle-ci peut grouper plusieurs participants à son capital et est apte à faire des actes de gestion. Les diverses formes de sociétés varient selon les pays.Il convient alors de distinguer la propriété effective de l'entreprise et le pouvoir d'accomplir des actes de gestion au nom de la société. Selon la forme sociale, le responsable de la marche courante de l'entreprise sera appelé un gérant, président-directeur général ou directeur général. Le titulaire de cette fonction peut être détenteur de parts sociales ou d'actions ou être mandaté pour cela par l'assemblée générale des associés.Le droit des sociétés français distingue notamment les statuts de société anonyme (SA), société à responsabilité limitée (SARL), société par actions simplifiée (SAS), société civile (SC), société d'exercice libéral à responsabilité limitée (SELARL) et société en nom collectif (SNC). Un statut spécial nommé Euro 2016 SAS a été créé en 2014 afin que l'UEFA puisse organiser la coupe d'Europe de Football de 2016 en France sans devoir payer des impôts autre que la TVA (étant une taxe réglementée à l'international).Le fait qu'une entreprise utilise une forme de société par actions n'implique pas nécessairement que ces titres soient cotés en bourse (ou même qu'elle soit considérée comme faisant un appel public à l'épargne). Si c'est le cas, des achats en bourse ou des offres publiques peuvent faire changer la majorité de contrôle de l'entreprise, et aboutir aussi au changement de sa direction.La fonction première d'une entreprise varie selon l'entreprise ou même selon les points de vue au sein d'une même entreprise (par exemple, point de vue de l'actionnaire, de l'employé, du syndicat, de la direction, etc.). Parmi les différentes fonctions opérationnelles habituellement observées, on trouve :servir le marché, en produisant et distribuant des biens et services correspondant à une demande solvable. C'est sa seule justification économique, aucune entreprise ne pouvant survivre sans en faire sa priorité, à moins d'être protégée et en dehors du champ de la concurrence (exemple : cas de certains services publics), ce qui, d'un point de vue purement économique, peut la conduire à consommer plus de ressources qu'elle ne présente d'utilité ;gagner de l'argent, c'est-à-dire extraire des bénéfices financiers en « récoltant plus d'argent que d'argent investi », notamment pour attirer les investisseurs institutionnels et les petits actionnaires ;produire un excédent de trésorerie, qui sera investi avec un plus grand profit dans le développement des activités ou une autre entreprise (dans le cadre d'un « groupe ») ;maximiser l'utilité sociale ou environnementale. Certaines sociétés (entreprises à mission) se donnent même statutairement l'utilité sociale comme finalité ;atteindre un but technique : réalisation d'un ouvrage (tunnel, pont, route, etc.), fabrication d'un produit manufacturé, la conception et réalisation d'un service donnant satisfaction à un client. Ce but technique peut lui-même être extrêmement varié, on citera notamment :les activités qui ne sont pas, pour l'entrepreneur, l'enjeu principal, mais un moyen au service d'une autre activité : par exemple, la possession d'un groupe de presse, de production de ressources stratégiques ou d'entreprises vecteurs d'images (à l'exemple de la présence des cigarettiers dans l'industrie du prêt-à-porter),les coopératives agricoles qui sont des entreprises qui visent à dégager un bénéfice non pour elles-mêmes, mais pour les coopérateurs adhérents,les « entreprises d'insertion » visent à rendre aptes leurs employés à occuper un travail « normal », sans chercher dans certains cas (atelier chantier d'insertion) à générer du bénéfice.Certaines sociétés peuvent être constituées pour détourner les fonctions premières de l'entreprise, notamment pour camoufler des activités légales ou illégales (exemple : certaines activités comme le jeu, le change, le lavage de voitures, l'immobilier sont connues pour permettre le « recyclage » ou le « blanchiment » de l'argent issu d'activités illégales).Divers points de vue politiques sur l'utilité fonctionnelle de l'entreprise privée ont été formalisés au cours de l'histoire et de l'élaboration de la pensée économique :de son inutilité totale, aboutissant à sa suppression ou sa collectivisation ;à sa complète utilité (notamment en termes de création d'emplois), aboutissant à son encouragement et au développement des PME, des TPE, des sociétés artisanales et des professions libérales.Les entreprises se soucient de plus en plus de relégitimer leur rôle dans la société à travers divers vecteurs, particulièrement notables à partir de la fin du XXe siècle :les rapports de développement durable rédigés par les grandes sociétés mettent en avant leur rôle social et environnemental. La communication sur les efforts en faveur de l'environnement est devenue un argument majeur au début du XXIe siècle. En France, elle est rendue obligatoire par la loi sur les nouvelles régulations économiques (article 116) ;le mécénat (artistique, humanitaire, social, etc.) constitue autant un moyen de légitimation de la place de l'entreprise qu'une action de communication institutionnelle en faveur de l'image de l'entreprise ;en France, le thème de l'« entreprise citoyenne », en vogue au tout début des années 2000, a fait avancer la réflexion sur la place de l'entreprise dans la société.L'évaluation de la triple performance économique, sociale et écologique (3P pour People Planet Profit) de l'entreprise se fait par des agences de notation sociétale, qui examinent les rapports de développement durable pour noter les entreprises. Les investissements socialement responsables permettent de s'orienter vers les entreprises les mieux notées sur le plan sociétal.Ainsi, une nouvelle forme d'entreprise émerge, appelée à prendre en compte les intérêts à long terme de l'ensemble des parties prenantes de l'entreprise, et non plus seulement le seul intérêt à court terme des seuls actionnaires. En effet, le développement durable fait intervenir non seulement le marché, mais aussi l'État et la société civile.Le mode de gouvernance des entreprises conforme au développement durable s'appelle la responsabilité sociétale des entreprises.Pour le droit de la concurrence, la forme juridique (personne morale de droit privé ou de droit public, société, association) et le but (lucratif ou pas) de l'entreprise sont indifférents. Ainsi pour le droit communautaire, « la notion d'entreprise comprend toute entité exerçant une activité économique, indépendamment du statut juridique de cette entité et de son mode de financement » (Cour de justice des communautés européennes (CJCE), arrêt Höffner, 1991).Néanmoins, n'exerce pas une activité économique, et n'est plus une entreprise soumise au droit de la concurrence, l'organisme qui remplit une fonction exclusivement sociale (CJCE, Poucet 1993) ou celui qui exerce des prérogatives de puissance publique (CJCE, Eurocontrol, 1994).Acquisition et cession d'entrepriseConcurrencedroit de la concurrenceFusionMonopoleOligopolePlan marketing Finalité : rémunérer le risque pris par l'apporteur de capital Parmi les différents buts possibles pour une entreprise, la recherche du bénéfice occupe une place importante. Le bénéfice de l'entreprise (différent du profit) sert avant tout à rémunérer le capital investi.Les entreprises peuvent prendre plusieurs formes juridiques correspondant à des caractéristiques différentes de l'apporteur de capital : entreprises individuelles, sociétés de personnes, sociétés de capitaux. Les grandes entreprises sont en général des sociétés de capitaux.Dans le cas des sociétés de capitaux, si un investisseur (une des personnes qui financent l'entreprise) décide de le placer dans une entreprise plutôt que de le conserver, c'est qu'il souhaite que l'argent ainsi placé dans l'entreprise lui rapporte plus. Si une entreprise ne génère pas un profit suffisant redistribué sous forme de dividendes, sa réputation ternit et elle n'attire plus les investisseurs. Sa capacité de développement (en général consommatrice de capitaux pour, par exemple, ouvrir des filiales à l'étranger ou démarrer de nouveaux programmes d'innovation), voire sa survie, s'en trouvent alors obérées, voire peuvent être remises en cause.Pour chaque secteur d'activité, il existe un niveau de profit « normal » attendu. Ainsi, par exemple, dans le secteur pharmaceutique des années 2000, le niveau moyen de profit attendu était de 15 % par an du capital investi. Si une entreprise génère moins de profit, les actionnaires qui y ont placé leurs économies (directement ou plus souvent indirectement via une banque ou une caisse de retraite) sont déçus, perdent éventuellement confiance dans l'investissement consenti et vendent leurs actions : le prix de l'entreprise (qu'elle soit en bourse ou non) diminue alors et les investisseurs restants y perdent.Une entreprise capitaliste dont les profits sont faibles trop longtemps n'a pas de justification économique : elle est en général fermée ou rachetée. Dans le cas d'entreprise de l'économie sociale, elle perdura si elle apporte une utilité sociale à la société (exemple : entreprise de réinsertion) et si elle trouve un bailleur de fonds apte à en financer les pertes éventuelles (exemple : collectivité territoriale). Enfin, les entreprises familiales, à la fois privées et non cotées, peuvent trouver un équilibre entre profits élevés et utilité sociale, tout en réussissant sur le long terme, notamment par leur taille à l'échelle humaine et la proximité du management vis-à-vis des salariés. L'origine du bénéfice De manière simplifiée, la rentabilité d'une activité s'obtient en vendant le plus cher possible un produit ou service et en dépensant le moins possible pour le produire.On distingue des revenus normaux et des revenus exceptionnels :les revenus normaux sont les produits des ventes et des opérations financières courantes sur l'année en cours (crédits clients et fournisseurs) ;les revenus exceptionnels ne font pas, par définition, partie des opérations courantes de l'entreprise. Il peut s'agir de vente d'actifs (bâtiments, machines, etc.), de vente de filiales ou d'opérations comptables diverses (exemple : réévaluation de la valeur financière d'un stock).La marge, calculée comme différence entre le prix de vente et le coût de revient des marchandises incorporées dans le produit vendue représente la principale contribution au bénéfice de l'entreprise.Pour augmenter cette marge, il existe uniquement deux leviers :augmenter le prix des produits ou services vendus (exemple : vendre un véhicule automobile à 15 000 €) ;diminuer le coût de production des produits ou services vendus (exemple : produire le véhicule avec 12 000 €).Les moyens d'action sur la réduction des coûts sont extrêmement divers, notamment :négocier avec les fournisseurs pour baisser les prix d'achat des marchandises incorporées ;améliorer la qualité pour produire avec moins de rebut ;améliorer la productivité des machines ;améliorer la productivité des hommes (amélioration de la qualification, ajustement du ratio entre la rémunération fixe et celle indexée sur les résultats, amélioration des conditions de travail, audit des pratiques dans le but de les améliorer, meilleure gestion du personnel, management des compétences, audit des outils) ;diminuer les taxes et prélèvements sur la production (impôt sur les profits, diminution des cotisations salariales des caisses sociales ou de retraites, bénéficier d'exonérations) ;réduire les stocks pour réduire le capital immobilisé ;négocier des conditions de règlement plus rapides vis-à-vis des clients afin d'avoir moins de frais financiers ;utiliser des logiciels libres pour réduire le capital immobilisé par les logiciels propriétaires payants ;s'implanter à côté des lieux de production des matières premières ;réduire la masse salariale et les avantages sociaux ;utiliser l'analyse de la valeur (c'est souvent le moyen le plus puissant puisqu'on peut réduire parfois les coûts dans des proportions considérables). Innovation technique et technologique La solution à ces déplacements mondiaux des centres de production de faible valeur ajoutée passe par l'innovation, la création d'activités à forte valeur ajoutée (exemple : Airbus A380, TGV, automobiles intelligentes, microprocesseurs, nouveaux matériaux, logiciels sophistiqués, biotechnologies, armements, centrales nucléaires, robot d'assistance aux personnes âgées, textiles intelligents, haute couture, etc.) demandant une main-d'œuvre créative et hautement qualifiée, ainsi que le développement de services de proximité.En 2008, les services représentent 70 % du PIB du monde occidental, ce qui consacre l’évolution des pays développés vers l’économie post-industrielle[réf. nécessaire]. L'entreprise dans la mise en œuvre de la Connaissance Il y a toujours des organisations, des hommes et des machines. Les entreprises sont de plus en plus globales (même petites) et connectées en réseaux leur permettant de réagir vite à des opportunités et associer des bonnes compétences pour accompagner des « idées au succès ». Les connaissances jouent un rôle prépondérant dans la façon de faire des affaires. On commence à prendre en compte non seulement le capital financier, mais aussi les capitaux immatériels qu'il faut fructifier. La santé et l'avenir des entreprises dépendent de leur capacité à innover et leur savoir-faire en transformation des idées en valeurs à partager pour tous les participants. Dans ce contexte les ordinateurs sous toutes leurs formes jouent le rôle d'assistant intelligent de l'humain,,.L'entreprise privée, en tant qu'entité de création et de partage des richesses, a fait l'objet de nombreuses critiques. La critique, provenant en particulier depuis le XIXe siècle de la pensée du socialisme et du christianisme social, s'est révélée plus profonde dans les pays de culture catholique (où les rapports de la morale avec l'argent sont complexes) que dans les pays de culture protestante, dans lesquels la position et la fonction sociale de chaque individu est considérée comme étant le fruit de la volonté divine (selon la thèse de Max Weber sur l'éthique protestante et le capitalisme).L'entreprise privée est considérée par certains détracteurs comme une entité faisant primer ses intérêts particuliers au détriment de l'intérêt général.La critique socialiste apparue au XIXe siècle s'est d'abord portée sur les conséquences économiques avec la question de la répartition inégalitaire des richesses créées par l'entreprise, au profit des capitalistes (la rémunération du capital) et au détriment des salariés (qui apportent leur travail). Elle a notamment été théorisée par Karl Marx.Les critiques concernant l'influence des entreprises sur le pouvoir politique se sont ajoutées. Dans la théorie marxiste, la « superstructure » sociale, qui comprend les pouvoirs politique et religieux est au service de l'« infrastructure » économique. Cette critique, sur le lien entre hommes politiques et entreprises, même en dehors du courant de pensée marxiste, est très vivace au début du XXIe siècle.Les entreprises sont accusées de mener un jeu géopolitique propre, dicté par leurs seuls intérêts, indépendant, voire contradictoire avec celui des politiques étrangères nationales ou internationales (par exemple, sur la question des droits de l'homme).Historiquement, les (ou des) entreprises privées ont été accusées d'avoir promu le colonialisme et l'impérialisme occidental et la guerre. C'est par exemple, la critique de Lénine sur l'impérialisme, stade suprême du capitalisme.À partir de la fin du XXe siècle, les entreprises ont été accusées de dégrader l'environnement dans le cadre de leur activité.D'autres critiques se sont focalisées sur le fonctionnement interne de l'entreprise privée. On relèvera notamment :la critique d'exploitation du salarié compte tenu de l'asymétrie des rapports de force entre employeurs et employés, notamment en période de chômage ;des critiques sur la ligne de partage de la richesse (des gains de productivité, des bénéfices) entre ceux qui apportent le capital et ceux qui apportent le travail ;la critique du pouvoir dans l'entreprise qui appartient traditionnellement aux agents apportant les capitaux et non à ceux qui fournissent leur travail. D'où des tentatives d'équilibrage à travers, par exemple, la cogestion en Allemagne ;la critique des formes de pression exercée sur le salarié et conduisant à des phénomènes de stress, évoqués notamment à partir de la fin du XXe siècle.Face aux critiques, les défenseurs des entreprises soulignent que l'intérêt privé va en fait dans le sens de l'intérêt général :l'entreprise privée constitue le moyen le plus efficace d'allocation des ressources (capital, travail, matières premières et énergie) compte tenu notamment de la contrainte de rentabilité ;l'entreprise privée constitue le moteur le plus efficace de la croissance économique et de l'innovation technique. Même quand elle n'est pas à sa source, l'entreprise est le vecteur d'application et de diffusion des innovations techniques ;l'entreprise, guidée par le souci de son développement et de sa rentabilité, ne tient pas compte des distinctions de nationalité, de race ou de sexe pour ne se baser que sur le mérite personnel. L'entreprise est alors considérée comme un facteur de paix et de rapprochement international et d'intégration des personnes différentes.En ce qui concerne le fonctionnement interne de l'entreprise, ses défenseurs ajoutent que l'entreprise peut au contraire être un lieu d'épanouissement personnel. Les cas les plus en pointe de cette tendance se situent dans les entreprises de nouvelles technologies, dans lesquelles les entrepreneurs sont souvent jeunes et les rapports humains moins formels (la culture de la startup"
économie;"La gestion des ressources humaines ou GRH (anciennement gestion du personnel ; parfois appelée gestion du capital humain) est l'ensemble des pratiques mises en œuvre pour administrer, mobiliser et développer les ressources humaines impliquées dans l'activité d'une organisation.Ces ressources humaines sont l'ensemble des salariés de tous statuts (ouvriers, employés, cadres) faisant partie de l'organisation, mais aussi – et de plus en plus – liés à elle par des rapports de sujétion (ainsi, les prestataires extérieurs, ou sous-traitants, sont considérés comme faisant partie de fait du périmètre des ressources humaines de l'entreprise).Dans un premier temps, cette fonction est entendue dans une perspective opérationnelle. Il s'agit d'administrer un personnel qui peut être numériquement important et réparti en différents niveaux de hiérarchie ou de qualification : (gestion de la paie, droit du travail, contrat de travail, etc.).Dans un second temps, la fonction acquiert une dimension plus fonctionnelle[pas clair]. Il s'agit d'améliorer la communication transversale entre services et processus, et de mettre en œuvre un développement des salariés à l'intérieur de l'entreprise (gestion des carrières, gestion prévisionnelle des emplois et des compétences ou (GPEC), recrutement (sélection), formation, etc.).La gestion des ressources humaines intervient à tous les stades de la vie des salariés dans l'entreprise, dont leur entrée et leur départ. Elle se décline ainsi en de multiples tâches : définition des postes, recrutement, gestion des carrières, formation, gestion de la paie et des rémunérations, évaluation des performances, gestion des conflits, relations sociales et syndicales, motivation et l'implication du personnel, communication, les conditions de travail, sélection, et équité (justice distributive, interactive, etc.).Afin de valoriser les compétences, la motivation, l'information et l'organisation, il est possible de donner toute l'attention nécessaire à certains outils de gestion :le recrutement. En évaluant les compétences et la motivation lors du recrutement, on s'assure d'avoir un personnel adéquat en nombre et en qualification ;la formation et le coaching. Afin d'améliorer le niveau de compétence des salariés, mais aussi pour améliorer leur motivation ;la motivation positive (récompense : félicitation, prime, promotion, formation…) et négative (sanction : réprimandes, réduction ou suppression d'une prime, rétrogradation, voire licenciement).La motivation positive et la motivation négative ont chacune leur efficacité. La sanction peut être démotivante pour l'intéressé. Mais il faut relativiser cette crainte car elle fait appel au principe de responsabilité et d'exemplarité. Elle renvoie aussi l'individu au groupe. Ce dernier peut mal vivre des comportements non sanctionnés quand ils sont hors jeu. C’est peut être un facteur de démotivation quand une absence de sanction traduit de fait un déséquilibre entre celui qui se dévoue et celui qui ne fait rien. Le souci d'équité doit guider l'administrateur. De ce point de vue, la gestion des ressources humaines doit intégrer aussi dans sa pratique administrative, la notion de groupe ou d'équipe : par la communication et la transparence. Il est essentiel que le salarié ait les informations nécessaires à l'accomplissement de sa tâche, et ait une idée précise de l'évolution et des objectifs de l'entreprise elle-même, et de son environnement. De nos jours, l'abondance d'informations a rendu nécessaire la mise en place de systèmes de gestion de l'information, comme les systèmes de gestion des connaissances ; par la planification et le contrôle de l'avancement des tâches. L'optimisation de l'organisation, c'est-à-dire l'ordonnancement des tâches et leur affectation aux personnes les plus compétentes disponibles, permet d'améliorer l'efficacité d'exécution ; par l'administration du personnel. Il est coutumier de dire qu'une bonne gestion des ressources humaines se traduit en premier lieu par une administration fiable du personnel. En l'occurrence, il s'agit de sécuriser son effectif en assurant un paiement rigoureux des salaires et des primes, en suivant la gestion des présences et des absences, des heures supplémentaires, en planifiant les congés, en organisant les remplacements, etc.Ce point est essentiel, car il caractérise une part des obligations contractuelles (statutaires pour un fonctionnaire) d'une entreprise (d'un service public) envers son salarié. Lorsque l'entreprise traverse une crise, le rôle des ressources humaines est primordial. Une crise, même financière, naît souvent d'une erreur humaine[réf. nécessaire]. C'est le devoir des responsables des ressources humaines de mettre en place un projet de redressement et ceci passe par la nomination et le suivi d'une équipe d'intervention efficace. De l'identification à la sortie de crise, la gestion des ressources humaines est la véritable clé dont l'avenir de la structure peut dépendre.Un enjeu de la gestion des ressources humaines est la gestion des coûts, par exemple ceux liés à la rotation du personnel ou à l'absentéisme.La notion d'entreprise vue comme étant un « corps social » est intronisée et développée au début du XXe siècle, entre autres par des gestionnaires praticiens comme en France Henri Fayol. Dans cette perspective, la gestion des ressources humaines correspond à une véritable fonction de l'entreprise.Les directions des ressources humaines assurent leurs missions et fonctions en collaboration avec les autres directions et les responsables de terrain dans une logique d'objectifs fixés par l'entreprise, l'association ou l'administration. C'est ainsi que la gestion des ressources humaines est considérée - dans certaines organisations - comme coresponsable de domaines comme la production ou la gestion de la qualité.Il est possible d'identifier de nombreuses tâches pour cette fonction qui sont :l’administration du personnel (c’est sous cet aspect que la fonction commence à exister et à être perçue dans l’entreprise) :l’enregistrement, le suivi et le contrôle des données individuelles, et collectives du personnel de l’entreprise ;l’application des dispositions légales et réglementaires dans l’entreprise ;la préparation des commissions et des réunions ;le maintien de l’ordre et du contrôle et les travaux de pointage.la gestion au sens large (cette expression recouvre trois domaines) :l’acquisition des ressources humaines : par la gestion de l’emploi, programmes de recrutement, plans de carrières, mutations et promotion, analyse des postes et l’évaluation des personnes ;la gestion des rémunérations : par l’analyse et l’évolution des postes, grille de salaires, politique de rémunération, intéressement et participation ;la gestion de la formation : par la détection des besoins, l’élaboration des plans de formation, la mise en œuvre des actions de formation et l’évaluation des résultats.la communication, l’information : Les tâches de la direction des ressources humaines en cette matière sont :La définition des publications orientées vers l'extérieur et la conception des messages,La conception du bilan social de l’entreprise (s'avérant être une obligation annuelle pour les organisations ayant plus de 300 salariés),La gestion des moyens de communication : journal d’entreprise, affichage, audio-visuel, réunions systématiques ;l’amélioration des conditions de travail. En cette matière les principaux thèmes sont :l’hygiène et la sécurité au travail et dans les trajets,l'ergonomie des conditions de travail,la prévention des risques psychosociaux et des maladies professionnelles.la qualité de vie au travail.La GRH nécessite la mobilisation de connaissances et expertises variées : gestion, économie, droit, sociologie, psychologie…Il est possible de distinguer les approches théoriques suivantes :L'approche la plus fréquemment rencontrée réside dans l'approche de la gestion des ressources humaines au fur et à mesure des grandes phases du cycle de vie du contrat de travail. Ceci permet d'aborder la relation de l'organisation avec son salarié du recrutement à son départ de l'entreprise (retraite, licenciement, démission…). Elle doit nécessairement être complétée par une vision collective au travers de processus que sont les relations sociales et syndicales, les systèmes d'information, le contrôle de gestion sociale…Une autre approche reprise dans l'ouvrage Manager RH retient pour les ressources humaines quatre missions essentielles qui sont :Construire l’organisation : ce que l’on appelle le « marché du travail » sur lequel se déterminent les salaires ne ressemble pas à un marché boursier. Son fonctionnement est, en partie, « interne » à l’entreprise et dépend des procédures et de l’architecture (division verticale et horizontale du travail) construites par le responsable RH ;Mobiliser l’organisation : il ne suffit pas que les salariés possèdent les compétences requises. Encore faut-il qu’ils veuillent les utiliser. Cette volonté sera en fonction de ce que leur offrira l’entreprise : une rémunération (globale), des conditions de travail, des perspectives d’évolution, autant d’aspects qu’il appartient au responsable RH de mettre en forme ;Doter l’organisation des compétences requises : les compétences d’aujourd’hui seront ainsi obsolètes demain. Le recrutement, la formation, la gestion prévisionnelle des emplois et des compétences sont autant de moyens utilisables pour réaliser la transformation nécessaire des qualifications ;Réguler l’organisation : les dysfonctionnements constituent le mode normal de fonctionnement des organisations que le responsable RH doit cependant maîtriser pour éviter que leur expression ne menace la survie de l’entreprise. Il doit aussi en contrôler les effets externes sur le système social, c’est-à-dire assumer ce qu’on considère être la « responsabilité sociale » de l’entreprise.Les ressources humaines auraient quatre missions essentielles d'après l'ouvrage Human Resource Champions :être le partenaire de la stratégie de l'entreprise au quotidien (le DRH en tant que business partner) ;gérer et accompagner le changement (le DRH « maître d'œuvre » des politiques de formation, de développement des compétences) ;administrer le quotidien (le DRH « gestionnaire » : payer, administrer, répondre aux obligations légales, etc.) ;assister les salariés (le DRH « coach »).L’évaluation de la gestion des ressources humaines est un processus crucial dans l’évaluation du plan d’action d’une organisation. Elle peut se faire à partir de critères établis ou bien de résultats enregistrés après une mise en œuvre de stratégies de ressources humaines dans une organisation. L'évaluation de la gestion permet une révision complète des politiques du capital humain au sein d’une organisation et un ajustement de son plan d’action.Il est important et souvent très nécessaire d’évaluer méthodiquement les politiques ainsi que les pratiques de gestion des ressources humaines. Pour obtenir le succès prévu, il serait impératif de faire une bonne évaluation qui permet une amélioration constante. Cette étape est en quelque sorte une évaluation qui détermine la performance organisationnelle. Ainsi, elle peut soit être forte ou faible. Dans le cas où cette dernière se trouve à être faible, il faut ressortir tous les points négatifs de la fonction des ressources humaines et par la suite déterminer la source de ces problèmes. Ils peuvent être perçus dans la mise en œuvre des politiques de gestion ou dans le plan lui-même. Si le problème provient de l’application des politiques de ressources humaine, les gestionnaires auront tendance à s’opposer aux changements dans leurs plans stratégiques. De plus, les employés peuvent aussi avoir cette attitude envers les changements soudains, car ceci aura tendance à leur donner des nouvelles responsabilités. Pour éviter les conflits, il est important d’avoir des évaluations régulières afin d’appliquer les changements d’une façon constante, car une application soudaine des changements cause des conflits. « L’un des obstacles majeurs à franchir, autant pour les responsables que pour la Direction RH, est l’indifférence des systèmes actuels RH quant aux missions et projets de plus en plus transversaux. Ainsi, la participation des techniciens d’un laboratoire à une mission transversale va dégrader leur ratio de productivité au sein de leur unité de production. Et il en sera de même pour tous les autres membres des équipes transversales, qu’ils soient des services marketing, juridique, informatique, recherche… ».Des cas de pratiques de forced ranking, ou sous-notation forcée, dans certaines entreprises sont progressivement dévoilés. Il s'agit de sous-évaluer un salarié pour remplir des quotas de mauvais salariés et pouvoir les licencier pour insuffisance professionnelle, tels les cas rapportés chez Sanofi ou dans le secteur de l'automobile.Le but d’un entretien d'évaluation et de développement est d’identifier les écarts entre les compétences dont dispose un salarié et les exigences du poste qu’il occupe (telles que définies par son cahier des charges), afin de déterminer les objectifs de développement prioritaires. Cette évaluation peut avoir lieu dans le cadre de l’entretien annuel d’évaluation, ou faire l’objet d’un entretien spécifique.Afin de faire une évaluation adéquate de la gestion des ressources humaines, il suffit de faire la comparaison entre les objectifs fixés et les résultats finaux à l’aide des critères d’évaluation et de correction. Ces critères doivent refléter les résultats escomptés, dont il s’agit de mesurer la pertinence des actions entreprises pour atteindre les objectifs fixés en tenant compte des divers partenaires de l’organisation. Enfin le résultat des évaluations doit apporter des mesures correctives qui vont améliorer et repositionner les politiques de gestions des ressources humaines d’une organisation afin qu’elle soit performante dans son environnement interne et externe.L’évaluation de la performance des ressources humaines passe par un travail organisationnel de définition des indicateurs de performance individuelle et de coordination en vue de l’utilisation de ces indicateurs. Le recours à des solutions logicielles permet de faciliter l’accès à de nombreux indicateurs sur la gestion des talents et de mettre en lumière la performance des salariés clés de l’entreprise. On peut citer notamment le recours fréquent aux SIRH. Une étude indépendante ayant analysé la question a ainsi montré que les entreprises et administrations françaises ont recours à trois expertises différentes liées à l’utilisation de logiciels dans l’évaluation de la performance de la fonction ressources humaines : le conseil, l’externalisation et le décisionnel.Si les solutions de gestion des talents et d’évaluation de la performance RH sont historiquement apparues sur les marchés par l’intermédiaire de spécialistes d’un des trois domaines d’expertise, des solutions généralistes apparaissent également. Il existe de très nombreuses solutions informatiques, appelées SIRH, internalisables ou en SaaS qui permettent la gestion des ressources humaines :La gestion des compétencesLa gestion du planningLa gestion de la paieLa gestion de la formationLa gestion du recrutementLa gestion des risques professionnels La numérisation du processus des ressources humaines La fonction des Ressources Humaines n'échappe pas aux processus de numérisation. En quelques années, l'évolution des nouvelles technologies a poussé les entreprises à évoluer. Le domaine des ressources humaines est aujourd'hui particulièrement concerné par ce phénomène.Cette numérisation consiste en l'utilisation des nouvelles technologies et des NTIC (Nouvelles Technologies de l’Information et de la Communication) afin de rendre plus efficace l'ensemble des fonctions des ressources humaines. La numérisation est aujourd'hui un moyen d'optimiser la gestion du service RH via la réduction de tâches chronophages. On entend par là les nombreux documents (papiers) associés au service des ressources humaines. La dématérialisation de ces documents s'impose donc de manière logique dans les processus RH purement administratifs (paie, congés, absences…).Les fonctions des Ressources Humaines les plus impactées par la numérisation sont :Le recrutement : Le recrutement traditionnel fait place aujourd'hui à de nouveaux procédés. On parle aujourd'hui de recrutement en ligne, c'est-à-dire de recrutement qui utilise un ensemble d'outils informatique (Smartphones, visioconférence, salons virtuels…) mais également de recrutement 2.0, c'est-à-dire un recrutement qui utilise les outils internet (Candidature sur des sites, vivier numérique…).La gestion de la paie : L'utilisation des différents logiciels de paie permet aujourd'hui aux services RH de se concentrer sur des tâches plus « sociales », comme le management, le climat social ou le bien-être des salariés.La formation : Le domaine de la formation est une fonction qui est réellement impactée par le numérique. Même si la formation en présence reste une valeur sûre pour la formation des salariés (avec la formation continue notamment), de nouvelles méthodes prennent une place de plus en plus importante. En plus de la formation en ligne et des MOOC (Massive Open Online Courses ou cours en ligne ouverts et massifs), on trouve aujourd'hui de nouvelles formes de formation comme le Blended learning (on désigne par le terme de blended-learning, la formation dispensée selon plusieurs modalités d’apprentissage cumulatives : en présence, à distance asynchrone et à distance synchrone), les classes virtuelles, les « serious games », les plateformes d'engagement... Ces nouvelles formations ont un coût bien plus faible que les formations traditionnelles.Les professionnels des ressources humaines peuvent être membres d'associations.En France, on trouve par exemple l'ANDRH (Association Nationale des Directeurs de Ressources Humaines). Des associations de jeunes professionnelles permettent également de se tenir régulièrement informés Au niveau Européen, on trouve l'EAPM (European Association for People Management). Au Québec, le CRHA (Ordre des Conseillers en Ressources Humaines Agréés).Cahier de l'université d'hiver http://www.entreprise-personnel.com/#/entre-nous/activites/publications/etudesLa Gestion des ressources humaines, Tania Saba, Simon L. Dolan, Susan E. Jackson et Randall S. Schuler, Compagnon Web, édition 4, 2008, p. 71-72.De la lutte contre les discriminations à la promotion de la diversité au sein de l'entreprise, Paul Schiettecatte, Jean-françois Roquet, Catherine de Verdière, Didier Rapeaud, Bénédicte Michon (…), éditions EMD-Synthec (2007).Paroles d'experts RH, Yves Richez, Hélène Morel, Pierre-Eric Sutter, Frédéric Fougerat, Benjamin Chaminade, Didier Pitelet, Véronique Frogé, Isabelle Deprez, Editions Studyrama, collection Focus RH, 1er trimestre 2013, 284 p.Ressource relative à la santé : (en) Medical Subject Headings https://disruptifrh.com/2017/08/01/construire-sa-veille-rh/ Portail du management   Portail du travail et des métiers   Portail de la psychologie"
économie;"Selon le vocabulaire de la comptabilité nationale, l’investissement (mesuré par la formation brute de capital fixe, en abrégé FBCF), peut être le fait de différents agents économiques :pour les entreprises : c'est la valeur des biens durables acquis pour être utilisés pendant au moins un an dans leur processus de production. Il peut avoir trois formes : capacité, remplacement et productivité ;pour les ménages : la FBCF dans le cadre de leur activité domestique ne concerne que l'acquisition ou la production pour leur propre compte de logements ;pour les entrepreneurs individuels : la formation brute de capital fixe (FBCF) des ménages en tant qu'entrepreneurs est comptée dans la FBCF des entreprises.Les investissements financiers, les acquisitions de terrains et les investissements immatériels (publicité, etc.) ne sont pas comptabilisés dans la FBCF, bien que ces investissements aient pris depuis les années 1980 une grande importance dans la stratégies des sociétés.Le rendement d'un investissement fait l'objet d'un calcul prenant en compte sa durée de vie ou sa durée d'utilisation (avec dans ce cas la prise en compte de valeur finale résiduelle de l'investissement).L'investissement durable stratégique désigne un type d'investissement se voulant plus vertueux, dans la perspective de développement durable, ce qui implique de prendre en compte dans la prise de décision d'investissement de nouveaux paramètres comme la durabilité et la soutenabilité.L'impact d'un investissement sur une entreprise, qui produit et/ou met à disposition des biens et des services, peut être financier ou uniquement en propriété.Cet impact est financier si l'entreprise reçoit effectivement le montant de l'investissement. Cet investissement va donc augmenter son capital social.Seul un investissement sur le marché primaire (ex : lors de la fondation d'une société ou lors d'une émission d'actions d'une société existante) a un impact financier sur l'entreprise. Cet investissement sert en général à acquérir ou améliorer des moyens de production (machines, locaux, informatique, etc.).Un investissement sur le marché secondaire (ex : un produit financier d'une assurance vie composé d'un « panier » d'actions) est un échange financier (ex : entre un particulier et sa banque) dont pas un sous ne va aux entreprises dont les actions composent le panier.Dans tous les cas, que ce soit un investissement sur le marché primaire ou secondaire, il a un impact sur la propriété. Ainsi, les actions rachetées par d'autres lors d'une OPA peuvent provoquer des changements importants d'actionnaires qui, en vertu des pouvoirs que donne la propriété des actions, sont en mesure de modifier profondément les destinées de l'entreprise et de ses salariés, alors même que l'entreprise elle-même n'a pas reçu un sous : la transaction est uniquement entre investisseurs. Il en est de même lorsque des membres d'un C.A. de sociétés sont des gestionnaires de fonds d'investissement ou de pensions tenus de rentabiliser les placements de leurs petits ou gros épargnants.Ces deux marchés (primaire et secondaire) nous suggèrent une typologie des investissements :Investissements ayant une finalité d'accroissement du capital technique (ou capital fixe, ou capital productif) ;Investissement financier dont la finalité est de rechercher une contrepartie (placement) ou à plus ou moins long terme un gain financier (plus-value).Marx parle de cet investissement d'une manière que certains qualifieraient de manichéenne dans « Das Kapital, Band 2, Abschnitt 1, 1.4 Der Gesamtkreislauf » :« Geldmachen ist das treibende Motiv. Produktion erscheint nur als notwendiges Übel dazu. » soit « Gagner de l'argent est le motif moteur. Pour cela, la production n'apparaît que comme un mal nécessaire ». … à défaut de pouvoir s'en défaire ou d'en rêver comme il le précise dans la parenthèse ensuite : « Alle kapitalistischen Nationen ergreift periodisch ein Schwindel, den sie zur Geldmacherei frei von lästiger Produktion nutzen. » soit « Toutes les nations capitalistes ont périodiquement une chimère, celle de pouvoir faire du fric en se passant d'une production pesante ennuyeuse ».Ce rêve de légèreté et de vitesse des investissements se réalise justement dans la sphère financière, dans le marché secondaire, avec des « produits » financiers de toute sorte et les systèmes « électroniques » pour les transactions internationales. Il se réalise également dans la sphère de l'économie réelle, parfois au détriment de PdG trop adeptes d'une logique industrielle ou sociale et pas assez d'une logique « financière ».Ainsi, Pierre Suard, ancien PdG d'Alcatel, a été nommé par des investisseurs dont la finalité était productive. Il a créé un empire industriel à l'image de Siemens, son concurrent le plus semblable. Il a été débarqué et remplacé par Serge Tchuruk en 1995 après l'arrivée de nouveaux actionnaires dont les investissements étaient plutôt à finalité « financière ». Ce dernier a concentré Alcatel sur son « cœur de métier », escompté le plus rentable, et a vendu le reste. Le changement de slogan qui a suivi, même du « cœur de métier », est révélateur d'un changement de finalité des investissements, moins industriel et plus financier : le slogan « être un architecte d'un monde internet » est remplacé par « apporter de la valeur ajoutée aux actionnaires ».En mars 2021, la même mésaventure arrive à Emmanuel Faber, PdG de Danone débarqué sous l'impulsion d'actionnaires anglo-saxons insatisfaits des résultats financiers de leurs investissements, obérés, d'après eux, par la politique sociale de ce PdG.À la vue des réalités économiques actuelles, il semble que l'influence des investissements « financiers », y compris sur le marché primaire, soit de plus en plus grande.Cette domination des investissements « financiers » peut aussi s’apprécier en considérant les flux financiers : (1-) les flux financiers correspondant au marché primaire (à savoir investissements productifs) sont beaucoup moins importants que ceux correspondant au marché secondaire (à savoir investissements « financiers ») ; (2-) même au niveau du marché primaire, il semble que l'investisseur souhaite minimiser, rendre ""marginale"" son investissement et il a à sa disposition les outils juridiques pour le faire.En effet, le plus souvent, les entreprises investissent directement soit en recyclant une partie de leurs bénéfices, soit surtout en empruntant directement sur les marchés bancaires ou obligataires. La part d’investissement par le marché primaire (ex : par émission d'actions) est minime au regard de leur investissement direct : en 2016 investissement par émission d'actions : 22 M€ ; par emprunt des entreprises : 297 M€ (source : LaTribune et Insee). De plus, il faut déduire des investissements sur le marché primaire la part de plus en plus importante de « rachat d'actions » par l'entreprise sur ordre de ses « investisseurs », ce « rachat » consiste à reverser à ceux-ci les montants de la valorisation d'une partie de leurs actions pour les « annuler ». Souvent l'entreprise doit emprunter pour cela.Enfin, l'investissement, au regard des investissements directement faits par les entreprises, est à considérer en tenant compte du concept de « responsabilité limitée » conjugué avec la non réalité juridique de l'entreprise : les investisseurs d'une entreprise ont de fait la propriété et le contrôle de TOUS les moyens de production de celle-ci alors même qu'ils n'y ont que peu contribué par leur argent.Le concept de « responsabilité limitée » et sa mise en œuvre dans les lois au XIXe siècle (ex : en France, lois du 23 mai 1863 puis du 24 juillet 1867 ; en Angleterre lois de 1856 à 1862 sur les Joint-Stock Company limited) compte, d'après Y.N. Harari dans son célèbre ouvrage Sapiens, « parmi les inventions les plus ingénieuses de l’humanité » : « Peugeot est une création de notre imagination collective. Les juristes parlent de « fiction de droit ». Peugeot appartient à un genre particulier de fictions juridiques, celle des « sociétés anonymes à responsabilité limitée ». L’idée qui se trouve derrière ces compagnies compte parmi les inventions les plus ingénieuses de l’humanité ». Harari en explique les avantages : « Si une voiture tombait en panne, l’acheteur pouvait poursuivre Peugeot, mais pas Armand Peugeot. Si la société empruntait des millions avant de faire faillite, Armand Peugeot ne devait pas le moindre franc à ses créanciers. Après tout, le prêt avait été accordé à Peugeot, la société, non pas à Armand Peugeot, l’Homo sapiens » actionnaire !Cette explication montre que la « responsabilité limitée » est en fait non pas une limitation des risques mais un véritable transfert de responsabilité et des risques de l'investisseur-actionnaire à la société-entreprise, à son collectif de travail, responsabilité pénale et économique. Toutefois ce transfert ne s'accompagne pas en retour d'un transfert de propriété du fait de la non réalité juridique de l'entreprise : quel que soit le montant investi par l'investisseur-actionnaire il a toujours le pouvoir et est propriétaire de fait (de par sa possession des actions) de tous les moyens de production (locaux, machines, moyens informatiques, etc.), y compris de ceux acquis grâce aux « millions » empruntés : c'est l'entreprise, qui acquiert en empruntant, qui rembourse, et qui entretient à ses frais les moyens de production en plus, bien entendu, de payer les salaires, charges et taxes.Grâce à cette « responsabilité limitée » conjuguée avec la non-réalité juridique de l'entreprise, plusieurs procédés permettent aux investisseurs-actionnaires d'accroître les moyens de production qu'ils contrôlent en minimisant au maximum leur mise (le capital social) : investissement par effet de levier, achat à effet de levier, rachat d'actions. Il est donc très compréhensible que les investisseurs-actionnaires recourent à ces procédés plutôt que d’émettre des actions supplémentaires provoquant l'arrivée d'autres investisseurs-actionnaires avec qui certes les risques sont partagés mais également le pouvoir et la propriété. Si l'entreprise était, comme une association 1901, sujet de droit, la « responsabilité limitée » serait remplacée par les « responsabilités et propriétés partagées » entre actionnaires et le collectif de travail de l'entreprise, chacun selon sa contribution. Les procédés « à effet de levier » et autres au profit de certains ne seraient plus et beaucoup d'autres s'en réjouiraient.Sous la finalité générale d'accroissement du capital technique (ou capital fixe, ou capital productif) des objectifs plus précis peuvent être visés :l'investissement de remplacement ou de renouvellement, a pour but de maintenir l'activité à son niveau actuel ;l'investissement de modernisation ou de productivité, a pour but d'accroître la productivité en introduisant des équipements modernes et perfectionnés ;l'investissement de capacité ou d'expansion, a pour but d'augmenter la capacité de production de l'entreprise en ajoutant par exemple des unités de production que ce soit d'un produit déjà existant, il s'agit alors d'une expansion quantitative, ou d'un nouveau produit - on parle alors d'expansion qualitative ;l'investissement total.L'investissement peut être qualifié de :productif : attention double sens possible :soit renvoie à l'idée qu'il s'agit d'un investissement de nature directement productive,soit renvoie à l'idée de l'efficacité de son rendement : la valeur cumulée des biens et des satisfactions obtenues est supérieure voire très supérieure au coût investi ;non directement productif (voire improprement qualifié d'improductif): il concerne des biens et des services d'utilité publique (écoles, hôpitaux, etc) ;matériel : il se traduit par la création d'un bien ou actif réel (un bien de production, par exemple) ;immatériel : il concerne des services : formation, recherche-développement, innovation, marketing, technologies de l'information, publicité, etc., susceptibles d'apporter un développement futur ;financier : il doit être considéré à part compte tenu de ce que sa finalité est de rechercher une contrepartie (placement) ou à plus ou moins long terme un gain financier (plus-value) ;stratégique, lorsqu'il est jugé essentiel pour la survie ou l'avenir de l'investisseur ;réputationnel, lorsqu'il contribue ou est nécessaire à la réputation de l'entreprise ou à son maintien, avec par exemple la publicité, l'acquisition de certains labels et certifications, certaines formes de mécénat, le rappel de produit.L'investissement « productif » se décompose d'abord en bâtiments puis en équipements.La manière dont sont enregistrées et répertoriées les dépenses d'investissement peut conduire à des difficultés pratiques :Par exemple, les dépenses en technologies de l'information sont habituellement rattachées à des centres de coût dans les entreprises. Or dans ce type de dépenses, 50 % en moyenne[réf. nécessaire] concerne la maintenance d'applications existantes, (dépenses d'exploitation) les 50 % restant concernent les développements (dépenses d'investissement). Or la distinction est souvent perdue dans la comptabilité des entreprises (avec un impact fâcheux sur l'évaluation objective de l'effort d'investissement et/ou d'innovation).On parle d'investissement brut quand le flux d'investissement comprend l'investissement neuf et l'investissement de remplacement.Le calcul de l'Investissement net s'obtient par différence entre : Capital technique de fin de période - Capital technique en début de période. Il représente l'investissement brut moins l'amortissement. Selon la théorie économique L'investissement doit être fait jusqu'au point où son bénéfice marginal égale son coût marginal. Ceci suppose évidemment que les biens d'investissements nécessaires soient disponibles. Selon le critère de la rentabilité Investir revient à engager de l'argent dans un projet, en renonçant à une consommation immédiate ou à un autre investissement (coût d'opportunité) et en acceptant un certain risque, pour accroître ses revenus futurs.La rentabilité est mesurable selon différentes méthodes qui ne donnent pas toutes toujours exactement le même résultat, tout en restant globalement cohérentesle retour sur investissement, qui peut s'exprimer en taux ou en temps, mesure le ratio des sommes rapportées par l'investissement sur le montant investi ;la valeur actuelle nette : l'investissement rapporte la différence entre son coût et la VAN, qui dépend du taux d'actualisation retenu ; elle diffère du retour sur investissement en ce qu'elle tient compte du montant total investi : par exemple, le retour sur investissement peut être meilleur pour l'achat d'une bicyclette que d'une maison (rendement respectif de 100 % et 1 %) et la VAN en sens inverse (VAN respective de 200 € et 100 000 €) ;le taux de rentabilité interne (TRI) : l'investissement est d'autant plus rentable que ce taux est élevé (cependant, pour un taux d'actualisation donné et connu, la VAN est un indicateur plus significatif, alors qu'on peut trouver deux investissements A et B tels TRIA > TRIB et VANB > VANA).On peut également assimiler à la rentabilité des critères tels que le temps nécessaire pour atteindre le point mort (durée nécessaire pour que les flux générés soient égaux au montant de l’investissement initial).Le risque pris par l'investisseur est aussi un critère important, dont un indicateur est le ratio de la capacité d'autofinancement par rapport au montant investi ; il est souvent fait à titre prévisionnel pour déterminer si un investissement proposé est adapté, et dans quelle mesure il satisfera l'investisseur.Quelle que soit la méthode utilisée, les paramètres suivants doivent être convenablement appréciés et intégrés dans le calcul :le capital investi a une durée prévue d'utilisation à la fin de laquelle il peut encore présenter une valeur résiduelle ;le prix relatif du capital par rapport à celui du travail influe sur l'investissement. Lorsque le prix du capital baisse par rapport à celui du travail, il est intéressant d'engager des investissements de productivité, qui permettent de substituer du capital (moins cher) au travail (plus cher) ;les taux d'intérêt déterminent le coût des emprunts contractés pour effectuer un investissement et peuvent donc freiner l'investissement s'ils sont élevés ;le niveau d'endettement de l'entreprise joue aussi : une entreprise endettée devra consacrer ses profits à son désendettement au risque de disparaître ;les entreprises cherchent à anticiper la demande avant d'investir pour savoir s'il est nécessaire d'augmenter leurs capacités de production. Ainsi, des anticipations favorables où l'on prévoit une hausse de la demande, favorisent l'investissement tandis que les anticipations défavorables qui prévoient une stagnation ou une baisse de la demande, le freinent. C'est le principe de la demande anticipée ou effective évoquée par Keynes. C'est la demande anticipée des entrepreneurs qui va déterminer l'offre. Autres méthodes financières D'autres méthodes existent qui s'inscrivent dans la mouvance des théories financières intégrant davantage l'incertitude future liée aux valorisations découlant du marché :James Tobin a proposé un critère, appelé le Q de Tobin qui compare la valeur boursière de l'investissement avec son coût de remplacement ;Dixit et Pindyck(1994) proposent de faire l'analogie avec les options: pour la décision d'investissement, l'entrepreneur a le choix entre ne rien faire (attendre) ou investir tout de suite, choix dont l'irréversibilité joue un rôle important dans la productivité.Dans sa décision d'investir, l'entrepreneur compare le cout de l'investissement (I) et la somme des valeurs, actualisées et pondérées par les risques, des rentrées de trésorerie obtenues grâce à l'investissement (R). Le projet d'investissement sera réalisé si R > I. Dans l'analyse keynésienne, l'efficacité marginale du capital désigne le taux de rendement interne de l'investissement. Elle sert de taux d'actualisation des recettes tirées de l'investissement. À savoir, l'investissement est d'autant plus important que le taux d'intérêt est faible. Pour Keynes, l'investissement dépend de la comparaison entre l'efficacité marginale r de l'investissement et le taux d'intérêt pratiqué sur le marché des capitaux, i. Si r > i, la décision de réaliser l'investissement est justifiée. Il peut être financé soit à partir de fonds dont dispose l'entreprise, soit à partir d'emprunt dont le coût est inférieur au taux de rendement de l'investissement. La formule de Keynes n’est valable que pour un investissement financé uniquement par la dette. Si une partie du financement est apportée en fonds propres, il est nécessaire d’en calculer le coût puis de calculer le coût moyen pondéré du capital, qui sera substitué à i. Par ailleurs, l’entrepreneur prendra une marge de sécurité car en pratique, le rendement de l’investissement ne sera pas égal à celui anticipé.Dans l'analyse macro-économique, le terme d'investissement est réservé à la seule création de biens capitaux nouveau (machines, immeubles...). Pour Keynes, l'investissement dépend de l'efficacité marginale du capital et du taux d'intérêt. En fait, les dépenses en biens d'investissement dépendent principalement de deux variables :le rendement attendu de l'investissement, dit ""efficacité marginale du capital"",le taux d'intérêt i ou coût d'emprunt contracté pour financer l'acquisition de biens d'investissement.Pour une efficacité marginale donnée, l'investissement apparaît comme une fonction décroissante du taux d'intérêt. Le niveau du taux d'intérêt est donc la variable incitatrice ou désincitatrice privilégiée du processus d'investissement. Dans l'analyse Keynésienne, l'investissement est considéré comme autonome, c'est-à-dire indépendant du revenu.Avant toute chose, le dirigeant doit faire tout d'abord son métier en restituant l'investissement dans la stratégie d'entreprise et l'organisation d'entreprise. À défaut, il risque de prendre des décisions hâtives en matière de moyens mais sans chemin pertinent et/ou dans une facilité trompeuse qui juge inutile la nécessité de cette réflexion.Avant d'engager ses ressources propres à l'investissement, l'entreprise doit en effet examiner toutes les solutions possibles pour financer son besoin de financement : autofinancement, recours à l'emprunt, leasing, aides publiques (pour la R&D), augmentation de capital ou financement par prélèvement sur fonds propres. Ces sources de financement peuvent être combinées.Il faut aussi noter que les investissements peuvent aussi être financés par cession d'actifs , (dans l'hypothèse où l'entreprise désinvestit dans le cadre d'une stratégie de réorientation ou de recentrage de ses activités).Le législateur offre des possibilités de réduction d'impôt sur le revenu et impôt de solidarité sur la fortune (ISF) pour les particuliers qui investissent dans les PME. La PME doit répondre à des critères quantitatifs (CA < 50M€, emploie de moins de 250 salariés) et à la définition de PME communautaire.Pour l'ISF la réduction fiscale ne concerne que la souscription au capital initial de la société ou la souscription à une augmentation de capital de celle-ci. La possibilité de réduire son ISF risque donc de ne profiter principalement qu'à un cercle réduit de contribuables, sollicités par leur entourage pour participer à ce type d'opérations souvent réalisée en cercle restreint.L'autofinancement est le financement des investissements par des moyens internes à l'entreprise. L'autofinancement se mesure de deux manières : le taux de marge qui donne une indication sur les ressources de l'entreprise (excédent brut d'exploitation / valeur ajoutée) et le taux d'autofinancement : EB/FBCF (Formation Brute de Capital Fixe) qui mesure la part de l'investissement qui est financée par l'épargne brute (partie de l'EBE, hors dividendes, intérêts et impôts, servant à financer la FBCF).Cela consiste à lever des capitaux sous forme de prêt auprès de tiers. La durée de l'emprunt doit être en accord avec la durée d'amortissement du bien acheté (en général l'emprunt est un peu plus court que celle-ci). L'emprunt peut être de 2 types : bancaire ou obligataire.Il s'agit d'augmenter les capitaux propres de l'entreprise en faisant souscrire de nouvelles parts (SARL) ou actions (SA). Il est demandé, via une opération d'augmentation de capital en numéraire,aux actionnaires de mettre la main à la poche pour financer les investissements ;et/ou à de nouveaux actionnaires d'entrer dans le capital de l'entreprise.Cette méthode a l'avantage de renforcer la solvabilité de l'entreprise, laquelle de toute façon ne peut dépasser un certain montant de recours à l'emprunt sans perdre la confiance de ses banques et fournisseurs. Cela dit, cette opération est assez souvent mal vue par les actionnaires, car l'émission de nouvelles actions va « diluer » la valeur de leurs actions actuelles.Cette méthode n'est donc utilisable que si les actionnaires acceptent de remettre de l'argent dans la société. Cela dépendra en grande partie :de la rentabilité des fonds propres affichée ou visée par l'entreprise. Cette rentabilité et le risque qui lui est associé doit être comparée aux autres couples rentabilités/risques disponibles par ailleurs ;et, pour les sociétés cotées, du cours de bourse, qui doit être supérieur au prix d'émission des nouvelles actions pour qu'il y ait intérêt à souscrire celles-ci ;ou encore, facteur plus négatif mais qui entraîne une pression forte sur les actionnaires, d'une situation d'endettement critique risquant de faire sombrer l'entreprise si elle ne trouve pas de l'argent frais pour conforter ses capitaux propres.L'augmentation de capital en numéraire ne doit pas être confondue avec celle par incorporation de réserves (il ne s'agit alors que d'un transfert de poste comptable à l'intérieur des capitaux propres) ni celle par échange de titres (cas de fusion-acquisition)On parle de mal-investissement lorsque l'investissement est inadéquat : trop élevé (sur-investissement), trop faible (sous-investissement), ou les deux à la fois (i.e. : mal orienté).La décision d'investir ou de ne pas le faire, est toujours une forme de pari sur l'avenir : il n'est donc pas étonnant de rencontrer des investissements inadéquats. Lorsqu'une accumulation d'investisseurs se trouvent commettre la même erreur, plus ou moins simultanément, celle-ci peut générer - au niveau macro-économique, dans une filière d'activité ou dans une zone géographique - des situations pouvant aller de la simple récession à la crise économique de plus grande ampleur (voir l'analyse du cycle économique).En régime d'économie libre, la variable essentielle en la matière est le taux d'intérêt. Trop élevé, il rend impossible l'investissement même dans des projets a priori rentables. Trop bas, il favorise l'investissement dans des projets à la rentabilité trop faible.Des agents économiques trop optimistes peuvent sur-investir et créer des capacités de production excédentaires par rapport à la demande effective exprimée par le marché. À l'échelle d'un pays, ou d'une branche d'activité, l'insuffisance constatée des débouchés par rapport à l'offre ainsi créée va provoquer un effet déflationniste et la faillite des entreprises marginales (celles dont le prix de revient est le plus élevé).Goal-based investingDirection générale des entreprises (France)Oséo (France)Small Business Administration (États-Unis)Direction générale des entreprises (Espagne)Ressource relative à la santé : (en) Medical Subject Headings Ressource relative aux beaux-arts : (en) Grove Art Online  Portail de l’économie   Portail de la finance"
économie;"Il existe plusieurs définitions de la valeur selon le courant de pensée économique. Elles se rattachent à deux conceptions principales qui donnent au mot « valeur » des sens radicalement différents, et impliquent deux conceptions différentes de la relation entre valeur et prix.La conception subjective définit la valeur comme l'expression de l'intérêt qu'un agent particulier porte à un bien ou à un service, qui résulte d'un processus psychologique d'évaluation. C'est une notion subjective et privée dont la formation et l'explication relèvent de la psychologie et non de l'économie, et qui constitue une donnée externe pour le raisonnement économique. Le prix est une notion distincte, qui résulte du fonctionnement effectif des mécanismes du marché, et qui seule a un sens économique. Formation de la valeur et formation du prix sont considérés comme deux processus distincts, seul le second relevant de l'analyse économique,La conception objective pose que tout bien a une valeur indépendante de l'observateur, qui résulte des conditions de sa production et peut être déterminée par un calcul économique à partir des conditions et des coûts de production du bien ou du service. Le prix est alors généralement considéré comme une mesure de cette valeur.La conception subjective est dominante depuis l'origine de la pensée économique (Aristote, Thomas d'Aquin). Elle a été maintenue par les classiques français (Turgot, Say), alors que la conception objective a été proposée d'abord par les Physiocrates avec comme référence la terre, puis par les classiques anglais avec comme référence le travail, et enfin reprise par les économistes marxistes.Bien que fondamentalement adeptes de la conception subjective, les économistes du courant dominant néoclassique utilisent souvent les mots prix et valeur de façon interchangeable, Ce qu'ils appellent « théories de la valeur » sont plus précisément des théories de la formation des prix, censés représenter une « valeur sociale » plus ou moins objectivée. Seuls les économistes du courant « autrichien » s'efforcent d'être fidèles à cette distinction, sans toujours y parvenir.D'autres disciplines s'intéressent aux notions de « valeur » ou de « valeurs » : Voir les articles spécifiques qui leur sont consacrés.L'histoire du concept de Valeur est dans l'article : Valeur en philosophie.Dans cette perspective la valorisation d'un bien est fondée sur un facteur objectif, matérialisé par un critère mesurable, le travail.Les physiocrates et particulièrement François Quesnay mettent en avant la valeur provenant de la nature. Elle repose sur l’idée que seule la production agricole est créatrice de richesses et que toute autre activité artisanale ou commerciale n’est que l’addition de travail aux matières premières issues directement ou indirectement des produits de la terre.Ainsi, l'estimation de la valeur résulte de l'agrégation de « la valeur des produits naturels constituant le capital nécessaire à une production + la valeur des produits naturels nécessaires à la reproduction de la force de travail ».Les économistes Adam Smith et David Ricardo distinguent la valeur d'échange de la valeur d'usage et considèrent que c'est la première qui, en économie, joue un rôle déterminant. Ils cherchent une cause objective pouvant expliquer le prix des marchandises. Suivant William Petty, John Locke, lequel a justifié la propriété individuelle par le travail, et David Hume, les économistes classiques anglais estiment que le travail joue un rôle essentiel dans la détermination de la valeur d'un bien.Adam Smith voit dans le travail la source de la richesse des nations.« The annual labour of every nation is the fund which originally supplies it with all the necessaries and conveniences of life which it annually consumes, and which consist always either in the immediate produce of that labour, or in what is purchased with that produce from other nations. »Selon lui la valeur d'un bien est égale à la quantité de travail que cette marchandise peut acheter ou exiger.« Labour was the first price, the original purchase-money that was paid for all things. It was not by gold or by silver, but by labour, that all the wealth of the world was originally purchased; and its value, to those who possess it, and who want to exchange it for some new productions, is precisely equal to the quantity of labour which it can enable them to purchase or command. »Non seulement le travail est une composante du prix, mais il est aussi à l'origine du profit et de la rente.« The real value of all the different component parts of price, it must be observed, is measured by the quantity of labour which they can, each of them, purchase or command. Labour measures the value not only of that part of price which resolves itself into labour, but of that which resolves itself into rent, and of that which resolves itself into profit. »David Ricardo développe la notion de valeur-travail introduite par Adam Smith et cherche à comprendre comment le travail se transfère en profit et en rente. Il commence par transformer la notion de valeur-travail. Pour lui la valeur d'un bien est égale, non à la quantité de travail qu'il peut commander, mais à la quantité de travail, direct et indirect, nécessaire à sa fabrication.« The value of a commodity, or the quantity of any other commodity for which it will exchange, depends on the relative quantity of labour which is necessary for its production, and not on the greater or less compensation which is paid for that labour. »Pour les penseurs classiques, les taux de profit d'industries différentes tendent à se rapprocher vers une même valeur basse, à mesure que la compétition entre les entreprises augmente. L'idée étant que, si un secteur est plus rentable que les autres, il attire naturellement de nouveaux investisseurs qui quittent d'autres secteurs aux taux de rentabilité plus faibles.« In a country which had acquired its full complement of riches, where in every particular branch of business there was the greatest quantity of stock that could be employed in it, as the ordinary rate of clear profit would be very small. »Ce constat a une implication sur la valeur des biens. Ricardo pose la question de savoir si la valeur-travail est compatible avec un taux de rentabilité uniforme parmi toutes les industries. La réponse est négative si les instruments de production ont des durées de vie différentes (voir ""durability of the instruments of production"" chez Ricardo). Aussi le concept valeur-travail ne semble pas cohérent avec les autres propriétés basiques d'une économie.Karl Marx reprend l'idée de la valeur-travail développé par Ricardo: la valeur d'un bien dépend de la quantité de travail direct et indirect nécessaire à sa fabrication. Mais alors que Ricardo considère le travail comme une commodité ordinaire, Marx juge l'expression 'valeur du travail' incorrecte partant du principe que le travail est à l'origine de toute valeur. Pour Marx les salaires ne représentent pas la valeur du travail mais la location de la force de travail du salarié (Arbeitskraft). Il propose l'explication suivante à l'origine du profit : de la valeur nouvellement créée, le salaire du travailleur ne représente que la part nécessaire à sa propre survie, le reste constituant la plus-value (marxisme) créée par son travail.Pour rendre la valeur-travail compatible avec un taux de plus-value uniforme parmi les industries, Marx radicalise la division du capital introduite par Adam Smith. Il distingue la part nécessaire au paiement des salaires, capital variable (de), du reste, le capital constant. Selon Marx seul le travail permet une augmentation du capital, d'où le terme variable. Cette décomposition permet de poser les bases d'un système concept de valeur-travail compatible un taux uniforme des profits (voir Le problème de transformation chez Marx). Vision des marxistes Les Marxistes apportent les nuances suivantes :L'utilisation de machines dans la production ne change en rien cette analyse objective de la valeur puisqu'une machine ne produit pas de valeur mais transmet simplement la sienne au bien qu’elle produit : la valeur dégagée par une machine est égale à l'usure de celle-ci, car une machine n’est que du travail accumulé (Marx).la valeur d'un bien est affectée par l'expression d'un certain type de rapport social de production, déterminé par l'état des forces productives.la valeur est aussi une propriété émergente du fétichisme de la marchandise qui vient de ce que :les hommes s'en remettent à la circulation des choses dans le cadre concurrentiel de l'équivalence généralisée pour établir des liens productifs entre eux. Elle n'aurait donc de sens que dans le cadre d'une économie de marché.la valeur est l'expression d'un rapport social de production qui se décompose en trois aspects :sa forme (l'échangeabilité qui induit la coordination des producteurs de marchandises sans organisation préalable),sa substance (le travail abstrait qui représente le travail socialement nécessaire pour produire la marchandise)et sa grandeur (la quantité de travail abstrait déterminé par l'état des forces productives).Le concept d'utilité, attribuant à chaque personne des goûts et des besoins différents, a la faveur de la grande majorité des économistes contemporains. L'origine de ce courant est ancienne. On peut le faire remonter à Démocrite, à saint Thomas d'Aquin, et aux scolastiques espagnols.Étienne Bonnot de Condillac évoque l'exemple de la valeur d'un verre d'eau dans le désert pour montrer combien l'utilité et la valeur d'usage sont en réalité le fondement unique de la valeur.Turgot et Say reprennent la notion à leur compte. (Voir à la même époque les travaux du mathématicien Daniel Bernoulli.)Les économistes marginalistes considèrent la valeur comme étant la mesure du désir qu'un agent économique éprouve pour un bien ou un service. C'est alors une appréciation subjective non mesurable, liée aux préférences de la personne compte tenu de sa situation actuelle. William Jevons développe lors d’un congrès en 1862 la notion de « degré final d’utilité » (utilité marginale). Pour reprendre l'exemple du verre d'eau, un homme assoiffé dans le désert est prêt à payer une « fortune » pour UN verre d'eau, un peu moins pour le deuxième quand il s'est déjà abreuvé, encore moins pour le troisième, etc., et ce indépendamment de sa valeur de production. William Jevons introduit donc une subjectivité dans la détermination de la valeur. La théorie néoclassique adopte cette conception de la valeur comme liée à l'utilité dégagée par la dernière unité échangée et à la satisfaction des autres besoins. La formation des prix ne dépend alors plus que de cette utilité marginale.La problématique de l'équilibre général, introduite par Léon Walras et développée dans son formalisme mathématique par Arrow et Debreu, est basée sur le concept d'utilité. Dans ce modèle les prix sont la conséquence de préférences individuelles modélisées par des fonctions d'utilités.L'école autrichienne d'économie, qui s'écarte de l'école néo-classique car refusant des méthodes scientifiques comme la formalisation de modèles mathématiques, développe plus avant une conception subjective de la valeur considérant que l'étude de la formation de la valeur relève de la psychologie et non de l'économie. « Le domaine de notre science est l’action humaine, pas les événements psychologiques qui résultent en une action » ou encore « L'économie commence là où la psychologie s'arrête » (Ludwig von Mises).André Orléan constate que les valeurs d'usage, la valeur marchande et la valorisation boursière sont fonction respectivement de la rareté, de la monnaie et des conventions financières. Les deux dernières sont des concepts construits par la société, la rareté pouvant l'être dans certains cas. Les forces sociales qui se trouvent à l'origine de ces concepts ne peuvent être ni fabriquées, ni contrôlées car elles échappent à l'intentionnalité individuelle. Il en déduit que, dans ces cas, la valeur « est une production collective qui permet la vie en commun. Elle a la nature d'une institution ».Les institutionnalistes accordent une place importante aux institutions. Celles-ci établissent les règles et les moyens permettant aux hommes de fonctionner en tant que société. Elles contribuent à déterminer les comportements et influent sur la formation et transformation des valeurs. Elles font passer d'une conception individuelle à une conception sociale de l'utilité. Jacques Perrin évoque les contraintes environnementales et la montée des inégalités entre les groupes sociaux et entre les pays pour justifier la prise en compte de l'utilité sociale. Pour les institutionnalistes la valeur sociale permet d'apprécier la richesse de la société.André Orléan remarque que la valeur économique n'est pas seulement due à l'utilité ou au travail incorporé mais peut résulter d'un sentiment collectif. Les investisseurs professionnels déterminent leurs positions non pas en fonction de leur propre calcul de la valeur fondamentale du titre mais en fonction de l'évaluation de cette valeur par le marché à l'instar du concours de beauté de Keynes. De même le mimétisme joue dans la détermination de la valeur d'un bien lorsque ce bien est représentatif de prestige et de statut social. Thorstein Veblen, le premier, a mis en évidence l'importance de l'opinion des autres dans la valeur accordée à un bien. L'utilité n'est alors pas forcément absente pour l'acquisition du bien, mais son importance est secondaire par rapport au prestige. Reprenant cette thèse Orléan cite les phénomènes de mode. L'utilité est alors fonction du comportement des autres. Les pratiques du marketing et de la publicité témoignent également de l'importance de la motivation mimétique. Citant Akerlof Robert Boyer rappelle que « les jugements de valeurs rétroagissent sur la possibilité d'obtention d'équilibres économiques efficients ».Les économistes ont décomposé la valeur des biens environnementaux et/ou culturels. La valeur économique totale se compose de valeurs d'usage et de valeurs de non-usage,.Les chercheurs distinguent trois valeurs d'usage :La valeur d'usage direct est liée à l'usage direct du bien. Il s'agit notamment des usages récréatifs, du tourisme ou encore de l'exploitation des ressources naturelles.La valeur d'usage indirect est lié aux effets du site sur des valeurs d'usage direct ailleurs. Pour les biens environnementaux, on parle généralement de la valeur des services écosystémiques. Cela peut aussi désigner des effets économiques induits par les valeurs d'usage direct.La valeur d'option se réfère aux usages futurs. Cela prend en compte le fait que certains éléments peuvent avoir utilité dans le futur, sans que nous la connaissions dès à présent. Par exemple, les végétaux présents dans une forêt peuvent contenir des principes actifs capable de soigner des maladies. Si ces principes actifs sont déjà connus, il s'agit d'une valeur d'usage direct. Si ces principes sont inconnus, alors il s'agit bien d'une valeur d'option, car on souhaite préserver la possibilité d'un usage futur (si l'espèce disparaît, on ne pourra plus rien découvrir grâce à elle).Les économistes distinguent deux ou trois valeurs de non-usage :La valeur de quasi-option (ou valeur d'option informationnelle) est parfois omise. Cette valeur correspond au fait que certaines décisions ont des effets irréversibles. Ainsi, les solutions qui permettent de préserver différentes options futures ont une valeur supplémentaire par rapport aux décisions irréversibles. La valeur de quasi-option désigne donc l'ensemble des gains (monétaires ou non) générés par l'information future, qui aura pu être mobilisée parce que différentes options auront été maintenues.La valeur d'existence correspond à la satisfaction de savoir que le bien existe. Cette satisfaction est indépendante d'un quelconque usage (direct ou indirect) de la part de l'individu concerné, ou même d'un autre individu.La valeur de legs (ou valeur d'héritage) est liée à la transmission du bien aux générations futures. Cela correspond en quelque sorte à la valeur d'existence, mais à long terme. Il s'agit donc de la dimension patrimoniale du bien.Dans le langage courant, et même dans les livres d’économie, « valeur » et « prix » sont souvent employés l’un pour l’autre. Cependant Smith, Ricardo et Marx distinguent valeur d’usage et valeur d’échange. Seule la valeur d’échange a un prix. La valeur d’usage n’est pas susceptible d’être mesurée. Elle s’interprète subjectivement et ne peut être quantifiée par un prix. Pour les néoclassiques valeur d’échange et valeur d’usage ne font qu’un. L’utilité d’un produit est mesurée par le prix que lui fixe le marché. Valeur et prix sont synonymes. Est écarté ce qui ne passe pas par le marché, les productions domestiques et le bénévolat. Concernant les éléments naturels (lumière solaire, air, ressources, etc.) les néoclassiques considèrent qu’ils ont une valeur économique qui se révèle lorsqu’ils sont mis sur le marché. Les tenants de l’écologie profonde, se référant à la distinction déjà faite entre valeur d’usage et valeur d’échange, estiment que les biens naturels ont une valeur qui relève de l’éthique ou du politique. Autrement dit, les ressources naturelles sont des richesses et elles n’acquièrent de valeur économique que par l’intervention et la valorisation du travail humain (une ressource gisant au fond des océans n’a aucune valeur économique si elle est inaccessible).André Orléan, L'empire de la valeur, Seuil, 2011Jacques Perrin, Pourquoi les sciences économiques nous conduisent dans le mur ?, L'Harmattan, 2011Alternatives Economiques, Hors-série pratique, no 31, 2007Thorstein Veblen, Théorie de la classe de loisir, Gallimard, 1970 pour la traduction françaiseSerge Paugam (Sous la direction de), Repenser la solidarité, PUF, 2007Conception objective de la valeurConception subjective de la valeurThéorie de la valeur (marxisme)Valeur travail (économie)Valeur nette comptableValeur nominale et valeur réelleThéorie de la valeur, Le Capital, Karl Marx, 1867. Portail de l’économie   Portail de la finance"
économie;"L'économie (ou économie politique, science économique) est une discipline qui étudie l'économie en tant qu'activité humaine, qui consiste en la production, la distribution, l'échange et la consommation de biens et de services.Le nom économie provient du grec ancien ????????? / oikonomía qui signifie « administration d'un foyer ».Si dans l'Antiquité Xénophon et Aristote ont chacun écrit un traité sur l'économie, c'est à partir du XVIIe siècle que se développe la pensée économique moderne, avec le mercantilisme, puis au XVIIIe siècle avec les physiocrates. L'économie politique débute à la fin du XVIIIe siècle avec Adam Smith, puis David Ricardo ou encore Jean-Baptiste Say (les classiques) au début XIXe siècle. C'est avec la révolution marginaliste à la fin du XIXe siècle que l'économie se constitue comme une discipline scientifique et s'institutionnalise.Au sein de la discipline, on distingue deux grandes approches : la macroéconomie, qui étudie les grands agrégats économiques (épargne, investissement, consommation, croissance économique), et la microéconomie, qui étudie le comportement des agents économiques (individus, ménages, entreprises) et leurs interactions, notamment sur les marchés.Comme dans d'autres disciplines, l'économie se décline selon un spectre depuis la théorie économique, qui vise à construire un corpus de résultats fondamentaux et abstraits sur le fonctionnement de l'économie, jusqu'à l'économie appliquée, qui utilise les outils de la théorie économique et des disciplines connexes pour étudier des domaines importants comme l'environnement, le travail, la santé, l'immobilier, l'organisation industrielle ou encore l'éducation.Les origines de la pensée économique remontent aux civilisations mésopotamienne, grecque, indienne, chinoise, perse et arabe. Civilisations mésopotamiennes À partir de la fin du VIe millénaire av. J.-C. les cités-États de Sumer ont développé leurs commerces et leurs économies à partir des marchés de matières premières.Les premiers codes de loi de Sumer pourraient être considérés comme les premiers écrits économiques, dont de nombreux attributs sont encore en usage dans la valorisation des prix d'aujourd'hui tels les montants codifiés d'échange d'argent lors des échanges commerciaux (taux d'intérêt), amendes, règles d'héritage, lois concernant la façon dont la propriété privée doit être imposée ou divisée, etc.. Monde grec Dans la Grèce antique, le terme économique apparaît comme le titre d'un traité de Xénophon (Économique) et d'un ensemble de traités attribués à Aristote (Économiques), dont l'objet est la connaissance et la formulation des lois (« nomos ») permettant d'optimiser l'utilisation des biens d'une maison (« oikos »), considérée comme unité collective de production d'une famille élargie ou d'un clan.Chez Aristote, la richesse est considérée du point de vue de l'abondance des biens produits et de leur utilité, non de l'accumulation de monnaie par l'usure ou le négoce dont les procédés font l'objet d'une autre discipline qu'Aristote appelle chrématistique (de khréma (la richesse) et -atos (degré superlatif)) et qu'il considère comme des activités stériles, voire déshonorantes dans l'Éthique à Nicomaque).L'Économique est explicitement distingué du terme Politique, laquelle fait l'objet d'un autre traité d'Aristote et vise à établir l'harmonie et la justice entre les différentes classes de personnes et de familles qui constituent la cité. Inde Le philosophe indien Chânakya (340-293 av. J.-C.), conseiller auprès du trône de l'empire Maurya de l'ancienne Inde, développe de nombreux concepts économiques, notamment dans son œuvre principale Arthashastra (La Science des richesses et du bien-être),,. Moyen Âge Au Moyen Âge les penseurs économiques sont avant tout des théologiens comme Thomas d'Aquin ou Ibn Khaldoun.Dans sa Somme théologique, Thomas d'Aquin examine de nombreuses questions de nature économique, dont la justification de la propriété privée, du commerce et du profit.Raisonnant dans le cadre du droit naturel, les penseurs scolastiques, ils préfigurent l'économie moderne dans le domaine de la politique monétaire, de l'intérêt, et la théorie de la valeur dans le cadre du droit naturel. XVe siècle - XVIIe siècle À partir de la seconde moitié du XVe siècle et jusqu'au milieu du XVIIIe siècle, la pensée économique se structure autour de la doctrine du mercantilisme.Ce courant correspond à l'émergence de la notion d'État face au pouvoir papal et au système féodal.Les penseurs mercantilistes prônent le développement économique par l'enrichissement des nations au moyen du commerce extérieur qui permet de dégager un excédent de la balance commerciale grâce à l'investissement dans des activités économiques à rendement croissant. Ils accordent un rôle primordial à l'État et prône des politiques protectionnistes établissant des barrières tarifaires et encourageant les exportations tout en visant à l'unification du marché national. Cette doctrine économique connaît son apogée du XVIe  au  XVIIIe siècle. Elle estime que la richesse d'une nation dépend de l'importance de sa population et de l'accumulation d'or et d'argent. Les nations qui n'ont pas accès aux mines peuvent obtenir l'or et l'argent en favorisant leur outil productif et en stimulant leurs exportations,.C'est au XVIIe siècle qu'apparaît la notion d'économie politique avec la publication d'Antoine de Montchrestien Traité d'économie politique (1615). XVIIIe siècle Dans la seconde moitié du XVIIIe siècle, la doctrine mercantiliste est remise en cause par les physiocrates d'une part et par la naissance de l'économie classique avec Adam Smith d'autre part.Inspirés en particulier par des ouvrages comme celui de Richard Cantillon, Essai sur la nature du commerce en général (1755) les physiocrates considèrent que la seule activité réellement productive est l'agriculture. La terre multiplie les biens : une graine semée produit plusieurs graines. Au bout du compte, la terre laisse un produit net ou surplus. L'industrie et le commerce sont considérés comme des activités stériles car elles se contentent de transformer les matières premières produites par l'agriculture.Les physiocrates s'attachent à la recherche des lois naturelles qui régissent les activités des hommes. Ils ont notamment schématisé l'économie comme un flux de revenus et de dépenses améliorant le modèle de Boisguilbert,.En 1776, Adam Smith publie Recherches sur la nature et les causes de la richesse des nations, considéré comme l'ouvrage fondateur de l'économie classique. Cette publication propose une synthèse cohérente des connaissances économiques de cette époque. Si Adam Smith est aujourd’hui surtout connu en tant qu’économiste, il se considérait avant tout comme professeur de philosophie morale (qu’il avait enseignée à Glasgow). Ainsi, la Richesse des nations ne traite pas seulement d’économie (au sens moderne), mais aussi d’économie politique, de droit, de morale, de psychologie, de politique, d’histoire, ainsi que de l’interaction et de l’interdépendance entre toutes ces disciplines. L’ouvrage, centré sur la notion d'intérêt personnel, forme un ensemble avec la Théorie des sentiments moraux, où il avait exposé la sympathie inhérente à la nature humaine. Économie politique classique Pour Adam Smith, l'augmentation de la population est synonyme d'augmentation de la richesse. Thomas Malthus, pasteur chargé de l'aide aux pauvres dans sa commune, est frappé par la misère engendrée par les mauvaises récoltes de 1794 à 1800. Il s'intéresse alors aux problématiques du progrès, de la croissance de la population et de la richesse. Son ouvrage principal, Essai sur le principe de population (1798), connait une grande popularité et conduit à un des premiers recensements de la Grande-Bretagne.Avec la publication Des principes de l'économie politique et de l'impôt (1817), l'économiste et philosophe britannique David Ricardo développe et enrichit les thèses de la valeur, du libre-échange popularisées par Adam Smith.Pour Daniel Villey, « les bases essentielles du système ricardien — la loi de la population, la loi des rendements décroissants, la théorie de la rente — viennent de Malthus ». Pour Malthus, la population a tendance à augmenter géométriquement alors que la production de denrées alimentaires ne s'accroît que de manière arithmétique. Pour rétablir l'équilibre, la Nature dresse des obstacles efficaces (famines, épidémies, etc.) mais inhumains. Pour Malthus, un pasteur, il conviendrait plutôt de limiter la reproduction par des moyens artificiels. Il y a chez lui un certain pessimisme sur les capacités d'augmenter la production du fait de la loi des rendements décroissants, de la limitation des ressources naturelles et de la propension des humains à proliférer, qui devraient conduire à des famines. Les travaux de Malthus conduisent Thomas Carlyle à qualifier l'économie de « science lugubre ». Malthus conteste également qu'une économie de marché conduise automatiquement au plein emploi comme le fera également Keynes plus tard.Alors qu'Adam Smith s'intéressait à la production de revenus, David Ricardo axe ses recherches sur la distribution des revenus entre les propriétaires fonciers qui perçoivent des rentes, les travailleurs qui reçoivent des salaires (qui sont liés au minimum nécessaire pour subsister et donc au prix du blé) et les capitalistes dont les revenus sont constitués par les profits. Au centre de la problématique ricardienne se trouve le problème de la rente foncière (pour lui, la croissance de la population et des capitaux se heurte à une offre inchangée de terre qui pousse la rente foncière vers le haut et entraîne une baisse des salaires et des profits. L'œuvre de Ricardo se situe dans le contexte de l'abolition des corn laws qui favorisent les propriétaires terriens et de la conversion de l'Angleterre au libre-échange dont Ricardo avec la loi des avantages comparatifs est l'un des grands théoriciens.À la fin de la tradition classique, John Stuart Mill (1806-1873) se distingue des économistes antérieurs de cette école sur la question de la redistribution des revenus produits par le marché. Il attribue deux rôles au marché : une capacité à répartir des ressources et une capacité à répartir les revenus. Si le marché est efficace dans l'allocation des ressources, il l'est moins dans la distribution des revenus, ce qui oblige la société à intervenir.La théorie de la valeur est un concept important dans la théorie classique. Adam Smith écrit que le prix réel de chaque chose est le labeur ou le travail et la peine de l'acquérir sous influence de sa rareté. Il soutient que, avec les rentes et les profits, les frais autres que les salaires entrent aussi dans le prix d'un produit. David Ricardo a systématisé et simplifié cet aspect de la pensée smithienne en élaborant ce qui a été appelé la « théorie de la valeur travail » qui a été plus tard reprise par Karl Marx alors que les néo-classiques lui ont substitué la théorie de l'utilité marginale. Révolution marginaliste La révolution marginaliste survient vers 1870-1871 quand Léon Walras, William Stanley Jevons et Carl Menger introduisent le concept d'utilité marginale centré sur la valeur pour le consommateur et récusent la valeur travail. Toutefois, entre les trois fondateurs du marginalisme, il est possible de relever de fortes différences.Léon Walras adopte une approche hypothético-déductive et propose un système d'équilibre général très abstrait.Stanley Jevons, tout comme Léon Walras, veut également mathématiser l'économie mais il est plus inductif, il veut partir de l'étude des faits, des réalités, en raisonnant dans un cadre qui reste utilitariste (raisonnement sur le plan du plaisir et de la peine ou des avantages et inconvénients). Cette démarche aura une forte influence sur l'économie notamment aux débuts du vingtième siècle et marque toute l'économie appliquée actuelle.Carl Menger rejette l’usage des mathématiques et considère l’utilisation d’équations simultanées « à la Walras » comme incapable de mettre en lumière les relations causales ainsi que de rendre compte de la fugacité des échanges. Il trouve qu'il y a quelque chose de collectiviste chez le fondateur de l'école de Lausanne ; ce que cherche Menger, c’est une science capable de rendre compte du comportement des agents, de saisir l’essence des phénomènes économiques. Institutionnalisation de la discipline À la fin du XIXe siècle et au début du XXe siècle, les sciences économiques se structurent comme une discipline académique avec la création de départements d'économie dans les universités, de revues académiques spécialisées et d'associations professionnelles.Par exemple aux États-Unis, le département d'économie de l'université Harvard est créé en 1897 et le département d'économie de l'université de Californie à Berkeley est créé en 1903,. L'American Economic Association est créée en 1885, le Quarterly Journal of Economics en 1886, le Journal of Political Economy en 1892 et l'American Economic Review en 1911.Au Royaume-Uni, la British Economic Association, ancêtre de la Royal Economic Society est fondée en 1890, la revue The Economic Journal en 1891 et la London School of Economics en 1895.Dans les années 1930, la science économique connaît deux grandes révolutions avec l'apparition de la macroéconomie et de l'économétrie.Avec la publication de la Théorie générale de l'emploi, de l'intérêt et de la monnaie (1936), John Maynard Keynes crée le champ de la macroéconomie.Les années 1930 sont aussi marquées par le développement de l'économétrie. Ragnar Frisch crée la société d'économétrie en 1930 et la revue Econometrica en 1933. Le développement de l'économétrie conduit à un usage de plus en plus important des statistiques dans la science économique. Les modèles économétriques peuvent aussi bien être utilisés pour calibrer un modèle économique existant que pour tester sa validité empirique.Dans les années 1940 et 1950, les sciences économiques sont marquées par le développement des théories de la croissance économique avec le modèle de Harrod-Domar et surtout le modèle de Solow (Solow 1956), le développement des fondements de la théorie des jeux avec l'ouvrage fondateur de John von Neumann et Oskar Morgenstern (von Neumann et Morgenstern 1944) et les travaux de John Nash, et l'accomplissement des recherches sur l'équilibre général en concurrence parfaite avec les travaux de Kenneth Arrow et Gérard Debreu qui montrent les conditions d'existence et d'unicité de l'équilibre général imaginé par Léon Walras.Dans les années 1960, les sciences économiques explorent de nouveaux sujets comme l'éducation, la criminalité ou encore la famille. Les travaux de Gary Becker sont emblématiques de cette tendance à utiliser la théorie économique pour analyser des sujets hors du domaine traditionnel de l'économie. En macroéconomie, les années 1960 sont marquées par les débats sur l'arbitrage inflation-chômage mis en évidence par la courbe de Phillips (Phillips 1958). La remise en cause de cette courbe avec le phénomène de stagflation conduit à formuler différentes hypothèses sur les anticipations des agents (anticipations adaptatives puis anticipations rationnelles).Dans les années 1970 se développent les modèles économiques en information imparfaite comme le modèle de George Akerlof sur les asymétries d'information dans un marché (Akerlof 1970).En macroéconomie, la fin des années 1980 et le début des années 1990 est marqué par un renouvellement des travaux sur la croissance économique autour de la notion de croissance endogène.Les années 1990 et 2000 sont caractérisées par une part de plus en plus importante de travaux empiriques dans la recherche en économie. Cette évolution est particulièrement vraie en économie du travail, en économie de l'éducation ou encore en économie du développement,. Le développement de l'économétrie appliquée dans ces années là est notamment lié au développement d'un champ de recherche autour de l'inférence causale (voir notamment le modèle causal de Neymann-Rubin) et la diffusion de protocoles de recherche comme la méthode des variables instrumentales, la méthode des doubles différences ou encore la régression sur discontinuité. Les travaux de David Card sur l'effet de l'immigration sur le marché du travail (Card 1990) ou de David Card et Alan Krueger sur l'effet du salaire minimum sur l'emploi (Card et Krueger 1994) sont représentatifs de ce champ de recherche.L'économie compte de nombreux courants et écoles de pensées. Certains courants comme le mercantilisme, les physiocrates ou l'école classique ont été importants dans l'histoire de la pensée économique mais ne sont plus vraiment représentés aujourd'hui.Parmi les écoles de pensées, certaines se définissent comme hétérodoxes. Ce sont certains membres de   institutionnalisme américain comme Hale Walton Hamilton qui ont été les premiers à revendiquer cette étiquette.Parmi les autres écoles hétérodoxes, les plus souvent citées sont l'école autrichienne, le marxisme, le post-keynésianisme, mais aussi l'économie féministe, l'économie évolutionniste, la théorie de la dépendance, l'économie structuraliste, la théorie des systèmes mondiaux, ou encore l'école de la régulation et l'économie des conventions.Les courants ou écoles de pensée qui ne se revendiquent pas de l'hétérodoxie sont qualifiés d'orthodoxes. Parmi ces courants, on compte l'école classique, l'école néoclassique, le keynésianisme, l'école de la synthèse, le monétarisme, la nouvelle économie classique ou encore la nouvelle économie keynésienne.L’école classique regroupe des économistes du XVIIIe siècle et du XIXe siècle. Ses membres les plus importants sont, en Grande-Bretagne, Adam Smith (1723-1790), David Ricardo (1772-1823), Thomas Malthus (1766-1834), John Stuart Mill (1806-1873), et en France, Étienne Bonnot de Condillac (1715-1780), Anne Robert Jacques Turgot (1727-1781), Jean-Baptiste Say (1767-1832) et Frédéric Bastiat (1801-1850).Le terme a été employé pour la première fois par Karl Marx dans Le Capital.Les auteurs postérieurs ont donné des définitions différentes de l'école classique. Par exemple, Karl Marx définit l’école classique par l’adhésion au concept de la valeur travail.Carl Menger caractérise lui aussi l’école classique par la notion de valeur travail.John Maynard Keynes définit l’école classique par l’adhésion à la « loi des débouchés » ou loi de Say dans la version popularisée par James Mill.L'école autrichienne d’économie est une école de pensée économique hétérodoxe qui prend comme point de départ l'individualisme méthodologique, rejette l’application à l’économie des méthodes employées par les sciences naturelles, s’intéresse aux relations causales entre les événements, dont l’origine est l’action des individus et développe également une conception subjective de la valeur et l'importance du marché comme révélateur des préférences individuelles et régulateur de la société.On la fait généralement débuter en 1871 avec la publication par Carl Menger de ses Principes d'économie. Ses principaux représentants sont Carl Menger, Eugen von Böhm-Bawerk, Ludwig von Mises, Friedrich Hayek, Murray Rothbard et Jesús Huerta de Soto. L'expression « école autrichienne » a été utilisée pour la première fois vers 1870 par les économistes allemands de l'école historique, comme expression de mépris envers les thèses de Carl Menger, opposées aux leurs.Les partisans de l'école autrichienne défendent généralement des idées très libérales en matière économique et plus généralement d’organisation de la société.L'école néoclassique représente le « mainstream » c'est-à-dire le corpus enseigné dans les grandes universités américaines et d'autres comme la London School of Economics.Si le premier à utiliser le mot néo-classique fut Thorstein Veblen en 1900 pour désigner l'économie marshallienne, son usage s'est imposé à travers la redécouverte de Walras par Hicks, un article de George Stigler de 1941 et surtout par l'emploi de ce terme par Samuelson dans son manuel d'économie qui était alors le plus répandu au monde.L'économie néoclassique a recours de manière systématique au mécanisme d'offre et de demande pour déterminer les quantités et les prix à l'équilibre et pour étudier comment cela affecte la répartition de la production et la redistribution des revenus. Les marginalistes refusent la théorie de la valeur-travail héritée de l'économie classique et lui substituent l'utilité marginale(l'utilité de la dernière unité consommée).Par la microéconomie, l'économie néoclassique présente les incitations et les coûts comme jouant un rôle omniprésent dans l'élaboration de la prise de décision. Par exemple, la théorie du consommateur et la demande individuelle isolent la façon dont les prix (les coûts) et le revenu touchent la quantité demandée. En macroéconomie, ceci se traduit par une rapide et durable synthèse néoclassique,.L'économie vue par les marxistes résulte des travaux de Karl Marx (notamment des trois livres constituant Das Kapital, publiés en 1867, 1885 et 1894) et de Friedrich Engels. Sur un plan général, l'économie n'est pas dans cette optique une science complètement séparée de la sociologie, de l'histoire, ou de l'anthropologie. Au contraire le matérialisme historique vise à unifier toutes les sciences sociales dans une science de la société. Par ailleurs, trois points essentiels caractérisent ainsi l'économie du point de vue marxiste : le travail salarié, l'exploitation du prolétariat et les crises liées à l'accumulation de capital.Si Marx reprend la théorie de la valeur travail de Ricardo, il reproche à cet auteur de ne pas avoir analysé comment le système capitaliste avait émergé et comment cela avait donné aux capitalistes le pouvoir et la capacité d'exploiter les travailleurs qui n'ont que leur force de travail à vendre. Les crises s'inscrivent dans le cadre des lois de l'évolution du mode de production capitaliste.Au niveau global, selon l'approche marxiste de l'économie, il y aurait des lois de l'évolution du capitalisme telles que : la propension des capitalistes à accumuler, la tendance à des révolutions technologiques constantes, la soif inextinguible des capitalistes pour la plus-value, la tendance à la concentration, la tendance du capital à devenir de plus en plus « organique » (c'est-à-dire à moins recourir au capital variable qu'est la force de travail), la tendance au déclin du taux de profit, la lutte des classes, la tendance à une polarisation sociale croissante, la tendance à ce que les salariés soient employés dans des entreprises de plus en plus grandes et enfin, l'inéluctabilité des crises dans le système capitaliste. Les crises sont dans ce cadre toujours des crises de surproduction alors que les crises précapitalistes étaient des crises de sous-production (crises frumentaires). Les crises sont vues par les marxistes comme un moyen pour le capitalisme de se renouveler.Cette économie part d'une analyse critique du système capitaliste.Selon elle, le capitalisme correspond à des rapports sociaux de domination du facteur travail par le facteur capital.Ses concepts sont : le mode de production – la théorie de la valeur (la valeur d'un bien et d'un service dépend de la quantité de travail socialement nécessaire pour la production de ce bien ou de ce service mais le salaire ne représente pas en réalité la valeur du bien ou du service) – la formation de la plus-value puis du profit – la théorie de l'exploitation – la baisse tendancielle du taux de profit.Pour eux il n'y a aucun progrès social avec les classiques malgré les progrès techniques[réf. nécessaire]. Les détenteurs de capitaux détiennent un pouvoir sur la production et donc sur les salariés. Ces économistes contestent la propriété privée lucrative des moyens de production et pensent que la maîtrise de la production devrait revenir directement aux salariés eux-mêmes ou en propriété d'état de manière transitoire selon les courants,.Pour John Maynard Keynes (1883-1946), une économie de marché ne possède pas de mécanismes qui la conduisent de façon automatique vers le plein emploi de ses ressources, d'où la possibilité d'un chômage involontaire qui rend nécessaire une intervention extérieure au marché. Keynes raisonne d'emblée sous l'angle macroéconomique d'offre globale et de demande globale. Dans son cadre macroéconomique, la production, et donc l'emploi, dépend des dépenses. Si la demande n'est pas suffisante, les entreprises ne produiront pas assez et n'emploieront pas tous les salariés (demande effective) d'où la nécessité pour le gouvernement de conduire des politiques de soutien à la demande, c'est-à-dire de soutien à la consommation et/ou à l'investissement. Keynes insiste particulièrement sur l'investissement. En effet, ce dernier est la source du multiplicateur keynésien.Au cœur de la révolution keynésienne se trouve la réfutation de la « loi dite des débouché » de Jean-Baptiste Say qui énonce que l'offre crée sa propre demande. Cette loi fonde ou plutôt exprime l'optimisme et aussi le naturalisme de l'économie classique qui veut qu'il ne puisse y avoir de crise de surproduction durable.Le keynésianisme de Keynes a eu trois successeurs. Les post-keynésiens, souvent associés à l'université de Cambridge et à Joan Robinson, mettent l'accent sur les rigidités macroéconomiques et d'ajustement. Les keynésiens de la synthèse néoclassique ont dominé la période des Trente Glorieuses et de nos jours la nouvelle économie keynésienne met davantage l'accent sur les comportements humains et les imperfections des marchés. Au niveau des théories de la croissance, ils utilisent des modèles de croissance endogènes.Parmi les économistes keynésiens, il faut noter l'Américain Howard Bowen, qui a joué un rôle fondateur dans l'émergence de la notion de responsabilité sociétale des entreprises (corporate social responsibility en anglais), à travers son ouvrage Social Responsibilities of the Businessman (1953),.Thorstein Veblen publie en 1899 « Why is Economics not an Evolutionary Science? », le document fondateur de l'école institutionnaliste.Il rejette de nombreux postulats de l'école néoclassique, comme l'hédonisme individuel justifiant la notion d'utilité marginale, ou l'existence d'un équilibre stable vers lequel l'économie converge naturellement.L'École institutionnaliste comprend des héritages de l'école historique allemande. elle se développe principalement aux États-Unis, où ses représentants sont : John Roger Commons, Arthur R. Burns, Simon Kuznets (inventeur du produit intérieur brut, le fameux PIB aujourd'hui critiqué pour ses imperfections, Robert Heilbroner, Gunnar Myrdal, John Kenneth Galbraith.Né dans les années 1980, ce courant de pensée économique est une réponse à la nouvelle économie classique. Il cherche à fournir des fondements microéconomiques solides à la macroéconomie de la synthèse néoclassique.L'un des fondateurs et des représentants les plus connus de ce courant est Joseph Stiglitz, qui acquiert sa notoriété à la suite de ses violentes critiques envers le Fonds Monétaire International (FMI) et la Banque mondiale, émises peu après son départ de la Banque mondiale en 2000, alors qu’il y est économiste en chef. En 2008, le président de la République française, Nicolas Sarkozy, lui confie une mission de réflexion sur le changement des instruments de mesure de la croissance française.Parmi les branches de l'économie, certaines, comme la microéconomie, la macroéconomie, l'économétrie ou l'économie de l'environnement (qui joue un rôle particulier dans une approche de développement durable), constituent des approches transverses de l'économie. D'autres branches de l'économie comme l'économie du travail, l'organisation industrielle, l'économie internationale ou l'économie de l'éducation, sont centrées sur une thématique ou un sujet particulier.Pour Paul Krugman et Robin Wells, « l'un des thèmes majeurs de la microéconomie est la recherche de la validité de l'intuition d'Adam Smith, à savoir que des individus cherchant à satisfaire leurs intérêts propres contribuent souvent à promouvoir les intérêts de la société dans son ensemble. » En effet, ce qui intéresse la microéconomie, c'est tout d'abord l'étude des choix des agents économiques, c’est-à-dire de la manière dont ils procèdent à des « arbitrages » entre différentes options possibles, en comparant leurs avantages et leurs inconvénients pour la poursuite de leurs objectifs ou la satisfaction de leurs intérêts, postulat utilitariste.La microéconomie examine les interactions existant sur les marchés en fonction de la rareté de l'information et la réglementation gouvernementale. On distingue le marché d'un produit ou service, par exemple celui du maïs frais, des marchés des facteurs de production, capital et travail. La théorie compare les agrégats de la quantité globale demandée par les acheteurs et la quantité fournie par les vendeurs et détermine ainsi le prix. Elle bâtit des modèles pour décrire comment le marché peut atteindre l'équilibre en matière de prix et de quantité ou comment réagir aux changements du marché au fil du temps, c'est ce qu'on appelle le mécanisme de l'offre et de la demande. Les structures de marché, telles que la concurrence parfaite, le monopole ou l'oligopole, sont analysées en fonction des conséquences sur le plan du comportement et de l'efficacité économique. L'analyse d'un marché unique se fait à partir d'hypothèses simplificatrices : rationalité des agents, équilibre partiel (c'est-à-dire qu'on suppose les autres marchés ne sont pas affectés). Un raisonnement en équilibre général permet d'analyser les conséquences sur les autres marchés, et peut permettre de comprendre les interactions et les mécanismes qui peuvent ramener à l'équilibre. Théorie microéconomique traditionnelle La théorie microéconomique standard suppose que les agents économiques, ménages ou entreprises, sont « rationnels » c’est-à-dire qu'ils sont censés disposer de capacités cognitives et d'informations suffisantes pour pouvoir, d'une part, construire des critères de choix entre différentes actions possibles et identifier les contraintes pesant sur ces choix, contraintes tant « internes » (leurs capacités technologiques s'il s'agit d'entreprises, par exemple), qu'« externes » (c’est-à-dire résultant de leur environnement économique), et, d'autre part, maximiser leur satisfaction sous contraintes. C'est le paradigme de l'Homo œconomicus qui n'implique pas a priori que les critères de choix des individus soient purement égoïstes, ces derniers pouvant parfaitement être « rationnellement » altruistes.Cette théorie doit son existence à la synthèse opérée par l'économie mathématique néoclassique des années 1940 et 1950 entre les apports du courant marginaliste du XIXe siècle et la théorie de l'équilibre général de Walras et de Pareto. John Hicks et Paul Samuelson sont considérés comme « les pères » de la microéconomie traditionnelle actuelle. Celle-ci s'organise autour de quatre volets :La théorie du consommateur, qui étudie le comportement de ménages devant effectuer des choix de consommation de biens sous contraintes budgétaires ;La théorie du producteur, qui étudie le comportement d'entreprises qui veulent maximiser leur profit sous contraintes technologiques ;La théorie de l'échange sur des marchés, ces marchés pouvant être concurrentiels ou non concurrentiels ;La théorie de l'optimum économique, qui mobilise le concept d'optimum de Pareto pour juger de l'efficacité économique collective des interactions entre agents au travers des échanges.La théorie traditionnelle s'inscrit dans la perspective de l'équilibre général walrassien et a tendance « à assimiler le fonctionnement réel de la société à celui du modèle abstrait d'équilibre général ». Marché et défaillances du marché La poursuite de l'intérêt particulier conduit souvent à l'intérêt général mais pas toujours.Dès le début du XXe siècle et les travaux de Arthur Cecil Pigou, le concept de défaillance du marché s’est imposé dans la théorie économique orthodoxe. C’est un cas dans lequel le marché échoue dans l'allocation optimale des ressources économiques et des biens et services. Si la théorie économique décrivait déjà des situations de monopole (ou d'un cartel), ce concept décrit également d’autres situations, comme celle où coexistent chômage et pénurie de main d'œuvre (logements vides et personnes sans logements, etc.), ou encore la présence de pollution.Une défaillance de marché, qui concerne l'allocation économique, est une notion différente de celle plus financière d'anomalie de marché, au sens de non efficience du marché. Cette dernière concerne plutôt une anomalie du rendement financier (et une anomalie de prix, puisque le rendement a pour dénominateur le prix"
économie;"Les facteurs de production sont les ressources mises en œuvre dans la production de biens et de services, par exemple les machines et travailleurs.L'école physiocratique se fonde sur une analyse des facteurs de production. Elle considère que la terre est la ressource naturelle fondamentale, et donc le facteur de production principal. C'est, selon elle, la seule source de la croissance économique. Le point de vue physiocrate est d'autant plus facile à défendre par ses tenants de l'époque que la France, où naît ce courant de pensée, est à l'époque très majoritairement agraire.Le travail de la terre est valorisé. Le capital l'est moins et n'est pas rangé dans la catégorie des facteurs de production ; les auteurs physiocrates reconnaissent toutefois l'importance de l'investissement en ce qu'il influe sur la productivité du sol.Les économistes de l'école classique retenaient deux facteurs de production, formalisés par Adam Smith : le capital et le travail. Smith, toutefois, fonde toute sa pensée sur le travail comme étant l'unique facteur de production, et le capital comme n'étant qu'un dérivé du travail. Le travail est d'autant plus facilement appareillé à une production qu'il est libre. Lorsque la richesse générée par le travail s'accumule, se forme alors le capital (voir Accumulation du capital).Adam Smith résume ainsi que le travail est « facteur nécessaire de la production. Il s'exerce d'abord exclusivement sur les agents naturels, et en particulier sur la terre ; puis son produit arrive à excéder les fonds nécessaires à la subsistance, il s'accumule et se crée ainsi un nouvel auxiliaire pour la production. Cet auxiliaire n'est autre chose que du travail accumulé ; c'est ce qu'on a nommé le capital ».John Maynard Keynes soutient que le travail est le seul véritable facteur de production. Selon lui, « la technique, les ressources naturelles, l'équipement et la demande effective constitu[e]nt le milieu déterminé où ce facteur opère ».Aujourd'hui, les économistes ne retiennent que ces deux facteurs de production, capital et travail. Le facteur capital se décompose en plusieurs sous-éléments :le capital physique (immobilier, matériels de production, biens durables, etc.), qui s'accroît avec l'investissement et, sans investissement, décroît au fil du temps (selon un taux de dépréciation du capital) ;la force de travail des individus : leur énergie musculaire et leur endurance sous l'effort ;le capital humain, qui correspond aux connaissances accumulées par les humains et mobilisables pour travailler (apprentissage, formation d'ingénieur, expérience, etc.) ;le capital immatériel, concept développé par l'économie du savoir, qui correspond à la valeur accumulée par une entreprise sous forme d'organisation, de savoir-faire accumulé, ou d'image de marque. L'économie des pays développés dépend de plus en plus du capital immatériel ;le capital social et le capital culturel, qui sont évoqués depuis peu comme variables explicatives de l'amélioration de la productivité ne résultant pas des autres facteurs ;le facteur « terre et sous-sol » (de plus en plus aménagé par la main de l'homme) fait partie du capital :soit comme une composante d'un facteur naturel plus large, les ressources naturelles incluant la biodiversité (la notion de capital naturel étant liée à celle de durabilité),soit comme la composante foncière du capital (propriété foncière).Pour simplifier, les quatre principaux facteurs de production apparaissent de nos jours être les suivants :le travail matériel ;le capital naturel (la terre en tant que monde matériel naturel) ;le capital physique ;le capital immatériel (savoir-faire, organisation, actifs incorporels s'ils sont comptabilisés, l'esprit entreprise, le travail immatériel, le savoir).Les experts estiment que le capital immatériel représente entre 60 et 70 % de la valeur des entreprises. Certains mettent la connotation de ce quatrième facteur dans le concept de managementL'investissement permet d'augmenter le volume des facteurs de production. La formation peut être considérée comme une forme d'investissement, puisqu'elle augmente les capacités du travailleur.Dans une économie fondée sur la rareté relative (les ressources existent, mais en quantités limitée), la combinaison optimale de ces éléments pour chaque produit ou service offert sur le marché détermine ce qu'on appelle généralement l’intensité des facteurs. On parle ainsi, concernant la quantité de capital utilisée par unité produite, d’intensité capitalistique.Dans une économie dynamique (c'est-à-dire en changement permanent), la croissance économique est assurée :soit par un accroissement des quantités de facteurs de production mobilisés (croissance extensive) ;soit par une amélioration de la combinaison productive de sorte que la même quantité de facteurs engendre davantage de produit (croissance intensive) ;soit par le progrès technique (dans le modèle de Solow), qui augmente la productivité globale des facteurs (travail et capital) ;soit par l'utilisation efficace des ressources naturelles ou leur existence en quantité élevée. Si le produit intérieur brut (PIB) national du Brésil a été multiplié par quatre du milieu des années 1960 à celui des années 1980, le rôle du facteur naturel est essentiel. L'activité industrielle et l'extraction de minéraux occupent 25 % d'actifs et correspondent au tiers du PIB durant la même période. L'énergie à base de canne à sucre fait fonctionner plus de deux millions de moyens de transport et l'énergie fossile permet de subvenir aux neufs dixièmes des besoins énergétiques vers la fin du vingtième siècle. L'environnement subsaharien de la Côte d'Ivoire permet à ce pays, outre la satisfaction de ses besoins internes, d'exporter une grande quantité de matières premières vers le reste du monde.On parle donc selon le cas de meilleure productivité du travail, ou de meilleure allocation des ressources ou des facteurs.En considérant le PIB comme une fonction du capital et du travail, la croissance résulte de trois paramètres :la quantité de capital utilisée ;la quantité de travail utilisée ;et la productivité globale des facteurs.Les nouvelles théories de la croissance et du développement économique s'efforcent de faire de la productivité un facteur endogène, que l'on explique par des variables telles que :l'effort de recherche et développement, et l'investissement dans le travail (effort de formation), lié à la notion de capital-savoir ;l'effort d'investissement collectif en infrastructures, matérielles (routières, de télécommunication) ou non (infrastructures institutionnelles, comme : système juridique, réglementaire, monétaire, éducatif, etc.).La critique de gauche de la conceptualisation des facteurs de production par la science économique pointe du doigt une confusion qui serait réalisée entre les facteurs (les agents) et les moyens (les objets) de production. En levant cette confusion, l'approche agent-objet fait apparaître que seul le travail humain est un facteur (c'est-à-dire un agent) de production car le capital non-financier n'est qu'un moyen de production, et doit par conséquent être classé dans la catégorie des biens & services. Aussi, dans une telle perspective, le capital financier n'est ni facteur ni moyen de production, il n'est que la contrepartie comptable ou juridique des biens & services de consommation/production.Capital immatériel, connaissance et performance, sous la direction d'Ahmed Bounfour, l'Harmattan, 2006.Travail (économie)Partage de la valeur ajoutéeDurabilitéPaul RomerÉconomie du savoirCapital immatérielMode de productionOptimisation (mathématiques)Terre (économie) Portail de l’économie   Portail de la production industrielle"
économie;"Le marketing mix ou mix marketing ou mix est, en marketing management opérationnel, l'ensemble des domaines opérationnels dans lesquels il faut élaborer des stratégies.Ne comptant longtemps et traditionnellement que 4 domaines de décisions, celles relatives au produit, au prix, au point-de-vente et à la publicité-promotion, le marketing mix compte aujourd'hui plus de domaines.Le Chartered Institute of Marketing, la plus importante association mondiale de professionnels du marketing, 40 000 membres, a adopté en 2009 un marketing mix à 7 variablesRetailing mix est l'équivalent du marketing mix appliqué à un point-de-vente, une enseigne de distribution (Carrefour, Leclerc, Casino, etc.) ou un centre commercial (par ex: Qwartz, etc.)Traffic mix est proposé pour application aux lieux de prestation de service en général (hôtels, parcs d'attraction, stations de sports d'hiver, etc.).Le terme « marketing mix » se diffuse après la publication en 1964 de l'article de Neil H. Borden intitulé « le concept de Marketing-mix » . Borden avait d'ailleurs déjà utilisé le mot dans son enseignement dès les premières années 1940 après que James Culiton eut décrit le « Marketing manager » comme un « mélangeur d'ingrédients ». Pour Borden, ces ingrédients incluent douze éléments : le plan produit, la tarification, la marque, les canaux de distribution, la vente personnelle, la publicité, les promotions, le packaging, la présentation, le service, la manutention, la recherche et l'analyse des faits et données.Il reviendra plus tard à E. Jerome Mc Carthy d'opérer un regroupement de ces éléments dans une présentation plus synthétique en 4 catégories, sous la dénomination des « 4P ».Le marketing mix est un mode de répartition d'analyse du marketing. L'objet du marketing étant l'analyse du marché, il avait été choisi par l'usage (de manière arbitraire) de l'analyser par cet outil mnémotechnique simple que sont les 4P. Certains préfèraient parler des 5P, ajoutant le P de Personnes, soit les consommateurs dans le marketing mix. L'analyse doit être répartie, car il n'est pas possible de réaliser raisonnablement une analyse identique dans des situations aux produits, lieux, mode de communication et prix très différents. Ces éléments portent en eux, en effet, le sens final de l'analyse du marché, c'est-à-dire a priori la recherche de l'identité des clients. La description de l'identité des clients est donc une conséquence (et non une cause) de l'analyse.La simplicité de la répartition est un point important à respecter à des fins d'intégration du marketing au sein de sciences de l'organisation (management) et plus particulièrement des sciences économiques. En effet, le marketing mix doit laisser l'opportunité d'un contrôle du marketing (son coût par exemple). Ceci n'est possible que si la répartition aboutit à l'utilisation d'indicateurs de gestion permettant de rapprocher la stratégie marketing de la stratégie d'entreprise. Il faut en effet rappeler que l'objet du marketing est d'obtenir les économies d'échelles par des ventes plus importantes ou plus généralement l'efficience qui va permettre la performance de l'entreprise.La distinction de la stratégie marketing et celle plus globale de l'entreprise peut aussi enfin être considérée comme une cause de la répartition en 4 parties du marketing mix. La stratégie d'entreprise se focalise sur la notion de processus que ne peut intégrer simplement une stratégie marketing focalisée dans les faits sur les produits, la publicité, le lieu de distribution et le prix. C'est que les synergies organisationnelles de l'activité peuvent être éloignées des objectifs du service marketing. Ainsi, le marketing mix étendu tend à intégrer des éléments analysés en amont par la stratégie globale de l'entreprise.D'après Philippe Villemus, « pour optimiser le profit de l'entreprise, la pratique du marketing consiste à construire son offre, compte tenu de la demande, du jeu des autres et des moyens dont on dispose dans un cadre politique choisi ».Faire son marketing mix, c’est établir de manière opérationnelle sa stratégie marketing. On définit ce que l’on vend, comment, où, pourquoi et toutes les variables nécessaires à la mise en vente d’un produit ou d'un service dans les meilleures conditions de marché. Les démarches préalables à la définition du marketing mix — (la stratégie marketing) — sont le diagnostic interne/externe de l'entreprise, la segmentation du marché, le positionnement de la marque et de chacun de ses produits ou services.Le diagnostic aboutit à la définition d'objectifs en termes de segmentation du marché, qui permettra à l'entreprise d'envisager certaines sources de volume de vente.Le marketing mix se fondait essentiellement selon Jerome McCarthy (1960), largement vulgarisé par Philip Kotler sur la règle dite des 4P, ces quatre politiques définissent le produit au sens large et ses implications commerciales au plan :Product : la politique de produit (choix de la gamme de produits : profondeur de gamme, largeur de gamme, etc.). Le mot produit est employé au sens générique et comprend les prestations liées aux produits (emballage, aide à l'utilisation, maintenance...) mais aussi tout le secteur des services en général qui, dans la société post-industrielle, représentent une part de plus en plus grande des offres marketing.Price : la politique de prix (ex. : écrémage, pénétration, prix d'acceptabilité, rentabilité, etc.)Place : la politique de distribution (choix du réseau et des canaux de distribution, force de vente, etc.). La distribution inclut également le commerce électronique.Promotion : la politique de communication (choix du type publicité, promotion, marketing direct, relations publiques, etc.).On aboutit au plan marketing de la période considérée lorsque les actions concernant ces quatre domaines sont :programmées pour une durée donnée (par exemple une année),chiffrées (budgets, objectifs de résultats...)avec leur détail par produit, segments de clientèle et unités de l'entreprise,et une explicitation des moyens affectés et des actions de terrain correspondantes.Bien que le modèle 4P soit l'un des plus connus de l'analyse marketing et encore un des plus enseignés aujourd'hui, il est fortement contesté depuis 1980 par celui de Bernard Booms & Mary BitnerDans le cas particulier des services et des points-de-service, ( CIC, Carrefour, Accor, Club Med, etc.) le modèle dit « des 7P » propose d'enrichir le modèle de base en ajoutant d'autres catégories comme :Process : caractérisée par l'interaction avec le client (ex : accueil, conseil, horaires d'ouverture, etc.).People : capacités de la force de vente (ex : présentation, formation, etc.).Physical evidence ou « Physical support » (support physique) : composantes matérielles du magasin (ex : vitrine, organisation des rayons, etc.), du service (ex : rapport annuel pour un expert-comptable, relevé de compte, carnet de chèque, ou carte bancaire pour une banque), ou identifiant le personnel, qui fait partie intégrante de la production pour un service (ex : uniforme ou tenue du personnel).Cible (Target Market) Controverses sur l'enrichissement du modèle des 4P Les défenseurs du modèle dit des 4P doutent de la validité de ces compléments. Pour eux, le « Process » est essentiellement un problème lié au produit. Le « People » est essentiellement lié tantôt à la production, donc au produit, ou parfois à la promotion. Le « Physical evidence » fait partie de la promotion.Pour Robert Lauterborn (1990), les 4P ont fait leur temps, les C-words prennent le dessus :Une nouvelle formulation s'impose, celle des 4C :P ? CONSOMMATEUR : « Oubliez le produit. Étudiez les désirs et besoins du consommateur. (...) On ne peut vendre que ce qu'un consommateur veut acheter. La frénésie d'achat est terminée. (...) Il faut le séduire en le considérant un par un, avec le quelque chose qu'il désire de façon particulière »P ? COÛT : « Oubliez le prix. Comprenez le coût pour le consommateur de la satisfaction de son désir ou besoin. L'enjeu si vous vendez des hamburgers n'est pas d'offrir un hamburger de plus pour quelques centimes de plus ou de moins. C'est le coût du temps nécessaire pour se rendre chez vous, de la conscience de manger de la viande, et peut-être le coût de la culpabilité de ne pas bien traiter ses enfants. La valeur n'est plus dans le fait d'offrir le plus gros hamburger pour le plus petit prix. C'est une équation complexe induisant autant de solutions valides que de catégories de clients. »P ? COMMODITÉ : « Oubliez le placement. Pensez à la commodité d'achat. Les gens n'ont plus la nécessité d'aller quelque part dans cette époque où les catalogues, les cartes de crédit, les téléphones sont présents chez eux. Et lorsqu'ils décident de se déplacer, ils ne vont pas forcément chez Kroger's. (...) Pensez plus loin que ces nets et jolis points de vente que vous avez investis pendant des années. Efforcez vous de connaitre comment chaque sous-segment du marché préfère acheter et soyez présents partout.».P ? COMMUNICATION : « Oubliez les promotions. Le mot est « communication » . Toute bonne publicité crée un dialogue. La promotion des années 1960 vient de nous, va vers l'extérieur, est manipulatrice. La communication des années 1990 vient du client, se veut coopérative. Ce contraste est en fait la différence fondamentale entre les 4P qui ont servi si bien et si longtemps et les 4C qui pourraient bien être la clé de succès, alors que nous quittons le second millénaire.».Koichi Shimizu (1972), les 4P ont fait leur temps, les C-words prennent le dessus :(C1):Corporation(C-O-S: Competitor-Organiztion-Stakeholder): Société – Le cœur de quatre Cs est la société (la compagnie et non l'organisation de profit). Parce que(organisation, concurrent, partie prenante) dans la société. Des questions à se poser au niveau stratégique... Make or Buy? Se recentrer sur son cœur d’activité SaaS ou PaaS/IaaS? Outil différentiant... Contrôle ou agilité ? Tout outsourcer ou ne rien outsourcer ? Créer une matrice personnalisée Control vs Agility (Behnia, et al., 2008).La boussole de consommateurs et de circonstances (l'environnement) est :(C6):Consommateur: – (L'aiguille de boussole au consommateur) Les facteurs rattachés aux consommateurs peuvent être expliqués par le premier caractère de quatre directions marquées sur le modèle de boussole. Ceux-ci peuvent être souvenus par les directions capitales, dorénavant le modèle de boussole de nom :N = Needs (les besoins)W = Wants (les envies)S = Security (la sécurité)E = Education (l'éducation de consommateur)[ Early adopters du Cloud Public ] Courbe d’adoption de l’innovation (Rogers, 1962). Une infrastructure (et applications) existante difficile à migrer ; Contexte social difficile (change management) ; Problématiques juridiques importantes (protection des données); Comformité ; Besoin de sécurité extrême (règles très dures) ; ou Pas d’activité saisonnière.(C7):Circumstances – (L'aiguille de boussole aux circonstances) En plus du consommateur, il y a des facteurs environnementaux externes incontrôlables différents encerclant les compagnies. Ici il peut aussi être expliqué par le premier caractère des quatre directions marquées sur le modèle de boussole :N = National et international (Politique, juridique et éthique) l'environnement,W = Weather (le Temps),S = E Social et culturel,E= Économique Ceux-ci peuvent aussi être souvenus par les directions capitales marquées sur une boussole. Spécificité des services La légitimité du modèle des 4P est aussi remise en cause au motif que ce modèle ne concernerait :que les produits et services destinés aux particuliersou plus radicalement qu'il ne pourrait valablement concerner les activités de service dans leur ensemble.Des auteurs, notamment Berry (1985), Eiglier et Langeard (1987) ou encore C. Lovelock (1996) ont proposé un nouveau modèle. Celui-ci tient compte des spécificités de la servuction (création d'un service) qui sont l'intangibilité, mais aussi l'hétérogénéité et le caractère périssable de ceux-ci ainsi que la coproduction. Cette dernière caractéristique, centrale, justifie que le client, participant à la production du service, soit particulière sensible au processus, aux évidences physiques perçues au cours de ce dernier et au comportement du personnel en contact. Ne pouvant évaluer a priori le service, de fait de ses attributs d'expérience voir de croyance, le consommateur va s'appuyer sur son expérience de service pour évaluer le service. Les modèles de servuction et le modèle 7P sont donc très complémentaire mais l'approche de Pierre Eiglier et E. Langeard permet de comprendre la dualité des services (Eiglier, 2004) et l'impossibilité de traiter séparément, dans les services les questions liées au marketing de celles liées à la production.La définition d'un positionnement au sein de la segmentation va nourrir l'image de marque de l'entreprise. Élaborée à partir de l'identité de la marque, définie comme une identité en mouvement, une nouvelle approche propose un marketing-mix élargi à 7 variables (5p2i) : politiques de produits/services, de tarification, de distribution, de communication, politique de partenariat, identité sensorielle et innovation,.Le modèle SAVE a été mis au point par Richard Ettenson, Eduardo Conrado, et Jonathan Knowles dans une étude intitulée « Rethinking the 4 P’s » publiée en janvier 2013 par Harvard Business Review. Il est présenté comme une évolution du modèle des 4P afin de couvrir les problématiques récentes du marketing B2B.SAVE est l’acronyme des termes suivants :Solution (Solution) : Présenter un produit par les besoins qu'il comble plutôt que par ses caractéristiques (fonctionnalités, avantages technologiques...).Acces (Accès) : Développer une présence multi-canal afin de considérer l'ensemble de l'expérience d'achat du client, plutôt que de se focaliser sur un seul canal ou lieu de vente.Value (Valeur) : Communiquer sur le bénéficie, la valeur apportée par le produit plutôt que d'expliquer son coût (production, concurrence).Education (Education) : Fournir des informations utiles et personnalisées aux besoins des clients à chaque point de contact plutôt que de s'appuyer sur une communication globale.Le Framework de Dawn Iacobucci  Pour elle, le marketing est une méthodologie dont l'algorithme est : 5C, STP, 7P's.Que l'on peut améliorer en PESTEL, 3C, STP, 7P'S Impact des NTIC dans la relation avec les clients Plus récemment le marketing-mix n'a pu échapper à l'impact des NTIC ( Nouvelles technologies de l'information et de la communication ). L'implication et la participation croissante du consommateur dans la relation d'échange via les techniques du Web 2.0 et plus particulièrement du Marketing 2.0 a ouvert un monde d'opportunités que le marketing-mix ne peut négliger. Opportunité d'un positionnement en termes de marque Côté entreprise, l'intérêt de la marque (marketing) s'est affirmé pour affronter des marchés :tantôt saturés, où il s'agit de se «démarquer» d'une concurrence indifférenciée et/ou de fidéliser une base client profitable grâce à « l'effet Loyauté » tantôt fortement innovants, où il faut réussir l'introduction d'un nouveau concept, qu'il s'agit de faire découvrir et agréer par un consommateur qu'il s'agit dans un premier temps de conquérir et puis de fidéliser lorsque les offreurs challengers se mettent à suivre le mouvement et font à leur tour des offres en vue de participer à la croissance du marché.Appelé aussi Retailing mix.Années 1960(en) E. Jerome McCarthy, Basic Marketing. A Managerial Approach, Richard D. Irwin, 1960.(en) Neil Borden, « The Concept of Marketing Mix », Journal of Advertising Research, June 1964, p. 2-7.Années 1980(en) Bernard Booms & Mary Bitner, Marketing Strategies and Organizational Structures for Service Firms, dans : James Donnelly and William George, Marketing of Services, American Marketing Association, 1981, p. 47-51. (Les 7P)(en) Koichi Shimizu (1989) Advertising Theory and Strategies , (Japanese) first edition, Souseisha Book Company in Tokyo.  (ISBN 4-7944-2030-7) C3034 P3980E) pp.63-102.Années 1990(en) Robert Lauterborn « New Marketing Litany; Four P's passe; C-words take over », dans : Advertising Age, October 1, 1990.(en) Steven Silverman, « An Historical Review and Modern Assessment of the Marketing Mix Concept », 7th Marketing History Conference Proceedings, Vol. VII, 1995.Philip Kotler et Bernard Dubois, « Le marketing mix du distributeur », dans : Marketing Management, 8e édition, Publi-Union, 1994, p. 542-548.(en) Christian Grönroos, « Quo Vadis, Marketing ? Toward a Relationship Marketing Paradigm » , Journal of Marketing Management, nov. 1994, p. 347-360.(en) Christian Grönroos, From Marketing Mix to Relationship Marketing: Towards a Paradigm Shift in Marketing, dans : Management Decision, Vol. 32 No. 2, 1994, p. 4-20.(en), Shostack,(en) Koichi Shimizu (1996) Symbiotic Marketing Strategies , (Japanese) First edition, Souseisha Book Company  (ISBN 4-7944-2158-3) C3034) pp.25-62.(en), Lutz, 2007.Années 2000(en)Koichi Shimizu (2003) Symbiotic Marketing Strategies , (Japanese) 4th edition, Souseisha Book Company  (ISBN 4-7944-2158-3) C3034) pp.25-62.(en), Lutz, 2007.(en) Marketing and the 7Ps. A brief summary of marketing and how it works, Chartered Institute Marketing, 2009Philippe Villemus, Le plan marketing à l'usage des entreprises, Eyrolles, 2008.[The 7Ps of marketing dans www.cim.co.uk/marketingresources, 2009 http://www.cim.co.uk/files/7ps.pdf](en) Dominici, 2009(en) Chai Lee Goi, « A Review of Marketing Mix : 4Ps or More ? » dans : International Journal of Marketing Studies, May 2009, pp. 2-15.2010Isabelle Calmé et autres, « Le marketing mix du point de vente », dans : Introduction à la gestion, 3e édition, Dunod, 2010, p. 128.2011(en) Brian Solis(2011) Engage!: The Complete Guide for Brands and Businesses to Build, Cultivate, and Measure Success in the New Web , John Wiley & Sons, Inc. pp. 201–202.Nathalie Van Laethem, « Comment le mix-marketing est passé de 4P à 10P », Le Blog de la Stratégie marketing, 20 mars 2011,(en) Walter van Waterschoot et Thomas Foscht « The marketing mix — a helicopter view », dans : Michael Baker and Michael Saren, Marketing Theory, Sage, 2011, p. 185-205.2012Véronique Méot, « Des 4P aux 6S… les mutations du marketing mix », dans : Marketing, no 163, 28/11/2012.Philip Kotler, Kevin Keller et Delphine Manceau, « Le marketing mix revisité », dans : Marketing Management, 14e édition, Pearson, 2012, p. 31.(en) Brian Monger, Time to Revisit the Marketing Mix?, Maanz International, 2012.2013(en) Richard Ettenson, Eduardo Conrado, and Jonathan Knowles, « Rethinking the 4 P's », dans : Harvard Business Review, January-February 20132014(en) Koichi Shimizu (2014) Advertising Theory and Strategies , (Japanese) 18th edition, Souseisha Book Company  (ISBN 4-7944-2132-X) C3034) pp.63-102.Gilles Bressy et Christian Konkuyt, Management et économie des entreprises, 11ed, chapitre 15, Éditions Sirey, 2014.Pamela Alison Meager, « Summary: The Concept of the Marketing Mix by Neil H. Borden », March 26, 2014.(en) Dawn Iacobucci, Marketing Management, Cengage Learning, 2014.2016(en)Koichi Shimizu (2016) Symbiotic Marketing Strategies , (Japanese) 5th edition, Souseisha Book Company  (ISBN 4-7944-2482-5) C3034) pp.25-62.(en) 7Cs Compass model(1979)in Japan(fr) L'arbre du marketing mix du 4P au 10PEtudes critiques du marketing Portail du management"
économie;"En économie, le secteur primaire est le premier secteur défini dans la loi des trois secteurs. Il est parmi les trois secteurs économiques définis dans la comptabilité nationale et regroupe les activités liées à l’exploitation de ressources naturelles : agriculture, sylviculture, pêche et activités minières. Le secteur primaire rassemble l'ensemble des activités qui produisent des matières premières non transformées.Le secteur primaire comprend l'agriculture, la pêche, l'exploitation forestière et l'exploitation minière. On désigne parfois les trois dernières par le terme « autres industries primaires ». Les industries primaires sont liées à l'extraction des ressources de la terre.Selon Fortune, le secteur de l’extraction représenterait 27 % de l'économie mondiale comprenant notamment les activités relatives à l’énergie ou les minières. Les vingt plus gros négociants du secteur ont engrangé $191 milliards de profit entre 2003 et 2012.C'est un processus par lequel les gens aménagent leurs écosystèmes pour satisfaire les besoins de leurs sociétés. Elle désigne l’ensemble des savoir-faire et activités ayant pour objet la culture des terres, et, plus généralement, l’ensemble des travaux sur le milieu naturel (pas seulement terrestre) permettant de cultiver et prélever des êtres vivants (végétaux, animaux, voire champignons ou microbes) utiles à l’être humain. En France En 1700, il fallait environ trois heures pour produire un kilogramme de blé, d'où la malnutrition et les famines. À cette période, 80 % de la population active travaillait dans l'agriculture; en 1880, il fallait encore un peu plus d'une heure ; en 1950, 30 minutes. Aujourd'hui, environ une minute. Cela explique l'évolution de la part de l'agriculture : en 1995, l'agriculture représentait en France 6 % de part de la population active ayant un emploi, contre 40 % en 1913[réf. souhaitée]. En 2008, l'agriculture en France pesait 3,5 % du PIB (2008), soit 66,8 milliards d'euros. En 2012, elle ne serait plus que de 2 % du PIB français. En Belgique En 1846, les cultivateurs représentent encore 52 % de la population économiquement active :en 1880, 22 % ;en 1913, 16 % ;actuellement 2 %.En 1846, intervient encore pour plus de 50 % dans le PNB :en 1880, 29 % ;en 1913, 15 % ;actuellement, 0,7 %. En Europe L'emploi dans le secteur agricole est en forte régression pour l'amont de la filière (agriculture) depuis plus d'un siècle, et dans l'UE27 il a encore diminué au XXIe siècle sous l'effet de l'industrialisation et de l'augmentation de la productivité ; selon Eurostat. Ceci correspond à la perte de 3,7 millions d’emplois à temps plein en 10 ans. L'emploi a ainsi baissé de 17 % dans l’UE152 et de 31 % dans les 12 États-membres (NEM122) ayant rejoint l’UE en 2004 et en 2007. En 2009, le secteur de l’agriculture employait dans l’UE27 l’équivalent de 11,2 millions de personnes travaillant à temps plein, dont 5,4 millions dans l’UE15 et 5,8 millions dans les NEM12. Dans le même temps (2000 ? 2009), le revenu réel moyen par actif a augmenté de 5 % (il a même doublé en Lettonie, Estonie et Pologne) de 2000 à 2009. Dans le monde En 2011, la production agricole mondiale était estimée à 4 949 milliards, soit 6,2 % de l'économie mondiale.Dans le secteur primaire, la section des mines est définie comme l'exploitation de différents roches ou minéraux.Secteur économiqueSecteur secondaireSecteur tertiaireSecteur quaternaire Portail de l’économie"
économie;"En économie, le secteur secondaire ou secteur industriel est le second secteur défini dans la loi des trois secteurs. Il est parmi les trois secteurs économiques définis dans la comptabilité nationale et regroupe les activités liées à la transformation des matières premières issues du secteur primaire (industrie manufacturière, construction)Ce secteur, même s’il représente une part relativement modeste du PIB des pays développés (par exemple 13,2 % aux États-Unis en 2006, 20,6 % en France en 2006 et 26,3 % en Suisse en 2005), est considéré comme stratégique ; il fournit des emplois d’ingénieur et fournit du travail de recherche et développement à des entreprises du secteur tertiaire.Selon la CIA, le secteur industriel représentait 30,7 % de l'économie mondiale en 2012. Mais, selon Fortune, le secteur industriel représenterait 13,2 % de l'économie mondiale en 2012 si l'on intègre les activités d'extraction au secteur primaire.Secteur économiqueIndustrieSociété industrielleSecteur secondaire en FranceLe secteur tertiaireSecteur quaternaire Portail de la production industrielle   Portail de l’économie"
économie;"Le secteur tertiaire produit des services, il fait partie du domaine de l'économie. C'est le troisième secteur défini dans la loi des trois secteurs. Il est parmi les trois secteurs économiques définis dans la comptabilité nationale et est de fait défini par complémentarité avec les activités agricoles et industrielles (secteurs primaire et secondaire respectivement).Le secteur tertiaire est composé du :tertiaire principalement marchand (commerce, transports, activités financières, services rendus aux entreprises, services rendus aux particuliers, hébergement-restauration, immobilier, information-communication) ;tertiaire principalement non marchand (administration publique, enseignement, santé humaine, action sociale).Dans les pays développés, c’est de loin le secteur le plus important en nombre d'actifs occupés. En 2012, le secteur tertiaire représentait près de 60 % de l'économie mondiale.La « tertiarisation » de l’économie pose des problèmes statistiques, conceptuels et méthodologiques : les notions de volume, de qualité et de productivité du travail sont probablement à revoir dans le « tertiaire moderne ». L'objet des études futures sera aussi d'examiner si les trois critères d'homogénéité (une part croissante de l'emploi, une relative insensibilité aux crises économiques, et surtout un progrès technique faible), sont aujourd'hui respectés dans un ensemble qui comprend 79 %[réf. nécessaire] de la population active française.Le secteur tertiaire de l'économie, généralement appelé secteur des services, est le troisième des trois secteurs économiques de la théorie des trois secteurs. Les autres sont le secteur secondaire (à peu près identique à celui de la fabrication) et le secteur primaire (matières premières).Le secteur des services consiste en la production de services au lieu de produits finis (en) . Les services (également appelés «biens immatériels») comprennent les soins, les conseils, l'accès, l'expérience et le travail affectif. La production d'informations a longtemps été considérée comme un service, mais certains économistes l'attribuent désormais à un quatrième secteur, le secteur quaternaire .Le secteur tertiaire de l'industrie implique la fourniture de services à d'autres entreprises ainsi qu'aux consommateurs finaux. Les services peuvent impliquer le transport, la distribution (en) et la vente de marchandises du producteur au consommateur, comme cela peut se produire dans la vente en gros et au détail, la lutte contre les ravageurs ou le divertissement. Les produits peuvent être transformés au cours du processus de fourniture du service, comme cela se produit dans l'industrie de la restauration. Cependant, l'accent est mis sur les gens en interagissant avec les gens et servant le client plutôt que des transformations les biens physiques.Il est parfois difficile de définir si une entreprise donnée fait partie intégrante du secteur secondaire ou tertiaire. Et ce ne sont pas seulement les entreprises qui ont été classées comme faisant partie de ce secteur dans certains régimes; le gouvernement et ses services tels que la police ou l'armée, et les organisations à but non lucratif telles que les organismes de bienfaisance et les associations de recherche peuvent également être considérés comme faisant partie de ce secteur.Afin de classer une entreprise en tant que service, on peut utiliser des systèmes de classification tels que la norme de classification industrielle standard internationale des Nations Unies, le système de codes de la classification industrielle standard (SIC) des États-Unis et son nouveau système de remplacement, le système de classification des industries de l'Amérique du Nord. (SCIAN), la nomenclature statistique des activités économiques dans la Communauté européenne (NACE) dans l'UE et des systèmes similaires ailleurs. Ces systèmes de classification gouvernementaux ont un premier niveau de hiérarchie qui indique si les biens économiques sont tangibles ou intangibles.Aux fins de la finance et des études de marché, des systèmes de classification basés sur le marché tels que le Global Industry Classification Standard et l'Industry Classification Benchmark (en) sont utilisés pour classer les entreprises qui participent au secteur des services. Contrairement aux systèmes de classification gouvernementaux, le premier niveau des systèmes de classification basés sur le marché divise l'économie en marchés ou industries fonctionnellement liés. Le deuxième ou le troisième niveau de ces hiérarchies indique alors si des biens ou des services sont produits.Les groupes sociaux non productifs les plus anciens sont les propriétaires fonciers, marchands, militaires et le clergés, etc. Ils établissent leur assise sur le foncier, moyen fondamental de production, sur la régulation, le contrôle et la commercialisation de ses produits, les derniers sur le spirituel, « l'établissement d'une relation au cosmos à travers les religionse »:« Les activités de services sont nées à partir du moment où les activités de survie de l'espèce humaine ont été assurées de façon suffisante pour permettre de dégager un surplus, même temporaire, susceptible d'être attribué à d'autres fonctions et, conjointement, à d'autres individus ou groupes humains, qui ont ainsi acquis la possibilité de s'abstraire, partiellement ou en totalité, des contraintes de la production. En fait, il y a une liaison directe entre le développement d'activités non directement productives et l'organisation d'une société permettant l'appropriation de ces activités par certaines classes sociales. »L'assise du pouvoir de ces classes sociales est toujours justifiée par le « service rendu », et par l'extraction d'une fonction confiée à un corps de « spécialistes », détenteurs d'une compétence qui est de fait déniée aux autres groupes sociaux. Les activités non directement productives se font dans une hiérarchisation croissante.Au cours des 100 dernières années, il y a eu un glissement substantiel des secteurs primaire et secondaire vers le secteur tertiaire dans les pays industrialisés. Ce changement s'appelle la tertiarisation. Le secteur tertiaire est désormais le plus grand secteur de l'économie du monde occidental, et c'est aussi le secteur qui connaît la croissance la plus rapide. En examinant la croissance du secteur des services au début des années 90, le mondialiste Ken'ichi Ohmae a noté le secteur tertiaire représenterait « 70 % de la force de travail aux États-Unis, 60 % au Japon et 50 % à Taïwan »:« In the United States 70 percent of the workforce works in the service sector; in Japan, 60 percent, and in Taiwan, 50 percent. These are not necessarily busboys and live-in maids. Many of them are in the professional category. They are earning as much as manufacturing workers, and often more. »Les économies ont tendance à suivre une progression de développement qui les fait passer d'une forte dépendance à l'agriculture et à l'exploitation minière au développement de l'industrie manufacturière (par exemple, automobiles, textiles, construction navale, acier) et finalement à une structure davantage axée sur les services. La première économie à suivre cette voie dans le monde moderne a été le Royaume-Uni. La vitesse à laquelle d'autres économies ont fait la transition vers des économies de services (ou « post-industrielles ») a augmenté avec le temps.Historiquement, le secteur manufacturier avait tendance à être plus ouvert au commerce international et à la concurrence que les services. Cependant, avec une réduction spectaculaire des coûts et des améliorations de la vitesse et de la fiabilité dans le transport des personnes et la communication de l'information, le secteur des services comprend désormais une partie de la concurrence internationale la plus intense, malgré un protectionnisme résiduel.Vous trouverez ci-dessous une liste de pays par production de services aux taux de change du marché en 2016:Les prestataires de services sont confrontés à des obstacles dans la vente de services auxquels les vendeurs de biens sont rarement confrontés. Les services sont intangibles, ce qui rend difficile pour les clients potentiels de comprendre ce qu'ils recevront et quelle valeur cela représentera pour eux. En effet, certains, comme les consultants et les prestataires de services d'investissement, n'offrent aucune garantie de la valeur pour le prix payé.Étant donné que la qualité de la plupart des services dépend en grande partie de la qualité des personnes qui fournissent les services, les « coûts de personnel » représentent généralement une fraction élevée des coûts des services. Alors qu'un fabricant peut utiliser la technologie, la simplification et d'autres techniques pour réduire le coût des produits vendus, le fournisseur de services est souvent confronté à un schéma constant d'augmentation des coûts.La différenciation des produits est souvent difficile. Par exemple, comment peut-on choisir un conseiller en placement plutôt qu'un autre, étant donné qu'ils sont souvent perçus comme fournissant des services identiques ? La facturation d'une prime pour les services n'est généralement une option que pour les entreprises les plus établies, qui facturent des frais supplémentaires en fonction de la reconnaissance de la marque.Des exemples d'industries tertiaires peuvent inclure:TélécommunicationHôtellerie TourismeMédias de masseSoins de santé/hôpitauxSanté publiquePharmacieTechnologie de l'informationGestion des déchetsConsultantJeu d'argentCommerce de détailBien de grande consommation (FMCG)FranchisageImmobilierÉducationServices financiersBanqueAssuranceInvestment management (en)Services professionnels (en)Services juridiques (en)Conseil en managementTransportÉducationAprès s'être développé jusqu'en 1960 selon un rythme annuel moyen de 1 % en France, l'emploi des branches tertiaires progresse très vivement de 1960 à 1980 (2 % par an), puis encore assez fortement de 1980 à 2000 (+1,7 %). Il ralentit ensuite entre 2000 et 2011 (+0,9 %), avec une quasi-stagnation de 2008 à 2011, voire une légère baisse dans certains services traditionnels aux ménages ou les télécommunications. Ceci est le résultat de deux tendances : une accélération de la croissance de la demande intérieure tertiaire (+ 4,3 % par an en volume entre 1959 et 2012) et des gains de productivité du travail plus faibles que dans le reste de l'économie (+ 2,5 % par an dans les services marchands contre + 4,5 % dans l’industrie). Face à une demande croissante, un secteur dont la productivité progresse relativement plus lentement ne peut que se développer en terme d'emploi. Depuis le début du siècle dernier, le progrès technique a toujours été plus faible dans le tertiaire que dans les autres secteurs. Expliquer « l'explosion » récente de l'emploi tertiaire revient donc à analyser les raisons de l'accélération de la demande tertiaireLa balance commerciale des services est excédentaire en France. Finance On parle dans ce cas là, des garages de voitures ou encore des magasins de ventes qui comprend en particulier le secteur bancaire et celui de l'assurance au point d'être parfois désigné par « Secteur Banque Assurance ». Information  Sécurité Cette branche du secteur tertiaire comprend notamment les activités de police, de milice et d'armée assurant la sécurité des biens et des personnes. Justice Aux États-Unis, 2 % du PIB est obtenu en justice. Bénévolat Selon une étude de l’Insee parue en 2004, le bénévolat représentait aux alentours de 1 point de PIB.Le tertiaire « supérieur » (ou mixte) regroupe les « métiers du savoir » qui fournissent aux entreprises et aux particuliers des prestations intellectuelles complexes. Essentiels au fonctionnement de l’économie et au développement stratégique des entreprises, élément clef du rayonnement et de l’attractivité du territoire, ces métiers constituent également par eux-mêmes un secteur économique majeur, en fort développement. Recherche et éducation Le secteur tertiaire, privé et public, via ses infrastructures (bâtiments tels que bureaux, hôtels, commerces, d'enseignement et les bâtiments administratifs, réseaux internet, serveurs..) est devenu un grand consommateur d'énergie et de foncier ; En France, à la suite de la loi Grenelle 2 de 2010, un décret annoncé pour fin 2012 mais un temps repoussé par le Conseil d’État avant d'être publié en mai 2017 et entrant en vigueur le 11 mai 2017, afin de « favoriser l'efficacité et la sobriété énergétiques », impose aux « collectivités territoriales services de l'Etat, propriétaires et occupants de bâtiments à usage tertiaire privé, professionnels du bâtiment, maîtres d'ouvrage, maîtres d'œuvre, bureaux d'études thermiques, sociétés d'exploitation, gestionnaires immobiliers, fournisseurs d'énergies » une « obligation d'amélioration de la performance énergétique dans les bâtiments à usage tertiaire », sur la base d'une « étude énergétique portant sur tous les postes de consommations du bâtiment », avec un niveau d'économie d'énergie à atteindre avant 2020 et « un ou plusieurs scénarios permettant de diminuer, d'ici 2030 » la consommation d'énergie. Un observatoire doit recueillir les données permettant d'évaluer les résultats et de mettre à jour les données. L'étude énergétique est accompagnée de propositions de travaux d'économie d'énergie et de « recommandations hiérarchisées selon leur temps de retour sur investissement » avec présentation des « interactions potentielles entre ces travaux ». Ce décret permet de mutualiser l'obligation sur l'ensemble d'un patrimoine, et prévoit le cas d'un changement de propriétaire ou de preneur (un dossier dédié sera annexé au contrat de vente ou de bail). Il demande aussi une sensibilisation des occupants au thème des économies d'énergie. La « non-atteinte » des objectifs malgré les actions et travaux entrepris devra pouvoir être justifiée auprès des services de l’État. Un propriétaire d'un ensemble de bâtiments ou de parties de bâtiments concernés peut remplir globalement ses obligations sur l'ensemble de son patrimoine.Michel Braibant, De la désindustrialisation à la tertiairisation, vers un mélange des genres, Paris, Société des écrivains, 2 mai 2015, 188 p. (ISBN 9782342034738).Secteur économiqueBranche d'activitéSecteur quaternaireTertiarisation du travailSecteur bénévoleSociété post-industrielleEffet de structure de fret Portail de l’économie"
économie;"Un secteur économique, secteur d'activité ou secteur d'activité économique est un ensemble d’activité économique, ayant des traits similaires. C'est également une subdivision macroéconomique de l’économie, regroupant l’activité des entreprises qui appartiennent à une même catégorie. Traditionnellement la répartition de l’ensemble de l’activité économique est répartie en trois grands secteurs économiques (primaire, secondaire, tertiaire). Cette classification des différentes activités à vu le jour la première fois en 1947 et c'est le statisticien et économiste Colin Clark qui l'a abordé.L'activité d'un secteur d'activité économique n'est pas tout à fait homogène et comprend des productions ou services secondaires qui relèveraient d'autres items de la nomenclature que celui du secteur considéré, au contraire d'une branche d'activité qui regroupe des unités de production homogènes. Par ailleurs, le terme secteur professionnel ou industriel regroupe lui aussi deux définitions :celle de multiples domaines d’activité économique, et pour laquelle chaque secteur professionnel regroupe des familles de métiers assez proches pour être considérées comme un appareil de production unique (par exemple, le « secteur du bâtiment, de la pêche, du textile et de la confection, de la banque et des assurances),celle de branches industrielles d'activité (code APE), et pour laquelle chaque secteur professionnel regroupe les entreprises ou les administrations qui sont assujetties à une même règlementation sociale, fiscale et professionnelle (par exemple de la sidérurgie, du bâtiment et des Travaux publics, du Commerce et de la grande distribution, de la Fonction publique territoriale). Leur regroupement n'est pas non plus arbitraire.Allan Fisher, Colin Clark et Jean Fourastié ont défini trois secteurs économiques principaux, selon la nature de l'industrie :le secteur primaire concerne la collecte et l'exploitation des ressources naturelles (matériaux, énergie, et certains aliments) ;le secteur secondaire implique les industries de transformation des matières premières ;le secteur tertiaire regroupe les industries du service (essentiellement immatériel : conseil, assurance, intermédiation, formation, études et recherche, administration, services à la personne, sécurité, nettoyage, etc.).Cette classification n'est pas rigide, l'agriculture par exemple ayant été à l'origine classée comme du secteur secondaire (le cultivateur transforme des graines en produits consommables, par exemple), par opposition à la chasse et la simple cueillette.Le secteur primaire correspond aux activités liées à l'extraction des ressources naturelles. Il comprend l'agriculture, la pêche, l'exploitation forestière et l'exploitation minière. On désigne parfois les trois dernières industries par « autres industries primaires ».Le secteur secondaire correspond aux activités liées à la transformation des matières premières, qui sont issues du secteur primaire. Il comprend des activités aussi variées que l’industrie du bois, l’aéronautique et l’électronique, le raffinage du pétrole, la production industrielle, la construction...Le secteur tertiaire regroupe toutes les activités économiques qui ne font pas partie des deux autres, essentiellement des services. Par exemple, le conseil, l’assurance, l'enseignement, la grande distribution, le tourisme, la restauration et les agences immobilières font partie du secteur tertiaire.Le décalage progressif des activités vers le secteur tertiaire (théorie du déversement développée par Alfred Sauvy, théorie des vagues de développement d'Alvin Toffler) a accru le nombre de « travailleurs intellectuels » selon la définition de Peter Drucker. Mais cet enrichissement des tâches ne s'est pas pour l'instant produit de façon massive, contrairement à ce que certains avaient cru vers les années 1960. Certains emplois de vigile, employé de guichet, gardien d'immeuble ou caissière de supermarché appartiennent bien au tertiaire, sans nécessairement représenter un gain en qualité de vie.Cela maintient certains débouchés pour les personnes peu qualifiées, encore que l'objectif éducatif dans de nombreux pays, y compris des pays émergents, soit d'accroître les qualifications et les capacités créatives. Cet objectif est considéré crucial pour faire face aux évolutions concurrentielles dans le cadre de l'économie à la fois mondialisée et tournée de plus en plus vers la connaissance (économie du savoir). Cette orientation vers le tertiaire et la technologie fait que c'est vers des bassins de main d'œuvre peu qualifiée et peu rémunérée et encore peu touché par le tertiaire que se délocalisent certains emplois (mais, il est vrai, pas l'essentiel de la valeur ajoutée).Il importe de distinguer secteur global d'une entreprise et répartition des activités à l'intérieur de celle-ci. Une entreprise du secteur secondaire (fabrication de lingots d'acier, par exemple) doit bien par la force des choses posséder des services administratifs, qui font pour leur part partie du tertiaire. Des exemples classiques des écoles de commerce sont les anciennes marques Téléavia et Caravanair, difficiles à imputer à un des secteurs plutôt qu'à l'autre.On parle parfois de secteur quaternaire qui regrouperait les industries hi-tech, (technologies informatiques, aérospatiale (lancement de satellites), bioindustrie, etc.) et les services très sophistiqués (recherche et éducation de pointe, conseil stratégique, ingénierie financière, médecine de pointe, etc.) généralement pour les pays les plus industrialisés (États-Unis, Union européenne, Japon, etc.).On peut distinguer :d'une part, les entreprises commercialisant des services immatériels (70 % du PIB) de celles commercialisant des produits matériels (30 %) ;et, d'autre part, celles commercialisant ces services ou ces produits auprès d'autres entreprises de celles les commercialisant directement ou indirectement auprès du grand public.Ceci conduit à distinguer quatre grandes catégories de secteur d'activité économique :Celle des entreprises de service grand public : banque Assurance, croisières, divertissement, transport aérien, transport ferroviaire, etc.Celle des entreprises de produits grand public : agroalimentaire, compagnie pétrolières ; cosmétique, constructeurs automobiles, électronique grand public, luxe, pharmacie, etc.)Celle des entreprises de services industriels : courrier et livraison ; publicité ; technologie et services informatiques, transports maritimes, etc.Celle des entreprises de produit industriels : aéronautique, aérospatiale et défense ; bâtiment, construction et travaux publics ; éoliennes ;  chimie ; matériel informatique ; santé ; etc.Les Nomenclatures des secteurs économiquesLa nomenclature d'activités française (NAF)La Classification générale des activités économiques (NOGA), en SuisseSecteur d'utilisation de l'énergieSecteurs institutionnels, utilisés dans les comptabilités nationales.Division du travailActivité économiqueAnalyse sectorielleBranche d'activitéClassement mondial des entreprises leader par secteurClassification type des industriesGlobal Industry Classification StandardNorth American Industry Classification SystemSystème de classification des industries de l'Amérique du NordActifs occupés selon le sexe et le secteur d'activité, en France, InseeNomemclature des secteurs d'activité - NAF rév. 2, 2008, en France, InseeMetadata Nomenclature statistique des activités économiques dans la Communauté européenne, Rév. 2 (2008)Détail Code : G Description :  Commerce Portail de l’économie"
économie;"En économie, un service est une prestation qui consiste en « la mise à disposition d'une capacité technique ou intellectuelle » ou en « la fourniture d'un travail directement utile pour l'usager, sans transformation de matière ». Les services correspondent au secteur tertiaire.Fournir un service correspond à une production économique de nature particulière puisqu'elle ne consiste pas en la fourniture d'un bien tangible à un client. De plus, les services — étant consommés dans le même temps nécessaire pour les produire — sont considérés comme n'étant pas « stockables ».Christopher Lovelock distingue quatre grandes catégories de service (ou de prestation ou de servuction). Il les différencie d'une part par la nature de la prestation : l'action concrète, tangible celle d'un kinésithérapeute ou d'un coiffeur qui fait physiquement quelque chose ou bien l'action psychologique, intellectuelle, immatérielle, d'un professeur, d'un psychothérapeute ou d'un expert-comptable ; et d'autre part, par l'objet du service, ce sur quoi il porte : des personnes (leur corps ou leurs esprit) ou des choses (tangibles ou intangibles comme les chiffres). Cela donne une matrice à quatre composantes :les services concrets rendus aux personnes : les coiffeurs, les transports de personnes, les soins médicaux et chirurgicaux, etc. ;les services concrets portant sur des choses : le transport de fret, le nettoyage à sec, la réparation automobile, le dépannage domestique ou professionnel (ascenseur, etc.) ;les services abstraits s'adressant à l'intelligence ou au sens : enseignement, divertissement ;les services portant sur des entités intangibles, numériques : compte bancaire, crédit, assurances.La production de services est devenue l'activité de production principale des économies développées. Elle est caractérisée par une gestion particulière de la production, celle-ci étant immatérielle, donc non stockable et réunissant simultanément consommation et production. Cela implique généralement une participation du client à la production. Les particularités de l'analyse de la valeur, le caractère précaire de l'innovation dans les services, le découpage classique de la production en front office (en interaction avec le client) et back office (en l'absence du client) demandent des méthodes et des outils différents de la production industrielle.La part des services (représentée grosso modo — pour les raisons évoquées ci-dessus — par le secteur tertiaire) augmente tant en chiffre d'affaires qu'en effectif employé dans la production, la consommation finale et la consommation intermédiaire (voir théorie du déversement énoncée par Alfred Sauvy).On distingue les services marchands, qui sont facilement procurables sur le marché, et les services non marchands, dont l'obtention s'opère dans des cadres et selon des règles plus spécifiques. Services non marchands Les prestations de ces services ne sont pas fournies contre rémunération (régime de la gratuité totale ou du paiement d'une contribution symbolique, ou par intervention d'un tiers payant) :justice ;maintien de l'ordre public (forces de l'ordre) ;défense nationale ;éducation publique et enseignement supérieur ;santé publique ;services sociaux et organismes caritatifs. Services marchands La prestation de ces services est obtenue moyennant un prix, généralement fixé librement par le marché :Écoles et organismes de formation privés ;services financiers, de banque ou d'assurances ;conseil ou services informatiques ;télécommunications ;transports et logistique ;prestations d'études ou de recherches appliquées.Un service public est « une activité d'intérêt général, assurée sous le contrôle de la Puissance publique par un organisme public ou privé bénéficiant de prérogatives lui permettant d'en assurer les obligations (notamment en matière de continuité et d'égalité) et relevant de ce fait en tout ou partie d'un régime de droit administratif (mission dite de service public) ».Toute autre activité de service – qui ne relève pas de l'exception définie par la catégorie précédente – doit être considérée comme une activité de nature privée. En France s'applique le principe constitutionnel de la liberté du commerce et de l'industrie. Services publics Les services publics sont les activités jugées utiles par et pour la collectivité et qui sont assurées dans un cadre particulier. Ce qui signifie qu'elles peuvent être exercées même lorsque les critères de simple rentabilité financière devraient conduire à leur abandon. Ils comprennent :les services publics administratifs (SPA) comme la perception des impôts ;les services publics sociaux comme le service des allocations familiales ;les publics industriels ou commerciaux (SPIC) comme la distribution du gaz ;les services publics professionnels comme l'Ordre des Médecins (qui assure la réglementation et la discipline de la profession médicale). Services privés Dans les pays développés les plus tertiarisés (on parle parfois d'« économie post-industrielle »), comme en France, les services représentent jusqu'à plus de 75 % de la production nationale (mesurée par le PIB)[réf. nécessaire] et sont devenus leur principal moteur de croissance économique.Cette évolution peut toutefois être légèrement relativisée par le fait que les entreprises industrielles externalisent une partie de leur processus de production en faisant appel à des prestataires qui sont classés dans les entreprises de services mais qui participent à la production industrielle.Les services génèrent en outre une grande partie du capital immatériel des entreprises.L'économie des services comporte des enjeux considérables de développement durable.Selon Jean Gadrey, « pour construire une économie écologique des services, il faut d'abord s'intéresser aux bilans écologiques complets de ces activités. Il apparaît alors que l'immatérialité parfois supposée des services est un mythe ». Selon le même auteur, l'activité de services comporte des externalités environnementales importantes qui auraient besoin d'être internalisées.Un exemple d'enjeu dans le secteur des services est constitué par la dématérialisation, qui se fait quelquefois dans un objectif de développement durable. La dématérialisation permettrait ainsi d'économiser du papier, voire de passer au « zéro papier ». Dans les projets de dématérialisation, on utilise massivement des services. Mais établir le bilan global d'une dématérialisation n'est pas aisé ! La dématérialisation agit sur les flux de gestion entre partenaires, pas sur la qualité environnementale des biens vendus.Les enjeux environnementaux concernent la pression environnementale et la contribution des services à l'émission de gaz à effet de serre.Une étude de l'IFEN montre que la pression environnementale directe et indirecte des services n'est pas négligeable.Jean-Marc Jancovici souligne également la contribution des services dans les émissions de gaz à effet de serre à travers ce qu'il appelle les émissions intermédiaires.Certains services sont émetteurs directs (transports, logistique), d'autres sont émetteurs indirects de gaz à effet de serre.Les enjeux sociaux sont également importants :emploi ;formation ;santé et sécurité au travail (stress…) ;salaire ;lutte contre la discrimination.Une partie des services est constituée par les sociétés de conseil et les sociétés informatiques qui fournissent des prestations de service aux entreprises de l'industrie (ou d'autres entreprises de services).Dans les faits, une très faible partie de l'activité des sociétés de conseil s'est orientée vers du conseil en développement durable. Pour les entreprises qui font du conseil en développement durable, encore faut-il que ce concept ne soit pas déformé (voir limites et dérives du concept de développement durable) et que l'entreprise cliente considère le développement durable comme stratégique. La recherche du profit à court terme éclipse trop souvent les questions de fond.Pour les SSII, la durabilité est souvent vue exclusivement sous l'angle du recyclage des équipements informatiques, pas sous l'angle de la gestion. Certes, la durée de vie des matériels et des matériels et logiciels est très courte en informatique. Par ailleurs, il existe de gros problèmes de compatibilité et d'interopérabilité entre systèmes.Les enjeux du développement durable sont porteurs de nouveaux modèles économiques, or les sociétés de conseil et les sociétés informatiques n'ont pas réellement revu leur modèle d'entreprise en fonction de ces enjeux. Selon Jean-Louis Lequeux, alors que le business model « classique » se vit à deux (l'acheteur, le vendeur), les modèles durables et éthiques se conjuguent à trois. Dans un cas comme l'autre, les deux parties reconnaissent à la fois l'existence, ou plutôt le droit à l'existence et le droit au respect, d'une troisième partie :notre planète Terre pour le « durable » ;les hommes, le tissu social et les économies locales pour « l'éthique ».Le business model doit donc tenir compte des attentes des parties prenantes.Gilles Bressy, Christian Konkuyt, Management et économie des entreprises, 12e éd., Sirey, 2018, chap. 17.(en) Christopher H. Lovelock, Jochen Wirtz, Services Marketing. People, Technology, Strategy, 7th Edition, Prentice Hall, 2010.(en) Jochen Wirtz, Patricia Chew, Christopher Lovelock, Essentials of Services Marketing, 2nd Edition, Pearson, 2012.Christopher LovelockPrestataire de servicesEngagement de serviceAccord de serviceDéfinition de « services », site de l’INSEE Portail de l’économie   Portail de la société"
économie;"Les services à la personne regroupent les activités liées à l'assistance des personnes dans leurs tâches quotidiennes à leur domicile. Ils peuvent concerner les services à la famille (garde d'enfants, soutien scolaire, etc.), les services de la vie quotidienne (ménage, jardinage, etc.) ou les services aux personnes fragiles, personnes âgées, enfants de moins de trois ans, personnes handicapées, qui ont besoin d'une aide à la vie quotidienne. Le secteur des services à la personne est une qualification juridique française ouvrant droit à un crédit d'impôt et des exonérations de cotisations sociales. Dans les années 50, les arts ménagers comprennent l'ensemble des techniques qui, dans le cadre du foyer familial, permettent de soutenir la vie physique et d'alimenter la vie intellectuelle. Les arts ménagers apparaissent comme un élément des sciences économiques avec la Production domestique et comme un élément des sciences biologiques et mentales.Une série de lois ont permis d'accélérer l'évolution des métiers liés aux services à la personne. En 1991, sous le gouvernement Édith Cresson, une loi offre une réduction d'impôts de 50 % sur le salaire versé à une personne pour un service à domicile. En 1996, sous le second gouvernement Alain Juppé, une nouvelle loi autorise les entreprises à investir le secteur, en plus des employés indépendants et des associations. Depuis 2000, la TVA appliquée aux entreprises du secteur passe de 19,6 à 5,5 % pour les activités d'assistance aux personnes dépendantes (personnes âgées, personnes handicapées, enfants de moins de 3 ans).L'expression « services à la personne » a été créée par la loi du 26 juillet 2005 n°2005-841, et intégrée dans l'article D.129-35 devenu article D.7231-1 du code du travail. Permettant une simplification des procédures administratives, les services à la personne ont une agence spécifique, l'agence nationale des services à la personne, elle a été dissoute par le décret du 2 juillet 2014 , un numéro de téléphone propre, le 39 39, et un régime fiscal et social particulier, comprenant des avantages sociaux et fiscaux et un mode de paiement particulier le chèque emploi service universel (CESU).En 2021 les députés Bruno Bonnell et François Ruffin, au cours d'une mission parlementaire qu'ils ont mené, proposent plutot l'expression « métiers du lien » pour désigner ces professions. Le terme de services à la personne est trop lié selon eux à des politiques de l'emploi sans réflexion, et le terme de services à domicile cacherait le fait que beaucoup de ces activités se déroulent hors d'un domicile, et n'est pas en relation avec les caractéristiques du travail fourni. Dans ces domaines, selon eux, la compétence et le service s'articulent autour du lien, non du bien. ,Cette mission propose quatre métiers comme les plus représentatifs des enjeux des liens humains dans la société française : accompagnant des élèves en situation de handicap, nourrice, auxiliaire de vie sociale (également nommé « aide à domicile ») et animatrice du périscolaire (voir Accueil périscolaire). Ces métiers, selon les rapporteurs, n'ont pas vraiment de statuts ni de salaires, sont peu visibles et très précarisés, mais essentiels pour la construction de la société française. En 2021, le vieillissement de la population, la hausse du taux d'activité des femmes et l'augmentation de la fragmentation des familles induiront une forte croissance de ces métiers. France Stratégie indique que le métier d'aide à domicile est celui qui a la plus forte croissance en France depuis 10 ans. Une projection sur 2030 prévoit que ce métier sera tenu à ce moment-là par 862 000 personnes, ce qui correspondra à 160 000 créations de poste. Si les conditions d'exercice de ce métier sont améliorées, ces professionnelles pourront atteindre le million. Tous les métiers du lien favoriseraient fortement la croissance économique générale s'ils étaient mieux construits. Ils déplacent la notion de modernité vers la relation humaine. Pour répondre à ces tensions, le gouvernement a annoncé qu'il allait promouvoir une réforme de l'immigration, de façon que beaucoup de femmes étrangères puissent exercer ces métiers. Mais, en l'état, cela revient à enfermer ces femmes dans une précarité durable. Ces métiers sont, en effet, fortement précarisés. Le marché des services à la personne est réparti entre plusieurs acteurs :des employeurs particuliers ;des organismes spécialisés ;des personnes morales ;des entrepreneurs individuels ;le travail dissimulé.Ce dernier, de par sa nature, est difficile à chiffrer. Les activités sont exercées selon plusieurs modes, l'emploi direct entre personnes (employeurs particuliers et salariés), le mode mandataire dans lequel un organisme assure la médiation (recrutement, diverses démarches administratives, [...]) pour le compte du particulier employeur et le mode prestataire dans lequel un organisme, personne morale ou entrepreneur individuel, intervient auprès d'un client dans le cadre d'une prestation de service, réalisée par le chef d'entreprise ou un de ses salariés.Plusieurs rapports affirment que les services à la personne, surtout depuis les différentes lois en leur faveur, sont en hausse soutenue depuis plusieurs années, passant de 300 millions d'heures travaillées en 1994 à plus de 700 millions en 2006,. Il faut cependant prendre en compte que la palette de services pris en compte s'est étendue.La répartition entre les différents acteurs est assez inégale, et en évolution constante. Ainsi, les particuliers employeurs de salariés indépendants représentaient en 1994 96 % des emplois déclarés du secteur, alors qu'en 2007, ils ne représentent que 74 %, même si le nombre d'heures travaillées a largement augmenté sur cette période. Les emplois restants se répartissent entre les associations de prestations et les entreprises. Ces dernières, bien que ne représentant qu'un faible pourcentage des prestations, sont en nette augmentation depuis ces dernières années, passant de 710 structures actives en 2004 à plus de 4 500 en 2007.Le Plan Borloo a fixé le maintien de la TVA à 5,5 % pour les professionnels et, pour les particuliers, une réduction d’impôt sur le revenu de 50 % des sommes versées à un salarié à domicile.Le sénateur centriste Joseph Kergueris présente en septembre 2010 un rapport pour le Sénat selon lequel les services à la personne ont coûté 16 milliards d'euros en 2010 contre 10 milliards en 2005.À la suite de la loi de finances rectificative pour 2011 entrée en vigueur le 1er janvier 2012, la TVA à taux réduit passe de 5,5 à 7 %, sauf pour les services liés aux « gestes essentiels » de la vie quotidienne des personnes en situation de dépendance qui restent à 5,5 %. Par conséquent le secteur des services à la personne est aussi touché.Depuis 2014, le taux de TVA réduit est passé à 10 % et certains services ne bénéficie plus de ce taux réduit tel que le Petit bricolage ou Petit jardinage.En 2019, le gouvernement décide de supprimer l'exonération de cotisations sociales sur l'emploi à domicile dont bénéficiaient les personnes âgées de plus de 70 ans. Selon les données de la Fédération des particuliers-employeurs, 700.000 retraités seront concernés.En 2009, 4 millions de foyers employaient des intervenants à domicile sous le statut particuliers-employeurs selon l'Agence nationale des services à la personne. 92 % des salariés des services à la personne sont des femmes et 74 % d'entre eux sont non diplômés ou ont un diplôme d'études secondaires sans baccalauréat.En 2011, le poids économique du secteur s’élevait à 17,4 milliards d'euros et 1,82 milliard d'heures prestées. 1,2 million de personnes étaient dépendantes selon la Direction de la recherche, des études, de l'évaluation et des statistiques.En 2012, deux millions de salariés âgés en moyenne de 46 ans sont employés dans ce secteur contre 600 000 en 1994. Selon le Centre d'analyse stratégique (CAS), 660 à 825 000 emplois seront créés d'ici 2030. 25 000 associations, entreprises et établissements publics sont actuellement agréés contre 5 500 en 2006. Selon l'enquête réalisé par le CROCIS, 89 % des chefs d'entreprises de services à la personne déclarent rencontrer des difficultés de recrutementEn 2016, l'assurance-maladie recense dans le secteur de l'aide et des services à la personne 94,6 accident du travail pour 1 000 salariés, soit près du triple de la moyenne nationale. Le taux d'absentéisme est de 30 % plus important que dans l'ensemble du secteur de la santé.Béatrice Belle, syndicaliste de la Confédération générale du travail (CGT), souligne qu'en ce qui concerne les aides à domiciles : « Nous avons des amplitudes horaires qui nous font travailler parfois 13 heures par jour, sept jours sur sept, notre employeur peut nous rappeler n’importe quand, même sur nos congés, pour remplacer au pied levé une collègue. Bien sûr, nous ne sommes rémunérées que sur nos interventions, et pas sur les trous entre deux visites. » En outre : « Le turnover est important car les conditions ne sont plus supportables, certaines intervenantes prennent plus de médicaments que les bénéficiaires qu’elles ont en charge ! Ce n’est pas une vie, mais de la survie. » Les aides à domiciles sont également confrontés au problème du coût des déplacements. D'après, Loïc Le Noc, chargé de la branche aide à domicile pour la CFDT : « Aujourd’hui, pour faire un kilomètre, on donne 35 centimes à chaque auxiliaire et ça c’est dans la meilleure des trois conventions. Mais le coût de revient du moindre véhicule est supérieur à 40 centimes. Donc elles s’appauvrissent littéralement à chaque kilomètre… À chaque fois que l’on a tenté de négocier une revalorisation de cette indemnité, les services du ministère nous ont dit niet. »Le taux de syndicalisation est très faible dans la profession d'aides à domicile. Parmi les raisons qui expliquent cette situation figurent les bas salaires, qui rendent difficile pour les aides à domicile d'en consacrer une part au militantisme syndical. D'après une déléguée de la CGT : « Lorsque j'explique à mes collègues que c'est 1 % du salaire mensuel, elles me répondent ""tu as vu les salaires qu'on a ?"" La plupart sont à temps partiel et n'ont plus rien à la fin du mois. Elles préfèrent consacrer 8 euros à nourrir leur famille plutôt qu'à un timbre syndical. » En outre les aides à domicile travaillent seuls, ce qui ne facilite pas l’investissement dans un mouvement collectif pour défendre des revendications.La liste de ces activités fait l'objet d'un décret. Le décret no 2016-750 du 6 juin 2016 fixe la liste des activités mentionnées à l’Article L. 7231-1code du travail.Ces activités sont les suivantes :Entretien de la maison et travaux ménagers ;Petits travaux de jardinage ;Prestations de petit bricolage, dites « hommes toutes mains » ;Garde d'enfant à domicile ;Soutien scolaire et cours à domicile ;Préparation de repas à domicile, y compris le temps passé aux commissions ;Gardiennage et surveillance temporaire, à domicile, de la résidence principale et secondaire ;Assistance administrative à domicile ;Collecte et livraison à domicile de linge repassé, à la condition que cette prestation soit comprise dans une offre de services incluant un ensemble d'activités effectuées à domicile.Assistance aux personnes âgées  qui ont besoin d'une aide personnelle du fait de leur dépendance, à l'exception d'actes de soins relevant d'actes médicaux ;L'allocation personnalisée d'autonomie - APA - Sert au financement de la dépendance elle est attribuée par le conseil général en fonction du niveau de dépendance -GIR- groupe iso-ressource elle est attribuée suivant le niveau de revenu. Les personnes âgées en maison de retraite ou en EHPAD peuvent en bénéficier elle sert à financer le budget Dépendance dans la convention tri- partie des établissements.Les personnes âgées avec des revenus importants qui utilisent les services d'aide à domicile peuvent également déduire cette prestation en fonction de leur niveau de GIR.     Assistance aux personnes handicapées, y compris les activités d'interprète en langue des signes, de technicien de l'écrit et de codeur en langage parlé complété. Le financement de l'aide relève de la compensation du handicap, celle-ci est attribuée sous condition du taux d'invalidité et du projet personnalisé par la Maison départementale des personnes handicapées (MDPH).Garde-malade, à l'exclusion des soins ; financement possible par les mutuelles santé et les assurances.L'APA et La PCH peuvent financer en fonction du projet personnalisé :Aide à la mobilité et transports de personnes ayant des difficultés de déplacement lorsque cette activité est incluse dans une offre de services d'assistance à domicile ;Prestation de conduite du véhicule personnel des personnes dépendantes, du domicile au travail, sur le lieu de vacances, pour les démarches administratives, à la condition que cette prestation soit comprise dans une offre de services incluant un ensemble d'activités effectuées à domicile ;Accompagnement des personnes âgées ou handicapées en dehors de leur domicile (promenades, transports, actes de la vie courante), à condition que cette prestation soit comprise dans une offre de services incluant un ensemble d'activités effectuées à domicile ;Livraison de courses à domicile, à la condition que cette prestation soit comprise dans une offre de services comprenant un ensemble d'activités effectuées à domicile ;Livraison de repas à domicile, à la condition que cette prestation soit comprise dans une offre de services incluant un ensemble d'activités effectuées à domicile ;Assistance informatique et internet à domicile ;Soins et promenades d'animaux domestiques, pour les personnes dépendantes ;Soins d'esthétique et coiffure à domicile pour les personnes dépendantes ;Activités qui concourent directement et exclusivement à coordonner et délivrer les services à la personne et téléassistance.Les associations et les entreprises sont agréées en application de l'article L. 129-1, du code du travail.L'ensemble des textes qui régissent le secteur sont disponibles sur le site de l'ANSP en particulier les articles L 129 et D 129 du code du travail et la loi du 25 juillet sur les services à la personne.Statut possible de l'organisme auprès de la DDTE :Les structures proposant des activités de services à la personne auprès de public fragile (personnes âgées, personnes en situation de handicap, enfants de moins de 3 ans) sont soumises à agrément délivré par le Préfet de département, l'instruction du dossier de demande d'agrément étant réalisée par la DIRECCTE.Par ailleurs, les structures proposant des activités de services à la personne peuvent bénéficier des avantages fiscaux et sociaux dès lors qu'elles se sont déclarées auprès du Préfet de département, et plus précisément auprès des services de la DIRECCTE.La déclaration ouvrant droit aux avantages fiscaux et sociaux n'est pas limitée dans le temps. L'agrément est délivré pour une période de 5 ans. Ces deux procédures sont distinctes.La personne âgée est cliente du service d'aide à domicile. Les intervenants sont salariés du service et sont sélectionnés par le prestataire. Les opérateurs doivent obtenir leur droit d'exercer par le Président du Conseil Départemental.Le service gère le recrutement et la gestion administrative de l'aide à domicile pour le compte de la personne âgée qui est l'employeur de son intervenant. Le mandataire doit disposer d'un agrément délivré par les services de l'Etat afin d'exercer.La personne âgée gère seule l'ensemble des démarches liées au recrutement de l'aide à domicile et devra respecter les formalités liées à l'embauche.Page consacrée aux services à la personne sur le site Service-Public.fr https://www.service-public.fr/particuliers/vosdroits/F13244Décret d'application du 14 octobre 2005. Portail du droit français   Portail du travail et des métiers   Portail des politiques sociales"
économie;"Un système économique est le mode d’organisation de l'activité économique d’une société ou d'une aire géographique donnée, qui détermine la production, la consommation, l'utilisation des ressources, etc. Dans un sens élargi, il peut être également compris comme l'organisation sociale induite par le système. Le système économique influence de nombreux facteurs, comme le niveau de vie des habitants, le niveau des inégalités, les relations avec les autres pays, ou la puissance économique.Un système économique est un mode d'organisation de la société analysé par le prisme économique. Un tel système regroupe l'ensemble des institutions, sociales comme normatives, qui agissent comme une courroie ou un récipiendaire de l'activité économique au niveau micro. Le système forme le niveau macroéconomique de la vie du pays. Les normes du système (via le droit du pays) détermine également la répartition des richesses.Ce système n'est pas neutre économiquement, dans le sens où ses structures conditionnent l'affectation des ressources et la productivité (production unitaire d'un facteur de production utilisé dans la combinaison productive) des facteurs. Sa modulation joue donc un rôle essentiel dans le cadre des politiques publiques visant à générer de la croissance ou à modifier le niveau d'inégalités.Les systèmes économiques varient en fonction des régions et des époques. Les pays occidentaux suivent une organisation fondée sur le capitalisme caractérisé par la propriété privée des moyens de production. Le système économique des pays de l'ancien bloc de l'Est était fondé sur le principe de l’économie communiste dont l'objectif est la propriété collective des moyens de production.Un système économique est lui-même composé d'une multitude de sous-systèmes, plus petits. Les agents qui le composent peuvent y être spécialisés : le système économique vigneron dispose de spécificités face au système économique de l'agroalimentaire auquel il appartient, qui, lui-même, n'est qu'un sous-système d'un ensemble plus grand.Du fait des combinaisons politiques et des modalités que peuvent prendre les institutions, il n'y a pas un seul modèle de système économique. Dans Les trois mondes de l'Etat-providence, Gøsta Esping-Andersen identifie trois types de systèmes économiques, correspondant à un système continental, un système nordique social, et un système anglo-saxon. Des recherches ultérieures ont identifié des systèmes économiques arabo-musulmans et asiatiques.Raymond Aron transpose par ailleurs le concept à la société internationale, en écrivant : « les États, par leurs politiques, contribuent à former le système économique mais celui-ci, inégalement déterminé par les États selon les pesanteurs de chacun d'eux, constitue un système différent du système interétatique, qu'on devrait plutôt qualifier de transnational que d'interétatique ».Un système économique est composé par des relations entre ses acteurs. Pierre-Joseph Proudhon note ainsi en 1850 que le système économique capitaliste d'Europe de l'Ouest est dominé par la figure du travailleur et celle du patron. Les relations entre les agents sont modulées par les politiques publiques, qui peuvent par exemple décider d'un niveau de centralisation plus ou moins élevé de l'activité économique.Le passage à un mode de développement durable suppose d'évoluer vers un nouveau modèle économique. Un système économique peut être refusé par certains de ses agents, voire remplacé. Le communisme est ainsi un système économique concurrent au système économique capitaliste. Un système économique se fond dans les institutions sociales ; dès lors, le changement de système économique requiert une modification desdites institutions.Le concept de souveraineté économique permet de qualifier un système économique dont les principales sources d'approvisionnement de matériaux et de biens stratégiques sont endogènes au système. Un système économique peut, à l'extrême, être autarcique, c'est-à-dire fonctionner sans relations avec d'autres systèmes économiques.Certains auteurs comme Jean-Marie Albertini constatent que le mot système ou modèle est utilisé par les économistes pour déterminer les différences théoriques essentielles entre les pays appartenant au même système (Japon et États-Unis, par exemple) ou, a fortiori entre des pays qui appartiennent à des systèmes économiques et sociaux totalement opposés (URSS et États-Unis lors de l'époque de la Guerre froide). Toutefois, ce mot ne permet pas de décrire l'évolution historique et réelle d'un pays ou d'un système. Pour cela, Jean-Marie Albertini propose d'utiliser le mot « régime ».Développement économique et social Portail de l’économie"
économie;"Une économie du marché est un système économique où les décisions de produire, d'échanger et d'allouer des biens et services rares sont déterminées majoritairement à l'aide d'informations résultant de la confrontation de l'offre et de la demande telle qu'établie par le libre jeu du marché. Confrontation qui détermine les informations de prix, mais aussi de qualité, de disponibilité. Au cœur de l'économie de marché, le mécanisme de l'offre et de la demande concourt à la découverte et à l'établissement des prix. Ce mécanisme opère par arbitrage pour un horizon donné et pour une qualité donnée entre des valeurs représentatives du bien ou du service concerné : d'une part la valeur de son coût intrinsèque (prix de revient) mais aussi d'autre part sa valeur d'échange (prix relatif, c'est-à-dire du prix d'un produit ou d'un service par rapport aux autres).Pour Robert Gilpin la dynamique de l'économie de marché fait intervenir également d'autres facteurs comme la concurrence et l'aptitude à la survie des acteurs dans l'activité économique .Cette dynamique propre au marché représente un facteur expliquant la diffusion de la croissance économique et l'extension géographique des échanges dans un espace plus large, au-delà des frontières politiques des États.Pour Roger Guesnerie « À l'aune de l'esquisse qui est faite ici d'une économie de marché -des marchés appuyés sur la monnaie et le droit-, nombre d'économies historiquement datées ont droit au label d'économies de marché ».D'une manière générale, il serait plus exact de parler des économies de marchés plutôt que de l'économie de marché, tant le système est dépendant des contextes et institutions très diverses qui accompagnent et soutiennent les marchés.Dans cette perspective, la volonté de prendre en compte les aspects sociaux en Europe après la Seconde Guerre mondiale a conduit à l'émergence du concept d'économie dite « sociale de marché », qui a été décliné selon différentes variantes propres aux pays concernés.Aujourd'hui, l'importance croissante accordée à l'environnement peut laisser entrevoir une évolution vers une « économie durable de marché » voire une « économie sociale et durable de marché » .Certains auteurs posent clairement une distinction entre économie de marché et capitalisme.Pour Fernand Braudel, les régimes de production/répartition des biens et services ont évolué selon trois formes historiques successives : celle de la vie matérielle primitive où le processus d'auto-suffisance et d'auto-consommation se déroule de manière très locale, à l'échelle de l'individu, de la famille ou de petits groupes. Ici on produit pour se suffire, uniquement, à soi-même. L'échange et donc le marché n'existent pas.celle de l'économie de marché, telle qu'elle découle des échanges rendus nécessaires par une plus grande spécialisation et une plus large division du travail : chacun produit une catégorie spécifique de bien et doit fatalement échanger avec les autres pour se procurer les biens qu'il ne produit plus et ainsi satisfaire l'ensemble de ses besoins.celle du capitalisme, amorcée par les entreprises de « commerce ou de négoce au long cours » et qui se financiarise inéluctablement pour engendrer un système où l'échange commercial n'est plus que le support ou le prétexte de gains financiers. Pour lui, « le capitalisme dérive par excellence des activités économiques au sommet ou qui tendent vers le sommet. En conséquence, ce capitalisme de haut vol flotte sur la double épaisseur sous-jacente de la vie matérielle et de l'économie cohérente de marché, il représente la zone de haut profit ». D'une façon générale, Braudel distingue deux types d'échanges : « l'un terre à terre, concurrentiel puisque transparent » qui relève de l'économie de marché et « l'autre supérieur, sophistiqué, dominant » qui relève du capitalisme.Pour Robert Gilpin, l'essence du marché réside dans le rôle des prix relatifs dans le processus d'allocation des ressources tandis que celle du capitalisme réside dans la propriété privée des moyens de production. Au niveau théorique, une économie socialiste de marché composée d'acteurs publics et de travailleur non libres est pour lui concevable comme cela est envisagé dans le concept d'économie socialiste de marché.Issue d'un concept et d'une pratique liée à l'ordo-libéralisme, l'expression recouvre aujourd'hui un sens plus large. Ainsi, Mario Monti, le commissaire européen, distingue les économies de marché de type anglo-saxon des économies sociales de marché allemande ou française. Pour lui, l'économie de marché doit non seulement être compatible, mais aussi être en mesure de financer la protection sociale par une imposition redistributive, de même que promouvoir un certain volontarisme des gouvernements et des institutions européennes en faveur de l'économie dans le respect des règles européennes de la concurrence. En effet, dans l'optique libérale la concurrence entraîne la baisse des prix. Cela protège le pouvoir d'achat des individus et favorise l'innovation,. Le capitalisme, lui, encouragerait en réalité des comportements criminels, crapuleux et opportunistes .Il n'y a pas sur le plan théorique d'unanimité quant à la définition précise de l'économie de marché. On constate en revanche l'existence et la pratique de modèles les plus divers où le mécanisme d'économie de marché est amené à coexister et à composer avec des logiques ou contraintes plus ou moins compatibles.Le régime de la concurrence pure et parfaite n'étant pas concrétisé dans la réalité,il n'existe pas « un » marché général où se produit la confrontation de toutes les offres et de toutes les demandes, mais « des » marchés plus ou moins inter-connectés sinon cloisonnés qui donnent lieu à des confrontations partielles. (ex : marché des biens, des matières premières, des services, du travail, des changes, des capitaux, marché monétaire, marché immobilier, etc.)les imperfections de marché prospèrent :Monopole, duopole, oligopole, cartel, entente, position dominante, etc.asymétrie de l'information, délai et effet retard, goulet d'étranglement, etc.la demande exprimée sur un marché est perçue la plupart du temps à travers le prisme déformant de la demande solvable, c'est-à-dire celle qui émane des opérateurs disposant du pouvoir d'achat monétaire suffisant.Des actions collectives peuvent être organisées pour promouvoir ou défendre des valeurs positives ou des règles sociales, culturelles, morales, voire religieuses :l'économie publique ayant une place complémentaire (ex : les services publics), ou centrale (ex: le capitalisme d’État) ;l'économie sociale pour pallier les insuffisances ou déficiences du marché (ex : les mécanismes de protection sociale, d'assistance, de solidarité, d'État providence) ;l'interventionnisme d'État ou dirigisme via l'économie planifiée ou l'économie administrée ;le commerce équitable comme projet d'organisation visant à faire une meilleure place à certains producteurs en danger d'être marginalisés ou évincés du marché courant ;l'élaboration de normes - à caractère volontaire, incitatif ou obligatoire - peuvent contribuer à préciser ou encadrer la définition des pratiques de conception, de production ou de distribution des biens et services pour des motifs de protection de la qualité ou de la sécurité dues au consommateur/usager.l'économie dite non marchande ou domestique ne donne pas lieu à échange rémunéré(ex : jardins familiaux, babysitting non rémunéré, femmes au foyer, aide des grands-parents…).l'économie de troc et/ou l'économie de subsistance se situe relativement à l'écart des flux économiques (ex. : pratiques de certaines zones rurales du tiers monde ou très déshéritées)l'économie spéculative ou exclusivement financière qui introduisent des logiques de type « argent ? marchandise ? argent » dénoncées par certains comme représentant des déconnexions forcées de l'économie réelle. (ex. : spéculation sur les matières premières)l'économie souterraine ou les « trafics » opérés sur des marchés parallèles ou occultes (ex: le trafic de drogue, le travail ou le marché au noir, ou le proxénétisme…).les phénomènes de corruption ou de délit d'initié qui visent par leur nature à fausser le libre jeu des forces du marché.L'économie de marché s'arrête difficilement au niveau d'un seul pays, si vaste soit-il. Au niveau international, elle est d'autant plus développée que les divers pays pratiquent le libre-échange.Cela dit, en pratique beaucoup de pays revendiquent pour leurs exportations les règles applicables à l'économie de marché (sinon la clause de la nation la plus favorisée), en organisant par ailleurs vis-à-vis des importations des règles fort peu réciproques (protectionnisme) :Certains pays issus du collectivisme — comme la Chine — se veulent « économie socialiste de marché », alors qu'ils sont encore des économies marquées par l'empreinte du Capitalisme d'État.D'autres pays bénéficient de conditions de couts qui leur permettent de pratiquer une concurrence jugée « déloyale » par leurs rivaux plus développés.D'autres pays se trouvent dans une situation où la structure des flux échangés les entraine vers la détérioration des termes de l'échange.Un débat donne lieu à un fort questionnement assorti de multiples prises de position :Échanges entre socialistes « réformistes » et socialistes « fondamentalistes » Michel Rocard se targue souvent « d'avoir mis des décennies à apprendre l'économie de marché aux socialistes ».le tournant de la rigueur survenu en 1983 motive la gauche réformiste pour accepter de facto l'économie sociale de marché.L'économie de marché non régulée n'est pas forcément compatible avec les exigences du développement durable. En effet, la recherche de la maximisation du profit par les entreprises ne va pas spontanément dans le sens d'un développement durable, car elle conduit à des raisonnements de court terme (voire de spéculation), et elle tend à la satisfaction des intérêts des seuls actionnaires des entreprises.L'encyclique Caritas in Veritate de Benoît XVI (juillet 2009) indique que les acteurs de la vie économique ne peuvent se limiter au marché seul, mais « que l'économie doit aussi impliquer l'État et la société civile » :« La vie économique a sans aucun doute besoin du contrat pour réglementer les relations d’échange entre valeurs équivalentes. Mais elle a tout autant besoin de lois justes et de formes de redistribution guidées par la politique, ainsi que d’œuvres qui soient marquées par l’esprit du don. L’économie mondialisée semble privilégier la première logique, celle de l’échange contractuel mais, directement ou indirectement, elle montre qu’elle a aussi besoin des deux autres, de la logique politique et de la logique du don sans contrepartie. »« Mon prédécesseur Jean-Paul II avait signalé cette problématique quand, dans Centesimus annus, il avait relevé la nécessité d’un système impliquant trois sujets : le marché, l’État et la société civile. »— Encyclique Caritas in Veritate, chapitre III, § 37 et 38L'intervention de l'État, qui représente les « intérêts publics » (notion à définir), est considérée par certains comme nécessaire. Elle se fait actuellement de la façon suivante :Être exemplaire en matière de développement durable :Définir une stratégie nationale de développement durable,Mettre en œuvre cette stratégie en mettant en place des organisations dédiées au développement durable dans les ministères et les collectivités territoriales (ex. : ministère de l'environnement ou du développement durable),Lancer des actions concrètes comme le Grenelle de l'environnement.Encadrer le marché par l'application forcée des règles le concernant (concurrence, propriété privée, droit du travail, ...).Participer aux différentes initiatives qui ont lieu au niveau international sur le développement durable, sommets de la Terre, sommets de l'eau, protocole de Kyoto et ses suites, réunions sur la biodiversité…Définir de nouvelles règles du jeu :En France, les entreprises cotées en bourse doivent rendre compte des conséquences sociales et environnementales de leur activité (article 116 de la loi sur les Nouvelles Régulations Économiques),Principe du bonus-malus écologique pour les véhicules automobiles,Mise en place d'une finance du carbone.Toutefois, les évaluations portant sur la mise en œuvre des Nouvelles Régulations Économiques en France montrent qu'assez peu d'entreprises se conforment réellement aux exigences de la loi. En effet, le non-respect de la loi n'entraîne aucune sanction vis-à-vis des entreprises. Il s'agit d'un droit mou.On peut imaginer d'autres actions des États :Changer les règles de comptabilisation de la richesse. Par exemple, des études ont montré que le produit intérieur brut (PIB) ne prend pas en compte la diminution du capital naturel. L'INSEE a retenu le PIB comme indicateur de développement durable, alors qu'en réalité les effets à long terme de la croissance économique sur l'environnement ne sont pas pris en compte par le PIB. Il est clairement de la responsabilité des États de définir des instruments de mesure de la croissance économique, en l'occurrence des indices macroéconomiques, qui rendent compte efficacement de la conformité des agents économiques par rapport aux principes de développement durable (PIB vert).Mettre en place une fiscalité favorable aux produits durables (taxe carbone),Adapter l'enseignement,Sensibiliser la société civile en donnant le feu vert à tous les moyens permettant de montrer les dangers sur l'homme des activités qui menacent l'environnement et le développement durable.etc.La société civile intervient par l'intermédiaire de ses représentants, organisés en parties prenantes (organisations professionnelles, organisations syndicales, organisations non gouvernementales…). Par exemple, en matière environnementale, les parties prenantes représentatives sont les ONG (organisations non gouvernementales) (environnementales (WWF, Greenpeace, Amis de la Terre…).Les parties prenantes peuvent se concevoir par rapport aux autorités politiques, ou bien par rapport aux entreprises.Par rapport à la perspective catholique, les sociaux-libéraux s'interrogent sur la notion même d'État. La distinction société civile/État leur pose un problème car elle suppose, à la manière de ce qui existe dans l'Église, une prépondérance donnée à la hiérarchie de l'État sur les citoyens.L'Organisation mondiale du commerce (OMC) octroie le « statut d’économie de marché » (SEM) aux États. Un pays qui importe des produits depuis un pays qui n'en bénéficie pas est autorisé à ne pas tenir compte du prix pratiqué sur le marché intérieur de l’État exportateur.La Chine s'est ainsi vue attribuer ce statut en 2016, conformément à l'accord convenu lors de son adhésion en 2001,. Avant même cette décision de l'OMC, plus de 80 pays dans le monde avaient reconnu le statut d'économie de marché à la Chine. Cependant, les États-Unis s'y opposent,. De son côté, l'Union européenne a mis en place une nouvelle méthodologie anti-dumping qui ne cible plus spécifiquement la Chine : Jean Quatremer estime ainsi qu'« en clair, l’Union va continuer à considérer que la Chine n’est pas un pays à économie de marché, mais sans le proclamer et en évitant les foudres de l’OMC »,. D'autre part, un rapport détaillé de la Commission européenne émettait des doutes en décembre 2017 sur la nature d'« économie de marché » de l'« économie socialiste de marché » de la Chine,.Robert Gilpin, 1987, The Political Economy of International Relation, Princeton University Press.Fernand Braudel, 1985, La Dynamique du capitalisme, Paris, Arthaud, 1985  (ISBN 2080811924)Roger Guesnerie 2006, L'Économie de marché, Le Pommier.John Kenneth Galbraith, Et le système fut rebaptisé. dans Les Mensonges de l'économie, traduction française de Paul Chemla, Paris, Grasset, 2004Michel Lafitte, 2007, Développement durable et économie de marché  (ISBN 2863254782) Portail du libéralisme   Portail de l’économie"
économie;"La comptabilité nationale est une représentation schématique et quantifiée de l'activité économique d'un pays. Elle consiste en une mesure des flux monétaires représentatifs de l'économie d'un pays pendant une période donnée, en principe une année, et les regroupe dans des totaux nommés agrégats, dans un but analytique direct. La comptabilité nationale prend en compte de nombreux indicateurs macroéconomiques, dont le plus important est le PIB (produit intérieur brut), qui correspond à la somme des valeurs ajoutées — auxquels il faut ajouter les impôts nets des subventions sur les produits — des biens et services produits dans un pays donné au cours d'une année. La comptabilité nationale prend en compte de nombreuses informations, contenues dans les documents comptables des entreprises d'une part, mais aussi dans les rapports des institutions administratives. La comptabilité nationale classe ainsi les différents agents économiques en catégories, les secteurs institutionnels, afin de recenser au mieux les différentes informations relatives à l'économie.Les premiers systèmes de comptabilité nationale datent de la Seconde Guerre mondiale, tout d'abord avec l'économiste britannique Keynes qui développe dès 1941 des instruments de mesure de l'économie, puis avec Jan Tinbergen et Wassily Leontief, considérés comme les véritables inventeurs de la comptabilité nationale. La comptabilité nationale s'est ensuite développée dans la plupart des pays développés. Ainsi, dans le cadre du système monétaire européen (SME), les systèmes de comptes nationaux ont été harmonisés autour de normes communes, et les États européens utilisent le même plan comptable : le SEC (système européen de comptabilité).La comptabilité nationale est née de la volonté des États d'intervenir dans une régulation conjoncturelle de l'économie. Selon un article du Figaro en 2009, « l'invention de la comptabilité nationale a été une réponse à la Grande Dépression des années 1930. On ne disposait à l'époque d'aucune statistique générale, en dehors des cours boursiers ou des données de production établies plus ou moins bien par les professions. Dès 1932, avant même l'élection de Roosevelt et le New Deal, le Congrès américain avait demandé à l'économiste Simon Kuznets (couronné par le Prix Nobel en 1971) d'estimer le recul de l'activité globale. Il s'est alors avéré qu'elle avait chuté de 40 % entre 1929 et 1932. »Le premier vrai système de comptabilité nationale fut créé par John Maynard Keynes (qui dirigeait alors la délégation britannique chargée de rédiger les accords de Bretton Woods) en 1941 à la suite de la demande du parlement de Grande-Bretagne. Les collaborateurs de Keynes élaborèrent une série de tableaux illustrant les ressources produites et leur utilisation sous forme de consommation, dépenses publiques, subventions et investissements. En outre, les travaux menés par l'américain Wassily Leontief (« Prix Nobel » d'économie en 1973) et le néerlandais Jan Tinbergen, « Prix Nobel » d'économie en 1969 ont permis de développer des analyses plus proches de celles que nous connaissons aujourd'hui.Les travaux de Richard Stone et de Simon Kuznets sont à l'origine de ce que l'on a baptisé un « modèle normalisé de la comptabilité nationale ».En ce qui concerne les tableaux de synthèse, en particulier le tableau entrées-sorties (TES) le précurseur fut l'économiste d'origine russe naturalisé américain Wassily Leontief.En France, François Quesnay, chef de file de l'école physiocratique, apparaît comme le premier à avoir élaboré un modèle dynamique, en 1758, pour représenter, à une échelle macroéconomique, la comptabilité nationale dans son ensemble. Au xixe siècle, plusieurs économistes ou hommes politiques s'efforcent de quantifier l'activité économique : Lesur dresse un bilan économique de la France en 1817 et y évalue la somme des revenus à cinq milliards ; en 1819, Jean-Antoine Chaptal estime la valeur de la production agricole et manufacturière en s’appuyant sur les données statistiques des préfectures et du cadastre. Des économistes comme François Perroux (également auteur de la théorie des « pôles de croissance ») ont les premiers établi des modèles modernes de comptabilité nationale sous le régime de Vichy et à la Libération. Selon une étude sur le sujet, « ces pionniers aux vues anticipatrices élaborent des outils statistiques et amorcent la réflexion sur la comptabilité nationale, à partir de la fin des années trente, puis pendant l'occupation. Ces économistes non traditionnels (Jean Fourastié) et ces statisticiens de l'Insee (André Vincent, Jacques Dumontier) se joignent ensuite à l'équipe de Jean Monnet à partir de 1945. »La comptabilité nationale a deux vocations principales : modéliser et étudier l'activité économique d'un pays donné pendant une durée précise d'une part, et prévoir l'évolution d'une conjoncture d'autre part. Elle peut ainsi être un outil de prévision pour aider un gouvernement à trouver des solutions ou à relancer la consommation par exemple. Les comptes nationaux sont publiés par trimestre ou par année.La comptabilité nationale est ex-post, elle s'effectue une fois l'année écoulée. Elle se mesure à prix constants, c'est-à-dire qu'elle ne tient pas compte de l'inflation.L'information la plus connue utilisée par la comptabilité nationale est le PIB (Produit intérieur brut). Le PIB est un indicateur macroéconomique nommé agrégat, c’est-à-dire une grandeur globale qui mesure l'activité économique. Il est possible de proposer trois approches du PIB, cependant, on le considère la plupart du temps comme la somme des valeurs ajoutées produites par l'ensemble des unités résidentes, c’est-à-dire les agents économiques effectivement présents sur le territoire pendant au moins 183 jours sur une année.Le PIB a ainsi une triple optique basée sur les grands principes de la comptabilité nationale :la production : PIB = somme des VAB + IP - SUBV. L'approche par la production, met ainsi en relation la somme des valeurs ajoutées brutes, l'impôt sur la production ainsi que les différentes subventions ;la formation de revenu : PIB = RS + EBE + RMB - SUBV + IP, avec RS la rémunération des salariés, EBE l'Excédent brut d'exploitation, RMB les revenus mixtes bruts, SUBV les subventions et IP les impôts sur la production (liés à la production et aux importations) ;la demande : PIB = CF + FBCF + (X-M), avec CF la consommation finale, FBCF la formation brute de capital fixe (l'investissement), X les exportations et M les importations.Le PIB (Produit intérieur brut) ne doit pas être confondu avec le PNB (produit national brut) qui est la somme des revenus primaires reçus effectivement par les agents économiques d'une même nationalité, qu'ils soient situés sur le territoire ou non. On a ainsi la relation PNB = PIB + revenus des facteurs en provenance de l'extérieur - revenus des facteurs versés à l'extérieur.Les différents agents économiques sont regroupés dans différentes branches baptisées unités institutionnelles. Elles constituent les unités de base de la comptabilité nationale.Une unité institutionnelle est un centre de décision autonome pouvant être une personne (ou plusieurs) physique, les économistes disent alors qu'il s'agit d'un ménage, ou une personne morale, c'est-à-dire une entreprise, une administration publique ou une association. Elles sont susceptibles de posséder elles-mêmes des actifs, de souscrire des engagements, de s'engager dans des activités économiques et de réaliser des opérations avec d'autres unités.Ces unités institutionnelles doivent exercer des opérations économiques pendant un an au moins sur le territoire national pour être comptabilisées dans les secteurs institutionnels. Ce territoire est, si on prend l'exemple de la France, la métropole et les départements d'outre-mer, les enclaves territoriales françaises hors du territoire, l'espace aérien, les eaux territoriales et les espaces qui regroupent des ressources appartenant à la France. En revanche, les enclaves étrangères, à l'image de consulats et ambassades présents sur le sol français, ne sont pas considérées comme des unités résidentes.Les unités institutionnelles ayant la même activité principale et la même source principale de revenu sont regroupées en cinq secteurs institutionnels.On distingue cinq secteurs institutionnels résidents :les ménages ;les sociétés non financières (SNF) ;les sociétés financières (SF) ;les administrations publiques (APU) ;les institutions sans but lucratif au service des ménages (ISBLSM).L'ensemble des unités non-résidentes, dans la mesure où elles entretiennent des relations économiques avec des unités résidentes, sont regroupées dans une catégorie appelée reste du monde, parfois baptisée catégorie « plus-un ».La fonction principale des ménages est la consommation à partir de ressources principales obtenues de deux manières :d'une part par la rémunération des facteurs de production, à savoir le travail, la terre, le capital ;d'autre part, par les transferts effectués par d'autres secteurs institutionnels à destination des ménages.Au sein des ménages, on peut distinguer :le ménage « ordinaire » ou « pur », à savoir un ensemble de personnes vivant dans un logement ;le ménage « collectif » qui est constitué par les populations des maisons de retraite, des foyers de travailleurs, etc.On retrouve également dans ce secteur les entreprises individuelles qui sont des unités économiques dont la fonction principale est la production de biens et services pour leur usage final propre. On retrouve ainsi dans cette catégorie les agriculteurs, les artisans, les professions libérales, les petits commerçants, etc.Les sociétés non financières (SNF) regroupent l'ensemble des sociétés et quasi-sociétés dont la fonction principale est de produire des biens et services marchands, c'est-à-dire dont le prix de vente couvre au moins 50 % du coût de production.Les ressources des sociétés et quasi-sociétés non financières sont le résultat de la production et des éventuelles subventions versées par les administrations publiques (collectivités locales).La CN classe actuellement les SNF en trois catégories, selon le contrôle :Les SNF sous contrôle public, c'est-à-dire sous le contrôle de l'État : la SNCF, la RATP… ;Les SNF sous contrôle privé national : Bouygues, Total… ;Les SNF sous contrôle privé étranger : Google France, Toyota France…Les sociétés financières (ou SF) sont constituées par l'ensemble des sociétés et quasi-sociétés dont la principale fonction est d'offrir des services d’intermédiation financière et/ou d'exercer des activités financières auxiliaires. Leurs ressources sont des fonds provenant des engagements financiers.Cinq sous-secteurs institutionnels constituent le secteur institutionnel des sociétés financières :Les banques centrales ;Les autres institutions financières monétaires (la compatibilité nationale y exclut par convention les sociétés d'assurance et les fonds de pension) ;Les intermédiaires financiers ;Les auxiliaires financiers ;Les sociétés d’assurance et les fonds de pension.Les administrations publiques sont regroupées sous le sigle APU. La fonction principale de ces unités institutionnelles est de produire des services non marchands et/ou d'effectuer des opérations de redistribution des revenus ou du patrimoine national. Elles tirent la majeure partie de leurs ressources de contributions obligatoires (impôts).En France, les administrations publiques (APU) se regroupent en trois sous-secteurs :Les APU centrales (APUC) : composées de l'État et des organismes divers APUC (ODAC) ; les universités, le CNRS, l'ANPE… ;Les APU locales (APUL) : régions, départements, communes + OAL (régie de transport municipal, chambre de commerce…) ;Les ASSO (Administration de sécurité sociale) : unités qui distribuent des prestations sociales à partir de cotisations sociales obligatoires + ODASS ; les ressources proviennent des assurances sociales (ex. : hôpitaux publics).Les institutions sans but lucratif au service des ménages (ISBLSM) regroupent diverses structures dont certaines associations (ex. : association de consommateurs, parti politique, syndicat, Église, organisme de charité, etc.). Leurs points communs sont que, d'une part, elles produisent des services pour les ménages, d'autre part, elles sont financées par des cotisations volontaires et parfois par la vente de biens et services marchands, mais dont le but n'est pas d'en tirer de bénéfice.D'un point de vue économique et du fait de la façon dont la comptabilité nationale les prend en compte, les ISBLSM affichent un rôle négligeable ; il en résulte que dans les statistiques globales, leur consommation est ajoutée à celle des ménages. La majorité des organismes à but non lucratif, qui regroupent l'ensemble des entreprises de l'économie sociale, n'est cependant pas regroupée dans cette catégorie des ISBLSM, ce qui contribue à minorer leur importance. Les différentes études menées situent l'importance de l'ensemble du secteur non lucratif (ISBLSM et économie sociale) à environ 10 % des emplois en France.Ce n'est pas un secteur institutionnel et à ce titre on le qualifie parfois de faux secteur, dans la mesure où les opérations ne sont pas décomposées en distinguant des catégories d'agents : il n'y a pas de compte des ménages ou des SNF du reste du monde. Ce secteur « plus un » regroupe ainsi les unités non résidentes qui effectuent des opérations avec l'économie nationale.Les flux sont enregistrés au moment de la réalisation de l'opération. Les flux financiers sont comptabilisés en « flux nets d'acquisition d'actifs » et « flux nets d'engagements contractuels » alors que les autres flux le sont en « emplois » et « ressources ».Il s'agit de l'ensemble des opérations qui concernent la création et l'utilisation des biens et des services.Parmi elles on distingue :La production, qui a évolué dans le temps; les entreprises y jouent un rôle majeur, mais les ménages ainsi que les administrations sont eux aussi considérés comme des producteurs ;La consommation ;La formation brute de capital fixe — FBCF — (c'est-à-dire l'investissement) ;Les opérations avec l'extérieur (c'est-à-dire les importations et les exportations de biens et de services). Ces opérations sont regroupées dans le TRE (tableau des ressources et des emplois).Ce sont les opérations par lesquelles la valeur ajoutée créée par la production est distribuée entre les salariés, les propriétaires d'entreprises et les administrations publiques, puis redistribuée du fait de l'action des administrations publiques (versements d'allocations financées par des prélèvements…).Pour simplifier on peut considérer ici la valeur ajoutée (VA) comme l'ensemble des richesses créées.VA = P - CI : Production - Consommations IntermédiairesUn indicateur, le taux de marge, résume pour l'essentiel la répartition des richesses créées entre les salariés et les propriétaires d'entreprises. Il mesure la part des profits des entreprises (EBE, excédent brut d'exploitation) dans la VA : taux de marge = EBE / VA x 100. Comme la valeur ajoutée se répartit principalement entre salaires et profits, à une hausse du taux de marge correspond une baisse de la part des richesses créées qui revient aux salariés, et une hausse de celle qui revient aux propriétaires des moyens de production (capital).Ces opérations sont regroupées dans le TCEI (tableau des comptes économiques intégrés).Les opérations financières représentent les engagements pris par les agents économiques les uns envers les autres, en contrepartie de monnaie ou de produits. Par exemple les prêts faits par certains représentent des emprunts pour les autres. La comptabilité nationale retrace ces opérations entre les principaux secteurs institutionnels dans le cadre du TOF « tableau des opérations financières ».(Cette partie de l'article fait la liste des principaux comptes. C'est une ébauche à compléter car chacun d'eux reste à présenter). Le compte de production                      P        ?        C        I        =        V        A              {\displaystyle P-CI=VA}  Le compte de production décrit les flux qui composent le processus de production à savoir les consommations intermédiaires qui sont des opérations sur biens et services : son solde est la valeur ajoutée ou la richesse créée. Le compte d'exploitation EBE (excédentaire brut d'exploitation) = Valeur Ajoutée - Salaires - Impôt (production) + Subvention (exploitation)ouEBE= PIB - Salaires - Impôts (production + produit) + Subvention (exploitation + produit) Le compte d'affectation des revenus primaires EBE + Revenus de la propriété reçus + revenus salariés + impôts sur la production - subventions - revenus de la propriété versés = SRPCe compte s'intéresse aux ressources des secteurs c'est la répartition des revenus liés directement au processus de production (revenus primaires). En emploi on a les revenus de la propriété que les secteurs versent. Le compte de distribution secondaire du revenu Srp + Impôts sur le revenu reçus + impôts sur le patrimoine reçus + Prestations sociales reçues + Autres transferts courants reçus+ Cotisations reçues - impôts sur le revenu versés - impôts sur le patrimoine versé - prestations sociales versées - cotisations sociales versés - autres transferts courants versés = RDBCe compte de répartition des revenus secondaires décrit les flux entre les différents secteurs que sont les ménages et les administrations publiques. En ressource de compte les impôts et cotisations sociales sont versés aux administrations publiques. Les ménages reçoivent des prestations sociales. Les autres transferts courants sont versés à l'ensemble des secteurs. En emplois on a les impôts versés et reçus par l'ensemble des secteurs institutionnels. Les cotisations sociales sont versées par les ménages et les entreprises. Le solde obtenu est le revenu disponible brut. Le compte d'utilisation du revenu disponible                     R        D        B        ?        C        F        =        E        B              {\displaystyle RDB-CF=EB}  Ce compte permet de distinguer la part du revenu disponible (RDB=revenu disponible brut) qui sera consacrée à la consommation de biens finaux (CF = consommation finale) de celle qui sera réservée à l'épargne (EB = épargne brute). Ce compte constitue en fait la charnière entre les comptes de résultat (ceux qui représentent des flux) d'une part et les comptes d'accumulation (parfois appelés comptes patrimoniaux et qui représentent des stocks). En effet c'est au départ de l'épargne que se constituent les masses capitalistiques. Le compte de capital Emplois+ FBCF (P51)+ CCF+ VS (P52)+ OV (P53 acquis - cédés)+ AF (NP1 + NP2 acquis - cédés)Ressources+ EB(B8) [solde précédent]+ TC(D9 reçu - D9 versé)Solde : Capacité/Besoin de financement (B9A)FBCF : Formation Brute de Capital Fixe Le compte financier le compte financier mesure la variation de l'actif et le passif financier du secteur institutionnel et du reste du monde.Il permet d'évaluer le patrimoine financier des secteurs institutionnels, en dressant un état de la valeur des actifs détenus et des engagements contractés (passif) à un moment donné. Cette opération a souvent lieu au 31 décembre de l'année.Le TEE est un tableau de synthèse qui donne une présentation simultanée des comptes de flux des secteurs institutionnels et des comptes d'opérations. Il rassemble les opérations économiques et financières de l'économie nationale pour une année donnée. Le TEE permet ainsi de mesurer les résultats économiques globaux, la contribution de chaque secteur institutionnel à ces résultats, ainsi que l'importance des relations entre l'économie nationale et le reste du monde. Il constitue également un outil très important pour la prévision économique.La comptabilité nationale utilise le « tableau économique d’ensemble » (TEE) qui rassemble l’origine et l’utilisation des ressources de chaque secteur (sociétés non financières, instituts de crédit, entreprises d’assurance, administrations publiques, administrations privées, ménages et reste du monde).Il est construit en valeur d'une part, en brut, cvs (corrigé des variations saisonnières) et cjo-cvs (corrigé de l'effet des jours ouvrables et des variations saisonnières) d'autre part. Ainsi que pour le TES, les comptes du TEE ne sont pas publiés.Le TEE se décompose en une succession de lignes et de colonnes qui aboutissent chacune à la mesure d'un solde correspondant. Chaque compte est séparé en emplois (actif) et en ressources (passif). Excepté dans le compte de production, les soldes des différents comptes sont évalués dans les comptes trimestriels tout simplement par solde.Le tableau entrées-sorties distingue les branches et secteurs. La branche est constituée par l'ensemble des activités qui élaborent un produit donné. Ainsi, il y a autant de branches que de produits. Un secteur est constitué par l'ensemble des entreprises ayant la même activité principale. Le TES indique le montant de chaque produit utilisé par les diverses branches de l'économie. Il permet de retrouver l'équilibre pour chaque branche entre les emplois et les ressources. Il permet d'expliquer a posteriori et de simuler a priori les incidences d'une modification des conditions économiques générales.La comptabilité nationale utilise le « tableau entrées-sorties » (TES) qui décrit l’équilibre des opérations sur biens et services pour toutes les branches de l’économie. On entend par branche l’ensemble des unités de production qui fabriquent un même produit. Ainsi le TES permet pour chaque branche et pour l’ensemble de l’économie, de faire ressortir un équilibre entre les emplois et les ressources de la branche. Sa structure repose sur une division par branches et par produits. Il constitue un outil utile aux comptables nationaux. Dans une perspective keynésienne, s’inspirant du tableau économique de Quesnay, le TES a été mis en évidence par l'analyse entrée-sortie de Wassily Leontief pour représenter l’ensemble des opérations des agents économiques au cours d’une période donnée.On va donc tout d’abord rappeler l’égalité de base, puis voir la structure du TES, et enfin son utilité. Rappel de l’égalité de base Ressources=Production_(P) + Importation_(M) + Impôts_(M)Emplois = Consommation intermédiaire (CI) + Consommation finale (CF) + FBCF + Exportations (X) + Variation des stocks (VS)Le TES présente l’équilibre emploi/ressources : P + M = CI + CF + FBCF + X + VSCet équilibre est toujours vérifié dans les comptes en T. La structure du TES En ligne : répartition des produits entre les branches c’est-à-dire le volume de produits utilisés par chaque branche.En colonne : volumes des produits nécessaires à chaque branche pour sa production.Le total des ressources de chaque branche est égal au total des emplois des produits correspondants.Le TES se compose :d'un tableau des emplois intermédiairesd'un tableau des emplois finauxd'un tableau des comptes de productiond'un tableau total des ressources L’utilité du TES Le TES donne une représentation cohérente de la production nationale et permet de représenter les branches qui contribuent le plus à la production nationale. Il permet de faire apparaître le degré d’indépendance des branches en faisant le calcul : (Total des consommations intermédiaires de branche/Production de la branche)*100Ainsi, toute modification de la production dans une branche entraîne des répercussions dans les autres branches.Le TES est aussi un instrument de prévision économique. On peut calculer des coefficients techniques : (Consommation intermédiaire en produit x / Production de la branche y)*100.L’ensemble des coefficients techniques donne une matrice sur laquelle on peut baser des prévisions relativement fiables à court terme. Il est notamment possible de prévoir :l’effet d’entraînement d’une branche sur les autres ;les conséquences sur les branches d’une augmentation globale de la production, des exportations, de la consommation des ménages… ;les conséquences de l’interdépendance des branches (goulets d’étranglement).On peut bien entendu critiquer la difficulté de construction d’un tel tableau pour une économie nationale, ainsi que les erreurs de mesure des grandeurs économiques qu’il renferme.Le TES peut servir de base à la construction d'une matrice de comptabilité sociale, entrée utile pour un modèle d'équilibre général calculable.Abréviations :P : production ;M : importation ;C : consommation ;CI : consommation intermédiaire ;CF : consommation finale ;FBCF : formation brute de capital fixe ;X : exportation ;VS : variation des stocks.Le TOF réunit l'ensemble des statistiques financières relatives aux secteurs institutionnels (SI) et permet d'analyser les aspects financiers de l'économie.En dépit de leur taille et de la masse d'informations qu'ils contiennent, ces tableaux sont d'une structure très simple et leur lecture est assez facile et posée.Comptes nationaux et régionaux de la Belgique publiés par la BNBComptes économiques et financier du CanadaComptes nationaux de la FranceBalance des paiements de la France en 2002Comptes économiques du QuébecDes organismes spécialisés sont chargés de vérifier les comptes nationaux : les Cours des comptes.En France, la loi organique relative aux lois de finances (LOLF), promulguée en août 2001 et mise en œuvre depuis le 1er janvier 2006, modifie en profondeur les finances publiques..La comptabilité nationale est assujettie à un principe de sincérité.Le rapport de la Cour des comptes de juin 2006 fait état de manques de précisions dans le système français de comptabilité nationale :« II ne comprend pas les passifs implicites ; il ignore bon nombre d'actifs ayant une utilité sociale, mais qui ne sont pas valorisés faute d'une valeur marchande de référence ; peu d'actifs incorporels sont recensés ; enfin, il se fonde sur une notion d'actif restrictive, excluant la plus grande partie du capital immatériel – éducation, recherche, santé. »La comptabilité nationale, a été conçue dans les années de reconstruction qui ont suivi la Seconde Guerre mondiale. Il fallait vérifier que le pays retrouvait le niveau de production d'avant guerre, puis qu'il rattrapait celui de l'Amérique. L'attention était focalisée sur le quantitatif, et les contraintes environnementales étaient ignorées.Ainsi conçu, et rigidifié par les institutions de la comptabilité nationale, le PIB ne serait pas adapté à l'économie actuelle, dont le but est différent.Concevoir la comptabilité nationale qui répondrait à des objectifs de développement durable suppose un gros effort intellectuel.Du point de vue environnemental, la comptabilité nationale tient compte actuellement de la consommation de ressources naturelles en tant que consommations intermédiaires.Edith Archambault, La Comptabilité nationale, Economica, 2003,  (ISBN 271784712X)Jean-Paul Piriou, La Comptabilité nationale, Repères, La Découverte, 2004,  (ISBN 2707143367)André Vanoli, Une Histoire de la comptabilité nationale, La Découverte, 2002,  (ISBN 2707137022)Gilbert Abraham-Frois, Économie politique, Economica, 2001],  (ISBN 2717842675) (l'ouvrage comporte une annexe sur la comptabilité nationale, claire et synthétique)Dictionnaire d’économie, J-Y Capul, Olivier Garnier, Hatier, 2005,  (ISBN 2218740591)DJ. Muller, P. Vanhove, PECF. Économie, Dunod, 1999Michel Braibant, Vers un tableau « entrées-sorties » idéal et mondial, Edilivre, 2018  (ISBN 9782414267040)Histoire de la pensée économique Normalisation ÉconométrieComptabilitéPlan comptable Mesure économique Produit intérieur brutTaux d'investissementFormation brute de capital fixe Belgique StatbelBudget fédéral de BelgiqueBanque nationale de BelgiqueBureau fédéral du PlanCour des comptes (Belgique) France InseeBudget de l'État françaisCour des comptes (France)Abrégé de comptabilité nationaleComptes nationaux et régionaux belgesLes définitions de l'InseeUNSTATS Portail de l’économie"
économie;"En économie, un indicateur est une statistique construite afin de mesurer certaines dimensions de l’activité économique, ceci de façon aussi objective que possible. Leurs évolutions ainsi que leurs corrélations avec d'autres grandeurs sont fréquemment analysées à l'aide de méthodes économétriques.Les indicateurs sont construits par l'agrégation d'indices qui figurent dans un document appelé « tableau de bord ». La construction des indicateurs découle d'un choix de conventions qui traduisent plus ou moins bien certaines priorités et valeurs éthiques et morales. Le « Tableau économique » de François Quesnay, l'un des premiers physiocrates qui a vécu au XVIIIe siècle, constitue l'un des premiers exemples d'un tel indicateur visant à mesurer la richesse d'un pays. Depuis les développements des comptes nationaux après la Seconde Guerre mondiale, le produit intérieur brut (PIB) et le produit national brut (PNB) sont les indicateurs les plus courants.Par ailleurs, il existe d'autres indicateurs qui prennent en compte d'autres facteurs ignorés par le PNB et le PIB afin de mesurer le bien-être des habitants d'un pays ; en incluant par exemple des indicateurs de santé, d’espérance de vie, de taux d'alphabétisation. Le Programme des Nations Unies pour le développement (PNUD) a ainsi créé l'indice de développement humain (IDH) dans les années 1990.Des tentatives pour prendre en compte d'autres dimensions telles la sécurité ou pour inclure la « soutenabilité écologique » de l'activité économique dans des indicateurs ont aussi été menées plus récemment.Parmi les nombreux indicateurs économiques très souvent utilisés figurent en premier lieu le Produit intérieur brut (PIB), dont on surveille le taux de croissance afin de mesurer la croissance économique, et le Produit national brut qui permet de comparer les puissances économiques des différentes nations. Sont aussi souvent utilisés le taux d'inflation et des indices du niveau des revenus, de celui de la richesse, ou encore le salaire minimum, le salaire moyen et l'indice de Gini, lesquels fournissent divers aperçus de la répartition et de l'inégalité des revenus. De nombreux indicateurs financiers sont enfin d'usage de plus en plus courant avec l'essor de la mondialisation financière.La mesure de la production d'un pays se fait généralement par le Produit national brut (PNB) et le Produit intérieur brut (PIB). Le PIB est défini comme la valeur totale de la production interne de biens et services dans un pays donné au cours d'une année donnée par les agents résidents à l’intérieur du territoire national. C'est aussi la mesure du revenu provenant de la production dans un pays donné. Ces indicateurs correspondent au développement des comptes nationaux mis en place après la Seconde Guerre mondiale. Ils sont limités du fait des conditions historiques de leur apparition, à la fois dans leur mesure et au niveau conceptuel.Le Produit national brut (PNB) vise à évaluer la valeur des productions nationales réalisées aussi bien sur le territoire d'un pays qu'à l'étranger. Pour ce faire, il retranche du PIB les productions et services réalisés sur le territoire par les non-résidents (donnant lieu au versement de revenus hors du pays) et lui ajoute la valeur des produits et services effectués à l'étranger par des résidents (entreprises ou personnes qui ont donc reçu des paiements de revenus à l'étranger). En dehors de ces ajustements comptables correspondant à la balance des paiements, le PNB présente les mêmes défauts et qualités que le PIB.Pour évaluer la richesse, on utilise souvent le Revenu national brut (RNB) qui fournit une mesure des revenus monétaires acquis durant l'année par les ressortissants d'un pays. Cet agrégat comptable est, au niveau d'un pays, peu différent de la production nationale brute du fait que le PNB est égal à la somme des revenus bruts des secteurs institutionnels, à savoir de la rémunération des salariés, des impôts sur la production et des importations moins les subventions, de l'excédent brut d’exploitation (assimilé au revenu des entreprises) et du solde de revenu avec l'extérieur.Mais les données de patrimoine constituent de meilleures mesures de la richesse proprement dite. Il est difficile toutefois d'obtenir des évaluations comparables du patrimoine quand bien même on se limite seulement aux valeurs monétaires. Le problème devient encore plus ardu si on veut inclure des évaluations du patrimoine physique (immeubles, usines, outils de production, etc.), du patrimoine culturel (monuments, œuvres d'art présentes dans des musées, etc.), et plus encore du patrimoine social. Il serait nécessaire pour cela d'établir des conventions comptables et, si l'on veut effectuer des comparaisons internationales, de se mettre d'accord au niveau mondial sur leur utilisation. Or de telles opérations nécessitent des négociations et des accords internationaux très longs à réaliser.La montée de la mondialisation financière depuis les dérégulations impulsées par les administrations Reagan et Thatcher s’est traduite depuis les années 1980 dans un développement considérable des besoins d’information sur les évolutions des marchés financiers internationaux et les données financières relatives aux obligations d'information et aux états financiers des sociétés cotées. Depuis cette époque, les chiffres de la croissance et du PNB voisinent de plus en plus avec le spectacle des évolutions de l’euro, du dollar et du yen d’un côté, du Dow Jones, du NASDAQ, du Nikkei ou du CAC 40 de l’autre. En effet, les acteurs de la mondialisation que sont les cadres des entreprises financières ou non financières tournées vers l'exportation ont besoin de suivre quotidiennement ces variables de base de leurs arbitrages que sont les taux de change et les niveaux de valorisation boursière. Ainsi, avec la multiplication des expositions des firmes et des nations aux risques de change et aux risques financiers se développent des besoins d’indicateurs en tout genre, de « risque client » ou de « risque pays émergent », associés à chaque type de transactions. La Caisse des dépôts et consignations a créé ainsi des indicateurs synthétiques de libéralisation financière et de crise bancaire informant sur les vulnérabilités associées aux opérations financières mondialisées dans les pays émergents.Les critiques ont été historiquement nombreuses vis-à-vis des indicateurs économiques « classiques ». Marilyn Waring, première femme députée au Parlement néo-zélandais, a souligné que les tâches ménagères et le temps consacré par les parents à l’éducation des enfants, en particulier par les femmes et surtout les femmes dites « inactives », étaient occultés par les mesures de production par individu. En outre, des indicateurs comme le PIB mesurent mal l'économie informelle ou les services domestiques comme le faisait remarquer Alfred Sauvy. Enfin, ils se concentrent sur la valeur ajoutée, et non sur la richesse possédée (stock de capital). Dès lors, une catastrophe naturelle qui détruit de la richesse va pourtant contribuer au PIB à travers l'activité de reconstruction qu'elle va générer. Cette contribution ne reflète pas la destruction de capital, ni le coût de la reconstruction. Cette contradiction était dénoncée dès 1850 par l'économiste français Frédéric Bastiat qui, dans Sophisme de la vitre cassée, écrivait que « la société perd la valeur des objets inutilement détruits », ce qu'il résumait par : « destruction n'est pas profit. »Depuis la fin des années 1980, de multiples mouvements ont mis en cause les capacités du PNB à représenter toutes les dimensions du niveau de vie. Ainsi, au début des années 1990, certaines institutions internationales du système de l'Organisation des Nations unies ont fait un travail de pionnier en proposant de nouveaux indicateurs de développement. La collaboration d'économistes comme Amartya Sen, avec le Programme des Nations unies pour le développement (PNUD), a permis de proposer successivement toute une batterie de nouveaux indicateurs multidimensionnels du développement qui incluent, en plus du PNB, des critères sociaux. Le plus connu est l'Indice de développement humain (IDH). Depuis, de nombreuses autres initiatives se sont multipliées.Le PIB (défini ici comme ""la valeur monétaire des biens et services produits durant une certaine période dans un pays""), est un indicateur très superficiel car :- c'est un indicateur global, qui ne tient pas compte de la répartition de la richesse créée (même en le divisant par la population on obtient qu'un indicateur de la répartition potentielle de la richesse produite, et non de la répartition réelle) ? le PIB/population peut s'accroître avec la richesse d'une minorité de la population tandis qu'une majorité devient plus pauvre ;- c'est un indicateur à court terme, qui ne tient pas compte de l'impact de la production sur la déplétion du capital naturel.- il repose sur l'hypothèse que le prix donne une mesure ""exacte"" de la qualité de la richesse produite globalement, c'est-à-dire de la mesure dans laquelle elle répond aux besoins de l'ensemble de la population; or les sondages d'opinions montrent que les budgets militaires (qui représentent une part importante du PIB et qui sont financés par les impôts payés par la population) ont toujours été supérieurs au niveau souhaité par la population, qui préfère que l'on consacre plus de ressources au sport, à la culture ou aux transports en commun; d'autre part une partie importante du PIB résulte d'achats provoqués par le conditionnement imposé que constitue la publicité dans les lieux publics, et qui donc gonflent artificiellement le PIB.Indice de la puissance économique d'une nation, le PNB mesure la richesse d'un pays. Mais il ne fournit qu'une mesure très approximative du bien-être des habitants qui y vivent. Il ne fournit en effet qu'une agrégation comptable des valeurs des différents biens et services marchands produits, quelles que soient les utilités de ces productions. Par exemple, le PNB ne prend pas en compte les externalités négatives de la production (les dégâts causés à l'environnement, les prélèvements sur le patrimoine, etc.). Il ne mesure pas non plus l'impact de toutes les activités non monétarisées et réalisées hors du champ économique proprement dit (travaux domestiques, éducation des enfants, activités artistiques, etc. – ensemble théorisés par l'opéraïsme italien sous le nom de « travail social », et qui concerne souvent les femmes), lesquelles augmentent le bien-être général.Dans le cas des États-Unis, par exemple, le PNB agglomère indistinctement la production de biens qui ne contribuent pas directement au bien-être des habitants (aides au développement, etc.), avec celles des biens ou services produits et consommés par les américains.L'indice de développement humain (IDH) est le premier des indices créés par le Programme des Nations Unies pour le développement (PNUD). Utilisé depuis les années 1990, l'IDH combine trois facteurs permettant d'apprécier les « capacités » des résidents de ces pays (leurs capabilities selon l'économiste Amartya Sen) : l'espérance de vie à la naissance,l'accès à l'éducation, mesuré à partir de la durée moyenne de scolarisation des adultes (en années) et de la durée attendue de scolarisation des enfants en âge scolaire (en années).ainsi que le niveau de vie réel par habitant calculé à partir du logarithme du revenu national brut par habitant en parité de pouvoir d'achat (PPA).L'IDH classe les pays en établissant la moyenne entre ces trois indices principaux « normalisés » (c'est-à-dire ramenés à une échelle de 0 à 1).Le PNUD publie trois indicateurs synthétiques intégrant d'autres dimensions que l'activité économique :D'abord, à partir de 1995, l'Indicateur Sexospécifique (ou sexué) de développement humain (ISDH), qui permet de corriger l'IDH d'un facteur d'autant plus positif que les différences entre les situations des femmes et des hommes sont moins importantes du point de vue des trois critères pris en compte dans le développement humain.Puis, à partir de 1995 également, l'Indicateur de Participation des Femmes (IPF) à la vie économique et politique, lequel complète le précédent en faisant la moyenne d'un certain nombre de taux de participation des femmes à des postes politiques ou économiques valorisés.L'Indicateur de pauvreté humaine (IPH) est introduit à partir de 1997. Il est construit sous un autre principe que celui des capabilities de Amartya Sen. Il signale les manques, privations ou exclusions fondamentales d'une partie de la population en tenant compte de quatre facteurs : longévité, éducation, emploi et niveau de vie. Deux variantes de calculs sont distinguées :une variante 1 (IPH-1) pour les pays économiquement en développementune variante 2 (IPH-2) pour les pays économiquement développés. Pour les pays développés, l’IPH-2 tient compte de quatre critères auxquels il accorde le même poids : la probabilité de mourir avant soixante ans, l'illettrisme, le pourcentage de personnes en deçà du seuil de pauvreté, soit 50 % du revenu médian, le pourcentage de chômeurs de longue durée.Au début des années 2000, dans la lignée du mouvement impulsé par le PNUD, de nombreuses institutions se sont mises à discuter des limites du PNB pour tenter de les dépasser. En novembre 2004 à Palerme, l'Organisation de coopération et de développement économiques (OCDE) a organisé un premier Forum mondial de l’OCDE sur ce thème. Fin juin 2007, l'OCDE a organisé un second colloque à Istanbul portant sur « les statistiques, les connaissances et les politiques ». Il a débouché sur une déclaration énergique exhortant les bureaux statistiques du monde entier à « ne plus se limiter aux indices économiques classiques comme le produit intérieur brut (PIB) ».Face aux mises en cause multiformes de la mondialisation, il s’agissait d’abord, comme le déclarait le secrétaire général de l'OCDE Angel Gurria, de « mesurer en quoi le monde est devenu meilleur ». Afin de mettre en œuvre et généraliser cette déclaration signée par l’ONU et le PNUD, la Commission européenne a réuni les 19 et 20 novembre 2007 à Bruxelles un colloque international dénommé Beyond the GDP (Au-delà du PIB), durant laquelle son président, José Manuel Durão Barroso, défendait la mise en place de nouveaux indices pour mesurer les problèmes contemporains.Ces réunions institutionnelles ont rassemblé une large part des nombreux indicateurs alternatifs mis au point dans le monde entier afin d'évaluer le bien-être social et environnemental. Parmi ces indicateurs synthétiques alternatifs, certains concernent les problèmes sociaux contemporains, d'autres les inégalités et la pauvreté, la sécurité économique et sociale ou le patrimoine écologique.L’Indice de santé sociale (ISS) a été mis au point aux États-Unis par deux chercheurs, Marc et Marque-Luisa Miringoff. L’ISS est un indice social synthétique visant à compléter le PIB pour évaluer le progrès économique et social. C'est une sorte de résumé des grands problèmes sociaux présents dans le débat public aux États-Unis dans les années 1990. Il se traduit dans seize indicateurs sociaux dont il fait une sorte de moyenne. Sont ainsi regroupés dans cet indice des critères de santé, d'éducation, de chômage, de pauvreté et d'inégalités, d'accidents et de risques divers. L'ISS a acquis une grande réputation internationale en 1996, année de la parution d'un article majeur dans la revue économique Challenge montrant le décrochage des courbes de progression du PNB et de l'ISS aux États-Unis, le premier continuant à progresser alors que le second plongeait durablement après les années 1973-1975. Ce graphique montre ainsi en quoi les années Reagan et Bush père ont porté un rude coup à la santé sociale des États-Unis, laquelle se trouvait en 1996 à un niveau nettement inférieur à celui de 1959, en dépit d’une très belle courbe de croissance économique.Le BIP 40 est un indicateur synthétique de l'évolution des inégalités en France dont le nom est une référence ironique à la fois au PIB (inversé) et au CAC 40. Cet indicateur a été mis au point et présenté à la presse en 2002 par réaction au fait que la santé économique et la santé boursière ont droit à des indices synthétiques fortement médiatisés, alors que ce n'est pas le cas pour ceux de la « santé sociale ». Cela même si l’Insee publie de nombreuses études et indicateurs sur le sujet. L'équipe de militants syndicalistes, d'économistes et de statisticiens français qui ont agrégé des indicateurs pour former le BIP 40 est associée à un réseau associatif militant pour la réduction des inégalités, le Réseau d’alerte sur les inégalités (RAI).De façon récente, des chercheurs de grandes institutions internationales (comme Guy Standing au BIT à Genève) et de pays développés (tels Lars Osberg et Andrew Sharpe au Canada ou Georges Menahem en France) ont mis au point des indicateurs visant à cerner le degré de protection économique des personnes contre les principaux risques de perte ou de diminution forte de leurs revenus, par exemple en matière de chômage, de maladie, de retraite, etc.  L'indicateur de bien-être économique de Osberg et Sharpe Osberg et Sharpe prennent ainsi en compte quatre composantes caractérisant le bien-être des populations dans la construction d’un Indicateur du bien-être économique (IBEE) :les flux effectifs de consommation par habitant, qui incluent la consommation de biens et services marchands, les flux effectifs par habitant de biens et services non marchands et les changements dans la pratique des loisirs ;l’accumulation nette dans la société des stocks de ressources productives, y compris l’accumulation nette de biens corporels et de parcs de logements, l’accumulation nette de capital humain et des investissements en Recherche & Développement (RD), les coûts environnementaux et la variation nette du niveau de l’endettement extérieur ;la répartition des revenus, selon l’indice de Gini sur l’inégalité, ainsi que l’ampleur et l’impact de la pauvreté ;la sécurité économique contre le chômage, la maladie, la précarité des familles monoparentales et des personnes âgées.Grâce à leur indicateur ils sont en mesure de comparer les tendances d’évolution du bien-être économique dans six pays de l’OCDE : États-Unis, Royaume-Uni, Canada, Australie, Norvège et Suède. Des comparaisons sont ainsi données sur le site du laboratoire de ces deux chercheurs canadiens. Une application de l'IBEE au cas de la France a été proposée par Florence Jany-Catrice et Stephan Kampelmann en juillet 2007. L'indicateur de sécurité de Standing à l'OIT Dans les travaux de Guy Standing effectués dans le cadre du Bureau international du travail (BIT), la vision est centrée sur le travail et vise à cerner la sécurité économique dans sept domaines. Dans un deuxième temps, un indice synthétique permet d'effectuer une moyenne de ces sept domaines : les revenus (y compris les prestations sociales), la participation à l’activité économique, la sécurité d’emploi, la sécurité du travail (contre les risques d'accidents ou de maladies professionnels), la sécurité des compétences et qualifications, la sécurité de carrière, et enfin celle de la représentation syndicale et d’expression des salariés. Une série de grandes enquêtes ont ainsi été menées par les missions locales du BIT dans une vingtaine de pays. Les pays scandinaves sont à nouveau aux premières places pour cet indicateur. L'indicateur de sécurité de Menahem En France, Georges Menahem a mis au point en 2005 un indicateur baptisé taux de sécurité économique (TSE). Selon ses dernières publications, la sécurité économique peut être décomposée entre une partie « marchandisée » dépendante des relations salariales et de la vente des produits, et une partie ""démarchandisée"" relative aux prestations et aides auxquelles les individus ont droit indépendamment de leurs relations actuelles avec le marché (comme la retraite, les allocations familiales, de logement, de chômage ou le RMI). Ses estimations sur une trentaine de pays montrent que le taux de sécurité démarchandisée est un bon indicateur de l'efficacité du système de protection sociale : il est maximum en Suède et dans les pays Nordiques, il est encore important dans les pays continentaux tels l'Autriche, l'Allemagne ou la France, mais il est faible au Royaume-Uni et dans les pays Européens du Sud comme l'Italie, la Grèce ou l'Espagne et très limité dans les pays d'Europe Centrale et Orientale tels la Lettonie ou la Lituanie. Quant aux États-Unis, leur taux de sécurité démarchandisée est négatif, ce qui témoigne du mauvais état des protections sociales dans ce pays présenté comme un modèle de l'économie de marché. Ce taux n'est que faiblement positif dans deux autres exemples du modèle « libéral » selon le sociologue danois Gosta Esping-Andersen : en Australie et au Canada, à un niveau à peine plus élevé pour ce dernier car les programmes sociaux y sont plus étendus.L'Empreinte écologique est un indicateur visant à mesurer les pressions économiques sur l'environnement. L’empreinte écologique d’une population est la surface de la planète, exprimée en hectares, dont cette population dépend compte tenu de ce qu’elle consomme. Les principales surfaces concernées sont consacrées à l’agriculture, à la sylviculture, à la pêche, aux terrains construits et aux forêts capables de recycler les émissions de CO2. Il s’agit d’un indicateur synthétique, qui « convertit » en surfaces utiles de multiples pressions humaines sur l’environnement, mais pas toutes.On peut calculer cette empreinte pour une population allant d’un seul individu à celle de la planète, et par grands « postes » de la consommation. Par exemple, la consommation alimentaire annuelle moyenne d’un Français exige 1,6 hectare dans le monde ; son empreinte totale (alimentation, logement, transports, autres biens et services) est de 5,3 hectares. Pour un Américain, on obtient 9,7 hectares : le record du monde. Or l’empreinte par personne « supportable » par la planète aujourd’hui, compte tenu des rythmes naturels de régénération des ressources était de 2,9 hectares en 1970, et elle ne cesse de diminuer sous l’effet de la progression de la population, de la régression des terres arables, des forêts, des ressources des zones de pêche, etc. Elle est passée à 2 hectares en 1990 et elle n’est plus que de 1,8 hectare en 2001. Si tous les habitants de la planète avaient le mode de vie des Américains, il faudrait 5,3 planètes pour y faire face. Si tous avaient le niveau de vie moyen des Français, il en faudrait près de trois.De nombreux rapports ont déjà été produits, dont ceux particulièrement documentés et fiables du Global environmental conservation organization (soit le World Wide Fund for Nature ou WWF). Mais leurs conséquences sont limitées compte tenu de la faible visibilité dans la sphère publique de ce problème, ses conséquences négatives sur la vie quotidienne ne touchant pas encore vraiment les acteurs économiques, politiques et médiatiques dominants et les nations les plus favorisées même si leur empreinte écologique est pourtant de loin la plus importante. De ce fait, elles peuvent croire encore dans les bénéfices d’une croissance matérielle soutenue et indéfinie, les indicateurs des limites de notre planète matériellement finie étant difficiles à percevoir.L'empreinte écologique est un indicateur abstrait et synthétique qui ne traduit qu'une faible part des conséquences du dérèglement du climat et des dégradations des écosystèmes. La comparaison de l'empreinte de l'Afrique et de l'Europe montre certes que les pays les plus pauvres ont encore, pour quelque temps, une empreinte écologique par personne très supportable par la planète, ce qui permet aux pays favorisés d'utiliser bien plus que leur surface. Ainsi, les dommages restent au faible niveau des premiers signes que nous observons actuellement. Selon le WWF, ce résultat traduit une dette écologique des pays riches par rapport aux pays pauvres : les premiers « empruntant » aux seconds d’énormes surfaces de ressources naturelles, terres arables, forêts, aux pays du Sud. Tout se passe comme s'ils y exportaient leur pollution, au moins celle qui ne connait pas de frontière, à commencer par celle des gaz à effet de serre.Mais l'empreinte écologique est limitée car elle ne permet d'illustrer que très indirectement l'importance des conséquences du réchauffement climatique :L’accélération du réchauffement climatique dans la période récente directement liée aux émissions d’origine humaine de gaz à effet de serre.La dimension des catastrophes humaines mondiales prévisibles au-delà d’un réchauffement de deux degrés : sècheresses, inondations et tempêtes, élévation du niveau des mers, etc.L'importance et la diversité de ces catastrophes à venir suggère qu'il faudrait compléter l'empreinte écologique par une batterie d'indicateurs d'inégalités économiques et sociales afin d'évaluer en quoi certaines populations plus pauvres sont davantage touchées par ce que les populations riches nomment les « aléas » climatiques. La moitié de la population mondiale vit ainsi dans des zones côtières qui risquent d'être submergées si le niveau des mers s’élevait d’un mètre, évolution possible pour le siècle à venir selon le Groupe d'experts intergouvernemental sur l'évolution du climat (GIEC) si les tendances actuelles persistent.Jean Gadrey et Florence Jany-Catrice, Les nouveaux indicateurs de richesse, Édition La Découverte, 2005Dominique Méda,  Au-delà du PIB. Pour une autre mesure de la richesse, Champs-Flammarion, 2008Rapport mondial sur le développement humain 2005 : La coopération internationale à la croisée des chemins : l’aide, le commerce et la sécurité dans un monde marqué par les inégalités, New York, Programme des Nations unies pour le développement, janvier 2005, 385 p. (ISBN 2-7178-5114-3, lire en ligne), chapitre 2 « Inégalité et développement humain ».WWF: Global environmental conservation organization (ou Organisation de conservation de l'environnement mondial)PIB et développement durableHappy planet indexIndice rouge à lèvresIndicateur du développementLes grands indicateurs de l'économie française, site de l'InseeLes indicateurs économiques du Loiret , iD Loiret Portail de l’économie"
économie;"La macroéconomie est une discipline de l'économie qui étudie le système économique au niveau agrégé à travers les relations entre les grands agrégats économiques que sont le revenu, l'investissement, la consommation. La macroéconomie constitue l'outil essentiel d'analyse des politiques économiques des États ou des organisations internationales. Il s'agit d'expliquer les mécanismes par lesquels sont produites les richesses à travers le cycle de la production, de la consommation, et de la répartition des revenus au niveau national ou international.La macroéconomie est une branche de la science économique. Elle se consacre à l'étude des grandes variables économiques nationales ou internationales, et aux relations entre ces variables. Elle repose sur une approche globale, quoiqu'elle puisse se fonder sur des comportements microéconomiques et micro-ondes.Parce qu'elle fonctionne par la comparaison d'agrégats, la macroéconomie est avant-tout une représentation hiérarchisée de l'économie, articulée entre ses agents, via des flux. Elle cherche à expliciter ces relations et à prédire leur évolution face à une modification de certaines variables. La macroéconomie permet par exemple d'estimer la réaction d'un système économique possédant telles caractéristiques face à un choc pétrolier, ou à une politique économique particulière.Contrairement à la microéconomie, qui favorise les raisonnements en équilibre partiel, la macroéconomie se place toujours dans une perspective d'équilibre général, ce qui l'amène à accorder plus d'attention au bouclage des modèles et à la dynamique de création et de maintien d'institutions essentielles, comme les marchés, la monnaie.La macroéconomie a évolué à travers le temps pour devenir plus précise et plus sûre. Ses origines se trouvent dans les premiers travaux économiques du XVIIIe siècle (voir Histoire de la pensée économique), mais elle prend véritablement forme grâce aux travaux de John Maynard Keynes. Sa théorie, le keynésianisme, se fonde sur ce qui fut appelée la macroéconomie keynésienne, qui est l'interprétation keynésienne de la macroéconomie. Elle crée des outils de base de la macroéconomie (le modèle IS/LM, la courbe de Phillips, etc.).La macroéconomie n'est aujourd'hui plus rattachée à une quelconque école de pensée. Elle a évolué vers la construction de modèles économiques complexes, incluant à la fois des relations supposées entre variables et des relations comptables servant à définir les agrégats. Très utilisés pour analyser et prévoir les résultats des politiques économiques, ces vastes modèles qualifiés, le plus souvent, d'économétriques (les plus frustes comportent une dizaine d'équations, les plus complexes dépassent les 1 500) sont à l'heure actuelle employés par la plupart des gouvernements, institutions statistiques (comme l'INSEE), organisations internationales (OCDE) et certains acteurs privés voulant disposer de leurs propres prévisions quant à la conjoncture.Les penseurs de la Grèce antique, tels que Platon, Aristote et Xénophon, avancent des idées économiques. Celles-ci sont toutefois souvent liées à la gestion de la maisonnée, et l'économie désigne l'art de bien administrer sa maison. La microéconomie primitive naît ainsi avant la macroéconomie. Certains auteurs font toutefois remarquer que, chez Platon notamment, la place de l'économie de la cité est parfois pensée dans le cadre des relations géopolitiques,. Il y aurait donc des bribes de pensée macro chez les Grecs.Au XVIIe siècle, des premières réflexions sur la monnaie et l'économie agricole émergent. Les pensées développées sont assez primitives, et il faut attendre le XVIIIe siècle, avec son courant physiocrate, pour qu'un premier système macroéconomique soit ébauché. Cette représentation se trouve dans l'ouvrage de François Quesnay appelé le Tableau économique. Quesnay, médecin de la famille royale, cherchait à représenter l'économie sur les bases de la circulation du sang, comme un système cohérent et dynamique,. Certains auteurs rechignent toutefois à utiliser le terme de macroéconomie pour désigner la pensée de Quesnay, et préfèrent celui de circuit économique.Les Classiques pensent à la fois le niveau micro et le niveau macro. Adam Smith étudie les questions de répartition des richesses et de fonctionnement global de l'économie, tandis que David Ricardo marque l'histoire de la théorie du commerce international. Ils posent donc les jalons de la macroéconomie, conceptualisant le marché et sa concurrence, et mobilisant une loi générale (la loi des débouchés élaborée par Jean Baptiste de Say) qui explique l'économie comme un système. Ils ont toutefois également une pensée micro, car ils réfléchissent à la théorie de la valeur. La pensée des Classiques va jusqu'à créér des relations entre la micro et la macroéconomie. Adam Smith considère, ainsi, que la satisfaction des intérêts particuliers (niveau micro) permet la réalisation de l'intérêt général (niveau macro).Karl Marx propose à son tour une représentation schématique de l'économie industrielle de son époque. Il conceptualise le cycle monnaie-marchandise-monnaie et la baisse tendancielle du taux de profit. Il raisonne en classes sociales, comme ses prédécesseurs, quoiqu'il les redéfinisse. S'attachant à traiter d'agrégats afin de ne pas se perdre dans l'individualité, il adopte une approche macro à l'économie. Il est à ce titre cité dans les manuels de macroéconomie de référence comme un précurseur.Parallèlement, les fondateurs de l'école néoclassique ont utilisé la théorie marginaliste, pour agréger les comportements des agents économiques, c'est-à-dire les consommateurs et les producteurs. Cette microéconomie agrégée, approche souvent à la base de certaines théories macroéconomiques, est à la base de la théorie de l'équilibre général de Léon Walras, et complété par Kenneth Arrow et Gérard Debreu. Cette vision de l'économie ne peut toutefois pas se confondre avec la macroéconomie, étant donné qu'elle ne se base que sur des comportements individuels, et n'analyse pas l'économie dans son ensemble.La distinction systématique entre la microéconomie et la macroéconomie n'émerge vraiment qu'au cours des années 1930. Les travaux de John Maynard Keynes, dont notamment la Théorie générale de l'emploi, de l'intérêt et de la monnaie de 1936, indiquent un intérêt renouvelé pour l'étude agrégée des grandes variables, détachées des questionnements propres à l'économie du niveau micro. Il est question pour cette macroéconomie d'étudier les grandes variables économiques agrégées afin de fournir aux décideurs les clefs de compréhension du monde et des moyens d'action.Keynes écrit ainsi, en 1939, dans la préface à l'édition française de la Théorie générale : « J'ai appelé ma théorie une théorie générale. Par là, j'ai voulu signifier que j'étais principalement intéressé par le comportement du système économique dans son ensemble, avec des revenus globaux, des profits globaux, un produit global, un emploi global, un investissement global, une épargne globale, plutôt qu'avec des revenus, des profits, un produit, l'emploi, l'investissement d'industries, d'entreprise set d'individu particuliers. » Il continue : « je soutiens que l'on a commis d'importantes erreurs en étendant au système dans son ensemble ce qui a été établi correctement pour une de ses parties prise isolément. »L'après-guerre voit l'extension du keynésianisme et sa mise à jour. Repris aux États-Unis, la synthèse néoclassique, macroéconomique, forme le socle des politiques économiques occidentales des Trente Glorieuses. Le monde académique accepte la séparation nette entre microéconomie et macroéconomie. La microéconomie se spécialise alors sur les problèmes d'allocation des ressources par le moyen des prix relatifs (prix d'un produit, ou de plusieurs produits, exprimé par un ou plusieurs autres produits), alors que la macroéconomie étudie la production globale et le niveau des prix.Les échecs de la synthèse néoclassique à prévoir et à enrayer la stagflation (hausse des prix et baisse de la croissance) conduit, au XXe siècle, à l'émergence de nouvelles écoles de pensée qui rénovent les théories macroéconomiques dominantes. Le monétarisme, la nouvelle économie classique et la nouvelle économie keynésienne font partie de ces mouvements de pensée.Robert E. Lucas de la nouvelle économie classique critique la macroéconomie pour son manque de fondements microéconomiques. Cette critique dite de Lucas souligne que la synthèse néoclassique a formé un cadre macroéconomique qui a trop gommé la microéconomie, c'est-à-dire l'étude de la réaction des agents. En effet, l'école de la synthèse les considère généralement comme passifs et incapables d'anticiper les effets des politiques économiques.Parallèlement, les progrès réalisés par le monde académique permet la construction de modèles de plus en plus complexes et élaborés. Rendue possible par l'augmentation des capacités de calcul des ordinateurs ainsi que la généralisation des techniques d'optimisation dynamique, l'explosion des données économiques permettent à la macroéconométrie d'affiner ses capacités prédictives. La critique de Lucas mène aussi à la création de la microéconométrie.Au début du XXIe siècle, des économistes cherchent à dépasser la distinction entre microéconomie et macroéconomie. La plupart des modèles macroéconomiques actuels font l'hypothèse qu'ils ne constituent qu'une simplification de la réalité, dont ils étudient un aspect particulier, comme l'effet de l'innovation sur la croissance, ou des structures monétaires sur l'investissement. De ce fait, ils mélangent relations macroéconomiques et extensions au niveau macroéconomique de relations microéconomiques pour autant que ces extensions soient compatibles avec les faits stylisés qu'on cherche à analyser.La croissance désigne l'augmentation de la richesse produite dans un système économique. Ses causes sont l'un des sujets majeurs de la macroéconomie, qui a proposé différents modèles explicatifs. Le premier grand modèle fut le modèle de la croissance exogène. Il a été dépassé par les modèles de la croissance endogène. Au niveau de ces derniers, le prix Nobel d'économie, l'américain Paul Romer a apporté des contributions majeures.La consommation est l'une des variables majeures étudiées en macroéconomie. Elle est l'une des composantes de la croissance. Elle peut être issue des ménages, des entreprises ou de la puissance publique. La consommation peut être issue de l'extérieur, auquel cas il s'agira d'exportations. En cas d'un manque d'autosuffisance, la consommation peut s'adresser au reste du monde et correspond à des importations.La macroéconomie s'intéresse aux stocks et aux flux. Le stock est une quantité à un moment donnée. Le flux est exprimé en unité de temps (par exemple, sur un an). La dette est un stock constitué de l'ensemble des flux de déficits.La macroéconomie étudie les grands déséquilibres internationaux. Les déficits commerciaux, les déficits budgétaires, les situations de déséquilibres sur les différents marchés internationaux..Toutes les pages avec « macroéconomie » dans le titre(fr) Bernard Guerrien, Une brève histoire de la macroéconomie [lire en ligne (page consultée le 11/09/2015)]Paul Krugman et Robin Wells, Macroéconomie, De Boeck, coll. « Ouvertures économiques », 2009, 2e éd., 998 p.Gregory N. Mankiw, Macroéconomie, De Boeck, coll. « Ouvertures économiques », 2003, 3e éd., 655 p.Olivier Blanchard, Daniel Cohen, Macroéconomie, Pearson éducation, 2002,  (ISBN 2-84211-121-4)David Romer, Macroéconomie approfondie, McGraw Hill / EdiscienceMichel De Vroey, Pierre Malgrange, La théorie et la modélisation macroéconomiques, d’hier à aujourd’hui Document de travail PSE Lire en ligne(en) Gregory Mankiw,""The Macroeconomist as Scientist and Engineer"", NBER Working Paper 12349, juin 2006 Lire en ligneMichael Wickens, Analyse Macroéconomique Approfondie, De Boeck, 2010,  (ISBN 2-8041-6193-5)Gilbert Abraham-Frois, « La macroéconomie en l’an 2000 », Revue économique, vol. 52, no 3,? 2001 (lire en ligne, consulté le 28 juin 2011)Pascal Salin, Macroéconomie, PUF, coll. « Premier cycle », 1991, 400 p. (ISBN 978-2-13-043530-3, lire en ligne) Portail de l’économie"
économie;"Le marketing management ou la direction de la mercatique est le processus de gestion de l’ensemble complexe des méthodologies permettant à l’organisation d’être compétitive dans un univers concurrentiel dynamique sur différents marchés.Il ne faut pas confondre le marketing management, souvent appelé, par raccourci, « marketing », avec le concept du marketing ou l’esprit marketing, qui sont, eux, une culture organisationnelle, l’apprentissage de ces méthodologies, de leur respect et de leur intériorisation par l’ensemble des collaborateurs de l'entreprise.Les méthodologies du marketing management s'appliquent, quant à elles, et quelle que soit la taille de l'entreprise ou de l'organisation, à deux niveaux :Le marketing opérationnel : il s’agit — au niveau marque, service ou produit — de l’optimisation, de la coordination et de la combinaison des leviers (le marketing mix) permettant d'agir sur l’offre, ainsi que de la mise en œuvre d’outils et de techniques choisis pour conquérir des parts de marché.Le marketing stratégique : il s’agit du choix du/des marché(s) sur le(s) quel(s) l’entreprise ou l’organisation décide de se battre et de la définition générale de son attitude et de son positionnement face aux concurrents qui s’y trouvent.Ces méthodologies s’orientent soit vers le marketing de la demande lorsqu’il s’agit de l’appliquer à la demande exprimée par le marché ; soit vers le marketing de l’offre — ou de l’innovation — lorsqu’elle est appliquée à optimiser une offre inédite, technologique ou servicielle.Tous les bons auteurs : Gary Armstrong, Pierre-Louis Dubois, Philip Kotler, Jacques Lendrevie et Julien Lévy, etc. et toutes les grandes associations : American Marketing Association, Association française du marketing, etc. présentent leur définition du marketing management qu'ils abrègent généralement en « marketing ».L'approche du marketing est diverse. C'est, suivant les auteurs : une « méthode », un « ensemble de techniques et de méthodes », un « moyen d'action », une « stratégie d'adaptation », un « processus social et managérial », un « ensemble de décisions et d'actes de gestion », un « ensemble de moyens », une « méthodologie », une « fonction », etc. Dubois, Jolibert, Gavard-Perret et Fournier (2013) Pour Pierre-Louis Dubois, Alain Jolibert, Marie-Laure Gavard-Perret et Christophe Fournier :« Le marketing [management] est l’ensemble des processus mis en œuvre par une organisation (ou autre entité sociale) destiné à comprendre, influencer dans le sens de ses objectifs et contrôler les conditions de l’échange entre elle-même et d’autres entités, individus, groupes ou organisations afin de créer de la valeur pour l’ensemble des parties prenantes. »« Le marketing [management] est alors considéré comme un ensemble de « processus » comprenant des techniques, des méthodes et surtout une méthodologie. Ces techniques et méthodes seront « mises en œuvre  », l’action étant l’un des fondements des sciences de l’action, qui supposent à la fois connaissance, décision et action ». Armstrong, Kotler, Le Nagard-Assayag et Lardinoit (2013) Pour Gary Armstrong et Philip Kotler, adaptés par Le Nagard-Assayag et Lardinoit, le marketing [management] est un processus social et managérial :« Le marketing [management] est un processus social et managérial qui permet à des personnes ou à des organisations de créer de la valeur et de l’échanger avec d’autres, afin d’obtenir ce dont elles ont besoin et d’en retirer de la satisfaction. »« Dans le contexte plus étroit de l’entreprise, le marketing [management] suppose l’établissement de relations d’échanges rentables avec les clients, fondées sur la notion de valeur à long terme. », p. 3. Le Mercator (Lendrevie et Lévy, 2014) Jacques Lendrevie et Julien Lévy donnent, dans leur Mercator, deux définitions du marketing [management] : une définition sommaire et développée, qui soulignent l'importance de l'influence et de la valeur perçue :Une définition sommaire : « Le marketing [management] est un moyen d’action qu’utilisent les organisations pour influencer en leur faveur le comportement des publics dont elles dépendent », p. 3.Une définition développée : « Le marketing [management] est la stratégie d’adaptation des organisations à des marchés concurrentiels pour influencer en leur faveur le comportement des publics dont elles dépendent, par une offre dont la valeur perçue est durablement supérieure à celles des concurrents. Dans le secteur marchand, le rôle du marketing est de créer de la valeur économique pour l’entreprise en créant de la valeur perçue par les clients », p. 5. Houver (2016) Pour Nathalie Houver :« Le marketing [management] est un ensemble de techniques et de méthodes permettant aux entreprises de conquérir, de séduire, de satisfaire, de fidéliser ses clients tout en préservant sa rentabilité ». Hiam et Heilbrun (2016) Alexander Hiam et Benoît Heilbrun considèrent le marketing [management] comme :« Une démarche d’ensemble que les organisations (donc pas seulement les entreprises) mettent en œuvre pour créer de la valeur sur leur marché de référence (quitte à créer ce marché comme ce fut le cas pour toutes les grandes marques !) ».Les associations proposent des définitions qui ont une inertie beaucoup plus longue mais qui regroupent des synthèses nationales. American Marketing Association (2012) Pour l'American Marketing Association, le marketing management est « le processus de fixation des objectifs marketing d’une organisation (en tenant compte de ses ressources internes et des opportunités du marché), de planification et d’exécution de ses activités pour atteindre ces objectifs et de mesure des progrès dans leur réalisation ». Association française du marketing (2016) Pour l'Association française du marketing :« Le marketing management est le regroupement fonctionnel et opérationnel des méthodologies et des pratiques marketing mises en œuvre de façon coordonnée par des organisations pour atteindre leurs objectifs de compétitivité concurrentielle, ces méthodologies et ses pratiques comprenant :l’étude des différents publics, de leurs besoins, usages, désirs et aspirations ;la création d’offres de produits, de services et d’expériences ;la diffusion de ces offres dans une perspective marchande ou non.Ceci implique pour elle :l’établissement de relations équitables avec les différents partenaires de ces organisations, dans le respect des réglementations ;la prise en compte des conséquences futures de ces pratiques sur l’ensemble des parties prenantes et sur la société au sens large ».Le marketing est une façon de penser. Le marketing management est une façon de faire. « Faire du marketing » signifie, en fait, faire du marketing management.Si la formalisation et la diffusion publique des principes du marketing management datent des années soixante, avec la parution du livre de Jerome McCarthy, Basic Marketing. A Managerial Approach, dans lequel il introduit le concept de marketing mix inventé en 1942 par Neil Borden, la pratique du marketing management est beaucoup plus ancienne, le premier produit — au sens moderne du terme — à être lancé en respectant les principes étant L’Eau de mélisse des Carmes Boyer, en 1611.Peter Drucker pense, quant à lui, que le marketing management a été inventé en 1673 au Japon par Mitsui avec son grand magasin Mitsukoshi, puis, vers 1850, aux États-Unis, dans un autre domaine, par Cyrus McCormick avec ses moissonneuses-batteuses.Mais c’est très vite la création à Paris, en 1852, du grand magasin Au Bon Marché d'Aristide Boucicaut et à Philadelphie, en 1861, du Grand Dépôt de John Wanamaker.Dans le domaine des produits de grande consommation, citons, par exemple, les lancements de la sauce Tabasco en 1868, des savonnettes Ivory Soap par Procter & Gamble en 1879, des savonnettes Sunlight par Lever Brothers en Grande-Bretagne, en 1884, appliquant tous, de façon informelle, les principes du marketing management.L'invention du marketing management (ébauche)                                   L'activité économique mondiale — l'argent qui s'échange contre des biens — n'est pas homogène.Si l'on fait nettement la distinction entre :les marchés des services et produits commercialisés par des entreprises, comme IBM, Intel, GE, Airbus, Alstom, Michelin, etc., marketing business to business auprès d'autres entreprises, à des clients, acheteurs professionnels ;les marchés des services et produits de grande consommation et qui s'adresse à des consommatrices et consommateurs — c'est celui de Danone, Louis Vuitton, Coca-Cola, etc.nous parvenons à quatre grands types de marchés.Les quatre grands types de marché et de marketing.Ces quatre grands types de marché sur lesquels les marques ont des fonctions et des valeurs économiques très différentes génèrent des business modèles, des marketing managements et des marketing mix très différents. Les composants du marketing mix de Chronopost n'ont rien à voir avec ceux de No 5 de Chanel ni avec ceux d'une marque de petits pois.L'ensemble de l'activité économique mondiale peut être divisée en secteur d'activité économique — on dit aussi « industrie ». Les diverses classifications existantes s'accordent pour en dénombrer environ deux douzaines : le secteur du tourisme, l'industrie agro-alimentaire, le luxe, etc. Chacun d'entre eux va générer un marketing particulier.Les grands domaines des 24 grands secteurs d'activité économiquesPar ailleurs, quelle que soit sa taille, TPE ou multinationale ; la nature de son activité, production ou distribution ; et la nature de sa production : produits ou services, une entreprise utilise la méthodologie du marketing management dans quatre contextes différents ;d'une part, au niveau stratégique ou au niveau opérationnel ;d'autre part, pour gérer la demande exprimée par le marché ou pour gérer les nouvelles offres proposées aux marchés.Contextes qui génèrent quatre grands types de décision marketing management :I. Un marketing (management) opérationnel de la demande,II. Un marketing stratégique de la demande,III. Un marketing opérationnel de l'offre,IV. Un marketing stratégique de l'offre.Ce qui donne la matrice suivante :Les quatre grands types de décisions du marketing managementLe Marketing Resource Management est à la fois un outil, un processus et une stratégie d’organisation qui s'inscrit dans la continuité du marketing management.Le marketing resource management ou MRM regroupe un ensemble de technologies, de moyens et de processus de production associés à la création et à l’organisation des ressources marketing.Un outil de MRM est une solution technologique qui vient supporter le marketing management, de la conception des ressources marketing jusqu'à leur diffusion.2011Catherine Deydier et Olivier Dauchez, L’Eau de mélisse des Carmes Boyer de 1611 à 2011. 400 ans de bienfaits, Éditions Larivière, 2011.Emmanuelle Le Nagard-Assayag & Delphine Manceau, Le marketing de l'innovation. De la création au lancement de nouveaux produits, 2e édition, Dunod, 2011.2013Sophie Rieunier, Marketing sensoriel du point de vente. Créer et gérer l'ambiance des lieux commerciaux, 4e éd., Dunod, 2013.Pierre-Louis Dubois, Alain Jolibert, Marie-Laure Gavard-Perret, Christophe Fournier, Le Marketing. Fondements et Pratique, 5e édition, Économica, 2013.Gary Armstrong, Philip Kotler. Adapté par Emmanuelle Le Nagard-Assayag et Thierry Lardinoit, Principes de marketing, 11e édition, 2013.2014Jacques Lendrevie et Julien Lévy, Mercator, 11e édition, Dunod, 2014.Jean-Pierre Helfer, Jacques Orsoni, Ouidade Sabri, Marketing (1990), 13e édition, Vuibert, 2014.2015(de) Philip Kotler, Kevin Keller, Marc Opresnik, Marketing-Management. Konzepte - Instrumente - Unternehmensfallstudien, Pearson Studium, 2015.Philip Kotler, Delphine Manceau, Marketing Management, Pearson, 2015.Soraya Cabezon, Marine Lapierre, Internet Marketing 2016, Elenbi Éditeur, 2015.2016Nathalie Houver, Le Petit Marketing, Les pratiques clés en 14 fiches, 6e éd., Dunod, 2016.Alexander Hiam, Benoît Heilbrunn, Le Marketing pour les Nuls, 3e édition, First, 2016.Arnaud de Baynast, Jacques Lendrevie, Julien Lévy, Mercator, 12ième ed. Dunod, 2017.Philip Kotler, Hermawan Kartajaya, Iwan Setiawan, Marc Vandercammen - Marketing 4.0 : Le passage au digital, De Boeck supérieur, 2017.Gilles Bressy, Christian Konkuyt, Management et économie des entreprises 12ième ed. Ch 14 et 15, Aide-mémoire Sirey, Ed. Dalloz, 2018.Sandrine Medioni, Sarah Benmoyal Bouzaglo, Marketing digital, Dunod, 2018.Etudes critiques du marketing et de la consommation Portail du management"
économie;"La monnaie est définie par Aristote par trois fonctions : unité de compte, réserve de valeur et intermédiaire des échanges. À la période contemporaine, cette définition ancienne persiste mais doit être amendée, entre autres par la suppression de toute référence à des matières précieuses (à partir du IVe siècle en Chine) avec la dématérialisation progressive des supports monétaires, et les aspects légaux de l'usage de la monnaie — et notamment les droits juridiques qui sont attachés au cours légal et au pouvoir libératoire —, qui sont plus apparents. Ces droits sont fixés par l'État et font de la monnaie une institution constitutionnelle et la référence à un territoire marchand sous la forme d'un marché national (lié par une unité monétaire, de compte commun).La monnaie est l'instrument de paiement en vigueur en un lieu et à une époque donnée :du fait de la loi : on parle de cours légal ;du fait des usages : les agents économiques l'acceptent en règlement d'un achat, d'une prestation ou d'une dette.La monnaie est censée remplir trois fonctions principales :intermédiaire dans les échanges : la capacité d'éteindre les dettes et les obligations, notamment fiscales, constitue le « pouvoir libératoire » de la monnaie ;réserve de valeur ;unité de compte pour le calcul économique ou la comptabilité.Une monnaie se caractérise par la confiance qu'ont ses utilisateurs dans la persistance de sa valeur et de sa capacité à servir de moyen d'échange. Elle a donc des dimensions sociales, politiques, psychologiques, juridiques et économiques. En période de troubles, de perte de confiance, une monnaie de nécessité peut apparaître.La monnaie a pris au cours de l'histoire les formes les plus diverses : bœuf, sel, nacre, ambre, métal, papier, coquillages, etc. Après une très longue période où l'or et l'argent (ainsi que divers métaux) en ont été les supports privilégiés, la monnaie est aujourd'hui principalement dématérialisée : les espèces, ou monnaie fiduciaire, ne constituent plus qu'une petite partie de la masse monétaire.Chaque monnaie est définie, sous le nom de devise, pour une zone monétaire. Elle y prend la forme principalement de crédits qui font les dépôts et accessoirement de billets de banque et de pièces de monnaie. Les devises s'échangent entre elles dans le cadre du système monétaire international.En raison de l'importance de la monnaie, les États cherchent très tôt à s'assurer le maximum de pouvoir monétaire. Ils définissent la devise officielle en usage sur leur territoire et font en sorte que cette devise soit symbole et marque de leur puissance. Ils s'arrogent progressivement le monopole de l'émission des billets et des pièces et exercent un contrôle sur la création monétaire des banques via la législation et la politique monétaire des banques centrales.L'origine et l'histoire de la monnaie sont largement développées dans les articles suivants :Voir également Proto-monnaie Création du terme La notion de paléomonnaie a été proposée par Jean-Michel Servet, qui a « inventé (le) terme » vers 1976-1977, avant d'en « justifi(er) l'emploi » dans son Essai sur les origines de la monnaie paru en 1979. Il désigne une monnaie primitive. Le mot «monnaie» indique que les paléomonnaies remplissent les fonctions qui sont dévolues à une monnaie au sens élargi. Le préfixe « paléo- » signifie qu'aux yeux de Servet ces monnaies ne sont pas des antécédents des monnaies actuelles, mais des formes monétaires simples, répondant aux besoins du milieu qui les produit. En cela, il rejoint les points de vue de Karl Polanyi ou de Bronislaw Malinowski, selon lequel la culture est un tout indivisible dans lequel prend place l'ensemble des institutions. Usages Les paléomonnaies ont pour fonction de satisfaire des obligations sociales ou rituelles. Elles servent à régler des naissances, des mariages et des deuils, à déclarer la guerre ou à faire la paix et à compenser des meurtres, des injures, des offenses et des dommages physiques ou moraux. Elles sont amenées à changer de mains selon les circonstances. Certaines obligations rituelles nécessitent la détention de paléomonnaies. Il est alors possible de les acheter ou de les emprunter. Les paléomonnaies consacrent des hiérarchies dans la société. Elles peuvent constituer des moyens de pouvoir. Les formes et les usages sont variés d'une société à l'autre, voire à l'intérieur d'une société. Formes Les formes sont très diverses et variables : coquillages (travaillés ou non), dents de chien ou de marsouin, plumes collées, pierres polies, etc. Dans tous les cas les paléomonnaies ont un caractère inutile, précieux et rare.Une forme particulière de ces proto-monnaies (monnaie de commodité) est la monnaie de pierre des Îles Yap (Océanie, États fédérés de Micronésie) Caractère monétaire Jean-Michel Servet estime que les paléomonnaies sont une genèse de la monnaie. Nombre de leurs caractéristiques les rapprochent d'une monnaie. Elles ont un caractère inutile, précieux et rare. Elles sont standardisées et codifiées. Leurs techniques de fabrication sont parfois très sophistiquées. Elles sont fondées sur la confiance.Les paléomonnaies indispensables à l'exécution d'un rite ou à la réalisation d'obligations sociales sont parfois prêtables. Elles peuvent alors être soumises à intérêt.Les trois fonctions des monnaies de nos jours, unité de compte, instrument de paiement et instrument de réserve, se retrouvent dans les paléomonnaies soit seules, soit réunies. Elles codifient et rythment des activités et des biens à la manière d'unités de compte. Étant standardisées, elles préfigurent des moyens de paiement. Vu leurs rituels de conservation et le jeu des dettes et des créances, elles préfigurent un instrument de réserve.Jean-Michel Servet relève dans les rites et mythes de nombreuses sociétés une correspondance entre excréments et paléomonnaies. Il rejoint en cela les interprétations psychanalytiques de l'argent développées notamment par Sandor Ferenczi.Voir dans les paléomonnaies une genèse de la monnaie relève d'une théorie évolutionniste de la monnaie. D'autres chercheurs, notamment Jacques Mélitz, contestent cette vision. Ils estiment que le fait monétaire est le résultat d'une diffusion.L'utilisation d'un type d'objet privilégié (comme des coquillages) servant de référence pour l'établissement des prix et utilisé comme moyen d'échange, et l'utilisation d'une unité de compte par les scribes des civilisations antiques pour établir une comptabilité précise de leur empire, est considéré depuis Adam Smith comme marquant le passage d'une économie de troc à une économie de marché. La plus ancienne monnaie connue — au sens actuel du terme — fut créée par le roi de Lydie, Gygès, qui en, 687 av. J.-C., substitua aux lingots d'or des morceaux d'électrum (alliage naturel d'or et d'argent provenant de filons locaux, notamment de la rivière Pactole) dotés des caractéristiques suivantes : poids invariable, formes identiques, et marqués d'un signe authentifiant leur étalonnage.Le développement de la monnaie métallique est parallèle au développement de vastes territoires politiquement unifiés et centralisateurs tels l'Empire romain et la Chine Qin. La monnaie permet en effet de gouverner à distance, de payer les soldats et l'administration : cette gouvernance passe nécessairement par le biais d'instruments de crédit ou « lettre de change » : un document authentifié permet de débloquer une masse de métal précieux en échange d'un service.Après la chute de l'Empire romain, l'usage de la monnaie connaît une régression dans l'Europe du Haut Moyen Âge avec les restrictions au commerce et la mise en place presque partout de systèmes féodaux laissant peu de place aux libertés économiques.Au Moyen Âge, toutes les unités monétaires locales sont définies partout en référence à leur poids d'or ou d'argent. En France, les seigneurs qui parfois créent des monnaies locales sont régulièrement rappelés à l'ordre par des ordonnances ou règlements royaux, dont par une ordonnance de 1315. Le monde musulman, s'inspirant du monnayage parthe (IIIe siècle), met en place un système monétaire trimétallique.Avec le développement du commerce international, la banque, au sens moderne, fait son apparition en Europe. Venise, républicaine et indépendante, devient la plateforme monétaire du monde. Son succès repose principalement sur l'arbitrage entre les cours respectifs de l'or et de l'argent entre Orient et Occident. Elle assèche l'argent existant en Europe provoquant de nombreuses difficultés monétaires et, par ricochet, favorisant les manipulations monétaires. En contrepartie les rois de France, par exemple, usent de tous les artifices pour fausser en leur faveur le rapport entre valeur nominale des monnaies et teneur en métal. L'histoire monétaire devient celle de la production relative de l'or et de l'argent et des conséquences de la variation des taux d'échange entre ces deux métaux. Ils varieront dans des proportions de 1 à 7 et 1 à 12 entre le XIVe siècle et la fin du XIXe siècle.En 1550, dans le Royaume et territoires assujettis, le privilège de battre monnaie est limité aux villes de Paris, Rouen, Troyes, Digeon, Lyon, Grenoble, Turin, Marseille, Montpellier, Toulouse, Bayonne, Bordeaux, La Rochelle, Limoges, Poitiers, Bourges, Tours, Angers et Rhennequi sont également limité dans le fonctionnement de leur cour et parlements sur le sujet de la monnaie.La Première Guerre mondiale marque la fin des monnaies indexées sur les métaux précieux : les états européens continentaux sont dans l'incapacité de rembourser leurs dettes en or. Après la Seconde Guerre mondiale, la plupart des monnaies sont indexées sur le dollar, qui seul reste théoriquement convertible en or. La guerre du Viêt Nam mettra fin à l'étalon-or. En 1976, avec les accords de Kingston, le cours des devises devient flottant. C'est l'explosion du système monétaire international qui se traduit par la fin des parités fixes en Asie une quinzaine d'années plus tard.Par le passé, les historiens de l'anthropologie économique considéraient que la monnaie avait quatre fonctions principales (moyen d'échange — notion la plus familière —, unité de compte, réserve de valeur et norme de paiement différé). Les manuels d'économie modernes ne distinguent plus que trois fonctions, celle de norme de paiement différé (impôts, amendes) étant englobée dans les autres.Il y a eu de nombreux débats historiques sur la distinction entre ces différentes fonctions, d'autant plus que la monnaie, actif généralement accepté comme moyen de paiement, est dominée par des actifs plus rentables (tels les Bons du Trésor) aussi le terme « capital financier » est plus général pour désigner les liquidités et la fusion de l'ensemble des fonctions de la monnaie.Selon une conception élargie de la monnaie (conception substantive de Karl Polanyi), il suffit qu'un objet réponde à une de ces fonctions pour qu'il puisse être qualifié d'« objet monétaire ». Intermédiaire des échanges En l'absence de monnaie, les échanges commerciaux et relations professionnelles ne peuvent se réaliser que sous forme de troc d'un bien ou d'un service contre un autre. Pour que deux agents A et B échangent des biens X et Y, il faut que celui qui possède X préfère Y et que celui qui possède Y préfère X. C'est ce qu'on appelle la condition de « double coïncidence des désirs ». Cette condition limite le nombre de situations où le troc est immédiatement possible pour ces échanges et relations.La monnaie permet de s'affranchir des limitations du troc en constituant une valeur échangeable contre biens et services dans la mesure où les autres acteurs de l'économie l'acceptent aussi. La monnaie a pour valeur la convention collective de l'utiliser pour tous les échanges qui nécessiteraient sinon du troc ou une autre comptabilité pour des échanges différés dans le temps.Un échange d'un bien contre un autre utilise alors la monnaie comme un intermédiaire qui dissocie deux opérations distinctes : d'abord la vente du bien possédé contre de la monnaie, et ensuite l'achat du bien désiré. La fonction de moyen de paiement, quelquefois présentée comme une quatrième fonction de la monnaie est de servir d'intermédiaire commun comme moyen d'échange immédiat. En facilitant les échanges par rapport au troc, la monnaie est un outil essentiel du commerce libre[réf. souhaitée]. Contrats La monnaie facilite aussi le paiement de rémunérations de travailleurs libres qui autrement ne peut se faire qu'au pair ou plus généralement par compensation. Ces dernières méthodes sont lourdes, potentiellement arbitraires et sujettes à contentieux.La monnaie facilite l'emploi salarié, la division du travail et l'établissement des contrats. Elle donne une expression commode aux obligations privées nées de toutes les sortes de contrat, ou publiques (amendes, taxes, impôts) dès lors que la puissance publique lui donne un pouvoir libératoire.C'est une institution fondamentale pour l'économie des sociétés modernes fondées sur la liberté du travail, des productions, de la consommation et de l'épargne.Par réserve de valeur ou d'épargne, on entend la capacité que possède un instrument financier ou réel de transférer du pouvoir d'achat dans le temps. Ainsi, un bien immobilier constitue une réserve de valeur puisqu'il peut être acheté aujourd'hui et revendu dans le futur en procurant un pouvoir d'achat à son détenteur. On appelle cela un actif réel par opposition à la notion d'actifs financiers ou de titres, dont les actions et les obligations font partie.La capacité de la monnaie est pratiquement garantie à court terme : il est rare qu'elle soit amputée fortement de sa valeur du jour au lendemain, même si cela s'est déjà produit. À plus long terme le pouvoir d'achat de l'unité monétaire est réduit par l'inflation. Pour échapper à ce phénomène, les épargnants cherchent à placer leur épargne plutôt qu'à la conserver sous forme de monnaie, sauf en cas de panique.La thésaurisation de la monnaie est le placement le plus liquide. La propension collective à conserver plus ou moins « liquide » son épargne conditionne tous les marchés financiers et est suivie avec attention par les autorités monétaires. Lorsque les agents économiques accroissent leurs encaisses, c'est qu'ils se détournent des placements et la conséquence la plus fréquente est une restriction du crédit. Les paniques financières se manifestent par des ruées vers les espèces (monnaie de banque centrale) qui déstabilisent gravement le système bancaire.La monnaie est une unité de compte, un moyen standardisé d'expression de la valeur des flux et des stocks. On parle de calcul économique quand cette évaluation est faite a priori et de comptabilité quand elle est faite a posteriori. Il existe des unités de compte qui ne sont pas de la monnaie. Monnaie fiduciaire (ou corporelle) Une monnaie fiduciaire (du latin fides, la confiance) est une monnaie (ou plus généralement un instrument financier) dont les supports sont dépourvus de valeur intrinsèque et qui ne peuvent être convertis en or. Ce n'est plus la valeur des métaux précieux qui servent de gage à la monnaie mais la confiance du public. Cette confiance peut porter sur l'émetteur et lorsque l'émetteur est une banque centrale publique, la confiance se porte sur la société tout entière. L'expression de monnaie fiduciaire a été utilisée pour caractériser les monnaies de billon d'alliage métallique qui n'avaient pas de valeur intrinsèque. Mais lorsque les unités monétaires ont perdu leur définition en or — soit leur convertibilité — (le franc français en 1936, le dollar américain en 1976), c'est toute la monnaie émise par une banque centrale qui est devenue fiduciaire. Aussi, c'est le corps - d'où l'expression de monnaie corporelle - qui caractérise les billets et les pièces et non la confiance puisque, de nos jours, toutes les monnaies reposent sur la confiance. Monnaie scripturale La monnaie scripturale, littéralement écrite, est constituée des dépôts bancaires sur les comptes courants dans les banques commerciales. Ces écritures longtemps tenues dans des registres sont maintenant gérées par informatique. Ils forment l'essentiel de la masse monétaire, très loin devant les billets et les pièces. Support électronique Avec le développement des outils informatiques on assiste à une numérisation de la monnaie. Alors que la carte de paiement a déplacé la banque sur le lieu de transaction, la monnaie électronique entraîne la suppression de l'organisme de contrôle lors de l'échange. Aussi le droit limite fortement l'usage de la monnaie électronique à cause des risques de fraude qu'elle pose. Quasi monnaies et mesures de la masse monétaire La masse monétaire est une mesure de la quantité de monnaie en circulation. À l'origine la masse monétaire correspondait aux réserves d'or disponibles dans le coffre de la banque centrale. Mais l'abandon de l'étalon or et le développement de la monnaie scripturale nécessitent une nouvelle mesure. De fait il existe plusieurs masses monétaires selon les types de compte qui sont comptabilisés. En effet si un compte courant créditeur représente une dette d'une banque vis-à-vis d'une personne (la banque est engagée par la loi à fournir au détenteur du compte la somme créditée en billets de banque), il existe d'autres types de compte bancaire comme le livret A et plus généralement d'autres types de dette. Ainsi on distingue les masses monétaires :M0 appelée aussi base monétaire ou monnaie centrale représente l'ensemble des engagements monétaires d'une banque centrale (pièces et billets en circulation, avoirs en monnaie scripturale comptabilisée par la banque centrale) ;M1 correspond aux billets, pièces et dépôts à vue ;M2 correspond à M1 plus les dépôts à terme inférieurs ou égaux à deux ans et les dépôts assortis d'un préavis de remboursement inférieur ou égal à trois mois (par exemple, pour la France, le livret jeune ou le CODEVI, les livrets A et bleu, le compte d'épargne logement, le livret d'épargne populaire...) ;M3 correspond à M2 plus les instruments négociables sur le marché monétaire émis par les institutions financières monétaires (IFM), et qui représentent des avoirs dont le degré de liquidité est élevé avec peu de risque de perte de capital en cas de liquidation (ex. : OPCVM monétaire, certificat de dépôt, créance inférieure ou égale à deux ans) ;M4 correspond à M3 plus les bons du Trésor, les billets de trésorerie et les bons à moyen terme émis par les sociétés non financières.La création monétaire est le processus par lequel la masse monétaire d'un pays ou d'une région (comme la zone euro) est augmentée. Dans le système des réserves fractionnaires (réserves obligatoires déposées auprès des banques centrales) la création monétaire résultait essentiellement de l'effet multiplicateur du crédit, i.e. de la création de dette. Les banques centrales créent de la monnaie en achetant des actifs financiers comme des bons du trésor ou en prêtant de l'argent aux banques commerciales en échange d'une reconnaissance de dette. Les banques commerciales peuvent pour leur part créer aussi de l'argent en prêtant à des particuliers ou à des entreprises. Les réserves obligatoires exigées par les banques centrales étant devenues symboliques ou nulles pour ne pas nuire à la liquidité bancaire, la quantité de monnaie qui peut être créée par les banques commerciales est désormais limitée essentiellement par des règles prudentielles de solvabilité et liquidité fixés dans des traités internationaux comme Bâle III.La création de monnaie permanente peut pallier, dans certains cas, l'incapacité de la monnaie d'endettement à atteindre le niveau souhaitable du PIB.La politique monétaire est l'action par laquelle l'autorité monétaire, en général la banque centrale, agit sur l'offre de monnaie dans le but de remplir ses objectifs. Ceux-ci, définis dans le mandat de la banque centrale, peuvent recouvrir la stabilité des prix, le plein emploi, ou encore des objectifs environnementaux. Dans les pays à dominance monétaire, la stabilité des prix est son objectif principal ou unique.On formalise souvent les objectifs de la banque centrale sous la forme d'un triangle dit keynésien : la croissance, le plein emploi, l'équilibre extérieur. Depuis le début de la crise économique de 2008, les Banques centrales ont de plus en plus recours à des politiques dites non conventionnelles dont l'assouplissement quantitatif (en anglais, quantitative easing).La politique monétaire se distingue de la politique budgétaire. Ces deux politiques interagissent et forment ensemble le policy-mix.Le marché monétaire désigne le marché informel où les institutions financières – Trésors nationaux, banques centrales, banques commerciales, gestionnaires de fonds, assureurs, etc. – et les grandes entreprises (marché des billets de trésorerie), placent leurs avoirs ou empruntent à court terme (moins d'un ou deux ans). De plus avec l'adoption des changes flottants, les devises sont devenues des commodités comme les autres, un bien qui s'achète et se vend. Le marché monétaire est un élément essentiel au fonctionnement des marchés financiers.La monnaie a eu une profonde influence sur l'évolution du droit et réciproquement. L'émission de monnaie de crédit est strictement encadrée par le droit bancaire et des institutions étatiques de contrôle.En l'absence de monnaie, la sanction publique ne peut prendre que des formes physiques : confiscation de bien ; travail forcé. Elle est relativement difficile à étager. La monnaie permet de simplifier le système des amendes et de proposer des sanctions nuancées qui peuvent pour les délits sans trop d'importance ne pas entraver la vie courante des contrevenants.Dans le domaine civil l'absence de monnaie impose la compensation, c'est-à-dire la recherche d'une indemnisation en nature systématique et souvent très difficile à mettre en œuvre de façon juste et simple. L'indemnisation pécuniaire a été un grand progrès.Les pouvoirs publics sont seuls capables de donner un pouvoir libératoire à une monnaie, c'est-à-dire une capacité d'éteindre toute dette y compris les dettes fiscales et les dettes pénales ou civiles, en tout lieu et à tout moment dans la zone où un moyen de paiement a cours légal. Toutes les formes monétaires n'ont pas nécessairement cours légal. Généralement n'en sont dotés que seuls les billets émis par une Banque centrale et les pièces de monnaie. Le chèque n'a généralement pas cours légal. Il peut être refusé par les commerçants.Pourtant, inversement, il n'est pas possible d'effectuer tous les paiements avec une forme monétaire ayant cours légal. Par exemple en France, alors que l'article R642-3 du Code pénal prévoit que « le fait de refuser de recevoir des pièces de monnaie ou des billets de banque ayant cours légal est puni de l'amende prévue pour les contraventions de deuxième classe », la Cour de Cassation s'appuie sur l'article L112-5 du Code monétaire et financier qui dispose qu'« en cas de paiement en billets et pièces, il appartient au débiteur de faire l'appoint ».Les théories économiques cherchent à établir des liens entre les grandeurs comme les prix, la croissance, le chômage, l'inflation, les taux d'intérêt, les salaires...La pensée économique sur la monnaie est multiple.La théorie quantitative de la monnaie résulte du constat que les prix sont influencés par la quantité de monnaie en circulation. Cette théorie a été développée par différents auteurs dans différents pays. Son précurseur est Martin d'Azpilcueta, illustre Dominicain de l'École de Salamanque, et nous pouvons citer aussi Nicolas Copernic au XVIe siècle. Jean Bodin est le premier à la formuler, David Ricardo développe ses travaux, et c'est Irving Fisher qui formule en 1911 l'équation de la théorie quantitative de la monnaie (MV=PT) en. Elle a été reformulée par les théories monétaristes au cours des années 1970, dans une version restrictive, pour attaquer les théories keynésiennes.Pour John Keynes la monnaie n'est pas neutre, mais au contraire joue un rôle actif dans le fonctionnement de l'économie. Dans son livre Tract on Monetary Reform de 1923, il souligne que l'inflation peut conduire à la révolution, qu'une réforme monétaire est nécessaire pour reconstruire l'Europe et qu'il vaut mieux dévaluer que recourir à la déflation. Récemment, certains modèles nouveaux Keynésiens ont montré que la monnaie a un rôle à court terme sur les dynamiques économiques en fonction du niveau d'aversion au risque des ménages.Le monétarisme est un courant de pensée économique pour lequel l'action de l'État en matière monétaire est inutile voire nuisible. La réflexion sur ce thème est ancienne (cf. les écrits de Jean Bodin, David Hume, ou plus récemment Irving Fisher). Mais le rénovateur de ce courant est sans conteste l'économiste Milton Friedman (chef de file de l'École de Chicago), qui a contribué à réhabiliter et à relancer la théorie quantitative de la monnaie contre le paradigme dominant de l'époque, le keynésianisme. Ainsi la politique monétaire est réapparue sur le devant de la scène pour figurer depuis quelques années parmi les instruments essentiels de la politique économique.Le Chartalisme est une théorie monétaire. Selon cette théorie, la monnaie est une émanation de l'état. L'état crée de la monnaie en payant les personnes à son service comme les soldats et en exigeant que les dettes fiscales de ses sujets soient soldées par une certaine somme de monnaie. Les sujets sur le territoire contrôlé par l'état sont obligés de travailler ou d'échanger avec les personnes qui possèdent de la monnaie afin de payer les taxes réclamées par l'état. La valeur de la monnaie découle selon cette théorie directement des taxes qu'elle permet de solder. La monnaie représente donc une fraction du pouvoir de l'état.La quantité de monnaie est conservée lors d'un échange économique (voir aussi Atomicité (économie)). La conservation de la monnaie lors des échanges économiques implique que la monnaie tend à se répartir entre les agents économiques suivant une distribution exponentielle indépendamment de la nature des échanges. En l'absence de dette, cette distribution ne dépend que de la quantité de monnaie moyenne par agent et en présence de dette (monnaie négative) elle dépend aussi du niveau de dette autorisée.La question est : quelles sont les règles à appliquer à l'émission des billets de banque ? La querelle se produit en Angleterre, d'abord en 1810 quand la banque d'Angleterre suspend la convertibilité en métal de ses billets, puis dans les années 1840 à la suite d'une crise bancaire qui a vu la faillite de plusieurs banques, puis encore, aux États-Unis, dans les années 1870 à propos des greenbacks (Demand Note et United States Note).Le currency principle dispose que les billets remplacent les monnaies métalliques 1 pour 1. Tout billet émis peut donc être converti sans aucune difficulté ce qui assiéra la confiance et permettra de bénéficier des avantages du billet sans les risques d'insolvabilité des banques que l'on constate.Le banking principle considère que l'émission des billets doit être ajustée au besoin de l'économie qui, si elle est contrainte par le faible accroissement des ressources en métal, ne sera pas optimale. Selon cette doctrine, le fait que le public a toujours la faculté d'exiger le remboursement en or des billets suffit à en garantir la valeur, pourvu toutefois que les actifs de la banque, non seulement en or, mais aussi sous n'importe quelle autre forme (doctrine des effets réels) restent suffisants.La loi de 1844, le Banking Act, tranche la querelle au profit du currency principle, du moins en théorie puisqu'en pratique à chaque crise des mesures d'exceptions seront adoptées.La démonétisation de l'or et de l'argent a rendu cette querelle très inactuelle, elle subsiste néanmoins sous la forme de la question de la garantie des dépôts et du niveau de réserve (en monnaie banque centrale) qu'on exige des banques.L'argent métal est démonétisé aux États-Unis en 1873, dans le cadre d'un mouvement international qui verra la fin du bimétallisme au profit de l'étalon-or. La question agite fortement la vie politique américaine au point qu'un « parti de l'argent » est constitué qui aura un rôle dans toutes les élections présidentielles et législatives de la fin du XIXe siècle appuyé par les états producteurs de ce métal.La querelle durera jusque dans les années trente où Roosevelt remonétise partiellement l'argent, provoquant une raréfaction en Asie qui mettra en difficulté le régime chinois de Tchang Kai Check et favorisera involontairement la révolution communiste.Milton Friedman donnera raison rétrospectivement aux partisans du bimétallisme en montrant que la raréfaction de monnaie due à la disparition de l'argent monétaire explique pour une partie importante la récession qui a suivi.Les questions monétaires ont toujours agité les États-Unis. Après l'épisode d'hyperinflation des billets du Congrès on ressent le besoin d'une émission monétaire un peu mieux contrôlée. Une banque des États-Unis est créée en 1791 par Alexander Hamilton, dont la charte, temporaire, dure 20 ans. Elle ouvrit huit succursales, servit de dépôt pour les fonds de l'État, assura les transferts d'un bout à l'autre des États-Unis et joua le rôle de payeur général des dépenses publiques. Elle émit des billets convertibles en or ou en argent. Ces billets ne perdirent pas de leur valeur et « connurent l'estime générale ».La Constitution américaine définit strictement la monnaie et donne au Congrès (Sénat et Chambre des représentants réunis) la responsabilité des questions monétaires. Une grande querelle politique s'installa lorsqu'il s'agit de renouveler ou non la franchise de la banque. Menée par Thomas Jefferson, l'opposition au renouvellement gagna. Une seconde Banque des États-Unis vit le jour peu de temps après. Cette fois là c'est le Président Andrew Jackson qui l'étouffa.L'idée d'une banque centrale s'effaça pour longtemps (80 ans).L'avis de Jefferson était sans nuance : « J'ai toujours été l'ennemi des banques : non de celles qui acceptent des dépôts mais bien de celles qui vous refilent leurs billets de papier, écartant ainsi les honnêtes espèces de la circulation. Mon zèle contre ces institutions était tel qu'à l'ouverture de la Banque des États-Unis je m'amusais comme un fou des contorsions de ces bateleurs de banquiers cherchant à arracher au public la matière de leur jongleries financières et de leurs gains stériles. »Les banques se développeront à un rythme très rapide, surtout dans la seconde partie du XIXe siècle. Par exemple la Wells Fargo ouvrit 3 500 succursales entre 1871 et 1900. Les Westerns rendent compte de cette frénésie bancaire en montrant que dans tout village qui se crée se monte aussitôt un relais de diligence, un saloon et… une banque. Il est vrai que les colons qui accédaient à un lopin de terre n'avaient pas de ressources. La banque les leur fournissait, avec la terre comme garantie et les résultats d'exploitation comme source de remboursement. Il fallut attendre la crise de 1907 qui vit de nombreuses faillites de banques pour que l'idée d'une banque centrale assurant la fonction de « prêteur de dernier ressort » prît corps à nouveau.Mais les préventions étaient telles qu'on lui donna un nom neutre (Système Fédéral de Réserve, dit familièrement FED) et on créa dans plusieurs régions (states) un établissement similaire avec de larges pouvoirs. Ce n'est que bien après le déclenchement de la crise de 1929 et la faillite de plus de 9000 banques que la FED obtint de Roosevelt, en 1935, tous les pouvoirs d'une véritable banque centrale (1929 : 659 faillites de banque, 1930 : 1352, 1931 : 2294 ; fin 1933, près de la moitié des banques avaient disparu car 4004 banques firent faillite cette année-là). Mais ce n'est pas à la FED que l'on doit l'arrêt des faillites bancaires mais à la Société Fédérale D'assurances des dépôts, « Federal Deposit Insurance Corporation » (FDIC), qui offrit une garantie d'État aux déposants. En 1934, 62 banques cessèrent leur paiement. La crise bancaire était terminée.Note : Cette situation se répéta en 2008 où après la crise de confiance suivant la chute des bourses et la faillite de Lehman Brothers, ce sont les États qui déclarèrent garantir les déposants, et non les banques centrales.Le projet, historiquement entièrement nouveau, de créer une zone monétaire unifiée plurinationale en Europe a été une source de tensions politiques extrêmement fortes. Celles-ci ont suscité de très vives dissensions au sein des partis de gouvernement dans tous les pays concernés.Les souverainistes expliquèrent que la monnaie était un attribut fondamental de la nation qui ne pouvait être transféré et que l'abandon de la souveraineté monétaire signifi"
économie;"La parité de pouvoir d'achat (PPA) (on parle de valeurs mesurées en parité de pouvoir d'achat) est une méthode utilisée en économie pour établir une comparaison, entre pays, du pouvoir d'achat des devises nationales, ce qu’une simple utilisation des taux de change ne permet pas de faire.Le pouvoir d'achat d’une quantité donnée d’argent dépend en effet du coût de la vie, c’est-à-dire du niveau général des prix. La PPA permet de mesurer combien une devise permet d’acheter de biens et services dans chacune des zones que l’on compare.Les économistes forment un « panier » normalisé de biens et de services, dont le contenu peut être sujet à discussion (à ce sujet, voir en:Discussion and clarification of PPP).La monnaie couramment utilisée comme référence est le dollar américain, pris à une année donnée.Dans un marché global et unifié, sans coûts de transport, les produits identiques ont tous le même prix au même instant et à tous les endroits de ce marché : c'est la loi de prix unique.Cette loi, de nature microéconomique, est théorique. Elle se définit produit par produit, manufacturé ou non (par exemple le cuivre, le café, le ciment, les pneus d'une dimension donnée, une canette de boisson, un hamburger, ce dernier parfois utilisé comme indice rudimentaire de PPA à lui tout seul — l'indice Big Mac). Le monde réel fournit des exemples d'autant plus proches de cette situation théorique que les produits considérés sont mieux standardisés et moins coûteux à transporter.Pour la plupart des produits au contraire, les hypothèses sur lesquelles cette loi repose ne sont pas vérifiées, car le monde est loin d'être un marché unique : les coûts de transport ne sont pas nuls, les réglementations diffèrent en fonction des pays, les droits de douane appliqués aux importations augmentent leurs prix de vente. Par ailleurs, les coûts de fabrication varient fortement en fonction des pays : certaines ressources naturelles sont plus ou moins abondantes, le climat varie, le coût de la main-d'œuvre varie fortement. Les prix sont donc différents d'un endroit à l'autre.Cependant on peut considérer que le consommateur d'un pays substitue à certains produits plus chers dans son pays certains autres moins chers[réf. souhaitée]. Il y a donc non pas un seul produit, mais un ensemble de produits nécessaires à la vie du consommateur moyen. C'est le « panier », qui reflète les habitudes de consommation : au Japon la quantité de protéines nécessaire à la vie est apportée par du soja et du poisson alors qu'en France elle est apportée par de la viande de volaille ou de bovidés.La loi de parité du pouvoir d'achat exprime un coût égal du panier dans tous les pays ayant un niveau de vie raisonnablement comparable. C'est une loi de nature macroéconomique.Les taux de change PPA sont utilisés avant tout dans les comparaisons internationales de niveau de vie. La comparaison internationale des PIB conduit à ne pas prendre en compte les différences de prix existant entre les pays. Les écarts entre les taux de change réels et les taux de change PPA peuvent être significatifs. Ainsi, lorsque le yen, la monnaie japonaise, est surévalué, comme en 1999, le PIB par habitant paraît beaucoup plus élevé que son équivalent américain, alors que mesuré en PPA, il est en réalité beaucoup plus bas.Cette méthode permet de s'affranchir de trois problèmes :les taux de change des devises peuvent connaître des variations subites et brutales sans qu'il y ait modification des conditions économiques. Une comparaison internationale des évolutions à court terme serait faussée par une utilisation des taux de change du marché ;les devises des pays pauvres sont systématiquement sous-évaluées sur le marché des changes du fait de leur moindre productivité (c’est l’effet Balassa-Samuelson) ;certains pays fixent administrativement le taux de change de leur devise. Cela a pour effet de fausser les statistiques et les comparaisons internationales. C'était en particulier le cas des pays d’Europe de l'Est avant 1989.L'utilisation des PPA permet de s'affranchir de ces trois effets.La PPA est parfois utilisée comme un indicateur de la sous-évaluation ou surévaluation d'une devise par rapport à une autre sur le marché des changes. L'exercice est hasardeux, compte tenu des incertitudes inhérentes à cet instrument de mesure.La PPA absolue définit un cours de change entre deux monnaies. Elle est déterminée en définissant un panier de consommation dans un pays A et en évaluant le prix d’un panier « semblable » dans un pays B par la formule :                    P        P                  A                      t                          =                                            P                              t                                                    P                              t                                            ?                                                    .              {\displaystyle PPA_{t}={\frac {P_{t}}{P_{t}^{*}}}.}  où PPAt est la PPA absolue entre les deux pays sur la période t, et Pt est le prix sur la période t du panier de référence dans le pays A. L'autre pays, B, est marqué par un astérisque.Pour prendre un exemple chiffré, fictif, si un panier de produits évalués à 1 000 $ aux États-Unis a un coût moyen de 900 € en France, alors le taux de change en PPA du dollar par rapport à l'euro sera de 0,90. Ce taux est calculé indépendamment du cours de l’euro en dollar sur les marchés des changes, qui peut fortement fluctuer.La PPA relative mesure la variation de la PPA entre deux périodes. Elle s'exprime ainsi :                                                        P              P                              A                                  t                                                                    P              P                              A                                  t                  ?                  1                                                                    =                                                            P                                  t                                                            /                                            P                                  t                  ?                  1                                                                                    P                                  t                                                  ?                                                            /                                            P                                  t                  ?                  1                                                  ?                                                                          {\displaystyle {\frac {PPA_{t}}{PPA_{t-1}}}={\frac {P_{t}/P_{t-1}}{P_{t}^{*}/P_{t-1}^{*}}}}  où PPAt est le taux de change et Pt est le prix à la période t. Le pays étranger est marqué par un astérisque.Une variation de la PPA relative permet de mettre en évidence un différentiel d’inflation entre deux régions du monde.Plusieurs arguments limitent la pertinence et l’usage des PPA :les PPA peuvent varier de façon très importante suivant le choix du panier de produits. En ce sens, il est soumis aux mêmes limitations que les indices des prix ;les habitudes de consommation et les choix sont parfois très variables entre pays. Les produits consommés par les populations en dépendent et construire deux paniers équivalents est un travail très subjectif ;les différences de qualité pour deux produits mis en équivalence sont difficiles à évaluer ;les prix peuvent beaucoup varier à l’intérieur d’un même pays. Le prix d’un verre de bière est beaucoup plus élevé dans un bar sur les Champs-Élysées que dans un village du Massif central ;les prix des produits importés dépendent du taux de change. Une modification du taux de change a donc une influence sur la PPA alors que celle-ci est construite pour définir une parité de change décorrélée du marché des changes.Des indicateurs comme l’indice Big Mac, construit initialement par The Economist, ou l’indice iPod, assez frustes, sont parfois utilisés à des fins pédagogiques.Liste des pays par PIB (PPA)Liste historique des régions et pays par PIB (PPA)Programme de comparaison internationale(en) Penn World Table: tableaux de référence sur la parité de pouvoir d'achat et les revenus nationaux convertis en prix internationaux pour plus de 169 pays sur la période 1950 jusqu'à récemment. Site de l'université de Groningue.(fr) Le marché ou la PPA : Quelle base de comparaison choisir ? - Étude du FMI, mars 2007 [PDF](fr) Parités de pouvoir d’achat : mesure et utilisations - Cahier statistique de l'OCDE sur les PPA, mars 2002 [PDF](fr) Prix et salaires 2015 - Est-ce que je gagne assez pour me permettre la vie que je veux ? [PDF] Portail de l’économie"
économie;"Un processus d'affaires, également appelé processus métier ou processus d'entreprise ou processus opérationnel (en anglais « Business process »), désigne un ensemble d'activités corrélées ou en interaction qui contribue aux finalités des affaires d'une organisation. Il peut être structuré en procédés d'affaires, c'est-à dire selon ISO 19510 en un ou plusieurs ensembles définis d'activités qui représentent les étapes nécessaires pour atteindre des objectifs relatifs aux affaires, y compris les flux et utilisations d'informations et de ressources.Bien que le terme de « processus métier » soit souvent utilisé dans un sens général de manière interchangeable avec « processus d'affaires », il peut aussi être employé de manière plus restrictive pour marquer la différence entre les processus spécifiques au métier de l'entreprise et les processus plus généraux de support,,. L'idée du processus émerge au XVIIIe siècle, sans être nommée, en relation avec le concept de métier. En effet, en 1751 parait le premier tome de l'Encyclopédie ou Dictionnaire raisonné des sciences, des arts et des métiers de Diderot et d'Alembert, un ouvrage de référence qui décrit minutieusement de nombreux métiers. Le cinquième tome paru en 1755 contient par exemple un article de l'encyclopédiste Delaire relatant en détail les 18 étapes de la fabrication d'épingles, de l'arrivée du fil de laiton et son jaunissement, jusqu'à l'emballage du produit fini et ses débouchés. On ne parle alors pas encore de processus. Le terme procédé est, lui, déjà utilisé dans l'Encyclopédie pour décrire des techniques employées pour certaines étapes (exemple : procédés métallurgiques, ou différents procédés d’émaillage).En 1776, l'économiste écossais Adam Smith publie Recherche sur la nature et les causes de la richesse des nations, ouvrage dans lequel il élabore sur la base d'un exemple très similaire, le principe de la division du travail source de productivité. L'industrialisation se développe en suivant ce modèle économique basé sur le principe de spécialisation. Vers 1880, l'ingénieur américain Frederick Winslow Taylor approfondit ce principe de spécialisation avec l'organisation scientifique du travail sur laquelle se base le taylorisme. Les processus et les activités ultra-spécialisées deviennent la norme des industries de production de masse. En conséquence, les entreprises se structurent par fonction; ainsi, dès 1916, l'ingénieur français Henri Fayol identifie 6 grands groupes de fonctions, dont les fonctions techniques, commerciales, et financières.Dans les années 1950, l'ingénieur japonais Taiichi ?no cherche à fluidifier les processus de production de Toyota, et met au point les principes du juste-à-temps basé sur la réduction des délais et les inefficacités engendrés par un trop grand morcellement de l'activité et par un manque d'implication du personnel. A la même époque, l'américain Edwards Deming commence à propager sa théorie sur la qualité. Néanmoins, ces deux théories vont à l'encontre du taylorisme toujours en vogue. Ce ne sera donc que vers la fin des années 1970 que se popularisent les principes du management par la qualité totale (TQM) et la vision plus complète que celle-ci requiert sur les activités.   Les années 1980 voient alors renaître un intérêt managérial pour les processus vus dans leur globalité, que ce soit du point de vue de la stratégie, des finances ou des opérations. En 1985, l'économiste Michael Porter développe ainsi dans son ouvrage Competitive Advantage la notion de chaine de création de la valeur. Celle-ci s'articule autour d'« activités » transverses de l'entreprise et de combinaisons d'activités au travers de plusieurs entreprises. Son analyse ne s'intéresse alors pas aux aspects opérationnels ou organisationnels, mais aux avantages concurrentiels qu'offre une bonne combinaison de segments stratégiques avec des synergies entre processus. En 1986, le Consortium for Advanced Manufacturing International (CAM-I) publie de son côté Cost Management in Today's Advanced Manufacturing Environment: The CAM-I Conceptual Design, qui définit l'analyse de coûts par activité (ABC), et son corollaire, une analyse financière des processus. Enfin, en 1987, l'Organisation Internationale de Normalisation (ISO) adopte la norme de management de la qualité ISO 9001, qui se base sur une approche par processus.       Dans les années 1990, le concept de ré-ingénierie des processus d'affaires se développe, capitalisant sur l'approche par les processus et les possibilités qu'offrent les nouvelles technologies, pour « repenser les processus d'affaires ». Les processus d'affaires confortent ainsi leur place dans la boite à outils du management pour planifier, gérer et optimiser la conduite des affaires.       Un processus d'affaires correspond à un ensemble cohérent d'activités relatif aux affaires d'une organisation privée ou publique. Il regroupe des activités corrélées ou en interaction, réparties au travers des structures organisationnelles et concourant à une même finalité. Un processus d'affaires peut être découpé en processus plus simples, jusqu'à en arriver aux activités élémentaires.Les processus peuvent être catégorisés par niveau de synthèse : Au plus haut niveau, on distingue les processus d'entreprise. Ils désignent les grandes familles d'activités de l'entreprise, comme le développement de la stratégie, le développement de produits, le développement de capacités de production, et l'exécution des commandes. Ce sont des processus transverses auxquels participent plusieurs directions/fonctions d'entreprise. Une cartographie de ces processus présente de façon synthétique la structure de l'ensemble des activités, au même titre que l'organigramme présente la structure de l'organisation.A un niveau intermédiaire on distingue les processus métiers. Ils désignent des processus propres à une famille de métiers, comme les processus de production, les processus financiers, et les processus logistiques. Ce sont des processus propres à un métier (éventuellement décliné par marché ou secteur d'activité) ou à l'une des grandes fonctions de l'entreprise.Au bas de l'échelle on trouve l'activité élémentaire, pour laquelle il n'y a pas de sens à la découper davantage. La norme ISO 19510 désigne ce type d'activités par le terme « tâche ».Cette terminologie n'est toutefois pas utilisée de manière homogène. En effet, les termes « processus d'entreprise » et « processus métier » sont souvent utilisés de façon interchangeable pour désigner un processus d'affaires sans nécessairement faire la distinction qui peut exister (par exemple entre l'anglais « business process » et « enterprise process » ou entre en allemand entre « Geschäftsprozess » et « Unternehmensprozess »).Les processus peuvent également être catégorisés en fonction de leur finalités. C'est l'approche qu'a utilisée par exemple American Productivity and Quality Center (APQC), c'est-à-dire le Centre américain pour la productivité et la qualité pour son référentiel de classification de processus, dont l'un des objectifs est aussi de faciliter les comparaisons entre différentes entreprises d'un même secteur d'activité. Les processus d'affaires sont nommés :  soit par le début et la fin de l'activité, dans le cas de processus complexes, afin d'éviter la confusion avec le nom d'une direction (par exemple : « concept à prototype » au lieu de développement produit ou « prospect à commande » au lieu de ventes) ;soit à l'aide d'un verbe, pour éviter la confusion avec le résultat du processus (par exemple : « payer les factures », « traiter les commandes clients » et « facturer les clients »).Pour chaque processus on peut clarifier les limites avec les autres processus, décrire les objectifs et les activités, et identifier les entrées (par exemple matières, finances, travail, informations) qui seront consommées par le processus, les sorties (c'est-à-dire les produits par exemple matériels, financiers, ou services intangibles) qui seront produites, les entités organisationnelles ou acteurs impliqués, ainsi que les fournisseurs et clients internes ou externes.Les processus peuvent être représentés par exemple sous forme :  arborescente, pour montrer la décomposition ;de liste hiérarchique avec une numérotation décimale pour montrer les différents niveaux de décomposition ;de chaîne de valeur ou de chaîne de processus schématisant la succession d'activités ;d'un organigramme avec ou sans répartition des étapes par entité organisationnelle ou par chronologie ;d'un modèle de processus d'affaire en suivant la notation de la norme ISO/CEI 19510.Au-delà de la simple représentation textuelle ou graphique des processus d'affaires à des fins de partage de connaissances, les différentes techniques de modélisation de processus peuvent être appliqués aux processus d'affaires, à des fins d'analyse, de simulation, de conception ou de mise en œuvre.      La principale application des processus d'affaires est la conduite des affaires. En effet, selon Hammer et Champy, « Les processus n'ont pas été inventés pour que l'on écrive à leur sujet. Chaque entreprise sur la planète est constituée de processus. Les processus sont ce que les entreprises font ». L'analyse de l'entreprise sous l'angle de ses processus d'affaires et la modélisation de ceux-ci ont de nombreuses applications, et en particulier :  le management des processus d'affaires, pour gérer l'exécution des activités et suivre les performances ;le management de la qualité, pour définir améliorer la qualité de l'exécution des processus industriels et de leurs produits ;l'optimisation de processus, pour améliorer les déroulement des processus et la création de la valeur ;la réingénierie des processus d'affaires, pour repenser radicalement les activités de l'entreprise, y inclus sa stratégie ;dans les études d'impacts, par exemple pour anticiper les effets d'opérations de fusions/acquisitions ou de partenariats ;dans la méthode des coûts par activité, qui analyse la formation des coûts par activité et mesure la performance des processus transverses.Les termes processus, procédé et procédure sont voisins. Ils ont pour même origine le verbe latin « procedere », qui signifie « aller de l'avant, progresser », mais portent des nuances dans leur signification :    Un processus est un ensemble d’activités en vue d'une même finalité. Ainsi par exemple, la norme ISO 9000 dans sa révision de 2015 définit le processus comme un « ensemble d'activités corrélées ou en interaction qui utilise des éléments d'entrée pour produire un résultat escompté » ;Un procédé est une technique, une méthode ou une pratique pour obtenir un résultat déterminé. Ainsi par exemple, la norme ISO/CEI 19510 définit un procédé d'affaires comme « un ensemble défini d'activités d'affaires qui représente les étapes nécessaires pour atteindre un objectif d'affaires. il inclut les flux et utilisations d'informations et de matière » ;Une procédure est un ensemble de formalités, démarches et règles à appliquer dans une situation déterminée. Elles décrit de façon précise la succession de tâches à réaliser, les rôles et responsabilités, ainsi que les règles et critères applicables. Elle est complétée par des modes opératoires qui précisent pour chaque tâche de la procédure, les instructions à suivre par un agent pour l'exécuter.En pratique, il est souvent difficile de distinguer un processus et un procédé. En effet, selon les définitions ci-dessus, un processus se distingue du procédé en ce qu'il ne présuppose pas une technique, une méthode ou une pratique définie pour atteindre son objectif. Toutefois, lorsqu'il est analysé, que ses activités sont décrites en détail, et qu'il est mis en œuvre de manière systématique, le processus répond alors également à la définition d'un procédé. Par ailleurs, les deux termes se traduisent en anglais par « process ».  Le processus d'affaire est un terme générique qui couvre tous les processus des entreprises privées et organisations publiques. Il n'est pas limité aux processus de gestion administrative. Le terme généralise la notion de processus tel qu'il a été utilisé dans le contexte du management de la qualité. Celui-ci était en effet défini, avant la version 2015 de ISO 9001, comme un processus de transformation, ce qui lui donnait une connotation de processus industriel ou de processus de production.Le processus d'affaire ne doit pas être confondu avec la gestion par affaire. Dans le domaine de l'ingénierie, la gestion par affaire est un mode de gestion qui « a pour but d'organiser et de suivre chaque affaire du devis jusqu'à l'installation sur site et éventuellement l'après-vente ». Une affaire correspond alors à un système complexe conçu, réalisé ou configuré pour répondre à un besoin client et correspond à un projet client. La gestion par affaire correspond donc à une configuration particulière de processus (en anglais « engineer to order »).     Le processus d'affaire n'a pas nécessairement une vocation de permanence, de continuité ou de répétitivité. Un processus peut être ainsi être lui-même défini en résultat d'un autre processus. Ainsi la norme ISO 21500 définit un projet comme étant « un ensemble unique de processus, constitués d’activités (...) pour atteindre les objectifs du projet ».  Les processus d'affaires couvrent par définition toutes les activités internes de l'entreprise. Celles-ci comprennent également les activités relatives aux échanges avec l'extérieur, qu'il s'agisse de partenaires commerciaux (par exemple : clients, fournisseurs, sociétés du groupe) ou de partenaires institutionnels (par exemple : banques, investisseurs, État).  Une intégration plus approfondie des processus internes de l'entreprise avec les processus métiers de ses partenaires, par exemple à l'aide d'interconnexions entre systèmes d'information, permet de constituer une entreprise étendue et de bénéficier de gains d'efficacité dans la chaine de valeur globale. C'est une alternative flexible par rapport à une stratégie de Croissance par adjacence.Les progiciels de gestion intégré (PGI), également appelés ERP (acronyme de l'anglais « Enterprise Resource Planing » c'est-à-dire « planification des ressources de l'entreprise ») sont apparus dans les années 1970, initialement pour répondre à un besoin de planification intégrée dans le domaine de la gestion de production. Ils se sont développés dans les années 1980 pour couvrir l'ensemble des activités de l'entreprise.  Avec leurs fonctions logicielles intégrant les différentes activités et données de l'entreprise selon un modèle de processus d'affaires standardisé et éprouvé, les ERP sont un facilitateur pour la mise en œuvre de processus d'entreprise efficaces. Toutefois, l'implémentation de ces systèmes doit s'accompagner d'un changement dans la définition des processus d'affaires existants. De plus, il est nécessaire d'identifier les processus d'affaires qui correspondent au cœur de métier et qui sont source d'avantages concurrentiels pour veiller à ce que la standardisation imposée par l'ERP ne cause pas la perte de cet avantage.  Dès les années 1980, mus par la recherche de gains de productivité, certains secteurs d'activités commencent à interconnecter les systèmes d'informations de clients et des fournisseurs. On ne parle pas encore de commerce électronique, mais d'échange de données informatisées (EDI). Le but est alors de gagner en efficacité dans les processus d'affaires en automatisant la passation de commande et la facturation. En France, le secteur automobile se dote ainsi dès 1984 de GALIA pour réfléchir aux formats informatiques d'échange. En 1988, l'Organisation Internationale de Normalisation (OSI) adopte EDIFACT, une norme élaborée par l'UN/CEFACT qui définit des messages électroniques standardisés pour un large éventail de transactions commerciales. Dans les années 2000, le commerce électronique et en particulier le commerce électronique entre entreprises se développe sous l'impulsion d'internet, dont les technologies sont plus souples et moins onéreuses à mettre en œuvre que celles utilisées pour EDIFACT, Cette évolution technologique est un facilitateur pour intégrer les processus d'affaires et concevoir une gestion de la chaîne logistique étendue en bénéficiant de l’interopérabilité des systèmes. En 2004, l'ISO lance la série de normes ISO 15000, élaborée conjointement par UN/CEFACT et OASIS, qui définit la technologie ebXML qui vise à simplifier le commerce électronique et réduire les coûts par une standardisation accrue.Les technologies liées au workflow (c'est-à-dire « flux de travaux » en français) et à l'exécution de processus métiers se développent à partir des années 2000. Leur but est de faciliter la mise en œuvre informatique des processus d'affaires, avec des mécanismes qui vont automatiser la coopération entre plusieurs systèmes (dont des systèmes des clients ou des fournisseurs) pour dérouler les étapes dans le bon ordre, et en appliquant les règles.    Ces technologies reposent en particulier sur des moteurs de workflow et les chorégraphies de services web. Techniquement, ces solutions ont été rendues possibles suite à d'un côté l'émergence de langages informatiques de description de processus d'affaires, dont XPDL en 1998, et surtout BPEL en 2004, de l'autre le recours accru à des architectures informatiques orientées services et des services web, deux technologies facilitant l’interconnexion et l’interopérabilité de systèmes. Toutefois, c'est la naissance de la technique de modélisation BPMN en 2004 et son adossement à BPEL à partir de 2006, qui permettra de lier la technologie au monde métier des processus d'affaires.      EntrepriseFonctionnement et organisation de l'entrepriseBusiness Process Model and Notation (BPMN), norme internationale ISO/IEC 19510:2013 intitulée ""Modèle de procédé d'affaire et notation de l'OMG""Les processus d'affaires dans le contexte d'ebXML Portail du management   Portail d’Internet   Portail du commerce"
économie;"La stagnation économique, ou plus simplement stagnation, est une période de faible croissance économique (mesurée à l'aide du PIB). Le taux de croissance du PIB est inférieur à la croissance potentielle.L'idée d'une stagnation économique est ancienne en science économique. Thomas Malthus développe une pensée (malthusianisme économique) centrée sur le rapport entre la démographie et la stagnation de l'économie. Plusieurs siècles plus tard, Alvin Hansen conceptualise et précise la notion de stagnation séculaire. Elle désigne, selon lui, une situation économique où la fin de la croissance démographique et du progrès technique conduisent à une période d'activité économique anémique. La stagnation séculaire est médiatisée dans les années 2000. Elle est reprise par Lawrence Summers. La lente reprise après la grande récession de 2007-2008 et la baisse du taux d'inflation à des valeurs inférieures à 2 % sont des signes qui rappellent les craintes exprimées par Hansen. Le seuil d'un taux d'intérêt nul peut empêcher d'atteindre l'égalité entre épargne et investissement et le plein emploi. Il est possible de voir apparaître une déflation déstabilisante avec un taux d'intérêt réel encore plus élevé. Par ailleurs, des taux d'intérêt très bas voire presque nuls mettent en péril la stabilité financière avec la création de bulles spéculatives.Les autres raisons sont la faible hausse de la population active, la hausse des inégalités, le progrès technique et la technologie de l'information qui diminuent la demande de biens d'équipement.Les craintes de Hansen ne se sont pas vérifiées à cause du baby boom et des nouvelles découvertes technologiques. Aujourd'hui, il y a toujours des optimistes en ce qui concerne les effets du progrès scientifique mais personne ne prévoit un nouveau baby boom.D'autres économistes, comme Robert J. Gordon, ont travaillé sur cette théorie notamment dans le domaine de la productivité (ou de l'offre). Du point de vue de Gordon, l'informatique génère peu de gains de productivité par rapport aux inventions et innovations précédentes, de sorte que la croissance économique pourrait être plus faible à l'avenir.Ralentissement économiqueCrise économiqueCroissance économiqueDépression économiqueStagflationG.B. Eggertsson, N.R. Mehrotra, A Model Of Secular Stagnation, NBER Working Paper, Cambridge, 2014B. Eichengreen, Secular Stagnation: The Long View, NBER Working Paper, Cambridge, 2015Alvin Hansen, "" Economic Progress and Declining Population Growth "", American Economic Review, 1939, p. 1-15C. Teulings and R. Baldwin, Secular Stagnation: Facts, Causes and Cures, CEPR Press, 2014C. Homs, Stagnation séculaire ou agonie du capital ?, avril 2016. Portail de l’économie"
économie;Le taux de croissance économique est un indicateur utilisé pour mesurer la croissance de l'économie d'un pays d'une année sur l'autre. Il est défini par la formule suivante qui relie les produits intérieurs bruts (PIB) de l'année N et de l'année N-1 :                    t        a        u        x                 d        e                 c        r        o        i        s        s        a        n        c        e        =                                            P              I                              B                                  a                  n                  n                  e                  e                                     N                                            ?              P              I                              B                                  a                  n                  n                  e                  e                                     N                  ?                  1                                                                    P              I                              B                                  a                  n                  n                  e                  e                                     N                  ?                  1                                                                    ×        100              {\displaystyle taux\ de\ croissance={\frac {PIB^{annee\ N}-PIB^{annee\ N-1}}{PIB^{annee\ N-1}}}\times 100}  où les PIB sont mesurés en volume (pour éviter de considérer l'inflation des prix comme de la croissance économique). Ou ln (PIB annee N \ PIB annee N - 1).On peut également utiliser les valeurs des PIB en prix, en mesurant les PIB des années N et N-1 en prix constants (prix en base 2000 par exemple).Le taux de croissance est généralement mesuré annuellement (en moyenne annuelle) ou trimestriellement (d'un trimestre au suivant).Les économistes ont eu des difficultés à calculer un taux de croissance mondiale en raison de la disparité des économies et de la déconnexion des cycles économiques d'un pays à l'autre. Ils ont préféré évaluer les cycles économiques mondiaux par décennie. Ainsi, les très fortes croissance mondiale des années 1830 et croissance mondiale des années 1850, sont suivies par la Grande Dépression (1873-1896). De même, la grande dépression des années 1930 fait suite à la croissance économique de la Belle Époque et à la puissante expansion des années 1920. Ensuite, la très forte croissance des années 1950, socle des Trente Glorieuses, est portée par la reconstruction après la guerre.Le calcul de taux de croissance ne se limite bien sûr pas au PIB, et le taux de croissance de toute autre variable se calcule de la même manière.Le taux de croissance annuel composé (en anglais : CAGR ou Compound Annual Growth Rate) est le taux qui respecte l'équation suivante :                              C          A          G          R                (                  t                      0                          ,        t        )        =                              (                                                            V                  (                  t                  )                                                  V                  (                                      t                                          0                                                        )                                                      )                                              1                              t                ?                                  t                                      0                                                                                      ?        1              {\displaystyle \mathrm {CAGR} (t_{0},t)=\left({\frac {V(t)}{V(t_{0})}}\right)^{\frac {1}{t-t_{0}}}-1}  Il s'agit d'un ratio à progression géométrique qui donne un taux de croissance constant sur la période étudiée                     (        t        ?                  t                      0                          )              {\displaystyle (t-t_{0})}  .Le taux de croissance annuel moyen (TCAM) permet de calculer un taux d'évolution moyen sur une durée de n périodes. D'autres dénominations existent, telles TAMA (Taux Annuel Moyen d'Accroissement), TAAM (Taux d'Accroissement Annuel Moyen), ou TAMV (Taux Annuel Moyen de Variation). Formule Le taux de croissance annuel moyen, exprimé en pourcentage, sur                     n              {\displaystyle n}   périodes (années, mois, semaines, etc.) est donné par la formule :                    T        C        A        M        =                  (                                                                                                                                                                                                      valeur finale                                                                                                                                                                                                              valeur initiale                                                                                                              n                                                      ?            1                    )                      {\displaystyle TCAM=\left({\sqrt[{n}]{\cfrac {\text{valeur finale}}{\text{valeur initiale}}}}-1\right)}  Exemple : si entre 1997 et 2008, le montant des crédits distribués est passé de 48003,1 à 249012,3 millions de francs congolais, le taux de croissance annuel moyen sur ces onze années est donné par la formule                              (                                                                                          249012                    ,                    3                                                        48003                    ,                    1                                                                    11                                                      ?            1                    )                ?        16        ,        14        %              {\displaystyle \left({\sqrt[{11}]{\frac {249012,3}{48003,1}}}-1\right)\approx 16,14\%}  Soit un taux d'accroissement annuel moyen de 16,14 %Il faut préciser que ce chiffre est exprimé en pourcentage. Intérêt L'intérêt du TCAM est de fournir une indication sur le taux de croissance moyen sur une période donnée. Et la comparaison de deux TCAM permet, par suite, de comparer les fluctuations du phénomène observé relativement à ces deux périodes. Par exemple, en économie, une application du TCAM est de comparer la croissance pendant les Trente Glorieuses (1945-1975) avec la croissance de la période suivante (1975 à aujourd'hui).Cet outil de calcul est en outre employé en démographie pour décrire le taux d’accroissement de la population entre deux recensements (solde démographique relatif). Limites Le TCAM donne une moyenne des évolutions annuelles mais ne tient pas compte des variations internes de la période étudiée. En effet seules les valeurs initiales et finales entrent dans son calcul. Exemples Population en Lorraine en 2007 : 2 339 881 habitantsPopulation en Lorraine en 2012 : 2 349 816 habitantsTCAM =                     [                                            2349816                              /                            2339881                                      5                                      ?        1        ]        =        0.085        %              {\displaystyle [{\sqrt[{5}]{2349816/2339881}}-1]=0.085\%}  Interprétation : Entre 2007 et 2012 la population lorraine a augmenté de 0,085 % tous les ans. Démonstration de la formule On vérifie simplement qu'en multipliant la valeur initiale V1 par (1+T) n fois on obtient bien la valeur finale V2 :                    1        +        T        =                                                            V                                  2                                                            V                                  1                                                                    n                                            {\displaystyle 1+T={\sqrt[{n}]{\frac {V_{2}}{V_{1}}}}}                                V                      1                          ×                              (                                                                              V                                  2                                                            V                                  1                                                                    n                                                                          )                                            n                          =                  V                      2                                {\displaystyle V_{1}\times {\Biggr (}{\sqrt[{n}]{\frac {V_{2}}{V_{1}}}}{\Biggr )}^{n}=V_{2}}   Portail de l’économie   Portail de la finance   Portail des probabilités et de la statistique
économie;"L'économétrie est une branche de la science économique qui a pour objectif d'estimer et de tester les modèles économiques.L'économétrie en tant que discipline naît dans les années 1930 avec la création de la société d'économétrie par Irving Fisher et Ragnar Frisch (1930) et la création de la revue Econometrica (1933). Depuis lors, l'économétrie n'a cessé de se développer et de prendre une importance croissante au sein de la science économique.L'économétrie théorique se focalise essentiellement sur deux questions, l'identification et l'estimation statistique.L'économétrie appliquée utilise les méthodes économétriques pour comprendre des domaines de l'économie comme l'analyse du marché du travail, l'économie de l'éducation ou encore tester la pertinence empirique des modèles de croissance.L'économétrie appliquée utilise aussi bien des données issues d'un protocole expérimental, que ce soit une expérience de laboratoire ou une expérience de terrain, que des données issues directement de l'observation du réel sans manipulation du chercheur. Lorsque l'économètre utilise des données issues directement de l'observation du réel, il est fréquent d'identifier des expériences naturelles pour retrouver une situation quasi-expérimentale. On parle parfois de révolution de crédibilité, terme controversé, pour désigner l'essor fulgurant de ces méthodes de recherche dans la discipline, et en économie en général.L'économétrie est née autour des années 1930. Elle hérite toutefois des développements de la statistique réalisés au cours du XIXe siècle et au début du XXe siècle. avec la création de la société d'économétrie et de la Cowles commission aux États-Unis d'une part et au département d'économie appliquée de l'université de Cambridge au Royaume-Uni d'autre part.Néanmoins, certains auteurs comme Mary Morgan ou Philippe Le Gall se sont attachés à montrer qu'il a existé avant les années 1930 des programmes scientifiques qui se rapprochent de l'économétrie, notamment par la volonté de rapprocher l'économie et les statistiques. En Angleterre, William Stanley Jevons avait tenté de combiner ainsi l'économie et les statistiques. Aux États-Unis, Henry Ludwell Moore avait effectué un effort similaire en 1908. En France, Le Gall voit dans les travaux d'Augustin Cournot, de Jean-Edmond Briaune et de Jules Regnault des précurseurs de l'économétrie au XIXe siècle. Ensuite, il considère une seconde génération au début du XXe siècle avec Lucien March, Henry Bunle et Marcel Lenoir.Le premier prix Nobel de sciences économiques fut attribué en 1969 conjointement aux deux principaux fondateurs de l'économétrie : Ragnar Frisch et Jan Tinbergen. Au cours des années 1940, la Cowles Commission, un groupe de recherche travaillant à l'université de Chicago puis à l'université de Yale, a construit les bases de la méthode économétrique en relation avec l'analyse économique, le calcul des probabilités et la statistique mathématique.C'est avec la création de la société d'économétrie et de la Cowles Commission que l'économétrie se dote d'un cadre institutionnel.La société d'économétrie est fondée le 29 décembre 1930 à Cleveland.En 1930, Ragnar Frisch et Irving Fisher fondent la Société d'économétrie (Econometric society) dont l'objet essentiel est de « favoriser les études à caractère quantitatif qui tendent à rapprocher le point de vue théorique du point de vue empirique dans l’exploration des problèmes économiques », puis en 1933, Frisch crée la revue Econometrica qui devient le principal véhicule de la pensée économétrique.Les travaux de la Cowles Commission for Research in Economics (groupe de recherche créé en 1932 à l'université du Colorado, qui s'installe à l'université de Chicago puis à l'université Yale.On attribue l'origine du terme économétrie à Ragnar Frisch. Dans son éditorial du premier numéro de la revue Econometrica, il définit les objectifs de l'économétrie : « Son principal objectif devrait être de promouvoir les études qui visent à l'unification des approches quantitatives théoriques et empiriques des problèmes économiques et qui sont mues par une pensée constructive et rigoureuse similaire à celle qui domine dans les sciences naturelles (Frisch 1933). »En 1935, Jan Tinbergen présente au meeting de la société d'économétrie de Namur un premier modèle économétrique. Il est ensuite embauché par la société des Nations pour tester la pertinence de la théorie du cycle des affaires et publie ses résultats en 1939,. John Maynard Keynes s'oppose vivement à la méthode de Tinbergen.La première avancée importante de l'économétrie provient d'une solution formelle au problème d'identification. On dit qu'un modèle est identifiable si tous ses paramètres peuvent être obtenus à partir de la distribution jointe des variables observables.Les travaux de la Cowles commission portent essentiellement sur l'identification et l'estimation du modèle à équations simultanées.En 1944, Trygve Haavelmo publie un article fondamental dans Econometrica intitulé The Probability Approach in Econometrics dans lequel il défend l'idée que les modèles économiques doivent être probabilistes de manière à pouvoir être cohérents avec les données.1945/1950 : Lawrence Klein présente les premiers modèles dont la solution est obtenue par la méthode du maximum de vraisemblance.1954 : Henri Theil et Robert Basmann introduisent la méthode des doubles moindres carrés.1955 : premier modèle prévisionnel conçu par Klein/Goldberger.Les années 1960 ont vu le développement des modèles décrivant l'activité économique par des systèmes d'équations de grande taille (notamment les travaux de Lawrence Klein, Prix Nobel d'économie en 1980, de Henri Theil, ou ceux plus méthodologiques de Denis Sargan).À partir du milieu des années 1970, les méthodes issues de l'analyse des séries chronologiques ont profondément imprégné l'économétrie (méthodes de George E. P. Box et Gwilym M. Jenkins, modèles dynamiques multidimensionnels et théorie de la non-causalité développés par Clive Granger et Christopher Sims). La microéconométrie est également apparue dans les années 1970 et 1980, grâce en particulier aux travaux de Zvi Grilliches, ainsi que de Daniel L. McFadden et James J. Heckman, tous deux Prix Nobel d'économie en 2000.Dans les années 1960 et années 1970, l'avancée des technologies de l'information entraîne l'apparition de modèles macroéconomiques conçus à des fins de prévision. Par exemple, le modèle de Brookings comprend 400 équations. Après 1970 furent utilisés des modèles standards comme celui de Wharton.Le développement de la puissance de calcul des ordinateurs et de bases de données microéconomiques ont permis le développement de la microéconométrie avec les travaux de James Tobin sur le modèle tobit, de Mundlak sur les modèles à effets fixes (1961), les travaux de James Heckman sur les modèles de sélection, les travaux de Daniel McFadden sur les modèles de choix discret et les travaux de James Heckman et Burton Singer sur les modèles de durée (1984),.À partir des années 1960, on assiste à la naissance de l'économétrie des données de panel. On appelle panel des données dans lesquelles on observe une unité statistique (individu, entreprise, ménage, État) à différents moments dans le temps. Ces données permettent de contrôler l'hétérogénéité individuelle qui ne peut être mesurée par les variables observées. Diffusion des travaux de la société d'économétrie en France Marianne Fischman et Emeric Lendjel ont analysé l'intérêt que portent les membres du groupe X-CRISE à l'économétrie dès les années 1930. Ils partagent en effet avec les membres de la société d'économétrie le souci de comprendre la crise économique. Robert Gibrat constitue avec Georges Guillaume au sein de X-crise un groupe de travail sur l'économétrie. Il propose régulièrement des « Notes sur l'économétrie » permettant aux membres d'X-crise de se tenir au courant. François Divisia participe à la société d'économétrie. Toutefois, les auteurs expliquent qu'X-crise a davantage été un lieu de diffusion des travaux économétriques qu'un lieu de recherche.Dans une étude publiée en 2006, les économistes Kim, Morse et Zingales ont classifié les articles publiés dans les principales revues d'économie et ayant reçu un grand nombre de citations selon que leur contribution principale était théorique, empirique ou méthodologique. Au début des années 1970, seulement 11 % des articles les plus cités étaient empiriques alors qu'à la fin des années 1990, 60 % des articles les plus cités sont des études empiriques et quantitatives. Cette évolution témoigne d'une transformation profonde de la science économique qui a été critiquée notamment en n'étant pas aussi descriptive qu'elle pourrait le laisser croire.Les différents domaines de l'économétrie partagent une méthodologie statistique commune. Des applications plus spécifiques utilisent des procédures propres de traitement des données que nous évoquerons dans les deux sections suivantes.Les méthodes statistiques de l'économétrie sont construites à partir du modèle de régression qui est une structure mathématique décrivant la réaction d'une variable à d'autres variables en présence d'éléments aléatoires inobservables. Soit une grandeur Y (par exemple la demande d'un ménage ou le nombre de chômeurs) que l'on observera pour différents ménages ou différentes périodes de temps.L'économétrie des modèles mal spécifiés qui se demande quelles caractéristiques du phénomène peuvent être exhibées ou supprimées par l'approximation a été développé pour surmonter les limites des modèles de régression. Une autre stratégie pour surmonter les limites des modèles de régression consiste à rendre de plus en plus complexes les modèles étudiés. Les modèles linéaires sont remplacés par exemple par des polynômes ou par des modèles non linéaires plus élaborés. On peut aussi adopter, ce qui est de plus en plus courant, une approche « non paramétrique ».Malgré ses différents prolongements, le modèle de régression n'est pas adapté à l'estimation de modèles structurels. La théorie des équations simultanées constitue la première extension du modèle de régression ayant permis de considérer une situation d'équilibre ; elle a joué un rôle majeur en économétrie. Les modèles développés dans le cadre de cette théorie sont caractérisés par plusieurs équations, comme le modèle d'offre et de demande précédemment évoqué, dont la résolution permet de calculer l'équilibre. Très schématiquement, la considération de plusieurs équations transforme des variables exogènes en endogènes. Les méthodes statistiques habituelles (estimation des moindres carrés) ne permettent alors pas d'estimer les équations structurelles et de nouvelles procédures statistiques ont dû être établies (doubles moindres carrés, triples moindres carrés).À la suite des travaux notamment de Lars P. Hansen, une approche assez générale de l'économétrie structurelle s'est développée : la méthode des moments généralisée (G.M.M.). Chaque agent ou type d'agent est représenté comme effectuant ses choix par une opération de maximisation d'un objectif, ce qui se traduit par la résolution d'une équation. Cette équation s'obtient en annulant les dérivées partielles par rapport aux variables de choix de l'objectif, et constitue la condition du premier ordre de la maximisation. On suppose que cette condition n'est en fait réalisée qu'en moyenne (d'où le nom de méthode des moments), ce qui permet l'introduction d'erreurs et de variables inobservables. L'ensemble des comportements des différents agents est modélisé alors par un système de conditions dont se déduit l'estimation des éléments inconnus à partir des données observées. Les travaux les plus récents de l'économétrie relâchent la notion de moyenne et associent de manière plus complexe les relations de comportements des agents économiques et la distribution de probabilité des données observées. Dans des modèles de choix financiers, le rendement moyen d'un placement n'est pas suffisant et on introduit par exemple des probabilités d'événements rares comme la faillite d'une entreprise ou une crise boursière majeure. Approche structurelle et approche en forme réduite On oppose souvent l'approche structurelle à l'approche en forme réduite. Dans l'approche structurelle, l'économètre part d'un modèle économique formel et cherche à identifier et estimer les paramètres du modèle à partir d'observations de certaines quantités prédites par le modèle. Par exemple, dans un modèle de demande, les paramètres de la fonction d'utilité des consommateurs déterminent la courbe de demande sur un marché ; en observant la quantité de demande à différents niveaux de prix, il s'agit alors d'estimer les paramètres de la fonction d'utilité qui pourraient donner lieu à ces niveaux de demande. De même dans un modèle d'offre, dans lequel la quantité d'offre sur un marché dépend notamment de paramètres de la fonction de production des entreprises sur ce marché, l'observation des quantités d'offre permet sous certaines conditions d'identifier les paramètres de la fonction de production sur ce marché.Dans l'approche structurelle, le modèle économique prédit une relation précise entre les données observées par l'économètre (par exemple les quantités de demande à certains niveaux de prix) et les paramètres que l'on cherche à estimer (par ex. les paramètres de la fonction d'utilité des consommateurs). En observant l'un, on peut donc estimer l'autre relativement précisément. Mais le risque de l'approche structurelle réside dans le fait qu'avec un modèle économique différent, les mêmes données donneraient lieu à des paramètres estimés différents. La qualité de l'estimation des paramètres dépend donc de l'utilisation du modèle économique approprié et donc à des hypothèses fortes qu'il n'est pas toujours possible de défendre autrement que par des arguments de plausibilité.Dans l'approche en forme réduite, au lieu d'estimer toutes les équations ou paramètres d'un modèle, l'économètre estime une version simplifiée du modèle,. Il s'agit moins d'obtenir une quantification précise et davantage de déterminer si deux quantités ou objets d'études dépendent positivement ou négativement l'un de l'autre, souvent de manière causale (par ex. savoir si une année supplémentaire à l'école entraîne une diminution du risque de grossesse pendant l'adolescence). L'avantage de cette approche est d'être plus agnostique et donc plus robuste à l'utilisation d'un modèle économique erroné; en contre-partie, la quantification est moins précise, et l'on ne peut pas nécessairement utiliser les données pour distinguer les mécanismes économiques en jeu.L'objectif de l'économétrie théorique est de déterminer quelles conclusions peuvent être tirées à partir des données et d'un ensemble d'hypothèses. Les problèmes peuvent être séparés en deux grandes classes : les problèmes d'identification et les problèmes d'inférence statistique. Les problèmes d'identification visent à déterminer quelles conclusions pourraient être obtenues si on observait une quantité infinie de données. À l'inverse, les problèmes d'inférence statistique (ou d'estimation) cherchent à déterminer quelles conclusions peuvent être tirées à partir d'un nombre fini d'observations.Dans un article de 1949 intitulé « Identification problems in economic model construction », Tjalling Koopmans définit la notion d'identification comme l'étude des conclusions que l'on pourrait tirer si l'on connaissait la distribution de probabilités des observations.Historiquement, la notion d'identification s'est construite autour du problème de simultanéité. Le problème de simultanéité vient notamment lorsque l'on veut identifier la courbe d'offre et la courbe de demande à partir de l'observation des quantités échangées sur un marché et des prix.Parmi les approches non-structurelles, un programme de recherche s'est développé autour de l'évaluation des politiques publiques et du modèle causal de Neyman-Rubin. Dans ce programme de recherche, on considère un modèle économique très simple dans lequel on définit pour chaque individu deux variables d'intérêt, l'une qui correspond au cas où l'individu ne reçoit pas le traitement (ie la politique que l'on souhaite évaluer) et l'autre qui correspond au cas où l'individu reçoit ce traitement. Pour chaque individu, l'une des deux variables est observée et l'autre est contrefactuelle. On attribue généralement ce modèle à Donald Rubin et Jerzy Neyman. Dans ce programme de recherche, on cherche soit à organiser des expériences aléatoires à grande échelle pour évaluer l'effet du traitement, soit à trouver des situations quasi-expérimentales permettant d'évaluer l'effet du traitement de manière convaincante,.Dans la littérature sur l'évaluation des politiques publiques, les expériences de terrain sont considérées comme la méthode la plus pertinente pour évaluer l'effet d'une politique publique. Ces méthodes sont de plus en plus populaires, et, corrélativement, considérés comme les plus fiables à tel point qu'on parle parfois de révolution de crédibilité, terme controversé.La microéconométrie étudie empiriquement les comportements des agents économiques à partir d'observations produites par les instituts de statistique, les administrations ou les entreprises. Le premier type d'agent économique est constitué par les entreprises, et on étudie les fonctions de production qui résument les relations technico-économiques entre les quantités produites et les quantités de facteurs de production (travail, matières premières, capital, etc.) ou les fonctions de coût qui relient le coût de production aux quantités produites et aux prix des facteurs de production. Ces secondes relations ont un caractère plus économique car elles incorporent la stratégie de minimisation des coûts de l'entreprise. Les consommateurs constituent le second type d'agent économique ; on examine les dépenses qu'ils consacrent à différents types de biens et leur épargne. La fonction de demande d'un bien exprime la dépense consacrée à ce bien en fonction de son prix, du revenu du ménage, de caractéristiques socio-démographiques, de prix de biens substituts, etc. En général, le ménage est l'unité de l'observation, mais certains modèles très fins examinent le processus de décision au sein même du ménage.Les différents aspects du marché du travail sont abondamment étudiés. L'importance du chômage a par exemple entraîné la réalisation de modèles de durée du chômage où cette dernière dépend des caractéristiques de l'individu, du mécanisme d'arrivée d'offres de travail, des mesures d'aide au chômeurs et du type de sortie du chômage. La figure 3 donne un exemple de résultat de ce type de modèle en présentant un taux de sortie estimé du chômage pour deux types de population. Plus généralement, les économètres (et les démographes) ont étudié les mécanismes de transition des individus entre les différents stades définissant la situation individuelle vis-à-vis du marché du travail (formation, emploi stable ou précaire, chômage, inactivité, retraite, etc.).Une attention particulière a été portée au marché du travail des femmes, à la décision de travailler (modèles de participation) et au niveau de cette participation. Ces études ont surtout été réalisées dans les pays d'Europe du Nord où la participation féminine est plus faible et le travail à temps partiel plus développé qu'en France. Les économètres du travail s'intéressent également aux mécanismes de détermination du salaire et donc au rôle de la qualification des travailleurs. L'économie de l'éducation est un champ important de la modélisation économétrique.La référence au modèle élémentaire du marché concurrentiel n'est souvent pas pertinente pour décrire l'interaction d'un petit nombre d'agents. On utilise alors des formalisations issues de la théorie des jeux. Supposons, par exemple, que l'on souhaite étudier les réponses d'un petit nombre d'entreprises à un appel d'offres. On utilisera un modèle qui décompose le prix proposé pour chaque entreprise en un coût de production plus une marge dont l'importance dépend de la stratégie de la firme qui arbitre entre l'accroissement des bénéfices et la baisse de la probabilité de gagner le marché.Les modèles de choix qualitatifs sont une composante importante des méthodes statistiques de la microéconométrie. On constate que la décision d'un agent économique repose souvent sur deux ou un nombre fini de possibilités (utiliser les transports en commun ou son véhicule personnel, acheter ou non son logement, choisir une énergie pour le chauffage de sa maison, etc.). Le but de l'étude est alors d'expliquer par un ensemble de variables ce choix dichotomique ou polytomique. De nombreux modèles ont été examinés pour traiter des choix qualitatifs en relation notamment avec la théorie microéconomique du consommateur. Les travaux de Daniel McFadden portent notamment sur cette modélisation.L'économètre doit aussi souvent expliquer simultanément un choix qualitatif et une variable quantitative associée à ce choix. À l'occasion d'une enquête sur l'emploi, on étudie par exemple une population féminine. Certaines femmes travaillent et leur salaire est observable. Celles qui ne travaillent pas (par choix ou par contrainte) ont toutefois un salaire potentiel (ou latent) par nature inobservable. Il serait incorrect de négliger dans l'étude du salaire le fait que ce dernier n'est observé que pour une certaine catégorie de salariées ; ce biais de sélection doit être corrigé par des méthodes statistiques adaptées. Ces méthodes ont été développées en particulier par James Heckman.De tels traitements de données sont habituels en statistique, mais l'originalité de l'économétrie au sein de la recherche en statistique est de considérer les mauvaises qualités des données (données partiellement observées, non-réponses, etc.) comme résultant en partie de décisions de comportement des agents économiques et comme une caractéristique essentielle du phénomène étudié. Ainsi que nous l'avons déjà souligné, un élément important de cette modélisation consiste à introduire des variables aléatoires inobservables, caractéristiques de l'hétérogénéité individuelle et explicatives en particulier des biais de sélection.Les données macroéconomiques ou financières sont généralement des séries chronologiques, c'est-à-dire des grandeurs observées à des périodes de temps différentes. L'objectif est d'analyser la dynamique des variables considérées, plus précisément, leur évolution, la propagation de la variation de l'une d'entre elles sur les autres, leurs causalités, leurs variations saisonnières. L'étude approfondie de ces phénomènes dynamiques est une composante essentielle de l'économétrie.Le point de vue le plus simple et le plus descriptif est le traitement d'une unique variable observée à des périodes différentes. On étudie les interdépendances temporelles de cette variable de manière à modéliser sa valeur en t en fonction de ses réalisations à des périodes antérieures.L'élaboration de modèles dynamiques complexes examinant plusieurs séries conjointement étudie la détermination d'une variable par sa propre histoire et par le passé d'autres grandeurs. Les modèles vectoriels autorégressifs (V.A.R.) constituent à cet égard une classe de modèles très populaire en économétrie.Un ensemble de séries chronologiques est qualifié de stationnaire si le mécanisme générateur de ces variables ne se modifie pas dans le temps. La stationnarité implique par exemple que l'effet à six mois d'une hausse des prix sur les salaires a été de la même nature dans les années 1970 et dans les années 1990, ce qui est certainement faux. En général, les séries économiques ne satisfont pas la condition de stationnarité. La non-stationnarité est encore un moyen de prendre en compte la spécificité de chaque période. Ce n'est qu'au début des années 1980 que l'étude explicite de la non-stationnarité a figuré au premier plan de la recherche économétrique.Dans les modèles de rupture, les relations elles-mêmes ou les paramètres qui les caractérisent peuvent se modifier. Ces modèles répondent à ce que l'on appelle la critique de Robert E. Lucas (Prix Nobel en 1995), qui peut se résumer par l'argument suivant : une mesure de politique économique va avoir un double effet ; un effet direct, décrit par les relations (si le revenu des ménages augmente la consommation augmente) et un effet sur les comportements des consommateurs (les arbitrages consommation-épargne vont se modifier) qui se traduit par une modification des relations. Ces deux effets peuvent se compenser et rendre difficile la prévision macroéconomique. Formellement, les modèles de rupture se ramènent à des relations non linéaires parfois discontinues entre variables.Les années 1970 consacrent également la remise en cause des modèles macroéconométriques traditionnels. Notamment parce que, à la suite de leur inefficacité à expliquer et prévoir la stagflation consécutive aux chocs pétroliers, ils seront accusés de ne pas posséder suffisamment de fondations microéconomiques. Lucas montre par exemple dès 1972 le lien entre les anticipations des agents économiques et la variation des coefficients structurels des modèles macroéconométriques. Sa conclusion est alors que toute mesure de politique économique conduit à un changement dans le comportement des agents, et que par conséquent, ces agents sont à même de contrer les politiques gouvernementales en les anticipant. Ce qui réduit considérablement l'intérêt des politiques budgétaires et monétaires.En économie de la croissance, Gregory Mankiw, David Romer et David Weil utilisent un modèle de régression linéaire pour tester empiriquement la pertinence du modèle de Solow. Ils montrent que le modèle de Solow augmenté du capital humain est cohérent avec les données observées.Daron Acemoglu, Simon Johnson et James Robinson utilisent une régression linéaire pour estimer l'effet des institutions sur le développement actuel des pays.Dès 1975, Isaac Ehrlich a utilisé des méthodes économétriques pour mesurer l'effet dissuasif de la peine de mort.Steven Levitt utilise un modèle linéaire à variables instrumentales pour estimer l'effet causal du nombre de policiers sur la criminalité.Il existe une importante littérature sur les choix éducatifs et l'estimation des rendements privés de l'éducation. Michael Keane et Kenneth Wolpin estiment un modèle de choix d'éducation et d'occupation pour une cohorte d'hommes américains nés en 1979. Ils montrent que leur modèle permet d'obtenir des prédictions cohérentes avec les données.Il existe aussi de nombreux travaux cherchant à trouver les déterminants de la réussite scolaire. Parmi ces travaux, certains portent sur la taille des classes. Joshua Angrist et Victor Lavy (en) ont utilisé une méthode de régression sur discontinuité pour évaluer l'effet causal de la taille des classes sur la réussite scolaire des enfants à partir des données israéliennes. Les auteurs utilisent la règle de Maïmonide qui veut qu'il n'y ait pas plus de 40 élèves par classe comme une expérience naturelle pour trouver des variations de la taille des classes indépendantes du niveau des élèves.Une frange significative de l'économétrie s'attache à l'estimation des fonctions de productions. On peut prendre l'exemple de l'article de Steven Olley et Ariél Pakes.La fonction de production d'une entreprise détermine comment l'entreprise transforme des ressources, appelées facteurs de production (par exemple, de capital et de travail), en produits finis. L'estimation de la fonction de production de chaque entreprise permet de mesurer sa productivité, c'est-à-dire le rapport entre la quantité de ressources utilisées et la quantité de biens ou services produits. Avec cette estimation, il devient alors possible de mesurer non seulement la productivité moyenne des entreprises d'un secteur et son évolution dans le temps, mais aussi de mesurer les écarts de productivité, par exemple entre les entreprises d'un même secteur, ou d'étudier la contribution de chaque entreprise à la productivité de son secteur ou de l'économie en général.L'estimation économétrique des fonctions de production n'est pas forcément simple. Le choix des facteurs de production par une entreprise n'est pas indépendant de son niveau de productivité, cela implique qu'une application directe de la méthode des moindres carrés donnerait des résultats statistiquement biaisés. Pour remédier à cela, Olley et Pakes furent les pionniers d'une sous-branche qui suggère d'autres techniques d'estimation, reposant notamment sur une approche avec variable instrumentale.La cliométrie est une école d'histoire économique quantitative utilisant les méthodes de l'économétrie pour comprendre l'histoire.Il existe une controverse  entre les tenants d'un programme d'économétrie structurelle au sens fort et les défenseurs des méthodes quasi-expérimentales et du modèle causal de Neyman et Rubin. Par exemple en 2010 dans le Journal of Economic Perspectives, les économètres Joshua Angrist et Jörn-Stephen Pischke défendent l'idée que l'économétrie est devenue crédible grâce au développement des méthodes quasi-expérimentales et de l'approche de Neyman et Rubin. À l'inverse, Michael Keane défend l'ambition du programme structurel et l'idée qu'il est nécessaire d'avoir un modèle économique pour interpréter les paramètres estimés et faire des analyses ex ante des politiques publiques.EconometricaJournal of EconometricsJournal of Applied EconometricsThe Econometrics Journal(en) Ragnar Frisch, « Editor's Note », Econometrica, vol. 1, no 1,? janvier 1933, p. 1-4 (JSTOR 1912224)(en) Trygve Haavelmo, « The Probability Approach in Econometrics », Econometrica, vol. 12,? juillet 1944, iii-vi+1-115 (JSTOR 1906935)(en) Tjalling C. Koopmans, « Identification problems in economic model construction », Econometrica,? 1949, p. 125-144 (JSTOR 1905689)(en) James Heckman, « Sample Selection Bias as a Specification Error », Econometrica,? 1979, p. 153-161 (JSTOR 1912352)(en) James Heckman et Burton Singer, « A Method for Minimizing the Impact of Distributional Assumptions in Econometric Models for Duration Data », Econometrica,? 1984, p. 271-320 (JSTOR 1911491)(en) John Rust, « Optimal Replacement of GMC Bus Engines: An Empirical Model of Harold Zurcher », Econometrica, vol. 55, no 5,? 1987, p. 999–1033 (DOI 10.2307/1911259, JSTOR 1911259)(en) Joshua D. Angrist et Alan B. Krueger, « Chapter 23 – Empirical Strategies in Labor Economics », dans Handbook of Labor Economics, 1999 (DOI 10.1016/S1573-4463(99)03004-7)(en) Charles Manski, Identification Problems in the Social Sciences, Harvard University Press, 1995(en) Charles Manski, Partial Identification of Probability Distributions, Springer-Verlag, 2003Régis Bourbonnais, Econométrie, Dunod, 11e Edt., 2021, 416 p.Edmond Malinvaud, Méthodes statistiques de l'économétrie, Dunod Saint-Amand, 1964, 1re éd., 634 p.Christian Gouriéroux et Alain Monfort, Statistique et modèles économétriques, vol. 1, Economica, 1995, 2e éd. (1re éd. 1989)Christian Gouriéroux et Alain Monfort, Statistique et modèles économétriques, vol. 2, Economica, 1995, 2e éd. (1re éd. 1989)(en) Jeffrey Wooldridge, Econometric Analysis of Cross Section and Panel Data, Cambridge, MIT Press, 2002, 7e éd., 776 p. (ISBN 978-0-262-23219-7, LCCN 2001044263, lire en ligne)(en) Fumio Hayashi, Econometrics, Princeton University Press, 2000, 690 p.Luc Behaghel, Lire l'économétrie, Paris, La Découverte, 7 août 2006, 120 p. (ISBN 978-2-7071-4748-6)Brigitte Dormont, Introduction à économétrie, Paris, Montchrestien, 2007, 518 p. (ISBN 978-2-7076-1398-1)(en) Jeffrey Wooldridge, Introductory Econometrics : A Modern Approach, South-Western, Division of Thomson Learning, 2008, 4e éd., 865 p. (ISBN 978-0-324-78890-7, LCCN 2008921832)(en) Colin Cameron et Pravin Trivedi, Microeconometrics : Methods And Applications, Cambridge University Press, 2005, 1056 p. (ISBN 978-0-521-84805-3, lire en ligne)(en) Joshua Angrist et Jörn-Steffen Pischke, Mostly Harmless Econometrics : An Empiricist's Companion, Princeton, Princeton University Press, 2008, 392 p. (ISBN 978-0-691-12035-5, LCCN 2008027917)(en) William Greene, Econometric Analysis, Harlow (homonymie), Pearson Education, 2011, 7e éd., 1232 p. (ISBN 978-0-273-75356-8, OCLC 712554018)Claudio Araujo, Jean-François Brun et Jean-Louis Combes, Econométrie : licence, master, Rosny, Bréal, 2008, 2e éd., 312 p. (ISBN 978-2-7495-0301-1)Valérie Mignon, Econométrie : théorie et applications, Economica, 2008, 400p.(en) N. Kærgaard, « The earliest history of econometrics : Some neglected Danish contributions », History of Political Economy, vol. 16,? 1984, p. 437-444(en) R.J. Epstein, A History of Econometrics, Amsterdam, North Holland, 1987(en) Mary Morgan, The h"
économie;"Le taux d'inflation optimal est un taux d'inflation hypothétique qui doit assurer à un système économique une augmentation saine de son niveau des prix.Le concept de taux d'inflation optimal est débattu par des économistes, académiques comme institutionnels. Les banques centrales se questionnent sur l'existence d'un tel taux car leur mandat exige souvent qu'elles assurent un taux d'inflation qui corresponde aux besoins de leur économie.Le monétariste Milton Friedman, dans un papier de 1969. Il considère que la banque centrale doit maintenir son taux d'intérêt nominal à 0, car un taux d'intérêt positif provoque une distorsion de l'économie. L'inflation devrait ainsi être négative. L'école de la nouvelle économie keynésienne a longtemps considéré le taux d'inflation optimal comme 0 %. Cela se base sur deux arguments. Le premier est celui des coûts des menus ; le deuxième est celui de la mauvaise allocation des ressources. En effet, si l'inflation est autre que zéro, les entreprises doivent modifier leurs prix, mais comme cela est difficile et lent du fait des rigidités nominales, cela n'est pas possible dans l'immédiat et crée une dispersion des prix, et donc une mauvaise allocation des ressources.En plus de cela, l'inflation a un effet redistributif adverse. L'inflation affecte les couches sociales de manière différenciée ; il s'agit d'une taxe sur les pauvres, qui subissent l'augmentation de l'inflation de plein fouet dans leur consommation quotidienne.La Banque centrale européenne a estimé un taux d'inflation optimal pour la zone euro à 2 % lors de sa revue stratégique de 2021. Ce taux d'inflation a plusieurs qualités, comme le fait d'être assez faible pour éviter une modification fréquente des prix par les agents de l'offre (coût des menus). Il évite aussi une volatilité de l'inflation. De plus, cette cible d'inflation assure une marge de sécurité contre le risque de déflation.Une cible d'inflation élevée permet de donner une marge de manœuvre aux banques centrales. Elles ne peuvent, en effet, faire baisser leurs taux directeurs en-dessous du taux plancher zéro ; or, si la cible d'inflation est de 2 % ou 3 %, la banque centrale dispose d'une plus grande marge pour faire baisser les taux directeurs.Certains économistes, comme Xavier Ragot, soutiennent qu'un niveau d'inflation aux alentours de 3 % est nécessaire sur le long terme pour huiler le système économique et atteindre un niveau optimal d'investissement. Il obtient ce résultat à partir d'un modèle à générations imbriquées. Ce modèle montre en effet que les banques centrales doivent faire baisser le taux d'intérêt réel, en présence de contraintes de crédit, pour stimuler la croissance ; or, pour faire baisser ce taux, elles peuvent jouer sur l'inflation. Cet effet est également remarqué par Weil (1991), Weiss (1980) et Summers (1981). Portail de la finance   Portail de l’économie"
économie;"Le prix, exprimé en un montant de référence (en général monétaire), est la traduction de la compensation qu'un opérateur est disposé à remettre à un autre en contrepartie de la cession d'un bien ou un service. Le prix mesure la valeur vénale d'une transaction et en constitue l'un des éléments essentiels.Le mécanisme de formation des prix est un des concepts centraux de la microéconomie, spécialement dans le cadre de l'analyse de l'économie de marché, où les prix jouent un rôle primordial dans la recherche et la définition d'un prix dit « d'équilibre » (alors qu'ils jouent un rôle plus mineur dans une économie administrée).Les niveaux de prix possibles sont en nombre potentiellement infini, selon les acteurs économiques, selon leurs estimations de la valeur de la chose pour eux-mêmes et pour les autres (spéculation). Si une transaction se réalise effectivement, le prix traduit le compromis entre les estimations de l'acheteur et celles du vendeur (reflet de l'offre et la demande).Le mécanisme de détermination des prix peut être affecté par d'autres facteurs :éventuelles imperfections régnant sur le marché (monopole, oligopole, pénurie, marché noir, etc.),contraintes légales lorsqu'il en existe (les prix n'étant pas toujours libres : « prix imposés » ou « administrés »),considérations techniques, telles que la méthode de mise en marché (commerce national et international, commerce de gros, commerce de détail, enchères, etc.) ou les contraintes que cela implique (délais de transmission des offres, définitions des priorités entre offres, ...).Selon l'objet concerné, le périmètre et la méthode de détermination du prix varie. On rencontre ainsi différentes sortes de prix :le prix d'achat.le prix de vente, qui indique le prix auquel un commerçant déclare être disposé à céder la chose et qui ne doit pas être inférieur au coût de revient (interdiction légale de la vente à perte) ;le prix de revient, censé refléter l'ensemble des dépenses liées aux intrants et à la fabrication d'un produit ou d'un service ; Le prix de revient ou coût de revient est égal au coût de production majoré des frais de transport ;le prix d'acceptabilité ou prix psychologique, qui définit le prix qu'une grande partie de la clientèle trouve justifié pour l'acquisition d'un bien ou d'un service ;le prix de cession, qui indique le prix auquel est facturée une cession entre deux services d'une même entreprise ou entre deux filiales d'un même groupe. En matière de comptabilité des entreprises, les prix de cession concernent les biens immobiliers (qui ont une longue durée de vie à l'instar des constructions ou des terrains non bâtis) par opposition aux prix de vente qui concernent, eux, les produits courants c'est-à-dire ceux qui sont relatifs à l'activité normale de l'entreprise.Raymond Barre distingue plusieurs types de prix en fonction du degré de liberté du marché (prix libres, prix administrés), du stade d'élaboration du produit (prix du gros ou de détail) ou encore de la nature des produits vendus (prix des produits agricoles, industriels ou des services)L'importance du système de prix libres a été mise en avant et débattue en particulier dans les années 1920-1930.Une vive controverse sur la question du calcul économique oppose les économistes de l'école autrichienne d'économie, Ludwig von Mises puis, ultérieurement Friedrich Hayek, aux tenants du socialisme de marché, Oskar Lange au premier chef. Pour Ludwig von Mises, le système de prix libres est le seul moyen de coordination des actions des millions d'individus qui composent l'économie d'un pays. Friedrich Hayek relaie cette idée et insiste pour sa part sur le rôle des prix comme vecteur de transmission de l'information disponible aux individus.L'économiste Milton Friedman résume cela en écrivant que le système de prix libres remplit trois fonctions :transmission de l'information sur l'offre et la demande ;.incitation pour les producteurs à s'orienter vers les secteurs aux prix élevés et, partant, à permettre un retour à l'équilibre ;répartition des revenus.Dans une économie planifiée, les prix n'ont pas la même importance. L'appareil productif peut s'en passer : au lieu de chercher à maximiser la valeur ajoutée de sa production comme il le ferait dans une économie de marché, un producteur peut se voir attribuer un quota de matières premières et un objectif de production ; les prix sont fixés par les pouvoirs publics à un niveau considéré comme « souhaitable », mais ils ne sont pas directement connectés aux décisions d'allocations des matières premières ou d'objectif de production, qui sont fixés par ailleurs. Il peut en résulter une pénurie (file d'attente et marché noir) ou un rationnement, si le prix est inférieur à l'utilité pour les consommateurs, ou des excès de production dans le cas contraire.En outre, certaines situations (par exemple, la guerre) incitent les autorités à recourir au contrôle des prix (ou du moins du prix de certains produits jugés nécessaires), ou à influer sur l'offre (protectionnisme, subvention...) et la demande.En réalité, la liberté totale des prix est rarement constatée, même dans les économies réputées les plus libérales, notamment à cause de l'impact de la fiscalité, de lois anti-dumping, des subventions, des engagements pris dans le cadre de contrats pluri-annuels, etc.Sur un marché libre le prix reflète l'équilibre entre l'offre et la demande. Mais les auteurs classiques (Adam Smith, David Ricardo, John Stuart Mill, ...) et Karl Marx considèrent qu'il est soumis plus aux influences de l'offre (coût exprimé par une certaine quantité de travail) que de la demande. Pour Karl Marx l'équilibre tend à se fixer autour de la valeur du travail incorporé. Ricardo estime également que le ""prix réel"" correspond à la quantité de travail incorporé mais constate que le ""prix courant"" est fonction de l'offre et de la demande. Le prix courant aurait tendance à se rapprocher du prix naturel. Selon Adam Smith le prix se dissocie de la ""valeur réelle"" car il tient compte de la valeur de la monnaie qui, elle, est variable. À partir de la fin du dix-neuvième siècle, les auteurs marginalistes (Léon Walras, Stanley Jevons, ...) estiment que le prix ou ""la valeur d'échange"" ne dépend pas de l'offre mais de la demande et donc de l'utilité exprimée par le consommateur. Malgré les influences de la demande sur la détermination du prix du marché admises par les marginalistes, Alfred Marshall considère que, de toute façon, on ne peut pas se passer du concours des deux (i.e l'offre et la demande) pour la fixation du prix. André Orléan estime que la fixation d'un prix peut s'établir par mimétisme et non en fonction du travail incorporé (côté offre) ou de l'utilité (côté demande),. Pour Jacques Perrin, les institutions jouent ou doivent jouer un rôle dans la constitution des prix en prenant en compte l'utilité sociale. Le prix n'est pas donc déterminé par l'unique confrontation de l'offre et de la demande qui sont exprimées par des agents économiques ""rationnels"". Elles subissent d'autres influences (psychologiques, sociologiques et institutionnelles). Par ailleurs, l'offre et la demande sont exprimées dans le temps. Ce dernier peut avoir une influence capitale dans les décisions des producteurs et des acheteurs. Avant de produire, de vendre ou d'acheter, ils procèdent notamment à des études futures du marché pour exploiter les opportunités avantageuses et éviter les menaces sources de risques majeurs.Les libéraux, en faisant appel au concept du consommateur-roi de Paul A. Samuelson, considèrent que les consommateurs, par leur pouvoir d'augmenter ou de baisser librement la demande exprimée sur le marché des biens de consommation, déterminent les prix et donnent le signal aux entreprises d'augmenter ou de baisser l'offre conséquente. Par conséquent, toute chose étant égale par ailleurs, les entreprises vont augmenter ou diminuer les demandes portant sur les marchés du travail, des biens de production et des capitaux déterminant ainsi les taux de salaire, d'intérêt et les prix sur le marché des biens de production. Cependant, John K. Galbraith a montré, dans les années 1960, que le fonctionnement réel de l'économie contemporaine ne correspond pas à ce schéma théorique. Les entreprises, en agissant sur le marché des biens de consommation (études des besoins du consommateur, étude de la concurrence, promotion des ventes) conditionnement la demande du consommateur aussi bien sur ce marché que sur ceux des biens de production, du travail et des capitaux et le privent de toute initiative. De plus, la liberté du consommateur est contrariée par la dépenses budgétaires de l'État visant à accroître les investissements publics pour augmenter la croissance économique. Cette nouvelle stratégie des entreprises est appelée par John K. Galbraith la "" filière inversée "".L'évolution des prix n'est pas l'inflation (l'augmentation du niveau général des prix), qui ne mesure le prix que par de la monnaie, alors que l'évolution des prix en général dépend du fonctionnement de l'économie, qui modifie le prix relatif des biens (i.e le prix d'un bien exprimé par d'autres biens). Cependant la mesure du prix de la monnaie ne peut être fait qu'indirectement, par mesure du prix d'un panier représentatif de biens : si le prix de ce panier augmente, c'est que la valeur (relative) de la monnaie diminue, et inversement.Il existe différents indices de prix pour différentes classes de biens et pour différents usages :les prix à la consommation sont mesurés par l'Indice des prix à la consommation (IPC ou, en anglais, CPI) ;les prix à la production sont mesurés séparément, et correspondent aux Coûts de production ;l'indice du coût de la construction ou l'indice de référence des loyers mesurent l'évolution du prix du logement ;etc.Pour un bien, on parle de « prix nominal » lorsque l'on fait référence au prix exprimé dans une monnaie donnée. On parle de « prix réel » lorsque l'on extrait du prix nominal la part due à l'évolution de la valeur de la monnaie, c'est-à-dire l'inflation.Les prix définissent une distance dans l'espace des commodités préservant la valeur des échanges.Soit A et B, Alphonse et Brigitte, deux agents économiques qui possèdent chacun les vecteurs commodités a et b, par exemple                     a        =        (        1        ,        1        ,        3        ,        0        ,        10        )              {\displaystyle a=(1,1,3,0,10)}   et                     b        =        (        0        ,        1        ,        4        ,        1        ,        100        )              {\displaystyle b=(0,1,4,1,100)}   avec l'ordre (voiture,table,chaises,machine à laver,monnaie), et p le vecteur des prix, la distance comparant la richesse des deux agents est définie par                    d        (        A        ,        B        )        =        d        (        a        ,        b        )        =                  |                          ?                      i                                    p                      i                          (                  a                      i                          ?                  b                      i                          )                  |                      {\displaystyle d(A,B)=d(a,b)=|\sum _{i}p_{i}(a_{i}-b_{i})|}  Cette pseudo-distance définit une relation d'équivalence dans l'espace des commodités qui préserve les écarts de richesse lors d'un échange. Par exemple si d(A,B) est de 100 et A et B échangent une même valeur de commodités d(a',b')= 0, i.e. A donne a' à B et B donne b' à A, après l'échange d(A,B) est toujours égale à 100. Cette préservation de la valeur ne serait pas vérifiée si d était une autre distance.Contrôle des loyersJuste prixLoi du maximum généralMarginalismeOffre et demandeParadoxe de l'eau et du diamantPrix prédateursValeurDistinction entre l'acompte et les arrhes(en) Price Theory par Milton Friedman(en) Theory of Price par George StiglerAndré Orléan, L'empire de la valeur, Seuil, 2011(en) Le système de prix libres, Henry Hazlitt(en) Four Thousand Years of Price Control, Ludwig von Mises Institute Portail de l’économie   Portail du management   Portail du commerce"
Informatique;"Un algorithme est une suite finie et non ambiguë d'instructions et d’opérations permettant de résoudre une classe de problèmes.Le mot algorithme vient d'Al-Khwârizmî (en arabe : ?????????), nom d'un mathématicien persan du IXe siècle.Le domaine qui étudie les algorithmes est appelé l'algorithmique. On retrouve aujourd'hui des algorithmes dans de nombreuses applications telles que le fonctionnement des ordinateurs, la cryptographie, le routage d'informations, la planification et l'utilisation optimale des ressources, le traitement d'images, le traitement de textes, la bio-informatique, etc.Un algorithme est une méthode générale pour résoudre un type de problèmes. Il est dit correct lorsque, pour chaque instance du problème, il se termine en produisant la bonne sortie, c'est-à-dire qu'il résout le problème posé.L'efficacité d'un algorithme est mesurée notamment par :sa durée de calcul ;sa consommation de mémoire vive (en partant du principe que chaque instruction a un temps d'exécution constant) ;la précision des résultats obtenus (par exemple avec l'utilisation de méthodes probabilistes) ;sa scalabilité (son aptitude à être efficacement parallélisé) ;etc.Les ordinateurs sur lesquels s'exécutent ces algorithmes ne sont pas infiniment rapides, car le temps de machine reste une ressource limitée, malgré une augmentation constante des performances des ordinateurs. Un algorithme sera donc dit performant s'il utilise avec parcimonie les ressources dont il dispose, c'est-à-dire le temps CPU, la mémoire vive et (objet de recherches récentes) la consommation électrique. L’analyse de la complexité algorithmique permet de prédire l'évolution en temps calcul nécessaire pour amener un algorithme à son terme, en fonction de la quantité de données à traiter.Donald Knuth (1938-) liste, comme prérequis d'un algorithme, cinq propriétés :finitude : « un algorithme doit toujours se terminer après un nombre fini d’étapes » ;définition précise : « chaque étape d'un algorithme doit être définie précisément, les actions à transposer doivent être spécifiées rigoureusement et sans ambiguïté pour chaque cas » ;entrées : « quantités qui lui sont données avant qu'un algorithme ne commence. Ces entrées sont prises dans un ensemble d'objets spécifié » ;sorties : « quantités ayant une relation spécifiée avec les entrées » ;rendement : « toutes les opérations que l'algorithme doit accomplir doivent être suffisamment basiques pour pouvoir être en principe réalisées dans une durée finie par un homme utilisant un papier et un crayon ».George Boolos (1940-1996), philosophe et mathématicien, propose la définition suivante :« Des instructions explicites pour déterminer le nième membre d'un ensemble, pour n un entier arbitrairement grand. De telles instructions sont données de façon bien explicite, sous une forme qui puisse être utilisée par une machine à calculer ou par un humain qui est capable de transposer des opérations très élémentaires en symboles. »Gérard Berry (1948-), chercheur en science informatique, en donne la définition grand public suivante :« Un algorithme, c’est tout simplement une façon de décrire dans ses moindres détails comment procéder pour faire quelque chose. Il se trouve que beaucoup d’actions mécaniques, toutes probablement, se prêtent bien à une telle décortication. Le but est d’évacuer la pensée du calcul, afin de le rendre exécutable par une machine numérique (ordinateur…). On ne travaille donc qu’avec un reflet numérique du système réel avec qui l’algorithme interagit. »Les algorithmes sont des objets historiquement dédiés à la résolution de problèmes arithmétiques, comme la multiplication de deux nombres. Ils ont été formalisés bien plus tard avec l'avènement de la logique mathématique et l'émergence des machines qui permettaient de les mettre en œuvre, à savoir les ordinateurs.La plupart des algorithmes ne sont pas numériques.On peut distinguer :des algorithmes généralistes qui s'appliquent à toute donnée numérique ou non numérique : par exemple les algorithmes liés au chiffrement, ou qui permettent de les mémoriser ou de les transmettre ;des algorithmes dédiés à un type de données particulier (par exemple ceux liés au traitement d'images).Voir aussi : Liste de sujets généraux sur les algorithmes (en)L'algorithmique intervient de plus en plus dans la vie quotidienne.Une recette de cuisine peut être réduite à un algorithme si on peut réduire sa spécification aux éléments constitutifs :des entrées (les ingrédients, le matériel utilisé) ;des instructions élémentaires simples (frire, flamber, rissoler, braiser, blanchir, etc.) dont les exécutions dans un ordre précis amènent au résultat voulu ;un résultat : le plat préparé.Cependant, les recettes de cuisine ne sont en général pas présentées rigoureusement sous forme non ambiguë : il est d'usage d'y employer des termes vagues laissant une liberté d'appréciation à l'exécutant alors qu'un algorithme non probabiliste stricto sensu doit être précis et sans ambiguïté.Le tissage, surtout tel qu'il a été automatisé par le métier Jacquard, est une activité que l'on peut dire algorithmique.Un casse-tête, comme le cube Rubik, peut être résolu de façon systématique par un algorithme qui mécanise sa résolution.En sport, l'exécution de séquences répondant à des finalités d'attaque, de défense, de progression, correspond à des algorithmes (dans un sens assez lâche du terme). Voir en particulier l'article tactique (football).En soins infirmiers, le jugement clinique est assimilable à un algorithme. Le jugement clinique désigne l'ensemble des procédés cognitifs et métacognitifs qui aboutissent au diagnostic infirmier. Il met en jeu des processus de pensée et de prise de décision dans le but d’améliorer l’état de santé et le bien-être des personnes que les soignants accompagnent.Un code juridique, qui décrit un ensemble de procédures applicables à un ensemble de cas, est un algorithme.Les progrès de ce qu'on appelle l'intelligence artificielle s'appuient sur un algorithmique de plus en plus complexe qui devient l'un des rouages cachés du Web 2.0 et des grands réseaux sociaux.À partir des années 2000, ce qui est appelé « algorithmique » est un ensemble de « boîtes noires » (autrement dit de processus informatiques dont on ne sait pas ce qu'il y a à l'intérieur) qui exploitent et influencent les comportements inconscients des consommateurs, et des électeurs.Au milieu des années 2010 la plate-forme logicielle Ripon permet secrètement l'élection de Donald Trump. Elle le fait grâce à une intelligence artificielle s'appuyant sur des logiciels issus de la guerre psychologique telle que développée en Afghanistan, et désormais nourrie du big data disponible sur l'Internet, et en particulier de données personnelles piratées dans plusieurs dizaines de millions de comptes Facebook. Ce piratage a été réalisé par Cambridge analytica au Royaume-Uni (devenu Emerdata en aout 2017) sur la plate-forme Facebook insuffisamment protégée. Les données ont été analysées et utilisées par sa société-sœur canadienne, Aggregate IQ, sous le contrôle du groupe SCL (leur société-mère) via Ripon. Cette plateforme Ripon ayant été conçue pour produire des profils psychographiques et des processus d'utilisation dans des campagnes électorales microciblées. Ces campagnes visaient à influer sur les émotions des électeurs, pour modifier leurs intentions de vote, ou les inciter à rester ou devenir abstentionnistes,,.Ces processus plus ou moins frauduleux (la législation de protection des individus sur l'Internet étant encore émergente) seront découvertes tardivement, dans le cadre du scandale Facebook-Cambridge Analytica/Aggregate IQ, après que ces outils aient conduits à l'élection de D. Trump, puis au Brexit, et qu'ils aient influencé au moins une vingtaine d'élections ou de référendums dans le monde. Dans les années 2010, les lanceurs d'alertes comme le canadien Christopher Wylie, Carole Cadwalladr, Shahmir Sanni, Brittany Kaiser, David Caroll, des journalistes comme Carole Cadwalladr et des ONG telles que AlgorithmWatch alertent sur les dérives éthiques qu'ils constatent dans l'usage malhonnête des algorithmes.Dans la vie quotidienne, un glissement de sens s'est opéré, ces dernières années, dans le concept d'« algorithme » qui devient à la fois plus réducteur, puisque ce sont pour l'essentiel des algorithmes de gestion du big data, et d'autre part plus universel en ce sens qu'il intervient dans tous les domaines du comportement quotidien. La famille des algorithmes dont il est question effectue des calculs à partir de grandes masses de données (les big data). Ils réalisent des classements, sélectionnent des informations et en déduisent un profil, en général de consommation, qui est ensuite utilisé ou exploité commercialement. Les implications sont nombreuses et touchent les domaines les plus variés. Mais les libertés individuelles et collectives pourraient être finalement mises en péril, comme le montre la mathématicienne américaine Cathy O'Neil dans le livre Weapons of Math Destruction, publié en 2016 et sorti en français en 2018 sous le titre Algorithmes : la bombe à retardement (aux éditions Les Arènes).« Aujourd’hui, les modèles mathématiques et les algorithmes prennent des décisions majeures, servent à classer et catégoriser les personnes et les institutions, influent en profondeur sur le fonctionnement des États sans le moindre contrôle extérieur. Et avec des effets de bords incontrôlables. […] Il s’agit d’un pouvoir utilisé contre les gens. Et pourquoi ça marche ? Parce que les gens ne connaissent pas les maths, parce qu’ils sont intimidés. C’est cette notion de pouvoir et de politique qui m’a fait réaliser que j’avais déjà vu ça quelque part. La seule différence entre les modèles de risque en finances et ce modèle de plus-value en science des données, c’est que, dans le premier cas, en 2008, tout le monde a vu la catastrophe liée à la crise financière. Mais, dans le cas des profs, personne ne voit l’échec. Ça se passe à un niveau individuel. Des gens se font virer en silence, ils se font humilier, ils ont honte d’eux. »Dans cet ouvrage, l'auteure alerte le lecteur sur les décisions majeures que nous déléguons aujourd'hui aux algorithmes dans des domaines aussi variés que l'éducation, la santé, l'emploi et la justice, sous prétexte qu'ils sont neutres et objectifs, alors que, dans les faits, ils donnent lieu à « des choix éminemment subjectifs, des opinions, voire des préjugés insérés dans des équations mathématiques ».L'opacité des algorithmes est l'une des raisons principales de ces critiques. Une meilleure information sur leur mode de fonctionnement spécifique permettrait de rendre plus clair le « contrat social passé entre les internautes et les calculateurs ». La description pour chaque algorithme de son propre principe de classement de l'information aide l'utilisateur à mieux comprendre les choix proposés par l'algorithme et les résultats obtenus.Les philosophes Wendell Wallach et Colin Allen ont soulevé des questions liées à l'implantation par les programmeurs de règles morales dans les algorithmes d'intelligence artificielle : « Aujourd'hui, les systèmes [automatiques] s'approchent d'un niveau de complexité qui, selon nous, exige qu'ils prennent eux-mêmes des décisions morales […]. Cela va élargir le cercle des agents moraux au-delà des humains à des systèmes artificiellement intelligents, que nous appellerons des agents moraux artificiels ». Dans son livre Faire la morale aux robots : une introduction à l'éthique des algorithmes, Martin Gibert met en évidence le rôle de la programmation dans l'éthique des robots, en traitant plus précisément des enjeux moraux liés à la construction des algorithmes. Il définit un algorithme comme « rien de plus qu'une suite d'instructions – ou de règles – pour parvenir à un objectif donné ». L'éthique des algorithmes poserait donc une question : « Quelles règles implanter dans les robots, et comment le faire ? ». Gibert souligne notamment l'ambiguïté de ces agents moraux artificiels :« Les agents moraux artificiels (AMA) ne sont pas cependant des agents moraux au sens fort du terme. Contrairement aux humains, ils ne semblent pas imputables [sic] de leurs actes. Ils n'ont toutefois pas besoin de l'être pour prendre des décisions moralement significatives et soulever tout un tas de questions en éthique des algorithmes. »Analyse de la complexité des algorithmesAlgorithmiqueCorrection d'un algorithmeBiais algorithmiqueRégulation des algorithmesRessource relative à la santé : (en) Medical Subject Headings Qu’est-ce qu'un algorithme ? par Philippe Flajolet et Étienne Parizot sur la revue en ligne IntersticesDéfinition du terme « algorithme » par des savants Portail de l'informatique théorique"
Informatique;"En informatique, le code source est un texte qui présente les instructions composant un programme sous une forme lisible, telles qu'elles ont été écrites dans un langage de programmation. Le code source se matérialise généralement sous la forme d'un ensemble de fichiers texte.Le code source est souvent traduit — par un assembleur ou un compilateur — en code binaire composé d'instructions exécutables par le processeur. Il peut sinon être directement interprété à l'exécution du programme. Dans ce deuxième cas, il est parfois traduit au préalable en un code intermédiaire dont l'interprétation est plus rapide.L'expression est une traduction de l'anglais source code. Les expressions omettant le terme de code sont communes : les sources, le source.Dans les tout premiers temps de l'informatique, les programmes étaient entrés dans la mémoire de l'ordinateur par l'intermédiaire des interrupteurs du pupitre de commande, sous forme du codage binaire des instructions machines. Ce qui ne convenait qu'à de tout petits programmes. Ils ont ensuite été chargés depuis des bandes perforées, puis des cartes perforées.Très rapidement, les programmes ont été rédigés dans un langage symbolique, langage d'assemblage ou langage évolué comme Fortran, Cobol, puis traduit automatiquement par un programme (assembleur, compilateur).Avec l'apparition des disques magnétiques et des consoles interactives, des éditeurs de lignes puis des éditeurs de textes ont été utilisés pour taper et modifier le code source.Les possibilités limitées des ordinateurs de l'époque nécessitaient souvent l'impression du code source sur papier continu (en) avec des bandes Carol.Aujourd'hui, il existe des environnements de développement, dits Environnement de développement intégré (IDE, Integrated Development Environment), qui intègrent notamment les tâches d'édition et de compilation.Un logiciel est une suite d'instructions données à une machine. Un processeur ne peut exécuter que des instructions représentées sous une forme binaire particulière. Sauf mécanismes expérimentaux, il n'est pas possible pour un être humain de saisir directement un code binaire dans la représentation qu'en attend le processeur : un être humain ne peut pas écrire directement les champs de bits aux adresses attendues. Il est obligé de passer par un code distinct appelé code source, et qui est par la suite traduit dans la représentation binaire attendue par la machine puis chargé et exécuté par la cible.Toutefois, l'écriture d'un code sous forme binaire, même dans un fichier séparé, pose de nombreux problèmes de compréhension aux êtres humains. C'est une représentation uniquement constituée d'une suite ininterrompue de 0 et de 1 qui est difficile à lire, à écrire et à maintenir sans assistance technique. La diversité des microprocesseurs et des composants présents dans un ordinateur ou automate, implique qu'un code binaire généré pour un système ne puisse pas être a priori le même que sur une machine distincte. Aussi, il existe autant de codes binaires que de configurations et une complexité accrue excluant que l'être humain puisse concevoir simplement un code binaire de grande ampleur.Pour éviter ces écueils, et puisqu'une traduction est toujours nécessaire, l'être humain écrit un code textuel afin qu'il soit plus lisible, plus compréhensible et plus simple à maintenir : c'est le code source écrit dans un langage de programmation. Il est, dans la plupart des cas, plus lisible, plus simple à écrire et indépendant du système cible. Un programme tiers (compilateur, interpréteur ou machine virtuelle) se charge de la traduction du code source en code binaire exécutable par la cible.Le code généré par l'être humain est appelé code source ; la façon dont est rédigé ce code source est appelée langage de programmation ; le traducteur de ce code dans sa représentation binaire est appelé compilateur, interpréteur ou machine virtuelle selon les modalités de la traduction.Dans la plupart des langages, on peut distinguer différents éléments dans un code source :les éléments décrivant l’algorithme et les données (le code source proprement dit) :des symboles identifiant des variables, des mots clefs dénotant des instructions, des représentations de données ;des constantes littérales.les commentaires, qui documentent le code source le plus souvent en langage naturel, destinés aux relecteurs du code source. Ils ne sont pas nécessaires à la production du code exécutable mais peuvent être utilisés par le compilateur pour, par exemple, produire automatiquement de la documentation.Un code est plus facile à lire et à écrire avec un éditeur fournissant une coloration syntaxique permettant de distinguer les différents éléments du code source. Les commentaires peuvent par exemple être mis en vert.Exemple de code en Ruby :Autre exemple de code en Ruby :Autre exemple de code en Ruby :L'analogie du code source et de la recette de cuisine est souvent employée dans une volonté de vulgarisation. Une recette est une liste organisée d'ingrédients dont les quantités et les fonctions sont définies. Le but est d'obtenir le résultat voulu par le cuisinier, selon une technique et un enchaînement d'opérations déterminés.Ainsi le code source peut être apparenté à une recette de cuisine.Ainsi, une personne dégustant un plat est en mesure de deviner les ingrédients qui le composent et d'imaginer comment le réaliser. Néanmoins, pour un plat très raffiné et subtil (comme pourrait l'être un programme), il est fort probable qu'elle ignore le mode opératoire du cuisinier. Pour le connaître, une recette détaillée serait nécessaire (pour un programme, la recette peut compter plusieurs millions de lignes de code). La solution alternative à cela serait d'acheter des plats préparés, c'est un peu ce que l'on fait lorsqu'on achète des logiciels.Le code source peut être public ou privé (voir logiciel libre et logiciel propriétaire). Toutefois, le code binaire n'étant qu'une traduction du code source, il est toujours possible d'étudier un logiciel à partir de son code binaire. La légalité des techniques utilisées à ces fins dépend du pays et de l'époque. Elle peut notamment être mise en œuvre pour percer les secrets d'une machine comme l'ES3B. Portail de la programmation informatique"
Informatique;"La cryptographie est une des disciplines de la cryptologie s'attachant à protéger des messages (assurant confidentialité, authenticité et intégrité) en s'aidant souvent de secrets ou clés. Elle se distingue de la stéganographie qui fait passer inaperçu un message dans un autre message alors que la cryptographie rend un message supposément inintelligible à autre que qui-de-droit.Elle est utilisée depuis l'Antiquité, mais certaines de ses méthodes les plus modernes, comme la cryptographie asymétrique, datent de la fin du XXe siècle.Le mot cryptographie vient des mots en grec ancien kruptos (???????) « caché » et graphein (???????) « écrire ». Beaucoup des termes de la cryptographie utilisent la racine « crypt- », ou des dérivés du terme « chiffre » :chiffrement : transformation à l'aide d'une clé d'un message en clair (dit texte clair) en un message incompréhensible (dit texte chiffré) pour celui qui ne dispose pas de la clé de déchiffrement (en anglais encryption key ou private key pour la cryptographie asymétrique) ;chiffre : un ensemble de règles permettant d'écrire et de lire dans un langage secret ;cryptogramme : message chiffré ;cryptosystème : algorithme de chiffrement ;décrypter : retrouver le message clair correspondant à un message chiffré sans posséder la clé de déchiffrement (terme que ne possèdent pas les anglophones, qui eux « cassent » des codes secrets) ;cryptographie : étymologiquement « écriture secrète », devenue par extension l'étude de cet art (donc aujourd'hui la science visant à créer des cryptogrammes, c'est-à-dire à chiffrer) ;cryptanalyse : science analysant les cryptogrammes en vue de les décrypter ;cryptologie : science regroupant la cryptographie et la cryptanalyse ;cryptolecte : jargon réservé à un groupe restreint de personnes désirant dissimuler leur communication.Plus récemment sont apparus les termes « crypter » (pour chiffrer) et « cryptage » pour chiffrement. Ceux-ci sont acceptés par l'Office québécois de la langue française dans son grand dictionnaire terminologique, qui note que « La tendance actuelle favorise les termes construits avec crypt-. ». Le Grand Robert mentionne également « cryptage », et date son apparition de 1980. Cependant le Dictionnaire de l'Académie française n'intègre ni « crypter » ni « cryptage » dans sa dernière édition (entamée en 1992). Ces termes sont d'ailleurs considérés comme incorrects par exemple par l'ANSSI, qui met en avant le sens particulier du mot « décrypter » (retrouver le message clair à partir du message chiffré sans connaître la clef) en regard du couple chiffrer/déchiffrer.La cryptographie est utilisée depuis l'antiquité, et l'une des utilisations les plus célèbres pour cette époque est le chiffre de César, nommé en référence à Jules César qui l'utilisait pour ses communications secrètes. Mais la cryptographie est bien antérieure à cela : le plus ancien document chiffré est une recette secrète de poterie datant du XVIe siècle av. J.-C., notée sur une tablette d'argile qui a été découverte dans l'actuel Irak.L'historien en cryptographie David Kahn considère l'humaniste Leon Battista Alberti comme le « père de la cryptographie occidentale », grâce à trois avancées significatives : « la plus ancienne théorie occidentale de cryptanalyse, l'invention de la substitution polyalphabétique, et l'invention du code de chiffrement ».Bien qu'éminemment stratégique, la cryptographie est restée pendant très longtemps un art, pour ne devenir une science qu'au XXIe siècle. Avec l'apparition de l'informatique, son utilisation se popularise et se vulgarise, quitte à se banaliser et à être utilisée à l'insu de l’utilisateur[réf. nécessaire].Enfin, la Cryptographie post-quantique est une sous-discipline de la cryptographie qui cherche à proposer des algorithmes résistant au calculateur quantique.Les domaines d'utilisations de la cryptographie sont vastes et vont du domaine militaire, au commercial, en passant par la protection de la vie privée.Les techniques de cryptographie sont parfois utilisées pour protéger notre vie privée. Ce droit est en effet plus facilement bafoué dans la sphère numérique. Ainsi les limites de la cryptographie quant à sa capacité à préserver la vie privée soulève des questionnements. Deux exemples qui illustrent bien ce sujet sont à trouver dans le domaine de la santé et celui de la blockchain.La santé est un domaine sensible quant à la protection des données : le secret médical est remis en question avec l’informatisation de la médecine.La cryptographie permet en théorie de protéger les données médicales pour qu’elles ne soient pas accessible à n’importe qui, mais elle n’est pas suffisante.Car tant que le droit n’est pas suffisamment large[pas clair], il existe des failles qui permettent à certains acteurs d’utiliser des données personnelles dès l'accord de l'usager donné, or cet accord est exigé pour l'accès au service, faisant ainsi perdre à l'utilisateur la possibilité de contrôle de ses  accès à nos données personnelles.De plus l’inviolabilité des données médicales est remise en question par les développements qui permettent le déchiffrement de ces données, en effet selon Bourcier et Filippi, l’« anonymat ne semble plus garanti de façon absolue en l’état actuel des techniques de cryptographie ». Avec cette double constatation ils proposent de protéger nos données médicales avec une réforme juridique qui permettrait de faire rentrer les données personnelles médicales non pas dans le droit à la vie privée qui est un droit personnel, mais dans un droit collectif qui permettrait de protéger plus efficacement des données telles que les données génétiques qui concernent plusieurs individus. La création d’un droit collectif pour la santé permettrait ainsi de compenser les limites de la cryptographie qui n’est pas en mesure d’assurer à elle seule la protection de ce type de données.La blockchain est elle aussi l’une des applications de la cryptographie en lien avec la protection de la vie privée. C’est un système décentralisé qui se base entre autres sur des techniques de cryptographie destinées à assurer la fiabilité des échanges tout en garantissant en principe la vie privée. Qui dit système décentralisé implique qu’il n’y a pas de tierce personne par laquelle passe les informations. Ainsi seuls les individus concernés ont accès aux données vu que les données sont chiffrées, d’où un respect important de la vie privée. En pratique cela dit, ce système présente des limites : « la décentralisation est acquise au prix de la transparence ». En effet un tel système ne protège pas les informations concernant la transaction : destinataire, date, et autres métadonnées qui sont nécessaires pour s’assurer de la légitimité. Ainsi une protection complète de la vie privée en blockchain nécessite que ces métadonnées soient elles aussi protégées, puisque celles-ci sont transparentes et donc visibles par tout le monde. Cette protection supplémentaire est rendue possible par de nouvelles techniques d'anonymisation des signatures telles que la signature aveugle, qui sont réputées de garantir la légitimité des transactions sans les rendre publiques. Mais ce processus n’est pas encore applicable partout et n’est qu’à l’état embryonnaire pour certaines techniques. Malgré tout avec le temps de plus en plus de systèmes permettront de résoudre cette limitation.[Quand ?]Le cadre législatif de la cryptographie est variable et sujet aux évolutions.D’une part, il est sujet aux évolutions des technologies, de leur efficacité et de leur accessibilité. En effet la démocratisation d’Internet et des ordinateurs personnels fondent un nouveau cadre dans les années 80-90, comme nous le verrons avec l’exemple de la loi française.D’autre part, ces lois évoluent selon le contexte politique. En effet, à la suite des attentats du 11 septembre 2001, les gouvernements occidentaux opèrent une reprise du contrôle des données circulant sur Internet et de toutes les données potentiellement cachées par la cryptographie.Cela se fait de plusieurs façons : d’une part, par la mise en place de lois obligeant les fournisseurs de systèmes de communication, cryptés ou non, à fournir à certaines entités étatiques des moyens d’accéder à toutes ces données. Par exemple en France, alors qu’en 1999, la loi garantit la protection des communications privées par voie électronique, celle-ci subit l’amendement à la Loi no 91-646 du 10 juillet 1991 relative au secret des correspondances émises par la voie des communications électroniques. Cet amendement formalise précisément le moyen législatif d’accéder à des données encryptées décrit précédemment.D’autre part, certains services gouvernementaux développent des systèmes d’inspection de réseaux afin de tirer des informations malgré le chiffrement des données. On peut notamment citer le programme de surveillance électronique Carnivore aux États-Unis.Toutefois, la réglementation sur les systèmes de cryptographie ne laisse que peu de place à un contrôle par des entités telles que des gouvernements. En effet, les logiciels et algorithmes les plus performants et répandus sont issus de la connaissance et des logiciels libres comme PGP ou OpenSSH. Ceux-ci offrent une implémentation fonctionnelle des algorithmes de chiffrement modernes pour assurer le chiffrement de courriels, de fichiers, de disques durs ou encore la communication dite sécurisée entre plusieurs ordinateurs. Ces logiciels étant sous licence libre, leur code source est accessible, reproductible et modifiable. Cela implique qu’il est techniquement très difficile de les rendre exclusifs à une entité — étatique par exemple — et d’en avoir le contrôle. Le chiffrement devient alors utilisable par nombre de personnes, permettant de contrevenir à une loi.Bien que la cryptographie puisse paraître être une opportunité pour la démocratie au premier abord, la réalité n’est pas forcément si unilatérale. Il est clair que l’utilisation de cette technologie permet de protéger la liberté d’expression. Toutefois, cela ne suffit pas à dire que la cryptographie est bénéfique à la démocratie, puisque l'enjeu démocratique dépasse la simple liberté l’expression. En particulier, la démocratie suppose un système de lois et de mécanismes de sanctions qui mène la liberté d’expression vers une activité politique constructive.Avec l’apparition de la cryptographie électronique et dans un monde toujours plus numérisé, la politique doit aussi s’adapter. Winkel observe trois politiques différentes pour les gouvernements: la stratégie libérale, la stratégie de prohibition et la stratégie du tiers de confiance. Stratégie de prohibition La stratégie de prohibition consiste à restreindre l’utilisation de la cryptographie en imposant des contrôles d’import-export, des restrictions d’utilisation ou encore d’autres mesures pour permettre à l’État et ses institutions de mettre en œuvre dans le monde virtuel la politique (principes et lois) du « vrai » monde. Cette stratégie est généralement appliquée dans des pays à régime politique autoritaire, par exemple en Chine avec le Grand Firewall ou en Corée du Nord. Stratégie du tiers de confiance La stratégie du tiers de confiance a pour but de garder la balance qu’il existe dans le « vrai » monde entre d’un côté la législation et les potentielles sanctions de l’État et de l’autre la protection de secrets économiques ou de la sphère privée, dans le monde virtuel. La mise en place d’un tel système est toutefois plus technique.Le principe consiste en un dépôt des copies des clés d’encryption des utilisateurs dans les mains d’un tiers de confiance. Celui-ci pourrait ensuite répondre à une demande d'une autorité légale compétente et lui transmettre une clef - par exemple à des fins d’audit - à condition que cette demande ait suivi une procédure bien définie. Cette solution, bien que paraissant optimale du point de vue de la théorie démocratique, présente déjà un certain nombre de difficultés techniques comme la mise en place et l'entretien de l’infrastructure requise. De plus, il est utopique d’imaginer que la mise en place de cadres légaux plus sévères découragera les criminels et organisations anticonstitutionnelles d’arrêter leurs activités. Cela s’applique à la stratégie du tiers de confiance et à celle de prohibition. Stratégie libérale La stratégie libérale répandue dans le monde laisse un accès ""total"" aux technologies de cryptographie, pour sécuriser la vie privée des citoyens, défendre la liberté d’expression dans l’ère numérique, laisser les entreprises garder leurs secrets et laisser les entreprises exporter des solutions informatiques sécurisées sur les marchés internationaux.Cependant, les criminels et opposants de la Constitution[Laquelle ?] peuvent utiliser cette technologie à des fins illicites — ou anticonstitutionnelles —[Laquelle ?] comme  armes, drogue ou pédopornographie sur le Dark Web. Autres formes de législation Les États-Unis et la France interdisent l'exportation de certaines formes de cryptographie, voir Lois sur les chiffrement sur wikipedia anglophone.Les premiers algorithmes utilisés pour le chiffrement d'une information étaient assez rudimentaires dans leur ensemble. Ils consistaient notamment au remplacement de caractères par d'autres. La confidentialité de l'algorithme de chiffrement était donc la pierre angulaire de ce système pour éviter un décryptage rapide.Exemples d'algorithmes de chiffrement faibles :ROT13 (rotation de 13 caractères, sans clé) ;Chiffre de César (décalage de trois lettres dans l'alphabet sur la gauche) ;Chiffre de Vigenère (introduit la notion de clé).Les algorithmes de chiffrement symétrique se fondent sur une même clé pour chiffrer et déchiffrer un message. L'un des problèmes de cette technique est que la clé, qui doit rester totalement confidentielle, doit être transmise au correspondant de façon sûre. La mise en œuvre peut s'avérer difficile, surtout avec un grand nombre de correspondants car il faut autant de clés que de correspondants.Quelques algorithmes de chiffrement symétrique très utilisés :Chiffre de Vernam (le seul offrant une sécurité théorique absolue, à condition que la clé ait au moins la même longueur que le message à chiffrer, qu'elle ne soit utilisée qu'une seule fois et qu'elle soit totalement aléatoire)DES3DESAESRC4RC5MISTY1et d'autres (voir la liste plus exhaustive d'algorithmes de cryptographie symétrique).Pour résoudre le problème de l'échange de clés, la cryptographie asymétrique a été mise au point dans les années 1970. Elle se base sur le principe de deux clés :une publique, permettant le chiffrement ;une privée, permettant le déchiffrement.Comme son nom l'indique, la clé publique est mise à la disposition de quiconque désire chiffrer un message. Ce dernier ne pourra être déchiffré qu'avec la clé privée, qui doit rester confidentielle.Quelques algorithmes de cryptographie asymétrique très utilisés :RSA (chiffrement et signature) ;DSA (signature) ;Protocole d'échange de clés Diffie-Hellman (échange de clé) ;et d'autres ; voir cette liste plus complète d'algorithmes de cryptographie asymétrique.Le principal inconvénient de RSA et des autres algorithmes à clés publiques est leur grande lenteur par rapport aux algorithmes à clés secrètes. RSA est par exemple 1000 fois plus lent que DES. En pratique, dans le cadre de la confidentialité, on s'en sert pour chiffrer un nombre aléatoire qui sert ensuite de clé secrète pour un algorithme de chiffrement symétrique. C'est le principe qu'utilisent des logiciels comme PGP par exemple.La cryptographie asymétrique est également utilisée pour assurer l'authenticité d'un message. L'empreinte du message est chiffrée à l'aide de la clé privée et est jointe au message. Les destinataires déchiffrent ensuite le cryptogramme à l'aide de la clé publique et retrouvent normalement l'empreinte. Cela leur assure que l'émetteur est bien l'auteur du message. On parle alors de signature ou encore de scellement.La plupart des algorithmes de cryptographie asymétrique sont vulnérables à des attaques utilisant un calculateur quantique, à cause de l'algorithme de Shor. La branche de la cryptographie visant à garantir la sécurité en présence d'un tel adversaire est la cryptographie post-quantique.Une fonction de hachage est une fonction qui convertit un grand ensemble en un plus petit ensemble, l'empreinte. Il est impossible de la déchiffrer pour revenir à l'ensemble d'origine, ce n'est donc pas une technique de chiffrement.Quelques fonctions de hachage très utilisées :MD5 ;SHA-1 ;SHA-256 ;et d'autres ; voir cette liste plus complète d'algorithmes de hachage.L'empreinte d'un message ne dépasse généralement pas 256 bits (maximum 512 bits pour SHA-512) et permet de vérifier son intégrité.Projet NESSIEAdvanced Encryption Standard processLes cryptologues sont des experts en cryptologie : ils conçoivent, analysent et cassent les algorithmes (voir cette liste de cryptologues).Le mouvement Cypherpunk, qui regroupe des partisans d'une idéologie dite « cyber libertarienne », est un mouvement créé en 1991 œuvrant pour défendre les droits civils numériques des citoyens, à travers la cryptographie.Essentiellement composé de hackers, de juristes et de militants de la liberté sur le web ayant pour objectif commun une plus grande liberté de circulation de l'information, ce groupe s'oppose à toute intrusion et tentative de contrôle du monde numérique par des grandes puissances, en particulier les États.Les crypto-anarchistes considèrent la confidentialité des données privées comme un droit inhérent. En s'inspirant du système politique libéral américain, ils défendent le monde numérique en tant qu'espace à la fois culturel, économique et politique à l'intérieur d'un réseau ouvert et décentralisé, où chaque utilisateur aurait sa place et pourrait jouir de tous ses droits et libertés individuelles.Les crypto-anarchistes cherchent à démontrer que les libertés numériques ne sont pas des droits à part, contraints d’exister seulement dans le domaine technique qu’est internet mais que maintenant le numérique est un élément important et omniprésent dans la vie quotidienne, et ainsi, il est primordial dans la définition des libertés fondamentales des citoyens. Les droits et libertés numériques ne doivent pas être considérées comme moins importante que celles qui régissent le monde matériel.La création des crypto-monnaies en mai 1992[réf. souhaitée], remplit un des objectifs du mouvement en offrant une monnaie digitale intraçable en ligne mais permet également l'expansion de marchés illégaux sur le web.L’apparition de nouvelles techniques (logiciels de surveillance de masse comme Carnivore, PRISM, XKeyscore...) a en fait mené à plus de surveillance, moins de vie privée, et un plus grand contrôle de la part des États qui se sont approprié ces nouvelles technologies.Crypto-anarchistes (pour l’anonymisation des communications) et États (pour le contrôle des communications) s’opposent le long de ces arguments.Un axiome central du mouvement Cypherpunk est que, pour rééquilibrer les forces entre l’État et les individus, il faut la protection des communications privées ainsi que la transparence des informations d’intérêt public, comme l’énonce la devise : « Une vie privée pour les faibles et une transparence pour les puissants ».Dans ce sens, Julian Assange (un des plus importants membres du mouvement Cypherpunk) a créé WikiLeaks, un site qui publie aux yeux de tous, des documents et des secrets d’État initialement non connus du grand public.Les événements du 11 septembre 2001 ont été des arguments de poids pour les États, qui avancent qu'une régulation et un contrôle du monde d'internet sont nécessaires afin de préserver nos libertés.L'apparition de lanceurs d'alerte comme Edward Snowden en 2013 est un événement important en faveur du mouvement crypto-anarchiste qui s'oppose au contrôle de l’État dans le monde numérique.D'autres groupes/mouvements importants sont créés pour défendre les libertés d’internet, partageant des objectifs avec le mouvement Cypherpunk :Les Anonymous qui défendent la liberté d'expression sur internet et en dehors.L'Electronic Frontier Foundation (EFF) qui défend la confidentialité des données numériques.Le Parti Pirate qui défend l’idée des partages des données et se bat pour les libertés fondamentales sur Internet (partage d’informations, de savoirs culturels et scientifiques qui sont parfois bannis d’internet).David Kahn (trad. de l'anglais par Pierre Baud, Joseph Jedrusek), La guerre des codes secrets [« The Codebreakers »], Paris, InterEditions, 1980, 405 p. (ISBN 2-7296-0066-3).Simon Singh (trad. Catherine Coqueret), Histoire des codes secrets [« The Code Book »], Librairie générale française (LFG), coll. « Le Livre de Poche », 3 septembre 2001, 504 p., Poche (ISBN 2-253-15097-5, ISSN 0248-3653, OCLC 47927316).Jacques Stern, La Science du secret, Paris, Odile Jacob, coll. « Sciences », 5 janvier 1998, 203 p. (ISBN 2-7381-0533-5, OCLC 38587884, lire en ligne)Gilles Zémor, Cours de cryptographie, Paris, Cassini, 15 décembre 2000, 227 p. (ISBN 2-84225-020-6, OCLC 45915497).« L'art du secret », Pour la science, dossier hors-série, juillet-octobre 2002.Douglas Stinson (trad. de l'anglais par Serge Vaudenay, Gildas Avoine, Pascal Junod), Cryptographie : Théorie et pratique [« Cryptography : Theory and Practice »], Paris, Vuibert, coll. « Vuibert informatique », 28 février 2003, 337 p., Broché (ISBN 2-7117-4800-6, ISSN 1632-4676, OCLC 53918605)(en) Handbook of Applied Cryptography, A.J. Menezes, éd. P.C. van Oorschot et S.A. Vanstone - CRC Press, 1996. Disponible en ligne : [1]Site thématique de la sécurité des systèmes d'information : site officiel de l'Agence nationale de la sécurité des systèmes d'information sur la question de la sécurité informatique. Présentation de la cryptographie, des signatures numériques, de la législation française sur le sujet, etc.Bruce Schneier (trad. de l'anglais par Laurent Viennot), Cryptographie appliquée [« Applied cryptography »], Paris, Vuibert, coll. « Vuibert informatique », 15 janvier 2001, 846 p., Broché (ISBN 2-7117-8676-5, ISSN 1632-4676, OCLC 46592374).Niels Ferguson, Bruce Schneier (trad. de l'anglais par Henri-Georges Wauquier, Raymond Debonne), Cryptographie : en pratique [« Practical cryptography »], Paris, Vuibert, coll. « En pratique / Sécurité de l'information et des systèmes », 18 mars 2004, 338 p., Broché (ISBN 2-7117-4820-0, ISSN 1632-4676, OCLC 68910552).Pierre Barthélemy, Robert Rolland et Pascal Véron (préf. Jacques Stern), Cryptographie : principes et mises en œuvre, Paris, Hermes Science Publications : Lavoisier, coll. « Collection Informatique », 22 juillet 2005, 414 p., Broché (ISBN 2-7462-1150-5, ISSN 1242-7691, OCLC 85891916).Auguste Kerckhoffs, La Cryptographie militaire, L. Baudoin, 1883.Marcel Givierge, Cours de cryptographie, Berger-Levrault, 1925.Jean-Guillaume Dumas, Pascal Lafourcade, Patrick Redon, Architectures de sécurité pour internet - 2e éd. Protocoles, standards et déploiement , Dunod 2020.Jean-Guillaume Dumas, Jean-Louis Roch, Sébastien Varrette, Eric Tannier,Théorie des codes - 3e éd. : Compression, cryptage, correction, Dunod 2018.Jean-Guillaume Dumas, Pascal Lafourcade, Etienne Roudeix, Ariane Tichit, Sébastien Varrette, Les NFT en 40 questions: Comprendre les jetons Non Fungible, Dunod 2022.Jean-Guillaume Dumas, Pascal Lafourcade, Ariane Tichit, Sébastien Varrette, Les blockchains en 50 questions - 2éd.: Comprendre le fonctionnement de cette technologie, Dunod 2022.Pascal Lafourcade, Malika More, 25 énigmes ludiques pour s'initier à la cryptographie, Dunod 2021.Henry Mamy, « La cryptographie », dans Science et Guerre, vol. 16, Bernard Tignole éditeur, 1888 (lire en ligne), disponible sur GallicaLa Cryptogr@phie expliquée!, démonstrations avec des applets Java.ACrypTA, cours, exercices, textes, liens concernant la cryptographie.Ars cryptographica , vulgarisation très complète.Cryptographie, ressources, algorithmes, des ressources sur les algorithmes cryptographiques de dernière génération et sur la cryptographie classique.Cryptographie, du chiffre et des lettres, exposé de François Cayre sur le site Interstices.(en) Handbook of Applied Cryptography, une référence de plus de 800 pages dont l'édition de 1996 peut être téléchargée gratuitement Portail de la cryptologie   Portail de la sécurité de l’information   Portail de la sécurité informatique"
Informatique;"L'informatique est un domaine d'activité scientifique, technique, et industriel concernant le traitement automatique de l'information numérique par l'exécution de programmes informatiques hébergés par des dispositifs électriques-électroniques : des systèmes embarqués, des ordinateurs, des robots, des automates, etc.Ces champs d'application peuvent être séparés en deux branches :théorique : concerne la définition de concepts et modèles ;pratique : s'intéresse aux techniques concrètes de mise en œuvre.Certains domaines de l'informatique peuvent être très abstraits, comme la complexité algorithmique, et d'autres peuvent être plus proches d'un public profane. Ainsi, la théorie des langages demeure un domaine davantage accessible aux professionnels formés (description des ordinateurs et méthodes de programmation), tandis que les métiers liés aux interfaces homme-machine (IHM) sont accessibles à un plus large public.Le terme « informatique » résulte de l'association du terme « information » au suffixe « -ique » signifiant « qui est propre à » :Comme adjectif, il s'applique à l'ensemble des traitements liés à l'emploi des ordinateurs et systèmes numériques.Comme substantif, il désigne les activités liées à la conception et à la mise en œuvre de ces machines. Des questions de télécommunications comme le traitement du signal ou la théorie de l'information, aussi bien que des problèmes mathématiques comme la calculabilité s'y rattachent.Dans le vocabulaire universitaire américain, l'informatique (« computer science ») désigne surtout l'informatique théorique : un ensemble de sciences formelles qui ont pour objet d'étude la notion d'information et des procédés de traitement automatique de celle-ci, l'algorithmique.Les applications de l'informatique depuis les années 1950 forment la base du secteur d'activité des technologies de l'information et de la communication. Ce secteur industriel et commercial est lié à la fois aux procédés (logiciels, à l'architectures de systèmes) et au matériel (électronique, télécommunication). Le secteur fournit également de nombreux services liés à l'utilisation de ses produits : développement, maintenance, enseignement, assistance, surveillance et entretien.En 1957, l'ingénieur allemand Karl Steinbuch crée le terme « Informatik » pour son essai intitulé Informatik: Automatische Informationsverarbeitung, pouvant être rendu en français par « Informatique : traitement automatique de l'information ».En mars 1962, Philippe Dreyfus, ancien directeur du Centre national de calcul électronique de Bull, utilise pour la première fois en France le terme « Informatique » pour son entreprise « Société d'informatique appliquée » (SIA). Selon certains, ce néologisme est un mot-valise qui agglomère « information » et « automatique », pour désigner le traitement automatique des données,.Le même mois, Walter Bauer inaugure la société américaine « Informatics Inc. » qui dépose son nom et poursuit toutes les universités qui utilisent ce mot pour décrire la nouvelle discipline, les forçant à se rabattre sur computer science, bien que les diplômés qu'elles forment soient pour la plupart des praticiens de l'informatique plutôt que des scientifiques au sens propre[réf. nécessaire]. L’Association for Computing Machinery, la plus grande association d'informaticiens au monde, approche même Informatics Inc. afin de pouvoir utiliser le mot informatics en remplacement de l'expression computer machinery, mais l'entreprise décline la proposition[réf. nécessaire]. En 1985 Sterling Software rachète la société Informatics Inc. qui cesse ses activités en 1986[réf. souhaitée]. Pour Donald Knuth, cependant, les Américains ont délibérément écarté le mot informatique, non pour un problème de marque mais pour des raisons sémantiques ; les ordinateurs ne traitent pas de l'information, mais des données, dont le sens informatif est parfaitement indifférent[réf. nécessaire].En 1966, l'Académie française consacre l'usage officiel du mot pour désigner la « science du traitement de l'information ». La presse, l'industrie et le milieu universitaire l'adoptent dès cette époque.En juillet 1968, le ministre fédéral de la Recherche scientifique d'Allemagne de l'Ouest, Gerhard Stoltenberg, prononce le mot « Informatik » lors d'un discours officiel sur la nécessité d'enseigner cette nouvelle discipline dans les universités de son pays ; on emploie ce même terme pour nommer certains cours dans les universités allemandes. Le mot informatica fait alors son apparition en Italie et en Espagne, de même qu’informatics au Royaume-Uni.Les fondateurs de la Compagnie Générale d'Informatique (CGI) reprennent le mot « informatique » en 1969.Dans l'usage contemporain, le substantif « informatique » devient un mot polysémique qui désigne autant le domaine industriel en rapport avec l'ordinateur (au sens de calculateur fonctionnant avec des algorithmes), que la science du traitement des informations par des algorithmes.Les expressions « science informatique », « informatique fondamentale » ou « informatique théorique » désignent sans ambiguïté la science, tandis que « technologies de l'information » ou « technologies de l'information et de la communication » désignent le secteur industriel et ses produits. Des institutions assimilent parfois la compétence des utilisateurs dans la manipulation des appareils à l'alphabétisation ou à la conduite automobile, comme veut le faire entendre l'expression European Computer Driving License (traduction littérale : « permis de conduire un ordinateur »),.Plusieurs termes en anglais désignent l'informatique :informatics (en) : surtout en tant que domaine scientifique (se rencontre en Europe de l'Ouest) ;computer science : l'informatique fondamentale ou science des calculateurs, une branche de la science en rapport avec le traitement automatique d'informations ;computingcomputing : qui qualifie les activités nécessitant une masse d'opérations mathématiques et logiques (par exemple, dans cloud computing ou decision support computing) ;electronic data processing : traitement des données à l'aide de l'électronique ;Information technology : souvent utilisé pour désigner le secteur industriel des technologies de l'information,.Dans le monde du travail, on parle volontiers d’I.T., le département informatique étant the I.T. department (les autres termes ne sont quasiment jamais utilisés).Depuis des millénaires, l'Homme a créé et utilisé des outils l'aidant à calculer (abaque, boulier, etc.), exigeant, comme les opérations manuelles, des algorithmes de calcul, dont des tables datant de l'époque d'Hammourabi (environ 1750 av. J.-C.) figurent parmi les exemples les plus anciens.Si les machines à calculer évoluent constamment depuis l'Antiquité, elles n'exécutent pas elles-mêmes l'algorithme : c'est l'homme qui doit apprendre et exécuter la suite des opérations, comme pour réaliser les différentes étapes d'une division euclidienne. En 1642, Blaise Pascal imagine une machine à calculer,, la Pascaline, qui fut commercialisée. Sept exemplaires subsistent dans des musées comme celui des Arts et Métiers à Paris, et deux sont dans des collections privées (IBM en possède une). Joseph Marie Jacquard avec ses métiers à tisser à cartes perforées illustre en premier le concept de programmation, comme enchaînement automatique d'opérations élémentaires. George Boole et Ada Lovelace esquissent une théorie de la programmation des opérations mathématiques.Le secteur très féminisé à ses débuts avec des pionnières comme Ada Lovelace, Grace Hopper, Frances Allen, Adele Goldberg est devenu progressivement plus masculin avec la professionnalisation des différents métiers dans l'informatique (premiers diplômes en informatique). La programmation était vue au début comme une activité essentiellement féminine avant de devenir une profession prisée et largement investie par les hommes. La place des femmes en informatique décroit dès le milieu des années 1980 en France.Dans les années 1880, Herman Hollerith, futur fondateur d'IBM, fonde la mécanographie en inventant une machine électromécanique destinée à faciliter le recensement en stockant les informations sur une carte perforée. Le gouvernement des États-Unis utilise pour la première fois à grande échelle les trieuses et les tabulatrices lors du recensement de 1890, à la suite de l'afflux des immigrants dans ce pays dans la seconde moitié du XIXe siècle.L'ingénieur norvégien Fredrik Rosing Bull a créé la première entreprise européenne qui a développé et commercialisé des équipements mécanographiques. Installé en Suisse dans les années 1930 il est ensuite venu en France pour s'attaquer au marché français. Pendant la Seconde Guerre mondiale, René Carmille utilisait des machines mécanographiques Bull.Les Allemands étaient équipés de machines mécanographiques avant la Seconde Guerre mondiale. Ces équipements étaient installés dans des ateliers composés de trieuses, interclasseuses, perforatrices, tabulatrices et calculatrices connectées à des perforateurs de cartes. Des machines électromécaniques utilisant aussi des lampes radio comme les triodes effectuaient les traitements. Ces lampes dégageaient de la chaleur qui attirait les insectes, et les bugs (terme anglais pour insectes, francisé en « bogue ») étaient une cause de panne courante.Les femmes occupent une place prépondérante au début de l'informatique dans les activités de calcul et de programmation. Les programmeuses de l'ordinateur ENIAC en 1944 sont six mathématiciennes : Marlyn Meltzer, Betty Holberton, Kathleen Antonelli, Ruth Teitelbaum, Jean Bartik, Frances Spence. Adele Goldstine est leur formatrice et elles sont surnommées les « ENIAC girls ».L'informatique moderne n'a pu émerger qu'à la suite de l'invention du transistor en 1947 et son industrialisation dans les années 1960.L'informatique moderne commence avant la Seconde Guerre mondiale, lorsque le mathématicien Alan Turing pose les bases d'une théorisation de ce qu'est un ordinateur, avec son concept de machine universelle de Turing. Turing pose dans son article les fondements théoriques de ce qui sépare la machine à calculer de l'ordinateur : la capacité de ce dernier à réaliser un calcul en utilisant un algorithme conditionnel.Après la Seconde Guerre mondiale, l'invention du transistor, puis du circuit intégré permettront de remplacer les relais électromécaniques et les tubes à vide, qui équipent les machines à calculs pour les rendre à la fois plus petites, plus complexes, plus économiques et plus fiables. Le capital-risque finance des dizaines de sociétés électroniques.Avec l'architecture de von Neumann, mise en application de la machine universelle de Turing, les ordinateurs dépassent la simple faculté de calculer et peuvent commencer à accepter des programmes plus évolués, de nature algorithmique.En 1961, Marion Créhange soutient une des premières thèses en informatique en France.Dans les années 1970, l'informatique se développe avec les télécommunications, avec Arpanet, le réseau Cyclades et la Distributed System Architecture (DSA) de réseau en couches, qui donnera naissance en 1978 au modèle OSI, appelé aussi « OSI-DSA », puis aux protocoles TCP-IP dans les années 1990, grâce à la baisse des prix des microprocesseurs. Les concepts de datagramme et d'informatique distribuée, d'abord jugés risqués, s'imposeront grâce à l'Internet.La série de livres The Art of Computer Programming de Donald Knuth, publiée à partir des années 1960, fait ressortir les aspects mathématiques de la programmation informatique. Edsger Dijkstra, Niklaus Wirth et Christopher Strachey travaillent et publient vers un même axe. Ces travaux préfigurent d'importants développements en matière de langage de programmation.L'amélioration de l'expressivité des langages de programmation a permis la mise en œuvre d'algorithmes toujours plus sophistiqués, appliqués à des données de plus en plus variées. La miniaturisation des composants et la réduction des coûts de production, associées à une augmentation de la demande en traitements des informations de toutes sortes (scientifiques, financières, commerciales, etc.), ont eu pour conséquence une diffusion de l'informatique dans tous les secteurs économiques, ainsi que dans la vie quotidienne des individus.Dans les années 1970, Xerox fait réaliser des études en psychologie cognitive et en ergonomie en vue de simplifier l'utilisation des outils informatiques. L'interface graphique propose un accès à la machine plus proche des objets ordinaires que l'interface en ligne de commande existant jusque-là. Les constructeurs souhaitant concurrencer le géant IBM promeuvent une informatique plus décentralisée.La démocratisation de l'utilisation d'Internet – réseau basé sur ARPANET – depuis 1995, a amené les outils informatiques à être de plus en plus utilisés dans une logique de réseau comme moyen de télécommunication, à la place des outils tels que la poste ou le téléphone. Elle s'est poursuivie avec l'apparition des logiciels libres, puis des réseaux sociaux et des outils de travail collaboratif dont Wikipédia n'est qu'un des nombreux exemples.Face à la demande pour numériser photos et musiques, les capacités de stockage, de traitement et de partage des données explosent et les sociétés qui ont parié sur la croissance la plus forte l'emportent le plus souvent, en profitant d'une énorme bulle spéculative sur les sociétés d'informatique.En France, l'informatique n'a commencé à se développer que dans les années 1960, avec le Plan Calcul. Depuis lors, les gouvernements successifs ont mené des politiques diverses en faveur de la recherche scientifique, l'enseignement, la tutelle des télécommunications, la nationalisation d'entreprises clés.La science informatique est une science formelle, dont l'objet d'étude est le calcul au sens large, c'est-à-dire, non pas exclusivement arithmétique, mais en rapport avec tout type d'information que l'on peut représenter par une suite de nombres. Ainsi, textes, séquences d'ADN, images, sons ou formules logiques peuvent faire l'objet de calculs. Selon le contexte, on parle d'un calcul, d'un algorithme, d'un programme, d'une procédure.Un algorithme est une manière systématique de procéder pour arriver à calculer un résultat.Un des exemples classiques est l'algorithme d'Euclide du calcul du « Plus grand commun diviseur » (PGCD) qui remonte au moins à 300 av. J.-C., mais il s'agit déjà d'un calcul complexe. Avant cela, le simple fait d'utiliser un abaque demande d'avoir réfléchi à un moyen systématique (et correct) d'utiliser cet outil pour réaliser des opérations arithmétiques.Des algorithmes existent donc depuis l'Antiquité, mais ce n'est que depuis les années 1930, avec les débuts de la théorie de la calculabilité, que les scientifiques se sont posés les questions « qu'est-ce qu'un modèle de calcul ? », « est-ce que tout est calculable ? » et ont tenté d'y répondre formellement.Il existe de nombreux modèles de calcul, dont les deux principaux sont la « machine de Turing » et le « lambda-calcul ». Ces deux systèmes formels définissent des objets qui peuvent représenter ce qu'on appelle des procédures de calcul, des algorithmes ou des programmes. Ils définissent ensuite un moyen systématique d'appliquer ces procédures, c'est-à-dire de calculer.Le résultat le plus important de la calculabilité est probablement le fait que les principaux modèles de calcul ont exactement la même puissance, c'est-à-dire qu'il n'existe pas de procédure que l'on pourrait exprimer dans un modèle mais pas dans un autre. La thèse de Church postule que ces modèles de calcul équivalents décrivent complètement et mathématiquement tout ce qui est physiquement calculable.Un deuxième résultat fondamental est l'existence de fonctions incalculables, une fonction étant ce que calcule une procédure ou un algorithme (ceux-ci désignant plutôt comment faire le calcul). On peut montrer qu'il existe des fonctions, bien définies, pour lesquelles il n'existe pas de procédure pour les calculer. L'exemple le plus connu étant probablement le problème de l'arrêt, qui montre qu'il n'existe pas de machine de Turing calculant si une autre machine de Turing donnée s'arrêtera (et donc donnera un résultat) ou non.Tous les modèles de calcul étant équivalents, ce résultat s'applique aussi aux autres modèles, ce qui inclut les programmes et logiciels que l'on peut trouver dans les ordinateurs courants. Il existe un lien très fort entre les fonctions que l'on ne peut pas calculer et les problèmes que l'on ne peut pas décider (voir Décidabilité).L'algorithmique est l'étude comparative des différents algorithmes. Tous les algorithmes ne se valent pas : le nombre d'opérations nécessaires pour arriver à un même résultat diffère d'un algorithme à l'autre. Ce nombre d'opérations, appelé la complexité algorithmique est le sujet de la théorie de la complexité des algorithmes, qui constitue une préoccupation essentielle en algorithmique.La complexité algorithmique sert en particulier à déterminer comment le nombre d'opérations nécessaires évolue en fonction du nombre d'éléments à traiter (la taille des données) :soit l'évolution peut être indépendante de la taille des données, on parle alors de complexité constante ;soit le nombre d'opérations peut augmenter selon un rapport logarithmique, linéaire, polynomial ou exponentiel (dans l'ordre décroissant d'efficacité et pour ne citer que les plus répandues) ;une augmentation exponentielle de la complexité aboutit très rapidement à des durées de calcul déraisonnables pour une utilisation en pratique ;tandis que pour une complexité polynomiale (ou meilleure), le résultat sera obtenu après une durée de calcul réduite, même avec de grandes quantités de données.Nous arrivons maintenant à un problème ouvert fondamental en informatique : « P est-il égal à NP ? ». En simplifiant beaucoup : P est « l'ensemble des problèmes pour lesquels on connaît un algorithme efficace » et NP « l'ensemble des problèmes pour lesquels on connaît un algorithme efficace pour vérifier une solution à ce problème ». Et en simplifiant encore plus : existe-t-il des problèmes difficiles ? Des problèmes pour lesquels il n'existe pas d'algorithme efficace ?Cette question est non seulement d'un grand intérêt théorique mais aussi pratique. En effet, un grand nombre de problématiques courantes et utiles sont des problèmes que l'on ne sait pas résoudre de manière efficace. C'est d'ailleurs un des problèmes du prix du millénaire et le Clay Mathematics Institute s'est engagé à verser un million de dollars aux personnes qui en trouveraient la solution.C'est un problème ouvert, donc formellement, il n'y a pas de réponse reconnue. Mais, en pratique, la plupart des spécialistes[réf. nécessaire] s'accordent pour penser que P?NP, c'est-à-dire qu'il existe effectivement des problèmes difficiles qui n'admettent pas d'algorithme efficace.Ce type de problème de complexité algorithmique est directement utilisé en cryptologie. En effet, les méthodes de cryptologie modernes reposent sur l'existence d'une fonction facile à calculer qui possède une fonction réciproque difficile à calculer. C'est ce qui permet de chiffrer un message qui sera difficile à décrypter (sans la clé).La plupart des chiffrements (méthode de cryptographie) reposent sur le fait que la procédure de décomposition en produit de facteurs premiers n'a pas d'algorithme efficace connu. Si quelqu'un trouvait un tel algorithme, il serait capable de décrypter la plupart des cryptogrammes facilement. On sait d'ailleurs qu'un calculateur quantique en serait capable, mais ce genre d'ordinateur n'existe pas, en tout cas pour le moment.Depuis les années 1960, et à la frontière avec la logique mathématique : la correspondance de Curry-Howard a jeté un pont entre le monde des démonstrations formelles et celui des programmes, dans la discipline des méthodes formelles.Citons aussi l'étude de la mécanisation des procédés de calcul et de pensée qui a permis de mieux comprendre la réflexion humaine, et apporté des éclairages en psychologie cognitive et en linguistique, par exemple, à travers la discipline du traitement automatique du langage naturel,.Le terme technologies de l'information et de la communication désigne un secteur d'activité et un ensemble de biens qui sont des applications pratiques des connaissances scientifiques en informatique ainsi qu'en électronique numérique, en télécommunication, en sciences de l'information et de la communication et en cryptologie.Le matériel informatique est un ensemble d'équipements (pièces détachées) servant au traitement des informations.Un logiciel contient des suites d'instructions qui décrivent en détail les algorithmes des opérations de traitement d'information ainsi que les informations relatives à ce traitement (valeurs clés, textes, images, etc.).Les appareils en électronique numérique utilisent tous un système logique. Les entrées et sorties des composants électroniques n'ont que deux états ; l'un correspondant à vrai, l'autre à faux. On démontre qu'en assimilant vrai au nombre 1 et faux au nombre 0, on peut établir les règles logiques qui fondent un système de numération binaire. Les appareils représentent toute l'information sous cette forme.Les appareils informatiques se décomposent en quatre ensembles qui servent respectivement à entrer des données, les stocker, les traiter, puis les faire ressortir de l'appareil, selon les principes de la machine de Turing et l'architecture de von Neumann. Les données circulent entre les pièces des différentes unités par des lignes de communication, les bus. Le processeur est la pièce centrale qui anime l'appareil en suivant les instructions des programmes qui sont enregistrés à l'intérieur.Il existe aujourd'hui une gamme étendue d'appareils capables de traiter automatiquement des informations. De ces appareils, l'ordinateur est le plus connu, le plus ouvert, le plus complexe et un des plus anciens. L'ordinateur est une machine modulable et universelle qui peut être adaptée à de nombreuses tâches par ajout de matériel ou de logiciel.Un système embarqué est un appareil équipé de matériel et de logiciel informatique, et affecté à une tâche bien précise.Exemples d'appareils :la console de jeu est un appareil destiné au jeu vidéo, une activité que l'on peut aussi exercer avec un ordinateur ;le NAS (acronyme de l'anglais network attached storage, littéralement « mémoire attachée à un réseau ») est un appareil destiné à garder des informations en mémoire et à les mettre à disposition via un réseau informatique ;le distributeur de billets : un automate qui distribue sur demande des billets de banque ou des tickets de transport public ; les distributeurs sont souvent des ordinateurs effectuant un nombre limité de tâches ;le récepteur satellite tout comme le décodeur de Télévision Numérique Terrestre : les émissions de télévision se font en numérique et sont captées et décodées par des appareils informatiques ;les appareils d'avionique sont des appareils électroniques et informatiques placés dans les avions et les véhicules spatiaux ; ils servent à la navigation, la prévention des collisions et la télécommunication ;le GPS : un appareil qui affiche une carte géographique, et se positionne sur la carte grâce à un réseau de satellites ; les cartes géographiques sont des informations créées par ordinateur ;le téléphone mobile : initialement c'est un simple appareil analogique utilisable par un nombre restreint d'utilisateurs, le téléphone portable numérisé est utilisable en masse et sert aussi à jouer, à visionner des images ou des vidéos ;Les smartphones sont de véritables ordinateurs de poche, intégrant de nombreux capteurs (positionnement GPS, accéléromètres multi-axes, Capteur photographique, thermomètre, hygromètre), regroupant ainsi plusieurs appareils différents dans un même boîtier ;les systèmes d'arme sont des dispositifs informatiques qui permettent l'organisation et le suivi des opérations militaires : positionnement géographique, calcul des tirs, guidage des appareils et des véhicules ;les robots sont des appareils électromécaniques qui effectuent, de manière autonome, des tâches pour assister ou remplacer des humains ; l'autonomie est assurée par un appareil informatique placé à l'intérieur et/ou à l'extérieur du robot.L'ensemble des composants électroniques, nécessaires au fonctionnement des appareils numériques, est appelé « en anglais hardware ». Dans un boîtier se trouvent les pièces centrales, par exemple, le processeur et des pièces périphériques servant à l'acquisition, au stockage, à la restitution et la transmission d'informations. L'appareil est un assemblage de pièces qui peuvent être de différentes marques. Le respect des normes industrielles par les différents fabricants assure le fonctionnement de l'ensemble. Carte mère La carte mère est un circuit imprimé avec de nombreux composants et ports de connexion constituant le support principal des éléments essentiels d'un ordinateur (Supports des microprocesseur, mémoires, connecteurs divers et autres ports d'entrée-sortie). Boîtier et périphériques L'intérieur du boîtier d'un appareil informatique contient un ou plusieurs circuits imprimés sur lesquels sont soudés des composants électroniques et des connecteurs. La carte mère est le circuit imprimé central, sur lequel sont connectés tous les autres équipements.Un bus est un ensemble de lignes de communication qui servent aux échanges d'information entre les composants de l'appareil informatique. Les informations sont transmises sous forme de signaux électriques. Le plus petit élément d'information manipulable en informatique correspond à un bit. Les bus transfèrent des bytes d’informations composés de plusieurs bits en parallèle.Les périphériques sont par définition, les équipements situés à l'extérieur du boîtier. Équipements d'entrée Les périphériques d'entrée servent à commander l'appareil informatique ou à y envoyer des informations.L'envoi des informations se fait par le procédé de numérisation. Il s'agit de transformer des informations brutes (une page d'un livre, les listes des éléments périodiques, etc.) en suite de nombres binaires pouvant être manipulées par un appareil informatique. La transformation est faite par un circuit électronique. La construction du circuit diffère en fonction de la nature de l'information à numériser.L'ensemble des dispositifs de commande et les périphériques de sortie directement associés forment une façade de commande appelée interface homme-machine. Stockage d'information Une mémoire est un dispositif électronique (circuit intégré) ou électromécanique destiné à conserver des informations dans un appareil informatique.Une mémoire de masse : dispositif de stockage de grande capacité, souvent électromagnétique (bandes magnétiques, disques durs), destiné à conserver longtemps une grande quantité d'informations.Un disque dur : mémoire de masse à accès direct, de grande capacité, composée d'un ou de plusieurs disques rigides superposés et magnétiques. L'IBM Ramac 305, le premier disque dur, a été dévoilé en 1956. Le disque dur est une des mémoires de masse les plus utilisées en informatique. Pour gérer de grandes volumétries, ces disques sont associés par des mécanismes logiciels permettant d'étendre leur capacité (jusqu'à plusieurs Po) et d'y intégrer une protection avancée (RAID et Réplication au niveau bloc. Réplication, Versioning et Snapshot au niveau fichier).Une mémoire morte (« Read Only Memory » en anglais, ou ROM) : mémoire composée de circuits intégrés où les informations ne peuvent pas être modifiées. Ce type de mémoire est toujours installé par le constructeur et utilisé pour conserver définitivement des logiciels embarqués.Une mémoire vive : mémoire composée de circuits intégrés où les informations peuvent être modifiées. Les informations non enregistrées sont souvent perdues à la mise hors tension. Processeur Le processeur est le ou les composants électroniques qui exécute des instructions (calcul, choix, gestion des taches). Un appareil informatique contient au moins un microprocesseur, voire deux, quatre, ou plus. Les ordinateurs géants contiennent des milliers de processeurs.L'acronyme CPU (pour l'anglais Central Processing Unit) désigne le ou les processeurs centraux de l'appareil. L'exécution des instructions par le ou les CPU influence tout le déroulement des traitements.Un microprocesseur multi-cœur réunit plusieurs circuits intégrés de processeur dans un seul boîtier. Un composant électronique construit de cette manière effectue le même travail que plusieurs processeurs. Équipements de sortie Les équipements de sortie servent à présenter les informations provenant d'un appareil informatique sous une forme reconnaissable par un humain.Un convertisseur numérique-analogique (en anglais Digital to Analog Converter ou DAC) est un composant électronique qui transforme une information numérique (une suite de nombres généralement en binaire) en un signal électrique analogique. Il effectue le travail inverse de la numérisation (exemple : un lecteur de CD audio).Un écran est une surface sur laquelle s'affiche une image (exemple : des fenêtres de dialogue et des documents). Les images à afficher sont générées par un circuit électronique convertisseur numérique-analogique en sortie des cartes vidéos pour l'affichage sur les écrans analogiques. De plus en plus souvent, l'étape du DAC est supprimée grâce à la connexion HDMI avec les écrans interprétant directement les images numériques.Un moniteur est un écran utilisant les mêmes techniques que celles utilisées par les téléviseurs, qui affiche des graphiques et des textes provenant de l'appareil informatique.Une imprimante est un équipement servant à produire des informations non volatiles, sous forme d'impression sur papier. Il peut s'agir de textes, de tableaux, de graphiques, de schémas, de photos, etc.Un haut-parleur ou un « jack » : on peut brancher un casque, un système d'enceintes amplifiées, ou tout système audio, afin de reproduire les sons dans le spectre audible par les humains, fabriqués ou passant par la carte son. Cette dernière utilisant aussi un DAC mais aussi ADC, permettant de numériser les signaux analogiques provenant de microphones ou de tout appareil électronique de reproduction sonore que l'on connecte au connecteur mic ou line. Équipements de réseau Les équipements de réseau servent à la communication d'informations entre des appareils informatiques, en particulier, à l'envoi d'informations, à la réception, à la retransmission, et au filtrage. Les communications peuvent se faire par câble, par onde radio, par satellite, ou par fibre optique.Un protocole de communication est une norme industrielle relative à la communication d'informations. La norme établit autant le point de vue électronique (tensions, fréquences) que le point de vue informationnel (choix des informations, format), ainsi que le déroulement des opérations de communication (qui initie la communication, comment réagit le correspondant, combien de temps dure la communication, etc.). Selon le modèle OSI – qui comporte sept niveaux –, une norme industrielle (en particulier un protocole de communication) d'un niveau donné, peut être combinée avec n'importe quelle norme industrielle d'une couche située en dessus ou en dessous.Une carte réseau est un circuit imprimé qui sert à recevoir et envoyer des informations conformément à un ou plusieurs protocoles.Un modem est un équipement qui sert à envoyer des informations sous forme d'un signal électrique modulé, ce qui permet de les faire passer sur une ligne de communication analogique telle une ligne téléphonique.Un logiciel est un ensemble d'informations relatives à un traitement automatisé, qui correspond à la « procédure » d'une Machine de Turing. La mécanique de cette machine correspondant au processeur. Le logiciel peut être composé d'instructions et de données. Les instructions mettent en application les algorithmes en rapport avec le traitement d'information voulu. Les données incluses dans un logiciel sont les informations relatives à ce traitement ou exigées par lui (valeurs clés, textes, images, etc.).Le logiciel peut prendre une forme exécutable (c'est-à-dire, directement compréhensible par le micro-processeur) ou source, c'est-à-dire que la représentation est composée d'une suite d'instructions directement compréhensible par un individu. Ainsi donc, on peut considérer le logiciel comme une abstraction qui peut prendre une multitude de formes : il peut être imprimé sur du papier, conservé sous forme de fichiers informatiques ou encore stocké dans une mémoire (une disquette, une clé USB).Un appareil informatique peut contenir de très nombreux logiciels, organisés en trois caté"
Informatique;"Un langage de programmation est une notation conventionnelle destinée à formuler des algorithmes et produire des programmes informatiques qui les appliquent. D'une manière similaire à une langue naturelle, un langage de programmation est composé d'un alphabet, d'un vocabulaire, de règles de grammaire, de significations, mais aussi d'un environnement de traduction censé rendre sa syntaxe compréhensible par la machine,.Les langages de programmation permettent de décrire d'une part les structures des données qui seront manipulées par l'appareil informatique, et d'autre part d'indiquer comment sont effectuées les manipulations, selon quels algorithmes. Ils servent de moyens de communication par lesquels le programmeur communique avec l'ordinateur, mais aussi avec d'autres programmeurs ; les programmes étant d'ordinaire écrits, lus, compris et modifiés par une équipe de programmeurs.Un langage de programmation est mis en œuvre par un traducteur automatique : compilateur ou interprète. Un compilateur est un programme informatique qui transforme dans un premier temps un code source écrit dans un langage de programmation donné en un code cible qui pourra être directement exécuté par un ordinateur, à savoir un programme en langage machine ou en code intermédiaire, tandis que l’interprète réalise cette traduction « à la volée ».Les langages de programmation offrent différentes possibilités d'abstraction et une notation proche de l'algèbre, permettant de décrire de manière concise et facile à saisir les opérations de manipulation de données et l'évolution du déroulement du programme en fonction des situations. La possibilité d'écriture abstraite libère l'esprit du programmeur d'un travail superflu, notamment de prise en compte des spécificités du matériel informatique, et lui permet ainsi de se concentrer sur des problèmes plus avancés.Chaque langage de programmation supporte une ou plusieurs approches de la programmation – paradigmes. Les notions induisant le paradigme font partie du langage de programmation et permettent au programmeur d'exprimer dans le langage une solution qui a été imaginée selon ce paradigme.Les premiers langages de programmation ont été créés dans les années 1950 en même temps que l'avènement des ordinateurs. Cependant, de nombreux concepts de programmation ont été initiés par un langage ou parfois plusieurs langages, avant d'être améliorés puis étendus dans les langages suivants. La plupart du temps la conception d'un langage de programmation a été fortement influencée par l'expérience acquise avec les langages précédents.Un langage de programmation est construit à partir d'une grammaire formelle, qui inclut des symboles et des règles syntaxiques, auxquels on associe des règles sémantiques. Ces éléments sont plus ou moins complexes selon la capacité du langage. Les modes de fonctionnement et de définition de la complexité d'un langage de programmation sont généralement déterminés par leur appartenance à l'un des degrés de la hiérarchie de Chomsky.Sous un angle théorique, tout langage informatique peut être qualifié de langage de programmation s'il est Turing-complet, c'est-à-dire qu'il permet de représenter toutes les fonctions calculables au sens de Turing et Church (en admettant néanmoins pour exception à la théorie que la mémoire des ordinateurs n'est pas un espace infini).Les règles de syntaxeDéfinies par une grammaire formelle, elles régissent les différentes manières dont les éléments du langage peuvent être combinés pour obtenir des programmes. La ponctuation (par exemple l'apposition d'un symbole ; en fin de ligne d'instruction d'un programme) relève de la syntaxe.Le vocabulaireParmi les éléments du langage, le vocabulaire représente l'ensemble des instructions construites d’après des symboles. L'instruction peut être mnémotechnique ou uniquement symbolique comme quand elle est représentée par des symboles d'opérations tels que des opérateurs arithmétiques (« + » et « - ») ou booléens (&& pour le et logique par exemple). On parle aussi parfois de mot clé pour désigner une instruction (par abus de langage car le concept de mot clé ne recouvre pas celui des symboles qui font pourtant eux aussi partie du vocabulaire).La sémantiqueLes règles de sémantique définissent le sens de chacune des phrases qui peuvent être construites dans le langage, en particulier quels seront les effets de la phrase lors de l'exécution du programme. La science l’étudiant est la sémantique des langages de programmation.L’alphabetL'alphabet des langages de programmation est basé sur les normes courantes comme ASCII, qui comporte les lettres de A à Z sans accent, des chiffres et des symboles, ou Unicode pour la plupart des langages modernes (dans lesquels l'utilisation se limite en général aux chaînes de caractères littérales et aux commentaires, avec quelques exceptions notables comme C? qui autorisent également les identifiants unicode).La plupart des langages de programmation peuvent prévoir des éléments de structure complémentaires, des méthodes procédurales, et des définitions temporaires et variables et des identifiants :Les commentairesLes commentaires sont des textes qui ne seront pas traduits. Ils peuvent être ajoutés dans les programmes pour y laisser des explications. Les commentaires sont délimités par des marques qui diffèrent d'un langage de programmation à l'autre tel que « -- », « /* » ou « // ».Les identifiantsLes éléments constitutifs du programme, tels que les variables, les procédures ou les types servent à organiser le programme et son fonctionnement. On peut ainsi, par exemple, diviser un programme en fonctions ou lui donner une structure par objets : ces éléments de structure sont définis par des identifiants ou des procédures par mot clé selon le langage.Un langage de programmation offre un cadre pour élaborer des algorithmes et exprimer des diagrammes de flux,. Il permet en particulier de décrire les structures des données qui seront manipulées par l'appareil informatique et quelles seront les manipulations. Un langage de programmation sert de moyen de communication avec l'ordinateur mais aussi entre programmeurs : les programmes étant d'ordinaire écrits, lus et modifiés par une équipe de programmeurs.Un langage de programmation offre un ensemble de notions qui peuvent être utilisées comme primitives pour développer des algorithmes. Les programmeurs apprécient que le langage soit clair, simple et unifié, qu'il y ait un minimum de notions qui peuvent être combinées selon des règles simples et régulières. Les qualités d'un langage de programmation influent sur la facilité avec laquelle les programmes pourront être écrits, testés, puis plus tard compris et modifiés.La facilité d'utilisation, la portabilité et la clarté sont des qualités appréciées des langages de programmation. La facilité d'utilisation, qui dépend de la syntaxe, du vocabulaire et des symboles, influence la lisibilité des programmes écrits dans ce langage et la durée d'apprentissage. La portabilité permet à un programme écrit pour être exécuté par une plateforme informatique donnée (un système d'exploitation) d'être transféré en vue d'être exécuté sur une autre plateforme.Les programmeurs apprécient que la syntaxe permette d'exprimer la structure logique inhérente au programme. Un des soucis en programmation est d'éviter des pannes, qu'il soit possible de les détecter, les éviter et les rectifier ; ceci est rendu possible par des mécanismes internes des langages de programmation. Des vérifications implicites sont parfois effectuées en vue de déceler des problèmes.Les programmeurs apprécient qu'un langage de programmation soit en ligne avec les bonnes pratiques de programmation et d'ingénierie, qu'il encourage la structuration du programme, facilite la maintenance des programmes et qu'il dissuade, voire interdise les mauvaises pratiques. L'utilisation de l'instruction goto, par exemple, qui existe depuis les premiers langages de programmation, est considérée comme une mauvaise pratique. Son utilisation est déconseillée, voire impossible dans les langages de programmation récents.L'alignement sur les standards industriels, la possibilité d'utiliser des fonctionnalités écrites dans un autre langage de programmation et l'exécution simultanée de plusieurs threads sont des possibilités appréciées des langages de programmation.Un langage de programmation repose sur un ensemble de notions telles que les instructions, les variables, les types et les procédures ou fonctions, qui peuvent être utilisées comme primitives pour développer des algorithmes. Une instruction Un ordre donné à un ordinateur. Une variable Un nom utilisé dans un programme pour faire référence à une donnée manipulée par programme. Une constante Un nom utilisé pour faire référence à une valeur permanente. Une expression littérale Une valeur mentionnée en toutes lettres dans le programme. Un type Chaque donnée a une classification, celle-ci influe sur la plage de valeurs possibles, les opérations qui peuvent être effectuées et la représentation de la donnée sous forme de bits. Chaque langage de programmation offre une gamme de types primitifs, incorporés dans le langage. Certains langages offrent la possibilité de créer des nouveaux types.Les types de données primitifs courants sont les nombres entiers, les nombres réels, le booléen, les chaînes de caractères et les pointeurs.Plus précisément, le type booléen est un type qui n'a que deux valeurs, vrai et faux, tandis que le type pointeur fait référence à une donnée qui se trouve quelque part en mémoire. Une structure de données Une manière caractéristique d'organiser un ensemble de données en mémoire, qui influe sur les algorithmes utilisés pour les manipuler. Les structures courantes sont les tableaux, les enregistrements, les listes, les piles, les files et les arbres. Une déclaration Une phrase de programme qui sert à renseigner au traducteur (compilateur, interpréteur...) les noms et les caractéristiques des éléments du programme tels que des variables, des procédures, de types, etc.Des vérifications sont effectuées au moment de la compilation ou lors de l'exécution du programme, pour assurer que les opérations du programme sont possibles avec les types de données qui sont utilisés. Dans un langage fortement typé, chaque élément du programme a un type unique, connu et vérifié au moment de la compilation, ce qui permet de déceler des erreurs avant d'exécuter le programme. Les procédures, fonctions, méthodes Divers langages de programmation offrent la possibilité d'isoler un fragment de programme et d'en faire une opération générale, paramétrable, susceptible d'être utilisée de façon répétée. Ces fragments sont appelés procédures, fonctions ou méthodes, selon le paradigme. Les modules Les langages de programmation peuvent également offrir la possibilité de découper un programme en plusieurs pièces appelées modules, chacune ayant un rôle déterminé, puis de combiner les pièces.Les notions de procédure et de module sont destinées à faciliter la création de programmes complexes et volumineux en assistant la prise en charge de cette complexité. Ces fonctions permettent en particulier la modularité et l'abstraction.Un paradigme est une façon d'approcher la programmation. Chaque paradigme amène sa philosophie de la programmation ; une fois qu'une solution a été imaginée par un programmeur selon un certain paradigme, un langage de programmation qui suit ce paradigme permettra de l'exprimer. Impératif, déclaratif, fonctionnel, logique, orienté objet, concurrent, visuel, événementiel et basé web sont des paradigmes de programmation. Chaque langage de programmation reflète un ou plusieurs paradigmes, apportant un ensemble de notions qui peuvent être utilisées pour exprimer une solution à un problème de programmation. Au cours de l'histoire, les scientifiques et les programmeurs ont identifié les avantages et les limitations d'un style de programmation et apporté de nouveaux styles. La plupart des langages de programmation contemporains permettent d'adopter plusieurs paradigmes de programmation à condition que ceux-ci soient compatibles.Le paradigme impératif ou procédural est basé sur le principe de l'exécution étape par étape des instructions tout comme on réalise une recette de cuisine. Il est basé sur le principe de la machine de Von Neumann. Un ensemble d'instructions de contrôle de flux d'exécution permet de contrôler l'ordre dans lequel sont exécutées les instructions qui décrivent les étapes. Le C, le Pascal, le Fortran et le Cobol sont des exemples de langage de programmation qui implémentent le paradigme impératif.Il y a essentiellement deux paradigmes déclaratifs ; ce sont le paradigme fonctionnel et le paradigme logique. En paradigme fonctionnel, le programme décrit des fonctions mathématiques. En paradigme logique, il décrit des prédicats : c'est-à-dire des déclarations qui, une fois instanciées, peuvent être vraies ou fausses ou ne pas recevoir de valeur de vérité (quand l'évaluation du prédicat ne se termine pas). Dans un modèle d'implantation, une machine abstraite effectue les opérations nécessaires pour calculer le résultat de chaque fonction ou chaque prédicat. Dans ces paradigmes, une variable n'est pas modifiée par affectation. Une des caractéristiques principales est la transparence référentielle, qui fait qu'une expression peut être remplacée par son résultat sans changer le comportement du programme. Fonctionnel Le paradigme fonctionnel a pour principe l'évaluation de formules, afin d'utiliser le résultat pour d'autres calculs ; il s'appuie sur la récursivité et il a pour modèle le lambda-calcul, plus précisément la réduction en forme normale de tête. Tous les calculs évaluent des expressions ou font appel à des fonctions. Pour simplifier, le résultat d'un calcul sert pour le calcul ou les calculs qui ont besoin de son résultat jusqu'à ce que la fonction qui produit le résultat du programme ait été évaluée. Le paradigme fonctionnel a été introduit par les langages Lisp et ISWIM ainsi qu'en ce qui concerne les fonctions récursives par Algol 60, dans les années 1960. Des langages tels que Ruby et Scala supportent plusieurs paradigmes dont le paradigme fonctionnel, tandis qu'Haskell ne supporte que le paradigme fonctionnel et OCaml privilégie le paradigme fonctionnel qu'il partage avec le paradigme objet et une petite dose d'impératif. Logique Le paradigme logique vise à répondre à une question par des recherches dans un ensemble, en utilisant des axiomes, des requêtes et des règles de déduction. L'exécution d'un programme est une cascade de recherches de faits dans un ensemble, en invoquant des règles de déduction. Les données obtenues, peuvent être associées à un autre ensemble de règles et peuvent alors être utilisées dans le cadre d'une autre recherche. L'exécution du programme se fait par évaluation : le système effectue une recherche de toutes les affirmations qui, par déduction, correspondent à au moins un élément de l'ensemble. Le programmeur exprime les règles, et le système pilote le processus. Le paradigme logique a été introduit par le langage Prolog en 1970.Le paradigme orienté objet est destiné à faciliter le découpage d'un grand programme en plusieurs modules isolés les uns des autres. Il introduit les notions d'objet et d'héritage. Un objet contient les variables et les fonctions en rapport avec un sujet. Les variables peuvent être privées, c'est-à-dire qu'elles peuvent être manipulées uniquement par l'objet qui les contient. Un objet contient implicitement les variables et les fonctions de ses ancêtres, et cet héritage aide à réutiliser du code. Le paradigme orienté objet permet d'associer fortement les données avec les procédures. Il a été introduit par le langage Simula dans les années 1960 et est devenu populaire dans les années 1980, quand l'augmentation de la puissance de calcul des ordinateurs a permis d'exécuter des grands programmes. Divers langages de programmation ont été enrichis en vue de permettre la programmation orientée objet ; c'est le cas de C++ (dérivé du langage C), Simula, Smalltalk, Swift et Java sont des langages de programmation en paradigme orienté objet.En paradigme concurrent un programme peut effectuer plusieurs tâches en même temps. Ce paradigme introduit les notions de thread, d'attente active et d'appel de fonction à distance. Ces notions ont été introduites dans les années 1980 lorsque, à la suite de l'évolution technologique, un ordinateur est devenu une machine comportant plusieurs processeurs et capable d'effectuer plusieurs tâches simultanément. Les langages de programmation contemporains de 2013 tels que C++ et Java sont adaptés aux microprocesseurs multi-cœur et permettent de créer et manipuler des threads. Plus récemment, on a vu apparaître des langages intégralement orientés vers la gestion de la concurrence, comme le langage Go.Dans la grande majorité des langages de programmation, le code source est un texte, ce qui rend difficile l'expression des objets bidimensionnels. Un langage de programmation tel que Delphi ou C# permet de manipuler des objets par glisser-déposer et le dessin ainsi obtenu est ensuite traduit en une représentation textuelle orientée objet et événementielle. Le paradigme visuel a été introduit à la fin des années 1980 par Alan Kay dans le langage Smalltalk, dans le but de faciliter la programmation des interfaces graphiques.Alors qu'un programme interactif pose une question et effectue des actions en fonction de la réponse, en style événementiel le programme n'attend rien et est exécuté lorsque quelque chose s'est passé. Par exemple, l'utilisateur déplace la souris ou presse sur un bouton. Dans ce paradigme, la programmation consiste à décrire les actions à prendre en réponse aux événements. Et une action peut en cascade déclencher une autre action correspondant à un autre événement. Le paradigme événementiel a été introduit par le langage Simula dans les années 1970. Il est devenu populaire à la suite de l'avènement des interfaces graphiques et des applications web.Avec l’avènement de l'Internet dans les années 1990, les données, les images ainsi que le code s'échangent entre ordinateurs. Si un résultat est demandé à un ordinateur, celui-ci peut exécuter le programme nécessaire et envoyer le résultat. Il peut également envoyer le code nécessaire à l'ordinateur client pour qu'il calcule le résultat lui-même. Le programme est rarement traduit en langage machine, mais plutôt interprété ou traduit en une forme intermédiaire, le bytecode, qui sera exécuté par une machine virtuelle, ou traduit en langage machine au moment de l'exécution (just-in-time). Java, PHP et Javascript sont des langages de programmation basée web.L'utilisation d'un langage est rendue possible par un traducteur automatique. Un programme qui prend un texte écrit dans ce langage pour en faire quelque chose, en général soit :Un programme qui traduit le texte dans un langage qui permettra son exécution, tel le langage machine, le bytecode ou le langage assembleur.Un programme qui exécute les instructions demandées. Il joue le même rôle qu'une machine qui reconnaîtrait ce langage.Chaque appareil informatique a un ensemble d'instructions qui peuvent être utilisées pour effectuer des opérations. Les instructions permettent d'effectuer des calculs arithmétiques ou logiques, déplacer ou copier des données, ou bifurquer vers l'exécution d'autres instructions. Ces instructions sont enregistrées sous forme de séquences de bits, où chaque séquence correspond au code de l'opération à effectuer et aux opérandes, c'est-à-dire aux données concernées ; c'est le langage machine.La traduction s'effectue en plusieurs étapes. En premier lieu, le traducteur effectue une analyse lexicale où il identifie les éléments du langage utilisés dans le programme. Dans l'étape suivante, l'analyse syntaxique, le traducteur construit un diagramme en arbre qui reflète la manière dont les éléments du langage ont été combinés dans le programme, pour former des instructions. Puis, lors de l'analyse sémantique, le traducteur détermine s'il est possible de réaliser l'opération et les instructions qui seront nécessaires dans le langage cible.Dans le langage de programmation assembleur, des mots aide-mémoire (mnémonique) sont utilisés pour référer aux instructions de la machine. Les instructions diffèrent en fonction des constructeurs et il en va de même pour les mnémoniques. Un programme assembleur traduit chaque mnémonique en la séquence de bits correspondante.Les langages de programmation fonctionnent souvent à l'aide d'un runtime.Un runtime (traduction : exécuteur) est un ensemble de bibliothèques logicielles qui mettent en œuvre le langage de programmation, permettant d'effectuer des opérations simples, telles que copier des données, mais aussi des opérations beaucoup plus complexes.Lors de la traduction d'un programme vers le langage machine, les opérations simples sont traduites en les instructions correspondantes en langage machine tandis que les opérations complexes sont traduites en des utilisations des fonctions du runtime. Dans certains langages de programmation, la totalité des instructions sont traduites en des utilisations du runtime qui sert alors d'intermédiaire entre les possibilités offertes par la plateforme informatique et les constructions propres au langage de programmation.Chaque langage de programmation a une manière conventionnelle de traduire l'exécution de procédures ou de fonctions, de placer les variables en mémoire et de transmettre des paramètres. Ces conventions sont appliquées par le runtime. Les runtime servent également à mettre en œuvre certaines fonctionnalités avancées des langages de programmation telles que le ramasse-miettes, ou la réflexion.Les langages de programmation sont couramment auto-implémentés, c'est-à-dire que le compilateur pour ce langage de programmation est mis en œuvre dans le langage lui-même. Exemple : un compilateur pour le langage Pascal peut être écrit en langage Pascal.Les fonctionnalités avancées telles que le ramasse-miettes (anglais garbage collector), la manipulation des exceptions, des événements ou des threads, ainsi que la liaison tardive et la réflexion sont mises en œuvre par les runtime des langages de programmation.Un mécanisme qui supprime les variables inutilisées et libère l'espace mémoire qui leur avait été réservé.Un fait inattendu, souvent accidentel, entraîne l'échec du déroulement normal du programme, et ce fait exceptionnel doit être pris en charge par le programme avant de pouvoir continuer. Certains langages de programmation permettent de provoquer délibérément l'arrêt du déroulement normal du programme.Une procédure qui va être exécutée lorsqu'une condition particulière est rencontrée. Les événements sont notamment utilisés pour mettre en œuvre les interfaces graphiques.Une suite d'instructions en train d'être exécutée. Les langages de programmation qui manipulent les threads permettent d'effectuer plusieurs tâches simultanément. Cette possibilité d'exécution simultanée, offerte par les systèmes d'exploitation, est également offerte en allégé par les runtime des langages de programmation.Le procédé de liaison (anglais late binding ou dynamic binding) consiste à associer chaque identifiant d'un programme avec l'emplacement de mémoire concerné. Cette opération peut être effectuée lors de la traduction du programme, au cours de l'exécution du programme ou juste avant, elle est dite tardive lorsque l'opération de liaison est effectuée très tard, juste avant que l'emplacement concerné ne soit utilisé.La possibilité pour un programme d'obtenir des informations concernant ses propres caractéristiques. Des instructions du langage de programmation permettent à un programme d'obtenir des informations sur lui-même et de les manipuler comme des données.Une structure permettant de manipuler des traits impératifs dans des langages fonctionnels purs.Bien que la notion de programme apparaisse progressivement au cours de la deuxième moitié du XIXe siècle, les premiers langages de programmation n'apparaissent qu'autour de 1950. Chacun pouvant créer son propre langage, il est impossible de déterminer le nombre total de langages existant à l'heure actuelle.On peut aussi classer les langages de programmation en fonction de leur utilisation car beaucoup de langages sont spécialisés à une application ou à un domaine particulier.Ce type de langage est utilisé pour une plus grande interaction entre un client et un serveur.Du côté du serveur web, cela permet de produire des pages dont le contenu est généré à chaque affichage. Ces langages sont par ailleurs souvent couplés avec un langage pour communiquer avec des bases de données (exemples : PHP, LiveCode).Côté client (en général le navigateur web), ces langages offrent la possibilité de réagir à certaines actions de l'utilisateur sans avoir à questionner le serveur. Par exemple, le JavaScript d'une page Web peut réagir aux saisies de l'utilisateur dans un formulaire (et vérifier le format des données).Certains langages permettent de développer à la fois les aspects client et serveur. C'est le cas d'Ocsigen, de Hop, de Dart ou bien encore du Server-Side JavaScript.On désigne parfois par langage de programmation théorique les systèmes formels utilisés pour décrire de façon théorique le fonctionnement des ordinateurs. Ils ne servent pas à développer des applications mais à représenter des modèles et démontrer certaines de leurs propriétés.On peut citer la machine de Turing et le ?-calcul de Church, qui datent tous les deux des années 1930, et donc antérieurs à l'invention de l'ordinateur. Le ?-calcul a par la suite servi de base théorique à la famille des langages de programmation fonctionnelle. Dans les années 1980, Robin Milner a mis au point le ?-calcul pour modéliser les systèmes concurrents.Les langages exotiques ont pour but de créer des grammaires complètes et fonctionnelles mais dans un paradigme éloigné des conventions. Beaucoup sont d'ailleurs considérés comme des blagues.Ces langages sont généralement difficiles à mettre en pratique et donc rarement utilisés.ABEL, langage pour la programmation électronique des PLDCDuce, langage fonctionnel d'ordre supérieur pour la manipulation de documents au format XML.Forme de Backus-Naur (BNF), formalisation des langages de programmationPROMELA, langage de spécification de systèmes asynchronesVRML, description de scènes en trois dimensions Langages synchrones Langages de programmation synchrones pour les systèmes réactifs : Esterel, Lustre. Langages à vocation pédagogique Les pseudo-codes ont généralement un but uniquement pédagogique.Logo est un langage fonctionnel simple à apprendre.Dans les années 1990, le langage BASIC était souvent conseillé pour débuter. Il avait cependant la réputation de favoriser la prise de mauvaises habitudes de programmation.Le Processing est un langage simplifié qui s'appuie sur Java. Il permet un développement d'applications fenêtrées sur tout type d'ordinateur équipé de Java.L'Arduino est un langage simplifié s'appuyant sur C/C++. Il permet un développement simple de projets électroniques à partir de carte Arduino (AVR).L'ArduinoEDU est un langage encore plus simple, en français, pour les grands débutants s'appuyant sur le langage C/C++/Arduino. Il permet un développement très simple de projets électroniques à partir de cartes Arduino (AVR).Flowgorithm est un outil de création et modification graphique de programmes informatiques sous forme d'Algorigramme. Langages pour l'électronique numérique Verilog, VHDL : langages de description matérielle, permettant de synthétiser de l'électronique numérique (descriptions de portes logiques) et d'en simuler le fonctionnementSystemC, langage de description matérielle de plus haut niveau que les précédents et permettant une simulation plus rapide Langages pour la statistique R, SAS et xLispStat sont à la fois un langage de statistiques et un logiciel. Langages de programmation de Commande Numérique (C.N.) Une machine-outil automatisée, ou Commande Numérique (C.N.), a besoin d'un langage de programmation pour réaliser les opérations de tournage ou de fraisage… Langages de programmation des automates programmables industriels (API) Sequential function chart, langage graphique, dérivé du grafcet (NB : le grafcet définit les spécifications de façon graphique).Langage Ladder, langage graphique. Langages de programmation audio Nyquist est un langage de synthèse et d'analyse sonore. Pure Data est un logiciel de création musicale graphique qui repose sur un langage de programmation procédural.Six chercheurs de trois universités portugaises ont mené une étude comparative de 27 langages de programmation, intitulée « Energy Efficiency Across Programming Languages ». Ils ont étudié la consommation d'énergie, le temps d'exécution et l'utilisation de la mémoire. Pour obtenir un ensemble de programmes comparables, les chercheurs ont exploré le Computer Language Benchmarks Game (CLBG).Le tableau obtenu présente les résultats globaux (en moyenne) pour la consommation d'énergie (Energy), le temps d'exécution (Time) et la consommation maximale de la mémoire (Mb) normalisés par rapport au langage le plus efficace pour le critère mesuré.Les cinq meilleurs langages sont :C : 1,00Rust : 1,03C++ : 1,34Ada : 1,70Java : 1,98C : 1,00Rust : 1,04C++ : 1,56Ada : 1,85Java : 1,89Pascal : 1,00Go : 1,05C : 1,17Fortran : 1,24C++ : 1,34La popularité de chaque langage est difficilement quantifiable ; néanmoins, il existe l'index TIOBE, calculé mensuellement, qui se base sur le nombre de formations/cours destinée aux ingénieurs et le nombre de revendeurs/free-lance spécialisés dans un langage de programmation. C'est une information parcellaire mais qui peut donner un ordre d'idée sur les tendances en matière de préférence des programmeurs.Chaque appareil informatique a un ensemble d'instructions qui peuvent être utilisées pour effectuer des opérations. Les instructions permettent d'effectuer des calculs arithmétiques ou logiques, déplacer ou copier des données, ou bifurquer vers l'exécution d'autres instructions. Ces instructions sont enregistrées sous forme de séquences de bits, où chaque séquence correspond au code de l'opération à effectuer et aux opérandes, c'est-à-dire aux données concernées ; c'est le langage machine.La traduction s'effectue en plusieurs étapes. En premier lieu, le traducteur effectue une analyse lexicale où il identifie les éléments du langage utilisés dans le programme. Dans l'étape suivante, l'analyse syntaxique, le traducteur construit un diagramme en arbre qui reflète la manière dont les éléments du langage ont été combinés dans le programme, pour former des instructions. Puis, lors de l'analyse sémantique, le traducteur détermine s'il est possible de réaliser l'opération et les instructions qui seront nécessaires dans le langage cible.Dans le langage de programmation assembleur, des mots aide-mémoire (mnémonique) sont utilisés pour référer aux instructions de la machine. Les instructions diffèrent en fonction des constructeurs et il en va de même pour les mnémoniques. Un programme assembleur traduit chaque mnémonique en la séquence de bits correspondante.Les langages de programmation fonctionnent souvent à l'aide d'un runtime.Liste de langages de programmationIdentificateurTuring-complet(en) Chronologie des langages de programmation Portail de la programmation informatique"
Informatique;"Un ordinateur est un système de traitement de l'information programmable tel que défini par Alan Turing et qui fonctionne par la lecture séquentielle d'un ensemble d'instructions, organisées en programmes, qui lui font exécuter des opérations logiques et arithmétiques. Sa structure physique actuelle fait que toutes les opérations reposent sur la logique binaire et sur des nombres formés à partir de chiffres binaires. Dès sa mise sous tension, un ordinateur exécute, l'une après l'autre, des instructions qui lui font lire, manipuler, puis réécrire un ensemble de données déterminées par une mémoire morte d'amorçage. Des tests et des sauts conditionnels permettent de passer à l'instruction suivante et donc d'agir différemment en fonction des données ou des nécessités du moment ou de l'environnement.Les données à manipuler sont acquises soit par la lecture de mémoires, soit en provenance de périphériques internes ou externes (déplacement d'une souris, touche appuyée sur un clavier, déplacement d'un stylet sur une tablette, température et autres mesures physiques…). Une fois utilisés, ou manipulés, les résultats sont écrits soit dans des mémoires, soit dans des composants qui peuvent transformer une valeur binaire en une action physique (écriture sur une imprimante ou sur un moniteur, accélération ou freinage d'un véhicule, changement de température d'un four…). L'ordinateur peut aussi répondre à des interruptions qui lui permettent d’exécuter des programmes de réponses spécifiques à chacune, puis de reprendre l’exécution séquentielle du programme interrompu.De 1834 à 1837, Charles Babbage conçoit une machine à calculer programmable en associant un des descendants de la Pascaline (première machine à calculer mécanique inventée par Blaise Pascal) avec des instructions écrites sur le même type de cartes perforées que celles inventées par Joseph Marie Jacquard pour ses métiers à tisser. C'est durant cette période qu'il imagine la plupart des caractéristiques de l'ordinateur moderne. Babbage passe le reste de sa vie à essayer de construire sa machine analytique, mais sans succès. Nombre de personnes essayent de développer cette machine, mais c'est cent ans plus tard, en 1937, qu'IBM inaugure l'ère de l'informatique en commençant le développement de l'ASCC/Mark I, une machine construite sur l'architecture de Babbage qui, une fois réalisée, est considérée comme l'achèvement de son rêve.La technique actuelle des ordinateurs date du milieu du xxe siècle. Les ordinateurs peuvent être classés selon plusieurs critères tels que le domaine d'application, la taille ou l'architecture.Le mot « ordinateur » fut introduit par IBM France en 1955, après que François Girard, alors responsable du service publicité de l'entreprise, eut l'idée de consulter son ancien professeur de lettres à Paris, Jacques Perret. Avec Christian de Waldner, alors président d'IBM France, ils demandèrent au professeur Perret de suggérer un « nom français pour sa nouvelle machine électronique destinée au traitement de l'information (IBM 650), en évitant d'utiliser la traduction littérale du mot anglais computer (« calculateur » ou « calculatrice »), qui était à cette époque plutôt réservé aux machines scientifiques ».En 1911, une description de la machine analytique de Babbage utilisait le mot ordonnateur pour en décrire son organe moteur : « Pour aller prendre et reporter les nombres… et pour les soumettre à l’opération demandée, il faut qu'il y ait dans la machine un organe spécial et variable : c'est l'ordonnateur. Cet ordonnateur est constitué simplement par des feuilles de carton ajourées, analogues à celle des métiers Jacquard… ».Le professeur proposa un mot composé centré autour d'ordonnateur : celui qui met en ordre et qui avait aussi la notion d'ordre ecclésiastique dans l'église catholique (ordinant). Il suggéra plus précisément « ordinatrice électronique », le féminin ayant pu permettre, selon lui, de mieux distinguer l'usage religieux de l'usage comptable du mot.« IBM France retint le mot ordinateur et chercha au début à protéger ce nom comme une marque. Mais le mot fut facilement et rapidement adopté par les utilisateurs et IBM France décida au bout de quelques mois de le laisser dans le domaine public. »Selon Bernard Cohen, auteur de l'ouvrage intitulé Howard Aiken: Portrait of a computer pioneer, « les historiens des technologies et les informaticiens intéressés en histoire, ont adopté un certain nombre de caractéristiques qui définissent un ordinateur. C'est ainsi que la question de savoir si le Mark I était ou n'était pas un ordinateur ne dépend pas d'une opinion majoritaire mais plutôt de la définition utilisée. Souvent, quelques-unes des caractéristiques fondamentales nécessaires pour être considérées comme un ordinateur sont :Qu'il soit électronique ;Numérique (au lieu d'analogique) ;Qu'il soit programmable ;Qu'il puisse exécuter les quatre opérations élémentaires (addition, soustraction, multiplication, division) et — souvent — qu'il puisse extraire une racine carrée ou adresser une table qui en contient ;Qu'il puisse exécuter des programmes enregistrés en mémoire.Une machine n'est généralement pas classifiée comme un ordinateur à moins qu'elle n'ait des caractéristiques supplémentaires comme la possibilité d’exécuter des opérations spécifiques automatiquement et ceci d'une façon contrôlée et dans une séquence prédéterminée. Pour d'autres historiens et informaticiens, il faut aussi que la machine ait été vraiment construite et qu'elle ait été complètement opérationnelle. »Sans une définition stricte il est impossible d'identifier la machine qui devint le premier ordinateur, mais il faut remarquer certaines des étapes fondamentales qui vont du développement du concept de la machine à calculer programmable par Charles Babbage en 1837 au premier développement de l'ère de l'informatique cent ans plus tard.En 1834, Charles Babbage commence à développer une machine à calculer programmable, sa machine analytique. Il pense la programmer grâce à un cylindre à picots comme dans les automates de Vaucanson, mais, deux ans plus tard, il remplace ce cylindre par la lecture de cartes Jacquard, et ainsi crée une machine à calculer infiniment programmable.En 1843, Ada Lovelace écrit le premier programme informatique pour calculer les nombres de Bernoulli, pour la machine analytique qui ne sera jamais construite.Henry Babbage construit une version extrêmement simplifiée de l'unité centrale de la « machine analytique » de son père et l'utilise en 1906, pour calculer et imprimer automatiquement les quarante premiers multiples du nombre Pi avec une précision de vingt-neuf décimales, démontrant sans ambiguïté que le principe de la machine analytique était viable et réalisable. En 1886, sa plus grande contribution fut de donner un ensemble mécanique de démonstration d'une des machines de son père à l'université Harvard. C'est cinquante ans plus tard, après avoir entendu la présentation de Howard Aiken sur son super calculateur, qu'un technicien de Harvard, Carmello Lanza, lui fit savoir qu'une machine similaire avait déjà été développée et qu'il lui montra l'ensemble mécanique de démonstration donné par Henry Babbage qui se trouvait dans un des greniers de l'université ; c'est ainsi qu'il découvrit les travaux de Babbage et qu'il les incorpora dans la machine qu'il présenta à IBM en 1937. C'était la troisième fois qu'il essayait de trouver un sponsor pour le développement de sa machine car son projet avait déjà été rejeté deux fois avant l'intégration des travaux de Babbage dans l'architecture de sa machine (une fois par la Monroe Calculating Company et une fois par l'université Harvard).Leonardo Torres Quevedo remplaça toutes les fonctions mécaniques de Babbage par des fonctions électromécaniques (addition, soustraction, multiplication et division mais aussi la lecture de cartes et les mémoires). En 1914 et en 1920, Il construisit deux machines analytiques, non programmable, extrêmement simplifiées mais qui montraient que des relais électromécaniques pouvaient être utilisés dans une machine à calculer qu'elle soit programmable ou non. Sa machine de 1914 avait une petite mémoire électromécanique et son arithmomètre de 1920, qu'il développa pour célébrer le centième anniversaire de l'invention de l'arithmomètre, était commandé par une machine à écrire qui était aussi utilisée pour imprimer ses résultats.Percy Ludgate améliora et simplifia les fonctions mécaniques de Babbage mais ne construisit pas de machine. Et enfin, Louis Couffignal essaya au début des années 1930, de construire une machine analytique « purement mécanique, comme celle de Babbage, mais sensiblement plus simple », mais sans succès. C'est cent ans après la conceptualisation de l'ordinateur par Charles Babbage que le premier projet basé sur l'architecture de sa machine analytique aboutira. En effet, c'est en 1937 qu'Howard Aiken présenta à IBM un projet de machine à calculer programmable qui sera le premier projet qui finira par une machine qui puisse être, et qui sera utilisée, et dont les caractéristiques en font presque un ordinateur moderne. Et donc, bien que le premier ordinateur ne sera jamais déterminé à l’unanimité, le début de l'ère de l'informatique moderne peut être considéré comme la présentation d'Aiken à IBM, en 1937, qui aboutira par l'ASCC.Les machines à calculer jouèrent un rôle primordial dans le développement des ordinateurs pour deux raisons tout à fait indépendantes. D'une part, pour leurs origines : c'est pendant le développement d'une machine à calculer automatique à imprimante qu'en 1834 Charles Babbage commença à imaginer sa machine analytique, l’ancêtre des ordinateurs. C’était une machine à calculer programmée par la lecture de cartes perforées (inspirées du Métier Jacquard), avec un lecteur de cartes pour les données et un pour les programmes, avec des mémoires, un calculateur central et des imprimantes et qui inspirera le développement des premiers ordinateurs à partir de 1937 ; ce qui nous amènera aux mainframes des années 1960.D'autre part, leur propagation se fit grâce à la commercialisation en 1971 du premier microprocesseur, l'Intel 4004, qui fut inventé pendant le développement d'une machine à calculer électronique pour la compagnie japonaise Busicom, qui est à l'origine de l'explosion de la micro-informatique à partir de 1975 et qui réside au cœur de tous les ordinateurs actuels quelles que soient leurs tailles ou fonctions (bien que seulement 2 % des microprocesseurs produits chaque année soient utilisés comme unités centrales d'ordinateur, les 98 % restant sont utilisés dans la construction de voitures, de robots ménagers, de montres, de caméras de surveillance…).Outre les avancées observées dans l'industrie du textile et celles de l'électronique, les avancées de la mécanographie à la fin du XIXe siècle, pour achever les recensements aux États-Unis, la mécanisation de la cryptographie au début du XXe siècle, pour chiffrer puis déchiffrer automatiquement des messages, le développement des réseaux téléphoniques (à base de relais électromécaniques), sont aussi à prendre en compte pour comprendre l'avènement de ce nouveau genre de machine qui ne calculent pas (comme font/faisaient les calculatrices), mais lisent et interprètent des programmes qui -eux- calculent. Pour le monde des idées, avant l'invention de ces nouvelles machines, l'élément fondateur de la science informatique est en 1936, la publication de l'article On Computable Numbers with an Application to the Entscheidungsproblem par Alan Turing qui allait déplacer le centre de préoccupation de certains scientifiques (mathématiciens et logiciens) de l'époque, du sujet de la calculabilité (ou décidabilité) ouvert par Hilbert, malmené par Gödel, éclairci par Church, vers le sujet de la mécanisation du calcul (ou calculabilité effective). Dans ce texte de 36 pages, Turing expose une machine théorique capable d'effectuer tout calcul ; il démontre que cette machine est aussi puissante, au niveau du calcul, que tout être humain. Autrement dit, un problème mathématique possède une solution, si et seulement si, il existe une machine de Turing capable de résoudre ce problème. Par la suite, il expose une machine de Turing universelle apte à reproduire toute machine de Turing, il s'agit des concepts d'ordinateur, de programmation et de programme. Il termine en démontrant qu'il existe au moins un problème mathématique formellement insoluble, le problème de l'arrêt.Peu avant la Seconde Guerre mondiale, apparurent les premières calculatrices électromécaniques, construites selon les idées d'Alan Turing. Les machines furent vite supplantées par les premiers calculateurs électroniques, nettement plus performants.La fin des années 1930 virent, pour la première fois dans l'histoire de l'informatique, le début de la construction de deux machines à calculer programmables. Elles utilisaient des relais et étaient programmées par la lecture de rouleaux perforés et donc, pour certains, étaient déjà des ordinateurs. Elles ne furent mises en service qu'au début des années 1940, faisant ainsi de 1940 la première décennie dans laquelle on trouve des ordinateurs et des machines à calculer programmables totalement fonctionnels. C'est d'abord en 1937 que Howard Aiken, qui avait réalisé que la machine analytique de Babbage était le type de machine à calculer qu'il voulait développer, proposa à IBM de la créer et de la construire ; après une étude de faisabilité, Thomas J. Watson accepta de la construire en 1939 ; elle fut testée en 1943 dans les locaux d'IBM et fut donnée et déménagée à l'université Harvard en 1944, changeant son nom de ASCC à Harvard Mark I ou Mark I.Mais c'est aussi Konrad Zuse qui commença le développement de son Zuse 3, en secret, en 1939, et qui le finira en 1941. Parce que le Zuse 3 resta inconnu du grand public jusqu’après la fin de la Seconde Guerre mondiale (sauf des services secrets américains qui le détruisirent dans un bombardement en 1943), ses solutions très inventives ne furent pas utilisées dans les efforts communs mondiaux de développement de l’ordinateur.Six machines furent construites durant ces 9 ans. Elles furent toutes décrites, au moins une fois, dans la multitude de livres de l'histoire de l'informatique, comme étant le premier ordinateur ; aucune autre machine, construite ultérieurement, ne fut décrite comme telle. Ces six précurseurs peuvent être divisées en trois groupes bien spécifiques :d'une part, deux machines à calculer. Ces deux machines n'étaient pas programmables, l'une était électromécanique, l'autre électronique :1937 : l'ABC qui pouvait résoudre des équations linéaires et reconnu comme le premier ordinateur numérique,1939 : le Complex Number Calculator de George Stibitz, conçu pour faire des opérations sur des nombres complexes ;d'autre part, deux machines électromécaniques programmables, programmées par la lecture de rouleaux perforés, mais qui ne possédaient pas d'instruction de branchement conditionnel, et donc ne pouvaient aller d'une partie d'un programme à une autre :1941 : le Zuse 3,1944 : l'ASCC/Mark I d'IBM ;« Sans un branchement conditionnel, et donc l’implémentation mécanique du mot SI, le plus grand des calculateurs ne serait qu'une super machine à calculer. Il pourrait être comparé à une ligne d'assemblage, tout étant organisé du début à la fin, avec aucune possibilité de changement une fois que la machine est mise en marche. »— Andrew Hodges,  Alan Turing: the enigma, 1983.et enfin, deux machines électroniques spécialisées. Initialement ces machines ne pouvaient faire que cela, et étaient programmées par le changement de fils et d'interrupteurs :le Colossus, conçu pour déchiffrer des messages secrets allemands,1946 : l'ENIAC, conçu pour calculer des trajectoires balistiques.« L'ENIAC et le Colosse étaient comme deux kits à assembler, desquelles beaucoup de machines similaires, mais différentes, pouvaient être construites. Aucun n’essaya d’implémenter l'universalité de la « machine de Babbage » dans laquelle la machine n'est jamais modifiée, et où seulement les instructions sont réécrites sur des cartes perforées. »— Andrew Hodges,  Alan Turing: the enigma, 1983.De ces six machines, seulement quatre furent connues de leurs contemporains, les deux autres, le Colosse et le Z3, utilisées dans l'effort de guerre, ne furent découvertes qu'après la fin de la Seconde Guerre mondiale, et donc ne participèrent pas au développement communautaire mondial des ordinateurs. Seulement deux de ces machines furent utilisées dans les années 1950, l'ASCC/Mark I et l'ENIAC, et chacune fut éventuellement modifiée pour en faire une machine Turing-complet. En juin 1945 est publié un article fondateur de John von Neumann donnant les bases de l'architecture utilisée dans la quasi-totalité des ordinateurs depuis lors. Dans cet article, von Neumann veut concevoir un programme enregistré et programmé dans la machine. La première machine correspondant à cette architecture, dite depuis architecture de von Neumann est une machine expérimentale la Small-Scale Experimental Machine (SSEM ou baby) construite à Manchester en juillet 1948. En août 1949 la première machine fonctionnelle, fondée sur les bases de von Neumann fut l'EDVAC.Cette chronologie demande qu'un ordinateur soit électronique et donc elle commence, en 1946, avec l'ENIAC qui, au départ, était programmé avec des interrupteurs et par le positionnement de fils sur un commutateur, comme sur un ancien standard téléphonique. Les ordinateurs de cette période sont énormes avec des dizaines de milliers de tubes à vide. L'ENIAC faisait 30 m de long, 2,40 m de haut et pesait 30 tonnes. Ces machines n’étaient pas du tout fiables, par exemple, en 1952, dix-neuf mille tubes furent remplacés sur l'ENIAC, soit plus de tubes qu'il n'en contient.« L'ENIAC prouva, sans ambiguïté, que les principes de base de l'électronique était bien fondés. Il était vraiment inévitable que d'autres machines à calculer de ce type seraient perfectionnées grâce aux connaissances et à l’expérience acquises sur cette première. »De nouveau, le titre de premier ordinateur commercialisé dépend de la définition utilisée ; trois ordinateurs sont souvent cités. En premier, le BINAC, conçu par la Eckert–Mauchly Computer Corporation et livré à la Northrop Corporation en 1949 qui, après sa livraison, ne fut jamais fonctionnel,. En deuxième, le Ferranti Mark I, dont le prototype avait été développé par l'université de Manchester, fut amélioré et construit en un exemplaire par la société Ferranti et revendu à l'université de Manchester en février 1951. Et en dernier, UNIVAC I, conçu par la « Eckert–Mauchly Computer Corporation », dont le premier fut vendu à l'United States Census Bureau le 30 mars 1951. Une vingtaine de machines furent produites et vendues entre 1951 et 1954.« L'utilisation de transistors au milieu des années 1950 changea le jeu complètement. Les ordinateurs devinrent assez fiables pour être vendus à des clients payants sachant qu'ils fonctionneraient assez longtemps pour faire du bon travail. » Les circuits intégrés réduisirent la taille et le prix des ordinateurs considérablement. Les moyennes entreprises pouvaient maintenant acheter ce genre de machines.Les circuits intégrés permettent de concevoir une informatique plus décentralisée les constructeurs souhaitant concurrencer le géant IBM. Le microprocesseur fut inventé en 1969 par Ted Hoff d'Intel pendant le développement d'une calculatrice pour la firme japonaise Busicom. Intel commercialisera le 4004 fin 1971. Ted Hoff avait copié l'architecture du PDP-8, le premier mini-ordinateur, et c'est grâce à la technologie de circuits intégrés LSI (large scale integration), qui permettait de mettre quelques milliers de transistors sur une puce qu'il put miniaturiser les fonctions d'un ordinateur en un seul circuit intégré. La fonction première du microprocesseur était de contrôler son environnement. Il lisait des interrupteurs, les touches d'un clavier et il agissait en exécutant les opérations requises (addition, multiplication, etc.) et en affichant les résultats. Le premier ordinateur personnel fut décrit dans le livre d'Edmund Berkeley, Giant brain, or machines that think, en 1949, et sa construction fut décrite dans une série d'articles du magazine Radio-Electronics à partir du numéro d'octobre 1950. En 1972, une société française développe le Micral, premier micro-ordinateur à être basé sur le microprocesseur 8008. Mais l’ordinateur qui créa l'industrie de l'ordinateur personnel est l'Altair 8800, qui fut décrit pour la première fois dans le magazine Radio-Electronics de janvier 1975. Bill Gates, Paul Allen, Steve Wozniak et Steve Jobs (ordre chronologique) firent tous leurs débuts dans la micro-informatique sur ce produit moins de six mois après son introduction.Les ordinateurs furent d'abord utilisés pour le calcul (en nombres entiers d'abord, puis flottants). On ne peut cependant les assimiler à de simples calculateurs, du fait de la possibilité quasi infinie de lancer d'autres programmes en fonction du résultat de calculs, ou de capteurs internes ou externes (température, inclinaison, orientation, etc.), ou de toute action de l'opérateur ou de son environnement.Dans l'architecture de von Neumann, les données sont banalisées et peuvent être interprétées indifféremment comme des nombres, des instructions, des valeurs logiques ou tout symbole défini arbitrairement (exemple : lettres de l’alphabet).Le calcul représente une des applications possibles. Dans ce cas, les données sont traitées comme des nombres.L’ordinateur est utilisé aussi pour ses possibilités d'organisation de l’information, entre autres sur des périphériques de stockage magnétique. On a calculé à la fin des années 1980 que sans les ordinateurs il faudrait toute la population française juste pour faire dans ce pays le seul travail des banques :cette capacité d’organiser les informations a généralisé l’usage du traitement de texte dans le grand public ;la gestion des bases de données relationnelles permet également de retrouver et de consolider des informations réparties vues par l'utilisateur comme plusieurs tables indépendantes.Cette création d'un néologisme fut à l'origine de traductions multiples des expressions supercomputer, superordinateur ou supercalculateur.L'expérience a appris à distinguer dans un ordinateur deux aspects, dont le second avait été au départ sous-estimé :l'architecture physique, matérielle (alias hardware ou hard) ;l'architecture logicielle (alias software ou soft).Un ordinateur très avancé techniquement pour son époque comme le Gamma 60 de la compagnie Bull n'eut pas le succès attendu, pour la simple raison qu'il existait peu de moyens de mettre en œuvre commodément ses possibilités techniques[réf. nécessaire].Le logiciel — et son complément les services (formation, maintenance…) — forme depuis le milieu des années 1980 l’essentiel des coûts d'équipement informatique, le matériel n’y ayant qu'une part minoritaire.Les ordinateurs peuvent être sensibles aux bombes IEM.[réf. nécessaire]Parmi toutes les machines inventées par l'Homme, l'ordinateur est celle qui se rapproche le plus du concept anthropologique suivant : Organe d'entrée, organe de traitement de l'information et organe de sortie. Chez l'humain, les organes d'entrée sont les organes sensoriels, l'organe de traitement est le cerveau dont les logiciels sont l'apprentissage avec des mises à jour constantes en cours de vie, puis les organes de sortie sont les muscles. Pour les ordinateurs modernes, les organes d'entrée sont le clavier et la souris et les organes de sortie, l'écran, l'imprimante, le graveur de DVD, etc. Les techniques utilisées pour fabriquer ces machines ont énormément changé depuis les années 1940 et sont devenues une technologie (c’est-à-dire un ensemble industriel organisé autour de techniques) à part entière depuis les années 1970. Beaucoup utilisent encore les concepts définis par John von Neumann, bien que cette architecture soit en régression : les programmes ne se modifient plus guère eux-mêmes (ce qui serait considéré comme une mauvaise pratique de programmation), et le matériel prend en compte cette nouvelle donne en séparant aujourd'hui nettement le stockage des instructions et des données, y compris dans les caches.L’architecture de von Neumann décomposait l’ordinateur en quatre parties distinctes :L’unité arithmétique et logique (UAL) ou unité de traitement : son rôle est d’effectuer les opérations de base, un peu comme le ferait une calculatrice ;L’unité de contrôle. C’est l’équivalent des doigts qui actionneraient la calculatrice ;La mémoire qui contient à la fois les données et le programme qui dira à l’unité de contrôle quels calculs faire sur ces données. La mémoire se divise entre mémoire vive (programmes et données en cours de fonctionnement) et mémoire permanente (programmes et données de base de la machine) ;Les entrées-sorties : dispositifs qui permettent de communiquer avec le monde extérieur.L’unité arithmétique et logique ou UAL est l’élément qui réalise les opérations élémentaires (additions, soustractions…), les opérateurs logiques (ET, OU, NI, etc.) et les opérations de comparaison (par exemple la comparaison d’égalité entre deux zones de mémoire). C’est l’UAL qui effectue les calculs de l’ordinateur. L’unité de contrôle prend ses instructions dans la mémoire. Celles-ci lui indiquent ce qu’elle doit ordonner à l’UAL et, comment elle devra éventuellement agir selon les résultats que celle-ci lui fournira. Une fois l’opération terminée, l’unité de contrôle passe soit à l’instruction suivante, soit à une autre instruction à laquelle le programme lui ordonne de se brancher.L'unité de contrôle facilite la communication entre l'unité arithmétique et logique, la mémoire ainsi que les périphériques. Elle gère la plupart des exécutions des instructions dans l'ordinateur.Au sein du système, la mémoire peut être décrite comme une suite de cellules numérotées contenant chacune une petite quantité d’informations. Cette information peut servir à indiquer à l’ordinateur ce qu’il doit faire (instructions) ou contenir des données à traiter. Dans la plupart des architectures, c'est la même mémoire qui est utilisée pour les deux fonctions. Dans les calculateurs massivement parallèles, on admet même que des instructions de programmes soient substituées à d’autres en cours d’opération lorsque cela se traduit par une plus grande efficacité. Cette pratique était jadis courante, mais les impératifs de lisibilité du génie logiciel l'ont fait régresser, hormis dans ce cas particulier, depuis plusieurs décennies. Cette mémoire peut être réécrite autant de fois que nécessaire. La taille de chacun des blocs de mémoire ainsi que la technologie utilisée ont varié selon les coûts et les besoins : 8 bits pour les télécommunications, 12 bits pour l’instrumentation (DEC) et 60 bits pour de gros calculateurs scientifiques (Control Data). Un consensus a fini par être trouvé autour de l’octet comme unité adressable et d’instructions sur format de 4 ou 8 octets.Dans tous les cas de figure, l'octet reste adressable, ce qui simplifie l'écriture des programmes. Les techniques utilisées pour la réalisation des mémoires ont compris des relais électromécaniques, des tubes au mercure au sein desquels étaient générées des ondes acoustiques, des transistors individuels, des tores de ferrite et enfin des circuits intégrés incluant des millions de transistors.Les dispositifs d’entrée/sortie permettent à l’ordinateur de communiquer avec l’extérieur. Ces dispositifs sont très importants, du clavier à l’écran. La carte réseau permet par exemple de relier les ordinateurs en réseau informatique, dont le plus grand est Internet. Le point commun entre tous les périphériques d’entrée est qu’ils convertissent l’information qu’ils récupèrent de l’extérieur en données compréhensibles par l’ordinateur. À l’inverse, les périphériques de sortie décodent l’information fournie par l’ordinateur afin de la rendre compréhensible par l’utilisateur.Ces différentes parties sont reliées par trois bus, le bus d'adresse, le bus de données et le bus de contrôle. Un bus est un groupement d'un certain nombre de fils électriques réalisant une liaison pour transporter des informations binaires codées sur plusieurs bits. Le bus d'adresse transporte les adresses générées par l'UCT (Unité Centrale de Traitement) pour sélectionner une case mémoire ou un registre interne de l'un des blocs. Le nombre de bits véhiculés par ce bus dépend de la quantité de mémoire qui doit être adressée. Le bus de données transporte les données échangées entre les différents éléments du système. Le bus de contrôle transporte les différents signaux de synchronisation nécessaires au fonctionnement du système : signal de lecture (RD), signal d'écriture (WR), signal de sélection (CS : Chip Select).La miniaturisation permet d’intégrer l’UAL et l’unité de contrôle au sein d’un même circuit intégré connu sous le nom de microprocesseur. Typiquement, la mémoire est située sur des circuits intégrés proches du processeur, une partie de cette mémoire, la mémoire cache, pouvant être située sur le même circuit intégré que l’UAL.L’ensemble est, sur la plupart des architectures, complété d’une horloge qui cadence le processeur. Bien sûr, on souhaite qu'elle soit le plus rapide possible, mais on ne peut pas augmenter sans limites sa vitesse pour deux raisons :plus l’horloge est rapide et plus le processeur dégage de la chaleur (selon le carré de la fréquence). Une trop grande température peut détériorer le processeur ;il existe une cadence où le processeur devient instable ; il génère des erreurs qui mènent le plus souvent à un plantage.La tendance a été à partir de 2004 de regrouper plusieurs UAL dans le même processeur, voire plusieurs processeurs dans la même puce. En effet, la miniaturisation progressive (voir Loi de Moore) le permet sans grand changement de coût. Une autre tendance, depuis 2006 chez ARM, est aux microprocesseurs sans horloge : la moitié de la dissipation thermique est en effet due aux signaux d'horloge quand le microprocesseur fonctionne ; de plus, un microprocesseur sans horloge a une consommation presque nulle quand il ne fonctionne pas : le seul signal d'horloge nécessaire est alors celui destiné au rafraîchissement des mémoires. Cet atout est important pour les modèles portables.Le principal écart fonctionnel aujourd’hui par rapport au modèle de von Neumann est la présence sur certaines architectures de deux antémémoires différentes : une pour les instructions et une pour les données (alors que le modèle de von Neumann spécifiait une mémoire commune pour les deux). La raison de cet écart est que la modification par un programme de ses propres instructions est aujourd’hui considérée (sauf sur les machines hautement parallèles) comme une pratique à proscrire. Dès lors, si le contenu du cache de données doit être récrit en mémoire principale quand il est modifié, on sait que celui du cache d’instructions n’aura jamais à l’être, d’où simplification des circuits et gain de performance.Les instructions que l’ordinateur peut comprendre ne sont pas celles du langage humain. Le matériel sait juste exécuter un nombre limité d’instructions bien définies. Des instructions typiques comprises par un ordinateur sont par exemple :Copier le contenu de la cellule 123 et le placer dans la cellule 456 ;Ajouter le contenu de la cellule 321 à celui de la cellule 654 ;Placer le résultat dans la cellule 777 ;Si le contenu de la cellule 999 vaut 0, exécuter l’instruction à la cellule 345.La plupart des instructions se composent de deux zones : l’une indiquant quoi faire, nommée code opération, et l’autre indiquant où le faire, nommée opérande.Au sein de l’ordinateur, les instructions correspondent à des codes — le code pour une copie étant par exemple 001. L’ensemble d’instructions qu’un ordinateur supporte se nomme son langage machine, langage qui est une succession de chiffres binaires, car les instructions et données qui sont comprises par le processeur (CPU) sont constituées uniquement de 0 (zéro) et de 1 (un) :0 = le courant électrique ne passe pas ;1 = le courant électrique passe.En général, ce type de langage n'est pas utilisé car on lui préfère ce que l’on appelle un langage de haut niveau qui est ensuite transformé en langage binaire par un programme spécial (interpréteur ou compilateur selon les besoins). Les programmes ainsi obtenus sont des programmes compilés compréhensibles par l'ordinateur dans son langage natif. Certains langages de programmation, comme l’assembleur sont dits langages de bas niveau car les instructions qu’ils utilisent sont très proches de "
Informatique;"La programmation, appelée aussi codage dans le domaine informatique, désigne l'ensemble des activités qui permettent l'écriture des programmes informatiques. C'est une étape importante du développement de logiciels (voire de matériel).L'écriture d'un programme se fait dans un langage de programmation. Un logiciel est un ensemble de programmes (qui peuvent être écrits dans des langages de programmation différents) destiné à la réalisation de certaines tâches par un (ou plusieurs) utilisateurs du logiciel.La programmation représente donc ici la rédaction du code source d'un logiciel. On utilise plutôt le terme développement pour dénoter l'ensemble des activités liées à la création d'un logiciel et des programmes qui le composent. Cela inclut la spécification du logiciel, sa conception, puis son implémentation proprement dite au sens de l'écriture des programmes dans un langage de programmation bien défini, ainsi que la vérification de sa correction, etc.La première machine programmable (c’est-à-dire machine dont les possibilités changent quand on modifie son programme) est probablement le métier à tisser de Jacquard, qui a été réalisé en 1801. La machine utilisait une suite de cartons perforés. Les trous indiquaient le motif que le métier suivait pour réaliser un tissage ; avec des cartes différentes le métier produisait des tissages différents. Cette innovation a été ensuite améliorée par Herman Hollerith d'IBM pour le développement de la fameuse carte perforée d'IBM.En 1936, la publication de l'article fondateur de la science informatique On Computable Numbers with an Application to the Entscheidungsproblem par Alan Turing allait donner le coup d'envoi à la création de l'ordinateur programmable. Il y présente sa machine de Turing, le premier calculateur universel programmable, et invente les concepts et les termes de programmation et de programme.Les premiers programmes d'ordinateur étaient réalisés avec un fer à souder et un grand nombre de tubes à vide (plus tard, des transistors). Les programmes devenant plus complexes, cela est devenu presque impossible, parce qu'une seule erreur rendait le programme entier inutilisable. Avec les progrès des supports de données, il devint possible de charger le programme à partir de cartes perforées, contenant la liste des instructions en code binaire spécifique à un type d'ordinateur particulier. La puissance des ordinateurs augmentant, on les utilisa pour faire les programmes, les programmeurs préférant naturellement rédiger du texte plutôt que des suites de 0 et de 1, à charge pour l'ordinateur d'en faire la traduction lui-même.Avec le temps, de nouveaux langages de programmation sont apparus, faisant de plus en plus abstraction du matériel sur lequel devraient tourner les programmes. Ceci apporte plusieurs facteurs de gain : ces langages sont plus faciles à apprendre, un programmeur peut produire du code plus rapidement, et les programmes produits peuvent tourner sur différents types de machines.JavaScriptPHPRubyJavaSwiftC#, C ou C++PythonJuliaScalaRL'immense majorité des programmes qui s'exécutent sur nos ordinateurs, téléphones et autres outils électroniques sont écrits dans des langages de programmation dits impératifs : les lignes du programme sont exécutées les unes après les autres. Chaque ligne du programme effectue soit une opération simple, soit exécute une fonction qui est elle-même une suite d'opérations simples.Le programme « Hello World! » est par tradition le premier programme écrit par tout programmeur, censé illustrer la syntaxe du langage de programmation. Le programme a pour unique fonction d'afficher le texte ""Hello World!"" dans la console ou dans une fenêtre de l'interface graphique.Voici une version d'un programme « Hello World! »:Le programme suivant écrit en langage simplifié et avec des commentaires, demande simplement à l'utilisateur d'entrer au clavier deux nombres entiers, et affiche leur quotient.Dans ce programme, les principales fonctionnalités de la programmation impérative sont utilisées : des variables de type nombre entier, nombre à virgule, chaîne de caractère, fonction calculant un résultat à partir de paramètres, fonction effectuant une tâche telle qu'afficher un message à l'écran, instruction if permettant d'exécuter un code ou un autre en fonction de la valeur de telle ou telle variable.Dans un programme informatique typique, on trouvera suivant les langages des boucles while ou for qui permettent d'exécuter un morceau de code en boucle ou simplement un certain nombre de fois, des new pour l'allocation dynamique de données (par exemple des tableaux), et très souvent des éléments de programmation objet permettant de structurer différemment le code et de créer des types de données personnalisés, ou encore des exceptions pour gérer certains cas d'erreurs plus facilement.On remarque que pour effectuer une tâche très simple, le code informatique peut être très laborieux, et encore ici on ne traite pas les erreurs (si l'utilisateur tape un mot au lieu d'un nombre), et l'affichage est minimaliste. C'est pourquoi les langages de programmation n'ont jamais cessé d'évoluer, dans le but d'aider le programmeur qui souhaite réaliser des programmes rapides à s'exécuter, sans dysfonctionnements, et surtout simples à écrire, du moins le plus possible.La phase de conception définit le but du programme. Si on fait une rapide analyse fonctionnelle d'un programme, on détermine essentiellement les données qu'il va traiter (données d'entrée), la méthode employée (appelée l'algorithme), et le résultat (données de sortie). Les données d'entrée et de sortie peuvent être de nature très diverse. On peut décrire la méthode employée pour accomplir le but d'un programme à l'aide d'un algorithme. La programmation procédurale et fonctionnelle est basée sur l'algorithmique. On retrouve en général les mêmes fonctionnalités de base. Programmation impérative ""Si""Si prédicatAlors faire ceciSinon faire cela""Tant que""Tant que prédicatFaire ...""Pour""Pour variable allant de borne inférieure à borne supérieureFaire ...""Pour"" (variante)Pour variable dans conteneurfaire ...Une fois l'algorithme défini, l'étape suivante est de coder le programme. Le codage dépend de l'architecture sur laquelle va s'exécuter le programme, de compromis temps-mémoire, et d'autres contraintes. Ces contraintes vont déterminer quel langage de programmation utiliser pour « convertir » l'algorithme en code source.Le code source n'est (presque) jamais utilisable tel quel. Il est généralement écrit dans un langage ""de haut niveau"", compréhensible pour l'homme, mais pas pour la machine. Compilation Certains langages sont ce qu'on appelle des langages compilés. En toute généralité, la compilation est l'opération qui consiste à transformer un langage source en un langage cible. Dans le cas d'un programme, le compilateur va transformer tout le texte représentant le code source du programme, en code compréhensible pour la machine, appelé code machine.Dans le cas de langages dits compilés, ce qui est exécuté est le résultat de la compilation. Une fois effectuée, l'exécutable obtenu peut être utilisé sans le code source.Il faut également noter que le résultat de la compilation n'est pas forcément du code machine correspondant à la machine réelle, mais peut être du code compris par une machine virtuelle (c'est-à-dire un programme simulant une machine), auquel cas on parlera de bytecode. C'est par exemple le cas en Java. L'avantage est que, de cette façon, un programme peut fonctionner sur n'importe quelle machine réelle, du moment que la machine virtuelle existe pour celle-ci.Dans le cas d'une requête SQL, la requête est compilée en une expression utilisant les opérateurs de l'algèbre relationnelle. C'est cette expression qui est évaluée par le système de gestion de bases de données. Interprétation D'autres langages ne nécessitent pas de phase spéciale de compilation. La méthode employée pour exécuter le programme est alors différente. La phase de compilation est la plupart du temps incluse dans celle d’exécution. On dit de ce programme qu'il interprète le code source. Par exemple, Python ou Perl sont des langages interprétés. Avantages, inconvénients Les avantages généralement retenus pour l'utilisation de langages « compilés », est qu'ils sont plus rapides à l'exécution que des langages interprétés, car l'interprète doit être lancé à chaque exécution du programme, ce qui mobilise systématiquement les ressources.Traditionnellement, les langages interprétés offrent en revanche une certaine portabilité (la capacité à utiliser le code source sur différentes plates-formes), ainsi qu'une facilité pour l'écriture du code. En effet, il n'est pas nécessaire de passer par la phase de compilation pour tester le code source. Il n'est pas non plus nécessaire de disposer d'un autre programme (debugger) afin d’ôter les bugs du programme, c'est l’interpréteur qui permet d'afficher directement le contenu des variables du programme. Appellation impropre Il faut noter qu'on parle abusivement de langages compilés ou interprétés. En effet, le caractère compilé ou interprété ne dépend pas du langage, qui n'est finalement qu'une grammaire et une certaine sémantique. D'ailleurs, certains langages peuvent être utilisés interprétés ou compilés. Par exemple, il est très courant d'utiliser Ruby avec un interprète, mais il existe également des compilateurs pour ce langage. On notera toutefois, qu'il peut être important de préciser comment le code source est exécuté. En effet, rares sont les organismes qui proposent à la fois un compilateur et un interpréteur, les résultats du programme peuvent différer à l'exécution, même si la norme du langage est clairement définie.Néanmoins, l'usage qu'on fait des langages est généralement fixé.C'est l'une des étapes les plus importantes de la création d'un programme. En principe, tout programmeur se doit de vérifier chaque partie d'un programme, de le tester. Il existe différents types de test. On peut citer en particulier :Test unitaireTest d'intégrationTest de performanceIl convient de noter qu'il est parfois possible de vérifier un programme informatique, c'est-à-dire prouver, de manière plus ou moins automatique, qu'il assure certaines propriétés.AlgorithmiqueGestion de versionsOptimisation du codeProgrammation systèmeRefactoringTest d'intégrationTest unitaireUn paradigme est un style fondamental de programmation, définissant la manière dont les programmes doivent être formulés.Un paradigme est la façon dont sont traitées les solutions aux problèmes et un style fondamental de programmation, définissant la manière dont les programmes doivent être formulés. Chaque paradigme amène sa philosophie de la programmation ; une fois qu'une solution a été imaginée par un programmeur selon un certain paradigme, un langage de programmation qui suit ce paradigme permettra de l'exprimer.Le paradigme impératif est le plus répandu, les opérations sont une suite d’instructions exécutées par l'ordinateur pour modifier l'état du programme. Programmation procédurale La programmation procédurale est un sous-ensemble de la programmation impérative. Elle introduit la notion de routine ou fonction qui est une sorte de factorisation de code, chaque procédure peut être appelée à n’importe quelle étape du programme. Ce paradigme permet aussi de supprimer les instructions goto,,Ce paradigme est très répandu, il est présent dans des langages comme le C, le COBOL ou le FORTRAN. Programmation structurée Apparue dans les années 70, la programmation structurée est un sous-ensemble de la programmation impérative. Elle est née avec les travaux de Nicklaus Wirth pour son Algol W et l'article fondateur de Dijkstra dans Communications of the ACM, visant à supprimer l’instruction goto.Tous les langages procéduraux peuvent faire de la programmation structurée, mais certains comme le FORTRAN s'y prêtent très mal.En programmation déclarative, le programme est indépendant de l’état de la machine, il s’affranchit donc de tout effet de bord et un appel à une même fonction produira toujours le même résultat.Le programme s’écrit non pas comme une suite d’instruction pour résoudre un problème mais (contrairement à la programmation impérative) comme la solution au problème. Programmation fonctionnelle La programmation fonctionnelle se base sur plusieurs principes comme : l’immutabilité, les fonctions pures (qui ne dépendent pas de l’état de la machine) et les lambda-calcul.Aujourd’hui, nombreux sont les langages qui offrent une approche fonctionnelle au programmeur. Certains comme LISP ou Scala sont intrinsèquement fonctionnels. D’autres comme JavaScript, Java ou PHP ont ajouté cette possibilité par la suite. Programmation logique La programmation logique consiste à exprimer les problèmes et les algorithmes sous forme de prédicats à l’aide d'une base de faits, d'une base de règles et d'un moteur d'inférence.La programmation orientée objet (abrégé POO) consiste en la définition et l'interaction de briques logicielles appelées objets ; ces objets représentes un concept, une idée. Chaque objet contient des attributs et des méthodes en rapport avec un sujet. Programmation orientée prototype La programmation orientée prototype est un sous ensemble de la programmation orientée objet. Dans ce paradigme, chaque objet est créé à partir d’un prototype qui est lui-même un objet. Le prototype a donc une existence physique en mémoire et est mutable contrairement aux classes.Le JavaScript, le Lua ou le Self sont des exemples de langages utilisant ce paradigme. Programmation orientée classe La programmation orientée classe est basée sur la notion de classes. Une classe est statique, c’est la représentation abstraite de l’objet, c’est à ce niveau que se passe l’héritage. Tout objet est donc l’instance d’une classe.Les langages à classes peuvent être sous forme fonctionnelle (CLOS) comme sous forme impérative (C++, Java), voir les deux (Python, OCaml).Programmation concurrenteProgrammation orientée aspectProgrammation orientée composantProgrammation par contratProgrammation par contraintesProgrammation par intentionProgrammation réactiveÉconomie d'énergie d'un programme informatiqueParadigme (programmation)Style de programmationChronologie des langages de programmationListe des langages de programmationProgrammation webForme de Backus-Naur (BNF), une grammaire de description de langageGoogle Code Jam, un concours international annuel de programmation très prisé des mordus d'algorithmique et de programmationKata (programmation)Éditeur de texteCode créatif Portail de la programmation informatique"
Informatique;"Un programme informatique est un ensemble d'instructions et d’opérations destinées à être exécutées par un ordinateur.Un programme source est un code écrit par un informaticien dans un langage de programmation. Il peut être compilé vers une forme binaire ou directement interprété.Un programme binaire décrit les instructions à exécuter par un microprocesseur sous forme numérique. Ces instructions définissent un langage machine,.Un programme fait généralement partie d'un logiciel que l'on peut définir comme un ensemble de composants numériques destiné à fournir un service informatique. Un logiciel peut comporter plusieurs programmes. On en retrouve ainsi dans les appareils informatiques (ordinateur, console de jeux, guichet automatique bancaire…), dans des pièces de matériel informatique, ainsi que dans de nombreux dispositifs électroniques (imprimante, modem, GPS, téléphone mobile, machine à laver, appareil photo numérique, décodeur TV numérique, injection électronique, pilote automatique…).Les programmes informatiques sont concernés par le droit d'auteur et font l'objet d'une législation proche des œuvres artistiques.En 1842, la comtesse Ada Lovelace crée des diagrammes pour la machine analytique de Charles Babbage. Ces diagrammes sont considérés aujourd'hui comme étant les premiers programmes informatiques au monde. Toutefois, cette théorie fait l'objet de controverses car Babbage a également écrit lui-même ses premiers programmes pour sa machine analytique, bien que la majorité n'ait jamais été publiée. Par exemple, Bromley note des exemples de programmes préparés par Babbage entre 1837 et 1840 : toutes ses notes sont antérieures à ceux écrits par Lovelace. Cependant, le concept de programmation et de programme enregistré est d'abord formulé de manière théorique en 1936 par Alan Turing.Dans les années 1940, les premiers ordinateurs, comme le Z3 ou le Mark I, sont créés. Les programmes informatiques étaient alors conçus par des analystes, rédigés par des programmeurs et saisis par des opératrices sur des bandes type télex ou des cartes en carton perforé. Exécuter un programme consistait à entrer la bande ou la pile de cartes correspondante dans un lecteur électro-mécanique.Le premier système d'exploitation a été développé en 1954. La même année sont apparus les premiers assembleurs et le premier compilateur pour le langage Fortran.L'enseignement de la programmation était d'abord organisé chez les constructeurs d'ordinateurs et dans les premières universités où ces machines sont installées – dès le début des années 1950 en Angleterre et aux États-Unis, puis au milieu de la même décennie en Europe continentale et au Japon. Ce sont des cours techniques, mais la complexification croissante du sujet (compilateurs, systèmes) entraînera progressivement la constitution d'une science nouvelle.L'avènement de la programmation structurée vers 1970 a grandement simplifié le travail des programmeurs et permis la création de programmes traitant des tâches plus nombreuses et plus complexes. Il en va de même avec l'avènement de la programmation orientée objet entre 1980 et 1990. Conformément à la phrase d'Edsger Dijkstra : « Les progrès ne seront possibles que si nous pouvons réfléchir sur les programmes sans les imaginer comme des morceaux de code exécutable ». De nouveaux langages de programmation ou de métaprogrammation sont régulièrement créés dans le but de simplifier et d’accélérer les possibilités offertes par programmation.Enfin, la miniaturisation des ordinateurs et la généralisation des interfaces graphiques ont largement contribué à la démocratisation de l'utilisation de l'ordinateur, au point que dans les années 2010, la généralisation des smartphones permet aux utilisateurs d’exécuter des programmes informatiques en permanence.La programmation consiste, partant d'une idée, à effectuer un travail de réflexion qui aboutit à la rédaction d'algorithmes dans un langage de programmation. Les langages de programmation ont été créés dans l'optique de faciliter le travail du programmeur en raccourcissant le chemin qui va de l'idée au code source.Les programmes sont créés par des programmeurs ou des ingénieurs logiciels. Les programmeurs travaillent principalement sur l'écriture de programmes tandis que les ingénieurs logiciels travaillent à toutes les étapes de la création du programme. Ils appliquent une démarche formelle et rigoureuse basée sur le génie industriel et les techniques de management.Avant de commencer à écrire un programme destiné à résoudre un problème, le programmeur doit déterminer les caractéristiques du problème à résoudre. Ceci se fait en plusieurs étapes indépendantes du langage de programmation utilisé. La technique courante est celle d'un cycle de développement, qui comporte des étapes de définition, de conception, d'écriture, de test, d'installation et de maintenance.Le problème est tout d'abord examiné en détail en vue de connaître l'étendue du programme à créer. L'étape suivante consiste à choisir des solutions et des algorithmes, puis décrire leur logique sous forme de diagrammes, en vue de clarifier le fonctionnement du programme et faciliter son écriture.Une fois le programme écrit, celui-ci subit une suite de tests. Les résultats produits par le programme sont comparés avec des résultats obtenus manuellement. De nombreux tests sont nécessaires et les mêmes tests sont exécutés plusieurs fois. Le programme est ensuite installé dans la machine de l'utilisateur final qui fera ses premières observations, puis sera modifié en fonction des commentaires faits par l'utilisateur et des inconvénients signalés.Les besoins des utilisateurs et des systèmes informatiques varient continuellement, et le programme est régulièrement reconstruit et modifié en vue d'être adapté aux besoins. De nouvelles fonctions y sont ajoutées et des erreurs qui n'avaient pas été décelées auparavant sont corrigées.Le but du cycle de développement est de réduire les coûts de fabrication tout en augmentant la qualité du programme. Les qualités recherchées sont l'efficacité, la flexibilité, la fiabilité, la portabilité et la robustesse. Il doit également être convivial et facile à modifier.Il est attendu d'un programme qu'il demande peu d'effort de programmation, que les instructions demandent peu de temps et nécessitent peu de mémoire, qu'il peut être utilisé pour de nombreux usages et donne les résultats attendus quels que soient les changements — permanents ou temporaires — du système informatique.Il est également attendu qu'il peut être facilement transféré sur un modèle d'ordinateur différent de celui pour lequel il est construit, qu'il produit des résultats probants y compris lorsque les informations entrées sont incorrectes, qu'il peut être facilement compris par un usager novice et que le code source peut être facilement modifié par la suite.Un langage de programmation est une notation utilisée pour exprimer des algorithmes et écrire des programmes. Un algorithme est un procédé pour obtenir un résultat par une succession de calculs, décrits sous forme de pictogrammes et de termes simples dans une langue naturelle. Jusqu'en 1950, les programmeurs exprimaient les programmes dans des langages machines ou assembleur, des langages peu lisibles pour des êtres humains et où chaque instruction fait peu de choses, ce qui rendait le travail pénible et le résultat sujet à de nombreuses erreurs. Dès 1950, les programmes ont été décrits dans des langages différents dédiés à l'humain et plus à la machine — des langages de programmation –, ce qui rendait les opérations plus simples à exprimer. Le programme était ensuite traduit automatiquement sous une forme qui permet d'être exécuté par l'ordinateur.Sur demande, l'ordinateur exécutera les instructions du programme. Bien qu'il exécute toujours exactement ce qui est instruit et ne se trompe jamais, il peut arriver que les instructions qu'il exécute soient erronées à la suite d'une erreur humaine lors de l'écriture du programme. Les langages de programmation visent à diminuer le nombre de ces bugs ; ceux-ci sont cependant inévitables dans des programmes de plusieurs milliers de lignes. Un programme de traitement de texte peut être fait de plus de 750 000 lignes de code et un système d'exploitation peut être fait de plus de 50 millions de lignes. En moyenne un programmeur prépare, écrit, teste et documente environ 20 lignes de programme par jour, et la création de grands programmes est le fait d'équipes et peut nécessiter plusieurs mois, voire plusieurs années.La programmation est un sujet central en informatique. Les instructions qu'un ordinateur devra exécuter doivent pouvoir être exprimées de manière précise et non ambiguë. Pour ce faire, les langages de programmation combinent la lisibilité de l'anglais avec l'exactitude des mathématiques. Les programmes sont créés par des programmeurs ou des ingénieurs logiciels. La création d'un programme comprend une série d'activités telles que la conception, l'écriture, le test et la documentation. En vue d'obtenir un programme de meilleure qualité, le travail de programmation se fait selon une démarche systématique et planifiée,.Un langage de programmation est un vocabulaire et un ensemble de règles d'écriture utilisées pour instruire un ordinateur d'effectuer certaines tâches. La plupart des langages de programmation sont dits de haut niveau, c'est-à-dire que leur notation s'inspire des langues naturelles (généralement l'anglais).Le processeur est le composant électronique qui exécute les instructions. Chaque processeur est conçu pour exécuter certaines instructions, dites instructions machine. La palette d'instructions disponibles sur un processeur forme le langage machine. Par exemple, le processeur Intel 80486 a une palette de 342 instructions.Le langage d'assemblage est une représentation textuelle des instructions machine, un langage de bas niveau, qui permet d'exprimer les instructions machine sous une forme symbolique plus facile à manipuler, où il y a une correspondance 1-1 entre les instructions machines et les instructions en langage d'assemblage.Les langages de programmation de haut niveau permettent d'exprimer des instructions de manière synthétique, en faisant abstraction du langage machine. Par rapport au langage d'assemblage, ils permettent d'exprimer des structures, permettent d'écrire des programmes plus rapidement, avec moins d'instructions. Les programmes écrits dans des langages de haut niveau sont plus simples à modifier et portables, et peuvent fonctionner avec différents processeurs. Cependant un programme exprimé en langage de haut niveau, puis compilé est moins efficace et comporte plus d'instruction que s'il avait été exprimé en langage d'assemblage.Entre 1950 et 2000, plus de 50 langages de programmation sont apparus. Chacun apportait un lot de nouveaux concepts, de raffinements et d'innovations. Jusque dans les années 1950, l'utilisation des langages de programmation était semblable à l'écriture d'instructions machines. L'innovation des années 1960 a été de permettre une notation proche des mathématiques pour écrire des instructions de calcul. Les innovations des années 1970 ont permis l'organisation et l'agrégation des informations manipulées par les programmes — voir structure de données et structure de contrôle. Puis l'arrivée de la notion d'objet a influencé l'évolution des langages de programmation postérieurs à 1980.Ci-dessous, le programme Hello world exprimé en langage de programmation Java :Le même programme, exprimé dans le langage d'assemblage des processeurs x86 :Un programme est typiquement composé d'un ensemble de procédures et de fonctions. Une procédure est une suite d'instructions destinées à réaliser une opération ; par exemple, trier une liste. Une fonction est une suite d'instructions destinées à produire un résultat ; par exemple, un calcul.Un bug est un défaut de construction dans un programme. Les instructions que l'appareil informatique exécute ne correspondent pas à ce qui est attendu, ce qui provoque des dysfonctionnements et des pannes. La pratique de la programmation informatique nécessite des outils pour traquer ou éviter les bugs, ou vérifier la correction du programme.L'exécution des programmes est basée sur le principe de la machine à programme enregistré de John von Neumann : les instructions de programme sont exécutées par un processeur. Ce composant électronique exécute chaque instruction de programme par une succession d'opérations charger/décoder/exécuter : l'instruction est tout d'abord copiée depuis la mémoire vers le processeur, puis elle est décomposée bit par bit pour déterminer l'opération à effectuer, qui est finalement exécutée. La plupart des opérations sont arithmétiques (addition, soustraction) ou logiques. L'exécution de programmes par le processeur central (anglais CPU) contrôle la totalité des opérations effectuées par l'ordinateur.L'exécution du cycle charger-décoder-exécuter est rythmée par une horloge branchée au processeur.En 2011, la fréquence d'horloge supportée par les processeurs contemporains se compte en mégahertz ou en gigahertz, ce qui correspond à des millions, voire des milliards de cycles par seconde.Les processeurs contemporains peuvent traiter plusieurs instructions simultanément : lorsqu'une instruction est chargée, le processeur charge immédiatement l'instruction suivante, sans attendre que cette instruction soit décodée puis exécutée, et les processeurs peuvent également charger/décoder/exécuter plusieurs instructions en un seul cycle d'horloge. Processus Pour être exécuté, un programme doit être chargé dans la mémoire de la machine. Le chargement d'un programme peut être soit automatique ou programmé lors de l'amorce de l'ordinateur par exemple, soit interactif et être déclenché par un ordre d'exécution explicite de l'utilisateur (une commande explicite, un appui sur une touche, un bouton, une icône…). Suivant la nature de l'action à effectuer, un programme peut être exécuté de manière ponctuelle (impression d'un texte), de manière répétitive (mise à jour de coordonnées GPS) ou de manière (presque) permanente (surveillance de capteurs).Un programme est une suite d'instructions qui spécifie étape par étape, de manière non-ambiguë, des représentations de données et des calculs. Les instructions sont destinées à manipuler les données lors de l'exécution du programme. Le programme lui-même est défini par un (ou des) algorithme(s) ou par une spécification. Un programme décrit de manière exacte les différentes étapes d'un algorithme : ce qu'il y a à faire, quand et avec quelles informations. Selon l'architecture de von Neumann créée en 1945, un programme est chargé dans la mémoire de l'ordinateur, ce qui permet de l'exécuter de manière répétée sans intervention humaine, et surtout d'utiliser la même machine pour exécuter autant de programmes que l'on veut. La mémoire dédiée aux programmes est aussi la mémoire dédiée aux données, ce qui permet de traiter les programmes comme des données comme les autres (par exemple, écrire de nouveaux programmes de la même manière qu'on écrirait un document textuel), puis de les exécuter.Des programmes peuvent être exécutés non seulement par les ordinateurs, mais par les nombreux appareils qui sont basés sur des composants informatiques – par exemple, certains robots ménagers, téléphones, fax, instruments de mesure, récepteur de télévision, ainsi que les pièces de matériel informatique telles que les disques durs, les routeurs, les imprimantes, les consoles de jeux vidéo, les assistants personnels et les automates bancaires. Contrairement aux ordinateurs, ces appareils ne contiennent souvent pas de système d'exploitation, les programmes sont enregistrés dans l'appareil lors de la fabrication et la vitesse d'exécution des programmes est souvent d'importance mineure.Sans contre-indication, les instructions d'un programme sont exécutées une après l'autre, de manière linéaire. Les langages de programmation permettent d'exprimer des alternatives : une suite d'instructions est exécutée uniquement si une condition donnée est remplie, dans le cas contraire une autre suite est exécutée. Les langages de programmation permettent également de faire répéter l'exécution d'une suite d'instructions jusqu'à ce qu'une condition donnée soit remplie.L'exécution se déroule de manière différente suivant si le langage de programmation s'utilise avec un compilateur ou un interpréteur.Un compilateur lit le programme source en entier et le transforme en instructions machines. La transformation peut se faire en plusieurs étapes et nécessiter plusieurs lectures du programme. Une fois traduit, le programme est ensuite enregistré en vue d'être plus tard copié en mémoire et exécuté par le processeur tel quel ;Un interpréteur opère ligne par ligne : lit une ligne de programme source, puis exécute immédiatement les instructions machines correspondantes. L'avantage d'un interpréteur est que les erreurs peuvent être immédiatement corrigées. Le désavantage est que l'exécution du programme est 10 à 100 fois moins rapide que si le programme avait été préalablement traduit et exécuté tel quel.Les ordinateurs modernes démarrent à leur lancement un programme « maître » dit système d'exploitation,. Il permet d'exécuter des sous-programmes qui peuvent alors profiter des fonctionnalités offertes par le système et qui dans certains cas doivent s'adapter à cet environnement. Les systèmes d'exploitation contemporains permettent d'exécuter simultanément plusieurs programmes dans des processus, même avec un seul processeur : un programme planificateur (en anglais : scheduler) du système d'exploitation interrompt régulièrement le programme en cours d'exécution pour donner la main à un autre. La vitesse de rotation donne l'illusion que les programmes sont exécutés en même temps.Un sous-programme du système d'exploitation peut lui-même être un environnement permettant d'exécuter des programmes (avec une interface différente) ; par exemple, une machine virtuelle.En droit, un programme est une œuvre écrite, protégée par le droit d'auteur. Celui-ci s'applique au programme du moment qu'il est enregistré de manière permanente, même s'il n'existe pas d'édition sur papier. Le droit d'auteur protège autant le programme source que le programme binaire.[Parsons Oja 2008] (en) June Jamrich Parsons et Dan Oja, New Perspectives on Computer Concepts, Cengage Learning, 2008, 800 p. (ISBN 978-1-4239-2518-7 et 1-4239-2518-1, lire en ligne). Portail de l’informatique   Portail de la programmation informatique"
Informatique;"Un système de traitement de l'information est un système constitué d'un ensemble de composants (mécaniques, électroniques, chimiques, photoniques ou biologiques) permettant de traiter automatiquement des informations. Il est régi par la théorie de l'information et c'est l'élément central de tout appareil informatique.Les premiers systèmes permettant de traiter automatiquement des informations étaient des appareils mécaniques tel que la pascaline ou encore la machine d'Anticythère qui représentent les tout premiers calculateurs spécifiques à l'inverse de l'ordinateur qui est programmable et universel.Aujourd'hui[Quand ?] il existe une panoplie d'appareils en électronique numérique permettant de traiter automatiquement des informations : commutateur réseau, ordinateur, console de jeu, calculatrice, certaines cartes à puce, distributeur automatique de billets, enregistreur vidéo personnel, GPS, téléphone portable, etc.Avant d'être un appareil fonctionnel, le système de traitement de l'information est d'abord un objet d'étude conceptuelle qui propose une approche systémique à l'informaticien. Il permet d'élargir les champs de recherche en informatique et permet de catégoriser les systèmes de traitement par rapport à leurs capacités plutôt qu'à leurs fonctionnements.Un système de traitement de l'information se caractérise par un minimum de quatre unités :l'unité d'entrée (anglais : input), recueille l'information ;l'unité de stockage (anglais : storage) conserve toute ou une partie de l'information ;l'unité de traitement (anglais : processing), transforme l'état de l'information ;l'unité de sortie (anglais : output) présente le résultat de la modification.L'évolution de l'état d'une information ne peut être observée qu'en accord avec la notion de temps. L'horloge est le système de traitement de l'information le plus élémentaire. Tous les systèmes de traitement de l'information sont jusqu'à présent mus par un système de balancement ou de mouvement oscillatoire dont la source d'entraînement est naturelle (ex. : l'actionnement d'une manivelle) ou artificielle (rotation d'un moteur, alimentation électrique d'un quartz, etc.).L'information est représenté par un  signal de type analogique ou numérique et qui peut être véhiculé sous forme matériel (engrenage, molécule, etc.) ou énergétique (particule élémentaire).La Pascaline est le premier calculateur mécanique. Il a été construit par Blaise Pascal en 1642.La première conceptualisation d'un système de traitement de l'information programmable et universel est appelé machine de Turing. C'est la thèse de Turing qui est aujourd'hui considérée comme l'article fondateur de l'ordinateur.Le premier calculateur électronique à utiliser le système binaire est l'EDVAC. Construit en 1945, il occupait une salle de 45 m2, et pesait près de 8 tonnes.C'est l'invention du transistor en 1947 et celle du circuit intégré en 1958 qui ont permis la miniaturisation électronique des systèmes de traitement de l'information.La première console de jeu, l'Odyssey a été construite en 1973. C'était un système de traitement de l'information qui n'était pas Turing-complet.Le premier type d'informations que les systèmes étaient capables de manipuler étaient des nombres, puis des textes, jusqu'à l'arrivée dans les années 1980 des systèmes multimédia, c'est-à-dire capables de manipuler divers types d'informations: images, sons, vidéos…Exemples d'informations :nombres : prix, poids, volume, température, vitesse, pression…textes : courrier, publications, articles de presse…images : plans, dessins, graphiques, diagrammes, cartes géographiques, photos, images 3D…sons : paroles, chants, bruitages ou musique ,vidéo : prises de vue, clips ou films ;les instructions d'un programme informatique sont aussi des informations.Dans le système de traitement de l'information, les informations circulent sous forme de suite de bits (chiffres en base 2) et le octet groupés dans des fichiers ou des enregistrements (voir : électronique numérique).Un format désigne la manière dont les bits sont disposés à l'intérieur du fichier ou de l'enregistrement pour stocker l'information.Un protocole est un ensemble de règles normalisées qui, lorsqu'elles sont appliquées de manière commune par deux appareils, leur permettent de s'échanger des informations.Les règles établies par un protocole concernent autant le point de vue électronique (tensions, fréquences) que le point de vue informationnel (choix des informations, format) ainsi que le déroulement des opérations de communication (qui initie la communication, comment réagit le correspondant, combien de temps dure la communication…).Les protocoles sont utilisés en informatique et en télécommunication (téléphonie, télévision).Un système de traitement de l'information est composé de quatre unités :l'unité d'entrée (anglais : input), qui permet de faire entrer les informations dans le système ;l'unité de stockage (anglais : storage) qui permet de conserver les informations ;l'unité de traitement (anglais : processor) qui comme son nom l'indique va traiter les informations ;l'unité de sortie' (anglais : output) qui permet de faire sortir les résultats des traitements.Les informations peuvent être introduites par une personne à l'aide d'un clavier et d'une souris, enregistrées à l'aide de différents appareils — microphone, caméra, scanner, appareil photo, ou apportées par des dispositifs de télécommunication (voir: réseau informatique).Les premiers appareils permettant d'introduire des informations étaient des lecteurs de cartes perforées. Un dispositif semblable à celui des pianos mécaniques. Cette technologie datant du XVIIIe siècle a été utilisée en informatique jusque dans les années 1980.La première étape des traitements consiste en la réceptions des informations en provenance des différents appareils.La numérisation est le procédé qui consiste à transformer une information provenant du monde réel en une suite de chiffres qui seront utilisés dans le système de traitement de l'information.Les informations stockées peuvent être des informations qui viennent d'être entrées dans la machine, ou résultats d'un traitement.Les informations sont conservées dans des dispositifs de stockage tels que disque durs, DVD, CD-ROM ou mémoire flash.La possibilité de stocker des informations existe depuis les années 1960, auparavant les informations étaient traitées à mesure qu'elles étaient entrées.Le système de traitement des informations effectue les traitements en suivant scrupuleusement les  instructions d'un programme informatique.Les traitements peuvent consister à :à partir de certaines informations, d'obtenir d'autres informations, par exemple par calcul ;transformer les informations ;stocker les informations dans le système d'informations, en vue d'effectuer des traitements plus tard ;extraire des informations préalablement stockées.Exemples de traitements :tri, classement, recherche ;calculs de comptabilité, statistiques, analyses de physique ou d'économie ;correction orthographique ;reconnaissance de texte ;reconnaissance vocale ;traitement d'images tels que fausse couleur, négatif.Un composant électronique qui effectue des traitements s'appelle un processeur.La sortie est l'étape finale des traitements qui consiste à faire sortir les résultats du système d'informations.Selon leur nature, les informations des résultats peuvent être restituées sur un écran, du papier par une imprimante ou un traceur, des enceintes audio ou tout autre appareil. Les informations peuvent aussi être transportées vers d'autres systèmes par des moyens de télécommunication (voir : réseau informatique).Boîte noireInformatiqueOrdinateurSystème binaireMario Borillo, Informatique pour les sciences de l'homme : limites de la formalisation du raisonnement, Éditions Mardaga, 1984  (ISBN 2-8700-9202-4) Portail de l’informatique"
Informatique;"Un système est un ensemble d'éléments interagissant entre eux selon certains principes ou règles. Par exemple une molécule, le système solaire, une ruche, une société humaine, un parti, une armée etc.Un système est déterminé par :sa frontière, c'est-à-dire le critère d'appartenance au système (déterminant si une entité appartient au système ou fait au contraire partie de son environnement) ;ses interactions avec son environnement ;ses fonctions (qui définissent le comportement des entités faisant partie du système, leur organisation et leurs interactions) ;Certains systèmes peuvent également avoir une mission (ses objectifs et sa raison d'être) ou des ressources, qui peuvent être de natures différentes (humaine, naturelle, matérielle, immatérielle...).Un sous-système (ou module ou composant) est un système faisant partie d'un système de rang supérieur.La notion de « système » dans son acception moderne remonte au XVIIIe siècle. Le Traité des systèmes d'Étienne Condillac en donne en 1749 la définition suivante : « le système est ce qui permet à l'esprit humain de saisir l'enchaînement des phénomènes ». Il pose les bases d'un concept qui aura un développement exceptionnel dans le domaine des sciences exactes, puis dans tous les domaines de la connaissance. Les systèmes n'existent pas dans la nature, mais sont des projections de l'esprit humain pour la modélisation et l'analyse des phénomènes naturels.Le concept de système est à la base de domaines scientifiques comme la cybernétique, qui au concept de « boîte noire », à entrées et sorties uniquement, ajoute la notion de boucle de rétroaction des sorties sur les entrées. Dans les sciences humaines, selon André Comte-Sponville, un système est « un ensemble d'idées que l'on considère dans leur cohérence plutôt que dans leur vérité ». Le caractère universel du concept a conduit des chercheurs à vouloir élaborer une théorie générale des systèmes (L. von Bertalanffy, 1949), un programme très ambitieux, qui est surtout un cadre conceptuel pour les applications concrètes dans les différents domaines scientifiques.En grec ancien, sust?ma signifie « organisation, ensemble », terme dérivé du verbe ????????? sunist?mi (de ??? ?????? sun hist?mi : « établir avec »), qui signifie « mettre en rapport, instituer, établir ».Un système peut être ouvert, fermé, ou isolé selon son degré d’interaction avec son environnement.Un système peut également être :soit de nature conceptuelle, comme :une construction théorique que forme l’esprit sur un sujet (ex. : une idée expliquant un phénomène physique et représentée par un modèle mathématique),un ensemble de propositions, d’axiomes, de principes et de conclusions qui forment un corps de doctrine ou un tout scientifique (ex. en philosophie : le système d’Aristote, ex. en physique : le système newtonien) ;soit de nature concrète, comme :un ensemble d'éléments en interactions (ex. : un écosystème, le système climatique),un ensemble d’éléments qui se coordonnent pour concourir à un résultat (ex. : le système nerveux),un ensemble de composants et de processus organisés ou institutionnalisés pour assurer une fonction (ex. : un système éducatif, un système de production, un système de défense),un ensemble de divers éléments analogues,un appareillage, dispositif, ou machine (automate) assurant une fonction déterminée (ex. : système d’éclairage, système automobile),un réseau, plus ou moins important et autonome, dont les éléments présentent la particularité de répondre en tout ou partie à un même objectif.Les systèmes sont au cœur de deux disciplines. La première, de nature applicative, est l'ingénierie des systèmes, démarche rationnelle pour la conception et l'ingénierie d'un système industriel, qui étudie tout son cycle de vie (exploitation, maintenance, démantèlement). La seconde, de nature plus fondamentale, est la théorie des systèmes, à travers l'étude et la recherche des propriétés générales des systèmes (contrôlabilité, stabilité, équivalence, linéarité, etc.) et le développement de méthodes pour en  décrire certains types.Système astronomique Système dynamique : système cybernétique, en général bouclé et modélisé par des équations différentielles, caractérisé par des variables d'état dont on cherche à prévoir les variations dans le temps.Système biologique : système en biologie ;Système en anatomie (aussi appelé appareil) : ensemble d'organes interagissant au sein d'un organisme dans la réalisation d'une fonction biologique commune (ex. : système digestif, système excréteur, système nerveux, etc.).Système cristallin : ensemble de formes géométriques types, caractérisées par leurs propriétés de symétrie fractale ou non, que peut prendre un cristal (système cubique).Écosystème : ensemble formé par une communauté d'êtres vivants et son environnement biologique, géologique, édaphique, hydrologique, climatique, etc.Système : terme utilisé dans la rhétorique antisystème.Système de santé : ensemble de tout ce qui contribue à promouvoir ou à protéger la santé.Système économique : mode d’organisation et de fonctionnement de l'activité économique. Un Secteur (ou filière) peut notamment être analysé comme un système autour des enjeux communs aux entreprises du secteur, les interactions entre fournisseurs et clients intermédiaires, sa régulation, etc.Système financier : ensemble des organisations, des établissements, des acteurs et des systèmes d'information bancaires et financiers à l'échelle mondiale.Système politique comme :Système communiste : ensemble des lois et doctrines qui vise à collectiviser les moyens de production et pratiqué pendant 70 ans par l'URSS ;Système libéral : ensemble des lois et doctrines qui permettent la liberté de propriété des moyens de production et pratiqué dans les économies de marché du monde occidental et asiatique.Système de coordonnées géographiquesSystème d'information géographiqueD'une manière générale, lorsque l'on parle simplement du système en informatique, en robotique ou en bureautique, on désigne l’ensemble des technologies (matériels et logiciels) qui constituent un appareil ou un réseau informatique :Système de traitement de l'information : représentation théorique d'un système en informatique.Système informatisé : machine automatique commandée par un ou plusieurs systèmes de traitement de l'information.Système d'exploitation : ensemble structuré et hiérarchisé de programmes et de processus regroupés autour d'un programme-maître appelé noyau, qui gère les divers éléments d'un appareil informatique.Système reconfigurable : système informatique matériel capable d'avoir sa structure interne modifiée afin d'adapter ses réponses à son environnement; système informatique logiciel susceptible d'être reconfiguré à distance par un automate ou un opérateur (avec ou sans fil de connexion)Système d'information : stratégie informatique mise en œuvre au sein d'une organisation humaine.Système de contrôle temps réel : ensemble de capteurs et d'actionneurs exhibant un comportement collectif en partageant leurs états selon un pas temporel court imposé appelé latence.Système mécatronique : ensemble complexe et structuré de composants mécaniques, électroniques et informatiques en interaction permanente et assurant une fonction d'usage (ex. : automobile, aéronef, train, lanceur spatial, ascenseur, téléphone mobile, centrale nucléaire…).Système formel : système logique composé d'un langage, d'un ensemble de règles de déductions et d'un ensemble d'axiomes.Système d'équations : ensemble de plusieurs équations devant être satisfaites simultanément :Système d'équations linéaires, algébriques ;Système d'équations différentielles.Systèmes dynamiques : branche de recherche des mathématiques.Systèmes de références :Système de numérationSystème de relations : ensemble de relations qui doivent être satisfaites simultanément.Système de référence : système d'axes par rapport auxquels on définit le mouvement d'un corps dans un espace à trois dimensions.Système de vecteurs : ensemble composé d'un nombre fini de vecteurs mobiles sur leur ligne d'action.Système : ensemble de portées devant être lues simultanément dans une partition de musique.Le « système de peintures » est l'ensemble des couches de peinture (« feuils ») appliquées sur un support de peinture (« subjectile »), respectant les compatibilités entre elles,. Les essais de Marcelin Pleynet regroupés dans son Système de la peinture (1977) se rapportent à la peinture moderne, sans rapport avec cet usage technique.Système : « Le système est l'ajointement de l'être lui-même, non pas seulement un cadre venant s'appliquer du dehors à l'étant et encore moins une collection arbitraire » Martin Heidegger. Le plus célèbre de ces systèmes construit de bout en bout selon des enchaînements logiques est celui de Spinoza, connu sous le titre d'Éthique.Système physique : ensemble d'éléments physiques concrets ou idéalisés (objet, point matériel, fluide, gaz parfait, champ électromagnétique…) dont on cherche à connaître la dynamique propre. Appelé simplement système en physique.Système de forces : ensemble d'un nombre fini de forces supposées appliquées à un même corps solide.Système d'unités :Système d'unités naturelles : système d'unités physiques fondé sur l'emploi d'un nombre minimum d'unités fondamentales indépendantes, choisies de façon à réduire à l'unité les coefficients numériques figurant dans certaines formules physiques très importantes, choisies comme fondamentales.Système international d'unités (SI) : système de mesure officiel en France depuis 1962, qui prolonge le système MKSA - système de mesure dont les unités fondamentales sont le mètre, le kilogramme, la seconde et l'ampère - par l'adjonction du kelvin et de la candela.Système métrique : ensemble coordonné d'unités servant à la mesure des différentes grandeurs.Système thermodynamique.Système familial (ou structure, modèle, type familial) : un groupe de parenté envisagé, en psychologie systémique et chez les historiens de la famille, sous l'angle du holisme méthodologique.Système social : l'organisation d'une société (groupe humain).Deux types de systèmes sont distingués en urbanisation des systèmes d'information :le système métier, qui est formé de l'ensemble des services et des processus de l'entreprise, des organisations qui les mettent en œuvre et des objets métiers associés ;le système informatique, qui est l'ensemble structuré des composants logiciels, matériels et des données, permettant d’automatiser tout ou partie du système métier.Système complexe : deux définitions différentes d'un système complexe coexistent : 1) ensemble de composants en interaction dont l'intégration permet de réaliser une mission commune, 2) ensemble d'agents simples qui, par leur interaction, amènent la structure globale du système à être modifiée de manière chaotique.Jacques Bouveresse, Qu'est-ce qu'un système philosophique ? : Cours 2007 et 2008, Paris, Collège de France, coll. « La philosophie de la connaissance », 2012, 200 p. (ISBN 978-2-7226-0152-9, lire en ligne).Martin Heidegger (trad. de l'allemand par Jean-François Courtine), Schelling : Le traité de 1809 sur l'essence de la liberté humaine, Paris, Gallimard, coll. « Bibliothèque de Philosophie », 1993, 349 p. (ISBN 2-07-073792-6).Daniel Krob, « Éléments de systémique. Architecture de systèmes », dans A. Berthoz, J.L. Petit, Complexité-Simplexité, Collège de France, 2014 (ISBN 9782722603301, DOI 10.4000/books.cdf.3388, lire en ligne).(en) Herbert Simon, The Architecture of Complexity, vol. 106, coll. « Proceedings of the American Philosophica », décembre 1962, 467-482 p., chap. 6.(en) Ludwig von Bertalanffy, General System Theory : Foundations, Development, Applications, Paris, George Braziller, 1976 (ISBN 978-2-04-007504-0).(en) Jules Vuillemin, What are Philosophical Systems?, Cambridge, University Press, 1986, 176 p. (ISBN 978-0-521-11228-4).Analyse décisionnelle des systèmes complexesAnalyse systémiqueArchitecture d'un systèmeSystème complexe(de) Definitionen von ""System"" (1572–2002) par Roland Müller.(en) Publications with the title ""System"" (1600–2008) par Roland Müller. Portail des sciences"
Informatique;En informatique théorique, un algorithme d'approximation est une méthode permettant de calculer une solution approchée à un problème algorithmique d'optimisation. Plus précisément, c'est une heuristique garantissant à la qualité de la solution qui fournit un rapport inférieur (si l'on minimise) à une constante, par rapport à la qualité optimale d'une solution, pour toutes les instances possibles du problème.L'intérêt de tels algorithmes est qu'il est parfois plus facile de trouver une solution approchée qu'une solution exacte, le problème pouvant par exemple être NP-complet mais admettre un algorithme d'approximation polynomial. Ainsi, dans les situations où l'on cherche une bonne solution, mais pas forcément la meilleure, un algorithme d'approximation peut être un bon outil.Selon la nature du problème (maximisation, minimisation, etc.) la définition peut varier, on donne ici la définition classique pour un problème de minimisation avec facteur d'approximation constant.Pour un problème de minimisation ayant une solution optimale de valeur                               z                      ?                                {\displaystyle z^{*}}  , un algorithme d'approximation de facteur                     ?        >        1              {\displaystyle \rho >1}   (i.e. un algorithme                     ?              {\displaystyle \rho }  -approché) est un algorithme donnant une solution de valeur                     z              {\displaystyle z}  , avec la garantie que                     z        ?        ?                  z                      ?                                {\displaystyle z\leq \rho z^{*}}  .Un schéma d'approximation est un algorithme prenant comme entrée les données du problèmes mais aussi une valeur                     ?        >        0              {\displaystyle \epsilon >0}   et calculant une solution approchée avec un facteur                     (        1        +        ?        )              {\displaystyle (1+\epsilon )}  . Si le temps de calcul est polynomial en la taille de l'entrée et en                     1                  /                ?              {\displaystyle 1/\epsilon }  , on parle de schéma d'approximation en temps polynomial (abrégé PTAS en anglais).Par exemple pour le problème du transversal minimum puisque tout transversal formé par les sommets incidents aux arêtes d'un couplage maximal pour l'inclusion a une cardinalité inférieure à deux fois l'optimum.C'est aussi le cas pour le cas particulier du voyageur de commerce où les poids satisfont les inégalités triangulaires car alors, le poids minimum d'un arbre couvrant est toujours inférieur à deux fois l'optimum. En affinant cette approche en utilisant un couplage bien choisi on peut aussi obtenir un facteur d'approximation de 3/2, avec l'algorithme de Christofides.Parmi les techniques utilisées, on compte les méthodes d'algorithmique classique, par exemple un algorithme glouton permet parfois d'obtenir une bonne approximation à défaut de calculer une solution optimale. On peut aussi citer des algorithmes de recherche locale et de programmation dynamique.Beaucoup d'algorithmes sont basées sur l'optimisation linéaire. On peut par exemple arrondir une solution fractionnaire ou utiliser un schéma primal-dual. Une technique plus avancée est d'utiliser l'optimisation SDP, comme pour le problème de la coupe maximum.Parmi les problèmes NP-complets certains sont dits difficile à approximer, c'est-à-dire qu'ils n'admettent pas d'algorithme d'approximation si l'on suppose certaines hypothèses, par exemple P différent de NP ou bien la conjecture des jeux uniques.Le problème du voyageur de commerce dans un graphe                     G        =        (        V        ,        E        )              {\displaystyle G=(V,E)}   avec des poids quelconques (positifs) est un problème qui n'admet pas d'algorithme d'approximation. En effet, tout algorithme d'approximation pour ce problème dans le graphe complet                               K                      V                                {\displaystyle K_{V}}   où les arêtes de                     E              {\displaystyle E}   ont une valeur nulle et les autres la valeur 1 fournit une réponse au problème de décision NP-complet de statuer sur l'hamiltonicité d'un graphe (en l'occurrence                     G              {\displaystyle G}   est hamiltonien si et seulement si l'algorithme approché fournit une solution de valeur nulle).Un autre problème est le problème du k-centre métrique qui admet une réduction simple au problème de l'ensemble dominant.Il existe des techniques plus complexes pour montrer des résultats de difficulté d'approximation. Elles tournent essentiellement autour du théorème PCP.Plus récemment la conjecture des jeux uniques a été utilisée pour montrer des résultats plus forts.Des algorithmes d'approximation ont été découverts avant même la mise en place de la théorie de la NP-complétude, par exemple par Paul Erd?s dans les années 1960, pour le problème de la coupe maximum ou Ronald Graham, pour les algorithmes d'ordonnancement,. Cependant c'est à la suite de cette théorie que le domaine s'est vraiment développé. L'utilisation de l'optimisation linéaire est due à László Lovász dans les années 1970, pour le problème de couverture par ensembles.Dans les années 1990, le théorème PCP a été une avancée très importante pour la non-approximabilité. Portail de l'informatique théorique
Informatique;"L'apprentissage automatique, (en anglais : machine learning, litt. « apprentissage machine, »), apprentissage artificiel ou apprentissage statistique est un  champ d'étude de l'intelligence artificielle qui se fonde sur des approches mathématiques et statistiques pour donner aux ordinateurs la capacité d'« apprendre » à partir de données, c'est-à-dire d'améliorer leurs performances à résoudre des tâches sans être explicitement programmés pour chacune. Plus largement, il concerne la conception, l'analyse, l'optimisation, le développement et l'implémentation de telles méthodes. On parle d'apprentissage statistique car l'apprentissage consiste à créer un modèle dont l'erreur statistique moyenne est la plus faible possible.L'apprentissage automatique comporte généralement deux phases. La première consiste à estimer un modèle à partir de données, appelées observations, qui sont disponibles et en nombre fini, lors de la phase de conception du système. L'estimation du modèle consiste à résoudre une tâche pratique, telle que traduire un discours, estimer une densité de probabilité, reconnaître la présence d'un chat dans une photographie ou participer à la conduite d'un véhicule autonome. Cette phase dite « d'apprentissage » ou « d'entraînement » est généralement réalisée préalablement à l'utilisation pratique du modèle. La seconde phase correspond à la mise en production : le modèle étant déterminé, de nouvelles données peuvent alors être soumises afin d'obtenir le résultat correspondant à la tâche souhaitée. En pratique, certains systèmes peuvent poursuivre leur apprentissage une fois en production, pour peu qu'ils aient un moyen d'obtenir un retour sur la qualité des résultats produits.Selon les informations disponibles durant la phase d'apprentissage, l'apprentissage est qualifié de différentes manières. Si les données sont étiquetées (c'est-à-dire que la réponse à la tâche est connue pour ces données), il s'agit d'un apprentissage supervisé. On parle de classification ou de classement si les étiquettes sont discrètes, ou de régression si elles sont continues. Si le modèle est appris de manière incrémentale en fonction d'une récompense reçue par le programme pour chacune des actions entreprises, on parle d'apprentissage par renforcement. Dans le cas le plus général, sans étiquette, on cherche à déterminer la structure sous-jacente des données (qui peuvent être une densité de probabilité) et il s'agit alors d'apprentissage non supervisé. L'apprentissage automatique peut être appliqué à différents types de données, tels des graphes, des arbres, des courbes, ou plus simplement des vecteurs de caractéristiques, qui peuvent être des variables qualitatives ou quantitatives continues ou discrètes.Depuis l'antiquité, le sujet des machines pensantes préoccupe les esprits. Ce concept est la base de pensées pour ce qui deviendra ensuite l'intelligence artificielle, ainsi qu'une de ses sous-branches : l'apprentissage automatique.La concrétisation de cette idée est principalement due à Alan Turing (mathématicien et cryptologue britannique) et à son concept de la « machine universelle » en 1936, qui est à la base des ordinateurs d'aujourd'hui. Il continuera à poser les bases de l'apprentissage automatique, avec son article sur « L'ordinateur et l'intelligence » en 1950, dans lequel il développe, entre autres, le test de Turing.En 1943, le neurophysiologiste Warren McCulloch et le mathématicien Walter Pitts publient un article décrivant le fonctionnement de neurones en les représentant à l'aide de circuits électriques. Cette représentation sera la base théorique des réseaux neuronaux.Arthur Samuel, informaticien américain pionnier dans le secteur de l'intelligence artificielle, est le premier à faire usage de l'expression machine learning (en français, « apprentissage automatique ») en 1959 à la suite de la création de son programme pour IBM en 1952. Le programme jouait au Jeu de Dames et s'améliorait en jouant. À terme, il parvint à battre le 4e meilleur joueur des États-Unis,.Une avancée majeure dans le secteur de l'intelligence machine est le succès de l'ordinateur développé par IBM, Deep Blue, qui est le premier à vaincre le champion mondial d'échecs Garry Kasparov en 1997. Le projet Deep Blue en inspirera nombre d'autres dans le cadre de l'intelligence artificielle, particulièrement un autre grand défi : IBM Watson, l'ordinateur dont le but est de gagner au jeu Jeopardy!. Ce but est atteint en 2011, quand Watson gagne à Jeopardy! en répondant aux questions par traitement de langage naturel.Durant les années suivantes, les applications de l'apprentissage automatique médiatisées se succèdent bien plus rapidement qu'auparavant.En 2012, un réseau neuronal développé par Google parvient à reconnaître des visages humains ainsi que des chats dans des vidéos YouTube,.En 2014, 64 ans après la prédiction d'Alan Turing, le dialogueur Eugene Goostman est le premier à réussir le test de Turing en parvenant à convaincre 33 % des juges humains au bout de cinq minutes de conversation qu'il est non pas un ordinateur, mais un garçon ukrainien de 13 ans.En 2015, une nouvelle étape importante est atteinte lorsque l'ordinateur « AlphaGo » de Google gagne contre un des meilleurs joueurs au jeu de Go, jeu de plateau considéré comme le plus dur du monde.En 2016, un système d'intelligence artificielle à base d'apprentissage automatique nommé LipNet parvient à lire sur les lèvres avec un grand taux de succès,.L'apprentissage automatique (AA) permet à un système piloté ou assisté par ordinateur comme un programme, une IA ou un robot, d'adapter ses réponses ou comportements aux situations rencontrées, en se fondant sur l'analyse de données empiriques passées issues de bases de données, de capteurs, ou du web.L'AA permet de surmonter la difficulté qui réside dans le fait que l'ensemble de tous les comportements possibles compte tenu de toutes les entrées possibles devient rapidement trop complexe à décrire et programmer de manière classique (on parle d'explosion combinatoire). On confie donc à des programmes d'AA le soin d'ajuster un modèle pour simplifier cette complexité et de l'utiliser de manière opérationnelle. Idéalement, l'apprentissage visera à être non supervisé, c'est-à-dire que les réponses aux données d’entraînement ne sont pas fournies au modèle.Ces programmes, selon leur degré de perfectionnement, intègrent éventuellement des capacités de traitement probabiliste des données, d'analyse de données issues de capteurs, de reconnaissance (reconnaissance vocale, de forme, d'écriture…), de fouille de données, d'informatique théorique…L'apprentissage automatique est utilisé dans un large spectre d'applications pour doter des ordinateurs ou des machines de capacité d'analyser des données d'entrée comme : perception de leur environnement (vision, Reconnaissance de formes tels des visages, schémas, segmentation d'image, langages naturels, caractères dactylographiés ou manuscrits ; moteurs de recherche, analyse et indexation d'images et de vidéo, en particulier pour la recherche d'image par le contenu ; aide aux diagnostics, médical notamment, bio-informatique, chémoinformatique ; interfaces cerveau-machine ; détection de fraudes à la carte de crédit, cybersécurité, analyse financière, dont analyse du marché boursier ; classification des séquences d'ADN ; jeu ; génie logiciel ; adaptation de sites Web ; robotique (locomotion de robots, etc.) ; analyse prédictive dans de nombreux domaines (financière, médicale, juridique, judiciaire), diminution des temps de calcul pour les simulations informatiques en physique (calcul de structures, de mécanique des fluides, de neutronique, d'astrophysique, de biologie moléculaire, etc.),, optimisation de design dans l'industrie,,, etc.Exemples :un système d'apprentissage automatique peut permettre à un robot ayant la capacité de bouger ses membres, mais ne sachant initialement rien de la coordination des mouvements permettant la marche, d'apprendre à marcher. Le robot commencera par effectuer des mouvements aléatoires, puis, en sélectionnant et privilégiant les mouvements lui permettant d'avancer, mettra peu à peu en place une marche de plus en plus efficace[réf. nécessaire] ;la reconnaissance de caractères manuscrits est une tâche complexe car deux caractères similaires ne sont jamais exactement identiques. Il existe des systèmes d'apprentissage automatique qui apprennent à reconnaître des caractères en observant des « exemples », c'est-à-dire des caractères connus. Un des premiers système de ce type est celui de reconnaissance des codes postaux US manuscrits issu des travaux de recherche de Yann Le Cun, un des pionniers du domaine ,, et ceux utilisés pour la  reconnaissance d'écriture ou OCR.Les algorithmes d'apprentissage peuvent se catégoriser selon le mode d'apprentissage qu'ils emploient.Si les classes sont prédéterminées et les exemples connus, le système apprend à classer selon un modèle de classification ou de classement ; on parle alors d'apprentissage supervisé (ou d'analyse discriminante). Un expert (ou oracle) doit préalablement étiqueter des exemples. Le processus se passe en deux phases. Lors de la première phase (hors ligne, dite d'apprentissage), il s'agit de déterminer un modèle à partir des données étiquetées. La seconde phase (en ligne, dite de test) consiste à prédire l'étiquette d'une nouvelle donnée, connaissant le modèle préalablement appris. Parfois il est préférable d'associer une donnée non pas à une classe unique, mais une probabilité d'appartenance à chacune des classes prédéterminées ; on parle alors d'apprentissage supervisé probabiliste.Fondamentalement, le machine learning supervisé revient à apprendre à une machine à construire une fonction f telle que Y = f(X), Y étant un (ou plusieurs) résultat(s) d'intérêt calculé en fonction de données d'entrées X effectivement à la disposition de l'utilisateur. Y peut être une grandeur continue (une température par exemple), et on parle alors de régression, ou discrète (une classe, chien ou chat par exemple), et on parle alors de classification.Des cas d'usage typiques d'apprentissage automatique peuvent être d'estimer la météo du lendemain en fonction de celle du jour et des jours précédents, de prédire le vote d'un électeur en fonction de certaines données économiques et sociales, d'estimer la résistance d'un nouveau matériau en fonction de sa composition, de déterminer la présence ou non d'un objet dans une image. L'analyse discriminante linéaire ou les SVM en sont d'autres exemples typiques. Autre exemple, en fonction de points communs détectés avec les symptômes d'autres patients connus (les exemples), le système peut catégoriser de nouveaux patients, au vu de leurs analyses médicales, en risque estimé de développer telle ou telle maladie.Quand le système ou l'opérateur ne dispose que d'exemples, mais non d'étiquette, et que le nombre de classes et leur nature n'ont pas été prédéterminées, on parle d'apprentissage non supervisé ou clustering en anglais. Aucun expert n'est requis. L'algorithme doit découvrir par lui-même la structure plus ou moins cachée des données. Le partitionnement de données, data clustering en anglais, est un algorithme d'apprentissage non supervisé.Le système doit ici — dans l'espace de description (l'ensemble des données) — cibler les données selon leurs attributs disponibles, pour les classer en groupes homogènes d'exemples. La similarité est généralement calculée selon une fonction de distance entre paires d'exemples. C'est ensuite à l'opérateur d'associer ou déduire du sens pour chaque groupe et pour les motifs (patterns en anglais) d'apparition de groupes, ou de groupes de groupes, dans leur « espace ». Divers outils mathématiques et logiciels peuvent l'aider. On parle aussi d'analyse des données en régression (ajustement d'un modèle par une procédure de type moindres carrés ou autre optimisation d'une fonction de coût). Si l'approche est probabiliste (c'est-à-dire que chaque exemple, au lieu d'être classé dans une seule classe, est caractérisé par un jeu de probabilités d'appartenance à chacune des classes), on parle alors de « soft clustering » (par opposition au « hard clustering »).Cette méthode est souvent source de sérendipité. ex. : Pour un épidémiologiste qui voudrait dans un ensemble assez large de victimes de cancer du foie tenter de faire émerger des hypothèses explicatives, l'ordinateur pourrait différencier différents groupes, que l'épidémiologiste chercherait ensuite à associer à divers facteurs explicatifs, origines géographique, génétique, habitudes ou pratiques de consommation, expositions à divers agents potentiellement ou effectivement toxiques (métaux lourds, toxines telle que l'aflatoxine, etc.).Contrairement à l’apprentissage supervisé où l’apprentissage automatique consiste à trouver une fonction f telle que Y = f(X), où Y est un résultat connu et objectif (par exemple Y = « présence d’une tumeur » ou « absence de tumeur » en fonction de X = image radiographique), dans l’apprentissage non supervisé, on ne dispose pas de valeurs de Y, uniquement de valeurs de X (dans l’exemple précédent, on disposerait uniquement des images radiographiques sans connaissance de la présence ou non d’une tumeur. L'apprentissage non supervisé pourrait découvrir deux ""clusters"" ou groupes correspondant à ""présence"" ou ""absence"" de tumeur, mais les chances de réussite sont moindres que dans le cas supervisé où la machine est orientée sur ce qu'elle doit trouver).L’apprentissage non supervisé est généralement moins performant que l’apprentissage supervisé, il évolue dans une zone « grise » où il n’y a généralement pas de « bonne » ou de « mauvaise » réponse mais simplement des similarités mathématiques discernables ou non. L’apprentissage non supervisé présente cependant l’intérêt de pouvoir travailler sur une base de données de X sans qu’il soit nécessaire d’avoir des valeurs de Y correspondantes, or les Y sont généralement compliqués et/ou coûteux à obtenir, alors que les seuls X sont généralement plus simples et moins coûteux à obtenir (dans l’exemple des images radiographiques, il est relativement aisé d’obtenir de telles images, alors qu’obtenir les images avec le label « présence de tumeur » ou « absence de tumeur » nécessite l’intervention longue et coûteuse d’un spécialiste en imagerie médicale).L’apprentissage non supervisé permet potentiellement de détecter des anomalies dans une base de données, comme des valeurs singulières ou aberrantes pouvant provenir d’une erreur de saisie ou d’une singularité très particulière. Il peut donc s’agir d’un outil intéressant pour vérifier ou nettoyer une base de données.Effectué de manière probabiliste ou non, il vise à faire apparaître la distribution sous-jacente des exemples dans leur espace de description. Il est mis en œuvre quand des données (ou « étiquettes ») manquent… Le modèle doit utiliser des exemples non étiquetés pouvant néanmoins renseigner. ex. : En médecine, il peut constituer une aide au diagnostic ou au choix des moyens les moins onéreux de tests de diagnostic.Probabiliste ou non, quand l'étiquetage des données est partiel. C'est le cas quand un modèle énonce qu'une donnée n'appartient pas à une classe A, mais peut-être à une classe B ou C (A, B et C étant trois maladies par exemple évoquées dans le cadre d'un diagnostic différentiel).L’apprentissage auto-supervisé consiste à construire un problème d’apprentissage supervisé à partir d’un problème non supervisé à l’origine.Pour rappel, l’apprentissage supervisé consiste à construire une fonction Y = f(X) et nécessite donc une base de données où l’on possède des Y en fonction des X (par exemple, en fonction du texte X correspondant à la critique d’un film, retrouver la valeur du Y correspondant à la note attribuée au film), alors que dans l’apprentissage non supervisé, on dispose uniquement des valeurs de X et pas de valeurs de Y (on disposerait par exemple ici uniquement du texte X correspondant à la critique du film, et pas de la note Y attribuée au film).L’apprentissage auto-supervisé consiste donc à créer des Y à partir des X pour passer à un apprentissage supervisé, en ""masquant"" des X pour en faire des Y. Dans le cas d'une image, l'apprentissage auto-supervisé peut consister à reconstruire la partie manquante d'une image qui aurait été tronquée. Dans le cas du langage, lorsqu’on dispose d’un ensemble de phrases qui correspondent aux X sans cible Y particulière, l’apprentissage auto-supervisé consiste à supprimer certains X (certains mots) pour en faire des Y. L’apprentissage auto-supervisé revient alors pour la machine à essayer de reconstruire un mot ou un ensemble de mots manquants en fonction des mots précédents et/ou suivants, en une forme d’auto-complétion. Cette approche permet potentiellement à une machine de « comprendre » le langage humain, son sens sémantique et symbolique. Les modèles IA de langage comme BERT ou GPT-3 sont conçus selon ce principe. Dans le cas d’un film, l’apprentissage auto-supervisé consisterait à essayer de prédire les images suivantes en fonction des images précédentes, et donc à tenter de prédire « l’avenir » sur la base de la logique probable du monde réel.Certains chercheurs, comme Yann Le Cun, pensent que si l’IA générale est possible, c’est probablement par une approche de type auto-supervisé qu’elle pourrait être conçue, par exemple en étant immergée dans le monde réel pour essayer à chaque instant de prédire les images et les sons les plus probables à venir, en comprenant qu’un ballon en train de rebondir et de rouler va encore continuer à rebondir et à rouler, mais de moins en moins haut et de moins en moins vite jusqu’à s’arrêter, et qu'un obstacle est de nature à arrêter le ballon ou à modifier sa trajectoire, ou à essayer de prédire les prochains mots qu’une personne est susceptible de prononcer ou le prochain geste qu’elle pourrait accomplir. L’apprentissage auto-supervisé dans le monde réel serait une façon d’apprendre à une machine le sens commun, le bon sens, la réalité du monde physique qui l’entoure, et permettrait potentiellement d’atteindre une certaine forme de conscience. Il ne s’agit évidemment que d’une hypothèse de travail, la nature exacte de la conscience, son fonctionnement et sa définition même restant un domaine actif de recherche.L'algorithme apprend un comportement étant donné une observation. L'action de l'algorithme sur l'environnement produit une valeur de retour qui guide l'algorithme d'apprentissage.Par exemple, l'algorithme de Q-learning est un exemple classique.L'apprentissage par renforcement peut aussi être vu comme une forme d'apprentissage auto-supervisé. Dans un problème d'apprentissage par renforcement, il n'y a en effet à l'origine pas de données de sorties Y, ni même de données d'entrée X, pour construire une fonction Y = f(X). Il y a simplement un ""écosystème"" avec des règles qui doivent être respectées, et un ""objectif"" à atteindre. Par exemple, pour le football, il y a des règles du jeu à respecter et des buts à marquer. Dans l'apprentissage par renforcement, le modèle crée lui-même sa base de donnes en ""jouant"" (d'où le concept d'auto-supervisé) : il teste des combinaisons de données d'entrée X et il en découle un résultat Y qui est évalué, s'il est conforme aux règles du jeu et atteint son objectif, le modèle est récompensé et sa stratégie est ainsi validée, sinon le modèle est pénalisé. Par exemple pour le football, dans une situation du type ""ballon possédé, joueur adverse en face, but à 20 mètres"", une stratégie peut être de ""tirer"" ou de ""dribbler"", et en fonction du résultat (""but marqué"", ""but raté"", ""balle toujours possédée, joueur adverse franchi""), le modèle apprend de manière incrémentale comment se comporter au mieux en fonction des différentes situations rencontrées.L’apprentissage par transfert peut être vu comme la capacité d’un système à reconnaître et à appliquer des connaissances et des compétences, apprises à partir de tâches antérieures, sur de nouvelles tâches ou domaines partageant des similitudes. Il s'agit d'identifier les similitudes entre la ou les tâche(s) cible(s) et la ou les tâche(s) source(s), puis de transférer la connaissance de la ou des tâche(s) source(s) vers la ou les tâche(s) cible(s),.Une application classique de l’apprentissage par transfert est l’analyse d’images. Pour une problématique de classification, l’apprentissage par transfert consiste à repartir d’un modèle existant plutôt que de repartir de zéro. Si par exemple on dispose déjà d’un modèle capable de repérer un chat parmi tout autre objet du quotidien, et que l’on souhaite classifier les chats par races, il est possible que réentraîner partiellement le modèle existant permette d’obtenir de meilleures performances et à moindre coût qu’en repartant de zéro,. Un modèle souvent utilisé pour réaliser un apprentissage par transfert de ce type est VGG-16, un réseau de neurones conçu par l'Université d'Oxford, entraîné sur ~14 millions d'images, capable de classer avec ~93% de précision mille objets du quotidien.Les algorithmes se classent en quatre familles ou types principaux :régressionclassificationpartitionnement de donnéesréduction de dimensions.Plus précisément :la régression linéaire ;la régression logistique ;les machines à vecteur de support ;les réseaux de neurones, dont les méthodes d'apprentissage profond (deep learning en anglais) pour un apprentissage supervisé ou non-supervisé ;la méthode des k plus proches voisins pour un apprentissage supervisé ;les arbres de décision, méthodes à l'origine des Random Forest, par extension également du boosting (notamment XGBoost) ;les méthodes statistiques comme le modèle de mixture gaussienne ;l'analyse discriminante linéaire ;les algorithmes génétiques et la programmation génétique ;le boosting ;le bagging.Ces méthodes sont souvent combinées pour obtenir diverses variantes d'apprentissage. Le choix d'un algorithme dépend fortement de la tâche à résoudre (classification, estimation de valeurs…), du volume et de la nature des données. Ces modèles reposent souvent sur des modèles statistiques.La qualité de l'apprentissage et de l'analyse dépendent du besoin en amont et a priori de la compétence de l'opérateur pour préparer l'analyse. Elle dépend aussi de la complexité du modèle (spécifique ou généraliste), de son adéquation et de son adaptation au sujet à traiter. In fine, la qualité du travail dépendra aussi du mode (de mise en évidence visuelle) des résultats pour l'utilisateur final (un résultat pertinent pourrait être caché dans un schéma trop complexe, ou mal mis en évidence par une représentation graphique inappropriée).Avant cela, la qualité du travail dépendra de facteurs initiaux contraignants, liées à la base de données :nombre d'exemples (moins il y en a, plus l'analyse est difficile, mais plus il y en a, plus le besoin de mémoire informatique est élevé et plus longue est l'analyse) ;nombre et qualité des attributs décrivant ces exemples. La distance entre deux « exemples » numériques (prix, taille, poids, intensité lumineuse, intensité de bruit, etc.) est facile à établir, celle entre deux attributs catégoriels (couleur, beauté, utilité…) est plus délicate ;pourcentage de données renseignées et manquantes ;bruit : le nombre et la « localisation » des valeurs douteuses (erreurs potentielles, valeurs aberrantes…) ou naturellement non-conformes au pattern de distribution générale des « exemples » sur leur espace de distribution impacteront sur la qualité de l'analyse.L'apprentissage automatique ne se résume pas à un ensemble d'algorithmes, mais suit une succession d'étapes,.Définir le problème à résoudre.Acquérir des données : l'algorithme se nourrissant des données en entrée, c'est une étape importante. Il en va de la réussite du projet, de récolter des données pertinentes et en quantité et qualité suffisantes, et en évitant tout biais dans leur représentativité.Analyser et explorer les données. L'exploration des données peut révéler des données d'entrée ou de sortie déséquilibrées pouvant nécessiter un rééquilibrage, le machine learning non supervisé peut révéler des clusters qu'il pourrait être utile de traiter séparément ou encore détecter des anomalies qu'il pourrait être utile de supprimer.Préparer et nettoyer les données : les données recueillies doivent être retouchées avant utilisation. En effet, certains attributs sont inutiles, d’autre doivent être modifiés afin d’être compris par l’algorithme (les variables qualitatives doivent être encodées-binarisées), et certains éléments sont inutilisables car leurs données sont incomplètes (les valeurs manquantes doivent être gérées, par exemple par simple suppression des exemples comportant des variables manquantes, ou par remplissage par la médiane, voire par apprentissage automatique). Plusieurs techniques telles que la visualisation de données, la transformation de données (en) ou encore la normalisation (variables projetées entre 0 et 1) ou la standardisation (variables centrées - réduites) sont employées afin d'homogénéiser les variables entre elles, notamment pour aider la phase de descente de gradient nécessaire à l'apprentissage.Ingénierie ou extraction de caractéristiques : les attributs peuvent être combinés entre eux pour en créer de nouveaux plus pertinents et efficaces pour l’entraînement du modèle. Ainsi, en physique, de la construction de nombres adimensionnels adaptés au problème, de solutions analytiques approchées, de statistiques pertinentes, de corrélations empiriques ou l'extraction de spectres par transformée de Fourier ,. Il s'agit d'ajouter l'expertise humaine au préalable de l'apprentissage machine pour favoriser celui-ci.Choisir ou construire un modèle d’apprentissage : un large choix d'algorithmes existe, et il faut en choisir un adapté au problème et aux données. La métrique optimisée doit être choisie judicieusement (erreur absolue moyenne, erreur relative moyenne, précision, rappel, etc.)Entraîner, évaluer et optimiser : l'algorithme d'apprentissage automatique est entraîné et validé sur un premier jeu de données pour optimiser ses hyperparamètres.Test : puis il est évalué sur un deuxième ensemble de données de test afin de vérifier qu'il est efficace avec un jeu de donnée indépendant des données d’entraînement, et pour vérifier qu'il ne fasse pas de surapprentissage.Déployer : le modèle est alors déployé en production pour faire des prédictions, et potentiellement utiliser les nouvelles données en entrée pour se ré-entraîner et être amélioré.Expliquer : déterminer quelles sont les variables importantes et comment elles impactent les prédictions du modèle en général et au cas par casLa plupart de ces étapes se retrouvent dans les méthodes et processus de projet KDD, CRISP-DM et SEMMA, qui concernent les projets d'exploration de données.Toutes ces étapes sont complexes et requièrent du temps et de l'expertise, mais il existe des outils permettant de les automatiser au maximum pour ""démocratiser"" l'accès à l'apprentissage automatique. Ces approches sont dites ""Auto ML"" (pour machine learning automatique) ou ""No Code"" (pour illustrer que ces approches ne nécessitent pas ou très peu de programmation informatique), elles permettent d'automatiser la construction de modèles d'apprentissage automatique pour limiter au maximum le besoin d'intervention humaine. Parmi ces outils, commerciaux ou non, on peut citer Caret, PyCaret, pSeven, Jarvis, Knime, MLBox ou DataRobot.La voiture autonome paraît en 2016 réalisable grâce à l’apprentissage automatique et les énormes quantités de données générées par la flotte automobile, de plus en plus connectée. Contrairement aux algorithmes classiques (qui suivent un ensemble de règles prédéterminées), l’apprentissage automatique apprend ses propres règles.Les principaux innovateurs dans le domaine insistent sur le fait que le progrès provient de l’automatisation des processus. Ceci présente le défaut que le processus d’apprentissage automatique devient privatisé et obscur. Privatisé, car les algorithmes d’AA constituent des gigantesques opportunités économiques, et obscurs car leur compréhension passe derrière leur optimisation. Cette évolution peut potentiellement nuire à la confiance du public envers l’apprentissage automatique, mais surtout au potentiel à long terme de techniques très prometteuses.La voiture autonome présente un cadre test pour confronter l’apprentissage automatique à la société. En effet, ce n’est pas seulement l’algorithme qui se forme à la circulation routière et ses règles, mais aussi l’inverse. Le principe de responsabilité est remis en cause par l’apprentissage automatique, car l’algorithme n’est plus écrit mais apprend et développe une sorte d’intuition numérique. Les créateurs d’algorithmes ne sont plus en mesure de comprendre les « décisions » prises par leurs algorithmes, ceci par construction mathématique même de l’algorithme d’apprentissage automatique.Dans le cas de l’AA et les voitures autonomes, la question de la responsabilité en cas d’accident se pose. La société doit apporter une réponse à cette question, avec différentes approches possibles. Aux États-Unis, il existe la tendance à juger une technologie par la qualité du résultat qu’elle produit, alors qu’en Europe le principe de précaution est appliqué, et on y a plus tendance à juger une nouvelle technologie par rapport aux précédentes, en évaluant les différences par rapport à ce qui est déjà connu. Des processus d’évaluation de risques sont en cours en Europe et aux États-Unis.La question de responsabilité est d’autant plus compliquée que la priorité chez les concepteurs réside en la conception d’un algorithme optimal, et non pas de le comprendre. L’interprétabilité des algorithmes est nécessaire pour en comprendre les décisions, notamment lorsque ces décisions ont un impact profond sur la vie des individus. Cette notion d’interprétabilité, c’est-à-dire de la capacité de comprendre pourquoi et comment un algorithme agit, est aussi sujette à interprétation.La question de l’accessibilité des données est sujette à controverse : dans le cas des voitures autonomes, certains défendent l’accès public aux données, ce qui permettrait un meilleur apprentissage aux algorithmes et ne concentrerait pas cet « or numérique » dans les mains d’une poignée d’individus, de plus d’autres militent pour la privatisation des données au nom du libre marché, sans négliger le fait que des bonnes données constituent un avantage compétitif et donc économique,.La question des choix moraux liés aux décisions laissées aux algorithmes d'AA et aux voitures autonomes en cas de situations dangereuses ou mortelles se pose aussi. Par exemple en cas de défaillance des freins du véhicule, et d'accident inévitable, quelles vies sont à sauver en priorité: celle des passagers ou bien celle des piétons traversant la rue ?Dans les années 2000-2010, l'apprentissage automatique est encore une technologie émergente, mais polyvalente, qui est par nature théoriquement capable d'accélérer le rythme de l'automatisation et de l'autoaprentissage lui-même. Combiné à l'apparition de nouveaux moyens de produire, stocker et faire circuler l'énergie, ainsi qu'à l'informatique ubiquiste, il pourrait bouleverser les technologies et la société comme l'ont fait la machine à vapeur et l'électricité, puis le pétrole et l'informatique lors des révolutions industrielles précédentes.L'apprentissage automatique pourrait générer des innovations et des capacités inattendues, mais avec un risque selon certains observateurs de perte de maîtrise de la part des humains sur de nombreuses tâches qu'ils ne pourront plus comprendre et qui seront faites en routine par des entités informatiques et robotisées. Ceci laisse envisager des impacts spécifiques complexes et encore impossibles à évaluer sur l'emploi, le travail et plus largement l'économie et les inégalités. Selon le journal Science fin 2017 : « Les effets sur l'emploi sont plus complexes que la simple question du remplacement et des substitutions soulignées par certains. Bien que les effets économiques du BA soient relativement limités aujourd'hui et que nous ne soyons pas confrontés à une « fin du travail » imminente comme cela est parfois proclamé, les implications pour l'économie et la main-d'œuvre sont profondes ».Il est tentant de s'inspirer des êtres vivants sans les copier naïvement pour concevoir des machines capables d'apprendre. Les notions de percept et de concept comme phénomènes neuronaux physiques ont d'ailleurs été popularisés dans le monde francophone par Jean-Pierre Changeux. L'apprentissage automatique reste avant tout un sous-domaine de l'informatique, mais il est étroitement lié opérationn"
Informatique;Dans le domaine informatique et de l'intelligence artificielle, l'apprentissage non supervisé désigne la situation d'apprentissage automatique où les données ne sont pas étiquetées. Il s'agit donc de découvrir les structures sous-jacentes à ces données non étiquetées. Puisque les données ne sont pas étiquetées, il est impossible à l'algorithme de calculer de façon certaine un score de réussite.L'absence d'étiquetage ou d'annotation caractérise les tâches d'apprentissage non supervisé et les distingue donc des tâches d'apprentissage supervisé.L'introduction dans un système d'une approche d'apprentissage non supervisé est un moyen d'expérimenter l'intelligence artificielle. En général, des systèmes d'apprentissage non supervisé permettent d'exécuter des tâches plus complexes que les systèmes d'apprentissage supervisé, mais ils peuvent aussi être plus imprévisibles. Même si un système d'IA d'apprentissage non supervisé parvient tout seul, par exemple, à faire le tri entre des chats et des chiens, il peut aussi ajouter des catégories inattendues et non désirées, et classer des races inhabituelles, introduisant plus de bruit que d'ordre.L'apprentissage non supervisé consiste à apprendre sans superviseur. Il s’agit d’extraire des classes ou groupes d’individus présentant des caractéristiques communes. La qualité d'une méthode de classification est mesurée par sa capacité à découvrir certains ou tous les motifs cachés.On distingue l'apprentissage supervisé et non supervisé. Dans le premier apprentissage, il s’agit d’apprendre à classer un nouvel individu parmi un ensemble de classes prédéfinies: on connaît les classes a priori. Tandis que dans l'apprentissage non supervisé, le nombre et la définition des classes ne sont pas donnés a priori.Différence entre les deux types d'apprentissage. Apprentissage supervisé On dispose d'éléments déjà classésExemple : articles en rubrique cuisine, sport, culture...On veut classer un nouvel élémentExemple: lui attribuer un nom parmi cuisine, sport, culture... Apprentissage non supervisé On dispose d'éléments non classésExemple : une fleurOn veut les regrouper en classesExemple: si deux fleurs ont la même forme, elles sont en rapport avec une même plante correspondante.Il existe deux principales méthodes d'apprentissage non supervisées :Les méthodes par partitionnement telles que les algorithmes des k-moyennes ou k-médoïdes.Les méthodes de regroupement hiérarchique.Les techniques d'apprentissage non supervisé peuvent être utilisées pour résoudre, entre autres, les problèmes suivants :le partitionnement de données (par exemple avec l'algorithme des k-moyennes, le regroupement hiérarchique),l'estimation de densité de distribution (distribution de mélange, estimation par noyau),la réduction de dimension (analyse en composantes principales, carte auto-adaptative)L'apprentissage non supervisé peut aussi être utilisé en conjonction avec une inférence bayésienne pour produire des probabilités conditionnelles pour chaque variable aléatoire étant donné les autres.K-means clustering (K-moyenne)Dimensionality Reduction (Réduction de la dimensionnalité)Principal Component Analysis (Analyse en composantes principales)Singular Value Decomposition (Décomposition en valeurs singulières)Independent Component Analysis (Analyse en composantes indépendantes)Distribution models (Modèles de distribution)Hierarchical clustering (Classification hiérarchique)Le regroupement ou Clustering est la technique la plus utilisée pour résoudre les problèmes d'apprentissage non supervisé. La mise en cluster consiste à séparer ou à diviser un ensemble de données en un certain nombre de groupes, de sorte que les ensembles de données appartenant aux mêmes groupes se ressemblent davantage que ceux d’autres groupes. En termes simples, l’objectif est de séparer les groupes ayant des traits similaires et de les assigner en grappes.Voyons cela avec un exemple. Supposons que vous soyez le chef d’un magasin de location et que vous souhaitiez comprendre les préférences de vos clients pour développer votre activité. Vous pouvez regrouper tous vos clients en 10 groupes en fonction de leurs habitudes d’achat et utiliser une stratégie distincte pour les clients de chacun de ces 10 groupes. Et c’est ce que nous appelons le Clustering.Le clustering consiste à grouper des points de données en fonction de leurs similitudes, tandis que l’association consiste à découvrir des relations entre les attributs de ces points de données:Les techniques de clustering cherchent à décomposer un ensemble d'individus en plusieurs sous ensembles les plus homogènes possiblesOn ne connaît pas la classe des exemples (nombre, forme, taille)Les méthodes sont très nombreuses, typologies généralement employées pour les distinguer  Méthodes de partitionnement / Méthodes hiérarchiquesAvec recouvrement / sans recouvrementAutre : incrémental / non incrémentalD'éventuelles informations sur les classes ou d'autres informations sur les données n'ont pas d'influence sur la formation des clusters, seulement sur leur interprétation.L'un des algorithmes le plus connu et utilisé en clustering est la K-moyenne.Cet algorithme va mettre dans des “zones” (Cluster), les données qui se ressemblent. Les données se trouvant dans le même cluster sont similaires.L’approche de K-Means consiste à affecter aléatoirement des centres de clusters (appelés centroids), et ensuite assigner chaque point de nos données au centroid qui lui est le plus proche. Cela s’effectue jusqu’à assigner toutes les données à un cluster. Portail de l’informatique   Portail des probabilités et de la statistique   Portail des données
Informatique;L'apprentissage supervisé (supervised learning en anglais) est une tâche d'apprentissage automatique consistant à apprendre une fonction de prédiction à partir d'exemples annotés, au contraire de l'apprentissage non supervisé. On distingue les problèmes de régression des problèmes de classement. Ainsi, on considère que les problèmes de prédiction d'une variable quantitative sont des problèmes de régression tandis que les problèmes de prédiction d'une variable qualitative sont des problèmes de classification.Les exemples annotés constituent une base d'apprentissage, et la fonction de prédiction apprise peut aussi être appelée « hypothèse » ou « modèle ». On suppose cette base d'apprentissage représentative d'une population d'échantillons plus large et le but des méthodes d'apprentissage supervisé est de bien généraliser, c'est-à-dire d'apprendre une fonction qui fasse des prédictions correctes sur des données non présentes dans l'ensemble d'apprentissage.Soit                     (        ?        ,                              A                          ,                  P                )              {\displaystyle (\Omega ,{\mathcal {A}},\mathbb {P} )}  , un espace probabilisé.Soit                     (                              X                          ,                                            F                                            X                          )        ,        (                              Y                          ,                                            F                                            Y                          )              {\displaystyle ({\mathcal {X}},{\mathcal {F}}_{X}),({\mathcal {Y}},{\mathcal {F}}_{Y})}   deux espaces mesurables. On peut définir une base de données d'apprentissage (ou ensemble d'apprentissage) comme un ensemble de couples entrée-sortie                     (                  x                      n                          ,                  y                      n                                    )                      1            ?            n            ?            N                                {\displaystyle (x_{n},y_{n})_{1\leq n\leq N}}   où chaque                               x                      n                          ?                              X                                {\displaystyle x_{n}\in {\mathcal {X}}}   et                               y                      n                          ?                              Y                                {\displaystyle y_{n}\in {\mathcal {Y}}}   sont des réalisations respectives des variables aléatoires                               X                      n                                {\displaystyle X_{n}}   et                               Y                      n                                {\displaystyle Y_{n}}  . Les couples de la suite                     (        (                  X                      n                          ,                  Y                      n                          )                  )                      n            ?            N                                {\displaystyle ((X_{n},Y_{n}))_{n\leq N}}   sont indépendants et identiquement distribués suivant la loi d'un couple                     (        X        ,        Y        )              {\displaystyle (X,Y)}   à valeurs dans                     (                              X                          ×                              Y                          ,                                            F                                            X                          ?                                            F                                            Y                          )              {\displaystyle ({\mathcal {X}}\times {\mathcal {Y}},{\mathcal {F}}_{X}\otimes {\mathcal {F}}_{Y})}  . On rappelle que cette loi est caractérisée par une mesure de probabilité                                           P                                (            X            ,            Y            )                                {\displaystyle \mathbb {P} _{(X,Y)}}   définie pour tout évènement                     A        ?                                            F                                            X                          ?                                            F                                            Y                                {\displaystyle A\in {\mathcal {F}}_{X}\otimes {\mathcal {F}}_{Y}}   par                                           P                                (            X            ,            Y            )                          (        A        )        =                  P                [        (        X        ,        Y                  )                      ?            1                          (        A        )        ]              {\displaystyle \mathbb {P} _{(X,Y)}(A)=\mathbb {P} [(X,Y)^{-1}(A)]}  Par exemple                               X                      n                                {\displaystyle X_{n}}   suit une loi uniforme et                               Y                      n                          =        f        (                  X                      n                          )        +                  ?                      n                                {\displaystyle Y_{n}=f(X_{n})+\epsilon _{n}}   où                               ?                      n                                {\displaystyle \epsilon _{n}}   est un bruit centré. Dans ce cas, la méthode d'apprentissage supervisé utilise cette base d'apprentissage pour déterminer une estimation de f notée g et appelée indistinctement fonction de prédiction, hypothèse ou modèle qui à une nouvelle entrée x associe une sortie g(x). Le but d'un algorithme d'apprentissage supervisé est donc de généraliser pour des entrées inconnues ce qu'il a pu « apprendre » grâce aux données déjà annotées par des experts, ceci de façon « raisonnable ». On dit que la fonction de prédiction apprise doit avoir de bonnes garanties en généralisation.Plus généralement, l'objectif de l'apprentissage supervisé est d'apprendre une fonction                     f              {\displaystyle f}   qui « minimise l'écart entre les variables aléatoires                     f        (        X        )              {\displaystyle f(X)}   et                     Y              {\displaystyle Y}   ». Pour définir cet écart, nous introduisons une fonction de perte                     L        :                              Y                          ×                              Y                          ?                              R                                +                                {\displaystyle L:{\mathcal {Y}}\times {\mathcal {Y}}\rightarrow \mathbb {R} _{+}}   qui quantifie la distance entre une prédiction du modèle                     f        (        x        )              {\displaystyle f(x)}   et une sortie attendue                     y              {\displaystyle y}  . À partir de cette fonction, nous pouvons définir le risque statistique d'une modèle                     f              {\displaystyle f}  . Il est noté                     R              {\displaystyle R}   et est défini par :En pratique, on n'a jamais accès directement à                                           P                                (            X            ,            Y            )                                {\displaystyle \mathbb {P} _{(X,Y)}}  , en revanche il est possible de l'estimer à partir du jeu de données en utilisant la mesure empirique                                           P                                (            X            ,            Y            )                                N                                {\displaystyle \mathbb {P} _{(X,Y)}^{N}}   définie pour tout                     A        ?                                            F                                            X                          ?                                            F                                            Y                                {\displaystyle A\in {\mathcal {F}}_{X}\otimes {\mathcal {F}}_{Y}}   par                                           P                                (            X            ,            Y            )                                N                          (        A        )        =                                            1              N                                                ?                      n            =            1                                N                                    ?                      (                          X                              n                                      ,                          Y                              n                                      )                          (        A        )              {\displaystyle \mathbb {P} _{(X,Y)}^{N}(A)={\dfrac {1}{N}}\sum _{n=1}^{N}\delta _{(X_{n},Y_{n})}(A)}  .Dès lors, un algorithme d'apprentissage supervisé mettra en œuvre des algorithmes d'optimisation afin de trouver une fonction                     f              {\displaystyle f}   qui minimise le risque empirique                               R                      N                          (        f        )        =                                            1              N                                                ?                      n            =            1                                N                          L        (                  Y                      n                          ,        f        (                  X                      n                          )        )              {\displaystyle R_{N}(f)={\dfrac {1}{N}}\sum _{n=1}^{N}L(Y_{n},f(X_{n}))}  . Il faut noter que                               R                      N                                {\displaystyle R_{N}}   n'est rien d'autre que la moyenne des écart (au sens de                     L              {\displaystyle L}  ) entre les prédictions du modèle et les sorties attendues.On distingue trois types de problèmes solubles avec une méthode d'apprentissage automatique supervisée :                                          Y                          ?                  R                      {\displaystyle {\mathcal {Y}}\subset \mathbb {R} }   : lorsque la sortie que l'on cherche à estimer est une valeur dans un ensemble continu de réels, on parle d'un problème de régression. La fonction de prédiction est alors appelée un régresseur.                                          Y                          =        {        1        ,        …        ,        I        }              {\displaystyle {\mathcal {Y}}=\{1,\ldots ,I\}}   : lorsque l'ensemble des valeurs de sortie est fini, on parle d'un problème de classification, qui revient à attribuer une étiquette à chaque entrée. La fonction de prédiction est alors appelée un classifieur.Lorsque                                           Y                                {\displaystyle {\mathcal {Y}}}   est un ensemble de données structurées, on parle d'un problème de prédiction structurée, qui revient à attribuer une sortie complexe à chaque entrée. Par exemple, en bio-informatique le problème de prédiction de réseaux d’interactions entre gènes peut être considéré comme un problème de prédiction structurée dans laquelle l'ensemble possible des sorties structurées est l'ensemble de tous les graphes modélisant les interactions possibles.Une bonne estimation de                     f              {\displaystyle f}   vérifierait                     f        (        X        )        =                  E                (        Y                  |                X        )              {\displaystyle f(X)=\mathbb {E} (Y|X)}  . On estimerait donc                     Y              {\displaystyle Y}   par son espérance conditionnelle par rapport à                     X              {\displaystyle X}  . Le théorème suivant montre l'intérêt d'utiliser la fonction de perte quadratique dans le cas d'une régression.BoostingMachine à vecteurs de supportMélanges de loisRéseau de neurones artificielsMéthode des k plus proches voisinsArbre de décisionClassification naïve bayésienneInférence grammaticaleEspace de versionsVision par ordinateurReconnaissance de formesReconnaissance de l'écriture manuscriteReconnaissance vocaleTraitement automatique de la langueBio-informatiqueReconnaissance optique de caractèresVincent Barra, Antoine Cornuéjols, Laurent Miclet, Apprentissage Artificiel : Concepts et algorithmes, Eyrolles, 2021 (ISBN 978-2-416-001-04-8) [détail des éditions](en) Tom M. Mitchell, Machine Learning, 1997 [détail des éditions](en) Christopher M. Bishop, Pattern Recognition And Machine Learning, Springer, 2006 (ISBN 0-387-31073-8) [détail des éditions] Portail des probabilités et de la statistique   Portail de l’informatique   Portail des données
Informatique;"L'informatique théorique est l'étude des fondements logiques et mathématiques de l'informatique. C'est une branche de la science informatique et la science formelle. Plus généralement, le terme est utilisé pour désigner des domaines ou sous-domaines de recherche centrés sur des vérités universelles (axiomes) en rapport avec l'informatique. L'informatique théorique se caractérise par une approche par nature plus mathématique et moins empirique de l'informatique et ses objectifs ne sont pas toujours directement reliés à des enjeux technologiques. De nombreuses disciplines peuvent être regroupées sous cette dénomination diffuse dont :la théorie de la calculabilité,l'algorithmique et la théorie de la complexité,la théorie de l'information,l'étude de la sémantique des langages de programmation,la logique mathématique,la théorie des automates et des langages formels.Dans cette section, nous donnons une histoire de l'informatique théorique en nous appuyant sur différentes sources :l'hommage à Maurice Nivat dans le bulletin de la SIF,l'histoire du CRIN,le livre Models of Computation de John E. Savage.Les logiciens Bertrand Russel, David Hilbert et George Boole furent des précurseurs de l'informatique théorique. Mais cette branche de l'informatique a surtout vu le jour à partir des travaux d'Alan Turing et Alonzo Church en 1936, qui ont introduit les modèles formels de calculs (les machines de Turing et le lambda calcul). Ils ont montré l'existence de machines universelles capables de simuler toutes les machines du même type, par exemple les machines de Turing universelles. En 1938, Claude Shannon montre que l'algèbre booléenne explique le comportement des circuits avec des relais électromécaniques. En 1945, John von Neumann introduit la notion d'architecture de von Neumann à la base des ordinateurs. En 1948, Claude Shannon publie A Mathematical Theory of Communication, fondateur de la théorie de l'information. En 1949, il annonce les fondements de la théorie de la complexité des circuits booléens.La programmation des ordinateurs de l'époque était difficile. Par exemple, il était nécessaire de brancher ou débrancher  une centaine de câbles sur l'ordinateur ENIAC afin de réaliser une simple opération.Une contribution importante des années 1950 est la création de langages de programmation comme FORTRAN, COBOL et LISP, qui offrent des constructions de haut-niveau pour écrire :des instructions conditionnelles,des structures de contrôle telles que des boucles de test,des procédures.La théorie des langages et des automates est un sujet important dans les années 1950, car il permet de comprendre l'expressivité des langages informatiques et leur mise en œuvre. Les machines à états finis et les automates à pile sont formellement définis. Puis Michael Oser Rabin et Dana Stewart Scott étudient mathématiquement le pouvoir expressif et les limites de ces modèles.En 1964, Noam Chomsky définit la hiérarchie de Chomsky. Plusieurs classes de langages (langages rationnels, langages algébriques, langages avec contextes, langages récursivement énumérables) correspondent à des types de machines théoriques différentes (automates finis, automates à pile, machine de Turing à mémoire linéaire, machine de Turing). Différentes variantes de ces classes de langages et machines sont étudiés.Hartmanis, Lewis et Stearns et d'autres classifient les langages selon le temps et/ou la mémoire qu'il faut pour les calculer. Ce sont les balbutiements de la théorie de la complexité.Durant les années 1970, la théorie de la complexité se développe. Les classes de complexité P et NP sont définis ; la NP-complétude est définie indépendamment par Stephen Cook et Leonid Levin. Richard Karp a démontré l'importance des langages NP-complets. La question P = NP est posée et les chercheurs conjecturaient que l'on pourrait la résoudre via la correspondance entre machines de Turing et circuits.Se développent aussi les méthodes formelles pour vérifier les programmes. On définit des sémantiques formelles aux langages de programmation.Se développement aussi des connections entre le modèle de base de données relationnelles et le calcul des relations, afin de réaliser des requêtes dans des bases de données de manière efficace.Ces années ont été florissantes également en algorithmique. Donald Knuth a beaucoup influencé le développement de l'algorithmique ainsi que Aho, Hopcroft et Ullman.Les années 1980 et 1990 sont propices au développement du calcul parallèle et des systèmes distribués.Il n'est pas facile de cerner précisément ce que l'on entend par « informatique théorique ». Le terme renvoie plutôt à une façon d'aborder les questions informatiques sous un angle plus mathématique et formel, en faisant souvent abstraction des aspects plus pratiques de l'informatique. En ce sens, l'informatique théorique est parfois considérée comme une branche des mathématiques discrètes. Ses objectifs se caractérisent généralement par une volonté d'identifier en principe les possibilités et les limites des ordinateurs.Le Special Interest Group on Algorithms and Computation Theory (SIGACT), regroupement affilié à l'Association for Computing Machinery (ACM) et voué au soutien à la recherche en informatique théorique en donne une définition assez large qui comprend des domaines aussi divers que l'algorithmique et les structures de données, la théorie de la complexité, le parallélisme, le VLSI, l'apprentissage automatique, la bio-informatique, la géométrie algorithmique, la théorie de l'information, la cryptographie, l'informatique quantique, la théorie algorithmique des nombres et de l'algèbre, la sémantique des langages de programmation, les méthodes formelles, la théorie des automates et l'étude de l'aléatoire en informatique.Les chercheurs en informatique théorique français sont regroupés au sein du GdR Informatique Mathématique et adhèrent à l'Association française d'informatique fondamentale, membre de la Société informatique de France au niveau français et membre de l'EATCS au niveau européen.La définition donnée par ACM SIGACT est à la fois trop restreinte en ce que la liste n'est pas exhaustive et trop large puisque plusieurs des domaines mentionnés ne sont pas uniquement axés sur des enjeux purement théoriques.Cette discipline tente de découvrir, d'améliorer et d'étudier de nouveaux algorithmes permettant de résoudre des problèmes avec une plus grande efficacité.Certains programmes sensibles nécessitent une parfaite fiabilité et de ce fait des outils mathématiques à mi-chemin entre l'algorithmique, la modélisation et l'algèbre sont développés afin de permettre de vérifier formellement les programmes et algorithmes.La théorie de l'information résulte initialement des travaux de Ronald A. Fisher. Ce statisticien théorise l'information dans sa théorie des probabilités et des échantillons. Techniquement, « l'information » est égale à la valeur moyenne du carré de la dérivée du logarithme de la loi de probabilité étudiée. À partir de l'inégalité de Cramer, la valeur d'une telle « information » est proportionnelle à la faible variabilité des conclusions résultantes. En d'autres termes, Fisher met l'information en relation avec le degré de certitude. D'autres modèles mathématiques ont complété et étendu de façon formelle la définition de l'information.Claude Shannon et Warren Weaver renforcent le paradigme. Ingénieurs en télécommunication, leurs préoccupations techniques les ont conduits à vouloir mesurer l'information pour en déduire les fondamentaux de la Communication (et non une théorie de l'information). Dans Théorie Mathématique de la Communication en 1948, ils modélisent l'information pour étudier les lois correspondantes : bruit, entropie et chaos, par analogie générale aux lois d'énergétique et de thermodynamique.Leurs travaux complétant ceux d'Alan Turing, de Norbert Wiener et de Von Neuman (pour ne citer que les principaux) constituent le socle initial des « Sciences de l'information ». La théorie de l'information s'appuie principalement sur deux notions caractéristiques que sont la variation d'incertitude et l'entropie (« désordre » au sens d'absence d'ordre et donc d'information dans l'ensemble considéré, d'où l'indétermination). Déclinant ces principes et ceux d'autres sciences dures, les technologies s'occupent de la façon d'implémenter, d'agencer et de réaliser des solutions pour répondre aux besoins des sociétés humaines.Certains chercheurs tentent de tirer des parallèles entre les concepts d'entropie en physique et d'entropie en informatique afin d'obtenir une formulation informatique de la cosmologie et de la réalité physique de notre monde qui, selon certains, pourraient trouver des clés dans des outils mathématiques que sont les automates cellulaires.Certains ne voient dans l'informatique qu'une déclinaison technologique de l'automatisation des traitements (incluant la transmission et le transport) d'information et considèrent l'usage des termes « sciences de l'informatique » comme incorrects, ces mêmes préfèrent donc l'appellation « technologies de l'information et de la communication » parce qu'ils disent qu'elle recouvre mieux les différents composants (systèmes de traitements, réseaux, etc.) de l'informatique au sens large. Cette vision très restrictive du terme « informatique » n'est pas partagée par tout le monde et d'ailleurs beaucoup d'anglo-saxons envient la richesse du mot « informatique » et commencent à l'emprunter,.La théorie des graphes permet de modéliser de nombreux problèmes discrets : calculs de trajets, allocations de ressource, problèmes SAT... On peut citer le théorème des quatre couleurs comme résultat classique de cette branche de l'informatique.La théorie de la complexité permet de classifier les algorithmes selon leur temps d'exécution asymptotique. C'est-à-dire selon leur comportement lorsqu'ils sont utilisés sur de très grandes données. C'est dans cette branche de l'informatique que se situe le célèbre problème P=NP par exemple.La théorie de la calculabilité a pour objet la caractérisation des fonctions qu'un algorithme peut calculer. En effet, il est possible de montrer qu'il existe des fonctions qui ne sont pas calculables par un ordinateur, et il est dès lors intéressant de savoir lesquelles le sont. Le problème du castor affairé ou la fonction d'Ackermann sont des exemples classiques d'objets étudiés dans ce domaine.La théorie des langages, souvent liée à la théorie des automates, s'intéresse à la reconnaissance d'ensemble de mots sur un vocabulaire donné. Elle est utilisée dans les algorithmes de traitement de la langue naturelle par exemple : traduction automatique, indexation automatique de documents, etc. ainsi que dans ceux des langues artificielles comme les langages de programmation : compilation, interprétation.La logique formelle est un outil fondamental de l'informatique, on y trouve notamment la théorie des types, le lambda calcul et la réécriture comme outils de base de la programmation fonctionnelle et des assistants de preuve.Discrete Mathematics and Theoretical Computer ScienceInformation and ComputationTheory of Computing (accès ouvert)Journal of the ACMSIAM Journal on Computing (SICOMP)SIGACT NewsTheoretical Computer ScienceTheory of Computing SystemsInternational Journal of Foundations of Computer ScienceFoundations and Trends in Theoretical Computer Science (en)Journal of Automata, Languages and CombinatoricsActa InformaticaFundamenta InformaticaeACM Transactions on AlgorithmsInformation Processing LettersOpen Computer Science (journal en accès ouvert)Lipton, Richard J. et Regan, Kenneth W., People, problems, and proofs. : Essays from Gödel’s lost letter: 2010, Berlin, Springer, 2013, XVIII-333 p. (ISBN 978-3-642-41421-3, zbMATH 1305.68025).Liste de publications importantes en informatique théorique Portail de l'informatique théorique   Portail de la logique   Portail des mathématiques   Portail de l’informatique"
Informatique;"L'intelligence artificielle (IA) est un « ensemble de théories et de techniques mises en œuvre en vue de réaliser des machines capables de simuler l'intelligence humaine ».Elle englobe donc un ensemble de concepts et de technologies, plus qu'une discipline autonome constituée. Des instances, telle la CNIL, notant le peu de précision de la définition de l'IA, l'ont présentée comme « le grand mythe de notre temps ».Souvent classée dans le groupe des mathématiques et des sciences cognitives, elle fait appel à la neurobiologie computationnelle (particulièrement aux réseaux neuronaux) et à la logique mathématique (partie des mathématiques et de la philosophie). Elle utilise des méthodes de résolution de problèmes à forte complexité logique ou algorithmique. Par extension, elle comprend, dans le langage courant, les dispositifs imitant ou remplaçant l'homme dans certaines mises en œuvre de ses fonctions cognitives.Ses finalités et enjeux ainsi que son développement suscitent, depuis l'apparition du concept, de nombreuses interprétations, fantasmes ou inquiétudes s'exprimant tant dans les récits ou films de science-fiction que dans les essais philosophiques. Si des outils relevant d'intelligences artificielles spécialisées ont fait leurs preuves, la réalité semble encore tenir l'intelligence artificielle généraliste loin des performances du vivant ; ainsi, l'IA reste encore bien inférieure au chat dans toutes ses aptitudes naturelles.Le terme « intelligence artificielle », créé par John McCarthy, est souvent abrégé par le sigle « IA » (ou « AI » en anglais, pour Artificial Intelligence). Il est défini par l’un de ses créateurs, Marvin Lee Minsky, comme « la construction de programmes informatiques qui s’adonnent à des tâches qui sont, pour l’instant, accomplies de façon plus satisfaisante par des êtres humains car elles demandent des processus mentaux de haut niveau tels que : l’apprentissage perceptuel, l’organisation de la mémoire et le raisonnement critique »,. On y trouve donc le côté « artificiel » atteint par l'usage des ordinateurs ou de processus électroniques élaborés et le côté « intelligence » associé à son but d'imiter le comportement. Cette imitation peut se faire dans le raisonnement, par exemple dans les jeux ou la pratique des mathématiques, dans la compréhension des langues naturelles, dans la perception : visuelle (interprétation des images et des scènes), auditive (compréhension du langage parlé) ou par d'autres capteurs, dans la commande d'un robot dans un milieu inconnu ou hostile.Même si elles respectent globalement la définition de Minsky, certaines définitions de l'IA varient sur deux points fondamentaux :les définitions qui lient l'IA à un aspect humain de l'intelligence, et celles qui la lient à un modèle idéal d'intelligence, non forcément humaine, nommée rationalité ;les définitions qui insistent sur le fait que l'IA a pour but d'avoir toutes les apparences de l'intelligence (humaine ou rationnelle), et celles qui insistent sur le fait que le fonctionnement interne du système d'IA doit ressembler également à celui de l'être humain et être au moins aussi rationnel.Historiquement, l'idée d'intelligence artificielle semble émerger dans les années 1950 quand Alan Turing se demande si une machine peut « penser ». Dans l'article « Computing Machinery and Intelligence » (Mind, octobre 1950), Turing explore ce problème et propose une expérience (maintenant dite test de Turing) visant à trouver à partir de quand une machine deviendrait « consciente ». Il développe ensuite cette idée dans plusieurs forums, dans la conférence « L'intelligence de la machine, une idée hérétique », dans la conférence qu'il donne à la BBC 3e programme le 15 mai 1951 « Les calculateurs numériques peuvent-ils penser ? » ou la discussion avec M.H.A. Newman, Sir Geoffrey Jefferson et R.B. Braithwaite les 14 et 23 janvier 1952 sur le thème « Les ordinateurs peuvent-ils penser ? ».Une autre origine probable est la publication, en 1949, par Warren Weaver d'un mémorandum sur la traduction automatique des langues qui suggère qu'une machine puisse faire une tâche qui relève typiquement de l'intelligence humaine.Le développement des techniques informatiques (augmentation de la puissance de calcul) aboutit ensuite à plusieurs avancées :dans les années 1980, l'apprentissage automatique se développe, notamment avec la renaissance du connexionnisme. L'ordinateur commence à déduire des « règles à suivre » en analysant seulement des données, ;parallèlement, des algorithmes « apprenants » sont créés qui préfigurent les futurs réseaux de neurones (l'apprentissage par renforcement, les machines à vecteurs de support, etc.). Ceci permet par exemple en mai 1997 à l’ordinateur Deep Blue de battre Garry Kasparov au jeu d'échecs lors d'un match revanche de six parties ;l'intelligence artificielle devient un domaine de recherche international, marquée par une conférence au Dartmouth College à l’été 1956, à laquelle assistaient ceux qui vont marquer la discipline ;depuis les années 1960, la recherche se fait principalement aux États-Unis, notamment à l'université Stanford sous l'impulsion de John McCarthy, au MIT sous celle de Marvin Minsky, à l'université Carnegie-Mellon sous celle de Allen Newell et Herbert Simon et à l'université d'Édimbourg sous celle de Donald Michie, en Europe et en Chine, ainsi qu'au Japon avec le projet « ordinateurs de cinquième génération (en) » du gouvernement. En France, l'un des pionniers est Jacques Pitrat ;dans les années 2000, le Web 2.0, le big data et de nouvelles puissances et infrastructures de calcul permettent à certains ordinateurs d'explorer des masses de données sans précédent ; c'est l'apprentissage profond (« deep learning »), dont l'un des pionniers est le français Yann Le Cun.Les bornes de ce domaine varient, ainsi optimiser un itinéraire était considéré comme un problème d'intelligence artificielle dans les années 1950 et n'est plus considéré aujourd’hui que comme un simple problème d'algorithmie.Vers 2015, le secteur de l'intelligence artificielle cherche à relever quatre défis : la perception visuelle, la compréhension du langage naturel écrit ou parlé, l'analyse automatique du langage et la prise de décision autonome. Produire et organiser des données nombreuses et de qualité, c'est-à-dire corrélées, complètes, qualifiées (sourcées, datées, géoréférencées…), historisées est un autre enjeu. La capacité déductive et de généralisation pertinente d'un ordinateur, à partir de peu de données ou d'un faible nombre d'évènements, est un autre objectif, plus lointain.Entre 2010 et 2016, les investissements auraient été décuplés, atteignant une dizaine de milliards de dollars en 2016.Si les progrès de l’intelligence artificielle sont récents, ce thème de réflexion est tout à fait ancien, et il apparaît régulièrement au cours de l’histoire. Les premiers signes d’intérêt pour une intelligence artificielle et les principaux précurseurs de cette discipline sont les suivants. Automates Une des plus anciennes traces du thème de « l’homme dans la machine » date de 800 avant notre ère, en Égypte. La statue du dieu Amon levait le bras pour désigner le nouveau pharaon parmi les prétendants qui défilaient devant lui, puis elle « prononçait » un discours de consécration. Les Égyptiens étaient probablement conscients de la présence d’un prêtre actionnant un mécanisme et déclarant les paroles sacrées derrière la statue, mais cela ne semblait pas être pour eux contradictoire avec l’incarnation de la divinité. Vers la même époque, Homère, dans L'Iliade (XVIII, 370–421), décrit les automates réalisés par le dieu forgeron Héphaïstos : des trépieds munis de roues en or, capables de porter des objets jusqu’à l’Olympe et de revenir seuls dans la demeure du dieu ; ou encore, deux servantes forgées en or qui l’assistent dans sa tâche. De même, le Géant de bronze Talos, gardien des rivages de la Crète, était parfois considéré comme une œuvre du dieu.Vitruve, architecte romain, décrit l’existence entre le IIIe et le Ier siècle avant notre ère, d’une école d’ingénieurs fondée par Ctesibius à Alexandrie, et concevant des mécanismes destinés à l’amusement tels des corbeaux qui chantaient. Héron L'Ancien décrit dans son traité « Automates », un carrousel animé grâce à la vapeur et considéré comme anticipant les machines à vapeur. Les automates disparaissent ensuite jusqu’à la fin du Moyen Âge. On a prêté à Roger Bacon la conception d'automates doués de la parole; en fait, probablement de mécanismes simulant la prononciation de certains mots simples.Léonard de Vinci a construit en 1515 un automate en forme de lion pour amuser le roi de France, François I. Gio Battista Aleotti et Salomon de Caus, eux, ont construit des oiseaux artificiels et chantants, des flûtistes mécaniques, des nymphes, des dragons et des satyres animés pour égayer des fêtes aristocratiques, des jardins et des grottes. René Descartes, lui, aurait conçu en 1649 un automate qu’il appelait « ma fille Francine ». Il conduit par ailleurs une réflexion d’un modernisme étonnant sur les différences entre la nature des automates, et celles d’une part des animaux (pas de différence) et d’autre part celle des hommes (pas d’assimilation). Ces analyses en font le précurseur méconnu d’un des principaux thèmes de la science-fiction : l'indistinction entre le vivant et l’artificiel, entre les hommes et les robots, les androïdes ou les intelligences artificielles.Jacques de Vaucanson a construit en 1738 un « canard artificiel de cuivre doré, qui boit, mange, cancane, barbote et digère comme un vrai canard ». Il était possible de programmer les mouvements de cet automate, grâce à des pignons placés sur un cylindre gravé, qui contrôlaient des baguettes traversant les pattes du canard. L’automate a été exposé pendant plusieurs années en France, en Italie et en Angleterre, et la transparence de l’abdomen permettait d’observer le mécanisme interne. Le dispositif permettant de simuler la digestion et d’expulser une sorte de bouillie verte fait l’objet d’une controverse. Certains commentateurs estiment que cette bouillie verte n’était pas fabriquée à partir des aliments ingérés, mais préparée à l’avance. D’autres estiment que cet avis n’est fondé que sur des imitations du canard de Vaucanson. L’incendie du musée de Nijni Novgorod en Russie, vers 1879, a détruit cet automate.Les artisans Pierre et Louis Jaquet-Droz fabriquèrent parmi les meilleurs automates fondés sur un système purement mécanique, avant le développement des dispositifs électromécaniques. Certains de ces automates, par un système de cames multiples, étaient capables d'écrire un petit billet (toujours le même). Enfin, Les Contes d'Hoffmann (et ballet) L'Homme au sable décrit une poupée mécanique dont s'éprend le héros. Pensée automatique Une des premières tentatives de formalisation de la pensée connue est le zairja, mécanisme qu'utilisaient les astrologues arabe pour générer des idées supposées logiques, dont l'invention est attribuée à Abu al-Abbas as-Sabti au XIIe siècle. Raymond Lulle s'en est probablement inspiré pour mettre au point son Ars Magna. Missionnaire, philosophe, et théologien espagnol du XIIIe siècle, il essaya lui aussi de générer des idées grâce à un système mécanique. Il combinait aléatoirement des concepts grâce à une sorte de règle à calcul, sur laquelle pivotaient des disques concentriques gravés de lettres et de symboles philosophiques. Il fondait sa méthode sur l’identification de concepts de base, puis leur combinaison mécanique soit entre eux, soit avec des idées connexes. Raymond Lulle l'appliqua à la métaphysique, puis à la morale, à la médecine et à l’astrologie. Mais il n’utilisait que la logique déductive, ce qui ne permettait pas à son système d’acquérir un apprentissage, ni davantage de remettre en cause ses principes de départ : seule la logique inductive le permet.Gottfried Wilhelm Leibniz, au XVIIe siècle, a imaginé un calcul pensant (calculus rationator), en assignant un nombre à chaque concept. La manipulation de ces nombres aurait permis de résoudre les questions les plus difficiles, et même d’aboutir à un langage universel. Leibniz a toutefois démontré que l’une des principales difficultés de cette méthode, également rencontrée dans les travaux modernes sur l’intelligence artificielle, est l’interconnexion de tous les concepts, ce qui ne permet pas d’isoler une idée de toutes les autres pour simplifier les problèmes liés à la pensée.George Boole a inventé la formulation mathématique des processus fondamentaux du raisonnement, connue sous le nom d’algèbre de Boole. Il était conscient des liens de ses travaux avec les mécanismes de l’intelligence, comme le montre le titre de son principal ouvrage paru en 1854 : Les Lois de la pensée (The laws of thought), sur l’algèbre booléenne.Gottlob Frege perfectionna le système de Boole en formalisant le concept de prédicat, qui est une entité logique soit vraie, soit fausse (toute maison a un propriétaire), mais contenant des variables non logiques, n’ayant en soi aucun degré de vérité (maison, propriétaire). Cette formalisation eut une grande importance puisqu'elle permit de démontrer des théorèmes généraux, simplement en appliquant des règles typographiques à des ensembles de symboles. La réflexion en langage courant ne portait plus que sur le choix des règles à appliquer. Par ailleurs, l’utilisateur joue un rôle important puisqu'il connaît le sens des symboles qu’il a inventés et ce sens n'est pas toujours formalisé, ce qui ramène au problème de la signification en intelligence artificielle, et de la subjectivité des utilisateurs.Bertrand Russell et Alfred North Whitehead publièrent au début du XXe siècle un ouvrage intitulé Principia Mathematica, dans lequel ils résolvent des contradictions internes à la théorie de Gottlob Frege. Ces travaux laissaient espérer d’aboutir à une formalisation complète des mathématiques.Kurt Gödel démontre au contraire que les mathématiques resteront une construction ouverte, en publiant en 1931 un article intitulé « Des propositions formellement indécidables contenues dans les Principia mathematica et autres systèmes similaires ». Sa démonstration est qu’à partir d’une certaine complexité d’un système, on peut y créer plus de propositions logiques qu’on ne peut en démontrer vraies ou fausses. L’arithmétique, par exemple, ne peut trancher par ses axiomes si on doit accepter des nombres dont le carré soit -1. Ce choix reste arbitraire et n’est en rien lié aux axiomes de base. Le travail de Gödel suggère qu’on pourra créer ainsi un nombre arbitraire de nouveaux axiomes, compatibles avec les précédents, au fur et à mesure qu’on en aura besoin. Si l'arithmétique est démontrée incomplète, le calcul des prédicats (logique formelle) est au contraire démontré par Gödel comme complet.Alan Turing invente des machines abstraites et universelles (rebaptisées les machines de Turing), dont les ordinateurs modernes sont considérés comme des concrétisations. Il démontre l’existence de calculs qu’aucune machine ne peut faire (un humain pas davantage, dans les cas qu'il cite), sans pour autant que cela constitue pour Turing un motif pour douter de la faisabilité de machines pensantes répondant aux critères du test de Turing.Irving John Good, Myron Tribus et E.T. Jaynes ont décrit de façon très claire les principes assez simples d’un robot à logique inductive utilisant les principes de l’inférence bayésienne pour enrichir sa base de connaissances sur la base du Théorème de Cox-Jaynes. Ils n’ont malheureusement pas traité la question de la façon dont on pourrait stocker ces connaissances sans que le mode de stockage entraîne un biais cognitif. Le projet est voisin de celui de Raymond Lulle, mais fondé cette fois-ci sur une logique inductive, et donc propre à résoudre quelques problèmes ouverts.Des chercheurs comme Alonzo Church ont posé des limites pratiques aux ambitions de la raison, en orientant la recherche (Herbert Simon, Michael Rabin, Stephen Cook) vers l’obtention des solutions en temps fini, ou avec des ressources limitées, ainsi que vers la catégorisation des problèmes selon des classes de difficulté (en rapport avec les travaux de Cantor sur l’infini)[réf. souhaitée].L'intelligence artificielle est un sujet d'actualité au XXIe siècle. En 2004, l'Institut Singularity a lancé une campagne Internet appelée « Trois lois dangereuses » : « Three Laws Unsafe » (en lien avec les trois lois d'Asimov) pour sensibiliser aux questions de la problématique de l'intelligence artificielle et l'insuffisance des lois d'Asimov en particulier. (Singularity Institute for Artificial Intelligence 2004).En 2005, le projet Blue Brain est lancé, il vise à simuler le cerveau des mammifères. Il s'agit d'une des méthodes envisagées pour réaliser une IA. Ils annoncent de plus comme objectif de fabriquer, dans dix ans, le premier « vrai » cerveau électronique. En mars 2007, le gouvernement sud-coréen annonce que plus tard dans l'année, il émettrait une charte sur l'éthique des robots, afin de fixer des normes pour les utilisateurs et les fabricants. Selon Park Hye-Young, du ministère de l'Information et de la communication, la Charte reflète les trois lois d'Asimov : la tentative de définition des règles de base pour le développement futur de la robotique. En juillet 2009, en Californie une conférence organisée par l'Association for the Advancement of Artificial Intelligence (AAAI), où un groupe d'informaticiens se demande s'il devrait y avoir des limites sur la recherche qui pourrait conduire à la perte de l'emprise humaine sur les systèmes informatiques, et où il est également question de l'explosion de l'intelligence (artificielle) et du danger de la singularité technologique conduisant à un changement d'ère, ou de paradigme totalement en dehors du contrôle humain,.En 2009, le Massachusetts Institute of Technology (MIT) a lancé un projet visant à repenser la recherche en intelligence artificielle. Il réunira des scientifiques qui ont eu du succès dans des domaines distincts de l'IA. Neil Gershenfeld déclare « Nous voulons essentiellement revenir 30 ans en arrière, et de revoir quelques directions aujourd'hui gelées ».En novembre 2009, l'US Air Force cherche à acquérir 2 200 PlayStation 3[réf. obsolète] pour utiliser le processeur cell à 7 ou 8 cœurs qu'elle contient dans le but d'augmenter les capacités de leur superordinateur constitué de 336 PlayStation 3 (total théorique 52,8 petaFLOPS en double précision). Le nombre sera réduit à 1 700 unités le 22 décembre 2009. Le projet vise le traitement vidéo haute-définition, et l'« informatique neuromorphique », ou la création de calculateurs avec des propriétés/fonctions similaires au cerveau humain. Années 2010 Le 27 janvier 2010, l'US Air Force demande l'aide de l'industrie pour développer une intelligence avancée de collecte d'information et avec la capacité de décision rapide pour aider les forces américaines pour attaquer ses ennemis rapidement à leurs points les plus vulnérables. L'US Air Force utilisera une intelligence artificielle, le raisonnement ontologique, et les procédures informatique basées sur la connaissance, ainsi que d'autres traitements de données avancés afin de frapper l'ennemi au meilleur point. D'autre part, d’ici 2020, plus de mille bombardiers et chasseurs F-22 et F-35 de dernière génération, parmi plus de 2 500 avions militaires, commenceront à être équipés de sorte que, d’ici 2040, tous les avions de guerre américains soient pilotés par intelligence artificielle, en plus des 10 000 véhicules terrestres et des 7 000 dispositifs aériens commandés d'ores et déjà à distance.Le 16 février 2011, Watson, le superordinateur conçu par IBM, remporte deux des trois manches du jeu télévisé Jeopardy! en battant largement ses deux concurrents humains en gains cumulés. Pour cette IA, la performance a résidé dans le fait de répondre à des questions de culture générale (et non un domaine technique précis) dans des délais très courts. En février 2016, l'artiste et designer Aaron Siegel propose de faire de Watson un candidat à l'élection présidentielle américaine afin de lancer le débat sur « le potentiel de l’intelligence artificielle dans la politique ».En mai 2013, Google ouvre un laboratoire de recherches dans les locaux de la NASA. Grâce à un super calculateur quantique conçu par D-Wave Systems et qui serait d'après cette société 11 000 fois plus performant qu'un ordinateur actuel (de 2013), ils espèrent ainsi faire progresser l'intelligence artificielle, notamment l'apprentissage automatique. Raymond Kurzweil est engagé en décembre 2012 par Google afin de participer et d'améliorer l'apprentissage automatique des machines et des IA.Entre 2014 et 2015, à la suite du développement rapide du deep learning, et à l'encontre des penseurs transhumanistes, quelques scientifiques et membres de la communauté high tech craignent que l'intelligence artificielle ne vienne à terme dépasser les performances de l'intelligence humaine. Parmi eux, l'astrophysicien britannique Stephen Hawking, le fondateur de Microsoft Bill Gates et le PDG de Tesla Elon Musk.Les géants de l'Internet s'intéressent de plus en plus à l'IA. Le 3 janvier 2016, le patron de Facebook, Mark Zuckerberg, s’est donné pour objectif de l’année de « construire une intelligence artificielle simple pour piloter ma maison ou m’aider dans mon travail ». Il avait déjà créé en 2013 le laboratoire Facebook Artifical Intelligence Research (FAIR) dirigé par le chercheur français Yann Le Cun et ouvert un laboratoire de recherche permanente dans le domaine à Paris.Apple a de son côté récemment acquis plusieurs start-up du secteur (Perceptio, VocalIQ, Emotient et Turi).En janvier 2018, des modèles d'intelligence artificielle développés par Microsoft et Alibaba réussissent chacun de leur côté à battre les humains dans un test de lecture et de compréhension de l'université Stanford. Le traitement du langage naturel imite la compréhension humaine des mots et des phrases et permet maintenant aux modèles d'apprentissage automatique de traiter de grandes quantités d'informations avant de fournir des réponses précises aux questions qui leur sont posées.En février 2019, l'institut de recherche OpenAI annonce avoir créé un programme d’intelligence artificielle capable de générer des textes tellement réalistes que cette technologie pourrait être dangereuse,. Si le logiciel est utilisé avec une intention malveillante, il peut générer facilement des fausses nouvelles très crédibles. Inquiet par l'utilisation qui pourrait en être faite, OpenAI préfère ne pas rendre public le code source du programme.En France, les pionniers sont Alain Colmerauer, Gérard Huet, Jean-Louis Laurière, Claude-François Picard, Jacques Pitrat et Jean-Claude Simon. Un congrès national annuel, « Reconnaissance de formes et intelligence artificielle », est créé en 1979 à Toulouse. En lien avec l'organisation de la conférence International Joint Conference on Artificial Intelligence à Chambéry en 1993, et la création d'un GRECO-PRC « intelligence artificielle », en 1983, il donne naissance à une société savante, l'Association française pour l'intelligence artificielle (AFIA) en 1989, qui, entre autres, organise des conférences nationales en intelligence artificielle.Le 17 janvier 2017, le fonds de capital risque Serena Capital lance un fonds de 80 millions d’euros destiné à l’investissement dans les start-ups européennes du big data et de l'intelligence artificielle. Le 19 janvier 2017, une audition se tient au Sénat : « L'intelligence Artificielle menace-t-elle nos emplois ? ». Le 20 janvier 2017, Axelle Lemaire entend valoriser les potentiels scientifiques et industriels français grâce au projet « France IA ».En janvier 2017, dans le cadre de sa mission de réflexion sur les enjeux éthiques et les questions de société soulevés par les technologies numériques, la Commission nationale de l'informatique et des libertés (CNIL) annonce l'organisation d'un débat public sur les algorithmes et l'intelligence artificielle. Le 15 décembre 2017, à l'issue d'un débat ayant mobilisé 60 partenaires (institutions publiques, associations, entreprises, acteurs du monde de la recherche, société civile), elle publie son rapport « Comment permettre à l'Homme de garder la main ? » comprenant des recommandations pour la construction d'un modèle éthique d'intelligence artificielle.En septembre 2017, Cédric Villani, premier vice-président de l'Office parlementaire d'évaluation des choix scientifiques et technologiques (OPECST), est chargé de mener une consultation publique sur l'intelligence artificielle. Il rend son rapport le 28 mars 2018, à la veille d'une intervention du président de la République Emmanuel Macron au Collège de France pour annoncer la stratégie de la France dans ce domaine. Il y dévoile un plan de 1,5 milliard d'euros sur l'ensemble du quinquennat, ainsi qu'une évolution de la législation française pour permettre la mise en application de l'intelligence artificielle, en particulier concernant la circulation des véhicules autonomes. Parallèlement à ces annonces, il est interviewé par Wired, magazine de référence pour la communauté mondiale des nouvelles technologies, et y exprime sa vision de l'intelligence artificielle, à savoir que les algorithmes utilisés par l'État doivent être ouverts, que l'intelligence artificielle doit être encadrée par des règles philosophiques et éthiques et qu'il faut s'opposer à l'usage d'armes automatiques ou de dispositifs prenant des décisions sans consulter un humain,.En mars 2018, Microsoft France lance l'École IA Microsoft, inaugurée par son président Carlo Purassanta, une formation ouverte aux décrocheurs scolaires et aux personnes éloignées de l'emploi, en partenariat avec Simplon.co. Dix écoles sont lancées en un an à partir de septembre 2018. Microsoft France mise sur le développement de l'intelligence artificielle comme nouveau vecteur d'inclusion professionnelleEn octobre 2019, le site ActuIA annonce le lancement du premier magazine papier consacré à l'intelligence artificielle.Le concept d’intelligence artificielle forte fait référence à une machine capable non seulement de produire un comportement intelligent, notamment de modéliser des idées abstraites, mais aussi d’éprouver une impression d'une réelle conscience, de « vrais sentiments » (quoi qu’on puisse mettre derrière ces mots), et « une compréhension de ses propres raisonnements ».L’intelligence artificielle forte a servi de moteur à la discipline, mais a également suscité de nombreux débats [Lesquels ?].En partant du principe, étayé par les neurosciences, que la conscience a un support biologique et donc matériel, les scientifiques ne voient généralement pas d’obstacle théorique à la création d'une intelligence consciente sur un support matériel autre que biologique. Selon les tenants de l'IA forte, si à l'heure actuelle il n'y a pas d'ordinateurs ou d'algorithmes aussi intelligents que l'être humain, ce n'est pas un problème d'outil mais de conception. Il n'y aurait aucune limite fonctionnelle (un ordinateur est une machine de Turing universelle avec pour seules limites celles de la calculabilité), seulement des limites liées à l'aptitude humaine à concevoir les logiciels appropriés (programme, base de données…).Comparer la capacité de traitement de l'information d'un cerveau humain à celle d'un ordinateur peut aider à comprendre les ordres de grandeur pour estimer la possibilité pratique ou non d'une intelligence artificielle forte, de même qu'un simple calcul de puissance en kW permet grosso modo de dire qu'un camion donné pourra espérer transporter commodément telle ou telle charge ou si cela lui sera impossible. Voici quelques exemples d'ordres de grandeur en traitement de l'information :Balance Roberval : 1 bit par seconde (comparaison de deux poids) ;mainframe typique des années 1970 : 1 million d'opérations par seconde sur 32 bits ;Intel Paragon XP/S, 4 000 processeurs i860 à 50 MHz (1992) : 160 milliards d'opérations par seconde ;Summit, 9 216 processeurs POWER9 (2018) : 200 pétaflops, soit 200 millions de milliards d'opérations par seconde.Fugaku 415-PFLOPS (2020-2021): 415 pétaflops, soit 415 millions de milliards d'opérations par seconde.Cette puissance n'est pas à prendre au pied de la lettre. Elle précise surtout les ordres de grandeur en présence et leur évolution relativement rapide (jusqu'en 2018).L'intelligence artificielle n'avait donné que des résultats mitigés sur les ordinateurs typiques de 1970 effectuant 107 opérations logiques par seconde,. Le cerveau humain, formé de 1011 neurones ne pouvant chacun commuter plus de 100 fois par seconde en raison de leur temps de relaxation permettait beaucoup plus de traitements logiques par unité de temps (1013 opérations logiques par seconde). Ce handicap technique précis n'existe plus sur les ordinateurs depuis les années 2000, travaillant en 64 bits et avec des horloges cadencées à 4 GHz environ, pour des processeurs destinés aux particuliers. Concernant des supercalculateurs comme Summit ou Fugaku 415-PFLOPS, le rapport du nombre de comparaisons par seconde entre ordinateur et cerveau a même complètement changé de sens.Le matériel serait donc maintenant disponible, toutefois l'IA souligne la difficulté à expliciter toutes les connaissances utiles à la résolution d'un problème complexe. Certaines connaissances dites implicites sont acquises par l'expérience et mal formalisables. L'apprentissage de ces connaissances implicites par l'expérience est exploitée depuis les années 1980 (voir Réseau de neurones). Néanmoins, un autre type de complexité apparaît : la complexité structurelle. Comment mettre en relation des modules spécialisés pour traiter un certain type d'informations, par exemple un système de reconnaissance des formes visuelles, un système de reconnaissance de la parole, un système lié à la motivation, à la coordination motrice, au langage, etc. En revanche, une fois un système cognitif conçu et son apprentissage par l'expérience réalisé, l'« intelligence » correspondante peut être distribuée en un grand nombre d'exemplaires, par exemple sur les portables d'actuaires ou de banquiers pouvant ainsi, comme le rappelle un slogan, dire oui ou non, mais le dire tout de suite grâce à des applications dites de credit scoring.Les principales opinions soutenues pour répondre à la question d’une intelligence artificielle forte (c'est-à-dire douée d'une sorte de conscience) sont les suivantes :impossible : la conscience serait le propre des organismes vivants (supérieurs), et elle serait liée à la nature des systèmes biologiques. Cette position est défendue par certains philosophes et sociologues comme Harry Collins, pour qui l'intelligence requiert une immersion dans la société humaine, et donc un corps humain, et peut rappeler le courant du vitalisme.impossible avec des machines manipulant des symboles comme les ordinateurs actuels, mais possible avec des systèmes dont l’organisation matérielle serait fondée sur des processus quantiques. Des algorithmes quantiques sont théoriquement capables de mener à bien des calculs hors de l'atteinte pratique des calculateurs conventionnels (complexité en                               N                      3                                {\displaystyle N^{3}}   au lieu de                               2                      N                                {\displaystyle 2^{N}}  , par exemple, sous réserve d'existence du calculateur approprié). Au-delà de la rapidité, certains scientifiques comme Roger Penrose défendent que la conscience nécessiterait un fonctionnement non compatible avec les lois de la physique classique, et accessible uniquement avec des systèmes quantiques. Toutefois, l'état de la recherche en informatique quantique n'est pas encore suffisamment avancé pour permettre de l'utiliser dans des applications concrètes hors laboratoires, rendant difficile la vérification de ces hypothèses.impossible car la pensée n'est pas un phénomène calculable par des processus discrets et finis. Cette théorie est notamment avancée par le philosophe John Searle et son expérience de la chambre chinoise. Une conscience est donc nécessaire pour accéder à l'intelligence, mais un système informatique ne serait capable que d'en simuler une, sans pour autant la posséder, renvoyant au concept philosophique du zombie.possible avec des ordinateurs manipulant des symboles. La notion de symbole est toutefois à prendre au sens large. Cette option inclut les travaux sur le raisonnement ou l'apprentissage symbolique basé sur la logique des prédicats, mais aussi les techniques connexionnistes telles que les réseaux de neurones, qui, à "
Informatique;"L'interaction homme-machine, appelé IHM, s’intéresse à la conception et au développement de systèmes interactifs en prenant en compte ses impacts sociétaux et éthiques. Les humains interagissent avec les ordinateurs qui les entourent et cette interaction nécessite des interfaces qui facilitent la communication entre l'humain et la machine. La facilitation de l'utilisation de dispositifs devient de plus en plus importante avec le nombre croissant d'interfaces numériques dans la vie quotidienne. L'IHM a pour but de trouver les moyens les plus efficaces, les plus accessibles et les plus intuitifs pour les utilisateurs de compléter une tâche le plus rapidement et le plus précisément possible. L'IHM, s'appuie notamment sur la linguistique, sur la vision par ordinateur et sur l'humain.L'interaction homme-machine est un domaine pluridisciplinaire entre ingénierie (informatique, électronique, mécanique…), science de la nature (sciences cognitives, psychologie, sociologie…) et art et design (design de produit, design interactif, ergonomie…).L'histoire de l'interaction homme-machine est aussi vieille que l'histoire de l'informatique. En 1945, Vannevar Bush décrit un système électronique imaginaire qui permet la recherche d'information et qui invente les concepts de navigation, d'indexation et d'annotation. En 1963, Ivan Sutherland a créé Sketchpad qui est considéré comme l’ancêtre des interfaces graphiques modernes. En 1964, Douglas Engelbart invente la souris pour facilement désigner des objets sur son écran. Dans les années 1970 et 80, les laboratoires de Xerox ont révolutionné les systèmes interactifs avec la sortie de Xerox Star et la présentation de What you see is what you get. Au début des années 1990, Robert Cailliau et Tim Berners-Lee inventent un système hypertexte qui entourera la planète, World Wide Web. En 1991, Mark Weiser présente sa vision de l'Informatique ubiquitaire qui envisage des écrans et des ordinateurs multiples capables de communiquer entre eux pour permettre l'utilisateur à accéder à l'information en toute circonstance. Cette vision préfigure clairement l'avènement des assistants personnels, Tablet PC et smartphones d'aujourd'hui.Il existe de nombreuses manières pour qu'un humain puisse interagir avec les machines qui l'entourent. Ces manières sont très dépendantes des dispositifs d'interactions et des forces ou compétences que l'être humain ne peut étendre qu’extérieurement.L'informatique a évolué très rapidement de ses débuts dans les années 1940 à aujourd'hui. Organes d'entrée Les premiers ordinateurs étaient utilisés sous forme de traitement par lots et toutes les entrées (programmes et données) étaient alimentées en entrée par des cartes perforées, des rubans perforés ou des bandes magnétiques. Il y avait un clavier pour interagir avec le système (console système).Avec l'arrivée de la micro-informatique, on a commencé à utiliser des cassettes audio et des claviers, puis des disquettes et des souris informatiques avant de passer aux écrans tactiles. Un système de pointage tel que la souris permet d'utiliser un ordinateur avec le paradigme WIMP qui s'appuie sur les interfaces graphiques pour organiser la présentation d'informations à l'utilisateur.Enfin, avec les assistants personnels intelligents, la voix devient un organe d'entrée intéressant en raison du taux potentiel de mots par minute qu'elle permet. Organes de sortie Les premiers organes de sorties ont été les imprimantes, les perforateurs de cartes et les perforateurs de ruban secondés ensuite par bandes magnétiques. La console système était équipée d'une imprimante, remplacée par la suite par un écran.Avec l'arrivée de la micro-informatique, on a utilisé d'abord des cassettes audio, puis des disquettes avant d'utiliser des CD puis des DVD. Organes interactifs Certaines techniques tentent de rendre l'interaction plus naturelle :la reconnaissance automatique de la parole ou de gestes permet d'envoyer des informations à un ordinateur ;la synthèse vocale permet d'envoyer un signal audio compréhensible par l'être humain ;les gants électroniques offrent une interaction plus directe que la souris ;les visiocasques essayent d'immerger l'être humain dans une réalité virtuelle ou d'augmenter la réalité ;les tables interactives permettent un couplage fort entre la manipulation directe par l'être humain sur une surface et le retour d'information.Dans le domaine de l'automatisation, les écrans tactiles sont des IHMs très populaires afin de centraliser le contrôle d'un procédé sur un seul écran. Ainsi, il est possible d'afficher plusieurs informations et de mettre à la disposition de l'opérateur des commandes qui affecteront le procédé. Les IHMs permettent aussi de remplacer des stations de boutons. Ils sont surtout utilisés en complément avec un API (automate programmable industriel) pour avoir un affichage des états des entrées/sorties et des alarmes du système.En informatique industrielle, les automates sont encore très souvent pilotés par des baies équipées de boutons poussoirs et de voyants. Les systèmes autonomes de type véhicules automatiques et drones tendent à peu à peu à intégrer une « interface adaptative », voire une intelligence artificielle embarquée.Dans l'automobile, l'être humain a, d'abord, interagi avec de simples moyens mécaniques. L'évolution de l'informatique et de la robotique fait que de plus en plus de capteurs et d'informations sont disponibles pour le conducteur qui doit choisir l'action à effectuer par l'intermédiaire :du volant ;de la pédale de frein ;d'interrupteurs divers (éclairage, régulateur de vitesse, etc.).On peut observer que les IHM sont de plus en plus déconnectées de l'implémentation réelle des mécanismes contrôlés. Dans son article de 1995, The Myth of Metaphor, Alan Cooper distingue trois grands paradigmes d'interface :Le paradigme technologique : l'interface reflète la manière dont le mécanisme contrôlé est construit. Cela conduit à des outils très puissants mais destinés à des spécialistes qui savent comment fonctionne la machine à piloter.Le paradigme de la métaphore qui permet de mimer le comportement de l'interface sur celui d'un objet de la vie courante et donc déjà maîtrisé par l'utilisateur. Exemple : la notion de document.Le paradigme idiomatique qui utilise des éléments d'interface au comportement stéréotypé, cohérent et donc simple à apprendre mais pas nécessairement calqué sur des objets de la vie réelle.L'interaction est dite multimodale si elle met en jeu plusieurs modalités sensorielles et motrices. Un système interactif peut contenir un ou plusieurs de ces modes d'interaction :Mode parlé : commandes vocales, guides vocaux…Mode écrit : entrées par le clavier et la tablette graphique, affichage du texte sur l'écran…Mode gestuel : désignation 2D ou 3D (souris, gants de données, écran tactile), retour d'effort…Mode visuel : graphiques, images, animations…D'un point de vue organique, on peut distinguer trois types d'IHM :Les interfaces d'acquisition : bouton, molette, souris, clavier accord, joystick, clavier d'ordinateur, clavier MIDI, télécommande, capteur de mouvement, microphone avec la reconnaissance vocale, etc.Les interfaces de restitution : écran, témoin à LED, voyant d'état du système, haut parleur, etc.Les interfaces combinées : écran tactile, multi-touch et les commandes à retour d'effort.Ce domaine évolue vers une interface plus large et pervasive de type « humain-environnement ».« Il serait sot de nier l'importance de la communication efficace entre l'homme et la machine, aussi bien que l'inverse. Ma prévision est toutefois que la vraie révolution des prochaines décennies viendra davantage encore de ce que les hommes ont à se dire par l'intermédiaire des machines. »— James Cannavino, The Next Generation of Interactive Technologies (Juillet 1989)L'immersion dans les mondes virtuels devrait également être rendue plus « réaliste ».Des jeux comme Le Deuxième Monde, Everquest ou Wolfenstein: Enemy Territory, où plusieurs joueurs évoluent en immersion globale dans un paysage commun, donnent une idée des nouvelles relations que peuvent mettre en place des interfaces réalistes.La plus grande association d'IHM est le pôle d'intérêt commun SIGCHI de l'Association for Computing Machinery (ACM). SIGCHI organise les conférences Conference on Human Factors in Computing Systems (CHI), MobileHCI, TEI et plusieurs autres.En France, l'association francophone d'Interaction humain-machine (AFIHM) organise la Conférence francophone IHM tous les ans. L'AFIHM parraine diverses manifestations et en particulier des Écoles d'été et les Rencontres Jeunes Chercheurs en Interaction (RJC-IHM). Portail de l’informatique"
Informatique;"La logique mathématique ou métamathématique est une discipline des mathématiques introduite à la fin du XIXe siècle, qui s'est donné comme objet l'étude des mathématiques en tant que langage. Les objets fondamentaux de la logique mathématique sont les formules représentant les énoncés mathématiques, les dérivations ou démonstrations formelles représentant les raisonnements mathématiques et les sémantiques ou modèles ou interprétations dans des structures qui donnent un « sens » mathématique générique aux formules (et parfois même aux démonstrations) comme certains invariants : par exemple l'interprétation des formules du calcul des prédicats permet de leur affecter une valeur de vérité.La logique mathématique est née à la fin du XIXe siècle de la logique au sens philosophique du terme ; elle est l'une des pistes explorées par les mathématiciens de cette époque afin de résoudre la crise des fondements mathématiques provoquée par la complexification des mathématiques et l'apparition des paradoxes. Ses débuts sont marqués par la rencontre entre deux idées nouvelles :la volonté chez Frege, Russell, Peano et Hilbert de donner une fondation axiomatique aux mathématiques ;la découverte par George Boole de l'existence de structures algébriques permettant de définir un « calcul de vérité ».La logique mathématique se fonde sur les premières tentatives de traitement formel des mathématiques, dues à Leibniz et Lambert (fin XVIIe siècle - début XVIIIe siècle). Leibniz a en particulier introduit une grande partie de la notation mathématique moderne (usage des quantificateurs, symbole d'intégration, etc.). Toutefois on ne peut parler de logique mathématique qu'à partir du milieu du XIXe siècle, avec les travaux de George Boole (et dans une moindre mesure ceux d'Auguste De Morgan) qui introduit un calcul de vérité où les combinaisons logiques comme la conjonction, la disjonction et l'implication, sont des opérations analogues à l'addition ou la multiplication des entiers, mais portant sur les valeurs de vérité faux et vrai (ou 0 et 1) ; ces opérations booléennes se définissent au moyen de tables de vérité.Le calcul de Boole véhiculait l'idée apparemment paradoxale, mais qui devait s'avérer spectaculairement fructueuse, que le langage mathématique pouvait se définir mathématiquement et devenir un objet d'étude pour les mathématiciens. Toutefois il ne permettait pas encore de résoudre les problèmes de fondements. Dès lors, nombre de mathématiciens ont cherché à l'étendre au cadre général du raisonnement mathématique et on a vu apparaître les systèmes logiques formalisés ; l'un des premiers est dû à Frege au tournant du XXe siècle.En 1900 au cours d'une très célèbre conférence au congrès international des mathématiciens à Paris, David Hilbert a proposé une liste des 23 problèmes non résolus les plus importants des mathématiques d'alors. Le deuxième était celui de la cohérence de l'arithmétique, c’est-à-dire de démontrer par des moyens finitistes la non-contradiction des axiomes de l'arithmétique.Le programme de Hilbert a suscité de nombreux travaux en logique dans le premier quart du siècle, notamment le développement de systèmes d'axiomes pour les mathématiques : les axiomes de Peano pour l'arithmétique, ceux de Zermelo complétés par Skolem et Fraenkel pour la théorie des ensembles et le développement par Whitehead et Russell d'un programme de formalisation des mathématiques, les Principia Mathematica. C'est également la période où apparaissent les principes fondateurs de la théorie des modèles : notion de modèle d'une théorie, théorème de Löwenheim-Skolem.En 1929 Kurt Gödel montre dans sa thèse de doctorat son théorème de complétude qui énonce le succès de l'entreprise de formalisation des mathématiques : tout raisonnement mathématique peut en principe être formalisé dans le calcul des prédicats. Ce théorème a été accueilli comme une avancée notable vers la résolution du programme de Hilbert, mais un an plus tard, Gödel démontrait le théorème d'incomplétude (publié en 1931) qui montrait irréfutablement l'impossibilité de réaliser ce programme.Ce résultat négatif n'a toutefois pas arrêté l'essor de la logique mathématique. Les années 1930 ont vu arriver une nouvelle génération de logiciens anglais et américains, notamment Alonzo Church, Alan Turing, Stephen Kleene, Haskell Curry et Emil Post, qui ont grandement contribué à la définition de la notion d'algorithme et au développement de la théorie de la complexité algorithmique (théorie de la calculabilité, théorie de la complexité des algorithmes). La théorie de la démonstration de Hilbert a également continué à se développer avec les travaux de Gerhard Gentzen qui a produit la première démonstration de cohérence relative et initié ainsi un programme de classification des théories axiomatiques.Le résultat le plus spectaculaire de l'après-guerre est dû à Paul Cohen qui démontre en utilisant la méthode du forcing l'indépendance de l'hypothèse du continu en théorie des ensembles, résolvant ainsi le 1er problème de Hilbert. Mais la logique mathématique subit également une révolution due à l'apparition de l'informatique ; la découverte de la correspondance de Curry-Howard, qui relie les preuves formelles au lambda-calcul de Church et donne un contenu calculatoire aux démonstrations, va déclencher un vaste programme de recherche.L'intérêt principal de la logique réside dans ses interactions avec d'autres domaines des mathématiques et les nouvelles méthodes qu'elle y apporte. De ce point de vue les réalisations les plus importantes viennent de la théorie des modèles qui est parfois considérée comme une branche de l'algèbre plutôt que de la logique ; la théorie des modèles s'applique notamment en théorie des groupes et en combinatoire (théorie de Ramsey).D'autres interactions très productives existent toutefois : le développement de la théorie des ensembles est intimement lié à celui de la théorie de la mesure et a donné lieu à un domaine mathématique à part entière, la théorie descriptive des ensembles.[non pertinent] La théorie de la calculabilité est l'un des fondements de l'informatique théorique.Depuis la fin du XXe siècle on a vu la théorie de la démonstration s'associer à la théorie des catégories et par ce biais commencer à interagir avec la topologie algébrique. D'autre part avec l'apparition de la logique linéaire elle entretient également des liens de plus en plus étroit avec l'algèbre linéaire, voire avec la géométrie non commutative. Plus récemment la théorie homotopique des types (en) créée une connexion riche entre la logique (la théorie des types) et les mathématiques (la théorie de l'homotopie) dont on n'entrevoit que les prémices.La formalisation des mathématiques dans des systèmes logiques, qui a suscité en particulier les travaux de Whitehead et Russell, a été l'une des grandes motivation du développement de la logique mathématique. L'apparition d'outils informatiques spécialisés, démonstrateurs automatiques, systèmes experts et assistants de preuve, a donné un nouvel intérêt à ce programme. Les assistants de preuve en particulier ont plusieurs applications en mathématique.Tout d'abord dans la fin du XXe siècle et au début du XXIe siècle deux anciennes conjectures ont été résolues en faisant appel à l'ordinateur pour traiter un très grand nombre de cas : le théorème des quatre couleurs et la conjecture de Kepler. Les doutes soulevés par cette utilisation de l'ordinateur ont motivé la formalisation et la vérification complète de ces démonstrations.D'autre part des programmes de formalisation de mathématiques utilisant les assistants de preuves se sont développés afin de produire un corpus complètement formalisé de mathématiques ; pour les mathématiques l'existence d'un tel corpus aurait en particulier l'intérêt de permettre des traitements algorithmiques comme la recherche par motif : trouver tous les théorèmes qui se déduisent du théorème des nombres premiers, trouver tous les théorèmes à propos de la fonction zeta de Riemann, etc.Quelques résultats importants ont été établis pendant la décennie 1930.Le théorème de complétude du calcul des prédicats du premier ordre que Gödel a montré dans sa thèse de doctorat, un an avant son célèbre théorème d'incomplétude. Ce théorème énonce que toute démonstration mathématique peut être représentée dans le formalisme du calcul des prédicats (qui est donc complet).L'ensemble des théorèmes du calcul des prédicats n'est pas calculable, c'est-à-dire qu'aucun algorithme ne permet de vérifier si un énoncé donné est prouvable ou non. Il existe cependant un algorithme qui étant donnée une formule du premier ordre en trouve une preuve en un temps fini s'il en existe une, mais continue indéfiniment sinon. On dit que l'ensemble des formules du premier ordre prouvables est « récursivement énumérable » ou « semi-décidable ».La cohérence (non-contradiction) d'une théorie (ensemble d'axiomes), permettant de formaliser (au moins) l'arithmétique (par exemple les axiomes de Peano) n'est pas une conséquence de ces seuls axiomes. C'est le fameux second théorème d'incomplétude de Gödel.Tout théorème purement logique peut être démontré en n'utilisant que des propositions qui sont des sous-formules de son énoncé. Connu sous le nom de propriété de la sous-formule, ce résultat est une conséquence du théorème d'élimination des coupures en calcul des séquents de Gerhard Gentzen : la propriété de la sous-formule a pour conséquence la cohérence de la logique, car elle interdit la dérivation de la formule vide (identifiée à l'absurdité).D'autres résultats importants ont été établis pendant la deuxième partie du XXe siècle. L'indépendance de l'hypothèse du continu par rapport aux autres axiomes de la théorie des ensembles (ZF) est achevée en 1963 par Paul Cohen. La théorie de la calculabilité se développe. Au tournant de la décennie 1980, la correspondance de Curry-Howard identifie la simplification des démonstrations et les programmes, créant ainsi un pont entre mathématiques et informatique. En 1990, cette correspondance est étendue à la logique classique. La mécanisation, et donc la formalisation, complète de théorème de mathématiques comme le Théorème des quatre couleurs ou le Théorème de Feit-Thompson.Au XXIe siècle, émergent de nouvelles branches prometteuses comme la Théorie des types homotopiques.Un système logique (ou une logique) est un système formel auquel on ajoute une sémantique. Un système formel se constitue :d'une syntaxe (ou langage) dont on lui attribue deux données : un ensemble de symboles servant à la construction de formules. Dans les systèmes de logique classique ou intuitionniste, les formules représentent des énoncés mathématiques exprimés formellement. La deuxième donnée est un ensemble de règles de constructions de formules. Les formules sont définies par des moyens combinatoires à l'aide de ces règles : suites de symboles, arbres étiquetés, graphes…d'un système de déduction (ou d'un système d'inférence, ou encore d'un calcul) disposant d'un ensemble d'axiomes et de règles d'inférence. Les déductions sont également définies par des moyens combinatoires. Une déduction permet de dériver des formules (les formules prouvables ou théorèmes) à partir de formules de départ (les axiomes) au moyen de règles (les règles d'inférence).La sémantique sert à attribuer à chaque formule construite à partir d'un système formel un sens, voire une valeur de vérité. Cette attribution se fait au moyen d'une  interprétation (ou valuation) des formules; il s'agit d'une fonction associant à toute formule un objet dans une structure abstraite appelée modèle, ce qui permet de définir la validité des formules. Elle permet ainsi d'interpréter les formules d'un système formel dans un contexte donné. Dans le cadre de la logique classique, il s'agit d'attribuer à chaque formule la valeur Vrai ou la valeur Faux, qu'on peut même respectivement identifier à donner la valeur 1 ou la valeur 0 (voir Algèbre de Boole). Il arrive parfois que l'on puisse définir la syntaxe comme étant le système de déduction, et qu'on appelle calcul le système formel entier, voire le système logique entier. De ce fait, il est courant d'utiliser le nom de calcul des propositions pour désigner ce qu'on devrait appeler logique des propositions, de la même façon qu'on peut confondre calcul des prédicats et logique des prédicats.La caractéristique principale des formules et des déductions est qu'il s'agit d'objets finis ; plus encore, chacun des ensembles de formules et de déductions est récursif, c'est-à-dire que l'on dispose d'un algorithme qui détermine si un objet donné est une formule correcte ou une déduction correcte du système. L'étude de logique du point de vue des formules et des expressions s'appelle la syntaxe.La sémantique, au contraire, échappe à la combinatoire en faisant appel (en général) à des objets infinis. Elle a d'abord servi à « définir » la vérité. Par exemple, en calcul des propositions, les formules sont interprétées, par exemple, par des éléments d'une algèbre de Boole.Une mise en garde est de rigueur ici, car Kurt Gödel (suivi par Alfred Tarski) a montré avec son théorème d'incomplétude l'impossibilité de justifier mathématiquement la rigueur mathématique dans les mathématiques. C'est pourquoi il est plus approprié de voir la sémantique comme une métaphore : les formules — objets syntaxiques — sont projetées dans un autre monde. Cette technique — plonger les objets dans un monde plus vaste pour raisonner sur eux — est couramment utilisée en mathématique et est efficace.Cependant, la sémantique ne sert pas qu'à « définir » la vérité. Par exemple, la sémantique dénotationnelle est une interprétation, non des formules, mais des déductions visant à capturer leur contenu calculatoire.En logiques classique et intuitionniste, on distingue deux types d'axiomes : les axiomes logiques qui expriment des propriétés purement logiques comme                     A        ?        ¬        A              {\displaystyle A\lor \lnot A}   (principe du tiers exclu, valide en logique classique mais pas en logique intuitionniste) et les axiomes extra-logiques qui définissent des objets mathématiques ; par exemple les axiomes de Peano définissent les entiers de l'arithmétique tandis que les axiomes de Zermelo-Fraenkel définissent les ensembles. Quand le système possède des axiomes extra-logiques, on parle de théorie axiomatique. L'étude des différents modèles d'une même théorie axiomatique est l'objet de la théorie des modèles.Deux systèmes de déduction peuvent être équivalents au sens où ils ont exactement les mêmes théorèmes. On démontre cette équivalence en fournissant des traductions des déductions de l'un dans les déductions de l'autre. Par exemple, il existe (au moins) trois types de systèmes de déduction équivalents pour le calcul des prédicats : les systèmes à la Hilbert, la déduction naturelle et le calcul des séquents. On y ajoute parfois le style de Fitch qui est une variante de la déduction naturelle dans laquelle les démonstrations sont présentées de façon linéaire. Alors que la théorie des nombres démontre des propriétés des nombres, on notera la principale caractéristique de la logique en tant que théorie mathématique : elle « démontre » des propriétés de systèmes de déduction dans lesquels les objets sont des « théorèmes ». Il y a donc un double niveau dans lequel il ne faut pas se perdre. Pour clarifier les choses, les « théorèmes » énonçant des propriétés des systèmes de déduction sont parfois appelés des « métathéorèmes ». Le système de déduction que l'on étudie est appelé la « théorie » et le système dans lequel on énonce et démontre les métathéorèmes est appelé la « métathéorie ».Les propriétés fondamentales des systèmes de déduction sont la correction, la cohérence, la complétude .La correction énonce que les théorèmes sont valides dans tous les modèles. Cela signifie que les règles d'inférence sont bien adaptées à ces modèles, autrement dit que le système de déduction formalise correctement le raisonnement dans ces modèles. Un système de déduction est cohérent (on emploie aussi l'anglicisme consistant) s'il admet un modèle, ou ce qui revient au même, s'il ne permet pas de démontrer n'importe quelle formule. On parle aussi de cohérence équationnelle lorsque le système admet un modèle non trivial, c'est-à-dire ayant au moins deux éléments. Cela revient à affirmer que le système ne permet pas de démontrer n'importe quelle équation.La complétude se définit par rapport à une famille de modèles. Un système de déduction est complet par rapport à une famille de modèles, si toute proposition qui est valide dans tous les modèles de la famille est un théorème. En bref, un système est complet si tout ce qui est valide est démontrable.Une autre propriété des systèmes de déduction apparentée à la complétude est la cohérence maximale. Un système de déduction est maximalement cohérent, si l'ajout d'un nouvel axiome qui n'est pas lui-même un théorème rend le système incohérent.Affirmer « tel système est complet pour telle famille de modèles » est un exemple typique de métathéorème.Dans ce cadre, la notion intuitive de vérité donne lieu à deux concepts formels : la validité et la démontrabilité. Les trois propriétés de correction, cohérence et complétude précisent comment ces notions peuvent être reliées, espérant qu'ainsi la vérité, quête du logicien, puisse être cernée.Les propositions sont des formules exprimant des faits mathématiques, c'est-à-dire des propriétés qui ne dépendent d'aucun paramètre, et qui donc ne peuvent qu'être vraies ou fausses, comme « 3 est un multiple de 4 » (au contraire de « n est un multiple de 4 », qui est vraie ou fausse selon la valeur que l'on donne au paramètre n) ou  « les zéros non triviaux de la fonction zêta de Riemann ont tous pour partie réelle 1/2 ». Les propositions élémentaires, appelées variables propositionnelles, sont combinées au moyen de connecteurs pour former des formules complexes. Les propositions peuvent être interprétées en remplaçant chaque variable propositionnelle par une proposition. Par exemple une interprétation de la proposition (P ? ¬P) ? Q pourrait être « si 3 est pair et 3 est impair alors 0 = 1 ». Les connecteurs sont présentés avec leur interprétation en logique classique.La disjonction de deux propositions P et Q est la proposition notée P ? Q ou « P ou Q » qui est vraie si au moins une des deux propositions est vraie; elle est donc fausse uniquement si les deux propositions sont fausses.La négation d’une proposition P est la proposition notée ¬P, ou « non P » qui est vraie lorsque P est fausse; elle est donc fausse lorsque P est vraie.À partir de ces deux connecteurs, on peut construire d’autres connecteurs ou abréviations utiles.La conjonction de deux propositions P et Q est la proposition suivante :¬((¬P) ? (¬Q)) c'est-à-dire non ( (non P) ou (non Q) )Celle-ci est notée P ? Q ou « P et Q » et n’est vraie que lorsque P et Q sont vraies et est donc fausse si l’une des deux propositions est fausse. L'implication de Q par P est la proposition (¬P) ? Q, notée « P ? Q » ou « P implique Q », et qui est fausse seulement si P est une proposition vraie et Q fausse.L'équivalence logique de P et Q est la proposition ( (P ? Q) ? ( Q ? P) ) ( ((P implique Q) et (Q implique P) )), notée « P ? Q » ou (P est équivalent à Q), et qui n'est vraie que si les deux propositions P et Q ont même valeur de vérité.Le ou exclusif ou disjonction exclusive de P et Q est la proposition P ? Q (parfois aussi notée P ? Q ou encore P | Q ou par les concepteurs de circuits ) qui correspond à (P ? Q) ? ¬(P ? Q), c'est-à-dire en français : soit P, soit Q, mais pas les deux à la fois. Le ou exclusif de P et Q correspond à P ? ¬Q ou encore à ¬(P ? Q). Cette proposition n'est vraie que si P et Q ont des valeurs de vérités distinctes.Une caractéristique du calcul propositionnel dit « classique » est que toutes les propositions peuvent s'exprimer à partir de deux connecteurs : par exemple ? et ¬ (ou et non). Mais d'autres choix sont possibles comme ? (implication) et ? (faux). On peut n'utiliser qu'un seul connecteur, le symbole de Sheffer « | » (Henry M. Sheffer, 1913), appelé « stroke » par son concepteur et appelé aujourd'hui Nand et noté  par les concepteurs de circuits ; on peut aussi n'utiliser que le connecteur Nor (noté ) comme l'a remarqué Charles Sanders Peirce (1880) sans le publier. En somme, en logique classique, un unique connecteur suffit pour rendre compte de toutes les opérations logiques.Le calcul des prédicats étend le calcul propositionnel en permettant d'écrire des formules qui dépendent de paramètres ; pour cela le calcul des prédicats introduit les notions de variables, de symboles de fonctions et de relations, de termes et de quantificateurs ; les termes sont obtenus en combinant les variables au moyen des symboles de fonction, les formules élémentaires sont obtenues en appliquant les symboles de relations à des termes.Une formule typique du calcul des prédicats est « ? a, b ( (p = a.b) ? ( (a = 1) ? (b = 1) ) ) » qui lorsqu'on l'interprète dans les entiers exprime que le paramètre p est un nombre premier (ou 1). Cette formule utilise deux symboles de fonction (le point, fonction binaire interprétée par la multiplication des entiers, et le symbole « 1 », fonction 0-aire, c'est-à-dire constante) et un symbole de relation (pour l'égalité). Les variables sont a, b et p, les termes sont a.b et 1 et les formules élémentaires sont « p = a.b », « a = 1 » et « b = 1 ». Les variables a et b sont quantifiées mais pas la variable p dont la formule dépend.Il existe plusieurs variantes du calcul des prédicats ; dans la plus simple, le calcul des prédicats du premier ordre, les variables représentent toutes les mêmes types d'objets, par exemple dans la formule ci-dessus, les 3 variables a, b et p vont toutes représenter des entiers. Dans le calcul des prédicats du second ordre, il y a deux types de variables : des variables pour les objets et d'autres pour les prédicats, c'est-à-dire les relations entre objets. Par exemple en arithmétique du second ordre on emploie des variables pour représenter des entiers, et d'autres pour des ensembles d'entiers. La hiérarchie continue ainsi, au 3e ordre on a 3 types de variables pour les objets, les relations entre objets, les relations entre relations, etc.Pour décrire le calcul des prédicats, une opération essentielle est la substitution qui consiste à remplacer dans une formule toutes les occurrences d'une variable x par un terme a, obtenant ainsi une nouvelle formule ; par exemple si on remplace la variable p par le terme n! + 1 dans la formule ci-dessus on obtient la formule « ? a, b ( (n! + 1 = a.b) ? ( (a = 1) ? (b = 1) ) ) » (qui exprime que la factorielle de l'entier n plus 1 est un nombre premier).Si P est une formule dépendant d'un paramètre x et a est un terme, l’assemblage obtenu en remplaçant x par a dans P est une formule qui peut se noter P[a/x] ou (a|x)P, ou d'autres variantes de ces notations.et s’appelle formule obtenue par substitution de x par a dans P.Pour mettre en évidence un paramètre x dont dépend une formule P, on écrit celle-ci sous la forme P{x} ; on note alors P{a} la proposition (a|x)P.On peut chercher à trouver la (les) substitution(s) qui rend(ent) une formule prouvable ; le problème est particulièrement intéressant dans le cas de formules dites équationnelles, c'est-à-dire de la forme t(x) = t'(x). La théorie qui cherche à résoudre de telles équations dans le cadre de la logique mathématique s'appelle l'unification.Les quantificateurs sont les ingrédients syntaxiques spécifiques du calcul des prédicats. Comme les connecteurs propositionnels ils permettent de construire de nouvelles formules à partir d'anciennes, mais ils s'appuient pour cela sur l'utilisation des variables.Soit une formule du calcul des prédicats P. On construit alors une nouvelle formule dite existentielle notée ? x P qui se lit « il existe x tel que P ». Supposons que P ne « dépende » que de x. La proposition ? x P est vraie quand il existe au moins un objet a (dans le domaine considéré, celui sur lequel « varie » x) tel que, quand on substitue a à x dans P on obtienne une proposition vraie. La formule P est vue comme une propriété, et ? x P est vraie quand il existe un objet ayant cette propriété.Le signe ? s’appelle le quantificateur existentiel.De même on peut construire à partir de P une formule dite universelle notée ? x P, qui se lit « pour tout x P » ou quel que soit x P. Elle signifie que tous les objets du domaine considéré (ceux que x est susceptible de désigner) possèdent la propriété décrite par P. Le signe ? s’appelle le quantificateur universel. En logique classique les quantificateurs universel et existentiel peuvent se définir l'un par rapport à l'autre, par négation car :En effet « il est faux que tout objet possède une propriété donnée » signifie « il en existe au moins un qui ne possède pas cette propriété ».  Utilisation des quantificateurs  Propriétés élémentaires Soient P et Q deux propositions et x un objet indéterminé.¬(? x P) ? (? x ¬ P)(? x) (P?Q) ? ( (? x) P ? (? x) Q )(? x) P ? (? x) Q ? (? x) (P ? Q )  (Implication réciproque fausse en général)(? x) (P?Q) ? ( (? x) P ? (? x) Q )(? x) (P?Q) ? ( (? x) P ? (? x) Q )  (Implication réciproque fausse en général) Propriétés utiles Soient P une proposition et x et y des objets indéterminés.(? x) (? y) P ? (? y) (? x) P(? x) (? y) P ? (? y) (? x) P(? x) (? y) P ? (? y) (? x) P S’il existe un x, tel que pour tout y, on ait P vraie, alors pour tout y, il existe bien un x (celui obtenu avant) tel que P soit vraie.L’implication réciproque est fausse en général, parce que si pour chaque y, il existe un x tel que P soit vraie, ce x pourrait dépendre de y et varier suivant y. Ce x pourrait donc ne pas être le même pour tout y tel que P soit vraie. Propriétés moins intuitives ?x Px ? ?x PxSi A est une formule où la variable x n'apparaît pas librement, on a :(A ? ?x Px) ? ?x (A ? Px)(A ? ?x Px) ? ?x (A ? Px)Mais aussi, ce qui est moins intuitif : (?x Px ? A) ? ?x (Px ? A)(?x Px ? A) ? ?x (Px ? A)Associé à cette dernière règle, voir le paradoxe du buveur.Le symbole de relation « = » qui représente l'égalité, a un statut un petit peu particulier, à la frontière entre les symboles logiques (connecteurs, quantificateurs) et les symboles non logiques (relations, fonctions). La formule a = b signifie que a et b représentent le même objet (ou encore que a et b sont des notations différentes du même objet), et se lit « a est égal à b ». La théorie des modèles classique se développe le plus souvent dans le cadre du calcul des prédicats avec égalité, c'est-à-dire que les théories considérées sont égalitaires : la relation d'égalité est utilisée en plus des symboles de la signature de la théorie.Du point de vue de la déduction, l'égalité est régie par des axiomes, décrits ci-dessous, exprimant essentiellement que deux objets égaux ont les mêmes propriétés (et, en logique du second ordre, que la réciproque est vraie).La négation de « = » est la relation « ? » qui est définie par a ? b si et seulement si ¬(a = b). Axiomes de l'égalité en logique du premier ordre En logique du premier ordre :? x (x = x) (réflexivité de =)? x (? y) ( (x = y) ? (y = x) ) (symétrie de =)? x (? y) (? z) ( ((x = y) ? (y = z)) ? (x = z) ) (transitivité de =)La relation = étant réflexive, symétrique et transitive, on dit que la relation = est une relation d'équivalence.Soit P{x} une formule dépendant d'une variable x. Soient a et b deux termes tels que a = b. Alors les propositions P{a} et P{b} sont équivalentes. Ces axiomes (un par formule P{x}) expriment que deux objets égaux ont les mêmes propriétés.Soit F(x) une fonction contenant une variable libre x. Soient a et b des termes tels que a = b. Alors les termes F(a) et F(b) (obtenus en remplaçant respectivement x par a et x par b dans F(x)) sont égaux.Ces deux dernières propriétés expriment intuitivement que = est la plus fine des relations d'équivalence. Définition en logique du second ordre En logique du second ordre on peut définir plus simplement l'égalité par :a = b si et seulement si ?P (Pa ? Pb)Autrement dit deux objets sont égaux si et seulement s'ils ont les mêmes propriétés (principe d'identité des indiscernables énoncé par Leibniz)Jon Barwise (dir.), Handbook of mathematical Logic, North Holland, 1977 (ISBN 0-7204-2285-X)  Dale Jacquette (dir.), A Companion to Philosophical Logic, Blackwell, coll. « Companions to Philosophy », 2002, 832 p. (ISBN 978-1-4051-4575-6, présentation en ligne)Stewart Shapiro (dir.), The Oxford Handbook of Philosophy of Mathematics and Logic, Oxford Scholarship, 2005, 774 p. (ISBN 978-0-19-514877-0, présentation en ligne)Handbook of Philosophical Logic, Kluwer Academic Publishers, depuis 2001 (présentation en ligne) Handbooks Dov Gabbay et John Woods éditeurs, Handbook of the History of Logic, en au moins 11 volumes, chez Elsevier.Dov Gabbay et Franz Guenthner éditeurs,  Handbook of Philosophical Logic, en au moins 16 volumes, chez Springer. Bande dessinée Logicomix, édition française Vuibert, 2010Scénario : Apóstolos K. Doxiàdis, Christos Papadimitriou - Dessin : Alecos Papadatos - Couleurs : Annie Di Donna [détail des éditions] Livres Geraldine BradyFrom Peirce to Skolem: A Neglected Chapter in the History of Logic, North Holland, coll. Studies in the History and Philosophy of Mathematics, Volume 4, 468 pages, 2000,  (ISBN 978-0-444-50334-3). Sur les apports en logique essentiellement de Peirce mais aussi de Mitchell, Schröder, Löwenheim et Skolem.Haskell Curry,(en) Outlines of a formalist philosophy of mathematics, North Holland, 1951Foundations of Mathematical Logic, Dover, 1979 (1re éd. 1963)Hao Wang(en) From Mathematics to Philosophy, Londres, Routledge & Kegan Paul, 1974(en) Popular Lectures on Mathematical Logic, New-York, Van Nostrand, 1981, 281 p. (ISBN 0-486-67632-3, lire en ligne)Willard Van Orman Quine (trad. J. Largeault), Philosophie de la logique [« Philosophy of Logic »], Paris, Aubier Montaigne, 1975 (1re éd. 1970 éd.: Prentice Hall)Whitehead, Alfred North & Russell, Bertrand: Principia Mathematica. 3 vols, Merchant Books, 2001,  (ISBN 978-1-60386-182-3) (vol. 1),  (ISBN 978-1-60386-183-0) (vol. 2),  (ISBN 978-1-60386-184-7) (vol. 3) Recueils de textes classiques ayant fondé la discipline (en) Martin Davis (dir.), The Undecidable : Basic Papers on Undecidable Propositions, Unsolvable Problems, and Computable Functions, Dover Publication, 1965(en) Jean van Heijenoort (dir.), From Frege To Gödel: A Source Book in Mathematical Logic, 1879-1931, Harvard Univ. Press., 1977 (1re éd. 1967) (présentation en ligne)(en) Hilary Putnam (dir.) et Paul Benacerraf (dir.), Philosophy of Mathematics : Selected Readings, Cambridge University Press, 1983 (1re éd. 1964) (ISBN 0-521-29648-X)Jean Largeault (dir.), Logique Mathématique : Textes, Armand Colin, 1972François Rivenc et Philippe de Rouilhan, Logique et fondements des mathématiques (1850-1914). Anthologie, Payot, 1992 Recueils de textes sur un auteur précis Alfred Tarski,Logique, sémantique, métamathématique, 1923-1944 : Sélection de textes par Gilles Gaston Granger et al., vol. 1 et 2, Paris, Armand Colin, 1974Kurt Gödel,Collected Works, posthume. En anglais Nous ne mentionnons pas ici les ouvrages de références en anglais dans les sous-domaines de la logique mathématique que sont la théorie de la démonstration, la théorie des modèles, la théorie des ensembles ou la calculabilité.Alonzo Church, Introduction to mathematical logic, Princeton, Princeton University Press, 1956 (1re éd. 1944), 378 p. (ISBN 0-691-02906-7, lire en ligne)Stephen Cole KleeneIntroduction to Metamathematics, Ishi Press International, 1952 (lire en ligne)Logique mathématique [« Mathematical Logic (John Wiley - Dover 1967) »]  (trad. de l'anglais par Jean Largeault), Paris, Armand Colin (1971) ou Gabay (1987), 1987, 412 p. (ISBN 978-2-87647-005-7 et 2-87647-005-5, lire en ligne)Elliott Mendelson (en), Introduction to mathematical logic, D. Van Nostrand, 1964J. R. Schoenfield, Mathematical Logic, Addison-Wesley, 1967Jean-Yves Girard, Yves Lafont et Paul Taylor, Proofs and types, Cambridge University Press, 1989Dirk van Dalen (en), Logic and Structure, Berlin Heidelberg, Springer-Verlag, 2004 (1re éd. 1994) En français La quasi-totalité des ouvrages de référence sont en anglais, néanmoins, existent aussi en français des ouvrages de référence comme ceux listés ci-dessous :René Cori et Daniel Lascar, Logique mathématique, tomes 1 [détail des éditions] et 2 [détail des éditions], Masson, 2003 (1re éd. 1993)René David, Karim Nour et Christophe Raffalli, Introduction à la logique. Théorie de la dé"
Informatique;"L’optimisation combinatoire, (sous-ensemble à nombre de solutions finies de l'optimisation discrète), est une branche de l'optimisation en mathématiques appliquées et en informatique, également liée à la recherche opérationnelle, l'algorithmique et la théorie de la complexité.Dans sa forme la plus générale, un problème d'optimisation combinatoire (sous-ensemble à nombre de solutions finies de l'optimisation discrète) consiste à trouver dans un ensemble discret un parmi les meilleurs sous-ensembles (ou solutions) réalisables, la notion de meilleure solution étant définie par une fonction objectif. Formellement, étant donnés :un ensemble discret                     N              {\displaystyle N}   fini;une fonction d'ensemble                     f        :                  2                      N                          ?                  R                      {\displaystyle f:2^{N}\rightarrow \mathbb {R} }  , dite fonction objectif ;et un ensemble                                           R                                {\displaystyle {\mathcal {R}}}   de sous-ensembles de                     N              {\displaystyle N}  , dont les éléments sont appelés les solutions réalisables,un problème d'optimisation combinatoire consiste à déterminer                              max                      S            ?            N                          {        f        (        S        )        :                S        ?                              R                          }        .              {\displaystyle \max _{S\subseteq N}\{f(S):\,S\in {\mathcal {R}}\}.}  L'ensemble des solutions réalisables ne saurait être décrit par une liste exhaustive car la difficulté réside ici précisément dans le fait que le nombre des solutions réalisables rend son énumération impossible, ou bien qu'il est NP-complet de dire s'il en existe ou non. La description de                                           R                                {\displaystyle {\mathcal {R}}}   est donc implicite, il s'agit en général de la liste, relativement courte, des propriétés des solutions réalisables.La plupart du temps, on est dans les cas particuliers suivants :                    N        =        {        1        ,        2        ,        …        ,        n        }              {\displaystyle N=\{1,2,\ldots ,n\}}   et                                           R                          =        X        ?                              Z                                n                                {\displaystyle {\mathcal {R}}=X\cap \mathbb {Z} ^{n}}   où                     X        ?                              R                                n                                {\displaystyle X\subset \mathbb {R} ^{n}}   et la cardinalité de                                           R                                {\displaystyle {\mathcal {R}}}   est exponentielle en                     n              {\displaystyle n}   ;                                          R                          =        c        o        n        v        (                              R                          )        ?                              Z                                n                                {\displaystyle {\mathcal {R}}=conv({\mathcal {R}})\cap \mathbb {Z} ^{n}}   donc les propriétés de                                           R                                {\displaystyle {\mathcal {R}}}   se traduisent en contraintes linéaires, c'est-à-dire que                     X              {\displaystyle X}   et un polyèdre de                                           R                                n                                {\displaystyle \mathbb {R} ^{n}}   ;                    f        (        S        )        =                  ?                      s            ?            S                          f        (        s        )              {\displaystyle f(S)=\sum _{s\in S}f(s)}  .Le problème du voyageur de commerce (TSP) où un voyageur de Commerce doit parcourir les N villes d'un pays en effectuant le trajet le plus court. Une résolution par énumération nécessite de calculer ((N-1)!)/2 trajets entre des villes. En particulier, pour 24 villes, il faudrait en générer (23!)/2                     ?              {\displaystyle \approx }   2,5×1022. Pour fournir un ordre de grandeur comparatif, une année ne compte qu'environ 3,16×1013 microsecondes.Trouver une solution optimale dans un ensemble discret et fini est un problème facile en théorie : il suffit d'essayer toutes les solutions, et de comparer leurs qualités pour voir la meilleure. Cependant, en pratique, l'énumération de toutes les solutions peut prendre trop de temps ; or, le temps de recherche de la solution optimale est un facteur très important et c'est à cause de lui que les problèmes d'optimisation combinatoire sont réputés si difficiles. La théorie de la complexité donne des outils pour mesurer ce temps de recherche. De plus, comme l'ensemble des solutions réalisables est défini de manière implicite, il est aussi parfois très difficile de trouver ne serait-ce qu'une solution réalisable.Quelques problèmes d'optimisation combinatoire peuvent être résolus (de manière exacte) en temps polynomial par exemple par un algorithme glouton, un algorithme de programmation dynamique ou en montrant que le problème peut être formulé comme un problème d'optimisation linéaire en variables réelles.Dans la plupart des cas, le problème est NP-difficile et, pour le résoudre, il faut faire appel à des algorithmes de branch and bound, à l'optimisation linéaire en nombres entiers ou encore à la programmation par contraintes.En pratique, la complexité physiquement acceptable n'est souvent que polynomiale. On se contente alors d'avoir une solution approchée au mieux, obtenue par une heuristique ou une métaheuristique.Pour le problème du VRP envisagé à large échelle, Richard Karp zone les villes, résout le problème zone par zone, puis assemble au mieux les parcours locaux.Pour un problème d'optimisation d'affectation d'équipages, l'optimisation linéaire en variables réelles dégrossit le problème : on considère comme définitives les valeurs des variables ainsi collées aux extrêmes de leur domaine, et on n'applique les méthodes d'optimisation combinatoire que sur le problème résiduel.Pour certains problèmes, on peut prouver une garantie de performance, c'est-à-dire que pour une méthode donnée l'écart entre la solution obtenue et la solution optimale est borné. Dans ces conditions, à domaine égal, un algorithme est préférable à un autre si, à garantie égale ou supérieure, il est moins complexe. Portail des mathématiques   Portail de l'informatique théorique"
Informatique;"Un robot est un dispositif mécatronique (alliant mécanique, électronique et informatique) conçu pour accomplir automatiquement des tâches imitant ou reproduisant, dans un domaine précis, des actions humaines. La conception de ces systèmes est l'objet d'une discipline scientifique, branche de l'automatisme nommée robotique.Le terme robot apparaît pour la première fois dans la pièce de théâtre (science-fiction) R. U. R. (Rossum's Universal Robots), écrite en 1920 par l'auteur Karel ?apek. Le mot a été créé par son frère Josef à partir du mot tchèque « robota » qui signifie « travail, besogne, corvée ».Les premiers robots industriels apparaissent, malgré leur coût élevé, au début des années 1970. Ils sont destinés à exécuter certaines tâches répétitives, éprouvantes ou toxiques pour un opérateur humain : peinture ou soudage des carrosseries automobiles. Aujourd'hui, l'évolution de l'électronique et de l'informatique permet de développer des robots plus précis, plus rapides ou avec une meilleure autonomie. Industriels, militaires ou spécialistes chirurgicaux rivalisent d'inventivité pour mettre au point des robots assistants les aidant dans la réalisation de tâches délicates ou dangereuses. Dans le même temps apparaissent des robots à usages domestiques : aspirateur, tondeuses, etc.L'usage du terme « robot » s'est galvaudé pour prendre des sens plus larges : automate distributeur, dispositif électro-mécanique de forme humaine ou animale, logiciel servant d'adversaire sur les plateformes de jeu, bot informatique.Le terme robot est issu des langues slaves et formé à partir du radical rabot, rabota (?????? en russe) qui signifie travail, corvée que l'on retrouve dans le mot Rab (???), esclave en russe. Ce radical présent dans les autres langues slaves (ex. : travailleur = robotnik en polonais, ???????? en biélorusse, pracovník en tchèque) provient de l'indo-européen *orbho- qui a également donné naissance au gotique arbais signifiant besoin, nécessité, lui-même source de l'allemand Arbeit, travail.Il fut initialement utilisé par l’écrivain tchécoslovaque Karel ?apek dans sa pièce de théâtre R. U. R. (Rossum's Universal Robots), écrite en 1920. Cette pièce fut jouée pour la première fois en public au Théâtre national à Prague le 25 janvier 1921. Bien que Karel ?apek soit souvent considéré comme l’inventeur du mot, il a lui-même désigné son frère Josef, peintre et écrivain, comme étant l’inventeur réel du terme.Ainsi certains assurent que le mot robot fut d’abord utilisé dans la courte pièce Opilec de Josef ?apek (The Drunkard), publiée dans la collection Lelio en 1917. Selon la Société des frères ?apek à Prague, ce serait néanmoins inexact. Le mot employé dans Opilec est automate, alors que c'est bien dans R.U.R. que le mot robot est apparu pour la première fois.Alors que les « robots » de Karel ?apek étaient des humains organiques artificiels, le mot robot fut emprunté pour désigner des humains « mécaniques ». Le terme androïde peut signifier l’un ou l’autre, alors que le terme cyborg (« organisme cybernétique » ou « homme bionique ») désigne une créature faite de parties organiques et artificielles.Quant au terme robotique, il fut introduit dans la littérature en 1942 par Isaac Asimov dans son livre Runaround. Il y énonce les « trois règles de la robotique » qui deviendront, par la suite, dans les œuvres de sciences fiction les Trois lois de la robotique.Un robot est un assemblage complexe de pièces mécaniques, électro-mécanique ou pièces électroniques. L'ensemble est piloté par une unité centrale appelée « système embarqué » : une simple séquence d'automatisme, un logiciel informatique ou une intelligence artificielle suivant le degré de complexité des tâches à accomplir. Lorsque les robots autonomes sont mobiles, ils possèdent également une source d'énergie embarquée : généralement une batterie d'accumulateurs électriques ou un générateur électrique couplé à un moteur à essence pour les plus énergivore.Les principales sortes de capteurs sont :Les sondeurs (ou télémètres) à ultrason ou Laser. Ces derniers sont à la base des scanners laser permettant à l'unité centrale du robot de prendre « conscience » de son environnement en 3D.Les caméras sont les yeux des robots. Il en faut au moins deux pour permettre la vision en trois dimensions. Le traitement automatique des images pour y détecter les formes, les objets, voire les visages, demande en général un traitement matériel car les microprocesseurs embarqués ne sont pas assez puissants pour le réaliser.Les roues codeuses permettent au robot se déplaçant sur roues, des mesures de déplacement précises en calculant les angles de rotation (information proprioceptive).Les microprocesseurs ou les microcontrôleurs sont des éléments essentiel du système de pilotage d'un robot. Ils permettent l'exécution de séquences d'instruction ou de logiciels commandant la réalisation d'actions ou de fonctions du robot. On trouve souvent, dans les robots de petite taille, des composants à très faible consommation électrique, car ils ne peuvent emporter que des sources d'énergie limitées.Les actionneurs les plus communs sont :des moteurs électriques rotatifs, qui sont fréquemment associés à des réducteurs mécaniques à engrenages.des vérins pneumatique, plus rarement hydraulique, alimentés par une pompe et permettant des actions toniques.Un actionneur est le constituant d'un système mécanique (exemple : bras, patte, roue motrice…) réalisant une action motrice suivant un degré de liberté. Il anime les interfaces haptiques réalisant les actions de saisies d'objets dans les applications de télémanipulation.Une certaine capacité d'adaptation à un environnement inconnu peut, dans les systèmes semi-autonomes actuels, être assurée pourvu que l'inconnu reste relativement prévisible : l'exemple déjà opérationnel du robot aspirateur en est une bonne illustration : le logiciel qui pilote cet appareil est en mesure de réagir aux obstacles qui peuvent se rencontrer dans une habitation, de les contourner, de les mémoriser. Il sauvegarde le plan de l'appartement et peut le modifier en cas de besoin. Il retourne en fin de programme se connecter à son chargeur. Il doit donc fournir une réponse correcte au plus grand nombre possible de stimulations, qui sont autant de données entrées, non par un opérateur, mais par l'environnement.L'autonomie suppose que le programme d'instructions prévoit la survenue de certains événements, puis la ou les réactions appropriées à ceux-ci. Lorsque l'aspirateur évite un buffet parce qu'il sait que le buffet est là, il exécute un programme intégrant ce buffet, par exemple les coordonnées X-Y de son emplacement. Si ce buffet est déplacé ou supprimé, le robot est capable de modifier son plan en conséquence et de traiter une zone du sol qu'il ne prenait pas en compte jusqu'alors.Les ancêtres des robots sont les automates.Le premier automate est le pigeon volant d'Archytas de Tarente aux alentours de 400 av.J.-C. Un automate très évolué fut présenté par Jacques de Vaucanson en 1738 : il représentait un homme jouant d’un instrument de musique à vent. Jacques de Vaucanson créa également un automate représentant un canard mangeant et refoulant sa nourriture après ingestion de cette dernière.Unimate est le premier robot industriel créé. Il fut intégré aux lignes d'assemblage de General Motors en 1961.En 1970, le robot lunaire Lunokhod 1, envoyé par l'Union soviétique, a voyagé sur une distance de 10 km et a transmis plus de 20 000 images.Le 25 octobre 2017, Sophia est le premier robot à avoir une nationalité. Avec l'obtention de la nationalité saoudienne,, cela a suscité la controverse, car il n'est pas évident de savoir si cela implique que Sophia peut voter ou se marier, ou si un arrêt délibéré du système peut être considéré comme un meurtre.La robotique possède de nombreux domaines d'application. Les robots ont été installés dans les industries, ce qui permet de faire des tâches répétitives avec une précision constante. À la suite de l'évolution des techniques on retrouve des robots dans des secteurs de pointe tels que le spatial, médecine, chez les militaires.Depuis quelques années on les retrouve même à domicile.L'image d'êtres automatisés est ancienne, des traces étant présentes dès l’Antiquité gréco-romaine. Pour autant, le sujet a largement évolué, allant du mythe de la création d'êtres humains par les hommes à la prise de pouvoir de ces êtres artificiels, et allant de l'utilisation des matériaux basiques (boue, morceaux humains) à l'utilisation des techniques et sciences modernes. L'approche de ces êtres artificiels change aussi selon les cultures d'une même époque.Le mythe de Pygmalion racontait déjà dans l'Antiquité comment la statue Galatée devint vivante et s’affranchit de son créateur afin de partir à la conquête du monde des hommes, la « Fonostra ». Il ne s'agit toutefois pas d'un robot au sens propre du terme, puisque Galatée n'a pas été conçue pour être autonome. Son autonomie est le fruit de la volonté divine, et non de celle de son créateur ; elle ne dépend ni de l'intelligence de celui-ci, ni des mécanismes (inexistants) qui la composent.Le premier exemple d’un robot de forme humaine fut donné par Léonard de Vinci en 1495. Ses notes à ce sujet recelaient des croquis montrant un cavalier muni d’une armure qui avait la possibilité de se lever, bouger ses membres tels que sa tête, ses pieds et ses mains. Le plan était probablement basé sur ses recherches anatomiques compilées dans l’homme vitruvien. On ne sait pas s’il a tenté de construire ce robot.Lorsque la technologie arriva au point où l’on put préfigurer des créatures mécaniques, les réponses littéraires au concept de robot suscitèrent la crainte que les humains soient remplacés par leurs propres créations.Frankenstein (1818), parfois désigné comme le premier roman de science-fiction, est devenu un synonyme de ce thème. Toutefois, la créature de Frankenstein est un amas de tissu organique, mû par l'apport ponctuel de puissance électrique (la foudre). Le robot n'est pas encore apparu comme tel.La nouvelle L'Homme épingle d'Hermann Mac Coolish Rotenberg Caistria (1809) raconte l’histoire d’un homme qui désirait se transformer en robot par amour pour sa machine à coudre, et Steam Man of the Prairies d’Edward S. Ellis (1865) exprime la fascination américaine de l’industrialisation. La littérature concernant la robotique connut des sommets notables avec l’Homme électrique de Luis Senarens en 1885.En France, le roman L'Ève future de Villiers de L'isle-Adam en 1883 tourne autour de la figure moderne du robot : création métallique, mobile par électricité, et autonome. Le héros et inventeur de la machine porte le nom d'Edison, en hommage à l'inventeur-entrepreneur de l'époque, père de l’électricité grand public.En 1900, la littérature enfantine et les illustrations de W.W. Denslow laissent apparaître dans Le_Magicien_d'Oz un bûcheron de fer-blanc comme un robot. En littérature Le mot robot est créé par Karel ?apek, dans sa pièce de théâtre : R. U. R. (Rossum's Universal Robots), mise en scène à Prague en 1921. Dans une petite île, un industriel humain a créé une chaîne de montages d'où sortent des serviteurs de métal, pour être envoyés partout dans le monde. Les robots se révolteront, prenant le contrôle de leur chaîne de montage, et chercheront à construire toujours plus de robots.Le thème prit donc une consonance économique et philosophique.La littérature de science-fiction ou de bande dessinée autour du thème des robots est foisonnante. Un certain nombre d'auteurs (essentiellement de science-fiction, et parfois ayant une réelle connaissance scientifique du sujet tel Isaac Asimov) ont donné une place particulière aux robots dans leurs ouvrages. Isaac Asimov est le premier à utiliser le mot robotique en 1941. Dans ses nombreux romans où apparaissent des robots (regroupés dans Le Grand Livre des robots), il s'intéressa tout particulièrement à leur interaction avec la société et à la manière dont cette dernière les accepte. Certains de ces romans ont d'ailleurs fait l'objet d'une adaptation cinématographique. Exemples :Isaac Asimov (Qui est également l'inventeur de la notion de robotique avant même que cette science ne soit reconnue)Les Robots, 1967 ((en) I, Robot, 1950), trad. Pierre Billon  (ISBN 978-2-290-34248-0, 2-290-31290-8, 2-277-13453-8 et 2-277-12453-2)Un défilé de robots, 1967 ((en) The Rest of the Robots, 1964), trad. Pierre Billon  (ISBN 978-2-277-12542-6 et 2-290-31125-1)Nous les robots, 1982 ((en) The Complete robot, 1982)  (ISBN 2-258-03291-1)Le Robot qui rêvait, 1988 ((en) Robot Dreams, 1986), trad. France-Marie Watkins  (ISBN 978-2-277-22388-7 et 2-290-31715-2)Les Cavernes d'acier, 1956 ((en) The Caves of Steel, 1953), trad. Jacques Brécard  (ISBN 978-2-277-12404-7 et 2-290-32794-8)Face aux feux du soleil, 1961 ((en) The Naked Sun, 1956), trad. André-Yves Richard  (ISBN 978-2-277-12468-9 et 2-290-32794-8)Les Robots de l'aube, 1984 ((en) Robots of Dawn, 1983), trad. France-Marie Watkins  (ISBN 2-290-33275-5)Les Robots et l'Empire, 1986 ((en) Robots and Empire, 1985), trad. Jean-Paul Martin  (ISBN 978-2-277-21996-5, 2-277-21996-7 et 2-290-31116-2)Douglas AdamsLe Guide du voyageur galactique, 1982 ((en) The Hitchhiker's Guide to the Galaxy, 1979), trad. Jean Bonnefoy, avec Marvin, son robot dépressif  (ISBN 2-207-30340-3).Philip K. Dick avec Le Grand O, James P. Crow, Service avant achat, Au service du maître, L'Ancien Combattant, Le Canon, Autofab (présence d'I.A.), Nanny, La Fourmi électrique, Nouveau Modèle, L'Imposteur, Progéniture...Les androïdes rêvent-ils de moutons électriques ?, 1976 ((en) Do Androids Dream of Electric Sheep ?, 1968), trad. Serge Quadruppani qui a inspiré le film Blade Runner  (ISBN 2-85184-066-5).Fredric BrownDeuxième chance, dans le recueil Fantômes et Farfafouilles  (ISBN 2-207-30065-X).Stanislas LemLe Bréviaire des robots, Denoël, coll. Présence du futur n° 96, 1967 ((pl) , 1961), trad. Halina Sadowska  (ISBN 2-07-034105-4)Contes inoxydables, Denoël, coll. Présence du futur n° 330, 1981 ((pl) Bajki robotów, 1964), trad. Dominique Sila  (ISBN 2-207-50330-5)Pierre BoulleLe Parfait Robot, dans le recueil Contes de l'absurde  (ISBN 2-266-00609-6).Jean-Pierre Andrevon dans de nombreuses nouvelles.Enrico Grassani, Automi. Passato, presente e futuro di una nuova ""specie"", Editoriale Delfino, Milano 2017,  (ISBN 978-88-97323-66-2) Au cinéma Les robots sont présents dans de nombreuses œuvres cinématographiques. Ces robots peuvent être des ennemis de l'Homme (par exemple dans Terminator), parfois trop intelligents pour rester des serviteurs (2001, l'Odyssée de l'espace, Blade Runner). Ces robots peuvent pourtant aussi être foncièrement bons, comme le sont R2-D2 et C-3PO dans Star Wars (1977), ou les robots de L'Homme bicentenaire et I, Robot (deux films adaptés de nouvelles d'Isaac Asimov).Citons aussi le film classique Metropolis (1927). Mais également Short Circuit, Matrix (les sentinelles), WALL-E, Robots, Transformers.Les protagonistes sont dans Robots (film, 2005).Un androïde dans Enthiran.La robotique dans Robots (film, 1988) (en).Dans Runaway : L'Évadé du futur il y a un futur avec des robots.Le robot géant dans The Mechanical Man (en).Des robots sont présents comme le ED-209 dans RoboCop.David qui est un enfant androïde dans A.I. Intelligence artificielle.La femme androïde dans Ex machina (film).Des robots deviennent incontrôlable dans Shopping (film, 1986).Dans le film ""Automatic"" de 1995, il y a un androïde héroïque joué par Olivier Gruner.Un androïde nommé Solo dans Le Guerrier d'acier.Dans Robosapien: Rebooted il y a un Robosapien qui est ami avec un môme nommé Henry.Les courts métrage de robots dans Robot Carnival. Dans la culture populaire Plusieurs séries télévisées comportent un certain nombre de robots ou d'androïdes. On peut ainsi citer les Réplicateurs de Stargate SG-1, les Cybermen de Doctor Who, les hubots de Real Humans (Äkta människor), ou encore les Cylons de Battlestar Galactica. Dans chaque univers, le robot a une place différente. Ainsi, les hubots de Real Humans ont découvert la notion de liberté de pensée et veulent s'affranchir des humains, tandis que les robots de la série Futurama vivent au sein même de leur société sans relation d'infériorité.Il existe aussi des mangas traitant le sujet (Astro, le petit robot, Dragon Ball Z, Medabots) ainsi que de nombreux jeux vidéo (Megaman, Sonic the hedgehog).Enfin, la série Il était une fois... l'Espace en présente de nombreux, soit hostiles soit grandement utiles. Leur présence permet de réfléchir en profondeur sur le libre-arbitre et la volonté d'indépendance.Des robots qui vivent avec des humains dans Cubix.Dans Zentrix, il y a des robots artificiels dans les combats.Dans Mon robot et moi, il y a des écoliers avec des robots.Dans Anatane et les enfants d'Okura, les robots enlèvent des jeunes hommes masculins pour les zombifier soit les changer en criminel de guerre.Des robots Faro dans le jeu vidéo Horizon Zero Dawn.Des androïdes, appelés « hôtes » (hosts) dans Westworld (série télévisée).Dans Le Maître des bots, il y a des robots intelligents comme des humains.Un robot géant dans Goldorak.Un robot géant dans Mazinger Z.Des robots géants dans Gundam Wing.Dans Chair de poule (collection), ""La Punition de la mort"" avec des doubles robotiques.John le robot dans ""Voyage sur la planète préhistorique"".Elias le robot dans ""Starcrash : Le Choc des étoiles"".Une série culte japonaise avec des robots Ganbare!! Robocon (en).Un robot géant dans Giant Robo (en).Isaac, Asimov, I, robot, Bantam Books, 2004, 304 p. (ISBN 978-0-553-90033-0 et 0553900331, OCLC 233705973, lire en ligne)Laurence Devillers, Des robots et des hommes : mythes, fantasmes et réalité, Paris, Plon, 2017, 236 p. (ISBN 978-2-259-25227-0 et 2259252273, OCLC 975286502)(en) Ritter, Helge, Human centered robot systems : cognition, interaction, technology, Berlin/Heidelberg, Springer, 2009, 217 p. (ISBN 978-3-642-10403-9 et 3642104037, OCLC 505433316, lire en ligne)Serge Tisseron, Le jour où mon robot m'aimera : vers l'empathie artificielle, Albin Michel, 2015, 208 p. (ISBN 978-2-226-38232-0 et 2226382321, OCLC 990819243)Ressource relative à la littérature : (en) The Encyclopedia of Science Fiction Ressource relative à la bande dessinée : (en) Comic Vine Actualité en Robotique et tutoriels : Zone Robotique Les recherches en robotique au CNRSVidéo : Conférence sur les Robots - Futuroscope - 22 décembre 2012Le robot a fêté ses 85 ans ! sur le site de Radio Praguehttps://video.vice.com/en_us/video/slutever-harmony-the-sex-robot/5aa6edcbf1cdb36f616c77a2%3Fjwsource%3Dclhttps://nixxons.fandom.com/wiki/Tara_(android)https://muc.fandom.com/wiki/Tara_the_Androidhttps://www.rts.ch/play/radio/le-journal-du-matin/audio/les-paysans-de-vinci-25-les-androdes-de-monsieur-wu?id=6993865http://french.peopledaily.com.cn/VieSociale/n3/2016/1117/c31360-9143061.htmlhttps://edition.cnn.com/2019/11/08/tech/mit-cheetah-robot-herd/index.htmlhttps://www.cs.cmu.edu/~coral-downloads/legged/movies/index.htmlhttps://www.newsbytesapp.com/news/science/watch-mit-robots-play-soccer-and-trip-excitedly/storyhttps://techcrunch.com/2019/11/09/watch-mits-mini-cheetah-robots-frolic-fall-flip-and-play-soccer-together/?guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&guce_referrer_sig=AQAAANH7n2ThssxdOlErjsMVigCGnoLnrRBTW9g1Bybh5KALAN-jZJvBLS5Ncs_qncr_i397tPsE9nCJBpwYTMi7JzzukqTgRsbrtVWMjC7rqlzVahCSI0kEkne3yRte5KY4Jhoy5W6BqW7v4AcmKxMNGc8lmZOkaDy4e7Bfco0s_qyg&guccounter=2https://abcnews.go.com/US/mits-adorable-back-flipping-robots-enjoy-frolicking-autumn/story?id=66853771https://techxplore.com/news/2019-11-fall-madness-mit-mini-cheetah.htmlhttps://tvmag.lefigaro.fr/programme-tv/programme/la-science-face-au-terrorisme-f156613689https://www.programme-tv.net/programme/culture-infos/r1549459761-la-science-face-au-terrorisme/16230392-la-robotique/https://objetconnecte.net/terrorisme-daesh-robots-futur/https://blog.francetvinfo.fr/bureau-londres/2017/12/07/des-robots-tueurs-seront-bientot-entre-les-mains-des-terroristes-selon-la-chambre-des-lords.htmlhttps://www.lesechos.fr/2015/07/robots-tueurs-la-mise-en-garde-des-grands-noms-de-la-tech-268609https://www.20minutes.fr/high-tech/1835379-20160427-chine-devoile-anbot-premier-robot-anti-terroriste-anti-emeutehttps://www.allocine.fr/tag-1245/films/https://www.senscritique.com/top/resultats/Les_meilleurs_films_avec_des_robots/785910https://www.senscritique.com/liste/Des_robots_plein_de_robots/1138091https://www.senscritique.com/liste/Les_Robots_au_cinema/141687https://www.senscritique.com/liste/Le_robot_au_cinema/539769https://www.cinetrafic.fr/top/film/robothttps://www.cinetrafic.fr/liste-film/1343/1/les-robots-et-les-androideshttps://www.tomsguide.fr/30-robots-qui-ont-marque-lhistoire-du-cinema/https://www.vodkaster.com/listes-de-films/les-robots-au-cinema/732069https://www.liberation.fr/cinema/2015/07/29/cinq-films-ou-les-robots-se-retournent-contre-les-hommes_1355549/ Portail de la robotique   Portail de la science-fiction"
Informatique;"C++ est un langage de programmation compilé permettant la programmation sous de multiples paradigmes, dont la programmation procédurale, la programmation orientée objet et la programmation générique. Ses bonnes performances, et sa compatibilité avec le C en font un des langages de programmation les plus utilisés dans les applications où la performance est critique.Créé initialement par Bjarne Stroustrup dans les années 1980, le langage C++ est aujourd'hui normalisé par l'ISO. Sa première normalisation date de 1998 (ISO/CEI 14882:1998), ensuite amendée par l'erratum technique de 2003 (ISO/CEI 14882:2003). Une importante mise à jour a été ratifiée et publiée par l'ISO en septembre 2011 sous le nom de ISO/IEC 14882:2011, ou C++11. Depuis, des mises à jour sont publiées régulièrement : en 2014 (ISO/CEI 14882:2014, ou C++14), en 2017 (ISO/CEI 14882:2017, ou C++17) puis en 2020 (ISO/IEC 14882:2020, ou C++20).En langage C, ++ est l'opérateur d'incrémentation, c'est-à-dire l'augmentation de la valeur d'une variable de 1. C'est pourquoi C++ porte ce nom : cela signifie que C++ est un niveau au-dessus de C.Bjarne Stroustrup commence le développement de C with Classes (C avec classes) en 1979. Il travaille alors dans les laboratoires Bell où il est notamment collègue de l'inventeur du C Dennis Ritchie. L'idée de créer un nouveau langage venait de l'expérience en programmation de Stroustrup pour sa thèse de doctorat. Il s'agissait en l'occurrence d'améliorer le langage C. Stroustrup trouvait que Simula avait des fonctionnalités très utiles pour le développement de gros programmes mais qu'il était trop lent pour être utilisé en pratique (cela était dû à un problème d'implémentation du compilateur Simula), tandis que BCPL était rapide mais de trop bas niveau et non adapté au développement de gros logiciels. Quand Stroustrup commença à travailler aux laboratoires Bell, on lui demanda d'analyser le noyau UNIX en vue de faire du calcul distribué. Se rappelant sa thèse, Stroustrup commença à améliorer le langage C avec des fonctionnalités similaires à celle de Simula. C fut choisi parce qu'il est rapide, portable et d'usage général. En outre, il était une bonne base pour le principe original et fondateur de C++ : « vous ne payez pas pour ce que vous n'utilisez pas ». Dès le départ, le langage ajoutait à C la notion de classe (avec encapsulation des données), de classe dérivée, de vérification des types renforcés (typage fort), d'« inlining », et d'argument par défaut.Alors que Stroustrup développait C with classes, il écrivit CFront, un compilateur qui générait du code source C à partir de code source C with classes. La première commercialisation se fit en octobre 1985. En 1983 le nom « C++ » est inventé, et en 1984 le nom du langage passa de C with classes à celui de « C++ ». Parmi les nouvelles fonctionnalités qui furent ajoutées au langage, il y avait les fonctions virtuelles, la surcharge des opérateurs et des fonctions, les références, les constantes, le contrôle du typage amélioré et les commentaires en fin de ligne. En 1985 fut publiée la première édition de The C++ Programming Language, apportant ainsi une référence importante au langage qui n'avait pas encore de standard officiel. En 1989, c'est la sortie de la version 2.0 de C++. Parmi les nouvelles fonctionnalités, il y avait l'héritage multiple, les classes abstraites, les fonctions membres statiques, les fonctions membres constantes, et les membres protégés. En 1990, The Annotated C++ Reference Manual (« ARM ») fut publié apportant les bases du futur standard. Les ajouts de fonctionnalités tardifs qu'il comportait couvraient les templates, les exceptions, les espaces de noms, les nouvelles conversions et le type booléen.Pendant l'évolution du langage C++, la bibliothèque standard évoluait de concert. Le premier ajout à la bibliothèque standard du C++ concernait les flux d'entrées/sorties qui apportaient les fonctionnalités nécessaires au remplacement des fonctions C traditionnelles telles que printf et scanf. Ensuite, parmi les ajouts les plus importants, il y avait la Standard Template Library. Après des années de travail, un comité réunissant l'ANSI et l'ISO standardisa C++ en 1998 (ISO/CEI 14882:1998), l'année où le comité de standardisation se réunissait à Sophia Antipolis dans le sud de la France. Pendant quelques années après la sortie officielle du standard, le comité traita des problèmes remontés par les utilisateurs, et publia en 2003 une version corrigée du standard C++.Personne ne possède le langage C++. Il est libre de droits ; cependant, le document de standardisation n'est quant à lui pas disponible gratuitement.On peut considérer que C++ « est du C » avec un ajout de fonctionnalités. Cependant, plusieurs programmes syntaxiquement corrects en C ne le sont pas en C++, à commencer bien sûr par ceux qui font usage d'identificateurs correspondant à des mots-clefs en C++.Parmi les fonctionnalités ajoutées figurent :le typage des « prototypes » de fonctions (repris dans ANSI C89) ;La surcharge des fonctions ;les déclarations reconnues comme instructions (repris dans C99) ;les opérateurs new et delete pour la gestion d'allocation mémoire ;le type de données bool (booléen) ;les références & ;les variables et les fonctions membres const (repris partiellement par C à la fin des années 1980) ;les fonctions inline (repris dans C99) ;les paramètres par défaut dans les fonctions ;les référentiels lexicaux (espaces de noms) et l'opérateur de résolution de portée :: ;les classes, ainsi que tout ce qui y est lié : l'héritage, les fonctions membres, les fonctions membres virtuelles, les constructeurs et le destructeur ;la surcharge des opérateurs ;les templates ;la gestion d'exceptions ;l'identification de type pendant l'exécution (RTTI : run-time type information) ;le commentaire sur une ligne introduit par // (existant dans BCPL, repris dans C99) ;les références de rvalue && (C++11) ;la déduction de type à la compilation via auto (C++11) ;les expressions constantes constexpr (C++11);les fonctions lambda (C++11, étendu dans tous les standards publiés depuis) ;les boucles for basées sur une plage (C++11, étendu en C++20) ;les modules via import, export et module (C++20) ;les contraintes et concepts via concept et requires (C++20) ;les fonctions immédiates consteval (C++20) ;les coroutines (C++20) ;La compilation d'un programme en C++ effectue également un contrôle plus minutieux du typage.La bibliothèque standard du C++ englobe la Standard Template Library (STL) qui met à la disposition du programmeur des outils puissants comme des collections (conteneurs) et des itérateurs.À l'origine, la STL était une bibliothèque développée par Alexander Stepanov qui travaillait pour Hewlett-Packard. Dans la norme, celle-ci n'est pas appelée STL, car elle est considérée comme faisant partie de la bibliothèque standard de C++. Toutefois, beaucoup de personnes l'appellent encore de cette manière pour distinguer d'une part, les fonctions d'entrées/sorties comprises dans cette bibliothèque et, d'autre part, celles fournies par la bibliothèque C.Comme en C, l'utilisation d'une bibliothèque peut se faire par l'intermédiaire de la directive #include (suivie du nom du fichier d'en-tête), et certaines d'entre elles (cmath, thread, etc.) nécessitent d'être liées explicitement. Depuis C++20 le mot clé import peut servir à des fins similaires.Le langage C++ utilise les concepts de la programmation orientée objet et permet entre autres :la création de classes ;l'encapsulation ;des relations entre les classes :la composition de classes (composition dans un diagramme de classes),l'association de classes (en) (association dans un diagramme de classes),l'agrégation de classes (agrégation dans un diagramme de classes),la dépendance (dépendance dans un diagramme de classes),l'héritage simple et multiple (héritage dans un diagramme de classes) ;le polymorphisme ;l'abstraction ;la généricité ;la méta-programmation.L'encapsulation permet de faire abstraction du fonctionnement interne (c'est-à-dire la mise en œuvre) d'une classe et ainsi de ne se préoccuper que des services rendus par celle-ci. C++ met en œuvre l'encapsulation en permettant de déclarer les membres d'une classe avec le mot réservé public, private ou protected. Ainsi, lorsqu'un membre est déclaré :public, il sera accessible depuis n'importe quelle fonction ;private, il sera uniquement accessible d'une part, depuis les fonctions qui sont membres de la classe et, d'autre part, depuis les fonctions autorisées explicitement par la classe (par l'intermédiaire du mot réservé friend) ;protected, il aura les mêmes restrictions que s'il était déclaré private, mais il sera en revanche accessible par les classes filles.C++ n'impose pas l'encapsulation des membres dans leurs classes. On pourrait donc déclarer tous les membres publics, mais en perdant une partie des bénéfices apportés par la programmation orientée objet. Il est de bon usage de déclarer toutes les données privées, ou au moins protégées, et de rendre publiques les fonctions membres agissant sur ces données. Ceci permet de cacher les détails de la mise en œuvre de la classe.Voici l'exemple de Hello world donné dans The C++ Programming Language, Third Edition de Bjarne Stroustrup :Dans l'exemple ci-dessus, le code source std::cout << ""Hello, new world!\n"" envoie la chaîne de caractères ""Hello, new world!\n"" à l'objet global cout, défini dans l'espace de noms standard std, grâce à l'opérateur << de cout.En C++, le mot clef namespace permet de définir et de nommer des espaces de noms (namespaces), notion déjà présente en langage C ; en effet, le corps d'une routine, d'une structure de contrôle de flux d'exécution, d'une structure de données ou d'une section de code (délimitée par les accolades { et }) constitue un espace de noms. En C++, le corps d'une classe, à l'instar du corps d'une structure de données, constitue aussi un espace de noms.Dans différents espaces de noms, on peut ainsi définir des entités (routines, variables, etc.) ayant le même identificateur. L'ambiguïté est résolue en utilisant le nom de l'espace de nom devant l'opérateur de portée (::) pour indiquer l'espace de noms dans lequel on veut accéder. Notez que l'espace de noms global du programme n'a pas de nom. Pour accéder à une entité globale, cachée par une entité locale par exemple, on utilise l'opérateur de portée précédé d'aucun nom.Il est possible de spécifier un espace de noms précis à utiliser afin d'éviter d'avoir à recourir à l'opérateur de résolution de portée. Pour cela, le mot-clé using est utilisé avec cette syntaxe :Ainsi, pour utiliser la variable cout définie dans le namespace standard sans utiliser l'opérateur de résolution de portée, il est possible d'écrire using namespace std; ou using std::cout;. Cela est valable pour tous les espaces de noms. Cette instruction se place en général avant le début du code source proprement dit :Il est aussi possible, et conseillé, d'importer un symbole particulier, ou de placer cette instruction dans une fonction afin de limiter la portée :Le mot-clé using peut aussi être utilisé dans les classes. Si une classe B hérite d'une classe A, elle peut grâce à ce mot-clé passer des membres protected de A en public dans B, ou encore démasquer une fonction membre de A qui le serait par une fonction membre de B de même nom :Le programme ci-dessus affiche :Il est aussi possible de définir un nouveau nom pour un namespace :Il est d'usage de séparer prototype (déclaration) et implémentation (définition) de classe dans deux fichiers : la déclaration se fait dans un fichier d'en-tête (dont l'extension varie selon les préférences des développeurs : sans extension dans le standard, .h comme en C, .hh ou .hpp ou .hxx pour différencier le code source C++ du C) alors que la définition se fait dans un fichier source (d'extension également variable : .c comme en C, .cc ou .cpp ou .cxx pour différencier C++ du C).Exemple de la déclaration d'une classe comportant des attributs privés et des fonctions membres publiques :Le nom d'une fonction membre déclarée par une classe doit nécessairement être précédé du nom de la classe suivi de l'opérateur de résolution de portée ::.Exemple de définition des fonctions membres d'une classe (celle déclarée précédemment) :Les Modèles (ou templates) permettent d'écrire des variables, des fonctions et des classes en paramétrant le type de certains de leurs constituants (type des paramètres ou type de retour pour une fonction, type des éléments pour une classe collection par exemple). Les modèles permettent d'écrire du code générique, c'est-à-dire qui peut servir pour une famille de fonctions ou de classes qui ne diffèrent que par le type de leurs constituants.Les paramètres peuvent être de différentes sortes :types simples, tels que les classes ou les types élémentaires (int, double, etc.) ;tableaux de taille constante, dont la taille, déduite par le compilateur, peut être utilisée dans l'instanciation du modèle ;constantes scalaires, c'est-à-dire de type entier (int, char, bool), mais pas flottant (float, double) car leur représentation binaire ne fait pas partie de la norme du langage (jusqu'en C++20 où ils sont autorisés) ;templates, dont la définition doit être passée en paramètre, ce qui permet notamment de s'appuyer sur la définition abstraite, par exemple, d'une collection ;pointeurs ou références, à condition que leur valeur soit définie à l'édition de liens ;fonction membre d'une classe, dont la signature et la classe doivent être aussi passées en paramètres ;attribut d'une classe, dont le type et la classe doivent être aussi passés en paramètres.En programmation, il faut parfois écrire de nombreuses versions d'une même fonction ou classe suivant les types de données manipulées. Par exemple, un tableau de int ou un tableau de double sont très semblables, et les fonctions de tri ou de recherche dans ces tableaux sont identiques, la seule différence étant le type des données manipulées. En résumé, l'utilisation des templates permet de « paramétrer » le type des données manipulées.Les avantages des modèles sont :des écritures uniques pour les fonctions et les classes ;moins d'erreurs dues à la réécriture ;Dans la bibliothèque standard C++, on trouve de nombreux templates. On citera à titre d'exemple, les entrées/sorties, les chaînes de caractères ou les conteneurs. Les classes string, istream, ostream et iostream sont toutes des instanciations de type char.Les fonctions de recherche et de tri sont aussi des templates écrits et utilisables avec de nombreux types.Dans la ligne float f = max<float>(1, 2.2f);, on doit explicitement donner le type float pour le type paramétré T car le compilateur ne déduit pas le type de T lorsqu'on passe en même temps un int (1) et un float (2.2f).Un template donné peut avoir plusieurs instanciations possibles selon les types donnés en paramètres. Si un seul paramètre est spécialisé, on parle de spécialisation partielle. Ceci permet par exemple :de choisir un type de calcul selon qu'un type est un entier, un flottant, une chaîne de caractères, etc. Spécialisons l'exemple précédent pour le cas des pointeurs de chaînes de caractères :d'effectuer au moment de la compilation des calculs arithmétiques, si et seulement si tous les arguments sont connus à ce moment. Un exemple classique est le calcul de la fonction factorielle :À partir de C++14 pour arriver aux mêmes fins nous pourrions aussi utiliser les variables templates :Ainsi nous pouvons écrire factorielle<8>; à la place de Factorielle<8>::value;.Le mécanisme décrit par l'abréviation SFINAE (Substitution Failure Is Not an Error) permet de surcharger un template par plusieurs classes (ou fonctions), même si certaines spécialisations, par exemple, ne peuvent pas être utilisées pour tous les paramètres de templates. Le nom décrit précisément le fonctionnement du mécanisme, littéralement l’acronyme de « Un échec de substitution n'est pas une erreur », le compilateur, lors de la substitution, ignore alors les instanciations inapplicables, au lieu d'émettre une erreur de compilation. Par exemple :Ici f est définie deux fois, le type de retour est conditionné par le type donné en paramètre, il est du type du retour de f.foo() dans le premier cas et de celui de f.bar() dans le deuxième cas. Ainsi, si on appelle f avec un objet de la classe A, seule la première fonction fonctionne puisque la classe A n'a pas de fonction membre bar() et donc la substitution est possible avec cette première version mais pas pour la deuxième. Ainsi, f(a) appelle la première version de f, f(b) appelle la deuxième avec le même raisonnement, mais cette fois pour la fonction membre bar().Si lors d'un développement à venir, un développeur venait à écrire une nouvelle classe ayant une fonction membre publique foo ou bien (ou exclusif) bar, il pourrait également utiliser f avec.Le polymorphisme d'inclusion est mis en œuvre à l'aide du mécanisme des fonctions membres virtuelles en C++. Une fonction membre est rendue virtuelle par le placement du mot-clé virtual devant la déclaration de la fonction membre dans la classe. Lorsqu'une fonction membre virtuelle est appelée, l'implémentation de la fonction membre exécutée est choisie en fonction du type réel de l'objet. L'appel n'est donc résolu qu'à l'exécution, le type de l'objet ne pouvant pas a priori être connu à la compilation.Le mot-clé virtual indique au compilateur que la fonction membre déclarée virtuelle est susceptible d'être redéfinie dans une classe dérivée. Il suffit alors de dériver une classe et de définir une nouvelle fonction membre de même signature (même nom, paramètres compatibles — voir la notion de covariance). Ainsi l'appel de cette fonction membre sur un objet accédé en tant qu'objet de la classe de base mais appartenant en réalité à la classe dérivée donnera lieu à l'appel de la fonction membre définie dans la classe dérivée.En particulier, il est obligatoire d'utiliser le mot-clé virtual devant la déclaration du destructeur de la classe de base lorsque le programme souhaite pouvoir détruire un objet via un pointeur d'instance de la classe de base au lieu d'un pointeur d'instance de la classe dérivée.Ce type de polymorphisme (le polymorphisme d'inclusion) est dit dynamique. Le mécanisme de la surcharge de fonction qui est un polymorphisme ad hoc est de type statique. Dans les deux cas il faut appliquer une logique (par exemple : le nombre et le type des paramètres) pour résoudre l'appel. Dans le cas de la surcharge de fonction, la logique est entièrement calculée à la compilation. Ce calcul permet des optimisations rendant le polymorphisme statique plus rapide que sa version dynamique. La liaison dynamique de fonctions membres issues du mécanisme des fonctions membres virtuelles induit souvent une table cachée de résolution des appels, la table virtuelle. Cette table virtuelle augmente le temps nécessaire à l'appel de fonction membre à l'exécution par l'ajout d'une indirection supplémentaire.Le choix entre liaison dynamique et surcharge (polymorphisme dynamique et statique) est typiquement un problème de calculabilité des appels, ayant souvent pour conséquence finale un choix entre expressivité et performance.Malgré ce dynamisme, il est à noter que le compilateur est capable de « dévirtualiser » les appels de fonctions membres qui peuvent être résolus au moment de la compilation. Dans gcc par exemple, l'option -fdevirtualize lors de la compilation permet cette optimisation, s'il est possible de faire une telle résolution.Un programme C++ peut être produit avec des outils qui automatisent le processus de construction. Les plus utilisés sont :make ;Ant (génération portable en XML) ;SCons (génération portable en Python) ;CMake (génération de Makefile portable) ;Bazel.Anjuta DevStudio ;C++ Builder ;CLion (en) ;Code::Blocks (open-source) ;Dev-C++ et son extension RAD WxDev-C++ ;Eclipse avec le plugin CDT (open-source) ;Emacs (libre) ;KDevelop ;NetBeans (open-source) ;QtCreator (open-source) ;Sun Studio ;Vim ;Microsoft Visual C++ (a été intégré au framework Visual Studio) ;Xcode.GCC pour GNU Compiler Collection (libre, multilangage et multiplateforme : UNIX, Windows, DOS, etc.) ;Clang ;Microsoft Visual C++ (Windows) ;Borland C++ Builder (Windows) ;Intel C++ Compiler (Windows, Linux, MacOS) ;Open64 (en) compilateur opensource d'AMD (Linux) ;Digital Mars C/C++ compiler (Windows) ;Open Watcom ;Boost ;Qt ;Gtkmm ;wxWidgets ;SFML ;OpenCV ;SDLmm, surcouche C++ à la SDL ;LLVM.etc. Ouvrages en langue anglaise [Deitel et Deitel 2011] (en) P. Deitel et H. Deitel, C++ : How to Program, 20 Hall, 2011, 8e éd., 1104 p. (ISBN 978-0-13-266236-9).[Dawson 2010] (en) M. Dawson, Beginning C++ Through Game Programming, Course Technology PTR, 2010, 3e éd., 432 p. (ISBN 978-1-4354-5742-3).[Gregoire, Solter et Kleper 2011] (en) Marc Gregoire, Nicolas A. Solter et Scott J. Kleper, Professional C++, John Wiley, octobre 2011, 1104 p. (ISBN 978-0-470-93244-5, présentation en ligne).[Josuttis 2011] (en) Nicolaï Josuttis, The C++ Standard Library, A Tutorial and Reference, Addison-Wesley, 2011, 2e éd., 1099 p. (ISBN 978-0-321-62321-8, présentation en ligne).[Koenig et Moo 2000] (en) A. Koenig et B. Moo, Accelerated C++ : Practical Programming by Example, Addison-Wesley, 2000, 1re éd., 352 p. (ISBN 978-0-201-70353-5).[Lippman, Lajoie et Moo 2012] (en) Stanley B. Lippman, Josée Lajoie et Barbara E. Moo, C++ Primer : 5th Edition, août 2012, 5e éd., 1399 p. (ISBN 978-0-321-71411-4).[Lischner 2003] (en) R. Lischner, C++ in a nutshell, O'Reilly Media, 2003, 1re éd., 704 p. (ISBN 978-0-596-00298-5).[Meyers 2005] (en) S. Meyers, Effective C++ : 55 Specific Ways to Improve Your Programs and Designs, Addison-Wesley Professional, 2005, 3e éd., 320 p. (ISBN 978-0-321-33487-9, présentation en ligne).[Oualline 2003] (en) S. Oualline, Practical C++ programming, O'Reilly Media, 2003, 2e éd., 600 p. (ISBN 978-0-596-00419-4, présentation en ligne).[Lafore 2001] (en) R. Lafore, Object-oriented programming in C++, Sams, 2001, 4e éd., 1040 p. (ISBN 978-0-672-32308-9).[Prata 2011] (en) S. Prata, C++ Primer Plus (Developer's Library), Addison-Wesley Professional, 2011, 6e éd., 1200 p. (ISBN 978-0-321-77640-2, présentation en ligne).[Stroustrup 2009] (en) Bjarne Stroustrup, Programming : Principles and Practice using C++, Addison-Wesley, 2009, 1236 p. (ISBN 978-0-321-54372-1).[Stroustrup 2013] (en) Bjarne Stroustrup, The C++ Programming Language : 4th Edition, Addison-Wesley Professional, 2013, 4e éd., 1368 p. (ISBN 978-0-321-56384-2).[Stroustrup 1994] (en) Bjarne Stroustrup, The Design and Evolution of C++, Addison-Wesley professional, 1994, 1re éd., 480 p. (ISBN 978-0-201-54330-8).[Sutter 1999] (en) H. Sutter, Exceptional C++ : 47 Engineering Puzzles, Programming Problems, and Solutions, Addison-Wesley Professional, 1999, 240 p. (ISBN 978-0-201-61562-3, présentation en ligne).[Vandevoorde et Josuttis 2002] (en) David Vandevoorde et Nicolaï Josuttis, C++ Templates : the Complete Guide, Addison-Weslay, 2002, 528 p. (ISBN 978-0-201-73484-3).[Vandevoorde 1998] (en) David Vandevoorde, C++ Solutions : Companion to the C++ Programming Language, Addison-Wesley, 1998, 3e éd., 292 p. (ISBN 978-0-201-30965-2). Ouvrages en langue française [Benharrats et Vittupier 2021] Mehdi Benharrat et Benoît Vittupier, Le guide du C++ moderne : de débutant à développeur, D-Booker, 2021, 1re éd., 708 p. (ISBN 978-2-8227-0881-4, présentation en ligne).[Chappelier et Seydoux 2005] J-C. Chappelier et F. Seydoux, C++ par la pratique : Recueil d'exercices corrigés et aide-mémoire, PPUR, 2005, 2e éd., 412 p. (ISBN 978-2-88074-732-9, présentation en ligne).[Deitel et Deitel 2004] P. Deitel et H. Deitel, Comment programmer en C++, Reynald Goulet, 2004, 1178 p. (ISBN 978-2-89377-290-5).[Delannoy 2001] Claude Delannoy, Programmer en langage C++, Paris, Eyrolles, 2011, 8e éd., 822 p. (ISBN 978-2-212-12976-2, présentation en ligne).[Delannoy 2007] Claude Delannoy, Exercices en langage C++, Paris, Eyrolles, 2007, 3e éd., 336 p. (ISBN 978-2-212-12201-5, présentation en ligne).[Géron et Tawbi 2003] Aurélien Géron et Fatmé Tawbi (préf. Gilles Clavel), Pour mieux développer avec C++ : Design patterns, STL, RTTI et smart pointers, Paris, Dunod, 2003, 188 p. (ISBN 978-2-10-007348-1).[Guidet 2008] Alexandre Guidet, Programmation objet en langage C++, Paris, Ellipses, coll. « Cours et exercices. », 2008, 364 p. (ISBN 978-2-7298-3693-1, OCLC 221607125, BNF 41206426).[Hubbard 2002] J. R. Hubbard (trad. Virginie Maréchal), C++ [« Schaum's easy outline of programming with C++ »], Paris, EdiScience, coll. « Mini Schaum's », 2002, 192 p. (ISBN 978-2-10-006510-3).[Liberty et Jones 2005] Jesse Liberty et Bradley Jones (trad. Nathalie Le Guillou de Penanros), Le langage C++ [« Teach yourself C++ in 21 days »], Paris, CampusPress, 2005, 859 p. (ISBN 978-2-7440-1928-9).[Stephens, Diggins, Turkanis et al. 2006] D. Ryan Stephens, Christopher Diggins, Jonathan Turkanis et J. Cogswell (trad. Yves Baily & Dalil Djidel), C++ en action [« C++ Cookbook - Solutions and Examples for C++ Programmers »], Paris, O'Reilly, 2006, 555 p. (ISBN 978-2-84177-407-4, OCLC 717532188, BNF 40170870).[Stroustrup 2012] Bjarne Stroustrup (trad. Marie-Cécile Baland, Emmanuelle Burr, Christine Eberhardt), Programmation : principes et pratique avec C++ : Avec plus de 1000 exercices. [« Programming : principles and practice using C++ »], Paris, Pearson education, 2012, 944 p. (ISBN 978-2-7440-7718-0).[Stroustrup 2003] Bjarne Stroustrup (trad. Christine Eberhardt), Le langage C++ [« The C++ programming language »], Paris, Pearson education, 2003, 1098 p. (ISBN 978-2-7440-7003-7 et 2-744-07003-3).[Sutter et Alexandrescu 2005] Herb Sutter et Andrei Alexandrescu, Standards de programmation C [« C++ Coding Standards: 101 Rules, Guidelines, and Best Practices »], Paris, Pearson Education France, coll. « C++ », 2005, 243 p. (ISBN 978-2-7440-7144-7 et 2-744-07144-7).Langage C(en) Le Comité du Standard C++(fr) Documentation du C++ (wiki sous double licence CC-BY-SA et GFDL)(en) Documentation du C++ (le même wiki mais en anglais et plus complet)(en) Documentation du C++ (contenu non libre, édité par The C++ ressources network) Portail de la programmation informatique   Portail de l’informatique"
Informatique;"Fortran (mathematical FORmula TRANslating system) est un langage de programmation généraliste dont le domaine de prédilection est le calcul scientifique et le calcul numérique. Il est utilisé aussi bien sur ordinateur personnel que sur les superordinateurs, où il sert d'ailleurs à tester leurs performances dans le cadre du classement TOP500 des superordinateurs les plus puissants au monde, entre autres grâce à la bibliothèque LINPACK.Le nombre de bibliothèques scientifiques écrites en Fortran, éprouvées et améliorées pendant de longues années, et les efforts continus consacrés aux compilateurs pour exploiter au fil des décennies les nouvelles possibilités des calculateurs (vectorisation, coprocesseurs, parallélisme) ont maintenu l'usage de ce langage qui ne cesse d'évoluer.Parmi les fonctionnalités ajoutées ces dernières décennies, on citera le calcul sur les tableaux (qui peuvent comporter jusqu'à quinze dimensions), la programmation modulaire, la programmation générique (Fortran 90), le calcul haute performance (Fortran 95), la programmation orientée objet et l'interopérabilité avec les bibliothèques du langage C (Fortran 2003), la programmation concurrente et le calcul parallèle à l'aide des cotableaux (Fortran 2008), des équipes, des évènements et des sous-routines collectives (Fortran 2018), en plus des interfaces OpenMP, OpenACC et de la bibliothèque Message Passing Interface. La prochaine norme sera Fortran 2023. Les discussions sur le contenu de la suivante, Fortran 202y, ont commencé.Projet lancé en 1954 et aboutissant à une première version en 1957, Fortran est le premier langage de programmation de haut niveau, suivi notamment par Lisp (1958), Algol (1958) et COBOL (1959). Il est le premier langage à être normalisé, au milieu des années 60, et est devenu une norme ISO depuis Fortran 90.Le nom du langage est généralement écrit en majuscules (FORTRAN) pour désigner les versions du langage antérieures à la norme Fortran 90 car à l'époque les lettres minuscules ne font pas partie du jeu de caractères du langage. Par contre, il est toujours écrit avec une majuscule à partir de Fortran 90. Enfin, depuis environ 2010 les titres des livres en anglais utilisent souvent l'expression modern Fortran (Fortran moderne) pour distinguer la forme actuelle du langage de ses formes historiques.1953 : John Backus, jeune ingénieur titulaire d'une maîtrise de mathématiques recruté en 1950 chez IBM, développe pour l'IBM 701 le système Speedcoding, un interpréteur qui facilite la programmation en particulier pour le calcul en virgule flottante. En décembre, il rédige une lettre à l'attention de son supérieur Cuthbert Hurd (en) pour lui proposer le projet FORTRAN destiné à l'IBM 704, première machine commerciale dont le processeur supporte directement les nombres en virgule flottante. L'objectif est d'accélérer considérablement le développement et le débogage des programmes, jusqu'alors écrits en langage machine, afin de réduire leur coût d'exploitation, qui pour moitié provient des salaires des informaticiens et pour moitié des machines.1954 : le groupe de recherche de Backus, le « Programming Research Group » basé à New York, rédige un rapport intitulé Preliminary Report, Specifications for the IBM Mathematical FORmula TRANslating System, FORTRAN, daté du 10 novembre 1954. Il faut encore deux ans d'efforts pour terminer le premier compilateur FORTRAN (25 000 lignes), désigné alors par le mot anglais translator (traducteur). Dès le départ, ce compilateur est conçu pour fournir un code très optimisé, en particulier pour le calcul sur les tableaux et le traitement des boucles imbriquées, quasiment aussi rapide que celui qu'aurait écrit un programmeur en langage machine (objectif alors accueilli avec scepticisme par les clients d'IBM). D'après Backus lui-même, le développement du langage ne peut pas être séparé de la conception du compilateur et c'est même sur le compilateur que porte l'essentiel de l'effort initial. Le langage est défini au fur et à mesure, avec comme guide principal la simplicité de la syntaxe.1957 : le compilateur FORTRAN est déployé courant avril sur tous les IBM 704, sur bande magnétique, avec son manuel intitulé Preliminary Operator's Manual. Fin 1957, un manuel plus complet, le Programmer's Primer rédigé par Grace E. Mitchell, est édité. FORTRAN est un succès et une révolution car il n'est plus nécessaire d'être un expert de l'ordinateur pour écrire et déboguer des programmes. Mary Tsingou, physicienne et mathématicienne au Los Alamos National Laboratory et qui travailla avec Fermi, Pasta et Ulam, dira ainsi : « Quand le Fortran est arrivé, c'était presque comme le paradis ». L'instruction GO TO permet de sauter à une ligne numérotée par une étiquette. Le IF de cette première version est arithmétique : IF (A-B) 10, 20, 30 permet de sauter aux instructions d'étiquettes 10, 20 ou 30 selon que l'expression A-B est négative, nulle ou positive.1958 : FORTRAN II,, apporte les fonctions FUNCTION et les sous-programmes SUBROUTINE que l’on appelle par l’instruction CALL, ce qui permet aux programmeurs de se répartir plus facilement le travail. L'instruction COMMON permet à plusieurs sous-programmes de partager des données communes. Le nouveau compilateur est également plus rapide que le compilateur Fortran I : il permet en particulier de découper un long programme en plusieurs parties pouvant être compilées indépendamment.1959 : FORTRAN III n'est déployé que sur une vingtaine de machines. Il est possible d'insérer des routines en langage assembleur symbolique dans le code mais cette fonctionnalité sera abandonnée car elle compromettrait la portabilité des programmes écrits en FORTRAN. Le groupe de Backus n'est plus chargé du développement du FORTRAN, activité transférée à l'Applied Programming Department. Backus préconise le développement de deux compilateurs : un compilateur rapide pour la phase de débogage et un compilateur optimiseur pour le programme final. Mais l'idée n'est pas suivie.1960 : FORTRAN devient l'un des premiers langages multi-plateforme, des compilateurs devenant disponibles sur quelques machines d'autres constructeurs qu'IBM.1962 : FORTRAN IV introduit les nombres réels double précision, les complexes et les booléens, ainsi que les opérateurs .AND., .OR. et .NOT.. Le IF logique permet d'écrire par exemple IF (A .GE. B) GOTO 10 (aller à 10 si A est supérieur ou égal à B). Le type des variables peut désormais être déclaré explicitement. Il est nécessaire de modifier les programmes écrits en FORTRAN II : le traducteur automatique SIFT (SHARE Internal FORTRAN Translator) est mis à disposition. Cette même année, chaque compilateur apportant ses extensions et variantes, un comité des normes FORTRAN est formé afin de normaliser le langage pour qu'il soit portable d'une machine à l'autre.1965 : ECMA-9 FORTRAN est la première norme FORTRAN, publiée en avril 1965 par l'ECMA (European Computer Manufacturers Association) dans le cadre d'une collaboration avec l'ANSI (American National Standards Institute). Il s'agit en fait d'une version du langage intermédiaire entre les deux niveaux du langage définis dans la norme FORTRAN 66.1966 : FORTRAN 66 (ANSI X3.9-1966) est la norme développée par l'ANSI, essentiellement basée sur FORTRAN IV. Elle définit en fait deux niveaux du langage : le FORTRAN proprement dit et une version simplifiée, le Basic FORTRAN. Le langage FORTRAN est le premier langage de programmation à avoir été normalisé.1972 : l'ISO publie l'ISO Recommendation for Fortran (R1539), constituée des deux niveaux du langage définis dans FORTRAN 66 et du niveau intermédiaire défini par l'ECMA FORTRAN. Mais il ne s'agit que d'une recommandation et il faudra attendre Fortran 90 pour que le langage devienne une norme ISO.1977 : John Backus reçoit le Prix Turing pour « ses contributions profondes, influentes et durables à la conception de systèmes de programmation pratiques de haut niveau, notamment par ses travaux sur le FORTRAN, et pour ses publications pionnières sur les procédures formelles pour la spécification des langages de programmation. »1978 : FORTRAN 77 (ANSI X3.9-1978), est une évolution majeure. Comme pour FORTRAN 66, la norme définit deux niveaux du langage : le FORTRAN complet, ou full language, et une version simplifiée, ou subset language. Cette norme inclut en particulier des extensions au langage introduites par les différents compilateurs depuis FORTRAN 66. Elle apporte, entre autres améliorations, la programmation structurée avec les blocs IF / THEN / ELSE / END IF, le type de données CHARACTER en remplacement des constantes d'Hollerith (en) (qui sont supprimées de la norme), les fonctions LGE, LGT, LLE, LLT pour la comparaison des chaînes de caractères, l'attribut PARAMETER pour déclarer des constantes, l'attribut SAVE pour la persistance des variables locales, etc. Fin 1978, l'extension MIL-STD-1753 du département de la Défense américain introduit entre autres le END DO en FORTRAN 77 (bien que le label final reste obligatoire), les blocks DO WHILE / END DO, l'instruction INCLUDE, l'instruction IMPLICIT NONE et des fonctions pour manipuler les bits des entiers.1991 : Fortran 90 (ISO/IEC 1539:1991, puis ANSI X3.198-1992) est une version majeure ayant pour objectif de mettre Fortran au niveau des autres langages modernes. La norme apporte en particulier les modules, la récursivité, les arguments optionnels et nommés, la surcharge des opérateurs, une syntaxe pour le calcul sur les tableaux, l'allocation dynamique des tableaux grâce à l'attribut ALLOCATABLE, les types dérivés, l'attribut POINTER, l'instruction IMPLICIT NONE pour rendre obligatoire la déclaration des variables, les structures de contrôle SELECT CASE, les procédures SYSTEM_CLOCK et DATE_AND_TIME pour accéder à l'horloge du système, etc. Les restrictions concernant la mise en forme des programmes (colonnes 1 à 72) disparaissent : l'écriture se fait en format libre. Afin de rester compatible avec les nombreux codes industriels écrits en FORTRAN (Nastran, bibliothèques NAG et IMSL, etc.), Fortran 90 est conçu de telle façon que FORTRAN 77 en constitue un sous-ensemble.1992 : IEEE 1003.9-1992, volet FORTRAN 77 de la norme POSIX.1994 : ISO/IEC 1539-2:1994, qui définit des chaînes de caractères de longueur variable. Cette norme a été révisée en 2000.1997 : Fortran 95 (ISO/CEI 1539-1:1997) : quoique mise à jour mineure, cette norme introduit en particulier les instructions FORALL et WHERE pour le calcul vectoriel, les procédures PURE et ELEMENTAL et rend obsolescentes certaines fonctionnalités telles que les boucles à compteur réel ou l'instruction PAUSE. La procédure CPU_TIME permet de mesurer le temps processeur utilisé par un segment de programme.1999 : ISO/IEC 1539-3:1999, qui définit des directives de compilation conditionnelle. Cette norme a été révisée en 2011.2004 : Fortran 2003 (ISO/CEI 1539-1:2004) est une révision majeure qui supporte la programmation orientée objet. L'interface avec le langage C est assurée par le module interne ISO_C_BINDING et les mots-clés BIND et VALUE, qui permettent à un programme Fortran d'accéder facilement aux bibliothèques disponibles en C. Les pointeurs de procédure permettent de choisir lors de l'exécution une procédure à exécuter. Les types dérivés sont améliorés, ainsi que les entrées/sorties. On peut désormais gérer les exceptions en calcul flottant de la norme IEEE 754. La norme apporte également la gestion des caractères ISO 10646, base de l'Unicode. L'intégration avec le système d'exploitation est améliorée avec l'introduction des instructions get_command_argument, get_command, et command_argument_count.2010 : Fortran 2008 (ISO/CEI 1539-1:2010),, initialement pensée comme une révision mineure, introduit finalement les co-tableaux (co-arrays) comme paradigme de programmation parallèle. Les traitements sur ces co-tableaux sont effectués par des images (instances parallèles d'un programme Fortran). Cette norme introduit également les boucles DO CONCURRENT pour la parallélisation des itérations sans interdépendance. Les modules peuvent désormais comporter des sous-modules. Et les structures BLOCK...END BLOCK permettent de déclarer des variables à portée limitée n'importe où à l'intérieur d'une routine. La modularité est améliorée par l'introduction des SUBMODULE. De nouvelles procédures intrinsèques sont introduites pour la gestion des bits. De nouvelles constantes sont ajoutées au module ISO_FORTRAN_ENV, en particulier les KIND des types d'entiers INT8, INT16, INT32, INT64 et de réels REAL32, REAL64, REAL128.2018 : Fortran 2018 (ISO/CEI 1539-1:2018), considérée comme une révision mineure, introduit en particulier :ISO/IEC TS 29113:2012 Interopérabilité ultérieure de Fortran avec CISO/IEC TS 18508:2015 Caractéristiques parallèles supplémentaires en Fortran : les images peuvent désormais être regroupées en équipes (teams) travaillant sur des tâches différentes. Avec les événements (events), une image peut poster un évènement à destination d'autres images, ou attendre de recevoir un évènement. Les sous-routines collectives (collective subroutines) permettent d'effectuer des tâches simples sur les résultats d'un ensemble d'images, par exemple calculer la somme des valeurs d'une variable dans les différentes images.ISO/IEC/IEEE 60559:2011 Systèmes de microprocesseurs — Arithmétique flottante2023 : le planning pour la norme Fortran 2023, initialement dénommée Fortran 202x, a démarré en juin 2017. Le document de travail du comité est disponible. La publication de la norme est prévue en juillet 2023. Il s'agit d'une version mineure qui apporte de nombreuses améliorations à diverses parties du langage. Par exemple, la longueur maximale des lignes de programme passera de 132 à 10000 caractères, et les expressions conditionnelles, reprenant la syntaxe du C, font leur apparition.Fortran fait partie des langages normalisés depuis 1965 et est devenu une norme ISO depuis Fortran 90. La norme Fortran est gérée par le groupe de travail ISO/IEC JTC1/SC22/WG5, généralement simplement appelé WG5 (pour Working Group 5), qui charge le comité Fortran US INCITS PL22.3 (généralement appelé J3, en référence à son ancien nom ANSI X3J3) de développer le langage. Le WG5 est composé d'experts chargés de faire des recommandations pour faire évoluer le langage. Le J3 est composé de fabricants de matériel, d'éditeurs de compilateurs, d'utilisateurs issus aussi bien de l'industrie que du monde académique.Chaque révision de la norme peut ajouter de nouveaux paradigmes ou fonctionnalités, éventuellement déjà implémentées par les compilateurs sous forme d'extensions au langage, clarifier des points restés ambigus, mais aussi rendre obsolescentes d'anciennes fonctionnalités. En effet, depuis Fortran 90, les normes comportent systématiquement en annexes une liste des fonctionnalités supprimées et une liste des fonctionnalités obsolètes et donc susceptibles d'être supprimées dans une prochaine révision de la norme. Les compilateurs continuent néanmoins généralement de supporter ces fonctionnalités pour assurer la pérennité des codes déjà développés. Enfin, il s'écoule généralement plusieurs années entre la publication d'une nouvelle norme et la prise en charge intégrale de ses nouvelles fonctionnalités dans les compilateurs,.En 1995, le WG5 met en place des rapports techniques de type 2 pour travailler sur des fonctionnalités importantes qui n'auront pas le temps d'être intégrées à la norme en cours de rédaction, mais en constitueront une extension qui pourra être intégrée dans la norme suivante.En 2019, un dépôt GitHub est créé afin que tous les utilisateurs du langage puissent proposer facilement au comité J3 des évolutions pour les normes futures. Il sert actuellement à proposer des nouveautés pour la norme Fortran 202Y qui succédera à Fortran 2023, comme par exemple l'amélioration de la programmation générique, des valeurs par défaut pour les arguments optionnels, etc.En 1955, IBM crée le groupe d'utilisateurs SHARE afin que ses clients puissent échanger entre eux. John Backus y fait des présentations régulières durant le développement du premier compilateur FORTRAN. En avril 1957, des ingénieurs de Westinghouse y rapportent la compilation du premier programme FORTRAN en dehors d'IBM. Alors que le langage se diffuse, un sous-groupe SHARE y est dédié : le FORTRAN Standard Commitee.Créé en juillet 1982, le bulletin mensuel FORTRAN Forum a été publié par le SIGPLan (Special Interest Group in Programming Languages) de l'ACM (Association for Computing Machinery) trois fois par an, jusqu'en avril 2020.Le 29 novembre 1983, un groupe de discussion net.lang.f77 est créé sur  Usenet. Le 7 novembre 1986, il est renommé comp.lang.fortran et est toujours l'un des principaux canaux de communication de la communauté Fortran.La liste de diffusion comp-fortran-90 est dédiée aux questions concernant le Fortran à partir de la norme Fortran 90. On peut en consulter les archives jusque 1997, mais l'activité y est désormais très réduite avec seulement six messages postés en 2020.Un groupe Fortran Programmers est créé sur LinkedIn en juillet 2008.Le site Fortran Wiki est créé en octobre 2008. Il est édité par les utilisateurs du langage et propose de nombreuses ressources.Début 2020, une nouvelle communauté d'utilisateurs fortran-lang.org est créée, afin de fédérer les efforts dans l'écosystème Fortran, sur le modèle de langages plus jeunes. En s'appuyant sur GitHub, elle développe en particulier une bibliothèque standard Fortran (stdlib) similaire à celle du C, un gestionnaire de paquets Fortran (fpm) faisant également office de système de compilation, le compilateur interactif LFortran, ainsi que des tutoriels pour apprendre le Fortran moderne. Certaines pages sont traduites en français. La communauté édite une lettre mensuelle résumant ses activités en cours et diffuse des informations sur Twitter. Une visioconférence mensuelle permet à ses membres de discuter des projets à mener. Son forum Fortran Discourse est devenu un lieu central de discussion pour la communauté Fortran. Les projets de la communauté ont reçu l'aide de cinq étudiants lors du Google Summer of Code 2021.L'International Fortran Conference (FortranCon) est créée en 2020. Initialement prévue à Zurich début juillet 2020, elle a lieu en visioconférence à cause de la pandémie de Covid-19. La seconde édition, FortranCon 2021, a également lieu en visioconférence les 23 et 24 septembre 2021. Les vidéos des conférences sont disponibles sur la chaîne YouTube FortranCon. La prochaine édition est prévue pour 2023, un rythme d'environ dix-huit mois ayant été choisi.Fortran est toujours l'un des langages les plus utilisés pour le calcul intensif, que ce soit pour l'astronomie, la modélisation climatique, la modélisation chimique, la modélisation en économie, la mécanique des fluides numérique, la physique numérique, l'analyse de données, la modélisation hydrologique, l'algèbre linéaire numérique et les bibliothèques numériques (LAPACK, IMSL et NAG), l'optimisation, la simulation de satellites, l'ingénierie des structures et les prévisions météorologiques. Les calculs peuvent aussi bien être réalisés sur des ordinateurs de bureau que sur des supercalculateurs.De nombreux tests de performance (benchmarks) destinés à évaluer les performances des nouveaux microprocesseurs sont écrits en Fortran.Avant la norme Fortran 90, le FORTRAN, créé à l'époque des cartes perforées (en particulier avec le système FMS), utilise une mise en page adaptée à ces supports :la colonne 1 peut contenir la lettre C indiquant un commentaire. Le caractère * est aussi accepté ;les colonnes 1 à 5 peuvent contenir une étiquette numérique (facultative) de l'instruction, dont la valeur peut être limitée à 32 767 ou 9 999 suivant le compilateur ;la colonne 6 indique une suite de l'instruction précédente ;le code commence à partir de la 7e colonne et ne doit pas dépasser la 72e. Les espaces n'ont pas de signification dans ces colonnes : une boucle DO 10 I=1,5 peut aussi s'écrire DO10I=1,5 (le 10 est ici l'étiquette obligatoire de fin de boucle) ;les colonnes 73 à 80 servent à l'identification et la numérotation des cartes perforées (souvent les trois initiales du projet, du chef de projet ou du programmeur, suivies de numéros de cinq chiffres attribués de dix en dix pour permettre des insertions de dernière minute).Les extensions de fichiers les plus courantes pour le format fixe sont .f et .for, mais ce n'est qu'une convention adoptée par la plupart des compilateurs. Rien n'empêche de les utiliser avec le format libre à condition d'en avertir le compilateur à l'aide de l'option adéquate.Depuis la norme Fortran 90, le code source est écrit suivant un format dit libre : il n'y a plus de colonne particulière, les lignes font au maximum 132 caractères (mais elles peuvent être continuées à l'aide du caractère &), les commentaires sont introduits par un point d'exclamation (éventuellement disposé à la suite d'une instruction Fortran). L'extension de nom de fichier la plus courante est alors .f90, même si le programme utilise des fonctionnalités de normes plus récentes telles que Fortran 2018.Notes :ce programme est écrit en Fortran moderne. Il nécessite un compilateur implémentant les bases de la norme Fortran 2008 ;l'instruction use permet d'importer le module intrinsèque iso_fortran_env qui définit des constantes, en particulier pour les types de réels disponibles (real32, real64, real128). Ici seule est importée la constante real128 qui sera désignée par l'alias wp (working precision). Les nombres réels apparaissant dans le programme sont suffixés par cet alias afin de définir leur type. Il suffirait ainsi de remplacer real128 par real64 ou real32 pour modifier de façon cohérente la précision numérique utilisée dans l'ensemble du programme, pour autant que le compilateur prenne en charge la précision correspondante ;l'instruction implicit none, introduite dans la norme Fortran 90, rend la déclaration des variables obligatoire. Historiquement, celle-ci est en effet facultative : les variables dont le nom commence par une des lettres I, J, K, L, M ou N sont par défaut de type integer, les autres de type real. Ce mécanisme est désormais fortement déconseillé et l'instruction implicit none doit donc être systématiquement utilisée ;la déclaration se fait en début de routine. Le type de données et les noms de variables sont séparés par ::. On utilise les entiers par défaut pour deg. La numérotation des tableaux commence par défaut à 1 en Fortran mais ici on la fait commencer à 0 pour le tableau radians(0:90). Les constantes, qui peuvent être calculées, sont spécifiées par l'attribut parameter. Les chaînes de caractères sont de longueur fixe, mais ici l'étoile indique que la longueur de la chaîne doit être définie en fonction de la longueur de son contenu ;le contenu du tableau radians() est calculé à l'aide d'un constructeur, avec deg pour variable de boucle variant de 0 à 90. Le Fortran permettant de calculer directement sur des tableaux, on aurait pu écrire également radians = coeff * [ (deg, deg=0,90) ] ;l'instruction write se réfère à une unité d'entrée-sortie (* désigne le terminal) et une spécification de format. Ce format est ici stocké dans la chaîne ligne_horizontale et décrit qu'il faudra afficher 49 tirets. On aurait également pu utiliser une déclaration format située sur une autre ligne et précédée d'un label numérique ;l'instruction do deg = 0, 90 indique de répéter en boucle les instructions qui suivent (jusqu'au end do) pour toutes les valeurs de deg de 0 à 90 par pas de 1 ;le write à l'intérieur de la boucle permet d'écrire sur le terminal les valeurs des variables deg et radians(deg) en utilisant deux caractères pour deg (qui est un entier) et 34 caractères dont 32 après la virgule pour radians(deg) qui est un réel.De nombreux compilateurs commerciaux ou libres sont disponibles. Compilateurs libres Avant sa version 4.0, le compilateur libre GCC incluait le compilateur g77 pour le FORTRAN 77, qui a été remplacé en 2005, par le compilateur GFortran, lui-même issu d'un fork réalisé en 2003 de G95, autre compilateur libre développé entre 2000 et janvier 2013. En septembre 2019, GFortran prend en charge quasiment intégralement Fortran 2003, presque tout Fortran 2008 et environ 20 % de Fortran 2018. Contrairement aux compilateurs Intel et Cray, il ne gère pas encore de façon native la programmation parallèle avec les co-tableaux mais nécessite l'installation de la bibliothèque OpenCoarrays.Omni Compiler est un méta-compilateur C et Fortran destiné à transformer du code contenant des directives XcalableMP et OpenACC en code parallèle natif. Compilateurs propriétaires On trouve de nombreux compilateurs commerciaux, parmi lesquels : Lahey, Absoft, Portland Group (en) (filiale de NVidia), NAG, etc. La plupart des fabricants de stations de travail ou d'ordinateurs destinés au calcul intensif, proposent également un compilateur de Fortran : Intel, IBM, Oracle (à la suite du rachat de Sun Microsystems), HPE Cray (Cray a été racheté par HP en 2019), etc. Le compilateur Intel Visual Fortran est l'héritier de DEC Visual Fortran, devenu Compaq Visual Fortran puis HP Visual Fortran.Certains de ces compilateurs commerciaux ont des versions gratuites pour une utilisation non commerciale : c'est le cas d'Oracle, Portland Group. On peut télécharger le compilateur BiSheng que Huawei a développé pour sa plateforme Kunpeng : pour le Fortran, il utilise Flang en frontal.Quant aux compilateurs Intel, ils sont depuis janvier 2021 gratuits pour tous les développeurs. Compilateurs en cours de développement Début 2021, de nouveaux compilateurs Fortran basés sur LLVM sont en développement : LFortran qui vise à permettre de tester du code de façon interactive, Flang et Intel Fortran Compiler ifx, le successeur d'ifort. Compilateurs en ligne Le Fortran fait partie des langages proposés par certains compilateurs en ligne, tels que codingground, OnlineGDB, JDOODLE et godbolt Compiler Explorer. Certains proposent le choix entre plusieurs compilateurs Fortran ou plusieurs versions d'un compilateur, ou permettent d'analyser le code avec un débogueur et de voir le langage machine généré. Ces outils permettent donc d'apprendre le langage sans installer de compilateur sur sa machine, ils permettent également de collaborer en ligne sur un projet.Open64 (en) est un compilateur Fortran 95 libre arrêté en 2011.Compilateur FORTRAN 77 libre Open Watcom : arrêté en 2010.PathScale (en) : arrêté en 2013.HP.Unisys.Certains fabricants ont disparu, tels que CDC ou DEC.La plupart des éditeurs de texte offrent une coloration syntaxique pour le Fortran : Emacs, Notepad++, Sublime Text, Vim, Neovim, Visual Studio Code... Parmi les environnements de développement intégrés, il existe une version de Code::Blocks dédiée au développement en Fortran. Il existe également une version d'Eclipse dédiée au Fortran, nommée Photran, mais dont la dernière version date de 2015. Sous Windows, le compilateur Intel Fortran est intégré à Visual Studio.Les systèmes de construction de projet tels que CMake et Meson gèrent généralement le Fortran. L'utilitaire makedepf90 permet de générer des fichiers Make pour un projet Fortran.Mais Fortran dispose depuis fin 2020 du gestionnaire de paquets fpm (Fortran Package Manager), qui fait office également de système de construction de projet. Il est inspiré par l'outil Cargo du langage Rust. Il permet en particulier de gérer les dépendances, qu'il peut télécharger automatiquement depuis GitHub. Et il est lui-même écrit en Fortran.fpt est un outil d'analyse de code Fortran. On peut également citer CamFort, un projet universitaire libre, ftncheck, qui est limité au FORTRAN 77 et qui n'est plus développé depuis 2005. findent est un indenteur de code source qui peut également transformer l'ancien format fixe du Fortran en format libre.Plusieurs analyseurs de type lint sont disponibles : Flint et fortran-linter, logiciels libres écrits en Python, et FortranLint, produit commercial.Lizard est un analyseur de complexité cyclomatique qui prend en charge une vingtaine de langages, dont le Fortran.Pour le débogage, on peut par exemple utiliser les débogueurs GNU gdb ou idb (Intel Debugger). Pour le profilage sous système de type UNIX, on peut utiliser gprof et Valgrind. La couverture de code peut être évaluée avec Gcov (en).Fypp est un préprocesseur Python qui peut être utilisé avec n'importe quel langage, tout en étant avant tout destiné au langage Fortran. Il utilise la syntaxe du Python.PFUnit (en) est un framework de tests unitaires, libre et initialement développé par la NASA. Le framework test-drive est utilisé dans les projets fpm et stdlib, et il prend en charge Meson, CMake et fpm. On peut également citer vegetables et FRUIT.Parmi les générateurs de documentation gérant le Fortran, on peut citer Doxygen, FORD et ROBODoc.Quickstart Fortran permet d'installer facilement sous Microsoft Windows, sans nécessiter les droits d’administration, les outils essentiels pour développer en Fortran : GCC-GFortran, Fortran Package Manager, Git for Windows, OpenBLAS (BLAS/LAPACK), GNU make. Il peut également faciliter l'installation d'Intel OneAPI et de la librairie stdlib en cours de développement.De nombreuses bibliothèques de calcul ont été développées en Fortran. Certaines sont développées, utilisées, testées et donc déboguées depuis des décennies, ce qui leur assure une très grande fiabilité.LAPACK (Linear Algebra Package) est une bibliothèque dédiée comme son nom l'indique à l'algèbre linéaire numérique.Basic Linear Algebra Subprograms (BLAS) est un ensemble de fonctions standardisées réalisant des opérations de base de l'algèbre linéaire.LINPACK est une bibliothèque de fonctions pour l'algèbre linéaire, et notamment la résolution numérique de systèmes d'équations linéaires.Physics Analysis Workstation (PAW).International Mathematics and Statistics Library (IMSL) est une bibliothèque logicielle d'objets utilisables pour le développement informatique d'applications d'analyse numérique.NAG Fortran Library propose plus de 1700 routines mathématiques et statistiques.SLATEC est une bibliothèque dans le domaine public proposant 1400 fonctions mathématiques, initalement développée en Fortran 77. Le code source a été adapté en Fortran moderne avec une version 4.2 datée de 2019....Les normes Fortran n'incluant pas d'instructions graphiques ou d'instructions pour construire des interfaces graphiques, la visualisation peut se faire après exécution avec des outils externes tels que ParaView, ou en appelant des outils de tracé tels que Gnuplot via l'instruction EXECUTE_COMMAND_LINE(), ou enfin à l'aide de bibliothèques :DISLIN, créé par le Max Planck Institute for Solar System Research, permet de tracer des données et de réaliser des interfaces graphiques. Multiplate-formes (UNIX, Linux, FreeBSD, OpenVMS, Windows et MS-DOS). Fonctionne avec de nombreux compilateurs, ainsi que d'autres langages que le Fortran. Téléchargeable gratuitement depuis la version 11.3 de mars 2020.GINO permet de tracer des données et de réaliser des interfaces graphiques. Logiciel commercial pour Windows et Linux.GrWin Graphics Library : logiciel libre pour Windows.gtk-fortran est une bibliothèque sous licence libre GPL 3 permettant de créer des interfaces graphiques GTK en Fortran, grâce aux fonctionnalités d'interopérabilité Fortran / C introduites depuis la norme Fortran 2003. Multi-plateforme (Linux, Windows, macOS, FreeBSD, Raspberry Pi...). Supporte GTK 4, la bibliothèque généraliste GLib et la bibliothèque PLplot. Peut être utilisée comme dépendance fpm.JAPI (Java Application Programming Interface) : interface Java/Fortran permettant de créer une interface graphique complète pour les programmes Fortran. Multiplate-formes (Windows, Linux, Solaris). Fonctionne avec de nombreux compilateurs (entre autres gfortran, Compaq Visual Fortran…). Logiciel libre sous licence LGPL.MATFOR : construction d'interfaces graphiques et bibliothèques numériques et graphiques pour Fortran et d'autres langages. Logiciel commercial pour Windows et Linux.ogpf permet d'accéder facilement à gnuplot depuis un programme Fortran, grâce à l'utilisation de la programmation orientée objet. Il peut être utilisé comme paquet fpm.PLplot (en) : bibliothèque permettant de dessiner des graphiques scientifiques. Multilangage et multiplate-formes (Linux, OpenSolaris, Unix, MS-DOS, Windows, Mac OS X). Logiciel libre sous licence LGPL.Quickwin : bibliothèque graphique fournie avec le Compaq Visual Fortran (désormais Intel Visual Fortran). Ne fonctionne que sous Windows.Winteracter : construction d'interfaces graphiques et d'outils de visualisation. Log"
Informatique;Une page web dynamique est une page web générée à la demande, par opposition à une page web statique. Le contenu d'une page web dynamique peut donc varier en fonction d'informations (heure, nom de l'utilisateur, formulaire rempli par l'utilisateur, etc.) qui ne sont connues qu'au moment de sa consultation. À l'inverse, le contenu d'une page web statique est a priori identique à chaque consultation.Lors de la consultation d'une page web statique, un serveur HTTP renvoie le contenu du fichier où la page est enregistrée.Lors de la consultation d'une page web dynamique, un serveur HTTP transmet la requête au logiciel correspondant à la requête, et le logiciel se charge de générer et envoyer le contenu de la page. La programmation web est le domaine de l'ingénierie informatique consacré au développement de tels logiciels. Les logiciels générant des pages web dynamiques sont fréquemment écrits avec les langages PHP, JavaServer Pages (JSP) ou Active Server Pages (ASP).Un site web dynamique peut ainsi fournir des informations aux utilisateurs en fonction de leur navigation sur celui-ci. Deux utilisateurs peuvent accéder simultanément à la même page web sans pour autant avoir le même contenu affiché à l'écran.Avec un site web dynamique, des modifications effectuées, par exemple via un système de soumission de commentaire ou bien une interface privée de gestion du site, pourront être directement visibles sur le site.Un site web dynamique peut permettre la mise en œuvre de différentes fonctionnalités, par exemples :- un système de gestion des accès, granulaire ou non, à certaines parties d'un site (administration du site, comptes utilisateurs, etc),- un système de soumission de commentaires publiques.En 2004, Le Journal du Net consacrait un article comparant les avantages des technologies statiques et dynamiques.Web profondProgrammation webFeuilles de style dynamiques en cascade Portail de l’informatique   Portail d’Internet
Informatique;"PHP: Hypertext Preprocessor, plus connu sous son sigle PHP (sigle auto-référentiel), est un langage de programmation libre, principalement utilisé pour produire des pages Web dynamiques via un serveur HTTP, mais pouvant également fonctionner comme n'importe quel langage interprété de façon locale. PHP est un langage impératif orienté objet.PHP a permis de créer un grand nombre de sites web célèbres, comme Facebook et Wikipédia. Il est considéré comme une des bases de la création de sites web dits dynamiques mais également des applications web.PHP est un langage de script utilisé le plus souvent côté serveur : dans cette architecture, le serveur interprète le code PHP des pages web demandées et génère du code (HTML, XHTML, CSS par exemple) et des données (JPEG, GIF, PNG par exemple) pouvant être interprétés et rendus par un navigateur web. PHP peut également générer d'autres formats comme le WML, le SVG et le PDF.Il a été conçu pour permettre la création d'applications dynamiques, le plus souvent développées pour le Web. PHP est le plus souvent couplé à un serveur Apache bien qu'il puisse être installé sur la plupart des serveurs HTTP tels que IIS ou nginx. Ce couplage permet de récupérer des informations issues d'une base de données, d'un système de fichiers (contenu de fichiers et de l'arborescence) ou plus simplement des données envoyées par le navigateur afin d'être interprétées ou stockées pour une utilisation ultérieure.C'est un langage peu typé et souple et donc facile à apprendre par un débutant mais, de ce fait, des failles de sécurité peuvent rapidement apparaître dans les applications. Pragmatique, PHP ne s'encombre pas de théorie et a tendance à choisir le chemin le plus direct. Néanmoins, le nom des fonctions (ainsi que le passage des arguments) ne respecte pas toujours une logique uniforme, ce qui peut être préjudiciable à l'apprentissage.Son utilisation commence avec le traitement des formulaires puis par l'accès aux bases de données. L'accès aux bases de données est aisé une fois l'installation des modules correspondants effectuée sur le serveur. La force la plus évidente de ce langage est qu'il a permis au fil du temps la résolution aisée de problèmes autrefois compliqués et est devenu par conséquent un composant incontournable des offres d'hébergements.Il est multi-plateforme : autant sur Linux qu'avec Windows il permet aisément de reconduire le même code sur un environnement à peu près semblable (quoiqu'il faille prendre en compte les règles d'arborescences de répertoires, qui peuvent changer).Libre, gratuit, simple d'utilisation et d'installation, ce langage nécessite comme tout langage de programmation une bonne compréhension des principales fonctions usuelles ainsi qu'une connaissance aiguë des problèmes de sécurité liés à ce langage.La version 5.3 a introduit de nombreuses fonctions nouvelles : les espaces de noms (Namespace) — un élément fondamental de l'élaboration d'extensions, de bibliothèques et de frameworks structurés, les fonctions anonymes, les fermetures, etc.En 2018, près de 80 % des sites web utilisent le langage PHP sous ses différentes versions.Le langage PHP fait l'objet, depuis plusieurs années maintenant, de rassemblements nationaux organisés par l'AFUP (l'Association Française des Utilisateurs de PHP), où experts de la programmation et du milieu se retrouvent pour échanger autour du PHP et de ses développeurs. L'association organise ainsi deux évènements majeurs : le « Forum PHP », habituellement en fin d'année, et les « AFUP Day », qui ont lieu au cours du premier semestre, simultanément dans plusieurs villes.Le langage PHP a été créé en 1994 par Rasmus Lerdorf pour son site web. C'était à l'origine une bibliothèque logicielle en C dont il se servait pour conserver une trace des visiteurs qui venaient consulter son CV. Au fur et à mesure qu'il ajoutait de nouvelles fonctionnalités, Rasmus a transformé la bibliothèque en une implémentation capable de communiquer avec des bases de données et de créer des applications dynamiques et simples pour le Web. Rasmus a alors décidé, en 1995, de publier son code, pour que tout le monde puisse l'utiliser et en profiter. PHP s'appelait alors PHP/FI (pour Personal Home Page Tools/Form Interpreter). En 1997, deux étudiants, Andi Gutmans et Zeev Suraski, ont redéveloppé le cœur de PHP/FI. Ce travail a abouti un an plus tard à la version 3 de PHP, devenu alors PHP: Hypertext Preprocessor. Peu de temps après, Andi Gutmans et Zeev Suraski ont commencé la réécriture du moteur interne de PHP. C’est ce nouveau moteur, appelé Zend Engine — le mot Zend est la contraction de Zeev et Andi — qui a servi de base à la version 4 de PHP.En 2002, PHP est utilisé par plus de 8 millions de sites Web à travers le monde, en 2007 par plus de 20 millions et en 2013 par plus de 244 millions.De plus, PHP est devenu le langage de programmation web côté serveur le plus utilisé depuis plusieurs années :Enfin en 2010, PHP est le langage dont les logiciels open source sont les plus utilisés dans les entreprises, avec 57 % de taux de pénétration.Depuis juin 2011 et le nouveau processus de livraison de PHP, le cycle de livraison de PHP se résume à une mise à jour annuelle comportant des changements fonctionnels importants.La durée de vie d'une branche est de 3 ans, laissant trois branches stables et maintenues (cela signifie que lorsqu'une nouvelle version de PHP 5.x sort, la version 5.x-3 n'est plus supportée). Version 8.1 La version 8.1, sortie le 25 novembre 2021, introduit de nouvelles fonctionnalités comme : les énumérations ;les fibers ;la propriété Readonly. Version 8 Sortie le 26 novembre 2020, cette version majeure se démarque principalement par la fonctionnalité de « compilation à la volée » (Just-in-time compilation) qui permet un gain de vitesse d'exécution de plus de 45 % pour certaines applications Web. D'autres nouveautés sont également introduites comme : les weakmaps ;la Stringable Interface ;l'expression throw. Version 7.4 La version 7.4 est sortie le 20 février 2020. Elle vise à être maintenue jusqu'en novembre 2022.La version 7.4 se démarque de ses précédentes versions par :les propriétés typées 2.0 ;le pré-chargement ;l'opérateur d'affectation de coalescence nulle  ;improve openssl_random_pseudo_bytes ;les références faibles ;FFI (Foreign Function Interface) ;l'extension de hachage omniprésente ;le registre de hachage de mot de passe ;le fractionnement des chaînes multi-octets ;la réflexion sur les références ;le retrait de ext/wddx ;un nouveau mécanisme de sérialisation d'objets personnalisés. Version 7.3 Le 6 décembre 2018, la sortie de la version 7.3 mettait l'accent sur :l'évolution de la syntaxe Heredoc et Nowdoc ;la prise en charge de l'affectation de référence et de la déconstruction de tableau avec `list()` ;la prise en charge de PCRE2 ;l'introduction de la fonction High Resolution Time `hrtime()` function. Version 7.2 Le 30 novembre 2017, la version de PHP 7.2, qui utilise Zend Engine 2, a introduit une modélisation objet plus performante, une gestion des erreurs fondée sur le modèle des exceptions, ainsi que des fonctionnalités de gestion pour les entreprises. PHP 5 apporte beaucoup de nouveautés, telles que le support de SQLite ainsi que des moyens de manipuler des fichiers et des structures XML basés sur libxml2 :une API simple nommée SimpleXML ;une API Document Object Model assez complète ;une interface XPath utilisant les objets DOM et SimpleXML ;l'intégration de libxslt pour les transformations XSLT via l'extension XSL ;une bien meilleure gestion des objets par rapport à PHP 4, avec des possibilités qui tendent à se rapprocher de celles de Java. Version 7 (PHP7) Au vu des orientations différentes prises par le langage de celles prévues par PHP 6, une partie des développeurs propose de nommer la version succédant à PHP 5 « PHP 7 » au lieu de « PHP 6 ». Un vote parmi les développeurs valide cette proposition par 58 voix contre 24.PHP 7.0.0 est sorti en décembre 2015.La nouvelle version propose une optimisation du code et, d'après la société Zend, offre des performances dépassant celles de machines virtuelles comme HHVM,. Les benchmarks externes montrent des performances similaires pour HHVM et PHP 7, avec un léger avantage d'HHVM dans la plupart des scénarios. PHP 6 et Unicode En 2005, le projet de faire de PHP un langage fonctionnant d'origine en Unicode a été lancé par Andrei Zmievski, ceci en s'appuyant sur la bibliothèque International Components for Unicode (ICU) et en utilisant UTF-16 pour représenter les chaînes de caractères dans le moteur.Étant donné que cela représentait un changement majeur tant dans le fonctionnement du langage que dans le code PHP créé par ses utilisateurs, il fut décidé d'intégrer cela dans une nouvelle version 6.0 avec d'autres fonctionnalités importantes alors en développement. Toutefois, le manque de développeurs experts en Unicode ainsi que les problèmes de performance résultant de la conversion des chaînes de et vers UTF-16 (rarement utilisé dans un contexte web), ont conduit au report récurrent de la livraison de cette version. Par conséquent, une version 5.3 fut créée en 2009 intégrant de nombreuses fonctionnalités non liées à Unicode qui était initialement prévues pour la version 6.0, notamment le support des espaces de nommage (namespaces) et des fonctions anonymes. En mars 2010, le projet 6.0 intégrant unicode fut abandonné et la version 5.4 fut préparée afin d'intégrer la plupart des fonctionnalités non liées à l'unicode encore dans la branche 6.0, telles que les traits ou l'extension des fermetures au modèle objet.Le projet est depuis passé à un cycle de livraison prévisible (annuel) contenant des avancées significatives mais contenues tout en préservant au maximum la rétro-compatibilité avec le code PHP existant (5.4 en 2012, 5.5 en 2013, 5.6 prévue pour l'été 2014). Depuis janvier 2014, l'idée d'une nouvelle version majeure introduisant Unicode mais se basant sur UTF-8 (largement devenu depuis le standard du Web pour l'Unicode) et permettant certains changements pouvant casser la rétro-compatibilité avec du code PHP ancien est de nouveau discutée et les RFC sont maintenant triées selon leur implémentation en 5.x (évolutions ne causant pas ou marginalement de cassure de la rétro-compatibilité) ou dans la future version majeure (évolutions majeures du moteur et évolutions impliquant une non-compatibilité ascendante). À noter Il est à noter qu'historiquement, PHP disposait d'une configuration par défaut privilégiant la souplesse à la sécurité (par exemple register globals, qui a été activé par défaut jusqu'à PHP 4.2). Cette souplesse a permis à de nombreux développeurs d'apprendre PHP mais le revers de la médaille a été que de nombreuses applications PHP étaient mal sécurisées. Le sujet a bien été pris en main par le PHPGroup qui a mis en place des configurations par défaut mettant l'accent sur la sécurité. Il en résultait une réputation de langage peu sécurisé, réputation d'insécurité qui n'a plus de raison d'être[réf. nécessaire]. Détail de l'historique complet des versions PHP appartient à la grande famille des descendants du C, dont la syntaxe est très proche. En particulier, sa syntaxe et sa construction ressemblent à celles des langages Java et Perl, à ceci près que du code PHP peut facilement être mélangé avec du code HTML au sein d'un fichier PHP.Dans une utilisation destinée à l'internet, l'exécution du code PHP se déroule ainsi : lorsqu'un visiteur demande à consulter une page de site web, son navigateur envoie une requête au serveur HTTP correspondant. Si la page est identifiée comme un script PHP (généralement grâce à l'extension .php), le serveur appelle l'interprète PHP qui va traiter et générer le code final de la page (constitué généralement d'HTML ou de XHTML, mais aussi souvent de feuilles de style en cascade et de JS). Ce contenu est renvoyé au serveur HTTP, qui l'envoie finalement au client.Ce schéma explique ce fonctionnement :Une étape supplémentaire est souvent ajoutée : celle du dialogue entre PHP et la base de données. Classiquement, PHP ouvre une connexion au serveur de SGBD voulu, lui transmet des requêtes et en récupère le résultat, avant de fermer la connexion.L'utilisation de PHP en tant que générateur de pages Web dynamiques est la plus répandue, mais il peut aussi être utilisé comme langage de programmation ou de script en ligne de commande sans utiliser de serveur HTTP ni de navigateur. Il permet alors d'utiliser de nombreuses fonctions du langage C et plusieurs autres sans nécessiter de compilation à chaque changement du code source.Pour réaliser en Linux/UNIX un script PHP exécutable en ligne de commande, il suffit comme en Perl ou en Bash d'insérer dans le code en première ligne le shebang : #! /usr/bin/php. Sous un éditeur de développement comme SciTE, même en Windows, une première ligne <?php suffit, si le fichier possède un type .php.Il existe aussi une extension appelée PHP-GTK permettant de créer des applications clientes graphiques sur un ordinateur disposant de la bibliothèque graphique GTK+, ou encore son alternative WinBinder.PHP possède un grand nombre de fonctions permettant des opérations sur le système de fichiers, exécuter des commandes dans le terminal, la gestion des bases de données, des fonctions de tri et hachage, le traitement de chaînes de caractères, la génération et la modification d'images, des algorithmes de compression...Le moteur de Wikipédia, MediaWiki, est écrit en PHP et interagit avec une base MySQL ou PostgreSQLQuelques exemples du traditionnel Hello world :echo étant une structure du langage, il est possible – et même recommandé – de ne pas mettre de parenthèses.Il est aussi possible d'utiliser la version raccourcie :Résultat affiché :Le code PHP doit être inséré entre les balises <?php et ?> (la balise de fermeture est facultative en fin de fichier).Il y existe d'autres notations pour les balises :<?= et ?> (notation courte avec affichage) ;<? et ?> (notation courte sans affichage non disponible en PHP 8) ;<% et %> (notation ASP) ;<script language=""php""> et </script> (notation script).Les notations autres que la standard (<?php et ?>) et la notation courte avec affichage (<?= et ?>) sont déconseillées, car elles peuvent être désactivées dans la configuration du serveur (php.ini ou .htaccess) : la portabilité du code est ainsi réduite.Depuis PHP 7, les notations ASP et script ont été supprimées. La notation courte sans affichage reste déconseillée.Les instructions sont séparées par des ; (il n'est pas obligatoire après la dernière instruction) et les sauts de ligne ne modifient pas le fonctionnement du programme. Il serait donc possible d'écrire :Pour des raisons de lisibilité, il est néanmoins recommandé d'écrire une seule instruction par ligne. Il est aussi préférable d'écrire le dernier ;.Le code PHP est composé par des appels à des fonctions, dans le but d'attribuer des valeurs à des variables, le tout encadré dans des conditions, des boucles. Exemple :Une condition est appliquée quand l'expression entre parenthèses est évaluée à true, et elle ne l'est pas dans le cas de false. Sous forme numérique, 0 représente le false, et 1 (et tous les autres nombres) représentent le true.Le code précédent pourrait aussi être écrit de cette manière :Ici on teste l'égalité entre $lang et 'fr', mais pas directement dans le if : le test retourne un boolean (c'est-à-dire soit true, soit false) qui est stocké dans la variable $is_lang_fr. On entre ensuite cette variable dans le if et celui-ci, selon la valeur de la variable, effectuera ou non le traitement.Les blocs if, elseif et else sont généralement délimités par les caractères { et }, qui peuvent être omis, comme dans les codes précédents, lorsque ces blocs ne contiennent qu'une instruction.Il est également possible d'écrire else if en deux mots, comme en C/C++.On peut générer du code HTML avec le script PHP, par exemple :Il est également possible d'utiliser une syntaxe alternative pour la structure if/else :Une autre approche consiste à concaténer l'intégralité du code HTML dans une variable et de réaliser un echo de la variable en fin de fichier :Dans le cas où l'utilisateur aura préféré l'utilisation de la commande echo à la concaténation, il lui sera possible de capturer le flux en utilisant les fonctions ob_start() et ob_get_clean() :PHP, tout comme JavaScript, permet aussi de construire un modèle objet de document (DOM), ce qui permet de créer ou modifier un document (X)HTML sans écrire de HTML, comme le montre l'exemple suivant :Qui crée le code HTML suivant :Cette méthode est cependant peu utilisée pour générer un document complet, on l'utilise généralement pour générer un fichier XML.La commande phpinfo() est aussi utilisée pour générer un code HTML décrivant les paramètres du serveur ; elle est aussi très utilisée pour tester la bonne exécution du moteur d’exécution PHP.Comme en C++ et en Java, PHP permet de programmer en orienté objet, en créant des classes contenant des attributs et des méthodes, qui peuvent être instanciées ou utilisées en statique.Toutefois, PHP est un langage à héritage simple, c'est-à-dire qu'une classe ne peut hériter que d'au plus une seule autre classe (sinon il faut utiliser un trait pour simuler l'héritage multiple par composition). Cependant les interfaces peuvent en étendre plusieurs autres.Voici un exemple de création d'une classe :Comme de nombreux projets Open Source, PHP possède une mascotte. Il s'agit de l'éléPHPant, dessiné en 1998 par El Roubio.El Roubio s'est inspiré de la ressemblance des lettres PHP avec un éléphant et du fait que deux des lettres du langage soient déjà présentes dans ce mot, ce qui a permis de créer le néologisme éléPHPant. Toutes les œuvres d'El Roubio sont distribuées sous licence GNU GPL. Une peluche de l'ÉléPHPant bleu existe. D'autres versions ont vu le jour ces dernières années (rose, jaune, rouge, violet et orange) sous l'impulsion de sociétés (PHP Architect ou Zend Technologies) ou de groupes utilisateurs comme PHP Women ou PHP Amsterdam. Le site afieldguidetoelephpant.net recense tous les éléphpants existants.Wiki (MediaWiki, DokuWiki...)forum (phpBB, Vanilla, IPB, punBB...)FacebookSystèmes de gestion de blog (Dotclear)Systèmes de gestion de contenu (appelés aussi CMS) (WordPress, SPIP, ExpressionEngine, Drupal, Xoops, Joomla, K-Box...)Administration de bases de données (phpMyAdmin, phpPgAdmin, Adminer...)Frameworks (Laravel, Symfony, Zend Framework, CodeIgniter, CakePHP, etc.)Logiciel ECMLogiciel BPM, CRM et ou ERP (Dolibarr...)E-commerce (PrestaShop, WooCommerce, Magento, osCommerce, Sylius, etc.)Partis politiques (Parti chrétien-démocrate (France), etc.)Universités et formations supérieures alliant art et sciences (Ingénieur IMAC, UPEM, etc.)Un serveur Web en architecture trois tiers est composé d'un système d'exploitation, un serveur HTTP, un langage serveur et enfin un système de gestion de base de données (SGBD), cela constituant une plate-forme.Dans le cas de PHP comme langage serveur, les combinaisons les plus courantes sont celles d'une plateforme LAMP (pour Linux Apache MySQL PHP) et WAMP (Windows Apache MySQL PHP). Une plate-forme WAMP s'installe généralement par le biais d'un seul logiciel qui intègre Apache, MySQL et PHP, par exemple EasyPHP, VertrigoServ, WampServer ou UwAmp. Il existe le même type de logiciels pour les plates-formes MAMP (Mac OS Apache MySQL PHP), à l'exemple du logiciel MAMP.Il existe d'autres variantes, par exemple les plates-formes LAPP (le M de MySQL est remplacé par le P de PostgreSQL) ou encore le logiciel XAMPP (Apache MySQL Perl PHP ; le X indique que le logiciel est multiplate-forme), un kit de développement multiplate-forme.On peut décliner une grande variété d'acronymes sous cette forme. Des confusions peuvent parfois exister entre la plate-forme en elle-même et le logiciel permettant de l'installer, si elles ont le même nom. Il faut également remarquer que la grande majorité des logiciels « tout en un » sont destinés au développement d'applications Web en local, et non à être installés sur des serveurs Web. Une exception à cette règle est peut-être Zend Server, le serveur distribué par Zend Technologies, qui est prévu pour fonctionner aussi bien en environnement de développement que de production.PHP est à la base un langage interprété, ce qui est au détriment de la vitesse d'exécution du code. Sa forte popularité associée à son utilisation sur des sites Web à très fort trafic (Yahoo, Facebook) ont amené un certain nombre de personnes à chercher à améliorer ses performances pour pouvoir servir un plus grand nombre d'utilisateurs de ces sites Web sans nécessiter l'achat de nouveaux serveurs.La réécriture du cœur de PHP, qui a abouti au Zend Engine pour PHP 4 puis au Zend Engine 2 pour PHP 5, est une optimisation. Le Zend Engine compile en interne le code PHP en bytecode exécuté par une machine virtuelle. Les projets open source APC et eAccelerator fonctionnent en mettant le bytecode produit par Zend Engine en cache afin d'éviter à PHP de charger et d'analyser les scripts à chaque requête. À partir de la version 5.5 de PHP, le langage dispose d'un cache d'opcode natif (appelé OpCache) rendant obsolète le module APC.Il existe également des projets pour compiler du code PHP :Roadsend et phc compilent du PHP en C ;Quercus compile du PHP en bytecode Java exécutable sur une machine virtuelle Java ;Phalanger compile du PHP en Common Intermediate Language exécutable sur le Common Language Runtime du framework .NET ;HipHop for PHP transforme du PHP en C++ qui est ensuite compilé en code natif. Ce projet open source a été démarré par Facebook.(en) Luke Welling et Laura Thomson, PHP and MySQL Web development, Sams Publishing, 2008, 4e éd. (ISBN 978-0-672-32916-6 et 0-672-32916-6, OCLC 854795897)Damien Seguy et Philippe Gamache, Sécurité PHP 5 et MySQL, 3e édition, Eyrolles, 1er décembre 2011, 277 p. (ISBN 978-2-212-13339-4 et 2-212-13339-1, lire en ligne)Jean Engels PHP 5 Cours et Exercices, 3e édition, Eyrolles 2013, 631 pages  (ISBN 978-2-212-13725-5)Paamayim Nekudotayim : nom de l'opérateur :: en PHPListe de frameworks PHP : liste des cadres de développement (Frameworks) en PHPSuhosin: module de durcissement de PHP5(en) Site officiel Portail des logiciels libres   Portail de la programmation informatique"
Informatique;"La programmation orientée objet (POO), ou programmation par objet, est un paradigme de programmation informatique. Elle consiste en la définition et l'interaction de briques logicielles appelées objets ; un objet représente un concept, une idée ou toute entité du monde physique, comme une voiture, une personne ou encore une page d'un livre. Il possède une structure interne et un comportement, et il sait interagir avec ses pairs. Il s'agit donc de représenter ces objets et leurs relations ; l'interaction entre les objets via leurs relations permet de concevoir et réaliser les fonctionnalités attendues, de mieux résoudre le ou les problèmes. Dès lors, l'étape de modélisation revêt une importance majeure et nécessaire pour la POO. C'est elle qui permet de transcrire les éléments du réel sous forme virtuelle.La programmation par objet consiste à utiliser des techniques de programmation pour mettre en œuvre une conception basée sur les objets. Celle-ci peut être élaborée en utilisant des méthodologies de développement logiciel objet, dont la plus connue est le processus unifié (« Unified Software Development Process » en anglais), et exprimée à l'aide de langages de modélisation tels que le Unified Modeling Language (UML).La programmation orientée objet est facilitée par un ensemble de technologies dédiées :les langages de programmation (chronologiquement : Simula, LOGO, Smalltalk, Ada, C++, Objective C, Eiffel, Python, PHP, Java, Ruby, AS3, C#, VB.NET, Fortran 2003, Vala, Haxe, Swift) ;les outils de modélisation qui permettent de concevoir sous forme de schémas semi-formels la structure d'un programme (Objecteering, UMLDraw, Rhapsody, DBDesigner…) ;les bus distribués (DCOM, CORBA, RMI, Pyro…) ;les ateliers de génie logiciel ou AGL (Visual Studio pour des langages Dotnet, NetBeans ou Eclipse pour le langage Java).Il existe actuellement deux grandes catégories de langages à objets : les langages à classes, que ceux-ci soient sous forme fonctionnelle (Common Lisp Object System), impérative (C++, Java) ou les deux (Python, OCaml) ;les langages à prototypes (JavaScript, Lua).En implantant les Record Class de Hoare, le langage Simula 67 pose les constructions qui seront celles des langages orientés objet à classes : classe, polymorphisme, héritage, etc. Mais c'est réellement par et avec Smalltalk 71 puis Smalltalk 80, inspiré en grande partie par Simula 67 et Lisp, que les principes de la programmation par objets, résultat des travaux d'Alan Kay, sont véhiculés : objet, encapsulation, messages, typage et polymorphisme (via la sous-classification) ; les autres principes, comme l'héritage, sont soit dérivés de ceux-ci ou une implantation. Dans Smalltalk, tout est objet, même les classes. Il est aussi plus qu'un langage à objets, c'est un environnement graphique interactif complet.À partir des années 1980, commence l'effervescence des langages à objets : C++ (1983), Objective-C (1984), Eiffel (1986), Common Lisp Object System (1988), etc. Les années 1990 voient l'âge d'or de l'extension de la programmation par objets dans les différents secteurs du développement logiciel.Depuis, la programmation par objets n'a cessé d'évoluer aussi bien dans son aspect théorique que pratique et différents métiers et discours mercatiques à son sujet ont vu le jour :l'analyse objet (AOO ou OOA en anglais) ;la conception objet (COO ou OOD en anglais) ;les bases de données objet (SGBDOO) ;les langages objets avec les langages à prototypes ;ou encore la méthodologie avec MDA (Model Driven Architecture).Aujourd'hui, la programmation par objets est vue davantage comme un paradigme, le paradigme objet, que comme une simple technique de programmation. C'est pourquoi, lorsque l'on parle de nos jours de programmation par objets, on désigne avant tout la partie codage d'un modèle à objets obtenu par AOO et COO.La programmation orientée objet a été introduite par Alan Kay avec Smalltalk. Toutefois, ses principes n'ont été formalisés que pendant les années 1980 et, surtout, 1990. Par exemple le typage de second ordre, qui qualifie le typage de la programmation orientée objet (appelé aussi duck typing), n'a été formulé qu'en 1995 par Cook.Concrètement, un objet est une structure de données qui répond à un ensemble de messages. Cette structure de données définit son état tandis que l'ensemble des messages qu'il comprend décrit son comportement :les données, ou champs, qui décrivent sa structure interne sont appelées ses attributs ;l'ensemble des messages forme ce que l'on appelle l'interface de l'objet ; c'est seulement au travers de celle-ci que les objets interagissent entre eux. La réponse à la réception d'un message par un objet est appelée une méthode (méthode de mise en œuvre du message) ; elle décrit quelle réponse doit être donnée au message.Certains attributs et/ou méthodes (ou plus exactement leur représentation informatique) sont cachés : c'est le principe d'encapsulation. Ainsi, le programme peut modifier la structure interne des objets ou leurs méthodes associées sans avoir d'impact sur les utilisateurs de l'objet.Un exemple avec un objet représentant un nombre complexe : celui-ci peut être représenté sous différentes formes (cartésienne (réel, imaginaire), trigonométrique, exponentielle (module, angle)). Cette représentation reste cachée et est interne à l'objet. L'objet propose des messages permettant de lire une représentation différente du nombre complexe. En utilisant les seuls messages que comprend notre nombre complexe, les objets appelants sont assurés de ne pas être affectés lors d'un changement de sa structure interne. Cette dernière n'est accessible que par les méthodes des messages.Dans la programmation par objets, chaque objet est typé. Le type définit la syntaxe (« Comment l'appeler ? ») et la sémantique des messages (« Que fait-il ? ») auxquels peut répondre un objet. Il correspond donc, à peu de chose près, à l'interface de l'objet. Toutefois, la plupart des langages objets ne proposent que la définition syntaxique d'un type (C++, Java, C#…) et rares sont ceux qui fournissent aussi la possibilité de définir formellement sa sémantique (comme dans le langage Eiffel avec sa conception par contrats).Un objet peut appartenir à plus d'un type : c'est le polymorphisme ; cela permet d'utiliser des objets de types différents là où est attendu un objet d'un certain type. Une façon de réaliser le polymorphisme est le sous-typage (appelé aussi héritage de type) : on raffine un type-parent en un autre type (le sous-type) par des restrictions sur les valeurs possibles des attributs. Ainsi, les objets de ce sous-type sont conformes au type parent. De ceci découle le principe de substitution de Liskov. Toutefois, le sous-typage est limité et ne permet pas de résoudre le problème des types récursifs (un message qui prend comme paramètre un objet du type de l'appelant). Pour résoudre ce problème, Cook définit en 1995 la sous-classification et le typage du second ordre qui régit la programmation orientée objet : le type est membre d'une famille polymorphique à point fixe de types (appelée classe). Les traits sont une façon de représenter explicitement les classes de types. (La représentation peut aussi être implicite comme avec Smalltalk, Ruby, etc.).On distingue dans les langages objets deux mécanismes du typage :le typage dynamique : le type des objets est déterminé à l'exécution lors de la création desdits objets (Smalltalk, Common Lisp, Python, PHP…) ;le typage statique : le type des objets est vérifié à la compilation et est soit explicitement indiqué par le développeur lors de leur déclaration (C++, Java, C#, Pascal…), soit déterminé par le compilateur à partir du contexte (Scala, OCaml…).De même, deux mécanismes de sous-typage existent : l'héritage simple (Smalltalk, Java, C#) et l'héritage multiple (C++, Python, Common Lisp, Eiffel, WLangage).Le polymorphisme ne doit pas être confondu avec le sous-typage ou avec l'attachement dynamique (dynamic binding en anglais).La programmation objet permet à un objet de raffiner la mise en œuvre d'un message défini pour des objets d'un type parent, autrement dit de redéfinir la méthode associée au message : c'est le principe de redéfinition des messages (ou overriding en anglais).Or, dans une définition stricte du typage (typage du premier ordre), l'opération résultant d'un appel de message doit être la même quel que soit le type exact de l'objet référé. Ceci signifie donc que, dans le cas où l'objet référé est de type exact un sous-type du type considéré dans l'appel, seule la méthode du type père est exécutée :Soit un type Reel contenant une méthode * faisant la multiplication de deux nombres réels, soient Entier un sous-type de Reel, i un Entier et r un Reel, alors l'instruction i * r va exécuter la méthode * de Reel. On pourrait appeler celle de Entier grâce à une redéfinition.Pour réaliser alors la redéfinition, deux solutions existent :le typage du premier ordre associé à l'attachement dynamique (c'est le cas de C++, Java, C#…). Cette solution induit une faiblesse dans le typage et peut conduire à des erreurs. Les relations entre type sont définies par le sous-typage (théorie de Liskov) ;le typage du second ordre (duquel découlent naturellement le polymorphisme et l'appel de la bonne méthode en fonction du type exact de l'objet). Ceci est possible avec Smalltalk et Eiffel. Les relations entre types sont définies par la sous-classification (théorie F-Bound de Cook).La structure interne des objets et les messages auxquels ils répondent sont définis par des modules logiciels. Ces mêmes modules créent les objets via des opérations dédiées. Deux représentations existent de ces modules : la classe et le prototype.La classe est une structure informatique particulière dans le langage objet. Elle décrit la structure interne des données et elle définit les méthodes qui s'appliqueront aux objets de même famille (même classe) ou type. Elle propose des méthodes de création des objets dont la représentation sera donc celle donnée par la classe génératrice. Les objets sont dits alors instances de la classe. C'est pourquoi les attributs d'un objet sont aussi appelés variables d'instance et les messages opérations d'instance ou encore méthodes d'instance. L'interface de la classe (l'ensemble des opérations visibles) forme les types des objets. Selon le langage de programmation, une classe est soit considérée comme une structure particulière du langage, soit elle-même comme un objet (objet non-terminal). Dans le premier cas, la classe est définie dans le runtime ; dans l'autre, la classe a besoin elle aussi d'être créée et définie par une classe : ce sont les méta-classes. L'introspection des objets (ou « méta-programmation ») est définie dans ces méta-classes.La classe peut être décrite par des attributs et des messages. Ces derniers sont alors appelés, par opposition aux attributs et messages d'un objet, variables de classe et opérations de classe ou méthodes de classe. Parmi les langages à classes on retrouve Smalltalk, C++, C#, Java, etc.Le prototype est un objet à part entière qui sert de prototype de définition de la structure interne et des messages. Les autres objets de mêmes types sont créés par clonage. Dans le prototype, il n'y a plus de distinction entre attributs et messages : ce sont tous des slots. Un slot est un label de l'objet, privé ou public, auquel est attachée une définition (ce peut être une valeur ou une opération). Cet attachement peut être modifié à l'exécution. Chaque ajout d'un slot influence l'objet et l'ensemble de ses clones. Chaque modification d'un slot est locale à l'objet concerné et n'affecte pas ses clones.Le concept de trait permet de modifier un slot sur un ensemble de clones. Un trait est un ensemble d'opérations de même catégorie (clonage, persistance, etc.) transverse aux objets. Il peut être représenté soit comme une structure particulière du langage, comme un slot dédié ou encore comme un prototype. L'association d'un objet à un trait fait que l'objet et ses clones sont capables de répondre à toutes les opérations du trait. Un objet est toujours associé à au moins un trait, et les traits sont les parents des objets (selon une relation d'héritage). Un trait est donc un mixin doté d'une parenté. Parmi les langages à prototype on trouve Javascript, Self, Io, Slater, Lisaac, etc.Différents langages utilisent la programmation orientée objet, par exemple PHP, Python, etc.En PHP la programmation orientée objet est souvent utilisée pour mettre en place une architecture MVC (Modèle Vue Contrôleur), où les modèles représentent des objets.La modélisation objet consiste à créer un modèle du système informatique à réaliser. Ce modèle représente aussi bien des objets du monde réel que des concepts abstraits propres au métier ou au domaine dans lequel le système sera utilisé.La modélisation objet commence par la qualification de ces objets sous forme de types ou de classes sous l'angle de la compréhension des besoins et indépendamment de la manière dont ces classes seront mises en œuvre. C'est ce que l'on appelle l'analyse orientée objet ou OOA (acronyme de « Object-Oriented Analysis »). Ces éléments sont alors enrichis et adaptés pour représenter les éléments de la solution technique nécessaires à la réalisation du système informatique. C'est ce que l'on appelle la conception orientée objet ou OOD (acronyme de « Object-Oriented Design »). À un modèle d'analyse peuvent correspondre plusieurs modèles de conception. L'analyse et la conception étant fortement interdépendants, on parle également d'analyse et de conception orientée objet (OOAD). Une fois un modèle de conception établi, il est possible aux développeurs de lui donner corps dans un langage de programmation. C'est ce que l'on appelle la programmation orientée objet ou OOP (en anglais « Object-Oriented Programming »). Pour écrire ces différents modèles, plusieurs langages et méthodes ont été mis au point. Ces langages sont pour la plupart graphiques. Les trois principaux à s'imposer sont OMT de James Rumbaugh, la méthode Booch de Grady Booch et OOSE de Ivar Jacobson. Toutefois, ces méthodes ont des sémantiques différentes et ont chacune des particularités qui les rendent particulièrement aptes à certains types de problèmes. OMT offre ainsi une modélisation de la structure de classes très élaborée. Booch a des facilités pour la représentation des interactions entre les objects. OOSE innove avec les cas d'utilisation pour représenter le système dans son environnement. La méthode OMT prévaut sur l'ensemble des autres méthodes au cours de la première partie de la décennie 1990.À partir de 1994, Booch et Jacobson, rapidement rejoints par Rumbaugh, décident d'unifier leurs approches au sein d'une nouvelle méthode qui soit suffisamment générique pour pouvoir s'appliquer à la plupart des contextes applicatifs. Ils commencent par définir le langage de modélisation UML (Unified Modeling Language) appelé à devenir un standard de l'industrie. Le processus de normalisation est confié à l'Object Management Group (OMG), un organisme destiné à standardiser des technologies orientées objet comme CORBA (acronyme de « Common Object Request Broker Architecture »), un intergiciel (« middleware » en anglais) objet réparti. Rumbaugh, Booch et Jacobson s'affairent également à mettre au point une méthode permettant d'une manière systématique et répétable d'analyser les exigences et de concevoir et mettre en œuvre une solution logicielle à l'aide de modèles UML. Cette méthode générique de développement orienté objet devient le processus unifié (également connu sous l'appellation anglo-saxonne de « Unified Software Development Process »). Elle est itérative et incrémentale, centrée sur l'architecture et guidée par les cas d'utilisation et la réduction des risques. Le processus unifié est de plus adaptable par les équipes de développement pour prendre en compte au mieux les particularités du contexte.Néanmoins pour un certain nombre de concepteurs objet, dont Bertrand Meyer, l'inventeur du langage orienté objet Eiffel, guider une modélisation objet par des cas d'utilisations est une erreur de méthode qui n'a rien d'objet et qui est plus proche d'une méthode fonctionnelle. Pour eux, les cas d'utilisations sont relégués à des utilisations plutôt annexes comme la validation d'un modèle par exemple[réf. nécessaire].(en) Brad J. Cox et Andrew J. Novobilski, Object-Oriented Programming : An Evolutionary Approach, Addison-Wesley, 1986 (ISBN 0-201-54834-8).Grady Booch, James Rumbaugh et Ivar Jacobson, Le guide de l'utilisateur UML, EYROLLES, 2000 (ISBN 2-212-09103-6).Erich Gamma, Richard Helm, Ralph Johnson et John Vlissides (trad. de l'anglais par Jean-Marie Lasvergères), Design Patterns : Catalogue des modèles de conception réutilisables, Vuibert, 1999 (ISBN 2-7117-8644-7).Bertrand Meyer (2000). Conception et programmation orientées objet,  (ISBN 2-212-09111-7).De Hugues Bersini (2007). L'Orienté Objet,  (ISBN 978-2-212-12084-4).Francisco Bonito (2000). La programmation : l'orienté objet.Introduction à la POO Apprendre simplement la Programmation Orientée ObjetDes paradigmes « classiques » à l'orienté objetAnalyse et conception orientée objet avec UML et RUP, un survol rapide(en) The Theory of Classification de Anthony J.H. Simons sur le JOT (Journal of Object Technology) Portail de la programmation informatique"
Informatique;"En informatique, la programmation procédurale est un paradigme  qui se fonde sur le concept d'appel procédural. Une procédure, aussi appelée routine, sous-routine ou fonction (à ne pas confondre avec les fonctions de la programmation fonctionnelle reposant sur des fonctions mathématiques), contient simplement une série d'étapes à réaliser. N'importe quelle procédure peut être appelée à n'importe quelle étape de l'exécution du programme, y compris à l'intérieur d'autres procédures, voire dans la procédure elle-même (récursivité).La programmation procédurale est un meilleur choix qu'une simple programmation séquentielle. Les avantages sont en effet les suivants :la possibilité de réutiliser le même code à différents emplacements dans le programme sans avoir à le dupliquer (principe « DRY »), ce qui a pour effet la réduction de la taille du code source et un gain en localité des modifications, donc une amélioration de la maintenabilité (compréhension plus rapide, réduction du risque de régression) ;une façon plus simple de suivre l'exécution du programme : la programmation procédurale permet de se passer d'instructions telles que goto, évitant ainsi bien souvent de se retrouver avec un programme compliqué qui part dans toutes les directions (appelé souvent « programmation spaghetti[réf. nécessaire] ») ; cependant, la programmation procédurale permet les « effets de bord », c'est-à-dire la possibilité pour une procédure qui prend des arguments de modifier des variables extérieures à la procédure auxquelles elle a accès (variables de contexte plus global que la procédure).La modularité est une caractéristique souhaitable pour un programme ou une application informatique, et consiste enle découpage du programme ou de l'application en unités sans effet de bord entre elles, c'est-à-dire dont le fonctionnement et le résultat renvoyé au module appelant ne dépend que des paramètres explicitement passés en argument (unités fonctionnelles). Un module est un ensemble de structure de données et de procédures, dont l'effet de bord est confiné à cet ensemble de données.De ce fait, un module offre un service. Un module peut avoir un contexte d'exécution différent de celui du code appelant : on parle alors de RPC (Remote Procedure Call) si ce contexte est un autre ordinateur, ou de communication inter-processus (légers ou système) s'il s'agit du même ordinateur. Lorsque la transmission des données ne se fait pas en mémoire mais par fichiers, les modules qui communiquent peuvent être compilés séparément et un script doit assurer l'enchainement des appels.On constate qu'il n'est pas contre-indiqué pour une procédure d'accéder en lecture et en écriture à des variables de contexte plus global (celui d'un module) : cela permet une réduction essentielle du nombre d'arguments passés, mais au détriment de la réutilisation telle quelle dans d'autres contextes d'une procédure. C'est le module en entier qui est réutilisable.Du fait de leur comportement sans effet de bord, chaque module peut être développé par une personne ou un groupe de personnes distinct de ceux qui développent d'autres modules. Les bibliothèques sont des modules. À noter que pour qu'une procédure puisse être considérée comme se comportant comme une « fonction pure » mathématique, il faut que la valeur de son résultat renvoyé au programme appelant prenne toujours la même valeur pour chaque valeur des arguments. Il faut donc qu'elle ne dépende pas d'une variable globale statique éventuelle du module, statique au sens qu'elle garde sa valeur après la fin de l'invocation du module (par une de ses procédures).La programmation objet et générique permet une mutualisation et une unicité de l'information et des traitements/procédures/méthodes (en théorie). En identifiant les variables globales à un module à une structure au sens C ou Pascal et à un type utilisateur, ces modules deviennent par définition des « classes » dont l'instanciation correspond à l'instanciation d'un type composé (une structure C ou Pascal).De plus, par le jeu du polymorphisme et de la généricité, les méthodes d'une classe, qui correspondent exactement aux procédures du module correspondant, en confiant l'effet de bord aux attributs de cette classe, peuvent accepter des arguments dont le type est variable (d'une manière contrôlée par le graphe d'héritage). De ce fait, la programmation objet va plus loin dans la factorisation des traitements que la programmation procédurale (en prolongeant celle-ci), et permet de répondre bien mieux à des besoins où des traitements similaires sont attendus dans des endroits différents d'une solution. Dans une programmation objet aboutie, les « procédures d'aiguillage » (routage de traitements en fonction du type d'une variable passée en argument) sont reléguées au compilateur par utilisation de la liaison dynamique. Le code source s'en trouve aussi réduit.Le plus vieil exemple de ce type de langage est l'ALGOL. D'autres exemples sont Fortran, PL/I, Modula-2 et Ada (dans sa première version). Portail de la programmation informatique"
Informatique;"Un serveur web est soit un logiciel de service de ressources web (serveur HTTP), soit un serveur informatique (ordinateur) qui répond à des requêtes du World Wide Web sur un réseau public (Internet) ou privé (intranet),,, en utilisant principalement le protocole HTTP.Un serveur informatique peut être utilisé à la fois pour servir des ressources du Web et pour faire fonctionner en parallèle d'autres services liés, comme l'envoi d'e-mails, l'émission de flux en streaming, le stockage de données dans des bases de données, le transfert de fichiers par FTP.Les serveurs web publics sont reliés à Internet et hébergent des ressources (pages web, images, vidéos, etc.) du Web. Ces ressources peuvent être statiques (servies telle quelles) ou dynamiques (construites à la demande par le serveur).Certains serveurs sont seulement accessibles sur des réseaux privés (intranets) et hébergent des sites utilisateurs, des documents, ou des logiciels, internes à une entreprise, une administration, etc.Techniquement il serait possible qu'un même ordinateur remplisse ces deux fonctions, mais c'est rarement le cas pour des raisons de sécurité[réf. nécessaire]. La fonction principale d'un serveur Web est de stocker et délivrer des pages web qui sont généralement rendues en HTML. Le protocole de communication Hypertext Transfer Protocol (HTTP) permet le dialogue via le réseau avec le logiciel client, généralement un navigateur web.Les deux termes sont utilisés pour le logiciel car le protocole HTTP a été développé pour le Web, et les pages Web sont en pratique toujours servies avec ce protocole. Cependant d'autres ressources du Web comme les fichiers à télécharger ou les flux audio ou vidéo sont parfois servis avec d'autres protocoles, telle que, par exemple, le protocole de transport Temps Réel (Real-time Transport Protocol), ainsi que son pendant sécurisé, le protocole de transport sécurisé Temps Réel (Secure Real-time Transport Protocol).CERN httpd est le premier serveur HTTP, inventé en même temps que le World Wide Web, en 1990 au CERN de Genève. Il est rapidement devenu obsolète en raison de l'évolution exponentielle des fonctionnalités du protocole.Quelques serveurs HTTP :Apache HTTP Server de la Apache Software Foundation, successeur du NCSA HTTPd ;Apache Tomcat de la Apache Software Foundation, évolution de Apache pour J2EE ;BusyBox httpd, utilisé dans le domaine de l'informatique embarquée, et notamment avec OpenWRT ;Google Web Server de Google ;Internet Information Services (IIS) de Microsoft ;lighttpd de Jan Kneschke ;Monkey web server de Eduardo Silva Pereira, dédié au noyau Linux, permettant d'utiliser pleinement ses fonctionnalités ;nginx d'Igor Sysoev ;Hiawatha de Hugo LeisinkNodeJS sous MIT Licence conçu par Ryan Lienhart Dahl en lignes de programmation en JavaScript ;Sun Java System Web Server de Sun Microsystems (anciennement iPlanet de Netscape, puis Sun ONE de Sun Microsystems) ;Tengine, fork de nginx, de Taobao (9e rang mondial Alexa en juillet 2014) ;Zeus Web Server de Zeus Technology ;Gunicorn est un serveur web HTTP WSGI écrit en Python pour Unix ;Zazouminiwebserver, serveur extrêmement léger (approx. 500 kilooctets), sous environnement Microsoft Windows.Abyss Web Server, un serveur gratuit, multi-plateforme (Linux, Windows, MacOS, BSD), permettant un paramétrage très facile via une interface graphique multilingue.Le serveur HTTP le plus utilisé est Apache HTTP Server qui sert environ 55 % des sites web en janvier 2013 selon Netcraft.Le serveur HTTP le plus utilisé dans les 1 000 sites les plus actifs est en revanche Nginx avec 38,2 % de parts de marché en 2016  selon w3techs et 53,9 % en avril 2017Historiquement, d'autres serveurs HTTP importants furent CERN httpd, développé par les inventeurs du Web, abandonné le 15 juillet 1996 et NCSA HTTPd, développé au NCSA en même temps que NCSA Mosaic, abandonné mi-1994, ainsi que WebObjects.Il existe aussi des serveurs HTTP qui sont des serveurs d'applications capables de faire serveur HTTP, comme Caudium et GlassFish. À l'inverse, on peut trouver des serveurs HTTP spécialisés dans un service distinct comme : HTTP File Server qui est uniquement destiné au partage de fichiersLe logiciel serveur HTTP ou daemon HTTP est le logiciel prenant en charge les requêtes client-serveur du protocole HTTP développé pour le World Wide Web. Ces logiciels intègrent généralement des modules permettant d'exécuter un langage serveur comme PHP pour générer des pages web dynamiques. Les plus connus sont Apache, Nginx, IIS, et Lighttpd.Le plus souvent, un serveur Web exécute continuellement d'autres logiciels qui fonctionnent en collaboration avec le logiciel de serveur HTTP. Selon les besoins, certains services gourmands en ressources, comme le serveur de base de données, peuvent être situés sur la même machine ou un serveur spécialisé.Certaines combinaisons de logiciels de base sont connues sous différents acronymes, notamment celle d'Apache (serveur HTTP) logiciel installé et exécuté sur le serveur web en parallèle de MySQL (serveur de base de données) et le script d'interprétation et d'exécution de PHP (voire PHP-FPM).Voir en PDF l'introduction « Qu'entend-t-on par serveur HTTP et serveur Web ? » d'Anthony Garcia (2008) - IBISC[source insuffisante] :LAMP pour « Linux, Apache, MySQL, PHP » ;WAMP pour « Windows, Apache, MySQL, PHP » ;MAMP pour « Macintosh, Apache, MySQL, PHP ».Il existe aussi la distribution de Microsoft nommée IIS pour « Internet Information Services » qui comprend plusieurs services : HTTP, FTP, SMTP et NNTP.L’équilibrage de charge des serveurs web, ou répartition de charge des serveurs Web, regroupe l’ensemble des mécanismes utilisés pour distribuer les requêtes sur de multiples serveurs Web. Cette pratique est devenue indispensable depuis l’explosion du trafic du Web qui a pour conséquence un accroissement important de la charge demandé au serveur. Cela a entraîné une évolution des architectures, destinée à apporter plus de scalabilité, de disponibilité et de performances. Portail des réseaux informatiques   Portail d’Internet   Portail de l’informatique"
Informatique;"Un superordinateur ou supercalculateur est un ordinateur conçu pour atteindre les plus hautes performances possibles avec les techniques connues lors de sa conception, en particulier en ce qui concerne la vitesse de calcul. Pour des raisons de performance, c'est presque toujours un ordinateur central, dont les tâches sont fournies en traitement par lots.La science des superordinateurs est appelée « calcul haute performance » (en anglais : high-performance computing ou HPC). Cette discipline se divise en deux : la partie matérielle (conception électronique de l'outil de calcul) et la partie logicielle (adaptation logicielle du calcul à l'outil). Ces deux parties font appel à des champs de connaissances différents.Les premiers superordinateurs (ou supercalculateurs) apparaissent dans les années 1960.En 1961, IBM développe l'IBM Stretch ou IBM 7030, dont une unité est exploitée en France en 1963.À cette époque, et jusque dans les années 1970, le plus important constructeur mondial de superordinateurs est la société Control Data Corporation (CDC), avec son concepteur Seymour Cray. Par la suite, Cray Research, fondée par Seymour Cray après son départ de CDC, prend l’avantage sur ses autres concurrents, jusqu’aux alentours de l'année 1990. Dans les années 1980, à l’image de ce qui s’était produit sur le marché des micro-ordinateurs des années 1970, de nombreuses petites sociétés se lancèrent sur ce marché, mais la plupart disparaissent dans le « crash » du marché des superordinateurs, au milieu des années 1990.Ce que désigne le terme superordinateur varie avec le temps, car les ordinateurs les plus puissants du monde à un moment donné tendent à être égalés, puis dépassés, par des machines d’utilisation courante plusieurs années après. Les premiers superordinateurs CDC étaient de simples ordinateurs mono-processeurs (mais possédant parfois jusqu’à dix processeurs périphériques pour les entrées-sorties) environ dix fois plus rapides que la concurrence. Dans les années 1970, la plupart des superordinateurs adoptent un processeur vectoriel, qui effectue le décodage d’une instruction une seule fois pour l’appliquer à toute une série d’opérandes.C’est seulement vers la fin des années 1980 que la technique des systèmes massivement parallèles est adoptée, avec l’utilisation dans un même superordinateur de milliers de processeurs. De nos jours, certains de ces superordinateurs parallèles utilisent des microprocesseurs de type « RISC », conçus pour des ordinateurs de série, comme les PowerPC ou les PA-RISC. D’autres supercalculateurs utilisent des processeurs de moindre coût, de type « CISC », microprogrammés en RISC dans la puce électronique (AMD ou Intel) : le rendement en est un peu moins élevé, mais le canal d’accès à la mémoire — souvent un goulet d’étranglement — est bien moins sollicité.Au XXIe siècle, les superordinateurs sont le plus souvent conçus comme des modèles uniques par des constructeurs informatiques « traditionnels » comme International Business Machines (IBM), Hewlett-Packard (HP), ou Bull, qu’ils aient derrière eux une longue tradition en la matière (IBM) ou qu’ils aient racheté dans les années 1990 des entreprises spécialisées, alors en difficulté, pour acquérir de l’expérience dans ce domaine.Les superordinateurs sont utilisés pour toutes les tâches qui nécessitent une très forte puissance de calcul, comme les prévisions météorologiques, l’étude du climat (à ce sujet, voir les programmes financés par le G8-HORCs), la modélisation d'objets chimiques (calcul de structures et de propriétés, modélisation moléculaire, etc.), les simulations physiques (simulations aérodynamiques, calculs de résistance des matériaux, simulation d'explosion d'arme nucléaire, étude de la fusion nucléaire, etc.), la cryptanalyse ou les simulations en finance et en assurance (calcul stochastique).Les institutions de recherche civiles et militaires comptent parmi les plus gros utilisateurs de superordinateurs.En France, on trouve ces machines dans les centres nationaux de calculs universitaires, tels que l'Institut du développement et des ressources en informatique scientifique (IDRIS), le Centre informatique national de l'enseignement supérieur (CINES), mais aussi au Commissariat à l'énergie atomique et aux énergies alternatives (CEA) ou dans certaines grandes entreprises, comme Total, EDF ou encore Météo-France.Les superordinateurs tirent leur supériorité sur les ordinateurs conventionnels à la fois grâce à :leur architecture, en « pipeline » (exécution d’une instruction identique sur une longue série de données) ou parallèle (nombre très élevé de processeurs fonctionnant chacun sur une partie du calcul), qui leur permet d’exécuter plusieurs tâches simultanément ;des composants électroniques rapides (structure de type serveurs lame utilisant des processeurs multi-cœur ou des cartes graphiques dédiées au calcul scientifique de dernière génération, de la mémoire vive et des équipements de stockage de masse — disque dur — reliés à la fibre optique en grande quantité, etc.) associés à un système d'exploitation dédié (comme Linux, majoritairement utilisé actuellement).Ils sont presque toujours conçus spécifiquement pour un certain type de tâches (le plus souvent des calculs numériques scientifiques : calcul matriciel ou vectoriel) et ne cherchent pas de performance particulière dans d'autres domaines.L’architecture mémorielle des supercalculateurs est étudiée pour fournir en continu les données à chaque processeur afin d’exploiter au maximum sa puissance de calcul. Les performances supérieures de la mémoire (meilleurs composants et meilleure architecture) expliquent pour une large part l’avantage des superordinateurs sur les ordinateurs classiques.Leur système d’entrée/sortie (bus) est conçu pour fournir une large bande passante, la latence étant moins importante puisque ce type d’ordinateur n’est pas conçu pour traiter des transactions.Comme pour tout système parallèle, la loi d’Amdahl s’applique, les concepteurs de superordinateurs consacrant une partie de leurs efforts à éliminer les parties non parallélisables du logiciel et à développer des améliorations matérielles pour supprimer les goulots d'étranglement restants.D'une part, les superordinateurs ont souvent besoin de plusieurs mégawatts de puissance électrique. Cette alimentation doit aussi être de qualité. En conséquence, ils produisent une grande quantité de chaleur et doivent donc être refroidis pour fonctionner normalement. Le refroidissement (par exemple à air) de ces ordinateurs pose souvent un problème important de climatisation.D'autre part, les données ne peuvent circuler plus vite que la vitesse de la lumière entre deux parties d'un ordinateur. Lorsque la taille d’un superordinateur dépasse plusieurs mètres, le temps de latence entre certains composants se compte en dizaines de nanosecondes. Les éléments sont donc disposés pour limiter la longueur des câbles qui relient les composants. Sur le Cray-1 ou le Cray-II, par exemple, ils étaient disposés en cercle.De nos jours, ces ordinateurs sont capables de traiter et de communiquer de très importants volumes de données en très peu de temps. La conception doit assurer que ces données puissent être lues, transférées et stockées rapidement. Dans le cas contraire, la puissance de calcul des processeurs serait sous-exploitée (goulot d’étranglement).           En 1993, l'Institut de Physique du Globe de Paris (IPGP) opère un ordinateur CM-5/128 qui utilise des processeurs SuperSPARC, il est classé 25e au TOP500. Trois ans plus tard, en 1996, l'Institut du développement et des ressources en informatique scientifique (IDRIS) parvient à atteindre la 12e place mondiale avec le T3E construit par Cray.À la mi-2002, le plus puissant des supercalculateurs français se classe 4e au TOP500, c'est le TERA basé sur des processeurs Alpha à 1 GHz (AlphaServer SC45) et développé par Hewlett-Packard ; il appartenait au Commissariat à l'énergie atomique (CEA). En janvier 2006, le TERA-10 de Bull lui succède, il génère une puissance de calcul de 60 téraFLOPS et se placera au 5e rang mondial du TOP500.En juin 2008, l'IDRIS et son Blue Gene/P Solution d'IBM affiche, selon le test LINPACK, une puissance de 120 téraflops et remporte la 10e place.En novembre 2009, la première machine française a pour nom Jade. De type « SGI Altix (en) » elle est basée au Centre informatique national de l'enseignement supérieur (CINES) de Montpellier. Ce supercalculateur se classe au 28e rang mondial avec 128 téraflops au test LINPACK. Peu après, la configuration de la machine Jade est complétée pour atteindre une performance de 237 téraflops. La machine passe en juin 2010 au 18e rang du TOP500. C’est alors le troisième système informatique européen et le premier français, il est destiné à la recherche publique.En novembre 2010, le record français est détenu par le TERA-100 de Bull. Installé au CEA à Bruyères-le-Châtel pour les besoins de la simulation militaire nucléaire française, avec une performance de 1 050 téraflops, cette machine se hisse au 6e rang mondial et gagne le 1er rang européen. Elle est constituée de 17 296 processeurs Intel Xeon 7500 dotés chacun de huit cœurs et connectés par un réseau de type InfiniBand.En mars 2012, Curie, un système conçu par Bull pour le GENCI, installé sur le site du Très Grand Centre de Calcul (TGCC) à Bruyères-le-Châtel, dispose d'une puissance de 1,359 pétaflops. Il sera le supercalculateur le plus puissant de France en prenant la 9e place du classement mondial. Il est conçu pour délivrer 2 pétaflops.En janvier 2013, les systèmes Ada et Turing construits par IBM sont installés à l'IDRIS d'Orsay. La somme de leur puissance dépasse le pétaflops. Ces deux machines sont à la disposition des chercheurs. En mars 2013, le supercalculateur Pangea détenu par la société Total est inauguré, il devient le système le plus performant jamais installé en France. Sa puissance de calcul s'élève à 2,3 pétaflops. Équivalant à 27 000 ordinateurs de bureau réunis, il obtient la 11e place mondiale.En janvier 2015, le système Occigen, conçu par Bull, Atos technologies, pour le GENCI est installé sur le site du CINES ; il est doté d'une puissance de 2,1 pétaflops. Il se situait en 26e position au classement mondial du TOP500 de novembre 2014.En mars 2016, Total annonce avoir triplé la capacité de calcul de son supercalculateur Pangea, passant à une puissance de calculs de 6,7 pétaflops en pics de performance et de 5,28 pétaflops en puissance utilisable. Cela lui permet de retrouver le 11e rang au TOP500 et le place ainsi en tête du secteur industriel mondial.En juin 2022, le GENCI met en service Adastra, un superordinateur fourni par HPE-Cray hébergé au CINES. Ses 46,10 pétaflops lui permettent de gagner le 10e rang mondial en termes de performances de calcul.L'essor des supercalculateurs a vu Linux devenir le système d'exploitation équipant la majorité des 500 supercalculateurs les plus puissants de la planète,, Unix perdant progressivement du terrain face à Linux, mais occupant pendant un temps une place de choix sur le marché des supercalculateurs (5 %).[réf. souhaitée]Windows ne fut exécuté que par deux des 500 supercalculateurs les plus puissants de la planète, soit 0,4 %, tandis que BSD n'était présent que sur une seule machine du top 500, soit 0,2 %. Enfin, les autres configurations (« Mixed », soit un ensemble de plusieurs types de systèmes d'exploitation) représentaient 4,6 %.[réf. souhaitée]En novembre 2017, Linux équipe la totalité des 500 superordinateurs les plus puissants au monde.Georges Karadimas (Snecma), « Les superordinateurs dans le secteur aérospatial français », dans Nouvelle revue Aéronautique et Astronautique, no 2, juin 1994  (ISSN 1247-5793).Site HPC du commissariat à l'énergie atomique (CEA)Site officiel du centre informatique national de l'enseignement supérieur (CINES)Site officiel de l'institut du développement et des ressources en informatique scientifique (IDRIS) Portail de l’informatique"
Informatique;"Le Langage de Modélisation Unifié, de l'anglais Unified Modeling Language (UML), est un langage de modélisation graphique à base de pictogrammes conçu comme une méthode normalisée de visualisation  dans les domaines du développement logiciel et en conception orientée objet.L'UML est une synthèse de langages de modélisation objet antérieurs : Booch, OMT, OOSE. Principalement issu des travaux de Grady Booch, James Rumbaugh et Ivar Jacobson, UML est à présent un standard adopté par l'Object Management Group (OMG). UML 1.0 a été normalisé en janvier 1997; UML 2.0 a été adopté par l'OMG en juillet 2005. La dernière version de la spécification validée par l'OMG est UML 2.5.1 (2017).UML est destiné à faciliter la conception des documents nécessaires au développement d'un logiciel orienté objet, comme standard de modélisation de l'architecture logicielle. Les différents éléments représentables sont :Activité d'un objet/logicielActeursProcessusSchéma de base de donnéesComposants logicielsRéutilisation de composants.Il est également possible de générer automatiquement tout ou partie du code, par exemple en langage Java, à partir des documents réalisés.UML est un langage de modélisation. La version actuelle, UML 2.5, propose 14 types de diagrammes dont sept structurels et sept comportementaux. À titre de comparaison, UML 1.3 comportait 25 types de diagrammes.UML n'étant pas une méthode, l'utilisation des diagrammes est laissée à l'appréciation de chacun. Le diagramme de classes est généralement considéré comme l'élément central d'UML. Des méthodes, telles que le processus unifié proposé par les créateurs originels de UML, utilisent plus systématiquement l'ensemble des diagrammes et axent l'analyse sur les cas d'utilisation (« use case ») pour développer par itérations successives un modèle d'analyse, un modèle de conception, et d'autres modèles. D'autres approches se contentent de modéliser seulement partiellement un système, par exemple certaines parties critiques qui sont difficiles à déduire du code.UML se décompose en plusieurs parties :Les vues : ce sont les observables du système. Elles décrivent le système d'un point de vue donné, qui peut être organisationnel, dynamique, temporel, architectural, géographique, logique, etc. En combinant toutes ces vues, il est possible de définir (ou retrouver) le système complet.Les diagrammes : ce sont des ensembles d'éléments graphiques. Ils décrivent le contenu des vues, qui sont des notions abstraites. Ils peuvent faire partie de plusieurs vues.Les modèles d'élément : ce sont les éléments graphiques des diagrammes.Une façon de mettre en œuvre UML est de considérer différentes vues qui peuvent se superposer pour collaborer à la définition du système :Vue des cas d'utilisation (use-case view) : c'est la description du modèle vu par les acteurs du système. Elle correspond aux besoins attendus par chaque acteur (c'est le quoi et le qui).Vue logique (logical view): c'est la définition du système vu de l'intérieur. Elle explique comment peuvent être satisfaits les besoins des acteurs (c'est le comment).Vue d'implémentation (implementation view) : cette vue définit les dépendances entre les modules.Vue des processus  (process view) : c'est la vue temporelle et technique, qui met en œuvre les notions de tâches concurrentes, stimuli, contrôle, synchronisation…Vue de déploiement (deployment view) : cette vue décrit la position géographique et l'architecture physique de chaque élément du système (c'est le où).Le pourquoi n'est pas défini dans UML.En UML 2.5, les diagrammes sont représentés sous deux types de vue : d'un point de vue statique ou structurelle du domaine avec les diagramme de structure (Structure Diagrams).D'un point de vue dynamique avec les diagrammes de comportement (Behavior Diagrams) et les diagrammes d’interactions (Interaction Diagrams).Les diagrammes sont dépendants hiérarchiquement et se complètent, de façon à permettre la modélisation d'un projet tout au long de son cycle de vie. Il en existe quatorze depuis UML 2.3. Diagrammes de structure ou diagrammes statiques Les diagrammes de structure (structure diagrams) ou diagrammes statiques (static diagrams) rassemblent :Diagramme de classes (class diagram) : représentation des classes intervenant dans le système.Diagramme d'objets (object diagram) : représentation des instances de classes (objets) utilisées dans le système.Diagramme de composants (component diagram) : représentation des composants du système d'un point de vue physique, tels qu'ils sont mis en œuvre (fichiers, bibliothèques, bases de données…)Diagramme de déploiement (deployment diagram) : représentation des éléments matériels (ordinateurs, périphériques, réseaux, systèmes de stockage…) et la manière dont les composants du système sont répartis sur ces éléments matériels et interagissent entre eux.Diagramme des paquets (package diagram) : représentation des dépendances entre les paquets (un paquet étant un conteneur logique permettant de regrouper et d'organiser les éléments dans le modèle UML), c'est-à-dire entre les ensembles de définitions.Diagramme de structure composite (composite structure diagram) : représentation sous forme de boîte blanche des relations entre composants d'une classe (depuis UML 2.x).Diagramme de profils (profile diagram) : spécialisation et personnalisation pour un domaine particulier d'un meta-modèle de référence d'UML (depuis UML 2.2). Diagrammes de comportement Les diagrammes de comportement (behavior diagrams) rassemblent :Diagramme des cas d'utilisation (use-case diagram) : représentation des possibilités d'interaction entre le système et les acteurs (intervenants extérieurs au système), c'est-à-dire de toutes les fonctionnalités que doit fournir le système.Diagramme états-transitions (state machine diagram) : représentation sous forme de machine à états finis du comportement du système ou de ses composants.Diagramme d'activité (activity diagram) : représentation sous forme de flux ou d'enchaînement d'activités du comportement du système ou de ses composants. Diagrammes d'interaction ou diagrammes dynamiques Les diagrammes d'interaction (interaction diagrams) ou diagrammes dynamiques (dynamic diagrams) rassemblent :Diagramme de séquence (sequence diagram) : représentation de façon séquentielle du déroulement des traitements et des interactions entre les éléments du système et/ou de ses acteurs.Diagramme de communication (communication diagram) : représentation de façon simplifiée d'un diagramme de séquence se concentrant sur les échanges de messages entre les objets (depuis UML 2.x).Diagramme global d'interaction (interaction overview diagram) : représentation des enchaînements possibles entre les scénarios préalablement identifiés sous forme de diagrammes de séquences (variante du diagramme d'activité) (depuis UML 2.x).Diagramme de temps (timing diagram) : représentation des variations d'une donnée au cours du temps (depuis UML 2.3).Un stéréotype est une marque de généralisation notée par des guillemets, montrant que l'objet est une variété d'un modèle.Un classeur est une annotation qui permet de regrouper des unités ayant le même comportement ou structure. Un classeur se représente par un rectangle conteneur, en traits pleins.Un paquet regroupe des diagrammes ou des unités.Chaque classe ou objet se définit précisément avec le signe « :: ». Ainsi l'identification d'une classe X en dehors de son paquet ou de son classeur sera définie par « Paquet A::Classeur B::Classe X ». Modèles d'éléments de type commun Symbolique des modèles d'éléments :                  Fourche (fork).             État initial (initial state).État final (final state).Interface (interface).O?--- sens du flux de l'interface.O)----- est un raccourci pour la superposition de ---?O et O?---. Modèles d'éléments de type relation Généralisation (generalisation).Association (association).    Réalisation.Utilisation. Autres modèles d'éléments Les stéréotypes peuvent dépendre du langage utilisé.Les archétypes.Les profils.UML n'est pas une norme en droit mais un simple standard « industriel » (ou norme de fait), parce que promu par l'OMG (novembre 1997) au même titre que CORBA et en raison de son succès. Depuis juillet 2005, la première version 2.x de UML est validée par l'OMG.Par ailleurs, depuis 2003, l'OMG a mis en place un programme de certification à la pratique et la connaissance d'UML OCUP qui recouvre trois niveaux de maîtrise.S'il existe de nombreux logiciels de modélisation UML, aucun ne respecte entièrement chacune des versions de UML, particulièrement UML 2, et beaucoup introduisent des notations non conformes. En revanche, de nombreux logiciels comportent des modules de génération de code, particulièrement à partir du diagramme de classes, qui est celui qui se prête le mieux à une telle automatisation.Grady Booch, James Rumbaugh, Ivar Jacobson, Le guide de l'utilisateur UML, 2000 (ISBN 2-212-09103-6)Laurent Audibert, UML 2, De l'apprentissage à la pratique (cours et exercices), Ellipses, 2009 (ISBN 978-2729852696)Franck Barbier, UML 2 et MDE, Ingénierie des modèles avec études de cas, 2009 (ISBN 978-2-10-049526-9)Craig Larman, UML 2 et les design patterns, Analyse et conception orientées objet et développement itératif (3e édition), Pearson Education, 2005  (ISBN 2-7440-7090-4)Martin Fowler et al., UML 2.0, Initiation aux aspects essentiels de la notation, 2004 (ISBN 2-7440-1713-2)Pascal Roques, UML 2, Modéliser une application Web, Eyrolles, 2007 (ISBN 2-212-12136-9)Pascal Roques, UML 2 par la pratique, Études de cas et exercices corrigés, Eyrolles, 2006 (ISBN 2-212-12014-1)Jim Conallen, Concevoir des applications web avec UML, Eyrolles, 2000, 288 p. (ISBN 978-2-212-09172-4)Unified ProcessIngénierie dirigée par les modèlesModel Driven ArchitectureATLAS Transformation LanguageObject Constraint LanguageTransformation de modèlesModeling and Analysis of Real Time and Embedded systems(en) UML.org(en) Dernière version de la spécification UML(en) OMG (Object Management Group)(en) Profil UML standardisé par l'ITU-T basé sur le Specification and Description Language Portail de l’informatique   Portail de la programmation informatique"
;
