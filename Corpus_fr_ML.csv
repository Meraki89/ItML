informatique;"Un algorithme est une suite finie et non ambiguë d'instructions et d’opérations permettant de résoudre une classe de problèmes.Le mot algorithme vient d'Al-Khwârizmî (en arabe : ?????????), nom d'un mathématicien persan du IXe siècle.Le domaine qui étudie les algorithmes est appelé l'algorithmique. On retrouve aujourd'hui des algorithmes dans de nombreuses applications telles que le fonctionnement des ordinateurs, la cryptographie, le routage d'informations, la planification et l'utilisation optimale des ressources, le traitement d'images, le traitement de textes, la bio-informatique, etc.Un algorithme est une méthode générale pour résoudre un type de problèmes. Il est dit correct lorsque, pour chaque instance du problème, il se termine en produisant la bonne sortie, c'est-à-dire qu'il résout le problème posé.L'efficacité d'un algorithme est mesurée notamment par :sa durée de calcul ;sa consommation de mémoire vive (en partant du principe que chaque instruction a un temps d'exécution constant) ;la précision des résultats obtenus (par exemple avec l'utilisation de méthodes probabilistes) ;sa scalabilité (son aptitude à être efficacement parallélisé) ;etc.Les ordinateurs sur lesquels s'exécutent ces algorithmes ne sont pas infiniment rapides, car le temps de machine reste une ressource limitée, malgré une augmentation constante des performances des ordinateurs. Un algorithme sera donc dit performant s'il utilise avec parcimonie les ressources dont il dispose, c'est-à-dire le temps CPU, la mémoire vive et (objet de recherches récentes) la consommation électrique. L’analyse de la complexité algorithmique permet de prédire l'évolution en temps calcul nécessaire pour amener un algorithme à son terme, en fonction de la quantité de données à traiter.Donald Knuth (1938-) liste, comme prérequis d'un algorithme, cinq propriétés :finitude : « un algorithme doit toujours se terminer après un nombre fini d’étapes » ;définition précise : « chaque étape d'un algorithme doit être définie précisément, les actions à transposer doivent être spécifiées rigoureusement et sans ambiguïté pour chaque cas » ;entrées : « quantités qui lui sont données avant qu'un algorithme ne commence. Ces entrées sont prises dans un ensemble d'objets spécifié » ;sorties : « quantités ayant une relation spécifiée avec les entrées » ;rendement : « toutes les opérations que l'algorithme doit accomplir doivent être suffisamment basiques pour pouvoir être en principe réalisées dans une durée finie par un homme utilisant un papier et un crayon ».George Boolos (1940-1996), philosophe et mathématicien, propose la définition suivante :« Des instructions explicites pour déterminer le nième membre d'un ensemble, pour n un entier arbitrairement grand. De telles instructions sont données de façon bien explicite, sous une forme qui puisse être utilisée par une machine à calculer ou par un humain qui est capable de transposer des opérations très élémentaires en symboles. »Gérard Berry (1948-), chercheur en science informatique, en donne la définition grand public suivante :« Un algorithme, c’est tout simplement une façon de décrire dans ses moindres détails comment procéder pour faire quelque chose. Il se trouve que beaucoup d’actions mécaniques, toutes probablement, se prêtent bien à une telle décortication. Le but est d’évacuer la pensée du calcul, afin de le rendre exécutable par une machine numérique (ordinateur…). On ne travaille donc qu’avec un reflet numérique du système réel avec qui l’algorithme interagit. »Les algorithmes sont des objets historiquement dédiés à la résolution de problèmes arithmétiques, comme la multiplication de deux nombres. Ils ont été formalisés bien plus tard avec l'avènement de la logique mathématique et l'émergence des machines qui permettaient de les mettre en œuvre, à savoir les ordinateurs.La plupart des algorithmes ne sont pas numériques.On peut distinguer :des algorithmes généralistes qui s'appliquent à toute donnée numérique ou non numérique : par exemple les algorithmes liés au chiffrement, ou qui permettent de les mémoriser ou de les transmettre ;des algorithmes dédiés à un type de données particulier (par exemple ceux liés au traitement d'images).Voir aussi : Liste de sujets généraux sur les algorithmes (en)L'algorithmique intervient de plus en plus dans la vie quotidienne.Une recette de cuisine peut être réduite à un algorithme si on peut réduire sa spécification aux éléments constitutifs :des entrées (les ingrédients, le matériel utilisé) ;des instructions élémentaires simples (frire, flamber, rissoler, braiser, blanchir, etc.) dont les exécutions dans un ordre précis amènent au résultat voulu ;un résultat : le plat préparé.Cependant, les recettes de cuisine ne sont en général pas présentées rigoureusement sous forme non ambiguë : il est d'usage d'y employer des termes vagues laissant une liberté d'appréciation à l'exécutant alors qu'un algorithme non probabiliste stricto sensu doit être précis et sans ambiguïté.Le tissage, surtout tel qu'il a été automatisé par le métier Jacquard, est une activité que l'on peut dire algorithmique.Un casse-tête, comme le cube Rubik, peut être résolu de façon systématique par un algorithme qui mécanise sa résolution.En sport, l'exécution de séquences répondant à des finalités d'attaque, de défense, de progression, correspond à des algorithmes (dans un sens assez lâche du terme). Voir en particulier l'article tactique (football).En soins infirmiers, le jugement clinique est assimilable à un algorithme. Le jugement clinique désigne l'ensemble des procédés cognitifs et métacognitifs qui aboutissent au diagnostic infirmier. Il met en jeu des processus de pensée et de prise de décision dans le but d’améliorer l’état de santé et le bien-être des personnes que les soignants accompagnent.Un code juridique, qui décrit un ensemble de procédures applicables à un ensemble de cas, est un algorithme.Les progrès de ce qu'on appelle l'intelligence artificielle s'appuient sur un algorithmique de plus en plus complexe qui devient l'un des rouages cachés du Web 2.0 et des grands réseaux sociaux.À partir des années 2000, ce qui est appelé « algorithmique » est un ensemble de « boîtes noires » (autrement dit de processus informatiques dont on ne sait pas ce qu'il y a à l'intérieur) qui exploitent et influencent les comportements inconscients des consommateurs, et des électeurs.Au milieu des années 2010 la plate-forme logicielle Ripon permet secrètement l'élection de Donald Trump. Elle le fait grâce à une intelligence artificielle s'appuyant sur des logiciels issus de la guerre psychologique telle que développée en Afghanistan, et désormais nourrie du big data disponible sur l'Internet, et en particulier de données personnelles piratées dans plusieurs dizaines de millions de comptes Facebook. Ce piratage a été réalisé par Cambridge analytica au Royaume-Uni (devenu Emerdata en aout 2017) sur la plate-forme Facebook insuffisamment protégée. Les données ont été analysées et utilisées par sa société-sœur canadienne, Aggregate IQ, sous le contrôle du groupe SCL (leur société-mère) via Ripon. Cette plateforme Ripon ayant été conçue pour produire des profils psychographiques et des processus d'utilisation dans des campagnes électorales microciblées. Ces campagnes visaient à influer sur les émotions des électeurs, pour modifier leurs intentions de vote, ou les inciter à rester ou devenir abstentionnistes,,.Ces processus plus ou moins frauduleux (la législation de protection des individus sur l'Internet étant encore émergente) seront découvertes tardivement, dans le cadre du scandale Facebook-Cambridge Analytica/Aggregate IQ, après que ces outils aient conduits à l'élection de D. Trump, puis au Brexit, et qu'ils aient influencé au moins une vingtaine d'élections ou de référendums dans le monde. Dans les années 2010, les lanceurs d'alertes comme le canadien Christopher Wylie, Carole Cadwalladr, Shahmir Sanni, Brittany Kaiser, David Caroll, des journalistes comme Carole Cadwalladr et des ONG telles que AlgorithmWatch alertent sur les dérives éthiques qu'ils constatent dans l'usage malhonnête des algorithmes.Dans la vie quotidienne, un glissement de sens s'est opéré, ces dernières années, dans le concept d'« algorithme » qui devient à la fois plus réducteur, puisque ce sont pour l'essentiel des algorithmes de gestion du big data, et d'autre part plus universel en ce sens qu'il intervient dans tous les domaines du comportement quotidien. La famille des algorithmes dont il est question effectue des calculs à partir de grandes masses de données (les big data). Ils réalisent des classements, sélectionnent des informations et en déduisent un profil, en général de consommation, qui est ensuite utilisé ou exploité commercialement. Les implications sont nombreuses et touchent les domaines les plus variés. Mais les libertés individuelles et collectives pourraient être finalement mises en péril, comme le montre la mathématicienne américaine Cathy O'Neil dans le livre Weapons of Math Destruction, publié en 2016 et sorti en français en 2018 sous le titre Algorithmes : la bombe à retardement (aux éditions Les Arènes).« Aujourd’hui, les modèles mathématiques et les algorithmes prennent des décisions majeures, servent à classer et catégoriser les personnes et les institutions, influent en profondeur sur le fonctionnement des États sans le moindre contrôle extérieur. Et avec des effets de bords incontrôlables. […] Il s’agit d’un pouvoir utilisé contre les gens. Et pourquoi ça marche ? Parce que les gens ne connaissent pas les maths, parce qu’ils sont intimidés. C’est cette notion de pouvoir et de politique qui m’a fait réaliser que j’avais déjà vu ça quelque part. La seule différence entre les modèles de risque en finances et ce modèle de plus-value en science des données, c’est que, dans le premier cas, en 2008, tout le monde a vu la catastrophe liée à la crise financière. Mais, dans le cas des profs, personne ne voit l’échec. Ça se passe à un niveau individuel. Des gens se font virer en silence, ils se font humilier, ils ont honte d’eux. »Dans cet ouvrage, l'auteure alerte le lecteur sur les décisions majeures que nous déléguons aujourd'hui aux algorithmes dans des domaines aussi variés que l'éducation, la santé, l'emploi et la justice, sous prétexte qu'ils sont neutres et objectifs, alors que, dans les faits, ils donnent lieu à « des choix éminemment subjectifs, des opinions, voire des préjugés insérés dans des équations mathématiques ».L'opacité des algorithmes est l'une des raisons principales de ces critiques. Une meilleure information sur leur mode de fonctionnement spécifique permettrait de rendre plus clair le « contrat social passé entre les internautes et les calculateurs ». La description pour chaque algorithme de son propre principe de classement de l'information aide l'utilisateur à mieux comprendre les choix proposés par l'algorithme et les résultats obtenus.Les philosophes Wendell Wallach et Colin Allen ont soulevé des questions liées à l'implantation par les programmeurs de règles morales dans les algorithmes d'intelligence artificielle : « Aujourd'hui, les systèmes [automatiques] s'approchent d'un niveau de complexité qui, selon nous, exige qu'ils prennent eux-mêmes des décisions morales […]. Cela va élargir le cercle des agents moraux au-delà des humains à des systèmes artificiellement intelligents, que nous appellerons des agents moraux artificiels ». Dans son livre Faire la morale aux robots : une introduction à l'éthique des algorithmes, Martin Gibert met en évidence le rôle de la programmation dans l'éthique des robots, en traitant plus précisément des enjeux moraux liés à la construction des algorithmes. Il définit un algorithme comme « rien de plus qu'une suite d'instructions – ou de règles – pour parvenir à un objectif donné ». L'éthique des algorithmes poserait donc une question : « Quelles règles implanter dans les robots, et comment le faire ? ». Gibert souligne notamment l'ambiguïté de ces agents moraux artificiels :« Les agents moraux artificiels (AMA) ne sont pas cependant des agents moraux au sens fort du terme. Contrairement aux humains, ils ne semblent pas imputables [sic] de leurs actes. Ils n'ont toutefois pas besoin de l'être pour prendre des décisions moralement significatives et soulever tout un tas de questions en éthique des algorithmes. »Analyse de la complexité des algorithmesAlgorithmiqueCorrection d'un algorithmeBiais algorithmiqueRégulation des algorithmesRessource relative à la santé : (en) Medical Subject Headings Qu’est-ce qu'un algorithme ? par Philippe Flajolet et Étienne Parizot sur la revue en ligne IntersticesDéfinition du terme « algorithme » par des savants Portail de l'informatique théorique"
informatique;"L'apprentissage automatique, (en anglais : machine learning, litt. « apprentissage machine, »), apprentissage artificiel ou apprentissage statistique est un  champ d'étude de l'intelligence artificielle qui se fonde sur des approches mathématiques et statistiques pour donner aux ordinateurs la capacité d'« apprendre » à partir de données, c'est-à-dire d'améliorer leurs performances à résoudre des tâches sans être explicitement programmés pour chacune. Plus largement, il concerne la conception, l'analyse, l'optimisation, le développement et l'implémentation de telles méthodes. On parle d'apprentissage statistique car l'apprentissage consiste à créer un modèle dont l'erreur statistique moyenne est la plus faible possible.L'apprentissage automatique comporte généralement deux phases. La première consiste à estimer un modèle à partir de données, appelées observations, qui sont disponibles et en nombre fini, lors de la phase de conception du système. L'estimation du modèle consiste à résoudre une tâche pratique, telle que traduire un discours, estimer une densité de probabilité, reconnaître la présence d'un chat dans une photographie ou participer à la conduite d'un véhicule autonome. Cette phase dite « d'apprentissage » ou « d'entraînement » est généralement réalisée préalablement à l'utilisation pratique du modèle. La seconde phase correspond à la mise en production : le modèle étant déterminé, de nouvelles données peuvent alors être soumises afin d'obtenir le résultat correspondant à la tâche souhaitée. En pratique, certains systèmes peuvent poursuivre leur apprentissage une fois en production, pour peu qu'ils aient un moyen d'obtenir un retour sur la qualité des résultats produits.Selon les informations disponibles durant la phase d'apprentissage, l'apprentissage est qualifié de différentes manières. Si les données sont étiquetées (c'est-à-dire que la réponse à la tâche est connue pour ces données), il s'agit d'un apprentissage supervisé. On parle de classification ou de classement si les étiquettes sont discrètes, ou de régression si elles sont continues. Si le modèle est appris de manière incrémentale en fonction d'une récompense reçue par le programme pour chacune des actions entreprises, on parle d'apprentissage par renforcement. Dans le cas le plus général, sans étiquette, on cherche à déterminer la structure sous-jacente des données (qui peuvent être une densité de probabilité) et il s'agit alors d'apprentissage non supervisé. L'apprentissage automatique peut être appliqué à différents types de données, tels des graphes, des arbres, des courbes, ou plus simplement des vecteurs de caractéristiques, qui peuvent être des variables qualitatives ou quantitatives continues ou discrètes.Depuis l'antiquité, le sujet des machines pensantes préoccupe les esprits. Ce concept est la base de pensées pour ce qui deviendra ensuite l'intelligence artificielle, ainsi qu'une de ses sous-branches : l'apprentissage automatique.La concrétisation de cette idée est principalement due à Alan Turing (mathématicien et cryptologue britannique) et à son concept de la « machine universelle » en 1936, qui est à la base des ordinateurs d'aujourd'hui. Il continuera à poser les bases de l'apprentissage automatique, avec son article sur « L'ordinateur et l'intelligence » en 1950, dans lequel il développe, entre autres, le test de Turing.En 1943, le neurophysiologiste Warren McCulloch et le mathématicien Walter Pitts publient un article décrivant le fonctionnement de neurones en les représentant à l'aide de circuits électriques. Cette représentation sera la base théorique des réseaux neuronaux.Arthur Samuel, informaticien américain pionnier dans le secteur de l'intelligence artificielle, est le premier à faire usage de l'expression machine learning (en français, « apprentissage automatique ») en 1959 à la suite de la création de son programme pour IBM en 1952. Le programme jouait au Jeu de Dames et s'améliorait en jouant. À terme, il parvint à battre le 4e meilleur joueur des États-Unis,.Une avancée majeure dans le secteur de l'intelligence machine est le succès de l'ordinateur développé par IBM, Deep Blue, qui est le premier à vaincre le champion mondial d'échecs Garry Kasparov en 1997. Le projet Deep Blue en inspirera nombre d'autres dans le cadre de l'intelligence artificielle, particulièrement un autre grand défi : IBM Watson, l'ordinateur dont le but est de gagner au jeu Jeopardy!. Ce but est atteint en 2011, quand Watson gagne à Jeopardy! en répondant aux questions par traitement de langage naturel.Durant les années suivantes, les applications de l'apprentissage automatique médiatisées se succèdent bien plus rapidement qu'auparavant.En 2012, un réseau neuronal développé par Google parvient à reconnaître des visages humains ainsi que des chats dans des vidéos YouTube,.En 2014, 64 ans après la prédiction d'Alan Turing, le dialogueur Eugene Goostman est le premier à réussir le test de Turing en parvenant à convaincre 33 % des juges humains au bout de cinq minutes de conversation qu'il est non pas un ordinateur, mais un garçon ukrainien de 13 ans.En 2015, une nouvelle étape importante est atteinte lorsque l'ordinateur « AlphaGo » de Google gagne contre un des meilleurs joueurs au jeu de Go, jeu de plateau considéré comme le plus dur du monde.En 2016, un système d'intelligence artificielle à base d'apprentissage automatique nommé LipNet parvient à lire sur les lèvres avec un grand taux de succès,.L'apprentissage automatique (AA) permet à un système piloté ou assisté par ordinateur comme un programme, une IA ou un robot, d'adapter ses réponses ou comportements aux situations rencontrées, en se fondant sur l'analyse de données empiriques passées issues de bases de données, de capteurs, ou du web.L'AA permet de surmonter la difficulté qui réside dans le fait que l'ensemble de tous les comportements possibles compte tenu de toutes les entrées possibles devient rapidement trop complexe à décrire et programmer de manière classique (on parle d'explosion combinatoire). On confie donc à des programmes d'AA le soin d'ajuster un modèle pour simplifier cette complexité et de l'utiliser de manière opérationnelle. Idéalement, l'apprentissage visera à être non supervisé, c'est-à-dire que les réponses aux données d’entraînement ne sont pas fournies au modèle.Ces programmes, selon leur degré de perfectionnement, intègrent éventuellement des capacités de traitement probabiliste des données, d'analyse de données issues de capteurs, de reconnaissance (reconnaissance vocale, de forme, d'écriture…), de fouille de données, d'informatique théorique…L'apprentissage automatique est utilisé dans un large spectre d'applications pour doter des ordinateurs ou des machines de capacité d'analyser des données d'entrée comme : perception de leur environnement (vision, Reconnaissance de formes tels des visages, schémas, segmentation d'image, langages naturels, caractères dactylographiés ou manuscrits ; moteurs de recherche, analyse et indexation d'images et de vidéo, en particulier pour la recherche d'image par le contenu ; aide aux diagnostics, médical notamment, bio-informatique, chémoinformatique ; interfaces cerveau-machine ; détection de fraudes à la carte de crédit, cybersécurité, analyse financière, dont analyse du marché boursier ; classification des séquences d'ADN ; jeu ; génie logiciel ; adaptation de sites Web ; robotique (locomotion de robots, etc.) ; analyse prédictive dans de nombreux domaines (financière, médicale, juridique, judiciaire), diminution des temps de calcul pour les simulations informatiques en physique (calcul de structures, de mécanique des fluides, de neutronique, d'astrophysique, de biologie moléculaire, etc.),, optimisation de design dans l'industrie,,, etc.Exemples :un système d'apprentissage automatique peut permettre à un robot ayant la capacité de bouger ses membres, mais ne sachant initialement rien de la coordination des mouvements permettant la marche, d'apprendre à marcher. Le robot commencera par effectuer des mouvements aléatoires, puis, en sélectionnant et privilégiant les mouvements lui permettant d'avancer, mettra peu à peu en place une marche de plus en plus efficace[réf. nécessaire] ;la reconnaissance de caractères manuscrits est une tâche complexe car deux caractères similaires ne sont jamais exactement identiques. Il existe des systèmes d'apprentissage automatique qui apprennent à reconnaître des caractères en observant des « exemples », c'est-à-dire des caractères connus. Un des premiers système de ce type est celui de reconnaissance des codes postaux US manuscrits issu des travaux de recherche de Yann Le Cun, un des pionniers du domaine ,, et ceux utilisés pour la  reconnaissance d'écriture ou OCR.Les algorithmes d'apprentissage peuvent se catégoriser selon le mode d'apprentissage qu'ils emploient.Si les classes sont prédéterminées et les exemples connus, le système apprend à classer selon un modèle de classification ou de classement ; on parle alors d'apprentissage supervisé (ou d'analyse discriminante). Un expert (ou oracle) doit préalablement étiqueter des exemples. Le processus se passe en deux phases. Lors de la première phase (hors ligne, dite d'apprentissage), il s'agit de déterminer un modèle à partir des données étiquetées. La seconde phase (en ligne, dite de test) consiste à prédire l'étiquette d'une nouvelle donnée, connaissant le modèle préalablement appris. Parfois il est préférable d'associer une donnée non pas à une classe unique, mais une probabilité d'appartenance à chacune des classes prédéterminées ; on parle alors d'apprentissage supervisé probabiliste.Fondamentalement, le machine learning supervisé revient à apprendre à une machine à construire une fonction f telle que Y = f(X), Y étant un (ou plusieurs) résultat(s) d'intérêt calculé en fonction de données d'entrées X effectivement à la disposition de l'utilisateur. Y peut être une grandeur continue (une température par exemple), et on parle alors de régression, ou discrète (une classe, chien ou chat par exemple), et on parle alors de classification.Des cas d'usage typiques d'apprentissage automatique peuvent être d'estimer la météo du lendemain en fonction de celle du jour et des jours précédents, de prédire le vote d'un électeur en fonction de certaines données économiques et sociales, d'estimer la résistance d'un nouveau matériau en fonction de sa composition, de déterminer la présence ou non d'un objet dans une image. L'analyse discriminante linéaire ou les SVM en sont d'autres exemples typiques. Autre exemple, en fonction de points communs détectés avec les symptômes d'autres patients connus (les exemples), le système peut catégoriser de nouveaux patients, au vu de leurs analyses médicales, en risque estimé de développer telle ou telle maladie.Quand le système ou l'opérateur ne dispose que d'exemples, mais non d'étiquette, et que le nombre de classes et leur nature n'ont pas été prédéterminées, on parle d'apprentissage non supervisé ou clustering en anglais. Aucun expert n'est requis. L'algorithme doit découvrir par lui-même la structure plus ou moins cachée des données. Le partitionnement de données, data clustering en anglais, est un algorithme d'apprentissage non supervisé.Le système doit ici — dans l'espace de description (l'ensemble des données) — cibler les données selon leurs attributs disponibles, pour les classer en groupes homogènes d'exemples. La similarité est généralement calculée selon une fonction de distance entre paires d'exemples. C'est ensuite à l'opérateur d'associer ou déduire du sens pour chaque groupe et pour les motifs (patterns en anglais) d'apparition de groupes, ou de groupes de groupes, dans leur « espace ». Divers outils mathématiques et logiciels peuvent l'aider. On parle aussi d'analyse des données en régression (ajustement d'un modèle par une procédure de type moindres carrés ou autre optimisation d'une fonction de coût). Si l'approche est probabiliste (c'est-à-dire que chaque exemple, au lieu d'être classé dans une seule classe, est caractérisé par un jeu de probabilités d'appartenance à chacune des classes), on parle alors de « soft clustering » (par opposition au « hard clustering »).Cette méthode est souvent source de sérendipité. ex. : Pour un épidémiologiste qui voudrait dans un ensemble assez large de victimes de cancer du foie tenter de faire émerger des hypothèses explicatives, l'ordinateur pourrait différencier différents groupes, que l'épidémiologiste chercherait ensuite à associer à divers facteurs explicatifs, origines géographique, génétique, habitudes ou pratiques de consommation, expositions à divers agents potentiellement ou effectivement toxiques (métaux lourds, toxines telle que l'aflatoxine, etc.).Contrairement à l’apprentissage supervisé où l’apprentissage automatique consiste à trouver une fonction f telle que Y = f(X), où Y est un résultat connu et objectif (par exemple Y = « présence d’une tumeur » ou « absence de tumeur » en fonction de X = image radiographique), dans l’apprentissage non supervisé, on ne dispose pas de valeurs de Y, uniquement de valeurs de X (dans l’exemple précédent, on disposerait uniquement des images radiographiques sans connaissance de la présence ou non d’une tumeur. L'apprentissage non supervisé pourrait découvrir deux ""clusters"" ou groupes correspondant à ""présence"" ou ""absence"" de tumeur, mais les chances de réussite sont moindres que dans le cas supervisé où la machine est orientée sur ce qu'elle doit trouver).L’apprentissage non supervisé est généralement moins performant que l’apprentissage supervisé, il évolue dans une zone « grise » où il n’y a généralement pas de « bonne » ou de « mauvaise » réponse mais simplement des similarités mathématiques discernables ou non. L’apprentissage non supervisé présente cependant l’intérêt de pouvoir travailler sur une base de données de X sans qu’il soit nécessaire d’avoir des valeurs de Y correspondantes, or les Y sont généralement compliqués et/ou coûteux à obtenir, alors que les seuls X sont généralement plus simples et moins coûteux à obtenir (dans l’exemple des images radiographiques, il est relativement aisé d’obtenir de telles images, alors qu’obtenir les images avec le label « présence de tumeur » ou « absence de tumeur » nécessite l’intervention longue et coûteuse d’un spécialiste en imagerie médicale).L’apprentissage non supervisé permet potentiellement de détecter des anomalies dans une base de données, comme des valeurs singulières ou aberrantes pouvant provenir d’une erreur de saisie ou d’une singularité très particulière. Il peut donc s’agir d’un outil intéressant pour vérifier ou nettoyer une base de données.Effectué de manière probabiliste ou non, il vise à faire apparaître la distribution sous-jacente des exemples dans leur espace de description. Il est mis en œuvre quand des données (ou « étiquettes ») manquent… Le modèle doit utiliser des exemples non étiquetés pouvant néanmoins renseigner. ex. : En médecine, il peut constituer une aide au diagnostic ou au choix des moyens les moins onéreux de tests de diagnostic.Probabiliste ou non, quand l'étiquetage des données est partiel. C'est le cas quand un modèle énonce qu'une donnée n'appartient pas à une classe A, mais peut-être à une classe B ou C (A, B et C étant trois maladies par exemple évoquées dans le cadre d'un diagnostic différentiel).L’apprentissage auto-supervisé consiste à construire un problème d’apprentissage supervisé à partir d’un problème non supervisé à l’origine.Pour rappel, l’apprentissage supervisé consiste à construire une fonction Y = f(X) et nécessite donc une base de données où l’on possède des Y en fonction des X (par exemple, en fonction du texte X correspondant à la critique d’un film, retrouver la valeur du Y correspondant à la note attribuée au film), alors que dans l’apprentissage non supervisé, on dispose uniquement des valeurs de X et pas de valeurs de Y (on disposerait par exemple ici uniquement du texte X correspondant à la critique du film, et pas de la note Y attribuée au film).L’apprentissage auto-supervisé consiste donc à créer des Y à partir des X pour passer à un apprentissage supervisé, en ""masquant"" des X pour en faire des Y. Dans le cas d'une image, l'apprentissage auto-supervisé peut consister à reconstruire la partie manquante d'une image qui aurait été tronquée. Dans le cas du langage, lorsqu’on dispose d’un ensemble de phrases qui correspondent aux X sans cible Y particulière, l’apprentissage auto-supervisé consiste à supprimer certains X (certains mots) pour en faire des Y. L’apprentissage auto-supervisé revient alors pour la machine à essayer de reconstruire un mot ou un ensemble de mots manquants en fonction des mots précédents et/ou suivants, en une forme d’auto-complétion. Cette approche permet potentiellement à une machine de « comprendre » le langage humain, son sens sémantique et symbolique. Les modèles IA de langage comme BERT ou GPT-3 sont conçus selon ce principe. Dans le cas d’un film, l’apprentissage auto-supervisé consisterait à essayer de prédire les images suivantes en fonction des images précédentes, et donc à tenter de prédire « l’avenir » sur la base de la logique probable du monde réel.Certains chercheurs, comme Yann Le Cun, pensent que si l’IA générale est possible, c’est probablement par une approche de type auto-supervisé qu’elle pourrait être conçue, par exemple en étant immergée dans le monde réel pour essayer à chaque instant de prédire les images et les sons les plus probables à venir, en comprenant qu’un ballon en train de rebondir et de rouler va encore continuer à rebondir et à rouler, mais de moins en moins haut et de moins en moins vite jusqu’à s’arrêter, et qu'un obstacle est de nature à arrêter le ballon ou à modifier sa trajectoire, ou à essayer de prédire les prochains mots qu’une personne est susceptible de prononcer ou le prochain geste qu’elle pourrait accomplir. L’apprentissage auto-supervisé dans le monde réel serait une façon d’apprendre à une machine le sens commun, le bon sens, la réalité du monde physique qui l’entoure, et permettrait potentiellement d’atteindre une certaine forme de conscience. Il ne s’agit évidemment que d’une hypothèse de travail, la nature exacte de la conscience, son fonctionnement et sa définition même restant un domaine actif de recherche.L'algorithme apprend un comportement étant donné une observation. L'action de l'algorithme sur l'environnement produit une valeur de retour qui guide l'algorithme d'apprentissage.Par exemple, l'algorithme de Q-learning est un exemple classique.L'apprentissage par renforcement peut aussi être vu comme une forme d'apprentissage auto-supervisé. Dans un problème d'apprentissage par renforcement, il n'y a en effet à l'origine pas de données de sorties Y, ni même de données d'entrée X, pour construire une fonction Y = f(X). Il y a simplement un ""écosystème"" avec des règles qui doivent être respectées, et un ""objectif"" à atteindre. Par exemple, pour le football, il y a des règles du jeu à respecter et des buts à marquer. Dans l'apprentissage par renforcement, le modèle crée lui-même sa base de donnes en ""jouant"" (d'où le concept d'auto-supervisé) : il teste des combinaisons de données d'entrée X et il en découle un résultat Y qui est évalué, s'il est conforme aux règles du jeu et atteint son objectif, le modèle est récompensé et sa stratégie est ainsi validée, sinon le modèle est pénalisé. Par exemple pour le football, dans une situation du type ""ballon possédé, joueur adverse en face, but à 20 mètres"", une stratégie peut être de ""tirer"" ou de ""dribbler"", et en fonction du résultat (""but marqué"", ""but raté"", ""balle toujours possédée, joueur adverse franchi""), le modèle apprend de manière incrémentale comment se comporter au mieux en fonction des différentes situations rencontrées.L’apprentissage par transfert peut être vu comme la capacité d’un système à reconnaître et à appliquer des connaissances et des compétences, apprises à partir de tâches antérieures, sur de nouvelles tâches ou domaines partageant des similitudes. Il s'agit d'identifier les similitudes entre la ou les tâche(s) cible(s) et la ou les tâche(s) source(s), puis de transférer la connaissance de la ou des tâche(s) source(s) vers la ou les tâche(s) cible(s),.Une application classique de l’apprentissage par transfert est l’analyse d’images. Pour une problématique de classification, l’apprentissage par transfert consiste à repartir d’un modèle existant plutôt que de repartir de zéro. Si par exemple on dispose déjà d’un modèle capable de repérer un chat parmi tout autre objet du quotidien, et que l’on souhaite classifier les chats par races, il est possible que réentraîner partiellement le modèle existant permette d’obtenir de meilleures performances et à moindre coût qu’en repartant de zéro,. Un modèle souvent utilisé pour réaliser un apprentissage par transfert de ce type est VGG-16, un réseau de neurones conçu par l'Université d'Oxford, entraîné sur ~14 millions d'images, capable de classer avec ~93% de précision mille objets du quotidien.Les algorithmes se classent en quatre familles ou types principaux :régressionclassificationpartitionnement de donnéesréduction de dimensions.Plus précisément :la régression linéaire ;la régression logistique ;les machines à vecteur de support ;les réseaux de neurones, dont les méthodes d'apprentissage profond (deep learning en anglais) pour un apprentissage supervisé ou non-supervisé ;la méthode des k plus proches voisins pour un apprentissage supervisé ;les arbres de décision, méthodes à l'origine des Random Forest, par extension également du boosting (notamment XGBoost) ;les méthodes statistiques comme le modèle de mixture gaussienne ;l'analyse discriminante linéaire ;les algorithmes génétiques et la programmation génétique ;le boosting ;le bagging.Ces méthodes sont souvent combinées pour obtenir diverses variantes d'apprentissage. Le choix d'un algorithme dépend fortement de la tâche à résoudre (classification, estimation de valeurs…), du volume et de la nature des données. Ces modèles reposent souvent sur des modèles statistiques.La qualité de l'apprentissage et de l'analyse dépendent du besoin en amont et a priori de la compétence de l'opérateur pour préparer l'analyse. Elle dépend aussi de la complexité du modèle (spécifique ou généraliste), de son adéquation et de son adaptation au sujet à traiter. In fine, la qualité du travail dépendra aussi du mode (de mise en évidence visuelle) des résultats pour l'utilisateur final (un résultat pertinent pourrait être caché dans un schéma trop complexe, ou mal mis en évidence par une représentation graphique inappropriée).Avant cela, la qualité du travail dépendra de facteurs initiaux contraignants, liées à la base de données :nombre d'exemples (moins il y en a, plus l'analyse est difficile, mais plus il y en a, plus le besoin de mémoire informatique est élevé et plus longue est l'analyse) ;nombre et qualité des attributs décrivant ces exemples. La distance entre deux « exemples » numériques (prix, taille, poids, intensité lumineuse, intensité de bruit, etc.) est facile à établir, celle entre deux attributs catégoriels (couleur, beauté, utilité…) est plus délicate ;pourcentage de données renseignées et manquantes ;bruit : le nombre et la « localisation » des valeurs douteuses (erreurs potentielles, valeurs aberrantes…) ou naturellement non-conformes au pattern de distribution générale des « exemples » sur leur espace de distribution impacteront sur la qualité de l'analyse.L'apprentissage automatique ne se résume pas à un ensemble d'algorithmes, mais suit une succession d'étapes,.Définir le problème à résoudre.Acquérir des données : l'algorithme se nourrissant des données en entrée, c'est une étape importante. Il en va de la réussite du projet, de récolter des données pertinentes et en quantité et qualité suffisantes, et en évitant tout biais dans leur représentativité.Analyser et explorer les données. L'exploration des données peut révéler des données d'entrée ou de sortie déséquilibrées pouvant nécessiter un rééquilibrage, le machine learning non supervisé peut révéler des clusters qu'il pourrait être utile de traiter séparément ou encore détecter des anomalies qu'il pourrait être utile de supprimer.Préparer et nettoyer les données : les données recueillies doivent être retouchées avant utilisation. En effet, certains attributs sont inutiles, d’autre doivent être modifiés afin d’être compris par l’algorithme (les variables qualitatives doivent être encodées-binarisées), et certains éléments sont inutilisables car leurs données sont incomplètes (les valeurs manquantes doivent être gérées, par exemple par simple suppression des exemples comportant des variables manquantes, ou par remplissage par la médiane, voire par apprentissage automatique). Plusieurs techniques telles que la visualisation de données, la transformation de données (en) ou encore la normalisation (variables projetées entre 0 et 1) ou la standardisation (variables centrées - réduites) sont employées afin d'homogénéiser les variables entre elles, notamment pour aider la phase de descente de gradient nécessaire à l'apprentissage.Ingénierie ou extraction de caractéristiques : les attributs peuvent être combinés entre eux pour en créer de nouveaux plus pertinents et efficaces pour l’entraînement du modèle. Ainsi, en physique, de la construction de nombres adimensionnels adaptés au problème, de solutions analytiques approchées, de statistiques pertinentes, de corrélations empiriques ou l'extraction de spectres par transformée de Fourier ,. Il s'agit d'ajouter l'expertise humaine au préalable de l'apprentissage machine pour favoriser celui-ci.Choisir ou construire un modèle d’apprentissage : un large choix d'algorithmes existe, et il faut en choisir un adapté au problème et aux données. La métrique optimisée doit être choisie judicieusement (erreur absolue moyenne, erreur relative moyenne, précision, rappel, etc.)Entraîner, évaluer et optimiser : l'algorithme d'apprentissage automatique est entraîné et validé sur un premier jeu de données pour optimiser ses hyperparamètres.Test : puis il est évalué sur un deuxième ensemble de données de test afin de vérifier qu'il est efficace avec un jeu de donnée indépendant des données d’entraînement, et pour vérifier qu'il ne fasse pas de surapprentissage.Déployer : le modèle est alors déployé en production pour faire des prédictions, et potentiellement utiliser les nouvelles données en entrée pour se ré-entraîner et être amélioré.Expliquer : déterminer quelles sont les variables importantes et comment elles impactent les prédictions du modèle en général et au cas par casLa plupart de ces étapes se retrouvent dans les méthodes et processus de projet KDD, CRISP-DM et SEMMA, qui concernent les projets d'exploration de données.Toutes ces étapes sont complexes et requièrent du temps et de l'expertise, mais il existe des outils permettant de les automatiser au maximum pour ""démocratiser"" l'accès à l'apprentissage automatique. Ces approches sont dites ""Auto ML"" (pour machine learning automatique) ou ""No Code"" (pour illustrer que ces approches ne nécessitent pas ou très peu de programmation informatique), elles permettent d'automatiser la construction de modèles d'apprentissage automatique pour limiter au maximum le besoin d'intervention humaine. Parmi ces outils, commerciaux ou non, on peut citer Caret, PyCaret, pSeven, Jarvis, Knime, MLBox ou DataRobot.La voiture autonome paraît en 2016 réalisable grâce à l’apprentissage automatique et les énormes quantités de données générées par la flotte automobile, de plus en plus connectée. Contrairement aux algorithmes classiques (qui suivent un ensemble de règles prédéterminées), l’apprentissage automatique apprend ses propres règles.Les principaux innovateurs dans le domaine insistent sur le fait que le progrès provient de l’automatisation des processus. Ceci présente le défaut que le processus d’apprentissage automatique devient privatisé et obscur. Privatisé, car les algorithmes d’AA constituent des gigantesques opportunités économiques, et obscurs car leur compréhension passe derrière leur optimisation. Cette évolution peut potentiellement nuire à la confiance du public envers l’apprentissage automatique, mais surtout au potentiel à long terme de techniques très prometteuses.La voiture autonome présente un cadre test pour confronter l’apprentissage automatique à la société. En effet, ce n’est pas seulement l’algorithme qui se forme à la circulation routière et ses règles, mais aussi l’inverse. Le principe de responsabilité est remis en cause par l’apprentissage automatique, car l’algorithme n’est plus écrit mais apprend et développe une sorte d’intuition numérique. Les créateurs d’algorithmes ne sont plus en mesure de comprendre les « décisions » prises par leurs algorithmes, ceci par construction mathématique même de l’algorithme d’apprentissage automatique.Dans le cas de l’AA et les voitures autonomes, la question de la responsabilité en cas d’accident se pose. La société doit apporter une réponse à cette question, avec différentes approches possibles. Aux États-Unis, il existe la tendance à juger une technologie par la qualité du résultat qu’elle produit, alors qu’en Europe le principe de précaution est appliqué, et on y a plus tendance à juger une nouvelle technologie par rapport aux précédentes, en évaluant les différences par rapport à ce qui est déjà connu. Des processus d’évaluation de risques sont en cours en Europe et aux États-Unis.La question de responsabilité est d’autant plus compliquée que la priorité chez les concepteurs réside en la conception d’un algorithme optimal, et non pas de le comprendre. L’interprétabilité des algorithmes est nécessaire pour en comprendre les décisions, notamment lorsque ces décisions ont un impact profond sur la vie des individus. Cette notion d’interprétabilité, c’est-à-dire de la capacité de comprendre pourquoi et comment un algorithme agit, est aussi sujette à interprétation.La question de l’accessibilité des données est sujette à controverse : dans le cas des voitures autonomes, certains défendent l’accès public aux données, ce qui permettrait un meilleur apprentissage aux algorithmes et ne concentrerait pas cet « or numérique » dans les mains d’une poignée d’individus, de plus d’autres militent pour la privatisation des données au nom du libre marché, sans négliger le fait que des bonnes données constituent un avantage compétitif et donc économique,.La question des choix moraux liés aux décisions laissées aux algorithmes d'AA et aux voitures autonomes en cas de situations dangereuses ou mortelles se pose aussi. Par exemple en cas de défaillance des freins du véhicule, et d'accident inévitable, quelles vies sont à sauver en priorité: celle des passagers ou bien celle des piétons traversant la rue ?Dans les années 2000-2010, l'apprentissage automatique est encore une technologie émergente, mais polyvalente, qui est par nature théoriquement capable d'accélérer le rythme de l'automatisation et de l'autoaprentissage lui-même. Combiné à l'apparition de nouveaux moyens de produire, stocker et faire circuler l'énergie, ainsi qu'à l'informatique ubiquiste, il pourrait bouleverser les technologies et la société comme l'ont fait la machine à vapeur et l'électricité, puis le pétrole et l'informatique lors des révolutions industrielles précédentes.L'apprentissage automatique pourrait générer des innovations et des capacités inattendues, mais avec un risque selon certains observateurs de perte de maîtrise de la part des humains sur de nombreuses tâches qu'ils ne pourront plus comprendre et qui seront faites en routine par des entités informatiques et robotisées. Ceci laisse envisager des impacts spécifiques complexes et encore impossibles à évaluer sur l'emploi, le travail et plus largement l'économie et les inégalités. Selon le journal Science fin 2017 : « Les effets sur l'emploi sont plus complexes que la simple question du remplacement et des substitutions soulignées par certains. Bien que les effets économiques du BA soient relativement limités aujourd'hui et que nous ne soyons pas confrontés à une « fin du travail » imminente comme cela est parfois proclamé, les implications pour l'économie et la main-d'œuvre sont profondes ».Il est tentant de s'inspirer des êtres vivants sans les copier naïvement pour concevoir des machines capables d'apprendre. Les notions de percept et de concept comme phénomènes neuronaux physiques ont d'ailleurs été popularisés dans le monde francophone par Jean-Pierre Changeux. L'apprentissage automatique reste avant tout un sous-domaine de l'informatique, mais il est étroitement lié opérationn"
informatique;Dans le domaine informatique et de l'intelligence artificielle, l'apprentissage non supervisé désigne la situation d'apprentissage automatique où les données ne sont pas étiquetées. Il s'agit donc de découvrir les structures sous-jacentes à ces données non étiquetées. Puisque les données ne sont pas étiquetées, il est impossible à l'algorithme de calculer de façon certaine un score de réussite.L'absence d'étiquetage ou d'annotation caractérise les tâches d'apprentissage non supervisé et les distingue donc des tâches d'apprentissage supervisé.L'introduction dans un système d'une approche d'apprentissage non supervisé est un moyen d'expérimenter l'intelligence artificielle. En général, des systèmes d'apprentissage non supervisé permettent d'exécuter des tâches plus complexes que les systèmes d'apprentissage supervisé, mais ils peuvent aussi être plus imprévisibles. Même si un système d'IA d'apprentissage non supervisé parvient tout seul, par exemple, à faire le tri entre des chats et des chiens, il peut aussi ajouter des catégories inattendues et non désirées, et classer des races inhabituelles, introduisant plus de bruit que d'ordre.L'apprentissage non supervisé consiste à apprendre sans superviseur. Il s’agit d’extraire des classes ou groupes d’individus présentant des caractéristiques communes. La qualité d'une méthode de classification est mesurée par sa capacité à découvrir certains ou tous les motifs cachés.On distingue l'apprentissage supervisé et non supervisé. Dans le premier apprentissage, il s’agit d’apprendre à classer un nouvel individu parmi un ensemble de classes prédéfinies: on connaît les classes a priori. Tandis que dans l'apprentissage non supervisé, le nombre et la définition des classes ne sont pas donnés a priori.Différence entre les deux types d'apprentissage. Apprentissage supervisé On dispose d'éléments déjà classésExemple : articles en rubrique cuisine, sport, culture...On veut classer un nouvel élémentExemple: lui attribuer un nom parmi cuisine, sport, culture... Apprentissage non supervisé On dispose d'éléments non classésExemple : une fleurOn veut les regrouper en classesExemple: si deux fleurs ont la même forme, elles sont en rapport avec une même plante correspondante.Il existe deux principales méthodes d'apprentissage non supervisées :Les méthodes par partitionnement telles que les algorithmes des k-moyennes ou k-médoïdes.Les méthodes de regroupement hiérarchique.Les techniques d'apprentissage non supervisé peuvent être utilisées pour résoudre, entre autres, les problèmes suivants :le partitionnement de données (par exemple avec l'algorithme des k-moyennes, le regroupement hiérarchique),l'estimation de densité de distribution (distribution de mélange, estimation par noyau),la réduction de dimension (analyse en composantes principales, carte auto-adaptative)L'apprentissage non supervisé peut aussi être utilisé en conjonction avec une inférence bayésienne pour produire des probabilités conditionnelles pour chaque variable aléatoire étant donné les autres.K-means clustering (K-moyenne)Dimensionality Reduction (Réduction de la dimensionnalité)Principal Component Analysis (Analyse en composantes principales)Singular Value Decomposition (Décomposition en valeurs singulières)Independent Component Analysis (Analyse en composantes indépendantes)Distribution models (Modèles de distribution)Hierarchical clustering (Classification hiérarchique)Le regroupement ou Clustering est la technique la plus utilisée pour résoudre les problèmes d'apprentissage non supervisé. La mise en cluster consiste à séparer ou à diviser un ensemble de données en un certain nombre de groupes, de sorte que les ensembles de données appartenant aux mêmes groupes se ressemblent davantage que ceux d’autres groupes. En termes simples, l’objectif est de séparer les groupes ayant des traits similaires et de les assigner en grappes.Voyons cela avec un exemple. Supposons que vous soyez le chef d’un magasin de location et que vous souhaitiez comprendre les préférences de vos clients pour développer votre activité. Vous pouvez regrouper tous vos clients en 10 groupes en fonction de leurs habitudes d’achat et utiliser une stratégie distincte pour les clients de chacun de ces 10 groupes. Et c’est ce que nous appelons le Clustering.Le clustering consiste à grouper des points de données en fonction de leurs similitudes, tandis que l’association consiste à découvrir des relations entre les attributs de ces points de données:Les techniques de clustering cherchent à décomposer un ensemble d'individus en plusieurs sous ensembles les plus homogènes possiblesOn ne connaît pas la classe des exemples (nombre, forme, taille)Les méthodes sont très nombreuses, typologies généralement employées pour les distinguer  Méthodes de partitionnement / Méthodes hiérarchiquesAvec recouvrement / sans recouvrementAutre : incrémental / non incrémentalD'éventuelles informations sur les classes ou d'autres informations sur les données n'ont pas d'influence sur la formation des clusters, seulement sur leur interprétation.L'un des algorithmes le plus connu et utilisé en clustering est la K-moyenne.Cet algorithme va mettre dans des “zones” (Cluster), les données qui se ressemblent. Les données se trouvant dans le même cluster sont similaires.L’approche de K-Means consiste à affecter aléatoirement des centres de clusters (appelés centroids), et ensuite assigner chaque point de nos données au centroid qui lui est le plus proche. Cela s’effectue jusqu’à assigner toutes les données à un cluster. Portail de l’informatique   Portail des probabilités et de la statistique   Portail des données
informatique;L'apprentissage supervisé (supervised learning en anglais) est une tâche d'apprentissage automatique consistant à apprendre une fonction de prédiction à partir d'exemples annotés, au contraire de l'apprentissage non supervisé. On distingue les problèmes de régression des problèmes de classement. Ainsi, on considère que les problèmes de prédiction d'une variable quantitative sont des problèmes de régression tandis que les problèmes de prédiction d'une variable qualitative sont des problèmes de classification.Les exemples annotés constituent une base d'apprentissage, et la fonction de prédiction apprise peut aussi être appelée « hypothèse » ou « modèle ». On suppose cette base d'apprentissage représentative d'une population d'échantillons plus large et le but des méthodes d'apprentissage supervisé est de bien généraliser, c'est-à-dire d'apprendre une fonction qui fasse des prédictions correctes sur des données non présentes dans l'ensemble d'apprentissage.Soit                     (        ?        ,                              A                          ,                  P                )              {\displaystyle (\Omega ,{\mathcal {A}},\mathbb {P} )}  , un espace probabilisé.Soit                     (                              X                          ,                                            F                                            X                          )        ,        (                              Y                          ,                                            F                                            Y                          )              {\displaystyle ({\mathcal {X}},{\mathcal {F}}_{X}),({\mathcal {Y}},{\mathcal {F}}_{Y})}   deux espaces mesurables. On peut définir une base de données d'apprentissage (ou ensemble d'apprentissage) comme un ensemble de couples entrée-sortie                     (                  x                      n                          ,                  y                      n                                    )                      1            ?            n            ?            N                                {\displaystyle (x_{n},y_{n})_{1\leq n\leq N}}   où chaque                               x                      n                          ?                              X                                {\displaystyle x_{n}\in {\mathcal {X}}}   et                               y                      n                          ?                              Y                                {\displaystyle y_{n}\in {\mathcal {Y}}}   sont des réalisations respectives des variables aléatoires                               X                      n                                {\displaystyle X_{n}}   et                               Y                      n                                {\displaystyle Y_{n}}  . Les couples de la suite                     (        (                  X                      n                          ,                  Y                      n                          )                  )                      n            ?            N                                {\displaystyle ((X_{n},Y_{n}))_{n\leq N}}   sont indépendants et identiquement distribués suivant la loi d'un couple                     (        X        ,        Y        )              {\displaystyle (X,Y)}   à valeurs dans                     (                              X                          ×                              Y                          ,                                            F                                            X                          ?                                            F                                            Y                          )              {\displaystyle ({\mathcal {X}}\times {\mathcal {Y}},{\mathcal {F}}_{X}\otimes {\mathcal {F}}_{Y})}  . On rappelle que cette loi est caractérisée par une mesure de probabilité                                           P                                (            X            ,            Y            )                                {\displaystyle \mathbb {P} _{(X,Y)}}   définie pour tout évènement                     A        ?                                            F                                            X                          ?                                            F                                            Y                                {\displaystyle A\in {\mathcal {F}}_{X}\otimes {\mathcal {F}}_{Y}}   par                                           P                                (            X            ,            Y            )                          (        A        )        =                  P                [        (        X        ,        Y                  )                      ?            1                          (        A        )        ]              {\displaystyle \mathbb {P} _{(X,Y)}(A)=\mathbb {P} [(X,Y)^{-1}(A)]}  Par exemple                               X                      n                                {\displaystyle X_{n}}   suit une loi uniforme et                               Y                      n                          =        f        (                  X                      n                          )        +                  ?                      n                                {\displaystyle Y_{n}=f(X_{n})+\epsilon _{n}}   où                               ?                      n                                {\displaystyle \epsilon _{n}}   est un bruit centré. Dans ce cas, la méthode d'apprentissage supervisé utilise cette base d'apprentissage pour déterminer une estimation de f notée g et appelée indistinctement fonction de prédiction, hypothèse ou modèle qui à une nouvelle entrée x associe une sortie g(x). Le but d'un algorithme d'apprentissage supervisé est donc de généraliser pour des entrées inconnues ce qu'il a pu « apprendre » grâce aux données déjà annotées par des experts, ceci de façon « raisonnable ». On dit que la fonction de prédiction apprise doit avoir de bonnes garanties en généralisation.Plus généralement, l'objectif de l'apprentissage supervisé est d'apprendre une fonction                     f              {\displaystyle f}   qui « minimise l'écart entre les variables aléatoires                     f        (        X        )              {\displaystyle f(X)}   et                     Y              {\displaystyle Y}   ». Pour définir cet écart, nous introduisons une fonction de perte                     L        :                              Y                          ×                              Y                          ?                              R                                +                                {\displaystyle L:{\mathcal {Y}}\times {\mathcal {Y}}\rightarrow \mathbb {R} _{+}}   qui quantifie la distance entre une prédiction du modèle                     f        (        x        )              {\displaystyle f(x)}   et une sortie attendue                     y              {\displaystyle y}  . À partir de cette fonction, nous pouvons définir le risque statistique d'une modèle                     f              {\displaystyle f}  . Il est noté                     R              {\displaystyle R}   et est défini par :En pratique, on n'a jamais accès directement à                                           P                                (            X            ,            Y            )                                {\displaystyle \mathbb {P} _{(X,Y)}}  , en revanche il est possible de l'estimer à partir du jeu de données en utilisant la mesure empirique                                           P                                (            X            ,            Y            )                                N                                {\displaystyle \mathbb {P} _{(X,Y)}^{N}}   définie pour tout                     A        ?                                            F                                            X                          ?                                            F                                            Y                                {\displaystyle A\in {\mathcal {F}}_{X}\otimes {\mathcal {F}}_{Y}}   par                                           P                                (            X            ,            Y            )                                N                          (        A        )        =                                            1              N                                                ?                      n            =            1                                N                                    ?                      (                          X                              n                                      ,                          Y                              n                                      )                          (        A        )              {\displaystyle \mathbb {P} _{(X,Y)}^{N}(A)={\dfrac {1}{N}}\sum _{n=1}^{N}\delta _{(X_{n},Y_{n})}(A)}  .Dès lors, un algorithme d'apprentissage supervisé mettra en œuvre des algorithmes d'optimisation afin de trouver une fonction                     f              {\displaystyle f}   qui minimise le risque empirique                               R                      N                          (        f        )        =                                            1              N                                                ?                      n            =            1                                N                          L        (                  Y                      n                          ,        f        (                  X                      n                          )        )              {\displaystyle R_{N}(f)={\dfrac {1}{N}}\sum _{n=1}^{N}L(Y_{n},f(X_{n}))}  . Il faut noter que                               R                      N                                {\displaystyle R_{N}}   n'est rien d'autre que la moyenne des écart (au sens de                     L              {\displaystyle L}  ) entre les prédictions du modèle et les sorties attendues.On distingue trois types de problèmes solubles avec une méthode d'apprentissage automatique supervisée :                                          Y                          ?                  R                      {\displaystyle {\mathcal {Y}}\subset \mathbb {R} }   : lorsque la sortie que l'on cherche à estimer est une valeur dans un ensemble continu de réels, on parle d'un problème de régression. La fonction de prédiction est alors appelée un régresseur.                                          Y                          =        {        1        ,        …        ,        I        }              {\displaystyle {\mathcal {Y}}=\{1,\ldots ,I\}}   : lorsque l'ensemble des valeurs de sortie est fini, on parle d'un problème de classification, qui revient à attribuer une étiquette à chaque entrée. La fonction de prédiction est alors appelée un classifieur.Lorsque                                           Y                                {\displaystyle {\mathcal {Y}}}   est un ensemble de données structurées, on parle d'un problème de prédiction structurée, qui revient à attribuer une sortie complexe à chaque entrée. Par exemple, en bio-informatique le problème de prédiction de réseaux d’interactions entre gènes peut être considéré comme un problème de prédiction structurée dans laquelle l'ensemble possible des sorties structurées est l'ensemble de tous les graphes modélisant les interactions possibles.Une bonne estimation de                     f              {\displaystyle f}   vérifierait                     f        (        X        )        =                  E                (        Y                  |                X        )              {\displaystyle f(X)=\mathbb {E} (Y|X)}  . On estimerait donc                     Y              {\displaystyle Y}   par son espérance conditionnelle par rapport à                     X              {\displaystyle X}  . Le théorème suivant montre l'intérêt d'utiliser la fonction de perte quadratique dans le cas d'une régression.BoostingMachine à vecteurs de supportMélanges de loisRéseau de neurones artificielsMéthode des k plus proches voisinsArbre de décisionClassification naïve bayésienneInférence grammaticaleEspace de versionsVision par ordinateurReconnaissance de formesReconnaissance de l'écriture manuscriteReconnaissance vocaleTraitement automatique de la langueBio-informatiqueReconnaissance optique de caractèresVincent Barra, Antoine Cornuéjols, Laurent Miclet, Apprentissage Artificiel : Concepts et algorithmes, Eyrolles, 2021 (ISBN 978-2-416-001-04-8) [détail des éditions](en) Tom M. Mitchell, Machine Learning, 1997 [détail des éditions](en) Christopher M. Bishop, Pattern Recognition And Machine Learning, Springer, 2006 (ISBN 0-387-31073-8) [détail des éditions] Portail des probabilités et de la statistique   Portail de l’informatique   Portail des données
informatique;"L'attaque par force brute est une méthode utilisée en cryptanalyse pour trouver un mot de passe ou une clé. Il s'agit de tester, une à une, toutes les combinaisons possibles. Cette méthode est en général considérée comme la plus simple concevable. Elle permet de casser tout mot de passe  en un temps fini indépendamment de la protection utilisée, mais le temps augmente avec la longueur du mot de passe. En théorie la complexité d'une attaque par force brute est une fonction exponentielle de la longueur du mot de passe, la rendant en principe impossible pour des mots de passe de longueur moyenne. En pratique des optimisations heuristiques peuvent donner des résultats dans des délais beaucoup plus courts.Cette méthode est souvent combinée avec l'attaque par dictionnaire et par table arc-en-ciel pour trouver le secret plus rapidement.Si le mot de passe contient N caractères, indépendants (la présence d'un caractère ne va pas influencer un autre) et uniformément distribués (aucun caractère n'est privilégié), le nombre maximum d'essais nécessaires se monte alors à : 26N si le mot de passe ne contient que des lettres de l'alphabet totalement en minuscules ou en majuscules ;36N si le mot de passe mélange des chiffres et des lettres de l'alphabet totalement en minuscules ou en majuscules ;62N si le mot de passe mélange les majuscules et les minuscules ainsi que les chiffres.Il suffit en fait d'élever la taille de « l'alphabet » utilisé à la puissance N. Il s'agit ici d'une borne supérieure et en moyenne, il faut deux fois moins d'essais pour trouver le mot de passe (si celui-ci est aléatoire). En réalité, bien peu de mots de passe sont totalement aléatoires et le nombre d'essais est bien inférieur aux limites données ci-dessus (grâce à la possibilité d'une attaque par dictionnaire). Le tableau ci-dessous donne le nombre maximum d'essais nécessaires pour trouver des mots de passe de longueurs variables.Un ordinateur personnel est capable de tester plusieurs centaines de milliers voire quelques millions de mots de passe par seconde. Cela dépend de l'algorithme utilisé pour la protection mais on voit qu'un mot de passe de seulement 6 caractères, eux-mêmes provenant d'un ensemble de 62 symboles (minuscules ou majuscules accompagnés de chiffres), ne tiendrait pas très longtemps face à une telle attaque. Dans le cas des clés utilisées pour le chiffrement, la longueur est souvent donnée en bits. Dans ce cas, le nombre de possibilités (si la clé est aléatoire) à explorer est de l'ordre de 2N où N est la longueur de la clé en bits. Une clé de 128 bits représente déjà une limite impossible à atteindre avec la technologie actuelle et l'attaquant doit envisager d'autres solutions cryptanalytiques si celles-ci existent. Il faut cependant prendre en compte que la puissance du matériel informatique évolue sans-cesse (voir Loi de Moore) et un message indéchiffrable à un moment donné peut l'être par le même type d'attaque une dizaine d'années plus tard.Le principe général de l'attaque par force brute reste toujours de tester l'ensemble des mots de passe possibles, cependant l'ordre de test peut être optimisé afin d'obtenir de meilleurs rendements qu'une attaque par ordre alphabétique.ordre aléatoire : certains systèmes de mots de passe sont capables de reconnaître les tentatives d'attaque par force brute suivant les algorithmes les plus courants et de les bloquer, l'introduction d'éléments aléatoires peut masquer l'attaque.Plutôt que d'utiliser des chaînes de caractère aléatoires comme mot de passe, les utilisateurs ont tendance à utiliser des mots courant plus faciles à retenir. Or, s'il existe un nombre important de combinaisons aléatoires pour une chaîne de longueur donnée, le nombre de mots présents dans un ou plusieurs langages est beaucoup plus faible (à titre d'exemple l'Académie française estime que les dictionnaires encyclopédiques comptent environ 200 000 mots). Connaissant ce phénomène culturel, il peut être judicieux de tester ces mots courants et leurs dérivés (y compris argot, dialectes, mots avec fautes d'orthographe courante…) en priorité.De manière générale, pour développer le principe de l'attaque par dictionnaire, l'attaquant peut tirer parti du fait qu'en l'état actuel des connaissances, il n'existe pas de générateur  aléatoire parfait et que de ce fait le générateur étant toujours pseudo-aléatoire (qu'il soit un processus informatique ou la saisie par une personne) il est toujours possible en observant de grands échantillons de mots de passe produits par un générateur donné d'identifier des tendances permettant un résultat généralement meilleur qu'une recherche alphabétique ou aléatoire.Outre ces attaques théoriques, il existe des attaques tirant parti de l'implémentation des systèmes pour augmenter le rendement de l'attaque par force brute, notamment la transmission des mots de passe sous la forme de hash autorise l'usage de tables arc en ciel, du fait des collisions liées aux fonctions de hachage il est possible de casser une protection sans connaitre le mot de passe réel.Outre ces améliorations algorithmiques, l'attaque peut également être accélérée en augmentant la puissance de calcul matérielle consacrée à celle-ci par exemple en utilisant des superordinateurs ou de l'informatique distribuée (parfois sous la forme d'un botnet).La première défense consiste à renforcer le mot de passe en évitant les écueils qu'exploitent les attaques par force brute optimisée.Renforcer la force brute du mot de passe consiste à :allonger le mot de passe ou la clé si cela est possible ;utiliser la plus grande gamme de symboles possibles (minuscules, majuscules, ponctuations, chiffres) ; l'introduction de caractères nationaux (Â, ÿ…) rend plus difficile le travail des pirates (mais parfois aussi l'entrée de son mot de passe quand on se trouve à l'étranger).Éviter toutes les formes ou habitudes (patterns) identifiées ou identifiables par les attaquantséviter l'emploi de mot du langage commun pour empêcher les attaques par dictionnaireéviter les répétitions de formes de mot de passe (par exemple les mots de passe constitués de caractères en majuscules, caractères en minuscule puis terminés par des symboles sont une famille identifiée et testée en priorité par les logiciels d'attaque par force brute)en poussant le raisonnement précédent jusqu'au bout il apparaît que la seule méthode de choix de mot de passe qui échappe à toute optimisation est la génération aléatoire (ou en pratique une génération pseudo-aléatoire de qualité suffisante).Les mots de passes sont censés rendre l'attaque virtuellement impossible par des temps de cassage extrêmement longs, or les temps de cassage sont souvent une fonction linéaire de la capacité de la ou des machine(s) attaquante. Certains attaquants (services secrets, laboratoires…) peuvent disposer de machines très puissantes et les capacités des machines disponibles au grand public sont en constante progression que ce soit par leur puissance brute ou par l'introduction de nouveaux paradigmes (calcul parallèle par exemple). Il en résulte qu'une protection qui paraissait suffisante à un instant donné peut se trouver dépassée par les capacités disponibles à l'attaquant. La principale méthode pour neutraliser la puissance de calcul d'un attaquant consiste à limiter les tentatives possibles dans le temps. La méthode la plus restrictive et la plus sûre (qu'on retrouve sur les cartes bancaires en France) consiste à n'autoriser qu'un nombre limité d'erreurs avant verrouillage du système. Des méthodes moins contraignantes peuvent être de limiter le nombre de tentatives par unité de temps. Ces méthodes présentent cependant des contraintes d'exploitation et peuvent être détournées par un attaquant pour créer des attaques par déni de service. Deux brevets principaux existent à ce sujet :Un des Laboratoires Bell consistant à doubler le temps d'attente après chaque essai infructueux, pour le faire redescendre ensuite en vol plané après un certain temps sans attaques ;Un de la compagnie IBM consistant à répondre « Mot de passe invalide » après N essais infructueux en un temps T, y compris si le mot de passe est valide : le pirate a alors toutes les chances de rayer de façon erronée le mot de passe valide en le considérant invalide. De plus, cette méthode empêche toute attaque visant à un déni de service pour l'utilisateur.Une variante de l'attaque de la limitation temporelle du nombre de tentatives consiste à augmenter les ressources nécessaires pour réaliser chaque tentative.Une première méthode consiste à utiliser une fonction de hachage cryptographique de complexité relativement élevée. Ainsi le coût de chaque tentative se trouve augmenté.Comparé à la simple temporisation, l'intérêt de l'augmentation du coût du hachage est qu'il ne peut être contourné (l'opération de hachage est strictement nécessaire pour effectuer une tentative). Les inconvénients est que le temps de l'opération baisse avec la puissance de la machine attaquante, là où la temporisation reste constante et peut être choisie de façon arbitraire, et que l'augmentation de coût s'applique également aux opérations légitimes.Un autre exemple de système de limitation des tentatives de connexion est l'utilisation de CAPTCHA. Ces dispositifs peuvent poser des difficultés significatives pour une machine tout en restant acceptables pour un utilisateur humain.Mais la protection contre les attaques par force brute est souvent apportée par des solutions pragmatiques, en adéquation avec les besoins propres à l’utilisateur, comme la restriction d’accès à une, plusieurs, ou à une plage entière d’adresses IP, ce qui correspond à la grande majorité des cas et se présente comme une alternative fiable à la limitation de durée de validité des mots de passe.Une solution peut consister à limiter la durée de validité des mots de passe à une durée inférieure à celle estimée pour leur cassage en les renouvelant à intervalles réguliers. Ceci peut passer soit par une politique de sécurité informatique appliquée avec rigueur pour des périodes de renouvellement jusqu'à quelques jours ou par des dispositifs physiques token pour des fréquences de renouvellement très élevées. Les systèmes de mots de passe comme celui d'Unix utilisent une version modifiée du chiffrement DES. Chaque mot de passe est accompagné d'une composante aléatoire appelée sel dont le but est de modifier la structure interne de DES et éviter ainsi une recherche exhaustive en utilisant du matériel spécialement conçu pour DES. Ce système peut cependant créer une faille de sécurité en facilitant les attaques par déni de service: le temps d'attente peut être utilisé pour gêner la connexion d'utilisateurs légitimes.En théorie et avec suffisamment de temps, l'attaquant peut toujours trouver le mot de passe, mais lorsque ce temps dépasse la décennie, il ne pourra pas en escompter un grand profit, et le mot de passe aura de toute façon changé. Il change même à chaque fois si l'on emploie le principe du masque jetable. Le problème est tout autre si l'attaquant récupère directement le fichier des hashs des mots de passe ; plus rien ne l'empêche alors de tester chez lui des mots de passe à la vitesse de son(es) ordinateur(s). C'est pourquoi dans tous les UNIX modernes ces hashs sont généralement situés dans le fichier /etc/shadow, lisible uniquement par l'utilisateur root. Une compromission de cet utilisateur permet par conséquent de récupérer ce fichier, et ainsi de lancer une attaque par force brute sur son contenu.Cassage de mot de passeAttaque par dictionnaireAuthentification forteRobustesse d'un mot de passe(fr) Programme qui génère des mots de passe, en évalue et en améliore la robustesse(fr) Calculer une bonne taille de clef (www.keylength.com) Portail de la cryptologie   Portail de la sécurité informatique"
informatique;"C++ est un langage de programmation compilé permettant la programmation sous de multiples paradigmes, dont la programmation procédurale, la programmation orientée objet et la programmation générique. Ses bonnes performances, et sa compatibilité avec le C en font un des langages de programmation les plus utilisés dans les applications où la performance est critique.Créé initialement par Bjarne Stroustrup dans les années 1980, le langage C++ est aujourd'hui normalisé par l'ISO. Sa première normalisation date de 1998 (ISO/CEI 14882:1998), ensuite amendée par l'erratum technique de 2003 (ISO/CEI 14882:2003). Une importante mise à jour a été ratifiée et publiée par l'ISO en septembre 2011 sous le nom de ISO/IEC 14882:2011, ou C++11. Depuis, des mises à jour sont publiées régulièrement : en 2014 (ISO/CEI 14882:2014, ou C++14), en 2017 (ISO/CEI 14882:2017, ou C++17) puis en 2020 (ISO/IEC 14882:2020, ou C++20).En langage C, ++ est l'opérateur d'incrémentation, c'est-à-dire l'augmentation de la valeur d'une variable de 1. C'est pourquoi C++ porte ce nom : cela signifie que C++ est un niveau au-dessus de C.Bjarne Stroustrup commence le développement de C with Classes (C avec classes) en 1979. Il travaille alors dans les laboratoires Bell où il est notamment collègue de l'inventeur du C Dennis Ritchie. L'idée de créer un nouveau langage venait de l'expérience en programmation de Stroustrup pour sa thèse de doctorat. Il s'agissait en l'occurrence d'améliorer le langage C. Stroustrup trouvait que Simula avait des fonctionnalités très utiles pour le développement de gros programmes mais qu'il était trop lent pour être utilisé en pratique (cela était dû à un problème d'implémentation du compilateur Simula), tandis que BCPL était rapide mais de trop bas niveau et non adapté au développement de gros logiciels. Quand Stroustrup commença à travailler aux laboratoires Bell, on lui demanda d'analyser le noyau UNIX en vue de faire du calcul distribué. Se rappelant sa thèse, Stroustrup commença à améliorer le langage C avec des fonctionnalités similaires à celle de Simula. C fut choisi parce qu'il est rapide, portable et d'usage général. En outre, il était une bonne base pour le principe original et fondateur de C++ : « vous ne payez pas pour ce que vous n'utilisez pas ». Dès le départ, le langage ajoutait à C la notion de classe (avec encapsulation des données), de classe dérivée, de vérification des types renforcés (typage fort), d'« inlining », et d'argument par défaut.Alors que Stroustrup développait C with classes, il écrivit CFront, un compilateur qui générait du code source C à partir de code source C with classes. La première commercialisation se fit en octobre 1985. En 1983 le nom « C++ » est inventé, et en 1984 le nom du langage passa de C with classes à celui de « C++ ». Parmi les nouvelles fonctionnalités qui furent ajoutées au langage, il y avait les fonctions virtuelles, la surcharge des opérateurs et des fonctions, les références, les constantes, le contrôle du typage amélioré et les commentaires en fin de ligne. En 1985 fut publiée la première édition de The C++ Programming Language, apportant ainsi une référence importante au langage qui n'avait pas encore de standard officiel. En 1989, c'est la sortie de la version 2.0 de C++. Parmi les nouvelles fonctionnalités, il y avait l'héritage multiple, les classes abstraites, les fonctions membres statiques, les fonctions membres constantes, et les membres protégés. En 1990, The Annotated C++ Reference Manual (« ARM ») fut publié apportant les bases du futur standard. Les ajouts de fonctionnalités tardifs qu'il comportait couvraient les templates, les exceptions, les espaces de noms, les nouvelles conversions et le type booléen.Pendant l'évolution du langage C++, la bibliothèque standard évoluait de concert. Le premier ajout à la bibliothèque standard du C++ concernait les flux d'entrées/sorties qui apportaient les fonctionnalités nécessaires au remplacement des fonctions C traditionnelles telles que printf et scanf. Ensuite, parmi les ajouts les plus importants, il y avait la Standard Template Library. Après des années de travail, un comité réunissant l'ANSI et l'ISO standardisa C++ en 1998 (ISO/CEI 14882:1998), l'année où le comité de standardisation se réunissait à Sophia Antipolis dans le sud de la France. Pendant quelques années après la sortie officielle du standard, le comité traita des problèmes remontés par les utilisateurs, et publia en 2003 une version corrigée du standard C++.Personne ne possède le langage C++. Il est libre de droits ; cependant, le document de standardisation n'est quant à lui pas disponible gratuitement.On peut considérer que C++ « est du C » avec un ajout de fonctionnalités. Cependant, plusieurs programmes syntaxiquement corrects en C ne le sont pas en C++, à commencer bien sûr par ceux qui font usage d'identificateurs correspondant à des mots-clefs en C++.Parmi les fonctionnalités ajoutées figurent :le typage des « prototypes » de fonctions (repris dans ANSI C89) ;La surcharge des fonctions ;les déclarations reconnues comme instructions (repris dans C99) ;les opérateurs new et delete pour la gestion d'allocation mémoire ;le type de données bool (booléen) ;les références & ;les variables et les fonctions membres const (repris partiellement par C à la fin des années 1980) ;les fonctions inline (repris dans C99) ;les paramètres par défaut dans les fonctions ;les référentiels lexicaux (espaces de noms) et l'opérateur de résolution de portée :: ;les classes, ainsi que tout ce qui y est lié : l'héritage, les fonctions membres, les fonctions membres virtuelles, les constructeurs et le destructeur ;la surcharge des opérateurs ;les templates ;la gestion d'exceptions ;l'identification de type pendant l'exécution (RTTI : run-time type information) ;le commentaire sur une ligne introduit par // (existant dans BCPL, repris dans C99) ;les références de rvalue && (C++11) ;la déduction de type à la compilation via auto (C++11) ;les expressions constantes constexpr (C++11);les fonctions lambda (C++11, étendu dans tous les standards publiés depuis) ;les boucles for basées sur une plage (C++11, étendu en C++20) ;les modules via import, export et module (C++20) ;les contraintes et concepts via concept et requires (C++20) ;les fonctions immédiates consteval (C++20) ;les coroutines (C++20) ;La compilation d'un programme en C++ effectue également un contrôle plus minutieux du typage.La bibliothèque standard du C++ englobe la Standard Template Library (STL) qui met à la disposition du programmeur des outils puissants comme des collections (conteneurs) et des itérateurs.À l'origine, la STL était une bibliothèque développée par Alexander Stepanov qui travaillait pour Hewlett-Packard. Dans la norme, celle-ci n'est pas appelée STL, car elle est considérée comme faisant partie de la bibliothèque standard de C++. Toutefois, beaucoup de personnes l'appellent encore de cette manière pour distinguer d'une part, les fonctions d'entrées/sorties comprises dans cette bibliothèque et, d'autre part, celles fournies par la bibliothèque C.Comme en C, l'utilisation d'une bibliothèque peut se faire par l'intermédiaire de la directive #include (suivie du nom du fichier d'en-tête), et certaines d'entre elles (cmath, thread, etc.) nécessitent d'être liées explicitement. Depuis C++20 le mot clé import peut servir à des fins similaires.Le langage C++ utilise les concepts de la programmation orientée objet et permet entre autres :la création de classes ;l'encapsulation ;des relations entre les classes :la composition de classes (composition dans un diagramme de classes),l'association de classes (en) (association dans un diagramme de classes),l'agrégation de classes (agrégation dans un diagramme de classes),la dépendance (dépendance dans un diagramme de classes),l'héritage simple et multiple (héritage dans un diagramme de classes) ;le polymorphisme ;l'abstraction ;la généricité ;la méta-programmation.L'encapsulation permet de faire abstraction du fonctionnement interne (c'est-à-dire la mise en œuvre) d'une classe et ainsi de ne se préoccuper que des services rendus par celle-ci. C++ met en œuvre l'encapsulation en permettant de déclarer les membres d'une classe avec le mot réservé public, private ou protected. Ainsi, lorsqu'un membre est déclaré :public, il sera accessible depuis n'importe quelle fonction ;private, il sera uniquement accessible d'une part, depuis les fonctions qui sont membres de la classe et, d'autre part, depuis les fonctions autorisées explicitement par la classe (par l'intermédiaire du mot réservé friend) ;protected, il aura les mêmes restrictions que s'il était déclaré private, mais il sera en revanche accessible par les classes filles.C++ n'impose pas l'encapsulation des membres dans leurs classes. On pourrait donc déclarer tous les membres publics, mais en perdant une partie des bénéfices apportés par la programmation orientée objet. Il est de bon usage de déclarer toutes les données privées, ou au moins protégées, et de rendre publiques les fonctions membres agissant sur ces données. Ceci permet de cacher les détails de la mise en œuvre de la classe.Voici l'exemple de Hello world donné dans The C++ Programming Language, Third Edition de Bjarne Stroustrup :Dans l'exemple ci-dessus, le code source std::cout << ""Hello, new world!\n"" envoie la chaîne de caractères ""Hello, new world!\n"" à l'objet global cout, défini dans l'espace de noms standard std, grâce à l'opérateur << de cout.En C++, le mot clef namespace permet de définir et de nommer des espaces de noms (namespaces), notion déjà présente en langage C ; en effet, le corps d'une routine, d'une structure de contrôle de flux d'exécution, d'une structure de données ou d'une section de code (délimitée par les accolades { et }) constitue un espace de noms. En C++, le corps d'une classe, à l'instar du corps d'une structure de données, constitue aussi un espace de noms.Dans différents espaces de noms, on peut ainsi définir des entités (routines, variables, etc.) ayant le même identificateur. L'ambiguïté est résolue en utilisant le nom de l'espace de nom devant l'opérateur de portée (::) pour indiquer l'espace de noms dans lequel on veut accéder. Notez que l'espace de noms global du programme n'a pas de nom. Pour accéder à une entité globale, cachée par une entité locale par exemple, on utilise l'opérateur de portée précédé d'aucun nom.Il est possible de spécifier un espace de noms précis à utiliser afin d'éviter d'avoir à recourir à l'opérateur de résolution de portée. Pour cela, le mot-clé using est utilisé avec cette syntaxe :Ainsi, pour utiliser la variable cout définie dans le namespace standard sans utiliser l'opérateur de résolution de portée, il est possible d'écrire using namespace std; ou using std::cout;. Cela est valable pour tous les espaces de noms. Cette instruction se place en général avant le début du code source proprement dit :Il est aussi possible, et conseillé, d'importer un symbole particulier, ou de placer cette instruction dans une fonction afin de limiter la portée :Le mot-clé using peut aussi être utilisé dans les classes. Si une classe B hérite d'une classe A, elle peut grâce à ce mot-clé passer des membres protected de A en public dans B, ou encore démasquer une fonction membre de A qui le serait par une fonction membre de B de même nom :Le programme ci-dessus affiche :Il est aussi possible de définir un nouveau nom pour un namespace :Il est d'usage de séparer prototype (déclaration) et implémentation (définition) de classe dans deux fichiers : la déclaration se fait dans un fichier d'en-tête (dont l'extension varie selon les préférences des développeurs : sans extension dans le standard, .h comme en C, .hh ou .hpp ou .hxx pour différencier le code source C++ du C) alors que la définition se fait dans un fichier source (d'extension également variable : .c comme en C, .cc ou .cpp ou .cxx pour différencier C++ du C).Exemple de la déclaration d'une classe comportant des attributs privés et des fonctions membres publiques :Le nom d'une fonction membre déclarée par une classe doit nécessairement être précédé du nom de la classe suivi de l'opérateur de résolution de portée ::.Exemple de définition des fonctions membres d'une classe (celle déclarée précédemment) :Les Modèles (ou templates) permettent d'écrire des variables, des fonctions et des classes en paramétrant le type de certains de leurs constituants (type des paramètres ou type de retour pour une fonction, type des éléments pour une classe collection par exemple). Les modèles permettent d'écrire du code générique, c'est-à-dire qui peut servir pour une famille de fonctions ou de classes qui ne diffèrent que par le type de leurs constituants.Les paramètres peuvent être de différentes sortes :types simples, tels que les classes ou les types élémentaires (int, double, etc.) ;tableaux de taille constante, dont la taille, déduite par le compilateur, peut être utilisée dans l'instanciation du modèle ;constantes scalaires, c'est-à-dire de type entier (int, char, bool), mais pas flottant (float, double) car leur représentation binaire ne fait pas partie de la norme du langage (jusqu'en C++20 où ils sont autorisés) ;templates, dont la définition doit être passée en paramètre, ce qui permet notamment de s'appuyer sur la définition abstraite, par exemple, d'une collection ;pointeurs ou références, à condition que leur valeur soit définie à l'édition de liens ;fonction membre d'une classe, dont la signature et la classe doivent être aussi passées en paramètres ;attribut d'une classe, dont le type et la classe doivent être aussi passés en paramètres.En programmation, il faut parfois écrire de nombreuses versions d'une même fonction ou classe suivant les types de données manipulées. Par exemple, un tableau de int ou un tableau de double sont très semblables, et les fonctions de tri ou de recherche dans ces tableaux sont identiques, la seule différence étant le type des données manipulées. En résumé, l'utilisation des templates permet de « paramétrer » le type des données manipulées.Les avantages des modèles sont :des écritures uniques pour les fonctions et les classes ;moins d'erreurs dues à la réécriture ;Dans la bibliothèque standard C++, on trouve de nombreux templates. On citera à titre d'exemple, les entrées/sorties, les chaînes de caractères ou les conteneurs. Les classes string, istream, ostream et iostream sont toutes des instanciations de type char.Les fonctions de recherche et de tri sont aussi des templates écrits et utilisables avec de nombreux types.Dans la ligne float f = max<float>(1, 2.2f);, on doit explicitement donner le type float pour le type paramétré T car le compilateur ne déduit pas le type de T lorsqu'on passe en même temps un int (1) et un float (2.2f).Un template donné peut avoir plusieurs instanciations possibles selon les types donnés en paramètres. Si un seul paramètre est spécialisé, on parle de spécialisation partielle. Ceci permet par exemple :de choisir un type de calcul selon qu'un type est un entier, un flottant, une chaîne de caractères, etc. Spécialisons l'exemple précédent pour le cas des pointeurs de chaînes de caractères :d'effectuer au moment de la compilation des calculs arithmétiques, si et seulement si tous les arguments sont connus à ce moment. Un exemple classique est le calcul de la fonction factorielle :À partir de C++14 pour arriver aux mêmes fins nous pourrions aussi utiliser les variables templates :Ainsi nous pouvons écrire factorielle<8>; à la place de Factorielle<8>::value;.Le mécanisme décrit par l'abréviation SFINAE (Substitution Failure Is Not an Error) permet de surcharger un template par plusieurs classes (ou fonctions), même si certaines spécialisations, par exemple, ne peuvent pas être utilisées pour tous les paramètres de templates. Le nom décrit précisément le fonctionnement du mécanisme, littéralement l’acronyme de « Un échec de substitution n'est pas une erreur », le compilateur, lors de la substitution, ignore alors les instanciations inapplicables, au lieu d'émettre une erreur de compilation. Par exemple :Ici f est définie deux fois, le type de retour est conditionné par le type donné en paramètre, il est du type du retour de f.foo() dans le premier cas et de celui de f.bar() dans le deuxième cas. Ainsi, si on appelle f avec un objet de la classe A, seule la première fonction fonctionne puisque la classe A n'a pas de fonction membre bar() et donc la substitution est possible avec cette première version mais pas pour la deuxième. Ainsi, f(a) appelle la première version de f, f(b) appelle la deuxième avec le même raisonnement, mais cette fois pour la fonction membre bar().Si lors d'un développement à venir, un développeur venait à écrire une nouvelle classe ayant une fonction membre publique foo ou bien (ou exclusif) bar, il pourrait également utiliser f avec.Le polymorphisme d'inclusion est mis en œuvre à l'aide du mécanisme des fonctions membres virtuelles en C++. Une fonction membre est rendue virtuelle par le placement du mot-clé virtual devant la déclaration de la fonction membre dans la classe. Lorsqu'une fonction membre virtuelle est appelée, l'implémentation de la fonction membre exécutée est choisie en fonction du type réel de l'objet. L'appel n'est donc résolu qu'à l'exécution, le type de l'objet ne pouvant pas a priori être connu à la compilation.Le mot-clé virtual indique au compilateur que la fonction membre déclarée virtuelle est susceptible d'être redéfinie dans une classe dérivée. Il suffit alors de dériver une classe et de définir une nouvelle fonction membre de même signature (même nom, paramètres compatibles — voir la notion de covariance). Ainsi l'appel de cette fonction membre sur un objet accédé en tant qu'objet de la classe de base mais appartenant en réalité à la classe dérivée donnera lieu à l'appel de la fonction membre définie dans la classe dérivée.En particulier, il est obligatoire d'utiliser le mot-clé virtual devant la déclaration du destructeur de la classe de base lorsque le programme souhaite pouvoir détruire un objet via un pointeur d'instance de la classe de base au lieu d'un pointeur d'instance de la classe dérivée.Ce type de polymorphisme (le polymorphisme d'inclusion) est dit dynamique. Le mécanisme de la surcharge de fonction qui est un polymorphisme ad hoc est de type statique. Dans les deux cas il faut appliquer une logique (par exemple : le nombre et le type des paramètres) pour résoudre l'appel. Dans le cas de la surcharge de fonction, la logique est entièrement calculée à la compilation. Ce calcul permet des optimisations rendant le polymorphisme statique plus rapide que sa version dynamique. La liaison dynamique de fonctions membres issues du mécanisme des fonctions membres virtuelles induit souvent une table cachée de résolution des appels, la table virtuelle. Cette table virtuelle augmente le temps nécessaire à l'appel de fonction membre à l'exécution par l'ajout d'une indirection supplémentaire.Le choix entre liaison dynamique et surcharge (polymorphisme dynamique et statique) est typiquement un problème de calculabilité des appels, ayant souvent pour conséquence finale un choix entre expressivité et performance.Malgré ce dynamisme, il est à noter que le compilateur est capable de « dévirtualiser » les appels de fonctions membres qui peuvent être résolus au moment de la compilation. Dans gcc par exemple, l'option -fdevirtualize lors de la compilation permet cette optimisation, s'il est possible de faire une telle résolution.Un programme C++ peut être produit avec des outils qui automatisent le processus de construction. Les plus utilisés sont :make ;Ant (génération portable en XML) ;SCons (génération portable en Python) ;CMake (génération de Makefile portable) ;Bazel.Anjuta DevStudio ;C++ Builder ;CLion (en) ;Code::Blocks (open-source) ;Dev-C++ et son extension RAD WxDev-C++ ;Eclipse avec le plugin CDT (open-source) ;Emacs (libre) ;KDevelop ;NetBeans (open-source) ;QtCreator (open-source) ;Sun Studio ;Vim ;Microsoft Visual C++ (a été intégré au framework Visual Studio) ;Xcode.GCC pour GNU Compiler Collection (libre, multilangage et multiplateforme : UNIX, Windows, DOS, etc.) ;Clang ;Microsoft Visual C++ (Windows) ;Borland C++ Builder (Windows) ;Intel C++ Compiler (Windows, Linux, MacOS) ;Open64 (en) compilateur opensource d'AMD (Linux) ;Digital Mars C/C++ compiler (Windows) ;Open Watcom ;Boost ;Qt ;Gtkmm ;wxWidgets ;SFML ;OpenCV ;SDLmm, surcouche C++ à la SDL ;LLVM.etc. Ouvrages en langue anglaise [Deitel et Deitel 2011] (en) P. Deitel et H. Deitel, C++ : How to Program, 20 Hall, 2011, 8e éd., 1104 p. (ISBN 978-0-13-266236-9).[Dawson 2010] (en) M. Dawson, Beginning C++ Through Game Programming, Course Technology PTR, 2010, 3e éd., 432 p. (ISBN 978-1-4354-5742-3).[Gregoire, Solter et Kleper 2011] (en) Marc Gregoire, Nicolas A. Solter et Scott J. Kleper, Professional C++, John Wiley, octobre 2011, 1104 p. (ISBN 978-0-470-93244-5, présentation en ligne).[Josuttis 2011] (en) Nicolaï Josuttis, The C++ Standard Library, A Tutorial and Reference, Addison-Wesley, 2011, 2e éd., 1099 p. (ISBN 978-0-321-62321-8, présentation en ligne).[Koenig et Moo 2000] (en) A. Koenig et B. Moo, Accelerated C++ : Practical Programming by Example, Addison-Wesley, 2000, 1re éd., 352 p. (ISBN 978-0-201-70353-5).[Lippman, Lajoie et Moo 2012] (en) Stanley B. Lippman, Josée Lajoie et Barbara E. Moo, C++ Primer : 5th Edition, août 2012, 5e éd., 1399 p. (ISBN 978-0-321-71411-4).[Lischner 2003] (en) R. Lischner, C++ in a nutshell, O'Reilly Media, 2003, 1re éd., 704 p. (ISBN 978-0-596-00298-5).[Meyers 2005] (en) S. Meyers, Effective C++ : 55 Specific Ways to Improve Your Programs and Designs, Addison-Wesley Professional, 2005, 3e éd., 320 p. (ISBN 978-0-321-33487-9, présentation en ligne).[Oualline 2003] (en) S. Oualline, Practical C++ programming, O'Reilly Media, 2003, 2e éd., 600 p. (ISBN 978-0-596-00419-4, présentation en ligne).[Lafore 2001] (en) R. Lafore, Object-oriented programming in C++, Sams, 2001, 4e éd., 1040 p. (ISBN 978-0-672-32308-9).[Prata 2011] (en) S. Prata, C++ Primer Plus (Developer's Library), Addison-Wesley Professional, 2011, 6e éd., 1200 p. (ISBN 978-0-321-77640-2, présentation en ligne).[Stroustrup 2009] (en) Bjarne Stroustrup, Programming : Principles and Practice using C++, Addison-Wesley, 2009, 1236 p. (ISBN 978-0-321-54372-1).[Stroustrup 2013] (en) Bjarne Stroustrup, The C++ Programming Language : 4th Edition, Addison-Wesley Professional, 2013, 4e éd., 1368 p. (ISBN 978-0-321-56384-2).[Stroustrup 1994] (en) Bjarne Stroustrup, The Design and Evolution of C++, Addison-Wesley professional, 1994, 1re éd., 480 p. (ISBN 978-0-201-54330-8).[Sutter 1999] (en) H. Sutter, Exceptional C++ : 47 Engineering Puzzles, Programming Problems, and Solutions, Addison-Wesley Professional, 1999, 240 p. (ISBN 978-0-201-61562-3, présentation en ligne).[Vandevoorde et Josuttis 2002] (en) David Vandevoorde et Nicolaï Josuttis, C++ Templates : the Complete Guide, Addison-Weslay, 2002, 528 p. (ISBN 978-0-201-73484-3).[Vandevoorde 1998] (en) David Vandevoorde, C++ Solutions : Companion to the C++ Programming Language, Addison-Wesley, 1998, 3e éd., 292 p. (ISBN 978-0-201-30965-2). Ouvrages en langue française [Benharrats et Vittupier 2021] Mehdi Benharrat et Benoît Vittupier, Le guide du C++ moderne : de débutant à développeur, D-Booker, 2021, 1re éd., 708 p. (ISBN 978-2-8227-0881-4, présentation en ligne).[Chappelier et Seydoux 2005] J-C. Chappelier et F. Seydoux, C++ par la pratique : Recueil d'exercices corrigés et aide-mémoire, PPUR, 2005, 2e éd., 412 p. (ISBN 978-2-88074-732-9, présentation en ligne).[Deitel et Deitel 2004] P. Deitel et H. Deitel, Comment programmer en C++, Reynald Goulet, 2004, 1178 p. (ISBN 978-2-89377-290-5).[Delannoy 2001] Claude Delannoy, Programmer en langage C++, Paris, Eyrolles, 2011, 8e éd., 822 p. (ISBN 978-2-212-12976-2, présentation en ligne).[Delannoy 2007] Claude Delannoy, Exercices en langage C++, Paris, Eyrolles, 2007, 3e éd., 336 p. (ISBN 978-2-212-12201-5, présentation en ligne).[Géron et Tawbi 2003] Aurélien Géron et Fatmé Tawbi (préf. Gilles Clavel), Pour mieux développer avec C++ : Design patterns, STL, RTTI et smart pointers, Paris, Dunod, 2003, 188 p. (ISBN 978-2-10-007348-1).[Guidet 2008] Alexandre Guidet, Programmation objet en langage C++, Paris, Ellipses, coll. « Cours et exercices. », 2008, 364 p. (ISBN 978-2-7298-3693-1, OCLC 221607125, BNF 41206426).[Hubbard 2002] J. R. Hubbard (trad. Virginie Maréchal), C++ [« Schaum's easy outline of programming with C++ »], Paris, EdiScience, coll. « Mini Schaum's », 2002, 192 p. (ISBN 978-2-10-006510-3).[Liberty et Jones 2005] Jesse Liberty et Bradley Jones (trad. Nathalie Le Guillou de Penanros), Le langage C++ [« Teach yourself C++ in 21 days »], Paris, CampusPress, 2005, 859 p. (ISBN 978-2-7440-1928-9).[Stephens, Diggins, Turkanis et al. 2006] D. Ryan Stephens, Christopher Diggins, Jonathan Turkanis et J. Cogswell (trad. Yves Baily & Dalil Djidel), C++ en action [« C++ Cookbook - Solutions and Examples for C++ Programmers »], Paris, O'Reilly, 2006, 555 p. (ISBN 978-2-84177-407-4, OCLC 717532188, BNF 40170870).[Stroustrup 2012] Bjarne Stroustrup (trad. Marie-Cécile Baland, Emmanuelle Burr, Christine Eberhardt), Programmation : principes et pratique avec C++ : Avec plus de 1000 exercices. [« Programming : principles and practice using C++ »], Paris, Pearson education, 2012, 944 p. (ISBN 978-2-7440-7718-0).[Stroustrup 2003] Bjarne Stroustrup (trad. Christine Eberhardt), Le langage C++ [« The C++ programming language »], Paris, Pearson education, 2003, 1098 p. (ISBN 978-2-7440-7003-7 et 2-744-07003-3).[Sutter et Alexandrescu 2005] Herb Sutter et Andrei Alexandrescu, Standards de programmation C [« C++ Coding Standards: 101 Rules, Guidelines, and Best Practices »], Paris, Pearson Education France, coll. « C++ », 2005, 243 p. (ISBN 978-2-7440-7144-7 et 2-744-07144-7).Langage C(en) Le Comité du Standard C++(fr) Documentation du C++ (wiki sous double licence CC-BY-SA et GFDL)(en) Documentation du C++ (le même wiki mais en anglais et plus complet)(en) Documentation du C++ (contenu non libre, édité par The C++ ressources network) Portail de la programmation informatique   Portail de l’informatique"
informatique;"En informatique, le code source est un texte qui présente les instructions composant un programme sous une forme lisible, telles qu'elles ont été écrites dans un langage de programmation. Le code source se matérialise généralement sous la forme d'un ensemble de fichiers texte.Le code source est souvent traduit — par un assembleur ou un compilateur — en code binaire composé d'instructions exécutables par le processeur. Il peut sinon être directement interprété à l'exécution du programme. Dans ce deuxième cas, il est parfois traduit au préalable en un code intermédiaire dont l'interprétation est plus rapide.L'expression est une traduction de l'anglais source code. Les expressions omettant le terme de code sont communes : les sources, le source.Dans les tout premiers temps de l'informatique, les programmes étaient entrés dans la mémoire de l'ordinateur par l'intermédiaire des interrupteurs du pupitre de commande, sous forme du codage binaire des instructions machines. Ce qui ne convenait qu'à de tout petits programmes. Ils ont ensuite été chargés depuis des bandes perforées, puis des cartes perforées.Très rapidement, les programmes ont été rédigés dans un langage symbolique, langage d'assemblage ou langage évolué comme Fortran, Cobol, puis traduit automatiquement par un programme (assembleur, compilateur).Avec l'apparition des disques magnétiques et des consoles interactives, des éditeurs de lignes puis des éditeurs de textes ont été utilisés pour taper et modifier le code source.Les possibilités limitées des ordinateurs de l'époque nécessitaient souvent l'impression du code source sur papier continu (en) avec des bandes Carol.Aujourd'hui, il existe des environnements de développement, dits Environnement de développement intégré (IDE, Integrated Development Environment), qui intègrent notamment les tâches d'édition et de compilation.Un logiciel est une suite d'instructions données à une machine. Un processeur ne peut exécuter que des instructions représentées sous une forme binaire particulière. Sauf mécanismes expérimentaux, il n'est pas possible pour un être humain de saisir directement un code binaire dans la représentation qu'en attend le processeur : un être humain ne peut pas écrire directement les champs de bits aux adresses attendues. Il est obligé de passer par un code distinct appelé code source, et qui est par la suite traduit dans la représentation binaire attendue par la machine puis chargé et exécuté par la cible.Toutefois, l'écriture d'un code sous forme binaire, même dans un fichier séparé, pose de nombreux problèmes de compréhension aux êtres humains. C'est une représentation uniquement constituée d'une suite ininterrompue de 0 et de 1 qui est difficile à lire, à écrire et à maintenir sans assistance technique. La diversité des microprocesseurs et des composants présents dans un ordinateur ou automate, implique qu'un code binaire généré pour un système ne puisse pas être a priori le même que sur une machine distincte. Aussi, il existe autant de codes binaires que de configurations et une complexité accrue excluant que l'être humain puisse concevoir simplement un code binaire de grande ampleur.Pour éviter ces écueils, et puisqu'une traduction est toujours nécessaire, l'être humain écrit un code textuel afin qu'il soit plus lisible, plus compréhensible et plus simple à maintenir : c'est le code source écrit dans un langage de programmation. Il est, dans la plupart des cas, plus lisible, plus simple à écrire et indépendant du système cible. Un programme tiers (compilateur, interpréteur ou machine virtuelle) se charge de la traduction du code source en code binaire exécutable par la cible.Le code généré par l'être humain est appelé code source ; la façon dont est rédigé ce code source est appelée langage de programmation ; le traducteur de ce code dans sa représentation binaire est appelé compilateur, interpréteur ou machine virtuelle selon les modalités de la traduction.Dans la plupart des langages, on peut distinguer différents éléments dans un code source :les éléments décrivant l’algorithme et les données (le code source proprement dit) :des symboles identifiant des variables, des mots clefs dénotant des instructions, des représentations de données ;des constantes littérales.les commentaires, qui documentent le code source le plus souvent en langage naturel, destinés aux relecteurs du code source. Ils ne sont pas nécessaires à la production du code exécutable mais peuvent être utilisés par le compilateur pour, par exemple, produire automatiquement de la documentation.Un code est plus facile à lire et à écrire avec un éditeur fournissant une coloration syntaxique permettant de distinguer les différents éléments du code source. Les commentaires peuvent par exemple être mis en vert.Exemple de code en Ruby :Autre exemple de code en Ruby :Autre exemple de code en Ruby :L'analogie du code source et de la recette de cuisine est souvent employée dans une volonté de vulgarisation. Une recette est une liste organisée d'ingrédients dont les quantités et les fonctions sont définies. Le but est d'obtenir le résultat voulu par le cuisinier, selon une technique et un enchaînement d'opérations déterminés.Ainsi le code source peut être apparenté à une recette de cuisine.Ainsi, une personne dégustant un plat est en mesure de deviner les ingrédients qui le composent et d'imaginer comment le réaliser. Néanmoins, pour un plat très raffiné et subtil (comme pourrait l'être un programme), il est fort probable qu'elle ignore le mode opératoire du cuisinier. Pour le connaître, une recette détaillée serait nécessaire (pour un programme, la recette peut compter plusieurs millions de lignes de code). La solution alternative à cela serait d'acheter des plats préparés, c'est un peu ce que l'on fait lorsqu'on achète des logiciels.Le code source peut être public ou privé (voir logiciel libre et logiciel propriétaire). Toutefois, le code binaire n'étant qu'une traduction du code source, il est toujours possible d'étudier un logiciel à partir de son code binaire. La légalité des techniques utilisées à ces fins dépend du pays et de l'époque. Elle peut notamment être mise en œuvre pour percer les secrets d'une machine comme l'ES3B. Portail de la programmation informatique"
informatique;"En informatique, un compilateur  est un programme qui transforme un code source en un code objet. Généralement, le code source est écrit dans un langage de programmation (le langage source), il est de haut niveau d'abstraction, et facilement compréhensible par l'humain. Le code objet est généralement écrit en langage de plus bas niveau (appelé langage cible), par exemple un langage d'assemblage ou langage machine, afin de créer un programme exécutable par une machine.Un compilateur effectue les opérations suivantes : analyse lexicale, pré-traitement (préprocesseur), analyse syntaxique (parsing), analyse sémantique, et génération de code optimisé. La compilation est souvent suivie d'une étape d’édition des liens, pour générer un fichier exécutable. Quand le programme compilé (code objet) est exécuté sur un ordinateur dont le processeur ou le système d'exploitation est différent de celui du compilateur, on parle de compilation croisée.On distingue deux options de compilation : Ahead-of-time (AOT), où il faut compiler le programme avant de lancer l'application : c'est la situation traditionnelle.Compilation à la volée (just-in-time, en abrégé JIT) : cette faculté est apparue dans les années 1980 (par exemple avec Tcl/Tk).La chaine de compilation                 Les logiciels des premiers ordinateurs étaient écrits en langage assembleur. Les langages de programmation de plus haut niveau (dans les couches d'abstraction) n'ont été inventés que lorsque les avantages apportés par la possibilité de réutiliser le logiciel sur différents types de processeurs sont devenus plus importants que le coût de l'écriture d'un compilateur. La capacité de mémoire très limitée des premiers ordinateurs a également posé plusieurs problèmes techniques dans le développement des compilateurs.Vers la fin des années 1950, des langages de programmation indépendants des machines font pour la première fois leur apparition. Par la suite, plusieurs compilateurs expérimentaux sont développés. Le premier compilateur, A-0 System (pour le langage A-0) est écrit par Grace Hopper, en 1952. L'équipe FORTRAN dirigée par John Backus d'IBM est considérée comme ayant développé le premier compilateur complet, durant la période 1954-1957, et il s'agit du premier compilateur optimiseur, l'objectif de l'équipe étant de générer un code en langage machine quasiment aussi rapide que celui qu'aurait généré un programmeur. COBOL, développé en 1959 et reprenant largement des idées de Grace Hopper, est le premier langage à être compilé sur plusieurs architectures.Dans plusieurs domaines d'application[Lesquels ?], l'idée d'utiliser un langage de plus haut niveau d'abstraction s'est rapidement répandue. Avec l'augmentation des fonctionnalités supportées par les langages de programmation plus récents et la complexité croissante de l'architecture des ordinateurs, les compilateurs se sont de plus en plus complexifiés.En 1962, le premier compilateur « auto-hébergé » - capable de compiler en code objet, son propre code source  exprimé en langage de haut niveau - est créé, pour le Lisp, par Tim Hart et Mike Levin au Massachusetts Institute of Technology (MIT). À partir des années 1970, il est devenu très courant de développer un compilateur dans le langage qu'il doit compiler, faisant du  Pascal et du C des langages de développement très populaires.On peut aussi utiliser un langage ou un environnement spécialisé dans le développement de compilateurs : on parle lors d'outils de méta-compilation, et on utilise par exemple un compilateur de compilateur. Cette méthode est particulièrement utile pour réaliser le premier compilateur d'un nouveau langage ; l'utilisation d'un langage adapté et rigoureux[Par exemple ?] facilite ensuite mise au point et évolution.La tâche principale d'un compilateur est de produire un code objet correct qui s'exécutera sur un ordinateur. La plupart des compilateurs permettent d'optimiser le code, c'est-à-dire qu'ils vont chercher à améliorer la vitesse d'exécution, ou réduire l'occupation mémoire du programme.En général, le langage source est « de plus haut niveau » que le langage cible, c'est-à-dire qu'il présente un niveau d'abstraction supérieur. De plus, le code source du programme est généralement réparti dans plusieurs fichiers.Un compilateur fonctionne par analyse-synthèse : au lieu de remplacer chaque construction du langage source par une suite équivalente de constructions du langage cible, il commence par analyser le texte source pour en construire une représentation intermédiaire qu'il traduit à son tour en langage cible.On sépare le compilateur en au moins deux parties : une partie avant (ou frontale), parfois appelée « souche », qui lit le texte source et produit la représentation intermédiaire ; et une partie arrière (ou finale), qui parcourt cette représentation pour produire le texte cible. Dans un compilateur idéal, la partie avant est indépendante du langage cible, tandis que la partie arrière est indépendante du langage source.Certains compilateurs effectuent des traitements substantiels sur la partie intermédiaire, devenant une partie centrale à part entière, indépendante à la fois du langage source et de la machine cible. On peut ainsi écrire des compilateurs pour toute une gamme de langages et d'architectures en partageant la partie centrale, à laquelle on attache une partie avant par langage et une partie arrière par architecture.Les étapes de la compilation incluent :le prétraitement, nécessaire pour certains langages comme C, qui prend en charge la substitution de macro et de la compilation conditionnelle.Généralement, la phase de prétraitement se produit avant l'analyse syntaxique ou sémantique ; par exemple dans le cas de C, le préprocesseur manipule les symboles lexicaux plutôt que des formes syntaxiques.l'analyse lexicale, qui découpe le code source en petits morceaux appelés jetons (tokens).Chaque jeton est une unité atomique unique de la langue (unités lexicales ou lexèmes), par exemple un mot-clé, un identifiant ou un symbole. La syntaxe de jeton est généralement un langage régulier, donc reconnaissable par un automate à états finis.Cette phase est aussi appelée à balayage ou lexing ; le logiciel qui effectue une analyse lexicale est appelé un analyseur lexical ou un scanner. Un analyseur lexical pour un langage régulier peut être généré par un programme informatique, à partir d'une description du langage par des expressions régulières. Deux générateurs classiques sont lex et flex.l'analyse syntaxique implique l'analyse de la séquence jeton pour identifier la structure syntaxique du programme.Cette phase s'appuie généralement sur la construction d'un arbre d'analyse ; on remplace la séquence linéaire des jetons par une structure en arbre construite selon la grammaire formelle qui définit la syntaxe du langage. Par exemple, une condition est toujours suivie d'un test logique (égalité, comparaison…). L'arbre d'analyse est souvent modifié et amélioré au fur et à mesure de la compilation. Yacc et GNU Bison sont les analyseurs syntaxiques les plus utilisés.l'analyse sémantique est la phase durant laquelle le compilateur ajoute des informations sémantiques à l'arbre d'analyse et construit la table des symboles.Cette phase vérifie le type (vérification des erreurs de type), ou l'objet de liaison (associant variables et références de fonction avec leurs définitions), ou une tâche définie (toutes les variables locales doivent être initialisées avant utilisation), peut émettre des avertissements, ou rejeter des programmes incorrects.L'analyse sémantique nécessite habituellement un arbre d'analyse complet, ce qui signifie que cette phase fait suite à la phase d'analyse syntaxique, et précède logiquement la phase de génération de code ; mais il est possible de replier ces phases en une seule passe.la transformation du code source en code intermédiaire ;l'application de techniques d'optimisation sur le code intermédiaire : c'est-à-dire rendre le programme « meilleur » selon  son usage (voir infra) ;la génération de code avec l'allocation de registres et la traduction du code intermédiaire en code objet, avec éventuellement l'insertion de données de débogage et d'analyse de l'exécution ;et finalement l'édition des liens.L'analyse lexicale, syntaxique et sémantique, le passage par un langage intermédiaire et l'optimisation forment la partie frontale.La génération de code et l'édition de liens constituent la partie finale.Ces différentes étapes font que les compilateurs sont toujours l'objet de recherches.L'implémentation (réalisation concrète) d'un langage de programmation peut être interprétée ou compilée. Cette réalisation est un compilateur ou un interpréteur, et un langage de programmation peut avoir une implémentation compilée, et une autre interprétée.On parle de compilation si la traduction est faite avant l'exécution (le principe d'une boucle est alors traduit une fois), et d'interprétation si la traduction est finie pas à pas, durant l'exécution (les éléments d'une boucle sont alors examinés à chaque usage).L'interprétation est utile pour la mise au point ou si les moyens sont limités. La compilation est préférable en exploitation.Les premiers compilateurs ont été écrits directement en langage assembleur, un langage symbolique élémentaire correspondant aux instructions du processeur cible et quelques structures de contrôle légèrement plus évoluées. Ce langage symbolique doit être assemblé (et non compilé) et lié pour obtenir une version exécutable. En raison de sa simplicité, un programme simple suffit à le convertir en instructions machines.Les compilateurs actuels sont généralement écrits dans le langage qu'ils doivent compiler ; par exemple un compilateur C est écrit en C, SmallTalk en SmallTalk, Lisp en Lisp, etc. Dans la réalisation d'un compilateur, une étape décisive est franchie lorsque le compilateur pour le langage X est suffisamment complet pour se compiler lui-même : il ne dépend alors plus d'un autre langage (même de l'assembleur) pour être produit.Il est complexe de détecter un bogue de compilateur. Par exemple, si un compilateur C comporte un bogue, les programmeurs en langage C auront naturellement tendance à mettre en cause leur propre code source, non pas le compilateur.Pire, si ce compilateur buggé (version V1) compile un compilateur (version V2) non buggé, l'exécutable compilé (par V1) du compilateur V2 pourrait être buggé. Pourtant son code source est bon. Le bootstrap oblige donc les programmeurs de compilateurs à contourner les bugs des compilateurs existants.La classification des compilateurs par nombre de passes a pour origine le manque de ressources matérielles des ordinateurs.La compilation est un processus coûteux et les premiers ordinateurs n'avaient pas assez de mémoire pour contenir un programme devant faire ce travail. Les compilateurs ont donc été divisés en sous programmes qui font chacun une lecture de la source pour accomplir les différentes phases d’analyse lexicale, d'analyse syntaxique et d'analyse sémantique.L'aptitude à combiner le tout en un seul passage a été considérée comme un avantage, car elle simplifie l'écriture du compilateur, qui s'exécute généralement plus rapidement qu’un compilateur multi passe.Ainsi, dus aux ressources limitées des premiers systèmes, de nombreux langages ont été spécifiquement conçus afin qu'ils puissent être compilés en un seul passage (par exemple, le langage Pascal). Structure non linéaire du programme Dans certains cas, telle ou telle fonctionnalité du langage requiert que son compilateur effectue plus d'une passe. Par exemple, considérons une déclaration figurant à la ligne 20 de la source qui affecte la traduction d'une déclaration figurant à la ligne 10. Dans ce cas, la première passe doit recueillir des renseignements sur les déclarations, tandis que la traduction proprement dite ne s’effectue que lors d'un passage ultérieur. Optimisations Le fractionnement d'un compilateur en petits programmes est une technique utilisée par les chercheurs intéressés à produire des compilateurs performants. En effet, l'inconvénient de la compilation en une seule passe est qu'elle ne permet pas l'exécution de la plupart des optimisations sophistiquées nécessaires à la génération de code de haute qualité. Il devient alors difficile de dénombrer exactement le nombre de passes qu’un compilateur optimisant effectue. Fractionnement de la démonstration de correction Démontrer la correction d'une série de petits programmes nécessite souvent moins d'effort que de démontrer la correction d'un plus grand programme unique équivalent.Un compilateur de compilateur est un programme qui peut générer une, voire toutes les parties d'un compilateur.On peut par exemple compiler les bases d'un langage, puis, utiliser les bases du langage pour compiler le reste.Selon l'usage et la machine qui va exécuter un programme, on peut vouloir optimiser la vitesse d'exécution, l'occupation mémoire, la consommation d'énergie, la portabilité sur d'autres architectures, ou le temps de compilation.Il existe des compilateurs qui sont vérifiés mathématiquement. Ces compilateurs garantissent que les propriétés de sécurité prouvées sur le code source sont également valables pour le code compilé exécutable,. Ce type de compilateurs est notamment utilisé pour le développement d'algorithmes de contrôle de vol et de navigation dans l'aviation ou dans le domaine de l'énergie nucléaire.La compilation croisée fait référence aux chaînes de compilation capables de traduire un code source en code objet dont l'architecture processeur diffère de celle où la compilation est effectuée. Ces chaînes sont principalement utilisées en informatique industrielle et dans les systèmes embarqués.Certains compilateurs traduisent un langage source en langage machine virtuel (dit langage intermédiaire), c'est-à-dire en un code (généralement binaire) exécuté par une machine virtuelle : un programme émulant les principales fonctionnalités d'un ordinateur. De tels langages sont dits semi-compilés. Le portage d'un programme ne requiert ainsi que le portage de la machine virtuelle, qui sera de fait soit un interprète, soit un second traducteur (pour les compilateurs multi-cibles). Ainsi, des compilateurs traduisent Pascal en P-Code, Modula 2 en M-Code, Simula en S-code, ou plus récemment du code Java en bytecode Java (code objet).     Quand la compilation repose sur un byte code, on parle de compilation à la volée. On utilise alors des machines virtuelles comme la machine virtuelle Java avec laquelle on peut notamment compiler du Scala. Il est possible dans certains langages d'utiliser une bibliothèque permettant la compilation à la volée de code entré par l'utilisateur, par exemple en C avec libtcc.D’autres compilateurs traduisent un code d’un langage de programmation vers un autre. On les appelle des transcompilateurs, ou bien encore par anglicisme, des transpileurs ou transpilateurs. Par exemple, le logiciel LaTeX permet, à partir d’un code source en LaTeX, d’obtenir un fichier au format PDF (avec par exemple la commande pdflatex sous Ubuntu) ou HTML. Autre exemple, LLVM est une bibliothèque aidant à réaliser des compilateurs, également utilisée par AMD pour développer « HIP », un transcompilateur de code CUDA (langage spécifique à NVIDIA et très utilisé) afin de l’exécuter sur les processeurs graphiques d’AMD.          Certains compilateurs traduisent, de façon incrémentale ou interactive, le programme source (entré par l’utilisateur) en code machine. On peut citer comme exemple certaines implantations de Common Lisp (comme SBCL (en)).Alfred Aho, Monica Lam, Ravi Sethi et Jeffrey Ullman (trad. de l'anglais par Philippe Deschamp, Bernard Lorho, Benoît Sagot et François Thomasset), Compilateurs : Principes, techniques et outils [« Compilers: Principles, Techniques, and Tools »], France, Pearson Education, novembre 2007, 2e éd. (1re éd. 1977), 901 p. (ISBN 978-2-7440-7037-2, présentation en ligne)InterprèteLow Level Virtual MachineCompilation à la voléeCompilation incrémentaleCompilation anticipéeDécompilateur, programme qui traduit un langage de bas niveau vers un langage de plus haut niveauGCC est une suite de compilation particulièrement connue, beaucoup utilisée pour les langages C et C++, mais également Java ou encore Ada.Clang est un front-end pour les langages de la famille du C, utilisant le back-end LLVMJavac, le compilateur Java le plus répanduGHC, un compilateur pour HaskellDe nombreux autres[Lesquels ?], pour les mêmes langages et pour d'autres[Lesquels ?](en) Liste de compilateurs gratuits et/ou libresCours plutôt complet et contenant des exemples en C/ASM. Portail de la programmation informatique   Portail de l’informatique"
informatique;"La cryptographie est une des disciplines de la cryptologie s'attachant à protéger des messages (assurant confidentialité, authenticité et intégrité) en s'aidant souvent de secrets ou clés. Elle se distingue de la stéganographie qui fait passer inaperçu un message dans un autre message alors que la cryptographie rend un message supposément inintelligible à autre que qui-de-droit.Elle est utilisée depuis l'Antiquité, mais certaines de ses méthodes les plus modernes, comme la cryptographie asymétrique, datent de la fin du XXe siècle.Le mot cryptographie vient des mots en grec ancien kruptos (???????) « caché » et graphein (???????) « écrire ». Beaucoup des termes de la cryptographie utilisent la racine « crypt- », ou des dérivés du terme « chiffre » :chiffrement : transformation à l'aide d'une clé d'un message en clair (dit texte clair) en un message incompréhensible (dit texte chiffré) pour celui qui ne dispose pas de la clé de déchiffrement (en anglais encryption key ou private key pour la cryptographie asymétrique) ;chiffre : un ensemble de règles permettant d'écrire et de lire dans un langage secret ;cryptogramme : message chiffré ;cryptosystème : algorithme de chiffrement ;décrypter : retrouver le message clair correspondant à un message chiffré sans posséder la clé de déchiffrement (terme que ne possèdent pas les anglophones, qui eux « cassent » des codes secrets) ;cryptographie : étymologiquement « écriture secrète », devenue par extension l'étude de cet art (donc aujourd'hui la science visant à créer des cryptogrammes, c'est-à-dire à chiffrer) ;cryptanalyse : science analysant les cryptogrammes en vue de les décrypter ;cryptologie : science regroupant la cryptographie et la cryptanalyse ;cryptolecte : jargon réservé à un groupe restreint de personnes désirant dissimuler leur communication.Plus récemment sont apparus les termes « crypter » (pour chiffrer) et « cryptage » pour chiffrement. Ceux-ci sont acceptés par l'Office québécois de la langue française dans son grand dictionnaire terminologique, qui note que « La tendance actuelle favorise les termes construits avec crypt-. ». Le Grand Robert mentionne également « cryptage », et date son apparition de 1980. Cependant le Dictionnaire de l'Académie française n'intègre ni « crypter » ni « cryptage » dans sa dernière édition (entamée en 1992). Ces termes sont d'ailleurs considérés comme incorrects par exemple par l'ANSSI, qui met en avant le sens particulier du mot « décrypter » (retrouver le message clair à partir du message chiffré sans connaître la clef) en regard du couple chiffrer/déchiffrer.La cryptographie est utilisée depuis l'antiquité, et l'une des utilisations les plus célèbres pour cette époque est le chiffre de César, nommé en référence à Jules César qui l'utilisait pour ses communications secrètes. Mais la cryptographie est bien antérieure à cela : le plus ancien document chiffré est une recette secrète de poterie datant du XVIe siècle av. J.-C., notée sur une tablette d'argile qui a été découverte dans l'actuel Irak.L'historien en cryptographie David Kahn considère l'humaniste Leon Battista Alberti comme le « père de la cryptographie occidentale », grâce à trois avancées significatives : « la plus ancienne théorie occidentale de cryptanalyse, l'invention de la substitution polyalphabétique, et l'invention du code de chiffrement ».Bien qu'éminemment stratégique, la cryptographie est restée pendant très longtemps un art, pour ne devenir une science qu'au XXIe siècle. Avec l'apparition de l'informatique, son utilisation se popularise et se vulgarise, quitte à se banaliser et à être utilisée à l'insu de l’utilisateur[réf. nécessaire].Enfin, la Cryptographie post-quantique est une sous-discipline de la cryptographie qui cherche à proposer des algorithmes résistant au calculateur quantique.Les domaines d'utilisations de la cryptographie sont vastes et vont du domaine militaire, au commercial, en passant par la protection de la vie privée.Les techniques de cryptographie sont parfois utilisées pour protéger notre vie privée. Ce droit est en effet plus facilement bafoué dans la sphère numérique. Ainsi les limites de la cryptographie quant à sa capacité à préserver la vie privée soulève des questionnements. Deux exemples qui illustrent bien ce sujet sont à trouver dans le domaine de la santé et celui de la blockchain.La santé est un domaine sensible quant à la protection des données : le secret médical est remis en question avec l’informatisation de la médecine.La cryptographie permet en théorie de protéger les données médicales pour qu’elles ne soient pas accessible à n’importe qui, mais elle n’est pas suffisante.Car tant que le droit n’est pas suffisamment large[pas clair], il existe des failles qui permettent à certains acteurs d’utiliser des données personnelles dès l'accord de l'usager donné, or cet accord est exigé pour l'accès au service, faisant ainsi perdre à l'utilisateur la possibilité de contrôle de ses  accès à nos données personnelles.De plus l’inviolabilité des données médicales est remise en question par les développements qui permettent le déchiffrement de ces données, en effet selon Bourcier et Filippi, l’« anonymat ne semble plus garanti de façon absolue en l’état actuel des techniques de cryptographie ». Avec cette double constatation ils proposent de protéger nos données médicales avec une réforme juridique qui permettrait de faire rentrer les données personnelles médicales non pas dans le droit à la vie privée qui est un droit personnel, mais dans un droit collectif qui permettrait de protéger plus efficacement des données telles que les données génétiques qui concernent plusieurs individus. La création d’un droit collectif pour la santé permettrait ainsi de compenser les limites de la cryptographie qui n’est pas en mesure d’assurer à elle seule la protection de ce type de données.La blockchain est elle aussi l’une des applications de la cryptographie en lien avec la protection de la vie privée. C’est un système décentralisé qui se base entre autres sur des techniques de cryptographie destinées à assurer la fiabilité des échanges tout en garantissant en principe la vie privée. Qui dit système décentralisé implique qu’il n’y a pas de tierce personne par laquelle passe les informations. Ainsi seuls les individus concernés ont accès aux données vu que les données sont chiffrées, d’où un respect important de la vie privée. En pratique cela dit, ce système présente des limites : « la décentralisation est acquise au prix de la transparence ». En effet un tel système ne protège pas les informations concernant la transaction : destinataire, date, et autres métadonnées qui sont nécessaires pour s’assurer de la légitimité. Ainsi une protection complète de la vie privée en blockchain nécessite que ces métadonnées soient elles aussi protégées, puisque celles-ci sont transparentes et donc visibles par tout le monde. Cette protection supplémentaire est rendue possible par de nouvelles techniques d'anonymisation des signatures telles que la signature aveugle, qui sont réputées de garantir la légitimité des transactions sans les rendre publiques. Mais ce processus n’est pas encore applicable partout et n’est qu’à l’état embryonnaire pour certaines techniques. Malgré tout avec le temps de plus en plus de systèmes permettront de résoudre cette limitation.[Quand ?]Le cadre législatif de la cryptographie est variable et sujet aux évolutions.D’une part, il est sujet aux évolutions des technologies, de leur efficacité et de leur accessibilité. En effet la démocratisation d’Internet et des ordinateurs personnels fondent un nouveau cadre dans les années 80-90, comme nous le verrons avec l’exemple de la loi française.D’autre part, ces lois évoluent selon le contexte politique. En effet, à la suite des attentats du 11 septembre 2001, les gouvernements occidentaux opèrent une reprise du contrôle des données circulant sur Internet et de toutes les données potentiellement cachées par la cryptographie.Cela se fait de plusieurs façons : d’une part, par la mise en place de lois obligeant les fournisseurs de systèmes de communication, cryptés ou non, à fournir à certaines entités étatiques des moyens d’accéder à toutes ces données. Par exemple en France, alors qu’en 1999, la loi garantit la protection des communications privées par voie électronique, celle-ci subit l’amendement à la Loi no 91-646 du 10 juillet 1991 relative au secret des correspondances émises par la voie des communications électroniques. Cet amendement formalise précisément le moyen législatif d’accéder à des données encryptées décrit précédemment.D’autre part, certains services gouvernementaux développent des systèmes d’inspection de réseaux afin de tirer des informations malgré le chiffrement des données. On peut notamment citer le programme de surveillance électronique Carnivore aux États-Unis.Toutefois, la réglementation sur les systèmes de cryptographie ne laisse que peu de place à un contrôle par des entités telles que des gouvernements. En effet, les logiciels et algorithmes les plus performants et répandus sont issus de la connaissance et des logiciels libres comme PGP ou OpenSSH. Ceux-ci offrent une implémentation fonctionnelle des algorithmes de chiffrement modernes pour assurer le chiffrement de courriels, de fichiers, de disques durs ou encore la communication dite sécurisée entre plusieurs ordinateurs. Ces logiciels étant sous licence libre, leur code source est accessible, reproductible et modifiable. Cela implique qu’il est techniquement très difficile de les rendre exclusifs à une entité — étatique par exemple — et d’en avoir le contrôle. Le chiffrement devient alors utilisable par nombre de personnes, permettant de contrevenir à une loi.Bien que la cryptographie puisse paraître être une opportunité pour la démocratie au premier abord, la réalité n’est pas forcément si unilatérale. Il est clair que l’utilisation de cette technologie permet de protéger la liberté d’expression. Toutefois, cela ne suffit pas à dire que la cryptographie est bénéfique à la démocratie, puisque l'enjeu démocratique dépasse la simple liberté l’expression. En particulier, la démocratie suppose un système de lois et de mécanismes de sanctions qui mène la liberté d’expression vers une activité politique constructive.Avec l’apparition de la cryptographie électronique et dans un monde toujours plus numérisé, la politique doit aussi s’adapter. Winkel observe trois politiques différentes pour les gouvernements: la stratégie libérale, la stratégie de prohibition et la stratégie du tiers de confiance. Stratégie de prohibition La stratégie de prohibition consiste à restreindre l’utilisation de la cryptographie en imposant des contrôles d’import-export, des restrictions d’utilisation ou encore d’autres mesures pour permettre à l’État et ses institutions de mettre en œuvre dans le monde virtuel la politique (principes et lois) du « vrai » monde. Cette stratégie est généralement appliquée dans des pays à régime politique autoritaire, par exemple en Chine avec le Grand Firewall ou en Corée du Nord. Stratégie du tiers de confiance La stratégie du tiers de confiance a pour but de garder la balance qu’il existe dans le « vrai » monde entre d’un côté la législation et les potentielles sanctions de l’État et de l’autre la protection de secrets économiques ou de la sphère privée, dans le monde virtuel. La mise en place d’un tel système est toutefois plus technique.Le principe consiste en un dépôt des copies des clés d’encryption des utilisateurs dans les mains d’un tiers de confiance. Celui-ci pourrait ensuite répondre à une demande d'une autorité légale compétente et lui transmettre une clef - par exemple à des fins d’audit - à condition que cette demande ait suivi une procédure bien définie. Cette solution, bien que paraissant optimale du point de vue de la théorie démocratique, présente déjà un certain nombre de difficultés techniques comme la mise en place et l'entretien de l’infrastructure requise. De plus, il est utopique d’imaginer que la mise en place de cadres légaux plus sévères découragera les criminels et organisations anticonstitutionnelles d’arrêter leurs activités. Cela s’applique à la stratégie du tiers de confiance et à celle de prohibition. Stratégie libérale La stratégie libérale répandue dans le monde laisse un accès ""total"" aux technologies de cryptographie, pour sécuriser la vie privée des citoyens, défendre la liberté d’expression dans l’ère numérique, laisser les entreprises garder leurs secrets et laisser les entreprises exporter des solutions informatiques sécurisées sur les marchés internationaux.Cependant, les criminels et opposants de la Constitution[Laquelle ?] peuvent utiliser cette technologie à des fins illicites — ou anticonstitutionnelles —[Laquelle ?] comme  armes, drogue ou pédopornographie sur le Dark Web. Autres formes de législation Les États-Unis et la France interdisent l'exportation de certaines formes de cryptographie, voir Lois sur les chiffrement sur wikipedia anglophone.Les premiers algorithmes utilisés pour le chiffrement d'une information étaient assez rudimentaires dans leur ensemble. Ils consistaient notamment au remplacement de caractères par d'autres. La confidentialité de l'algorithme de chiffrement était donc la pierre angulaire de ce système pour éviter un décryptage rapide.Exemples d'algorithmes de chiffrement faibles :ROT13 (rotation de 13 caractères, sans clé) ;Chiffre de César (décalage de trois lettres dans l'alphabet sur la gauche) ;Chiffre de Vigenère (introduit la notion de clé).Les algorithmes de chiffrement symétrique se fondent sur une même clé pour chiffrer et déchiffrer un message. L'un des problèmes de cette technique est que la clé, qui doit rester totalement confidentielle, doit être transmise au correspondant de façon sûre. La mise en œuvre peut s'avérer difficile, surtout avec un grand nombre de correspondants car il faut autant de clés que de correspondants.Quelques algorithmes de chiffrement symétrique très utilisés :Chiffre de Vernam (le seul offrant une sécurité théorique absolue, à condition que la clé ait au moins la même longueur que le message à chiffrer, qu'elle ne soit utilisée qu'une seule fois et qu'elle soit totalement aléatoire)DES3DESAESRC4RC5MISTY1et d'autres (voir la liste plus exhaustive d'algorithmes de cryptographie symétrique).Pour résoudre le problème de l'échange de clés, la cryptographie asymétrique a été mise au point dans les années 1970. Elle se base sur le principe de deux clés :une publique, permettant le chiffrement ;une privée, permettant le déchiffrement.Comme son nom l'indique, la clé publique est mise à la disposition de quiconque désire chiffrer un message. Ce dernier ne pourra être déchiffré qu'avec la clé privée, qui doit rester confidentielle.Quelques algorithmes de cryptographie asymétrique très utilisés :RSA (chiffrement et signature) ;DSA (signature) ;Protocole d'échange de clés Diffie-Hellman (échange de clé) ;et d'autres ; voir cette liste plus complète d'algorithmes de cryptographie asymétrique.Le principal inconvénient de RSA et des autres algorithmes à clés publiques est leur grande lenteur par rapport aux algorithmes à clés secrètes. RSA est par exemple 1000 fois plus lent que DES. En pratique, dans le cadre de la confidentialité, on s'en sert pour chiffrer un nombre aléatoire qui sert ensuite de clé secrète pour un algorithme de chiffrement symétrique. C'est le principe qu'utilisent des logiciels comme PGP par exemple.La cryptographie asymétrique est également utilisée pour assurer l'authenticité d'un message. L'empreinte du message est chiffrée à l'aide de la clé privée et est jointe au message. Les destinataires déchiffrent ensuite le cryptogramme à l'aide de la clé publique et retrouvent normalement l'empreinte. Cela leur assure que l'émetteur est bien l'auteur du message. On parle alors de signature ou encore de scellement.La plupart des algorithmes de cryptographie asymétrique sont vulnérables à des attaques utilisant un calculateur quantique, à cause de l'algorithme de Shor. La branche de la cryptographie visant à garantir la sécurité en présence d'un tel adversaire est la cryptographie post-quantique.Une fonction de hachage est une fonction qui convertit un grand ensemble en un plus petit ensemble, l'empreinte. Il est impossible de la déchiffrer pour revenir à l'ensemble d'origine, ce n'est donc pas une technique de chiffrement.Quelques fonctions de hachage très utilisées :MD5 ;SHA-1 ;SHA-256 ;et d'autres ; voir cette liste plus complète d'algorithmes de hachage.L'empreinte d'un message ne dépasse généralement pas 256 bits (maximum 512 bits pour SHA-512) et permet de vérifier son intégrité.Projet NESSIEAdvanced Encryption Standard processLes cryptologues sont des experts en cryptologie : ils conçoivent, analysent et cassent les algorithmes (voir cette liste de cryptologues).Le mouvement Cypherpunk, qui regroupe des partisans d'une idéologie dite « cyber libertarienne », est un mouvement créé en 1991 œuvrant pour défendre les droits civils numériques des citoyens, à travers la cryptographie.Essentiellement composé de hackers, de juristes et de militants de la liberté sur le web ayant pour objectif commun une plus grande liberté de circulation de l'information, ce groupe s'oppose à toute intrusion et tentative de contrôle du monde numérique par des grandes puissances, en particulier les États.Les crypto-anarchistes considèrent la confidentialité des données privées comme un droit inhérent. En s'inspirant du système politique libéral américain, ils défendent le monde numérique en tant qu'espace à la fois culturel, économique et politique à l'intérieur d'un réseau ouvert et décentralisé, où chaque utilisateur aurait sa place et pourrait jouir de tous ses droits et libertés individuelles.Les crypto-anarchistes cherchent à démontrer que les libertés numériques ne sont pas des droits à part, contraints d’exister seulement dans le domaine technique qu’est internet mais que maintenant le numérique est un élément important et omniprésent dans la vie quotidienne, et ainsi, il est primordial dans la définition des libertés fondamentales des citoyens. Les droits et libertés numériques ne doivent pas être considérées comme moins importante que celles qui régissent le monde matériel.La création des crypto-monnaies en mai 1992[réf. souhaitée], remplit un des objectifs du mouvement en offrant une monnaie digitale intraçable en ligne mais permet également l'expansion de marchés illégaux sur le web.L’apparition de nouvelles techniques (logiciels de surveillance de masse comme Carnivore, PRISM, XKeyscore...) a en fait mené à plus de surveillance, moins de vie privée, et un plus grand contrôle de la part des États qui se sont approprié ces nouvelles technologies.Crypto-anarchistes (pour l’anonymisation des communications) et États (pour le contrôle des communications) s’opposent le long de ces arguments.Un axiome central du mouvement Cypherpunk est que, pour rééquilibrer les forces entre l’État et les individus, il faut la protection des communications privées ainsi que la transparence des informations d’intérêt public, comme l’énonce la devise : « Une vie privée pour les faibles et une transparence pour les puissants ».Dans ce sens, Julian Assange (un des plus importants membres du mouvement Cypherpunk) a créé WikiLeaks, un site qui publie aux yeux de tous, des documents et des secrets d’État initialement non connus du grand public.Les événements du 11 septembre 2001 ont été des arguments de poids pour les États, qui avancent qu'une régulation et un contrôle du monde d'internet sont nécessaires afin de préserver nos libertés.L'apparition de lanceurs d'alerte comme Edward Snowden en 2013 est un événement important en faveur du mouvement crypto-anarchiste qui s'oppose au contrôle de l’État dans le monde numérique.D'autres groupes/mouvements importants sont créés pour défendre les libertés d’internet, partageant des objectifs avec le mouvement Cypherpunk :Les Anonymous qui défendent la liberté d'expression sur internet et en dehors.L'Electronic Frontier Foundation (EFF) qui défend la confidentialité des données numériques.Le Parti Pirate qui défend l’idée des partages des données et se bat pour les libertés fondamentales sur Internet (partage d’informations, de savoirs culturels et scientifiques qui sont parfois bannis d’internet).David Kahn (trad. de l'anglais par Pierre Baud, Joseph Jedrusek), La guerre des codes secrets [« The Codebreakers »], Paris, InterEditions, 1980, 405 p. (ISBN 2-7296-0066-3).Simon Singh (trad. Catherine Coqueret), Histoire des codes secrets [« The Code Book »], Librairie générale française (LFG), coll. « Le Livre de Poche », 3 septembre 2001, 504 p., Poche (ISBN 2-253-15097-5, ISSN 0248-3653, OCLC 47927316).Jacques Stern, La Science du secret, Paris, Odile Jacob, coll. « Sciences », 5 janvier 1998, 203 p. (ISBN 2-7381-0533-5, OCLC 38587884, lire en ligne)Gilles Zémor, Cours de cryptographie, Paris, Cassini, 15 décembre 2000, 227 p. (ISBN 2-84225-020-6, OCLC 45915497).« L'art du secret », Pour la science, dossier hors-série, juillet-octobre 2002.Douglas Stinson (trad. de l'anglais par Serge Vaudenay, Gildas Avoine, Pascal Junod), Cryptographie : Théorie et pratique [« Cryptography : Theory and Practice »], Paris, Vuibert, coll. « Vuibert informatique », 28 février 2003, 337 p., Broché (ISBN 2-7117-4800-6, ISSN 1632-4676, OCLC 53918605)(en) Handbook of Applied Cryptography, A.J. Menezes, éd. P.C. van Oorschot et S.A. Vanstone - CRC Press, 1996. Disponible en ligne : [1]Site thématique de la sécurité des systèmes d'information : site officiel de l'Agence nationale de la sécurité des systèmes d'information sur la question de la sécurité informatique. Présentation de la cryptographie, des signatures numériques, de la législation française sur le sujet, etc.Bruce Schneier (trad. de l'anglais par Laurent Viennot), Cryptographie appliquée [« Applied cryptography »], Paris, Vuibert, coll. « Vuibert informatique », 15 janvier 2001, 846 p., Broché (ISBN 2-7117-8676-5, ISSN 1632-4676, OCLC 46592374).Niels Ferguson, Bruce Schneier (trad. de l'anglais par Henri-Georges Wauquier, Raymond Debonne), Cryptographie : en pratique [« Practical cryptography »], Paris, Vuibert, coll. « En pratique / Sécurité de l'information et des systèmes », 18 mars 2004, 338 p., Broché (ISBN 2-7117-4820-0, ISSN 1632-4676, OCLC 68910552).Pierre Barthélemy, Robert Rolland et Pascal Véron (préf. Jacques Stern), Cryptographie : principes et mises en œuvre, Paris, Hermes Science Publications : Lavoisier, coll. « Collection Informatique », 22 juillet 2005, 414 p., Broché (ISBN 2-7462-1150-5, ISSN 1242-7691, OCLC 85891916).Auguste Kerckhoffs, La Cryptographie militaire, L. Baudoin, 1883.Marcel Givierge, Cours de cryptographie, Berger-Levrault, 1925.Jean-Guillaume Dumas, Pascal Lafourcade, Patrick Redon, Architectures de sécurité pour internet - 2e éd. Protocoles, standards et déploiement , Dunod 2020.Jean-Guillaume Dumas, Jean-Louis Roch, Sébastien Varrette, Eric Tannier,Théorie des codes - 3e éd. : Compression, cryptage, correction, Dunod 2018.Jean-Guillaume Dumas, Pascal Lafourcade, Etienne Roudeix, Ariane Tichit, Sébastien Varrette, Les NFT en 40 questions: Comprendre les jetons Non Fungible, Dunod 2022.Jean-Guillaume Dumas, Pascal Lafourcade, Ariane Tichit, Sébastien Varrette, Les blockchains en 50 questions - 2éd.: Comprendre le fonctionnement de cette technologie, Dunod 2022.Pascal Lafourcade, Malika More, 25 énigmes ludiques pour s'initier à la cryptographie, Dunod 2021.Henry Mamy, « La cryptographie », dans Science et Guerre, vol. 16, Bernard Tignole éditeur, 1888 (lire en ligne), disponible sur GallicaLa Cryptogr@phie expliquée!, démonstrations avec des applets Java.ACrypTA, cours, exercices, textes, liens concernant la cryptographie.Ars cryptographica , vulgarisation très complète.Cryptographie, ressources, algorithmes, des ressources sur les algorithmes cryptographiques de dernière génération et sur la cryptographie classique.Cryptographie, du chiffre et des lettres, exposé de François Cayre sur le site Interstices.(en) Handbook of Applied Cryptography, une référence de plus de 800 pages dont l'édition de 1996 peut être téléchargée gratuitement Portail de la cryptologie   Portail de la sécurité de l’information   Portail de la sécurité informatique"
informatique;"La cryptologie, étymologiquement la « science du secret » est considérée comme une science que depuis le XXe siècle. Elle englobe la cryptographie — l'écriture secrète – et la cryptanalyse – l'analyse de cette dernière.Le terme « crypto » provient du latin et du grec et signifie ce qui est dissimulé ou caché.À la fois art ancien et science nouvelle, la cryptologie est utilisée durant l'Antiquité par les Spartiates (la scytale) et elle devient thème de recherche scientifique académique universitaire, depuis les années 1970. Cette discipline est liée à beaucoup d'autres, notamment l'arithmétique modulaire, l'algèbre, la théorie de la complexité, la théorie de l'information ou encore les codes correcteurs d'erreurs.Les premières méthodes de chiffrement remontent à l’Antiquité et se sont améliorées, avec la fabrication de différentes machines de chiffrement, pour obtenir un rôle majeur lors de la Première Guerre mondiale et de la Seconde Guerre mondiale.La cryptographie se scinde en deux parties nettement différenciées :d'une part la cryptographie à clef secrète, encore appelée symétrique ou bien classique ;d'autre part la cryptographie à clef publique, dite également asymétrique ou moderne.La première est la plus ancienne, on peut la faire remonter à l'Égypte de l'an 2000 av. J.-C. en passant par Jules César ; la seconde remonte à l'article de W. Diffie et M. Hellman, New directions in cryptography daté de 1976.Toutes deux visent à assurer la confidentialité de l'information, mais la cryptographie à clef secrète nécessite au préalable la mise en commun entre les destinataires d'une certaine information : la clef (symétrique), nécessaire au chiffrement ainsi qu'au déchiffrement des messages. Dans le cadre de la cryptographie à clef publique, ce n'est plus nécessaire. En effet, les clefs sont alors différentes, ne peuvent se déduire l'une de l'autre, et servent à faire des opérations opposées, d'où l'asymétrie entre les opérations de chiffrement et de déchiffrement.Bien que beaucoup plus récente et malgré d'énormes avantages – signature numérique, échange de clefs... – la cryptographie à clef publique ne remplace pas totalement celle à clef secrète, qui pour des raisons de vitesse de chiffrement et parfois de simplicité reste présente. À ce titre, signalons la date du dernier standard américain en la matière, l'AES : décembre 2001, ce qui prouve la vitalité encore actuelle de la cryptographie symétrique.Dans le bestiaire des algorithmes de chiffrement, on peut citer :pour les systèmes symétriques, le DES, l'AES, Blowfish, IDEA, etc.pour les systèmes asymétriques, le RSA, DSA-DH, ElGamal, les courbes elliptiques, etc.Le pendant de cette confidentialité se trouve dans la cryptanalyse. Évidemment, depuis l'existence de ces codes secrets, on a cherché à les casser, à comprendre les messages chiffrés bien que l'on n'en soit pas le destinataire légitime, autrement dit décrypter. Si la cryptanalyse du système de César est aisée (un indice : les propriétés statistiques de la langue, en français, le e est la lettre la plus fréquente), des systèmes beaucoup plus résistants ont vu le jour. Certains ont résisté longtemps, celui de Vigenère (Le traité des secrètes manières d'écrire 1586) par exemple, n'ayant été cassé par Charles Babbage qu'au milieu du XIXe siècle. D'autres, bien que n'ayant pas de faille exploitable, ne sont plus utilisés car ils sont à la portée des puissances de calcul modernes. C'est le cas du DES avec sa clef de 56 bits jugée trop courte car elle peut être trouvée par recherche exhaustive (force brute).Dans un bestiaire de la cryptanalyse, il faudrait presque passer chaque système en revue — non seulement chaque système, mais aussi chaque mise en œuvre : à quoi sert la meilleure porte blindée si le mur qui la soutient est en contreplaqué ? Cela dit, si l'on veut vraiment citer quelques techniques, on a :la cryptanalyse différentielle, Biham et Shamir (le S de RSA), 1991, systèmes symétriques ;la cryptanalyse linéaire, Matsui, 1994, systèmes symétriques ;la factorisation, seul vrai moyen de déchiffrer RSA à l'heure actuelle ;la force brute, c'est-à-dire l'essai systématique de toutes les clefs possibles ;et d'autres encore.La confidentialité n'est que l'une des facettes de la cryptologie. Elle permet également :l'authentification ou l'authentification forte d'un message : l'assurance qu'un individu est bien l'auteur du message chiffré ;la non-répudiation est le fait de s'assurer qu'un contrat ne peut être remis en cause par l'une des parties ;l'intégrité : on peut vérifier que le message n'a pas été manipulé sans autorisation ou par erreur ;la preuve à divulgation nulle de connaissance — par exemple d'identité —, on peut prouver que l'on connaît un secret sans le révéler ;et autres, dont l'anonymat et la mise en gage.Pour l'essentiel, c'est la cryptographie à clef publique qui fournit les bases nécessaires à ces aspects de la cryptologie.La cryptologie a très longtemps été considérée comme une arme de guerre. Au IVe siècle av. J.-C., Énée le Tacticien, un général grec, y consacre un chapitre dans Commentaires sur la défense des places fortes. On peut aussi citer le siège de la Rochelle, où Antoine Rossignol (1600 - 1682) décrypte les messages que les huguenots assiégés tentent de faire sortir. Richelieu y apprend ainsi que les huguenots sont affamés et attendent la flotte anglaise. Celle-ci trouvera à son arrivée la flotte française, prête au combat, ainsi qu'une digue bloquant l'accès au port.Autre exemple, la Première Guerre mondiale, où le Room 40 — service du chiffre britannique — s'illustre tout particulièrement en décryptant un télégramme envoyé en janvier 1917 de Berlin à l'ambassadeur allemand à Washington, qui devait le retransmettre au Mexique. Ils apprennent ainsi que l'Allemagne va se lancer dans une guerre sous-marine totale et demande une alliance militaire, devant permettre au Mexique de récupérer le Nouveau-Mexique, le Texas et l'Arizona. Les Britanniques pouvaient transmettre directement ces renseignements aux États-Unis, mais ils auraient ainsi révélé aux Allemands l'interception et la mise à jour de leur code. Ils préfèrent donc envoyer un espion récupérer le message destiné aux Mexicains, faisant ainsi croire à une fuite côté Mexique. Le télégramme en clair se retrouve publié dans les journaux américains le 1er mars 1917. À la suite de cela, le président Wilson n'a pas de mal à obtenir l'accord du congrès, les États-Unis entrent en guerre.Ces exemples illustrent bien pourquoi les gouvernements sont prudents quant à l'utilisation de moyen cryptographique. Philip Zimmermann en a fait l'expérience lorsqu'il a mis à disposition son logiciel de messagerie sécurisée, Pretty Good Privacy (PGP), en 1991. Violant les restrictions à l'exportation pour les produits cryptographiques, PGP a été très mal accueilli par le gouvernement américain qui a ouvert une enquête en 1993 — abandonnée en 1996, peu avant que le gouvernement Clinton ne libéralise grandement, à l'aube de l'ère du commerce électronique, l'usage de la cryptographie.En France, depuis la loi pour la confiance dans l'économie numérique (LCEN), l'usage de la cryptologie est libre. Néanmoins, l'article 132-79 du code pénal prévoit que lorsqu'un moyen de cryptologie a été utilisé pour préparer ou commettre un crime ou un délit, ou pour en faciliter la préparation ou la commission, le maximum de la peine privative de liberté encourue est relevé.Les dispositions pénales ne sont toutefois pas applicables à l'auteur ou au complice de l'infraction qui, à la demande des autorités judiciaires ou administratives, leur a remis la version en clair des messages chiffrés ainsi que les conventions secrètes nécessaires au déchiffrement.Des logiciels de chiffrement avec une fonction de déni plausible permettent d'échapper à l'aggravation des peines (ex : FreeOTFE et TrueCrypt). Ouvrages historiques Cours de cryptographie du général Marcel Givierge, 1925Éléments de cryptographie du commandant Roger Baudouin, 1939 Ouvrages contemporains Simon Singh (trad. Catherine Coqueret), Histoire des codes secrets [« The Code Book »], Librairie générale française (LFG), coll. « Le Livre de Poche », 3 septembre 2001, 504 p., Poche (ISBN 2-253-15097-5, ISSN 0248-3653, OCLC 47927316)Gilles Zémor, Cours de cryptographie, Paris, Cassini, 15 décembre 2000, 227 p. (ISBN 2-84225-020-6, OCLC 45915497)L'Art du secret, Pour la science, dossier hors-série, juillet-octobre 2002.La Guerre des codes secrets, de D. Kahn, Interéditions, 1980 (trad. de The Codebreakers)(en) Handbook of Applied Cryptography, de A. J. Menezes, P. C. van Oorschot et S. A. Vanstone, CRC Press, 1996, en ligneLe Décryptement de A. Muller, PUF, 1983 (cryptanalyse des systèmes « traditionnels »)Les Écritures secrètes de A. Muller, PUF, 1971 (présentation des systèmes « traditionnels »)Bruce Schneier (trad. de l'anglais par Laurent Viennot), Cryptographie appliquée [« Applied cryptography »], Paris, Vuibert, coll. « Vuibert informatique », 15 janvier 2001, 846 p., Broché (ISBN 2-7117-8676-5, ISSN 1632-4676, OCLC 46592374)Douglas Stinson (trad. de l'anglais par Serge Vaudenay, Gildas Avoine, Pascal Junod), Cryptographie : Théorie et pratique [« Cryptography : Theory and Practice »], Paris, Vuibert, coll. « Vuibert informatique », 28 février 2003, 337 p., Broché (ISBN 2-7117-4800-6, ISSN 1632-4676, OCLC 53918605)Jacques Stern, La science du secret, Paris, Odile Jacob, coll. « Sciences », 5 janvier 1998, 203 p. (ISBN 2-7381-0533-5, OCLC 38587884, lire en ligne)Cryptologie, une histoire des écritures secrètes des origines à nos jours de Gilbert Karpman, éditions Charles Lavauzelle 2006Codage, cryptologie et applications de Bruno Martin, éditions PPUR 2004Théorie des codes (Compression, chiffrement, correction) de J.-G. Dumas, J.-L. Roch, E. Tannier et S. Varrette, éditions Dunod 2007Pierre Barthélemy, Robert Rolland et Pascal Véron (préf. Jacques Stern), Cryptographie : principes et mises en œuvre, Paris, Hermes Science Publications : Lavoisier, coll. « Collection Informatique », 22 juillet 2005, 414 p., Broché (ISBN 2-7462-1150-5, ISSN 1242-7691, OCLC 85891916)Ars Cryptographica Étude des messages secrets de l'Antiquité à nos jours, et cours de cryptologieCours et logiciels en téléchargementFAQ sur la longueur des clefsAlgorithmes sans limitation de longueur de clefFAQ en français du forum Usenet sci.cryptForum Usenet francophone consacré à la cryptologieExemple de nombreux messages chiffrés réels présents sur des groupes de discussions Usenet(fr) ACrypTA, cours, exercices, textes, liens concernant la cryptologie Portail de la cryptologie   Portail de la sécurité de l’information"
informatique;"Un cybercrime est une « infraction pénale susceptible de se commettre sur ou au moyen d’un système informatique généralement connecté à un réseau ».Il s’agit donc d’une nouvelle forme de criminalité et de délinquance qui se distingue des formes traditionnelles en ce qu’elle se situe dans un espace virtuel, le « cyberespace ». Depuis quelques années la démocratisation de l’accès à l’informatique et la globalisation des réseaux ont été des facteurs de développement du cybercrime.La cybercriminalité regroupe trois types d’infractions :les infractions spécifiques aux technologies de l’information et de la communication : parmi ces infractions, on recense les atteintes aux systèmes de traitement automatisé de données, les traitements non autorisés de données personnelles (comme la cession illicite des informations personnelles), les infractions aux cartes bancaires, les chiffrements non autorisés ou non déclarés ou encore les interceptions ;les infractions liées aux technologies de l’information et de la communication : cette catégorie regroupe la pédopornographie, l’incitation au terrorisme et à la haine raciale sur internet, les atteintes aux personnes privées et non aux personnages publics, les atteintes aux biens ;les infractions facilitées par les technologies de l’information et de la communication, que sont les escroqueries en ligne (cyberarnaques), le blanchiment d'argent, la contrefaçon ou toute autre violation de propriété intellectuelle.En France la cybercriminalité est prise juridiquement en compte depuis la loi informatique et libertés (loi relative à l'informatique, aux fichiers et aux libertés du 6 janvier 1978).La loi Godfrain du 5 février 1988 relative à la fraude informatique a introduit les articles 323-1 et suivants dans le Code pénal, concernant notamment la suppression ou modification de données (art 323-1 al 1), ou encore la tentative d’infraction sur un STAD (323-7).La loi du 15 novembre 2001 relative à la sécurité quotidienneLa loi du 18 mars 2003 pour la sécurité intérieureLa loi du 9 mars 2004 portant adaptation de la justice aux évolutions de la criminalitéLa loi pour la confiance dans l'économie numérique du 21 juin 2004, qui a modifié les articles 323-1 et suivant du Code pénal. Cette loi a, en outre, modifié l’article 94 du Code de procédure pénale relatif à l’inclusion des données informatiques dans la liste des pièces susceptibles d'être saisies lors des perquisitions réalisées en flagrant délit ou au cours d'une instruction (ces perquisitions sont aussi régies par les art. 56 et 97 du Code de procédure pénale).La loi du 9 juillet 2004 relative aux communications électroniques et aux services de communication audiovisuelle.La Loi du 23 janvier 2006 relative à la lutte contre le terrorisme et comportant diverses dispositions relatives à la sécurité et aux contrôles frontaliers.La loi du 5 mars 2007 relative à la prévention de la délinquancePar ailleurs de nombreux textes réglementaires ont été adoptés. On peut citer pour exemple le décret du 24 mars 2006 sur la conservation des données de trafic prévu par la loi relative à la sécurité quotidienne.La lutte contre la cybercriminalité est en pleine évolution et elle fait l’objet de nombreuses réflexions en France. Par exemple le plan de lutte contre la cybercriminalité qui a été présenté en février 2008 contient des mesures visant à moderniser les méthodes d’investigation. Par ailleurs, la même année, au mois d’octobre a été présenté le plan du numérique 2012 qui contient des propositions relatives à la lutte contre le cybercrime.Malgré cette évolution permanente le dispositif législatif français en matière de cybercriminalité est « éparpillé » dans divers textes. Il est donc peu aisé, autant pour les professionnels que pour les profanes, de connaître avec précision ce qui est aujourd’hui reconnu comme un acte cybercriminel par le droit français. Myriam Quéméner et Joël Ferry, dans Cybercriminalité Défi Mondial (2e édition) décrivent le dispositif législatif et réglementaire français comme un « ‘maquis’ quelque peu ésotérique ».Le 23 novembre 2001 les pays membres du Conseil de l'Europe ainsi que les États-Unis, le Canada, le Japon et l'Afrique du Sud, ont adopté la convention sur la cybercriminalité, aboutissement d'un long processus de négociations (vingt-sept versions antérieures et quatre années de négociations officielles). Il s'agit d'une convention pénale à vocation internationale destinée à lutter contre le cybercrime. En 2007, seuls quatorze États avaient ratifié la convention sur les quarante-sept signataires.Par ailleurs en 2003, a été ouvert à la signature le protocole additionnel à la convention sur la cybercriminalité, qui visait à élargir le champ d'application de la convention aux infractions de propagande raciste ou xénophobe commis via les réseaux internet. Ce protocole, non ratifié par les États-Unis, prévoit par ailleurs des mesures facilitant l'extradition et l'entraide judiciaire.La France a ratifié ces deux textes par la loi no 2005-493 du 19 mai 2005 autorisant l'approbation de la Convention du Conseil de l'Europe sur la cybercriminalité et du protocole additionnel à cette Convention.La convention sur la cybercriminalité de 2001 poursuit trois objectifs déterminés :L'harmonisation des législations des États signataires ;La modernisation de ces législations, notamment en matière procédurale ;L'amélioration de la coopération internationale en matière d'extradition et d'entraide répressive.Le premier axe est l'harmonisation des législations nationales en ce qui concerne la définition des infractions répertoriées par la Convention. Il s'agit donc d'incriminer quatre séries d'infractions qui sont :Les infractions informatiques : falsification et fraude informatique ;Les infractions de contenu : la pornographie enfantine. Le protocole additionnel inclut la propagation via Internet d'idées racistes et xénophobes ;Les infractions liées aux atteintes à la propriété intellectuelle et aux droits connexes : le partage non autorisé via Internet des œuvres protégées ;Les infractions contre la confidentialité, l'intégrité et la disponibilité des données et systèmes : accès illégal, interception illégale, atteinte à l'intégrité des données ou des systèmes.Ensuite, le deuxième axe, d'ordre procédural, définit les moyens d'enquêtes et de poursuites pénales les mieux adaptés à la mondialisation du réseau internet. La Convention prévoit des règles pour garantir les droits des individus, mais aussi pour faciliter la conduite d'enquête. En ce sens, on peut citer, entre autres, les règles régissant la conservation des données stockées, la conservation et la divulgation rapide des données relatives au trafic, la perquisition des systèmes informatiques, la saisie de données informatiques, la collecte en temps réel des données relatives au trafic et l'interception de données relatives au contenu.Enfin, le troisième axe concerne la mise en place d'un système rapide et efficace de coopération internationale. À côté des formes traditionnelles de coopération pénale internationale, prévues notamment par les Conventions européennes d'extradition et d'entraide judiciaire, la Convention sur la cybercriminalité prévoit des formes d'entraide correspondant aux pouvoirs définis préalablement par la Convention. Ces conditions sont exigées afin que les autorités judiciaires et les services de police d'un État membre puissent agir pour le compte d'un autre État dans la recherche de preuves électroniques, sans toutefois mener d'enquêtes ni de perquisitions transfrontalières. En outre, toute donnée obtenue devrait être rapidement communiqué à l'État intéressé.Sans doute, ce texte international — constitue un complément indispensable aux lois nationales pour contenir le phénomène de cette nouvelle criminalité « caméléon » dont on ne connaît pas encore - du moins avec certitude — toutes « les couleurs » et les menaces.Par ailleurs, le 17 janvier 2005 le Conseil de l'Union européenne a adopté la décision cadre 2005/222/JAI du Conseil « relative aux attaques visant les systèmes d'information », qui va permettre une harmonisation des règles pénales concernant les principales activités criminelles visant les systèmes d'information, l'atteinte à l'intégrité d'un système et l'atteinte à l'intégrité des données.Après les attaques de 2017 d’ampleur internationale (NotPetya et WannaCry notamment) le cyber risque est le second risque le plus craint par les entreprises du monde entier,.60 % des cyberattaques dans le monde en 2011 proviennent des États-Unis.En réponse à la cyber présumée espionnant sur des opposants aux meilleurs intérêts de l'Iran par le gouvernement iranien en 2010 et 2011, Les États-Unis ont aidé les Émirats arabes unis à la fin de 2011 avec la création de l'autorité nationale de la sécurité électronique (NESA) qui est l'équivalente des ÉAU à la NSA américaine. Project Raven Project Raven était une initiative confidentielle visant à aider les ÉAU visionnez d'autres gouvernements, militants et activistes des droits de l'homme. Son équipe comprenait d'anciens agents de renseignement américains, qui ont appliqué leur formation au piratage téléphonique et ordinateurs appartenant aux victimes du Projet Raven. L'opération était basée dans un manoir converti à Abou Dabi surnommé «la villa».CyberPoint fourni Projet Raven avec des entrepreneurs formés aux États-Unis d'environ 2014 à 2016. La réputation de Cyberpoint en tant que société de cybersécurité défensive a été terni en 2016 après que les nouvelles ont éclaté que la société avait travaillé avec l'équipe de piratage des groupes de logiciels espions italiens.Le 24 octobre 2016, un article de The Intercept a révélé la surveillance aux ÉAU. Le chef des finances de DarkMatter[Quoi ?], Samer Khalife, a transféré des citoyens américains de DarkMatter vers une nouvelle société appelée Systèmes de connexion et des équipes de Tiger ont été formées par DarkMatter pour contrer les allégations de l'article de The Intercept.Le FBI étudie les DarkMatter pour des crimes tels que l'espionnage numérique, la complicité de la mort de Jamal Khashoggi et la détention de dissidents d'outre-mer. Le FBI a également enquêté sur d'anciens employés américains de DarkMatter possiblement impliqués dans des cybercrimes.Le 14 septembre 2021, trois anciens officiers de renseignements américains, Marc Baiier, Ryan Adams et Daniel Gericke, recrutés par les ÉAU pour mener des cyberopérations sophistiquées admis au piratage des infractions et à la violation des règles d'exportation américaines interdisant le transfert de technologie militaire à d'autres gouvernements. En outre, ils sont convenus de remettre plus de 1,7 million de dollars et leurs autorisations de sécurité américaine en échange d'une restriction de leur travail futur et de « coopérer pleinement» avec des enquêteurs »[pas clair],,. Cas d'espionnage saoudien Ahmad Abouammo, un citoyen américain et libanais et ancien employé de Twitter, et Ali Alzabarah, autre ancien employé de Twitter, ont été approchés par Riyad en 2014 et 2015 pour transférer des informations personnelles sur les utilisateurs. En novembre 2019, Abouammo a vendu des informations personnelles sur des utilisateurs anonymes à Riyad. En retour, il a reçu dix milles dollars et une montre de luxe. En novembre 2019, il a été arrêté à Seattle. Après un procès de deux semaines devant le tribunal fédéral de San Francisco, il a été reconnu coupable de blanchiment d'argent, de complot en vue de commettre une fraude télégraphique, de falsification de dossiers et d'être agent pour l'Arabie saoudite, et a été condamné à 10 à 20 ans de prison. Le verdict est intervenu après que les défenseurs des droits de l'homme ont critiqué Joe Biden et Emmanuel Macron pour leur approche diplomatique du prince héritier Mohammed ben Salmane[C'est-à-dire ?], qui a été exclu de la scène internationale à la suite du meurtre du journaliste saoudien Jamal Khashoggi en Turquie en 2018. En outre, le prince héritier et son gouvernement sont fréquemment accusés, par les ONG d'espionnage[Quoi ?], d'enlèvement et de torture de dissidents ; Riyad réfute vigoureusement ces affirmations.La cybercriminalité est reconnue par beaucoup d'experts comme étant la nouvelle forme de criminalité du XXIe siècle. Dès lors, pour la contrôler, la France a mis en place de nombreux organes de lutte. Voici quelques exemples de cyber-investigation.Dès 1998, a été créé, au sein de la gendarmerie, le département de lutte contre la cybercriminalité au sein du service technique de recherches judiciaires et de documentation (STRJD, devenu SCRC). Celle cellule a évolué et est devenue la Division de lutte contre la cybercriminalité (DLCC) composée du Département coordination et appuis numériques (DCAN), du Département investigations sur Internet (D2I), du Département prévention et suivi des phénomènes sur Internet (DPSPI) et du Département répression des atteintes aux mineurs sur Internet (DRAMI) qui intègre le Centre national d'analyse des images de pédopornographie (CNAIP). Devenue un centre de lutte contre les criminalités numériques (C3N) en 2015, cette unité rejoint en 2021 le nouveau commandement de la gendarmerie dans le cyberespace (ComCyberGend).Le 15 mai 2000 a été créé l'Office central de lutte contre la criminalité liée aux technologies de l'information et de la communication (OCLCTIC), au sein de la direction centrale de la police judiciaire au Ministère de l'Intérieur. Elle regroupe notamment en son sein la plate-forme de signalement des contenus illicites sur internet. Cette même année, en complément de l'action de l'OCLCTIC, a été mise en place, la direction de la Surveillance du territoire (DST), qui est compétente pour diligenter des enquêtes judiciaires relatives à des actes de piratage sur les systèmes informatiques des établissements à régime restrictif ou des données classifiées de défense.Par ailleurs, en 2006 a été créé l'OCRVP, office central pour la répression des violences aux personnes, dont la mission est la coordination, sur le plan national, de la lutte contre les infractions violentes à l'encontre des personnes, notamment concernant la pédopornographie sur internet.Enfin, la police nationale dispose de services spéciaux comme le SITT service de l'informatique et des traces technologiques. Les directions inter régionales et régionales de police judiciaire disposent d'ICC (Investigateurs en CyberCriminalité) anciennement dénommés ESCI (Enquêteurs Spécialisés en Criminalité Informatique). Il existe, en outre, différentes brigades spécialisées, telle la Brigade d'enquêtes sur les fraudes aux technologies de l'information (BEFTI).Le 30 juin 2014, le magistrat Marc Robert remet son rapport à Bernard Cazeneuve, Axelle Lemaire, Arnaud Montebourg et Christiane Taubira, pour mettre en place des mesures juridiques et techniques visant à freiner les risques liés à la cybersécurité et améliorer la protection des internautes. Marc Robert prône la création d'un Centre d'Alerte, l'ouverture d'un 17 de l'internet, la mise en place d'une Délégation interministérielle à la lutte contre la cybercriminalité placée sous la responsabilité directe du Premier ministre, etc..Les États ont rapidement compris que pour être plus efficace la lutte contre la cybercriminalité devait être européenne. Des compétences dans ce domaine ont alors été rapidement confiées à INTERPOL dont le rôle est la facilitation d’échange de renseignements afin de lutter efficacement contre toute forme de criminalité et notamment la criminalité informatique.Europol est aussi compétent en ce qui concerne la facilitation d’échanges de renseignements entre polices nationales notamment en matière de cybercriminalité. L'Union européenne (UE) a établi un Centre européen de lutte contre la cybercriminalité au sein d'Europol : EC3 (European Cybercrime Centre). L'EC3 est compétent pour soutenir les enquêtes des services spécialisés des États membres de l'UE dans des domaines tels que toutes fraudes en ligne en particulier la fraude à la carte de crédit, l'exploitation sexuelle des enfants en ligne (pédopornographie sur internet), les cyberattaques contre les systèmes d'infrastructures critiques de l'UE. L'EC3 apporte également un soutien en termes d'analyse criminelle stratégique aux États-membres notamment en produisant des analyses de la menace thématiques sur les dernières tendances en matière de cybercrime.EUROJUST, organe de l’Union européenne, a pour compétence l’amélioration de l’efficacité des autorités compétentes des états membres dans la lutte contre la criminalité organisée transfrontalière, donc notamment la cybercriminalité transnationale.Par ailleurs, a été créée en 2004 l’ENISA, agence européenne chargée de la sécurité des réseaux et de l’information, qui a diverses missions, dont notamment le recueil et l’analyse des données relatives aux incidents liés à la sécurité, ou encore le suivi de l’élaboration des normes pour les produits et services en matière de sécurité de réseaux et de l’information, mais aussi la promotion d’activités d’évaluation et de gestion des risques.Enfin, il existe le programme européen Safer internet plus qui lutte contre les contenus illicites, le traitement des contenus non désirés et préjudiciables, et qui fait la promotion d’un environnement plus sûr.Malheureusement la lutte contre la cybercriminalité n’est pas aisée. Il existe plusieurs obstacles juridiques et non juridiques à cette lutte. En premier lieu, le caractère vaste des réseaux informatiques, mais aussi la rapidité de commission des infractions, la difficulté de rassembler des preuves, et enfin des méthodes d’investigation et de contrôle qui peuvent se révéler attentatoires aux droits fondamentaux, en particulier au droit à l’anonymat et à la liberté d’expression.Au niveau juridique, ce qui pose aujourd’hui beaucoup de difficultés c’est le fait qu’un même comportement en France et à l’étranger n’est pas pareillement considéré. Il peut constituer une infraction dans un pays et pas dans l’autre. On peut citer pour exemple, la « promotion du cannabis », ou encore la « provocation pour surprendre les pédophiles ». Cela renvoie à un autre problème celui de la loi applicable. En effet, la cybercriminalité « bouleverse le principe classique de la territorialité de la loi pénale ». La loi française sera applicable dès lors qu’un élément constitutif de l’infraction a eu lieu en France (TGI de Paris 17e chambre, 26 février 2002). Ainsi, par exemple, la simple réception par l’utilisateur est un élément constitutif de l’infraction. Mais s’il n’y a pas d’élément constitutif de l’infraction en France, la loi française ne sera pas applicable.Il faut alors lutter chaque jour contre les paradis juridiques « cyber paradis », pour une meilleure efficacité du droit relatif à la cyber criminalité.Pour Jean-Loup Richet (Research Fellow à l'ESSEC ISIS), une autre difficulté dans la lutte contre la cybercriminalité est la rapide diffusion de nouvelles techniques de hacking, la réduction des coûts de l'activité criminelle et enfin la réduction des connaissances requises pour devenir un cybercriminel. En effet, les barrières à l'entrée n'ont jamais été aussi réduites : les services offerts par les plateformes de cloud computing peuvent être détournés pour lancer des campagnes de spam à moindre coûts, cracker un mot de passe voire augmenter la puissance d'un botnet. Selon Jean-Loup Richet, plus besoin d'être un expert en informatique pour devenir un cybercriminel : les communautés de hackers black hat commercialisent des logiciels permettant à leurs utilisateurs de mener des cyber attaques sans aucune compétence technique (Crimeware-as-a-service). Les communautés en ligne de cybercriminels contribuent au développement du cybercrime, fournissant des astuces, techniques, outils clefs en main et proposant même dans certains cas du tutorat de débutants désireux de devenir des cybercriminels.Selon la Revue française de criminologie et de droit pénal, la difficulté de la lutte contre la cybercriminalité réside également dans l’ambiguïté du cadre de régulation. Si le but d'une agression informatique est le système informatique de l'adversaire alors ce système peut-être assimilé à l'adversaire lui-même. La question est donc de savoir s'il faut établir un encadrement légal entre les machines et leurs propriétaires pour identifier ces actes criminels.Le coût de la cybercriminalité étant difficile à évaluer, des chiffres divers sont donnés.Selon deux études menées par le FBI et IBM en 2006, la cybercriminalité coûterait 67 milliards de dollars par an, rien qu'aux États-Unis.Selon le chef d'Interpol Khoo Boon Hui, 80 % de la cybercriminalité est liée en 2012 à des bandes organisées transfrontalières et représente un coût financier (750 milliards d'euros par an en Europe) plus important que les coûts combinés des trafics de cocaïne, marijuana et héroïne. Selon le rapport du Center for Strategic and International Studies (CSIS) de l'éditeur en sécurité McAfee, les activités cybercriminelles coûteraient entre 375 et 575 milliards de dollars par an.Le cybercrime et le piratage ont lourdement pesés sur les ventes du jeu The Witness, de Jonathan Blow, à sa sortie, au point que son créateur avoua que cela risquerait fortement de le limiter pour la création d'un nouveau jeu par la suite .« Chaque année, plus de 26 millions de Français sont victimes de cybercrimes, dont 9,17 millions subissent une perte financière nette. Au-delà des particuliers, les entreprises françaises sont de plus en plus ciblées, pour un dommage de 8,7 millions d’euros » en 2019.Roman d'anticipationLouis Charbonneau, Le Grand Ordinateur (Intruder) 1982 ;Élise Fontenaille, Unica, Paris, éditions Stock, 2006 — polar d'anticipation autour de la cyber-pédophilie, prix du Lundi ou grand prix de la Science-Fiction Française 2007, prix Rosny aîné 2008.Le hacking ou la cybercriminalité sont les sujets, principaux ou pas, de nombreux films, comme :1995 : Traque sur Internet (The Net), d'Irwin Winkler ;2000 : Cybertraque2007 : Die Hard 4 : Retour en enfer (Live Free or Die Hard) de Len Wiseman ;2015 : Hacker (Blackhat) de Michael Mann.Mr. RobotEye CandyPierre Penalba et Abigaelle Penalba, Cyber crimes. Un flic 2.0 raconte, Albin Michel, 2020, 288 p. (lire en ligne)Myriam Quéméner et Jean-Paul Pinte, Cybersécurité des acteurs économiques : Risques, réponses stratégiques et juridiques, Hermes Science Publications, coll. « Cyberconflits et cybercriminalité », 13 décembre 2012, 274 p. (ISBN 978-2-7462-3915-9)Éric Freyssinet, La cybercriminalité en mouvement, Cachan, Hermes Science Publications, coll. « Management et informatique », 27 septembre 2012, 240 p. (ISBN 978-2-7462-3288-4)Myriam Quéméner et Christian Aghroum, Etablissements financiers & cyberfraudes, Paris, La Revue Banque, 6 juin 2011, 127 p. (ISBN 978-2-86325-563-6)Myriam Quéméner et Yves Charpenel, Cybercriminalité : droit pénal appliqué, Paris, Economica, 13 septembre 2010, 272 p. (ISBN 978-2-7178-5902-7)Mohamed Chawki, Combattre la cybercriminalité, Perpignan, Editions de Saint-Amans, 15 mai 2009, 458 p. (ISBN 978-2-35941-002-0)Myriam Quéméner et Joël Ferry, Cybercriminalité : Défi mondial et réponses - 2e édition, Perpignan, Economica, 9 mars 2009, 308 p. (ISBN 978-2-7178-5700-9)Myriam Quéméner, Cybermenaces, Entreprises et Internautes, Paris, Economica, 1er novembre 2008, 274 p. (ISBN 978-2-7178-5642-2)Jean-Loup Richet, From Young Hackers to Crackers. International Journal of Technology and Human Interaction (IJTHI), 2013, 9(3), 53-62.Les infractions commises sur Internet, Abbas JABER, Thèse, Université de Bourgogne, France, novembre 2007.Le business de la cybercriminalité, Rodolphe Monnet et Franck Franchin, Hermès - Lavoisier, avril 2005.Le droit penal à l’épreuve de la cybercriminalité, Mohamed Chawki, Thèse, Université Lyon III, France, septembre 2006.Convention sur la cybercriminalitéCyberattaqueCyberguerreCybersécuritéCybercriminalité au CanadaForum international de la cybersécurité (FIC)Internet Crime Complaint CenterKevin MitnickIngénierie socialeSécurité des systèmes d'informationInternet Crimes Against Children Task Force (en)PédopornographieDépartement de la Justice des États-UnisOffice of Juvenile Justice and Delinquency Prevention (en)Force opérationnelleUn blog consacré à la cybercriminalitéL'actualité de la piraterie dans tous les domainesEssai sur la notion de cybercriminalitéLe vol d'informationsLes enjeux des fichiers cookiesL'anonymat dans le cyberespaceLa Fraude 419Le vol d'identité via le cyberespace Portail du droit   Portail de la sécurité informatique"
informatique;"La sécurité des systèmes d’information (SSI) ou plus simplement sécurité informatique, est l’ensemble des moyens techniques, organisationnels, juridiques et humains nécessaires à la mise en place de moyens visant à empêcher l'utilisation non autorisée, le mauvais usage, la modification ou le détournement du système d'information. Assurer la sécurité du système d'information est une activité du management du système d'information.Aujourd’hui, la sécurité est un enjeu majeur pour les entreprises ainsi que pour l’ensemble des acteurs qui l’entourent. Elle n'est plus confinée uniquement au rôle de l’informaticien. Sa finalité sur le long terme est de maintenir la confiance des utilisateurs et des clients. La finalité sur le moyen terme est la cohérence de l’ensemble du système d’information. Sur le court terme, l’objectif est que chacun ait accès aux informations dont il a besoin. La norme traitant des systèmes de management de la sécurité de l'information (SMSI) est l’ISO/CEI 27001 qui insiste sur Confidentiality – Integrity – Availability, c'est-à-dire en français disponibilité, intégrité et confidentialité.Les responsables de systèmes d'information se préoccupent depuis longtemps de sécuriser les données. Le cas le plus répandu, et sans aucun doute précurseur en matière de sécurité de l'information, reste la sécurisation de l'information stratégique et militaire. Le Department of Defense (DoD) des États-Unis est à l'origine du TCSEC, ouvrage de référence en la matière. De même, le principe de sécurité multi-niveau trouve ses origines dans les recherches de résolution des problèmes de sécurité de l'information militaire. La défense en profondeur, tout droit sorti d'une pratique militaire ancienne, et toujours d'actualité aujourd'hui. Cette pratique consiste à sécuriser chaque sous-ensemble d'un système.Les conséquences d'une mauvaise sécurisation peuvent concerner les organisations, mais aussi la vie privée d'une ou plusieurs personnes, notamment par la diffusion d'informations confidentielles comme leurs coordonnées bancaires, leurs situations patrimoniales, leurs codes confidentiels, etc. De manière générale, la préservation des données relatives aux personnes fait l'objet d'obligations légales régies par la Loi Informatique et Libertés.Aujourd'hui, il est généralement admis que la sécurité ne peut être garantie à 100 % et requiert donc le plus souvent la mobilisation d'une panoplie de mesures pour réduire les chances de pénétration des systèmes d'information.« Le système d'information représente un patrimoine essentiel de l'organisation, qu'il convient de protéger. La sécurité informatique consiste à garantir que les ressources matérielles ou logicielles d'une organisation sont uniquement utilisées dans le cadre prévu ».La sécurité des systèmes d'information vise les objectifs suivants (C.A.I.D.) :Confidentialité : seules les personnes autorisées peuvent avoir accès aux informations qui leur sont destinées (notions de droits ou permissions). Tout accès indésirable doit être empêché.Authenticité : les utilisateurs doivent prouver leur identité par l'usage de code d'accès. Il ne faut pas mélanger identification et authentification : dans le premier cas, l'utilisateur n'est reconnu que par son identifiant public, tandis que dans le deuxième cas, il doit fournir un mot de passe ou un élément que lui-seul connaît (secret). Mettre en correspondance un identifiant public avec un secret est le mécanisme permettant de garantir l'authenticité de l'identifiant. Cela permet de gérer les droits d'accès aux ressources concernées et maintenir la confiance dans les relations d'échange.Intégrité : les données doivent être celles que l'on attend, et ne doivent pas être altérées de façon fortuite, illicite ou malveillante. En clair, les éléments considérés doivent être exacts et complets. Cet objectif utilise généralement des méthodes de calcul de checksum ou de hachage.Disponibilité : l'accès aux ressources du système d'information doit être permanent et sans faille durant les plages d'utilisation prévues. Les services et ressources sont accessibles rapidement et régulièrement.D'autres aspects peuvent aussi être considérés comme des objectifs de la sécurité des systèmes d'information, tels que :La traçabilité (ou « preuve ») : garantie que les accès et tentatives d'accès aux éléments considérés sont tracés et que ces traces sont conservées et exploitables.La non-répudiation et l'imputation : aucun utilisateur ne doit pouvoir contester les opérations qu'il a réalisées dans le cadre de ses actions autorisées et aucun tiers ne doit pouvoir s'attribuer les actions d'un autre utilisateur.Une fois les objectifs de la sécurisation déterminés, les risques pesant sur chacun de ces éléments peuvent être estimés en fonction des menaces. Le niveau global de sécurité des systèmes d'information est défini par le niveau de sécurité du maillon le plus faible. Les précautions et contre-mesures doivent être envisagées en fonction des vulnérabilités propres au contexte auquel le système d'information est censé apporter service et appui.Il faut pour cela estimer :La gravité des conséquences au cas où les risques se réaliseraient ;La vraisemblance des risques (ou leur potentialité, ou encore leur probabilité d'occurrence).Pour sécuriser les systèmes d'information, la démarche adopte une spirale évolutive régulière : la fin d'un cycle entraîne le début d'un nouveau, comme dans la roue de Deming. En sécurité cela consiste à :évaluer les risques et leur criticitéquels risques et quelles menaces, sur quelles données et quelles activités, avec quelles conséquences ?  On parle de « cartographie des risques ». De la qualité de cette cartographie dépend la qualité de la sécurité qui va être mise en œuvre.rechercher et sélectionner les paradesque va-t-on sécuriser, quand et comment ?  Étape difficile des choix de sécurité : dans un contexte de ressources limitées (en temps, en compétences et en argent), seules certaines solutions pourront être mises en œuvre.mettre en œuvre les protections et vérifier leur efficacitéC'est l'aboutissement de la phase d'analyse et là que commence la protection du système d'information. Une faiblesse fréquente de cette phase est d'omettre de vérifier que les protections sont bien efficaces (tests de fonctionnement en mode dégradé, tests de reprise de données, tests d'attaque malveillante, etc.). Étape 1 : Périmètre et Politique Il est important de prendre en compte les actifs ayant de la valeur en définissant un périmètre du système de management du système d’information. Il peut être orienté sur l’ensemble de l’entreprise, sur un site précis, sur un service en fonction de la stratégie de l’entreprise. Le capital intellectuel des entreprises intègre des informations sensibles, ce patrimoine informationnel doit être protégé. L’entreprise doit donc mettre en place une politique de sécurité des systèmes d’information, de sécurité des données, et des mécanismes d’identification. De plus, il faut définir une politique du SMSI, qui est l’engagement de l’entreprise sur un certain nombre de points en matière de sécurité. Ces deux points forment la pierre angulaire du SMSI, dans le but d’établir la norme ISO/CEI 27001 et ainsi d’apporter la confiance aux parties prenantes. Étape 2 : Évaluation des risques Tenter de sécuriser un système d'information revient à essayer de se protéger contre les menaces intentionnelles et d'une manière plus générale contre tous les risques pouvant avoir une influence sur la sécurité de celui-ci ou des informations qu'il traite. Méthode d'analyse des risques Différentes méthodes d'analyse des risques sur le système d'information existent. Voici les méthodes d’appréciation des risques les plus courantes :En France, la première méthode développée a été Marion. Aujourd’hui, elle a été remplacée, même si certaines entreprises ont conservé ce modèle initial, par la méthode Méhari (Méthode harmonisée d'analyse des risques) développée par le CLUSIF, et par la méthode EBIOS (Expression des besoins et identification des objectifs de sécurité) développée par l'Agence nationale de la sécurité des systèmes d'information (ANSSI).En Angleterre, Cramm est une méthode d'analyse des risques développée par l'organisation du gouvernement britannique ACTC (Agence centrale de communication et des télécommunications). C’est la méthode d'analyse des risques préférée par le gouvernement britannique, mais elle est également utilisée par beaucoup d’autre pays.Les États-Unis utilisent OCTAVE (Operationally Critical Threat, Asset, and Vulnerability Evaluation), développée par l'Université de Carnegie Mellon.À l'international, on utilise ISO/CEI 27005, qui est une norme internationale répondant point par point aux exigences de la certification ISO/CEI 27001. C’est la norme la plus récente, de plus elle est facilement applicable car pragmatique.Même si le but de ces méthodes est identique, les termes et les expressions utilisés peuvent varier. Celles utilisées ci-dessus sont globalement inspirés de la méthode Feros.Paradoxalement, dans les entreprises, la définition d'indicateurs « sécurité du SI » mesurables, pertinents et permettant de définir ensuite des objectifs dans le temps raisonnables à atteindre, s'avère délicate. Pour mesurer la performance on peut désigner comme indicateurs les états d'installation d'outils ou de procédures, mais les indicateurs de résultats sont plus complexes à définir et à apprécier, par exemple ceux concernant les « alertes virales »[évasif]. Identifier les actifs Cela consiste à faire une liste de tous les éléments importants en matière d’information au sein du périmètre SMSI. Il existe différent types d'actifs :matérielphysiquelogicielhumaindocumentsimmatérielsPour l’identification des actifs, trois problèmes se posent :Le niveau de granularité : plus la notion de granularité est élevée plus la notion d’actif est large.Lister le plus important : le but n’est pas de faire une liste exhaustive d’actifs mais de recenser les plus importants d’entre eux.Ne pas confondre les actifs avec les actifs d’information : un actif est un élément qui possède de la valeur pour l’entreprise, alors qu'un actif d’information est ce qui possède de l’importance en matière d’information.Il est aujourd'hui indispensable de disposer de plans de sécurisation de l'activité pour en assurer la continuité et la reprise si un sinistre survient (Plan de reprise d'activité). Ces plans tentent de minimiser les pertes de données et d’accroître la réactivité en cas de sinistre majeur. Un plan de continuité d'activité efficace est quasi-transparent pour les utilisateurs, et garantit l'intégrité des données sans aucune perte d'information. Identifier les personnes responsables C’est la personne responsable d’un bien qui en répond. Il s’agit en général de celle qui connaît le mieux la valeur et les conséquences de disponibilité, d’intégrité et de confidentialité de l’actif. Dans une entreprise c’est généralement le responsable de la sécurité des systèmes d’information qui connaît le mieux les actifs de l’information. Identifier les vulnérabilités Chaque actif recensé présente des vulnérabilités, c’est une propriété intrinsèque du bien qui l’expose à des menaces. Identifier et modéliser les menaces Les vulnérabilités précédemment identifiées exposent les biens à des menaces. La norme ISO/CEI 27001 impose l’identification des menaces pour tous les biens recensés.Les principales menaces auxquelles un système d’information peut être confronté sont :un utilisateur du système : l'énorme majorité des problèmes liés à la sécurité d'un système d'information a pour origine un utilisateur, généralement insouciant. Il n'a pas le désir de porter atteinte à l'intégrité du système sur lequel il travaille, mais son comportement favorise le danger ;une personne malveillante : une personne parvient à s'introduire sur le système, légitimement ou non, et à accéder ensuite à des données ou à des programmes auxquels elle n'est pas censée avoir accès. Le cas fréquent est de passer par des logiciels utilisés au sein du système, mais mal sécurisés. Le Shoulder surfing est également une faille.un programme malveillant : un logiciel destiné à nuire ou à abuser des ressources du système est installé (par mégarde ou par malveillance) sur le système, ouvrant la porte à des intrusions ou modifiant les données ; des données confidentielles peuvent être collectées à l'insu de l'utilisateur et être réutilisées à des fins malveillantes ;un sinistre (vol, incendie, dégât des eaux) : une mauvaise manipulation ou une malveillance entraînant une perte de matériel et/ou de données. Identifier les conséquences La norme ISO 27001 oblige l’évaluation de conséquences ; tel que : la perte de confidentialité, de disponibilité ou d’intégrité. Cela revient à donner une note en trois dimensions (confidentialité ; disponibilité et intégrité), selon des critères définis, pour chaque actif. Identifier les dommages Quatre types de dommages peuvent affecter le système d'information d'une organisation:Les dommages financiers :Sous forme de dommages directs, c'est l'action de reconstituer des bases de données qui ont disparu, de reconfigurer un parc de postes informatiques ou de réécrire une application,Sous la forme de dommages indirects, c'est le dédommagement des victimes d'un piratage, le vol d'un secret de fabrication ou la perte de marchés commerciaux ;La perte de l'image de marque :Perte directe par la publicité négative faite autour d'une sécurité insuffisante tel que l'hameçonnage,Perte indirecte par la baisse de confiance du public dans une société, par exemple, les techniques répandues de défacement ;Les dommages réglementaires :L'indisponibilité d'un systèmes d'informations peut mettre en défaut l'entité devant ses obligations légales et juridiques ;Les dommages écologiques et/ou Sanitaires :La défaillance d'un système peut provoquer des catastrophes écologiques (ex. : AZF, marées noires, etc.),La défaillance d'un système peut provoquer des dégâts sanitaires (ex. : les centrales nucléaires, etc.). Évaluer la vraisemblance Il s’agit de remettre le bien d’information dans son contexte environnemental et donc de prendre en compte les mesures qui sont déjà mises en place (ex. : si un fichier client est déjà chiffré, alors la vraisemblance de voir sa confidentialité compromise est limitée). Il est possible d’évaluer la notion de vraisemblance par une note sur une échelle de 1 à 5. Estimer les niveaux de risque L’attribution d’une note finale reflétera le niveau de risque réel tout en tenant compte des éléments ci-dessus. La norme ISO 27001 n’impose aucune formule c’est donc à l’implémenteur de la choisir. Il peut s’agir d’une note allant de 0 à 100 ou d’un code couleur. Étape 3 : Traiter le risque et identifier le risque résiduel L’entreprise peut traiter les risques identifiés de 4 façons :Accepter le risquesolution ponctuelle lorsque la survenance du risque entraîne des répercussions acceptables pour l’entreprise.Éviter le risquesolution lorsque les conséquences d’une attaque sont jugées trop périlleuses pour l’entreprise.Transférer le risquesolution quand l’entreprise ne peut pas faire face au risque par ses propres moyens (souscription d’une assurance ou contrat de sous-traitance).Réduire le risquesolution pour rendre le risque acceptable.Enfin, il ne faut pas oublier de prendre en compte les « Risques résiduels » qui persistent après la mise en place de l’ensemble des mesures de sécurité. Il faut prendre des mesures complémentaires de protection pour rendre ces risques acceptables. Étape 4 : Sélectionner les mesures à mettre en place (Annexe A à ISO/CEI 27001) L'implémentation de la norme ISO2/CEI 27001 se déroule généralement en cinq phases complémentaires:Phase 1 Réunion de lancement : cette réunion sert à cadrer la prestation et à présenter la démarche des consultants.Phase 2 Entretiens : rencontres avec les différents responsables des services clé de l'entreprise dans le but de faire le point sur leur niveau de conformité avec la norme ISO/CEI 27001.Phase 3 Prise de connaissance de la documentation : documents de la politique générale (politique de sécurité, charte utilisateurs, etc.), documents de politique spécifiques (mots de passe, accès distant et procédure).Phase 4 Rédaction du rapport : rapport tenant compte de tous les éléments obtenus lors des phases précédentes.Phase 5 Présentation des résultats : réunion au cours de laquelle les points suivants sont traités ; rappel synthétique des points clé, présentation du plan de mise en conformité ISO/CEI 27001 et discussion. Plan de traitement des risques L’étape de planification identifie les mesures à prendre dans l’organisation, mais ne permet pas de les mettre en place concrètement. Il faut les organiser, sélectionner les moyens nécessaires et définir les responsabilités en établissant un plan de traitement des risques. Cette étape relève de la gestion de projet. Déployer les mesures de sécurité De nombreux moyens techniques peuvent être mis en œuvre pour assurer une sécurité du système d'information. Il convient de choisir les moyens nécessaires, suffisants, et justes. Voici une liste non exhaustive de moyens techniques pouvant répondre à certains besoins en matière de sécurité du système d'information :Contrôle des accès au système d'information ;Surveillance du réseau : sniffer, système de détection d'intrusion ;Sécurité applicative : séparation des privilèges, audit de code, rétro-ingénierie ;Emploi de technologies ad hoc : pare-feu, UTM, anti-logiciels malveillants (antivirus, anti-spam, anti-logiciel espion) ;Cryptographie : authentification forte, infrastructure à clés publiques, chiffrement.Plan de continuité d'activité : sauvegarde et restauration de données, Plan de Reprise d'activité. Générer des indicateurs Une des nouveautés de la norme ISO/CEI 27001 est d’exiger une vérification régulière de la sécurité. Le responsable doit choisir des indicateurs qui permettent de mesurer sa fiabilité. Ils peuvent être de deux sortes :indicateurs de performance : ils mesurent l’efficacité des mesures ;indicateurs de conformité : ils mesurent l’adéquation des mesures aux normes. Former et sensibiliser le personnel L’information du personnel est primordiale dans la réussite d’un projet de sécurisation du SI, pour qu’il en comprenne l’utilité et sache l’appliquer. Une bonne pratique est donc de sensibiliser l’ensemble du personnel aux enjeux de la sécurité informatique pour leur organisation, de manière généraliste. Cette explication doit rappeler les engagements de l’organisation, et donner des exemples très pratiques et des procédures internes pour éviter les incidents les plus habituels. Les employés directement concernés par la sécurité informatique doivent être formés pour qu’ils sachent utiliser correctement les outils.Une formation incluant l’inoculation psychologique contre les techniques d’ingénierie sociale, permet aux gens de résister aux tentations de s’écarter des procédures et des principes de sécurité. Gérer le SMSI au quotidien La norme ISO/CEI 27001 n’impose pas seulement de mettre en place un système de sécurité, mais aussi de prouver son efficacité. Les entreprises doivent donc gérer correctement leurs ressources et développer la traçabilité. Détection et réaction rapide des incidents Cette phase repose sur la théorie de time-based security. Le principe est de prendre en compte le délai nécessaire pour qu’une attaque contre la sécurité réussisse. Pendant ce laps de temps, l’entreprise doit être capable de détecter la menace et de la contrer, avec une marge de sécurité supplémentaire.Il doit y avoir des moyens de contrôle pour surveiller l’efficacité du SMSI ainsi que sa conformité.Il existe des outils pour vérifier cela comme :Les audits internes : Audit planifié longtemps en avance et faisant appel à des auditeurs.Le contrôle interne : Contrôle en permanence au sein de l’organisation, pour vérifier que chacun applique les procédures au quotidien.Les réexamens : Prendre du recul pour mettre en adéquation le SMSI et son environnement.On peut s’aider de :COBIT : permet l’analyse des risques et le contrôle des investissementsITIL : l’objectif est de favoriser l’efficacité des affaires dans l’utilisation du SI dans le but de satisfaire les demandes d’organisation pour réduire les coûts tout en maintenant ou améliorant les services informatiquesISO/CEI 27007 : lignes directrices pour aider les auditeurs internes ou externes à contrôler si le SMSI est correctement développé.Après la mise en lumière de dysfonctionnements grâce à la phase Check, il est important de les analyser et de mettre en place des :Actions correctives : Il faut agir sur le dysfonctionnement et en supprimer les effets.Actions préventives : On agit avant que le dysfonctionnement ne se produise.Actions d'amélioration : On améliore les performances d’un processus.Daniel Guinier, Sécurité et qualité des systèmes d'information : Approche systémique, Masson, 1992, 298 p. (ISBN 978-2-225-82686-3)Laurent Bloch et Christophe Wolfhugel, Sécurité informatique : Principes et méthode, Paris, Eyrolles, 2011, 325 p. (ISBN 978-2-212-13233-5, lire en ligne)Fernandez-Toro, Management de la sécurité de l'information. Implémentation ISO 27001 et audit de certification, Eyrolles, 2012Bernard Foray, La fonction RSSI (Responsable Sécurité Système d'Information) : Guide des pratiques et retours d'expérience - 2e édition, Dunod, 2011Laurent Bloch et Christophe Wolfhugel, Sécurité informatique : Principes et méthodes à l'usage des DSI, RSSI et administrateurs, Eyrolles, 2009, 292 p. (ISBN 978-2-212-12525-2, lire en ligne) Portail du management   Portail de la sécurité de l’information   Portail de la sécurité informatique"
informatique;"Fortran (mathematical FORmula TRANslating system) est un langage de programmation généraliste dont le domaine de prédilection est le calcul scientifique et le calcul numérique. Il est utilisé aussi bien sur ordinateur personnel que sur les superordinateurs, où il sert d'ailleurs à tester leurs performances dans le cadre du classement TOP500 des superordinateurs les plus puissants au monde, entre autres grâce à la bibliothèque LINPACK.Le nombre de bibliothèques scientifiques écrites en Fortran, éprouvées et améliorées pendant de longues années, et les efforts continus consacrés aux compilateurs pour exploiter au fil des décennies les nouvelles possibilités des calculateurs (vectorisation, coprocesseurs, parallélisme) ont maintenu l'usage de ce langage qui ne cesse d'évoluer.Parmi les fonctionnalités ajoutées ces dernières décennies, on citera le calcul sur les tableaux (qui peuvent comporter jusqu'à quinze dimensions), la programmation modulaire, la programmation générique (Fortran 90), le calcul haute performance (Fortran 95), la programmation orientée objet et l'interopérabilité avec les bibliothèques du langage C (Fortran 2003), la programmation concurrente et le calcul parallèle à l'aide des cotableaux (Fortran 2008), des équipes, des évènements et des sous-routines collectives (Fortran 2018), en plus des interfaces OpenMP, OpenACC et de la bibliothèque Message Passing Interface. La prochaine norme sera Fortran 2023. Les discussions sur le contenu de la suivante, Fortran 202y, ont commencé.Projet lancé en 1954 et aboutissant à une première version en 1957, Fortran est le premier langage de programmation de haut niveau, suivi notamment par Lisp (1958), Algol (1958) et COBOL (1959). Il est le premier langage à être normalisé, au milieu des années 60, et est devenu une norme ISO depuis Fortran 90.Le nom du langage est généralement écrit en majuscules (FORTRAN) pour désigner les versions du langage antérieures à la norme Fortran 90 car à l'époque les lettres minuscules ne font pas partie du jeu de caractères du langage. Par contre, il est toujours écrit avec une majuscule à partir de Fortran 90. Enfin, depuis environ 2010 les titres des livres en anglais utilisent souvent l'expression modern Fortran (Fortran moderne) pour distinguer la forme actuelle du langage de ses formes historiques.1953 : John Backus, jeune ingénieur titulaire d'une maîtrise de mathématiques recruté en 1950 chez IBM, développe pour l'IBM 701 le système Speedcoding, un interpréteur qui facilite la programmation en particulier pour le calcul en virgule flottante. En décembre, il rédige une lettre à l'attention de son supérieur Cuthbert Hurd (en) pour lui proposer le projet FORTRAN destiné à l'IBM 704, première machine commerciale dont le processeur supporte directement les nombres en virgule flottante. L'objectif est d'accélérer considérablement le développement et le débogage des programmes, jusqu'alors écrits en langage machine, afin de réduire leur coût d'exploitation, qui pour moitié provient des salaires des informaticiens et pour moitié des machines.1954 : le groupe de recherche de Backus, le « Programming Research Group » basé à New York, rédige un rapport intitulé Preliminary Report, Specifications for the IBM Mathematical FORmula TRANslating System, FORTRAN, daté du 10 novembre 1954. Il faut encore deux ans d'efforts pour terminer le premier compilateur FORTRAN (25 000 lignes), désigné alors par le mot anglais translator (traducteur). Dès le départ, ce compilateur est conçu pour fournir un code très optimisé, en particulier pour le calcul sur les tableaux et le traitement des boucles imbriquées, quasiment aussi rapide que celui qu'aurait écrit un programmeur en langage machine (objectif alors accueilli avec scepticisme par les clients d'IBM). D'après Backus lui-même, le développement du langage ne peut pas être séparé de la conception du compilateur et c'est même sur le compilateur que porte l'essentiel de l'effort initial. Le langage est défini au fur et à mesure, avec comme guide principal la simplicité de la syntaxe.1957 : le compilateur FORTRAN est déployé courant avril sur tous les IBM 704, sur bande magnétique, avec son manuel intitulé Preliminary Operator's Manual. Fin 1957, un manuel plus complet, le Programmer's Primer rédigé par Grace E. Mitchell, est édité. FORTRAN est un succès et une révolution car il n'est plus nécessaire d'être un expert de l'ordinateur pour écrire et déboguer des programmes. Mary Tsingou, physicienne et mathématicienne au Los Alamos National Laboratory et qui travailla avec Fermi, Pasta et Ulam, dira ainsi : « Quand le Fortran est arrivé, c'était presque comme le paradis ». L'instruction GO TO permet de sauter à une ligne numérotée par une étiquette. Le IF de cette première version est arithmétique : IF (A-B) 10, 20, 30 permet de sauter aux instructions d'étiquettes 10, 20 ou 30 selon que l'expression A-B est négative, nulle ou positive.1958 : FORTRAN II,, apporte les fonctions FUNCTION et les sous-programmes SUBROUTINE que l’on appelle par l’instruction CALL, ce qui permet aux programmeurs de se répartir plus facilement le travail. L'instruction COMMON permet à plusieurs sous-programmes de partager des données communes. Le nouveau compilateur est également plus rapide que le compilateur Fortran I : il permet en particulier de découper un long programme en plusieurs parties pouvant être compilées indépendamment.1959 : FORTRAN III n'est déployé que sur une vingtaine de machines. Il est possible d'insérer des routines en langage assembleur symbolique dans le code mais cette fonctionnalité sera abandonnée car elle compromettrait la portabilité des programmes écrits en FORTRAN. Le groupe de Backus n'est plus chargé du développement du FORTRAN, activité transférée à l'Applied Programming Department. Backus préconise le développement de deux compilateurs : un compilateur rapide pour la phase de débogage et un compilateur optimiseur pour le programme final. Mais l'idée n'est pas suivie.1960 : FORTRAN devient l'un des premiers langages multi-plateforme, des compilateurs devenant disponibles sur quelques machines d'autres constructeurs qu'IBM.1962 : FORTRAN IV introduit les nombres réels double précision, les complexes et les booléens, ainsi que les opérateurs .AND., .OR. et .NOT.. Le IF logique permet d'écrire par exemple IF (A .GE. B) GOTO 10 (aller à 10 si A est supérieur ou égal à B). Le type des variables peut désormais être déclaré explicitement. Il est nécessaire de modifier les programmes écrits en FORTRAN II : le traducteur automatique SIFT (SHARE Internal FORTRAN Translator) est mis à disposition. Cette même année, chaque compilateur apportant ses extensions et variantes, un comité des normes FORTRAN est formé afin de normaliser le langage pour qu'il soit portable d'une machine à l'autre.1965 : ECMA-9 FORTRAN est la première norme FORTRAN, publiée en avril 1965 par l'ECMA (European Computer Manufacturers Association) dans le cadre d'une collaboration avec l'ANSI (American National Standards Institute). Il s'agit en fait d'une version du langage intermédiaire entre les deux niveaux du langage définis dans la norme FORTRAN 66.1966 : FORTRAN 66 (ANSI X3.9-1966) est la norme développée par l'ANSI, essentiellement basée sur FORTRAN IV. Elle définit en fait deux niveaux du langage : le FORTRAN proprement dit et une version simplifiée, le Basic FORTRAN. Le langage FORTRAN est le premier langage de programmation à avoir été normalisé.1972 : l'ISO publie l'ISO Recommendation for Fortran (R1539), constituée des deux niveaux du langage définis dans FORTRAN 66 et du niveau intermédiaire défini par l'ECMA FORTRAN. Mais il ne s'agit que d'une recommandation et il faudra attendre Fortran 90 pour que le langage devienne une norme ISO.1977 : John Backus reçoit le Prix Turing pour « ses contributions profondes, influentes et durables à la conception de systèmes de programmation pratiques de haut niveau, notamment par ses travaux sur le FORTRAN, et pour ses publications pionnières sur les procédures formelles pour la spécification des langages de programmation. »1978 : FORTRAN 77 (ANSI X3.9-1978), est une évolution majeure. Comme pour FORTRAN 66, la norme définit deux niveaux du langage : le FORTRAN complet, ou full language, et une version simplifiée, ou subset language. Cette norme inclut en particulier des extensions au langage introduites par les différents compilateurs depuis FORTRAN 66. Elle apporte, entre autres améliorations, la programmation structurée avec les blocs IF / THEN / ELSE / END IF, le type de données CHARACTER en remplacement des constantes d'Hollerith (en) (qui sont supprimées de la norme), les fonctions LGE, LGT, LLE, LLT pour la comparaison des chaînes de caractères, l'attribut PARAMETER pour déclarer des constantes, l'attribut SAVE pour la persistance des variables locales, etc. Fin 1978, l'extension MIL-STD-1753 du département de la Défense américain introduit entre autres le END DO en FORTRAN 77 (bien que le label final reste obligatoire), les blocks DO WHILE / END DO, l'instruction INCLUDE, l'instruction IMPLICIT NONE et des fonctions pour manipuler les bits des entiers.1991 : Fortran 90 (ISO/IEC 1539:1991, puis ANSI X3.198-1992) est une version majeure ayant pour objectif de mettre Fortran au niveau des autres langages modernes. La norme apporte en particulier les modules, la récursivité, les arguments optionnels et nommés, la surcharge des opérateurs, une syntaxe pour le calcul sur les tableaux, l'allocation dynamique des tableaux grâce à l'attribut ALLOCATABLE, les types dérivés, l'attribut POINTER, l'instruction IMPLICIT NONE pour rendre obligatoire la déclaration des variables, les structures de contrôle SELECT CASE, les procédures SYSTEM_CLOCK et DATE_AND_TIME pour accéder à l'horloge du système, etc. Les restrictions concernant la mise en forme des programmes (colonnes 1 à 72) disparaissent : l'écriture se fait en format libre. Afin de rester compatible avec les nombreux codes industriels écrits en FORTRAN (Nastran, bibliothèques NAG et IMSL, etc.), Fortran 90 est conçu de telle façon que FORTRAN 77 en constitue un sous-ensemble.1992 : IEEE 1003.9-1992, volet FORTRAN 77 de la norme POSIX.1994 : ISO/IEC 1539-2:1994, qui définit des chaînes de caractères de longueur variable. Cette norme a été révisée en 2000.1997 : Fortran 95 (ISO/CEI 1539-1:1997) : quoique mise à jour mineure, cette norme introduit en particulier les instructions FORALL et WHERE pour le calcul vectoriel, les procédures PURE et ELEMENTAL et rend obsolescentes certaines fonctionnalités telles que les boucles à compteur réel ou l'instruction PAUSE. La procédure CPU_TIME permet de mesurer le temps processeur utilisé par un segment de programme.1999 : ISO/IEC 1539-3:1999, qui définit des directives de compilation conditionnelle. Cette norme a été révisée en 2011.2004 : Fortran 2003 (ISO/CEI 1539-1:2004) est une révision majeure qui supporte la programmation orientée objet. L'interface avec le langage C est assurée par le module interne ISO_C_BINDING et les mots-clés BIND et VALUE, qui permettent à un programme Fortran d'accéder facilement aux bibliothèques disponibles en C. Les pointeurs de procédure permettent de choisir lors de l'exécution une procédure à exécuter. Les types dérivés sont améliorés, ainsi que les entrées/sorties. On peut désormais gérer les exceptions en calcul flottant de la norme IEEE 754. La norme apporte également la gestion des caractères ISO 10646, base de l'Unicode. L'intégration avec le système d'exploitation est améliorée avec l'introduction des instructions get_command_argument, get_command, et command_argument_count.2010 : Fortran 2008 (ISO/CEI 1539-1:2010),, initialement pensée comme une révision mineure, introduit finalement les co-tableaux (co-arrays) comme paradigme de programmation parallèle. Les traitements sur ces co-tableaux sont effectués par des images (instances parallèles d'un programme Fortran). Cette norme introduit également les boucles DO CONCURRENT pour la parallélisation des itérations sans interdépendance. Les modules peuvent désormais comporter des sous-modules. Et les structures BLOCK...END BLOCK permettent de déclarer des variables à portée limitée n'importe où à l'intérieur d'une routine. La modularité est améliorée par l'introduction des SUBMODULE. De nouvelles procédures intrinsèques sont introduites pour la gestion des bits. De nouvelles constantes sont ajoutées au module ISO_FORTRAN_ENV, en particulier les KIND des types d'entiers INT8, INT16, INT32, INT64 et de réels REAL32, REAL64, REAL128.2018 : Fortran 2018 (ISO/CEI 1539-1:2018), considérée comme une révision mineure, introduit en particulier :ISO/IEC TS 29113:2012 Interopérabilité ultérieure de Fortran avec CISO/IEC TS 18508:2015 Caractéristiques parallèles supplémentaires en Fortran : les images peuvent désormais être regroupées en équipes (teams) travaillant sur des tâches différentes. Avec les événements (events), une image peut poster un évènement à destination d'autres images, ou attendre de recevoir un évènement. Les sous-routines collectives (collective subroutines) permettent d'effectuer des tâches simples sur les résultats d'un ensemble d'images, par exemple calculer la somme des valeurs d'une variable dans les différentes images.ISO/IEC/IEEE 60559:2011 Systèmes de microprocesseurs — Arithmétique flottante2023 : le planning pour la norme Fortran 2023, initialement dénommée Fortran 202x, a démarré en juin 2017. Le document de travail du comité est disponible. La publication de la norme est prévue en juillet 2023. Il s'agit d'une version mineure qui apporte de nombreuses améliorations à diverses parties du langage. Par exemple, la longueur maximale des lignes de programme passera de 132 à 10000 caractères, et les expressions conditionnelles, reprenant la syntaxe du C, font leur apparition.Fortran fait partie des langages normalisés depuis 1965 et est devenu une norme ISO depuis Fortran 90. La norme Fortran est gérée par le groupe de travail ISO/IEC JTC1/SC22/WG5, généralement simplement appelé WG5 (pour Working Group 5), qui charge le comité Fortran US INCITS PL22.3 (généralement appelé J3, en référence à son ancien nom ANSI X3J3) de développer le langage. Le WG5 est composé d'experts chargés de faire des recommandations pour faire évoluer le langage. Le J3 est composé de fabricants de matériel, d'éditeurs de compilateurs, d'utilisateurs issus aussi bien de l'industrie que du monde académique.Chaque révision de la norme peut ajouter de nouveaux paradigmes ou fonctionnalités, éventuellement déjà implémentées par les compilateurs sous forme d'extensions au langage, clarifier des points restés ambigus, mais aussi rendre obsolescentes d'anciennes fonctionnalités. En effet, depuis Fortran 90, les normes comportent systématiquement en annexes une liste des fonctionnalités supprimées et une liste des fonctionnalités obsolètes et donc susceptibles d'être supprimées dans une prochaine révision de la norme. Les compilateurs continuent néanmoins généralement de supporter ces fonctionnalités pour assurer la pérennité des codes déjà développés. Enfin, il s'écoule généralement plusieurs années entre la publication d'une nouvelle norme et la prise en charge intégrale de ses nouvelles fonctionnalités dans les compilateurs,.En 1995, le WG5 met en place des rapports techniques de type 2 pour travailler sur des fonctionnalités importantes qui n'auront pas le temps d'être intégrées à la norme en cours de rédaction, mais en constitueront une extension qui pourra être intégrée dans la norme suivante.En 2019, un dépôt GitHub est créé afin que tous les utilisateurs du langage puissent proposer facilement au comité J3 des évolutions pour les normes futures. Il sert actuellement à proposer des nouveautés pour la norme Fortran 202Y qui succédera à Fortran 2023, comme par exemple l'amélioration de la programmation générique, des valeurs par défaut pour les arguments optionnels, etc.En 1955, IBM crée le groupe d'utilisateurs SHARE afin que ses clients puissent échanger entre eux. John Backus y fait des présentations régulières durant le développement du premier compilateur FORTRAN. En avril 1957, des ingénieurs de Westinghouse y rapportent la compilation du premier programme FORTRAN en dehors d'IBM. Alors que le langage se diffuse, un sous-groupe SHARE y est dédié : le FORTRAN Standard Commitee.Créé en juillet 1982, le bulletin mensuel FORTRAN Forum a été publié par le SIGPLan (Special Interest Group in Programming Languages) de l'ACM (Association for Computing Machinery) trois fois par an, jusqu'en avril 2020.Le 29 novembre 1983, un groupe de discussion net.lang.f77 est créé sur  Usenet. Le 7 novembre 1986, il est renommé comp.lang.fortran et est toujours l'un des principaux canaux de communication de la communauté Fortran.La liste de diffusion comp-fortran-90 est dédiée aux questions concernant le Fortran à partir de la norme Fortran 90. On peut en consulter les archives jusque 1997, mais l'activité y est désormais très réduite avec seulement six messages postés en 2020.Un groupe Fortran Programmers est créé sur LinkedIn en juillet 2008.Le site Fortran Wiki est créé en octobre 2008. Il est édité par les utilisateurs du langage et propose de nombreuses ressources.Début 2020, une nouvelle communauté d'utilisateurs fortran-lang.org est créée, afin de fédérer les efforts dans l'écosystème Fortran, sur le modèle de langages plus jeunes. En s'appuyant sur GitHub, elle développe en particulier une bibliothèque standard Fortran (stdlib) similaire à celle du C, un gestionnaire de paquets Fortran (fpm) faisant également office de système de compilation, le compilateur interactif LFortran, ainsi que des tutoriels pour apprendre le Fortran moderne. Certaines pages sont traduites en français. La communauté édite une lettre mensuelle résumant ses activités en cours et diffuse des informations sur Twitter. Une visioconférence mensuelle permet à ses membres de discuter des projets à mener. Son forum Fortran Discourse est devenu un lieu central de discussion pour la communauté Fortran. Les projets de la communauté ont reçu l'aide de cinq étudiants lors du Google Summer of Code 2021.L'International Fortran Conference (FortranCon) est créée en 2020. Initialement prévue à Zurich début juillet 2020, elle a lieu en visioconférence à cause de la pandémie de Covid-19. La seconde édition, FortranCon 2021, a également lieu en visioconférence les 23 et 24 septembre 2021. Les vidéos des conférences sont disponibles sur la chaîne YouTube FortranCon. La prochaine édition est prévue pour 2023, un rythme d'environ dix-huit mois ayant été choisi.Fortran est toujours l'un des langages les plus utilisés pour le calcul intensif, que ce soit pour l'astronomie, la modélisation climatique, la modélisation chimique, la modélisation en économie, la mécanique des fluides numérique, la physique numérique, l'analyse de données, la modélisation hydrologique, l'algèbre linéaire numérique et les bibliothèques numériques (LAPACK, IMSL et NAG), l'optimisation, la simulation de satellites, l'ingénierie des structures et les prévisions météorologiques. Les calculs peuvent aussi bien être réalisés sur des ordinateurs de bureau que sur des supercalculateurs.De nombreux tests de performance (benchmarks) destinés à évaluer les performances des nouveaux microprocesseurs sont écrits en Fortran.Avant la norme Fortran 90, le FORTRAN, créé à l'époque des cartes perforées (en particulier avec le système FMS), utilise une mise en page adaptée à ces supports :la colonne 1 peut contenir la lettre C indiquant un commentaire. Le caractère * est aussi accepté ;les colonnes 1 à 5 peuvent contenir une étiquette numérique (facultative) de l'instruction, dont la valeur peut être limitée à 32 767 ou 9 999 suivant le compilateur ;la colonne 6 indique une suite de l'instruction précédente ;le code commence à partir de la 7e colonne et ne doit pas dépasser la 72e. Les espaces n'ont pas de signification dans ces colonnes : une boucle DO 10 I=1,5 peut aussi s'écrire DO10I=1,5 (le 10 est ici l'étiquette obligatoire de fin de boucle) ;les colonnes 73 à 80 servent à l'identification et la numérotation des cartes perforées (souvent les trois initiales du projet, du chef de projet ou du programmeur, suivies de numéros de cinq chiffres attribués de dix en dix pour permettre des insertions de dernière minute).Les extensions de fichiers les plus courantes pour le format fixe sont .f et .for, mais ce n'est qu'une convention adoptée par la plupart des compilateurs. Rien n'empêche de les utiliser avec le format libre à condition d'en avertir le compilateur à l'aide de l'option adéquate.Depuis la norme Fortran 90, le code source est écrit suivant un format dit libre : il n'y a plus de colonne particulière, les lignes font au maximum 132 caractères (mais elles peuvent être continuées à l'aide du caractère &), les commentaires sont introduits par un point d'exclamation (éventuellement disposé à la suite d'une instruction Fortran). L'extension de nom de fichier la plus courante est alors .f90, même si le programme utilise des fonctionnalités de normes plus récentes telles que Fortran 2018.Notes :ce programme est écrit en Fortran moderne. Il nécessite un compilateur implémentant les bases de la norme Fortran 2008 ;l'instruction use permet d'importer le module intrinsèque iso_fortran_env qui définit des constantes, en particulier pour les types de réels disponibles (real32, real64, real128). Ici seule est importée la constante real128 qui sera désignée par l'alias wp (working precision). Les nombres réels apparaissant dans le programme sont suffixés par cet alias afin de définir leur type. Il suffirait ainsi de remplacer real128 par real64 ou real32 pour modifier de façon cohérente la précision numérique utilisée dans l'ensemble du programme, pour autant que le compilateur prenne en charge la précision correspondante ;l'instruction implicit none, introduite dans la norme Fortran 90, rend la déclaration des variables obligatoire. Historiquement, celle-ci est en effet facultative : les variables dont le nom commence par une des lettres I, J, K, L, M ou N sont par défaut de type integer, les autres de type real. Ce mécanisme est désormais fortement déconseillé et l'instruction implicit none doit donc être systématiquement utilisée ;la déclaration se fait en début de routine. Le type de données et les noms de variables sont séparés par ::. On utilise les entiers par défaut pour deg. La numérotation des tableaux commence par défaut à 1 en Fortran mais ici on la fait commencer à 0 pour le tableau radians(0:90). Les constantes, qui peuvent être calculées, sont spécifiées par l'attribut parameter. Les chaînes de caractères sont de longueur fixe, mais ici l'étoile indique que la longueur de la chaîne doit être définie en fonction de la longueur de son contenu ;le contenu du tableau radians() est calculé à l'aide d'un constructeur, avec deg pour variable de boucle variant de 0 à 90. Le Fortran permettant de calculer directement sur des tableaux, on aurait pu écrire également radians = coeff * [ (deg, deg=0,90) ] ;l'instruction write se réfère à une unité d'entrée-sortie (* désigne le terminal) et une spécification de format. Ce format est ici stocké dans la chaîne ligne_horizontale et décrit qu'il faudra afficher 49 tirets. On aurait également pu utiliser une déclaration format située sur une autre ligne et précédée d'un label numérique ;l'instruction do deg = 0, 90 indique de répéter en boucle les instructions qui suivent (jusqu'au end do) pour toutes les valeurs de deg de 0 à 90 par pas de 1 ;le write à l'intérieur de la boucle permet d'écrire sur le terminal les valeurs des variables deg et radians(deg) en utilisant deux caractères pour deg (qui est un entier) et 34 caractères dont 32 après la virgule pour radians(deg) qui est un réel.De nombreux compilateurs commerciaux ou libres sont disponibles. Compilateurs libres Avant sa version 4.0, le compilateur libre GCC incluait le compilateur g77 pour le FORTRAN 77, qui a été remplacé en 2005, par le compilateur GFortran, lui-même issu d'un fork réalisé en 2003 de G95, autre compilateur libre développé entre 2000 et janvier 2013. En septembre 2019, GFortran prend en charge quasiment intégralement Fortran 2003, presque tout Fortran 2008 et environ 20 % de Fortran 2018. Contrairement aux compilateurs Intel et Cray, il ne gère pas encore de façon native la programmation parallèle avec les co-tableaux mais nécessite l'installation de la bibliothèque OpenCoarrays.Omni Compiler est un méta-compilateur C et Fortran destiné à transformer du code contenant des directives XcalableMP et OpenACC en code parallèle natif. Compilateurs propriétaires On trouve de nombreux compilateurs commerciaux, parmi lesquels : Lahey, Absoft, Portland Group (en) (filiale de NVidia), NAG, etc. La plupart des fabricants de stations de travail ou d'ordinateurs destinés au calcul intensif, proposent également un compilateur de Fortran : Intel, IBM, Oracle (à la suite du rachat de Sun Microsystems), HPE Cray (Cray a été racheté par HP en 2019), etc. Le compilateur Intel Visual Fortran est l'héritier de DEC Visual Fortran, devenu Compaq Visual Fortran puis HP Visual Fortran.Certains de ces compilateurs commerciaux ont des versions gratuites pour une utilisation non commerciale : c'est le cas d'Oracle, Portland Group. On peut télécharger le compilateur BiSheng que Huawei a développé pour sa plateforme Kunpeng : pour le Fortran, il utilise Flang en frontal.Quant aux compilateurs Intel, ils sont depuis janvier 2021 gratuits pour tous les développeurs. Compilateurs en cours de développement Début 2021, de nouveaux compilateurs Fortran basés sur LLVM sont en développement : LFortran qui vise à permettre de tester du code de façon interactive, Flang et Intel Fortran Compiler ifx, le successeur d'ifort. Compilateurs en ligne Le Fortran fait partie des langages proposés par certains compilateurs en ligne, tels que codingground, OnlineGDB, JDOODLE et godbolt Compiler Explorer. Certains proposent le choix entre plusieurs compilateurs Fortran ou plusieurs versions d'un compilateur, ou permettent d'analyser le code avec un débogueur et de voir le langage machine généré. Ces outils permettent donc d'apprendre le langage sans installer de compilateur sur sa machine, ils permettent également de collaborer en ligne sur un projet.Open64 (en) est un compilateur Fortran 95 libre arrêté en 2011.Compilateur FORTRAN 77 libre Open Watcom : arrêté en 2010.PathScale (en) : arrêté en 2013.HP.Unisys.Certains fabricants ont disparu, tels que CDC ou DEC.La plupart des éditeurs de texte offrent une coloration syntaxique pour le Fortran : Emacs, Notepad++, Sublime Text, Vim, Neovim, Visual Studio Code... Parmi les environnements de développement intégrés, il existe une version de Code::Blocks dédiée au développement en Fortran. Il existe également une version d'Eclipse dédiée au Fortran, nommée Photran, mais dont la dernière version date de 2015. Sous Windows, le compilateur Intel Fortran est intégré à Visual Studio.Les systèmes de construction de projet tels que CMake et Meson gèrent généralement le Fortran. L'utilitaire makedepf90 permet de générer des fichiers Make pour un projet Fortran.Mais Fortran dispose depuis fin 2020 du gestionnaire de paquets fpm (Fortran Package Manager), qui fait office également de système de construction de projet. Il est inspiré par l'outil Cargo du langage Rust. Il permet en particulier de gérer les dépendances, qu'il peut télécharger automatiquement depuis GitHub. Et il est lui-même écrit en Fortran.fpt est un outil d'analyse de code Fortran. On peut également citer CamFort, un projet universitaire libre, ftncheck, qui est limité au FORTRAN 77 et qui n'est plus développé depuis 2005. findent est un indenteur de code source qui peut également transformer l'ancien format fixe du Fortran en format libre.Plusieurs analyseurs de type lint sont disponibles : Flint et fortran-linter, logiciels libres écrits en Python, et FortranLint, produit commercial.Lizard est un analyseur de complexité cyclomatique qui prend en charge une vingtaine de langages, dont le Fortran.Pour le débogage, on peut par exemple utiliser les débogueurs GNU gdb ou idb (Intel Debugger). Pour le profilage sous système de type UNIX, on peut utiliser gprof et Valgrind. La couverture de code peut être évaluée avec Gcov (en).Fypp est un préprocesseur Python qui peut être utilisé avec n'importe quel langage, tout en étant avant tout destiné au langage Fortran. Il utilise la syntaxe du Python.PFUnit (en) est un framework de tests unitaires, libre et initialement développé par la NASA. Le framework test-drive est utilisé dans les projets fpm et stdlib, et il prend en charge Meson, CMake et fpm. On peut également citer vegetables et FRUIT.Parmi les générateurs de documentation gérant le Fortran, on peut citer Doxygen, FORD et ROBODoc.Quickstart Fortran permet d'installer facilement sous Microsoft Windows, sans nécessiter les droits d’administration, les outils essentiels pour développer en Fortran : GCC-GFortran, Fortran Package Manager, Git for Windows, OpenBLAS (BLAS/LAPACK), GNU make. Il peut également faciliter l'installation d'Intel OneAPI et de la librairie stdlib en cours de développement.De nombreuses bibliothèques de calcul ont été développées en Fortran. Certaines sont développées, utilisées, testées et donc déboguées depuis des décennies, ce qui leur assure une très grande fiabilité.LAPACK (Linear Algebra Package) est une bibliothèque dédiée comme son nom l'indique à l'algèbre linéaire numérique.Basic Linear Algebra Subprograms (BLAS) est un ensemble de fonctions standardisées réalisant des opérations de base de l'algèbre linéaire.LINPACK est une bibliothèque de fonctions pour l'algèbre linéaire, et notamment la résolution numérique de systèmes d'équations linéaires.Physics Analysis Workstation (PAW).International Mathematics and Statistics Library (IMSL) est une bibliothèque logicielle d'objets utilisables pour le développement informatique d'applications d'analyse numérique.NAG Fortran Library propose plus de 1700 routines mathématiques et statistiques.SLATEC est une bibliothèque dans le domaine public proposant 1400 fonctions mathématiques, initalement développée en Fortran 77. Le code source a été adapté en Fortran moderne avec une version 4.2 datée de 2019....Les normes Fortran n'incluant pas d'instructions graphiques ou d'instructions pour construire des interfaces graphiques, la visualisation peut se faire après exécution avec des outils externes tels que ParaView, ou en appelant des outils de tracé tels que Gnuplot via l'instruction EXECUTE_COMMAND_LINE(), ou enfin à l'aide de bibliothèques :DISLIN, créé par le Max Planck Institute for Solar System Research, permet de tracer des données et de réaliser des interfaces graphiques. Multiplate-formes (UNIX, Linux, FreeBSD, OpenVMS, Windows et MS-DOS). Fonctionne avec de nombreux compilateurs, ainsi que d'autres langages que le Fortran. Téléchargeable gratuitement depuis la version 11.3 de mars 2020.GINO permet de tracer des données et de réaliser des interfaces graphiques. Logiciel commercial pour Windows et Linux.GrWin Graphics Library : logiciel libre pour Windows.gtk-fortran est une bibliothèque sous licence libre GPL 3 permettant de créer des interfaces graphiques GTK en Fortran, grâce aux fonctionnalités d'interopérabilité Fortran / C introduites depuis la norme Fortran 2003. Multi-plateforme (Linux, Windows, macOS, FreeBSD, Raspberry Pi...). Supporte GTK 4, la bibliothèque généraliste GLib et la bibliothèque PLplot. Peut être utilisée comme dépendance fpm.JAPI (Java Application Programming Interface) : interface Java/Fortran permettant de créer une interface graphique complète pour les programmes Fortran. Multiplate-formes (Windows, Linux, Solaris). Fonctionne avec de nombreux compilateurs (entre autres gfortran, Compaq Visual Fortran…). Logiciel libre sous licence LGPL.MATFOR : construction d'interfaces graphiques et bibliothèques numériques et graphiques pour Fortran et d'autres langages. Logiciel commercial pour Windows et Linux.ogpf permet d'accéder facilement à gnuplot depuis un programme Fortran, grâce à l'utilisation de la programmation orientée objet. Il peut être utilisé comme paquet fpm.PLplot (en) : bibliothèque permettant de dessiner des graphiques scientifiques. Multilangage et multiplate-formes (Linux, OpenSolaris, Unix, MS-DOS, Windows, Mac OS X). Logiciel libre sous licence LGPL.Quickwin : bibliothèque graphique fournie avec le Compaq Visual Fortran (désormais Intel Visual Fortran). Ne fonctionne que sous Windows.Winteracter : construction d'interfaces graphiques et d'outils de visualisation. Log"
informatique;"L'informatique est un domaine d'activité scientifique, technique, et industriel concernant le traitement automatique de l'information numérique par l'exécution de programmes informatiques hébergés par des dispositifs électriques-électroniques : des systèmes embarqués, des ordinateurs, des robots, des automates, etc.Ces champs d'application peuvent être séparés en deux branches :théorique : concerne la définition de concepts et modèles ;pratique : s'intéresse aux techniques concrètes de mise en œuvre.Certains domaines de l'informatique peuvent être très abstraits, comme la complexité algorithmique, et d'autres peuvent être plus proches d'un public profane. Ainsi, la théorie des langages demeure un domaine davantage accessible aux professionnels formés (description des ordinateurs et méthodes de programmation), tandis que les métiers liés aux interfaces homme-machine (IHM) sont accessibles à un plus large public.Le terme « informatique » résulte de l'association du terme « information » au suffixe « -ique » signifiant « qui est propre à » :Comme adjectif, il s'applique à l'ensemble des traitements liés à l'emploi des ordinateurs et systèmes numériques.Comme substantif, il désigne les activités liées à la conception et à la mise en œuvre de ces machines. Des questions de télécommunications comme le traitement du signal ou la théorie de l'information, aussi bien que des problèmes mathématiques comme la calculabilité s'y rattachent.Dans le vocabulaire universitaire américain, l'informatique (« computer science ») désigne surtout l'informatique théorique : un ensemble de sciences formelles qui ont pour objet d'étude la notion d'information et des procédés de traitement automatique de celle-ci, l'algorithmique.Les applications de l'informatique depuis les années 1950 forment la base du secteur d'activité des technologies de l'information et de la communication. Ce secteur industriel et commercial est lié à la fois aux procédés (logiciels, à l'architectures de systèmes) et au matériel (électronique, télécommunication). Le secteur fournit également de nombreux services liés à l'utilisation de ses produits : développement, maintenance, enseignement, assistance, surveillance et entretien.En 1957, l'ingénieur allemand Karl Steinbuch crée le terme « Informatik » pour son essai intitulé Informatik: Automatische Informationsverarbeitung, pouvant être rendu en français par « Informatique : traitement automatique de l'information ».En mars 1962, Philippe Dreyfus, ancien directeur du Centre national de calcul électronique de Bull, utilise pour la première fois en France le terme « Informatique » pour son entreprise « Société d'informatique appliquée » (SIA). Selon certains, ce néologisme est un mot-valise qui agglomère « information » et « automatique », pour désigner le traitement automatique des données,.Le même mois, Walter Bauer inaugure la société américaine « Informatics Inc. » qui dépose son nom et poursuit toutes les universités qui utilisent ce mot pour décrire la nouvelle discipline, les forçant à se rabattre sur computer science, bien que les diplômés qu'elles forment soient pour la plupart des praticiens de l'informatique plutôt que des scientifiques au sens propre[réf. nécessaire]. L’Association for Computing Machinery, la plus grande association d'informaticiens au monde, approche même Informatics Inc. afin de pouvoir utiliser le mot informatics en remplacement de l'expression computer machinery, mais l'entreprise décline la proposition[réf. nécessaire]. En 1985 Sterling Software rachète la société Informatics Inc. qui cesse ses activités en 1986[réf. souhaitée]. Pour Donald Knuth, cependant, les Américains ont délibérément écarté le mot informatique, non pour un problème de marque mais pour des raisons sémantiques ; les ordinateurs ne traitent pas de l'information, mais des données, dont le sens informatif est parfaitement indifférent[réf. nécessaire].En 1966, l'Académie française consacre l'usage officiel du mot pour désigner la « science du traitement de l'information ». La presse, l'industrie et le milieu universitaire l'adoptent dès cette époque.En juillet 1968, le ministre fédéral de la Recherche scientifique d'Allemagne de l'Ouest, Gerhard Stoltenberg, prononce le mot « Informatik » lors d'un discours officiel sur la nécessité d'enseigner cette nouvelle discipline dans les universités de son pays ; on emploie ce même terme pour nommer certains cours dans les universités allemandes. Le mot informatica fait alors son apparition en Italie et en Espagne, de même qu’informatics au Royaume-Uni.Les fondateurs de la Compagnie Générale d'Informatique (CGI) reprennent le mot « informatique » en 1969.Dans l'usage contemporain, le substantif « informatique » devient un mot polysémique qui désigne autant le domaine industriel en rapport avec l'ordinateur (au sens de calculateur fonctionnant avec des algorithmes), que la science du traitement des informations par des algorithmes.Les expressions « science informatique », « informatique fondamentale » ou « informatique théorique » désignent sans ambiguïté la science, tandis que « technologies de l'information » ou « technologies de l'information et de la communication » désignent le secteur industriel et ses produits. Des institutions assimilent parfois la compétence des utilisateurs dans la manipulation des appareils à l'alphabétisation ou à la conduite automobile, comme veut le faire entendre l'expression European Computer Driving License (traduction littérale : « permis de conduire un ordinateur »),.Plusieurs termes en anglais désignent l'informatique :informatics (en) : surtout en tant que domaine scientifique (se rencontre en Europe de l'Ouest) ;computer science : l'informatique fondamentale ou science des calculateurs, une branche de la science en rapport avec le traitement automatique d'informations ;computingcomputing : qui qualifie les activités nécessitant une masse d'opérations mathématiques et logiques (par exemple, dans cloud computing ou decision support computing) ;electronic data processing : traitement des données à l'aide de l'électronique ;Information technology : souvent utilisé pour désigner le secteur industriel des technologies de l'information,.Dans le monde du travail, on parle volontiers d’I.T., le département informatique étant the I.T. department (les autres termes ne sont quasiment jamais utilisés).Depuis des millénaires, l'Homme a créé et utilisé des outils l'aidant à calculer (abaque, boulier, etc.), exigeant, comme les opérations manuelles, des algorithmes de calcul, dont des tables datant de l'époque d'Hammourabi (environ 1750 av. J.-C.) figurent parmi les exemples les plus anciens.Si les machines à calculer évoluent constamment depuis l'Antiquité, elles n'exécutent pas elles-mêmes l'algorithme : c'est l'homme qui doit apprendre et exécuter la suite des opérations, comme pour réaliser les différentes étapes d'une division euclidienne. En 1642, Blaise Pascal imagine une machine à calculer,, la Pascaline, qui fut commercialisée. Sept exemplaires subsistent dans des musées comme celui des Arts et Métiers à Paris, et deux sont dans des collections privées (IBM en possède une). Joseph Marie Jacquard avec ses métiers à tisser à cartes perforées illustre en premier le concept de programmation, comme enchaînement automatique d'opérations élémentaires. George Boole et Ada Lovelace esquissent une théorie de la programmation des opérations mathématiques.Le secteur très féminisé à ses débuts avec des pionnières comme Ada Lovelace, Grace Hopper, Frances Allen, Adele Goldberg est devenu progressivement plus masculin avec la professionnalisation des différents métiers dans l'informatique (premiers diplômes en informatique). La programmation était vue au début comme une activité essentiellement féminine avant de devenir une profession prisée et largement investie par les hommes. La place des femmes en informatique décroit dès le milieu des années 1980 en France.Dans les années 1880, Herman Hollerith, futur fondateur d'IBM, fonde la mécanographie en inventant une machine électromécanique destinée à faciliter le recensement en stockant les informations sur une carte perforée. Le gouvernement des États-Unis utilise pour la première fois à grande échelle les trieuses et les tabulatrices lors du recensement de 1890, à la suite de l'afflux des immigrants dans ce pays dans la seconde moitié du XIXe siècle.L'ingénieur norvégien Fredrik Rosing Bull a créé la première entreprise européenne qui a développé et commercialisé des équipements mécanographiques. Installé en Suisse dans les années 1930 il est ensuite venu en France pour s'attaquer au marché français. Pendant la Seconde Guerre mondiale, René Carmille utilisait des machines mécanographiques Bull.Les Allemands étaient équipés de machines mécanographiques avant la Seconde Guerre mondiale. Ces équipements étaient installés dans des ateliers composés de trieuses, interclasseuses, perforatrices, tabulatrices et calculatrices connectées à des perforateurs de cartes. Des machines électromécaniques utilisant aussi des lampes radio comme les triodes effectuaient les traitements. Ces lampes dégageaient de la chaleur qui attirait les insectes, et les bugs (terme anglais pour insectes, francisé en « bogue ») étaient une cause de panne courante.Les femmes occupent une place prépondérante au début de l'informatique dans les activités de calcul et de programmation. Les programmeuses de l'ordinateur ENIAC en 1944 sont six mathématiciennes : Marlyn Meltzer, Betty Holberton, Kathleen Antonelli, Ruth Teitelbaum, Jean Bartik, Frances Spence. Adele Goldstine est leur formatrice et elles sont surnommées les « ENIAC girls ».L'informatique moderne n'a pu émerger qu'à la suite de l'invention du transistor en 1947 et son industrialisation dans les années 1960.L'informatique moderne commence avant la Seconde Guerre mondiale, lorsque le mathématicien Alan Turing pose les bases d'une théorisation de ce qu'est un ordinateur, avec son concept de machine universelle de Turing. Turing pose dans son article les fondements théoriques de ce qui sépare la machine à calculer de l'ordinateur : la capacité de ce dernier à réaliser un calcul en utilisant un algorithme conditionnel.Après la Seconde Guerre mondiale, l'invention du transistor, puis du circuit intégré permettront de remplacer les relais électromécaniques et les tubes à vide, qui équipent les machines à calculs pour les rendre à la fois plus petites, plus complexes, plus économiques et plus fiables. Le capital-risque finance des dizaines de sociétés électroniques.Avec l'architecture de von Neumann, mise en application de la machine universelle de Turing, les ordinateurs dépassent la simple faculté de calculer et peuvent commencer à accepter des programmes plus évolués, de nature algorithmique.En 1961, Marion Créhange soutient une des premières thèses en informatique en France.Dans les années 1970, l'informatique se développe avec les télécommunications, avec Arpanet, le réseau Cyclades et la Distributed System Architecture (DSA) de réseau en couches, qui donnera naissance en 1978 au modèle OSI, appelé aussi « OSI-DSA », puis aux protocoles TCP-IP dans les années 1990, grâce à la baisse des prix des microprocesseurs. Les concepts de datagramme et d'informatique distribuée, d'abord jugés risqués, s'imposeront grâce à l'Internet.La série de livres The Art of Computer Programming de Donald Knuth, publiée à partir des années 1960, fait ressortir les aspects mathématiques de la programmation informatique. Edsger Dijkstra, Niklaus Wirth et Christopher Strachey travaillent et publient vers un même axe. Ces travaux préfigurent d'importants développements en matière de langage de programmation.L'amélioration de l'expressivité des langages de programmation a permis la mise en œuvre d'algorithmes toujours plus sophistiqués, appliqués à des données de plus en plus variées. La miniaturisation des composants et la réduction des coûts de production, associées à une augmentation de la demande en traitements des informations de toutes sortes (scientifiques, financières, commerciales, etc.), ont eu pour conséquence une diffusion de l'informatique dans tous les secteurs économiques, ainsi que dans la vie quotidienne des individus.Dans les années 1970, Xerox fait réaliser des études en psychologie cognitive et en ergonomie en vue de simplifier l'utilisation des outils informatiques. L'interface graphique propose un accès à la machine plus proche des objets ordinaires que l'interface en ligne de commande existant jusque-là. Les constructeurs souhaitant concurrencer le géant IBM promeuvent une informatique plus décentralisée.La démocratisation de l'utilisation d'Internet – réseau basé sur ARPANET – depuis 1995, a amené les outils informatiques à être de plus en plus utilisés dans une logique de réseau comme moyen de télécommunication, à la place des outils tels que la poste ou le téléphone. Elle s'est poursuivie avec l'apparition des logiciels libres, puis des réseaux sociaux et des outils de travail collaboratif dont Wikipédia n'est qu'un des nombreux exemples.Face à la demande pour numériser photos et musiques, les capacités de stockage, de traitement et de partage des données explosent et les sociétés qui ont parié sur la croissance la plus forte l'emportent le plus souvent, en profitant d'une énorme bulle spéculative sur les sociétés d'informatique.En France, l'informatique n'a commencé à se développer que dans les années 1960, avec le Plan Calcul. Depuis lors, les gouvernements successifs ont mené des politiques diverses en faveur de la recherche scientifique, l'enseignement, la tutelle des télécommunications, la nationalisation d'entreprises clés.La science informatique est une science formelle, dont l'objet d'étude est le calcul au sens large, c'est-à-dire, non pas exclusivement arithmétique, mais en rapport avec tout type d'information que l'on peut représenter par une suite de nombres. Ainsi, textes, séquences d'ADN, images, sons ou formules logiques peuvent faire l'objet de calculs. Selon le contexte, on parle d'un calcul, d'un algorithme, d'un programme, d'une procédure.Un algorithme est une manière systématique de procéder pour arriver à calculer un résultat.Un des exemples classiques est l'algorithme d'Euclide du calcul du « Plus grand commun diviseur » (PGCD) qui remonte au moins à 300 av. J.-C., mais il s'agit déjà d'un calcul complexe. Avant cela, le simple fait d'utiliser un abaque demande d'avoir réfléchi à un moyen systématique (et correct) d'utiliser cet outil pour réaliser des opérations arithmétiques.Des algorithmes existent donc depuis l'Antiquité, mais ce n'est que depuis les années 1930, avec les débuts de la théorie de la calculabilité, que les scientifiques se sont posés les questions « qu'est-ce qu'un modèle de calcul ? », « est-ce que tout est calculable ? » et ont tenté d'y répondre formellement.Il existe de nombreux modèles de calcul, dont les deux principaux sont la « machine de Turing » et le « lambda-calcul ». Ces deux systèmes formels définissent des objets qui peuvent représenter ce qu'on appelle des procédures de calcul, des algorithmes ou des programmes. Ils définissent ensuite un moyen systématique d'appliquer ces procédures, c'est-à-dire de calculer.Le résultat le plus important de la calculabilité est probablement le fait que les principaux modèles de calcul ont exactement la même puissance, c'est-à-dire qu'il n'existe pas de procédure que l'on pourrait exprimer dans un modèle mais pas dans un autre. La thèse de Church postule que ces modèles de calcul équivalents décrivent complètement et mathématiquement tout ce qui est physiquement calculable.Un deuxième résultat fondamental est l'existence de fonctions incalculables, une fonction étant ce que calcule une procédure ou un algorithme (ceux-ci désignant plutôt comment faire le calcul). On peut montrer qu'il existe des fonctions, bien définies, pour lesquelles il n'existe pas de procédure pour les calculer. L'exemple le plus connu étant probablement le problème de l'arrêt, qui montre qu'il n'existe pas de machine de Turing calculant si une autre machine de Turing donnée s'arrêtera (et donc donnera un résultat) ou non.Tous les modèles de calcul étant équivalents, ce résultat s'applique aussi aux autres modèles, ce qui inclut les programmes et logiciels que l'on peut trouver dans les ordinateurs courants. Il existe un lien très fort entre les fonctions que l'on ne peut pas calculer et les problèmes que l'on ne peut pas décider (voir Décidabilité).L'algorithmique est l'étude comparative des différents algorithmes. Tous les algorithmes ne se valent pas : le nombre d'opérations nécessaires pour arriver à un même résultat diffère d'un algorithme à l'autre. Ce nombre d'opérations, appelé la complexité algorithmique est le sujet de la théorie de la complexité des algorithmes, qui constitue une préoccupation essentielle en algorithmique.La complexité algorithmique sert en particulier à déterminer comment le nombre d'opérations nécessaires évolue en fonction du nombre d'éléments à traiter (la taille des données) :soit l'évolution peut être indépendante de la taille des données, on parle alors de complexité constante ;soit le nombre d'opérations peut augmenter selon un rapport logarithmique, linéaire, polynomial ou exponentiel (dans l'ordre décroissant d'efficacité et pour ne citer que les plus répandues) ;une augmentation exponentielle de la complexité aboutit très rapidement à des durées de calcul déraisonnables pour une utilisation en pratique ;tandis que pour une complexité polynomiale (ou meilleure), le résultat sera obtenu après une durée de calcul réduite, même avec de grandes quantités de données.Nous arrivons maintenant à un problème ouvert fondamental en informatique : « P est-il égal à NP ? ». En simplifiant beaucoup : P est « l'ensemble des problèmes pour lesquels on connaît un algorithme efficace » et NP « l'ensemble des problèmes pour lesquels on connaît un algorithme efficace pour vérifier une solution à ce problème ». Et en simplifiant encore plus : existe-t-il des problèmes difficiles ? Des problèmes pour lesquels il n'existe pas d'algorithme efficace ?Cette question est non seulement d'un grand intérêt théorique mais aussi pratique. En effet, un grand nombre de problématiques courantes et utiles sont des problèmes que l'on ne sait pas résoudre de manière efficace. C'est d'ailleurs un des problèmes du prix du millénaire et le Clay Mathematics Institute s'est engagé à verser un million de dollars aux personnes qui en trouveraient la solution.C'est un problème ouvert, donc formellement, il n'y a pas de réponse reconnue. Mais, en pratique, la plupart des spécialistes[réf. nécessaire] s'accordent pour penser que P?NP, c'est-à-dire qu'il existe effectivement des problèmes difficiles qui n'admettent pas d'algorithme efficace.Ce type de problème de complexité algorithmique est directement utilisé en cryptologie. En effet, les méthodes de cryptologie modernes reposent sur l'existence d'une fonction facile à calculer qui possède une fonction réciproque difficile à calculer. C'est ce qui permet de chiffrer un message qui sera difficile à décrypter (sans la clé).La plupart des chiffrements (méthode de cryptographie) reposent sur le fait que la procédure de décomposition en produit de facteurs premiers n'a pas d'algorithme efficace connu. Si quelqu'un trouvait un tel algorithme, il serait capable de décrypter la plupart des cryptogrammes facilement. On sait d'ailleurs qu'un calculateur quantique en serait capable, mais ce genre d'ordinateur n'existe pas, en tout cas pour le moment.Depuis les années 1960, et à la frontière avec la logique mathématique : la correspondance de Curry-Howard a jeté un pont entre le monde des démonstrations formelles et celui des programmes, dans la discipline des méthodes formelles.Citons aussi l'étude de la mécanisation des procédés de calcul et de pensée qui a permis de mieux comprendre la réflexion humaine, et apporté des éclairages en psychologie cognitive et en linguistique, par exemple, à travers la discipline du traitement automatique du langage naturel,.Le terme technologies de l'information et de la communication désigne un secteur d'activité et un ensemble de biens qui sont des applications pratiques des connaissances scientifiques en informatique ainsi qu'en électronique numérique, en télécommunication, en sciences de l'information et de la communication et en cryptologie.Le matériel informatique est un ensemble d'équipements (pièces détachées) servant au traitement des informations.Un logiciel contient des suites d'instructions qui décrivent en détail les algorithmes des opérations de traitement d'information ainsi que les informations relatives à ce traitement (valeurs clés, textes, images, etc.).Les appareils en électronique numérique utilisent tous un système logique. Les entrées et sorties des composants électroniques n'ont que deux états ; l'un correspondant à vrai, l'autre à faux. On démontre qu'en assimilant vrai au nombre 1 et faux au nombre 0, on peut établir les règles logiques qui fondent un système de numération binaire. Les appareils représentent toute l'information sous cette forme.Les appareils informatiques se décomposent en quatre ensembles qui servent respectivement à entrer des données, les stocker, les traiter, puis les faire ressortir de l'appareil, selon les principes de la machine de Turing et l'architecture de von Neumann. Les données circulent entre les pièces des différentes unités par des lignes de communication, les bus. Le processeur est la pièce centrale qui anime l'appareil en suivant les instructions des programmes qui sont enregistrés à l'intérieur.Il existe aujourd'hui une gamme étendue d'appareils capables de traiter automatiquement des informations. De ces appareils, l'ordinateur est le plus connu, le plus ouvert, le plus complexe et un des plus anciens. L'ordinateur est une machine modulable et universelle qui peut être adaptée à de nombreuses tâches par ajout de matériel ou de logiciel.Un système embarqué est un appareil équipé de matériel et de logiciel informatique, et affecté à une tâche bien précise.Exemples d'appareils :la console de jeu est un appareil destiné au jeu vidéo, une activité que l'on peut aussi exercer avec un ordinateur ;le NAS (acronyme de l'anglais network attached storage, littéralement « mémoire attachée à un réseau ») est un appareil destiné à garder des informations en mémoire et à les mettre à disposition via un réseau informatique ;le distributeur de billets : un automate qui distribue sur demande des billets de banque ou des tickets de transport public ; les distributeurs sont souvent des ordinateurs effectuant un nombre limité de tâches ;le récepteur satellite tout comme le décodeur de Télévision Numérique Terrestre : les émissions de télévision se font en numérique et sont captées et décodées par des appareils informatiques ;les appareils d'avionique sont des appareils électroniques et informatiques placés dans les avions et les véhicules spatiaux ; ils servent à la navigation, la prévention des collisions et la télécommunication ;le GPS : un appareil qui affiche une carte géographique, et se positionne sur la carte grâce à un réseau de satellites ; les cartes géographiques sont des informations créées par ordinateur ;le téléphone mobile : initialement c'est un simple appareil analogique utilisable par un nombre restreint d'utilisateurs, le téléphone portable numérisé est utilisable en masse et sert aussi à jouer, à visionner des images ou des vidéos ;Les smartphones sont de véritables ordinateurs de poche, intégrant de nombreux capteurs (positionnement GPS, accéléromètres multi-axes, Capteur photographique, thermomètre, hygromètre), regroupant ainsi plusieurs appareils différents dans un même boîtier ;les systèmes d'arme sont des dispositifs informatiques qui permettent l'organisation et le suivi des opérations militaires : positionnement géographique, calcul des tirs, guidage des appareils et des véhicules ;les robots sont des appareils électromécaniques qui effectuent, de manière autonome, des tâches pour assister ou remplacer des humains ; l'autonomie est assurée par un appareil informatique placé à l'intérieur et/ou à l'extérieur du robot.L'ensemble des composants électroniques, nécessaires au fonctionnement des appareils numériques, est appelé « en anglais hardware ». Dans un boîtier se trouvent les pièces centrales, par exemple, le processeur et des pièces périphériques servant à l'acquisition, au stockage, à la restitution et la transmission d'informations. L'appareil est un assemblage de pièces qui peuvent être de différentes marques. Le respect des normes industrielles par les différents fabricants assure le fonctionnement de l'ensemble. Carte mère La carte mère est un circuit imprimé avec de nombreux composants et ports de connexion constituant le support principal des éléments essentiels d'un ordinateur (Supports des microprocesseur, mémoires, connecteurs divers et autres ports d'entrée-sortie). Boîtier et périphériques L'intérieur du boîtier d'un appareil informatique contient un ou plusieurs circuits imprimés sur lesquels sont soudés des composants électroniques et des connecteurs. La carte mère est le circuit imprimé central, sur lequel sont connectés tous les autres équipements.Un bus est un ensemble de lignes de communication qui servent aux échanges d'information entre les composants de l'appareil informatique. Les informations sont transmises sous forme de signaux électriques. Le plus petit élément d'information manipulable en informatique correspond à un bit. Les bus transfèrent des bytes d’informations composés de plusieurs bits en parallèle.Les périphériques sont par définition, les équipements situés à l'extérieur du boîtier. Équipements d'entrée Les périphériques d'entrée servent à commander l'appareil informatique ou à y envoyer des informations.L'envoi des informations se fait par le procédé de numérisation. Il s'agit de transformer des informations brutes (une page d'un livre, les listes des éléments périodiques, etc.) en suite de nombres binaires pouvant être manipulées par un appareil informatique. La transformation est faite par un circuit électronique. La construction du circuit diffère en fonction de la nature de l'information à numériser.L'ensemble des dispositifs de commande et les périphériques de sortie directement associés forment une façade de commande appelée interface homme-machine. Stockage d'information Une mémoire est un dispositif électronique (circuit intégré) ou électromécanique destiné à conserver des informations dans un appareil informatique.Une mémoire de masse : dispositif de stockage de grande capacité, souvent électromagnétique (bandes magnétiques, disques durs), destiné à conserver longtemps une grande quantité d'informations.Un disque dur : mémoire de masse à accès direct, de grande capacité, composée d'un ou de plusieurs disques rigides superposés et magnétiques. L'IBM Ramac 305, le premier disque dur, a été dévoilé en 1956. Le disque dur est une des mémoires de masse les plus utilisées en informatique. Pour gérer de grandes volumétries, ces disques sont associés par des mécanismes logiciels permettant d'étendre leur capacité (jusqu'à plusieurs Po) et d'y intégrer une protection avancée (RAID et Réplication au niveau bloc. Réplication, Versioning et Snapshot au niveau fichier).Une mémoire morte (« Read Only Memory » en anglais, ou ROM) : mémoire composée de circuits intégrés où les informations ne peuvent pas être modifiées. Ce type de mémoire est toujours installé par le constructeur et utilisé pour conserver définitivement des logiciels embarqués.Une mémoire vive : mémoire composée de circuits intégrés où les informations peuvent être modifiées. Les informations non enregistrées sont souvent perdues à la mise hors tension. Processeur Le processeur est le ou les composants électroniques qui exécute des instructions (calcul, choix, gestion des taches). Un appareil informatique contient au moins un microprocesseur, voire deux, quatre, ou plus. Les ordinateurs géants contiennent des milliers de processeurs.L'acronyme CPU (pour l'anglais Central Processing Unit) désigne le ou les processeurs centraux de l'appareil. L'exécution des instructions par le ou les CPU influence tout le déroulement des traitements.Un microprocesseur multi-cœur réunit plusieurs circuits intégrés de processeur dans un seul boîtier. Un composant électronique construit de cette manière effectue le même travail que plusieurs processeurs. Équipements de sortie Les équipements de sortie servent à présenter les informations provenant d'un appareil informatique sous une forme reconnaissable par un humain.Un convertisseur numérique-analogique (en anglais Digital to Analog Converter ou DAC) est un composant électronique qui transforme une information numérique (une suite de nombres généralement en binaire) en un signal électrique analogique. Il effectue le travail inverse de la numérisation (exemple : un lecteur de CD audio).Un écran est une surface sur laquelle s'affiche une image (exemple : des fenêtres de dialogue et des documents). Les images à afficher sont générées par un circuit électronique convertisseur numérique-analogique en sortie des cartes vidéos pour l'affichage sur les écrans analogiques. De plus en plus souvent, l'étape du DAC est supprimée grâce à la connexion HDMI avec les écrans interprétant directement les images numériques.Un moniteur est un écran utilisant les mêmes techniques que celles utilisées par les téléviseurs, qui affiche des graphiques et des textes provenant de l'appareil informatique.Une imprimante est un équipement servant à produire des informations non volatiles, sous forme d'impression sur papier. Il peut s'agir de textes, de tableaux, de graphiques, de schémas, de photos, etc.Un haut-parleur ou un « jack » : on peut brancher un casque, un système d'enceintes amplifiées, ou tout système audio, afin de reproduire les sons dans le spectre audible par les humains, fabriqués ou passant par la carte son. Cette dernière utilisant aussi un DAC mais aussi ADC, permettant de numériser les signaux analogiques provenant de microphones ou de tout appareil électronique de reproduction sonore que l'on connecte au connecteur mic ou line. Équipements de réseau Les équipements de réseau servent à la communication d'informations entre des appareils informatiques, en particulier, à l'envoi d'informations, à la réception, à la retransmission, et au filtrage. Les communications peuvent se faire par câble, par onde radio, par satellite, ou par fibre optique.Un protocole de communication est une norme industrielle relative à la communication d'informations. La norme établit autant le point de vue électronique (tensions, fréquences) que le point de vue informationnel (choix des informations, format), ainsi que le déroulement des opérations de communication (qui initie la communication, comment réagit le correspondant, combien de temps dure la communication, etc.). Selon le modèle OSI – qui comporte sept niveaux –, une norme industrielle (en particulier un protocole de communication) d'un niveau donné, peut être combinée avec n'importe quelle norme industrielle d'une couche située en dessus ou en dessous.Une carte réseau est un circuit imprimé qui sert à recevoir et envoyer des informations conformément à un ou plusieurs protocoles.Un modem est un équipement qui sert à envoyer des informations sous forme d'un signal électrique modulé, ce qui permet de les faire passer sur une ligne de communication analogique telle une ligne téléphonique.Un logiciel est un ensemble d'informations relatives à un traitement automatisé, qui correspond à la « procédure » d'une Machine de Turing. La mécanique de cette machine correspondant au processeur. Le logiciel peut être composé d'instructions et de données. Les instructions mettent en application les algorithmes en rapport avec le traitement d'information voulu. Les données incluses dans un logiciel sont les informations relatives à ce traitement ou exigées par lui (valeurs clés, textes, images, etc.).Le logiciel peut prendre une forme exécutable (c'est-à-dire, directement compréhensible par le micro-processeur) ou source, c'est-à-dire que la représentation est composée d'une suite d'instructions directement compréhensible par un individu. Ainsi donc, on peut considérer le logiciel comme une abstraction qui peut prendre une multitude de formes : il peut être imprimé sur du papier, conservé sous forme de fichiers informatiques ou encore stocké dans une mémoire (une disquette, une clé USB).Un appareil informatique peut contenir de très nombreux logiciels, organisés en trois caté"
informatique;"L'intelligence artificielle (IA) est un « ensemble de théories et de techniques mises en œuvre en vue de réaliser des machines capables de simuler l'intelligence humaine ».Elle englobe donc un ensemble de concepts et de technologies, plus qu'une discipline autonome constituée. Des instances, telle la CNIL, notant le peu de précision de la définition de l'IA, l'ont présentée comme « le grand mythe de notre temps ».Souvent classée dans le groupe des mathématiques et des sciences cognitives, elle fait appel à la neurobiologie computationnelle (particulièrement aux réseaux neuronaux) et à la logique mathématique (partie des mathématiques et de la philosophie). Elle utilise des méthodes de résolution de problèmes à forte complexité logique ou algorithmique. Par extension, elle comprend, dans le langage courant, les dispositifs imitant ou remplaçant l'homme dans certaines mises en œuvre de ses fonctions cognitives.Ses finalités et enjeux ainsi que son développement suscitent, depuis l'apparition du concept, de nombreuses interprétations, fantasmes ou inquiétudes s'exprimant tant dans les récits ou films de science-fiction que dans les essais philosophiques. Si des outils relevant d'intelligences artificielles spécialisées ont fait leurs preuves, la réalité semble encore tenir l'intelligence artificielle généraliste loin des performances du vivant ; ainsi, l'IA reste encore bien inférieure au chat dans toutes ses aptitudes naturelles.Le terme « intelligence artificielle », créé par John McCarthy, est souvent abrégé par le sigle « IA » (ou « AI » en anglais, pour Artificial Intelligence). Il est défini par l’un de ses créateurs, Marvin Lee Minsky, comme « la construction de programmes informatiques qui s’adonnent à des tâches qui sont, pour l’instant, accomplies de façon plus satisfaisante par des êtres humains car elles demandent des processus mentaux de haut niveau tels que : l’apprentissage perceptuel, l’organisation de la mémoire et le raisonnement critique »,. On y trouve donc le côté « artificiel » atteint par l'usage des ordinateurs ou de processus électroniques élaborés et le côté « intelligence » associé à son but d'imiter le comportement. Cette imitation peut se faire dans le raisonnement, par exemple dans les jeux ou la pratique des mathématiques, dans la compréhension des langues naturelles, dans la perception : visuelle (interprétation des images et des scènes), auditive (compréhension du langage parlé) ou par d'autres capteurs, dans la commande d'un robot dans un milieu inconnu ou hostile.Même si elles respectent globalement la définition de Minsky, certaines définitions de l'IA varient sur deux points fondamentaux :les définitions qui lient l'IA à un aspect humain de l'intelligence, et celles qui la lient à un modèle idéal d'intelligence, non forcément humaine, nommée rationalité ;les définitions qui insistent sur le fait que l'IA a pour but d'avoir toutes les apparences de l'intelligence (humaine ou rationnelle), et celles qui insistent sur le fait que le fonctionnement interne du système d'IA doit ressembler également à celui de l'être humain et être au moins aussi rationnel.Historiquement, l'idée d'intelligence artificielle semble émerger dans les années 1950 quand Alan Turing se demande si une machine peut « penser ». Dans l'article « Computing Machinery and Intelligence » (Mind, octobre 1950), Turing explore ce problème et propose une expérience (maintenant dite test de Turing) visant à trouver à partir de quand une machine deviendrait « consciente ». Il développe ensuite cette idée dans plusieurs forums, dans la conférence « L'intelligence de la machine, une idée hérétique », dans la conférence qu'il donne à la BBC 3e programme le 15 mai 1951 « Les calculateurs numériques peuvent-ils penser ? » ou la discussion avec M.H.A. Newman, Sir Geoffrey Jefferson et R.B. Braithwaite les 14 et 23 janvier 1952 sur le thème « Les ordinateurs peuvent-ils penser ? ».Une autre origine probable est la publication, en 1949, par Warren Weaver d'un mémorandum sur la traduction automatique des langues qui suggère qu'une machine puisse faire une tâche qui relève typiquement de l'intelligence humaine.Le développement des techniques informatiques (augmentation de la puissance de calcul) aboutit ensuite à plusieurs avancées :dans les années 1980, l'apprentissage automatique se développe, notamment avec la renaissance du connexionnisme. L'ordinateur commence à déduire des « règles à suivre » en analysant seulement des données, ;parallèlement, des algorithmes « apprenants » sont créés qui préfigurent les futurs réseaux de neurones (l'apprentissage par renforcement, les machines à vecteurs de support, etc.). Ceci permet par exemple en mai 1997 à l’ordinateur Deep Blue de battre Garry Kasparov au jeu d'échecs lors d'un match revanche de six parties ;l'intelligence artificielle devient un domaine de recherche international, marquée par une conférence au Dartmouth College à l’été 1956, à laquelle assistaient ceux qui vont marquer la discipline ;depuis les années 1960, la recherche se fait principalement aux États-Unis, notamment à l'université Stanford sous l'impulsion de John McCarthy, au MIT sous celle de Marvin Minsky, à l'université Carnegie-Mellon sous celle de Allen Newell et Herbert Simon et à l'université d'Édimbourg sous celle de Donald Michie, en Europe et en Chine, ainsi qu'au Japon avec le projet « ordinateurs de cinquième génération (en) » du gouvernement. En France, l'un des pionniers est Jacques Pitrat ;dans les années 2000, le Web 2.0, le big data et de nouvelles puissances et infrastructures de calcul permettent à certains ordinateurs d'explorer des masses de données sans précédent ; c'est l'apprentissage profond (« deep learning »), dont l'un des pionniers est le français Yann Le Cun.Les bornes de ce domaine varient, ainsi optimiser un itinéraire était considéré comme un problème d'intelligence artificielle dans les années 1950 et n'est plus considéré aujourd’hui que comme un simple problème d'algorithmie.Vers 2015, le secteur de l'intelligence artificielle cherche à relever quatre défis : la perception visuelle, la compréhension du langage naturel écrit ou parlé, l'analyse automatique du langage et la prise de décision autonome. Produire et organiser des données nombreuses et de qualité, c'est-à-dire corrélées, complètes, qualifiées (sourcées, datées, géoréférencées…), historisées est un autre enjeu. La capacité déductive et de généralisation pertinente d'un ordinateur, à partir de peu de données ou d'un faible nombre d'évènements, est un autre objectif, plus lointain.Entre 2010 et 2016, les investissements auraient été décuplés, atteignant une dizaine de milliards de dollars en 2016.Si les progrès de l’intelligence artificielle sont récents, ce thème de réflexion est tout à fait ancien, et il apparaît régulièrement au cours de l’histoire. Les premiers signes d’intérêt pour une intelligence artificielle et les principaux précurseurs de cette discipline sont les suivants. Automates Une des plus anciennes traces du thème de « l’homme dans la machine » date de 800 avant notre ère, en Égypte. La statue du dieu Amon levait le bras pour désigner le nouveau pharaon parmi les prétendants qui défilaient devant lui, puis elle « prononçait » un discours de consécration. Les Égyptiens étaient probablement conscients de la présence d’un prêtre actionnant un mécanisme et déclarant les paroles sacrées derrière la statue, mais cela ne semblait pas être pour eux contradictoire avec l’incarnation de la divinité. Vers la même époque, Homère, dans L'Iliade (XVIII, 370–421), décrit les automates réalisés par le dieu forgeron Héphaïstos : des trépieds munis de roues en or, capables de porter des objets jusqu’à l’Olympe et de revenir seuls dans la demeure du dieu ; ou encore, deux servantes forgées en or qui l’assistent dans sa tâche. De même, le Géant de bronze Talos, gardien des rivages de la Crète, était parfois considéré comme une œuvre du dieu.Vitruve, architecte romain, décrit l’existence entre le IIIe et le Ier siècle avant notre ère, d’une école d’ingénieurs fondée par Ctesibius à Alexandrie, et concevant des mécanismes destinés à l’amusement tels des corbeaux qui chantaient. Héron L'Ancien décrit dans son traité « Automates », un carrousel animé grâce à la vapeur et considéré comme anticipant les machines à vapeur. Les automates disparaissent ensuite jusqu’à la fin du Moyen Âge. On a prêté à Roger Bacon la conception d'automates doués de la parole; en fait, probablement de mécanismes simulant la prononciation de certains mots simples.Léonard de Vinci a construit en 1515 un automate en forme de lion pour amuser le roi de France, François I. Gio Battista Aleotti et Salomon de Caus, eux, ont construit des oiseaux artificiels et chantants, des flûtistes mécaniques, des nymphes, des dragons et des satyres animés pour égayer des fêtes aristocratiques, des jardins et des grottes. René Descartes, lui, aurait conçu en 1649 un automate qu’il appelait « ma fille Francine ». Il conduit par ailleurs une réflexion d’un modernisme étonnant sur les différences entre la nature des automates, et celles d’une part des animaux (pas de différence) et d’autre part celle des hommes (pas d’assimilation). Ces analyses en font le précurseur méconnu d’un des principaux thèmes de la science-fiction : l'indistinction entre le vivant et l’artificiel, entre les hommes et les robots, les androïdes ou les intelligences artificielles.Jacques de Vaucanson a construit en 1738 un « canard artificiel de cuivre doré, qui boit, mange, cancane, barbote et digère comme un vrai canard ». Il était possible de programmer les mouvements de cet automate, grâce à des pignons placés sur un cylindre gravé, qui contrôlaient des baguettes traversant les pattes du canard. L’automate a été exposé pendant plusieurs années en France, en Italie et en Angleterre, et la transparence de l’abdomen permettait d’observer le mécanisme interne. Le dispositif permettant de simuler la digestion et d’expulser une sorte de bouillie verte fait l’objet d’une controverse. Certains commentateurs estiment que cette bouillie verte n’était pas fabriquée à partir des aliments ingérés, mais préparée à l’avance. D’autres estiment que cet avis n’est fondé que sur des imitations du canard de Vaucanson. L’incendie du musée de Nijni Novgorod en Russie, vers 1879, a détruit cet automate.Les artisans Pierre et Louis Jaquet-Droz fabriquèrent parmi les meilleurs automates fondés sur un système purement mécanique, avant le développement des dispositifs électromécaniques. Certains de ces automates, par un système de cames multiples, étaient capables d'écrire un petit billet (toujours le même). Enfin, Les Contes d'Hoffmann (et ballet) L'Homme au sable décrit une poupée mécanique dont s'éprend le héros. Pensée automatique Une des premières tentatives de formalisation de la pensée connue est le zairja, mécanisme qu'utilisaient les astrologues arabe pour générer des idées supposées logiques, dont l'invention est attribuée à Abu al-Abbas as-Sabti au XIIe siècle. Raymond Lulle s'en est probablement inspiré pour mettre au point son Ars Magna. Missionnaire, philosophe, et théologien espagnol du XIIIe siècle, il essaya lui aussi de générer des idées grâce à un système mécanique. Il combinait aléatoirement des concepts grâce à une sorte de règle à calcul, sur laquelle pivotaient des disques concentriques gravés de lettres et de symboles philosophiques. Il fondait sa méthode sur l’identification de concepts de base, puis leur combinaison mécanique soit entre eux, soit avec des idées connexes. Raymond Lulle l'appliqua à la métaphysique, puis à la morale, à la médecine et à l’astrologie. Mais il n’utilisait que la logique déductive, ce qui ne permettait pas à son système d’acquérir un apprentissage, ni davantage de remettre en cause ses principes de départ : seule la logique inductive le permet.Gottfried Wilhelm Leibniz, au XVIIe siècle, a imaginé un calcul pensant (calculus rationator), en assignant un nombre à chaque concept. La manipulation de ces nombres aurait permis de résoudre les questions les plus difficiles, et même d’aboutir à un langage universel. Leibniz a toutefois démontré que l’une des principales difficultés de cette méthode, également rencontrée dans les travaux modernes sur l’intelligence artificielle, est l’interconnexion de tous les concepts, ce qui ne permet pas d’isoler une idée de toutes les autres pour simplifier les problèmes liés à la pensée.George Boole a inventé la formulation mathématique des processus fondamentaux du raisonnement, connue sous le nom d’algèbre de Boole. Il était conscient des liens de ses travaux avec les mécanismes de l’intelligence, comme le montre le titre de son principal ouvrage paru en 1854 : Les Lois de la pensée (The laws of thought), sur l’algèbre booléenne.Gottlob Frege perfectionna le système de Boole en formalisant le concept de prédicat, qui est une entité logique soit vraie, soit fausse (toute maison a un propriétaire), mais contenant des variables non logiques, n’ayant en soi aucun degré de vérité (maison, propriétaire). Cette formalisation eut une grande importance puisqu'elle permit de démontrer des théorèmes généraux, simplement en appliquant des règles typographiques à des ensembles de symboles. La réflexion en langage courant ne portait plus que sur le choix des règles à appliquer. Par ailleurs, l’utilisateur joue un rôle important puisqu'il connaît le sens des symboles qu’il a inventés et ce sens n'est pas toujours formalisé, ce qui ramène au problème de la signification en intelligence artificielle, et de la subjectivité des utilisateurs.Bertrand Russell et Alfred North Whitehead publièrent au début du XXe siècle un ouvrage intitulé Principia Mathematica, dans lequel ils résolvent des contradictions internes à la théorie de Gottlob Frege. Ces travaux laissaient espérer d’aboutir à une formalisation complète des mathématiques.Kurt Gödel démontre au contraire que les mathématiques resteront une construction ouverte, en publiant en 1931 un article intitulé « Des propositions formellement indécidables contenues dans les Principia mathematica et autres systèmes similaires ». Sa démonstration est qu’à partir d’une certaine complexité d’un système, on peut y créer plus de propositions logiques qu’on ne peut en démontrer vraies ou fausses. L’arithmétique, par exemple, ne peut trancher par ses axiomes si on doit accepter des nombres dont le carré soit -1. Ce choix reste arbitraire et n’est en rien lié aux axiomes de base. Le travail de Gödel suggère qu’on pourra créer ainsi un nombre arbitraire de nouveaux axiomes, compatibles avec les précédents, au fur et à mesure qu’on en aura besoin. Si l'arithmétique est démontrée incomplète, le calcul des prédicats (logique formelle) est au contraire démontré par Gödel comme complet.Alan Turing invente des machines abstraites et universelles (rebaptisées les machines de Turing), dont les ordinateurs modernes sont considérés comme des concrétisations. Il démontre l’existence de calculs qu’aucune machine ne peut faire (un humain pas davantage, dans les cas qu'il cite), sans pour autant que cela constitue pour Turing un motif pour douter de la faisabilité de machines pensantes répondant aux critères du test de Turing.Irving John Good, Myron Tribus et E.T. Jaynes ont décrit de façon très claire les principes assez simples d’un robot à logique inductive utilisant les principes de l’inférence bayésienne pour enrichir sa base de connaissances sur la base du Théorème de Cox-Jaynes. Ils n’ont malheureusement pas traité la question de la façon dont on pourrait stocker ces connaissances sans que le mode de stockage entraîne un biais cognitif. Le projet est voisin de celui de Raymond Lulle, mais fondé cette fois-ci sur une logique inductive, et donc propre à résoudre quelques problèmes ouverts.Des chercheurs comme Alonzo Church ont posé des limites pratiques aux ambitions de la raison, en orientant la recherche (Herbert Simon, Michael Rabin, Stephen Cook) vers l’obtention des solutions en temps fini, ou avec des ressources limitées, ainsi que vers la catégorisation des problèmes selon des classes de difficulté (en rapport avec les travaux de Cantor sur l’infini)[réf. souhaitée].L'intelligence artificielle est un sujet d'actualité au XXIe siècle. En 2004, l'Institut Singularity a lancé une campagne Internet appelée « Trois lois dangereuses » : « Three Laws Unsafe » (en lien avec les trois lois d'Asimov) pour sensibiliser aux questions de la problématique de l'intelligence artificielle et l'insuffisance des lois d'Asimov en particulier. (Singularity Institute for Artificial Intelligence 2004).En 2005, le projet Blue Brain est lancé, il vise à simuler le cerveau des mammifères. Il s'agit d'une des méthodes envisagées pour réaliser une IA. Ils annoncent de plus comme objectif de fabriquer, dans dix ans, le premier « vrai » cerveau électronique. En mars 2007, le gouvernement sud-coréen annonce que plus tard dans l'année, il émettrait une charte sur l'éthique des robots, afin de fixer des normes pour les utilisateurs et les fabricants. Selon Park Hye-Young, du ministère de l'Information et de la communication, la Charte reflète les trois lois d'Asimov : la tentative de définition des règles de base pour le développement futur de la robotique. En juillet 2009, en Californie une conférence organisée par l'Association for the Advancement of Artificial Intelligence (AAAI), où un groupe d'informaticiens se demande s'il devrait y avoir des limites sur la recherche qui pourrait conduire à la perte de l'emprise humaine sur les systèmes informatiques, et où il est également question de l'explosion de l'intelligence (artificielle) et du danger de la singularité technologique conduisant à un changement d'ère, ou de paradigme totalement en dehors du contrôle humain,.En 2009, le Massachusetts Institute of Technology (MIT) a lancé un projet visant à repenser la recherche en intelligence artificielle. Il réunira des scientifiques qui ont eu du succès dans des domaines distincts de l'IA. Neil Gershenfeld déclare « Nous voulons essentiellement revenir 30 ans en arrière, et de revoir quelques directions aujourd'hui gelées ».En novembre 2009, l'US Air Force cherche à acquérir 2 200 PlayStation 3[réf. obsolète] pour utiliser le processeur cell à 7 ou 8 cœurs qu'elle contient dans le but d'augmenter les capacités de leur superordinateur constitué de 336 PlayStation 3 (total théorique 52,8 petaFLOPS en double précision). Le nombre sera réduit à 1 700 unités le 22 décembre 2009. Le projet vise le traitement vidéo haute-définition, et l'« informatique neuromorphique », ou la création de calculateurs avec des propriétés/fonctions similaires au cerveau humain. Années 2010 Le 27 janvier 2010, l'US Air Force demande l'aide de l'industrie pour développer une intelligence avancée de collecte d'information et avec la capacité de décision rapide pour aider les forces américaines pour attaquer ses ennemis rapidement à leurs points les plus vulnérables. L'US Air Force utilisera une intelligence artificielle, le raisonnement ontologique, et les procédures informatique basées sur la connaissance, ainsi que d'autres traitements de données avancés afin de frapper l'ennemi au meilleur point. D'autre part, d’ici 2020, plus de mille bombardiers et chasseurs F-22 et F-35 de dernière génération, parmi plus de 2 500 avions militaires, commenceront à être équipés de sorte que, d’ici 2040, tous les avions de guerre américains soient pilotés par intelligence artificielle, en plus des 10 000 véhicules terrestres et des 7 000 dispositifs aériens commandés d'ores et déjà à distance.Le 16 février 2011, Watson, le superordinateur conçu par IBM, remporte deux des trois manches du jeu télévisé Jeopardy! en battant largement ses deux concurrents humains en gains cumulés. Pour cette IA, la performance a résidé dans le fait de répondre à des questions de culture générale (et non un domaine technique précis) dans des délais très courts. En février 2016, l'artiste et designer Aaron Siegel propose de faire de Watson un candidat à l'élection présidentielle américaine afin de lancer le débat sur « le potentiel de l’intelligence artificielle dans la politique ».En mai 2013, Google ouvre un laboratoire de recherches dans les locaux de la NASA. Grâce à un super calculateur quantique conçu par D-Wave Systems et qui serait d'après cette société 11 000 fois plus performant qu'un ordinateur actuel (de 2013), ils espèrent ainsi faire progresser l'intelligence artificielle, notamment l'apprentissage automatique. Raymond Kurzweil est engagé en décembre 2012 par Google afin de participer et d'améliorer l'apprentissage automatique des machines et des IA.Entre 2014 et 2015, à la suite du développement rapide du deep learning, et à l'encontre des penseurs transhumanistes, quelques scientifiques et membres de la communauté high tech craignent que l'intelligence artificielle ne vienne à terme dépasser les performances de l'intelligence humaine. Parmi eux, l'astrophysicien britannique Stephen Hawking, le fondateur de Microsoft Bill Gates et le PDG de Tesla Elon Musk.Les géants de l'Internet s'intéressent de plus en plus à l'IA. Le 3 janvier 2016, le patron de Facebook, Mark Zuckerberg, s’est donné pour objectif de l’année de « construire une intelligence artificielle simple pour piloter ma maison ou m’aider dans mon travail ». Il avait déjà créé en 2013 le laboratoire Facebook Artifical Intelligence Research (FAIR) dirigé par le chercheur français Yann Le Cun et ouvert un laboratoire de recherche permanente dans le domaine à Paris.Apple a de son côté récemment acquis plusieurs start-up du secteur (Perceptio, VocalIQ, Emotient et Turi).En janvier 2018, des modèles d'intelligence artificielle développés par Microsoft et Alibaba réussissent chacun de leur côté à battre les humains dans un test de lecture et de compréhension de l'université Stanford. Le traitement du langage naturel imite la compréhension humaine des mots et des phrases et permet maintenant aux modèles d'apprentissage automatique de traiter de grandes quantités d'informations avant de fournir des réponses précises aux questions qui leur sont posées.En février 2019, l'institut de recherche OpenAI annonce avoir créé un programme d’intelligence artificielle capable de générer des textes tellement réalistes que cette technologie pourrait être dangereuse,. Si le logiciel est utilisé avec une intention malveillante, il peut générer facilement des fausses nouvelles très crédibles. Inquiet par l'utilisation qui pourrait en être faite, OpenAI préfère ne pas rendre public le code source du programme.En France, les pionniers sont Alain Colmerauer, Gérard Huet, Jean-Louis Laurière, Claude-François Picard, Jacques Pitrat et Jean-Claude Simon. Un congrès national annuel, « Reconnaissance de formes et intelligence artificielle », est créé en 1979 à Toulouse. En lien avec l'organisation de la conférence International Joint Conference on Artificial Intelligence à Chambéry en 1993, et la création d'un GRECO-PRC « intelligence artificielle », en 1983, il donne naissance à une société savante, l'Association française pour l'intelligence artificielle (AFIA) en 1989, qui, entre autres, organise des conférences nationales en intelligence artificielle.Le 17 janvier 2017, le fonds de capital risque Serena Capital lance un fonds de 80 millions d’euros destiné à l’investissement dans les start-ups européennes du big data et de l'intelligence artificielle. Le 19 janvier 2017, une audition se tient au Sénat : « L'intelligence Artificielle menace-t-elle nos emplois ? ». Le 20 janvier 2017, Axelle Lemaire entend valoriser les potentiels scientifiques et industriels français grâce au projet « France IA ».En janvier 2017, dans le cadre de sa mission de réflexion sur les enjeux éthiques et les questions de société soulevés par les technologies numériques, la Commission nationale de l'informatique et des libertés (CNIL) annonce l'organisation d'un débat public sur les algorithmes et l'intelligence artificielle. Le 15 décembre 2017, à l'issue d'un débat ayant mobilisé 60 partenaires (institutions publiques, associations, entreprises, acteurs du monde de la recherche, société civile), elle publie son rapport « Comment permettre à l'Homme de garder la main ? » comprenant des recommandations pour la construction d'un modèle éthique d'intelligence artificielle.En septembre 2017, Cédric Villani, premier vice-président de l'Office parlementaire d'évaluation des choix scientifiques et technologiques (OPECST), est chargé de mener une consultation publique sur l'intelligence artificielle. Il rend son rapport le 28 mars 2018, à la veille d'une intervention du président de la République Emmanuel Macron au Collège de France pour annoncer la stratégie de la France dans ce domaine. Il y dévoile un plan de 1,5 milliard d'euros sur l'ensemble du quinquennat, ainsi qu'une évolution de la législation française pour permettre la mise en application de l'intelligence artificielle, en particulier concernant la circulation des véhicules autonomes. Parallèlement à ces annonces, il est interviewé par Wired, magazine de référence pour la communauté mondiale des nouvelles technologies, et y exprime sa vision de l'intelligence artificielle, à savoir que les algorithmes utilisés par l'État doivent être ouverts, que l'intelligence artificielle doit être encadrée par des règles philosophiques et éthiques et qu'il faut s'opposer à l'usage d'armes automatiques ou de dispositifs prenant des décisions sans consulter un humain,.En mars 2018, Microsoft France lance l'École IA Microsoft, inaugurée par son président Carlo Purassanta, une formation ouverte aux décrocheurs scolaires et aux personnes éloignées de l'emploi, en partenariat avec Simplon.co. Dix écoles sont lancées en un an à partir de septembre 2018. Microsoft France mise sur le développement de l'intelligence artificielle comme nouveau vecteur d'inclusion professionnelleEn octobre 2019, le site ActuIA annonce le lancement du premier magazine papier consacré à l'intelligence artificielle.Le concept d’intelligence artificielle forte fait référence à une machine capable non seulement de produire un comportement intelligent, notamment de modéliser des idées abstraites, mais aussi d’éprouver une impression d'une réelle conscience, de « vrais sentiments » (quoi qu’on puisse mettre derrière ces mots), et « une compréhension de ses propres raisonnements ».L’intelligence artificielle forte a servi de moteur à la discipline, mais a également suscité de nombreux débats [Lesquels ?].En partant du principe, étayé par les neurosciences, que la conscience a un support biologique et donc matériel, les scientifiques ne voient généralement pas d’obstacle théorique à la création d'une intelligence consciente sur un support matériel autre que biologique. Selon les tenants de l'IA forte, si à l'heure actuelle il n'y a pas d'ordinateurs ou d'algorithmes aussi intelligents que l'être humain, ce n'est pas un problème d'outil mais de conception. Il n'y aurait aucune limite fonctionnelle (un ordinateur est une machine de Turing universelle avec pour seules limites celles de la calculabilité), seulement des limites liées à l'aptitude humaine à concevoir les logiciels appropriés (programme, base de données…).Comparer la capacité de traitement de l'information d'un cerveau humain à celle d'un ordinateur peut aider à comprendre les ordres de grandeur pour estimer la possibilité pratique ou non d'une intelligence artificielle forte, de même qu'un simple calcul de puissance en kW permet grosso modo de dire qu'un camion donné pourra espérer transporter commodément telle ou telle charge ou si cela lui sera impossible. Voici quelques exemples d'ordres de grandeur en traitement de l'information :Balance Roberval : 1 bit par seconde (comparaison de deux poids) ;mainframe typique des années 1970 : 1 million d'opérations par seconde sur 32 bits ;Intel Paragon XP/S, 4 000 processeurs i860 à 50 MHz (1992) : 160 milliards d'opérations par seconde ;Summit, 9 216 processeurs POWER9 (2018) : 200 pétaflops, soit 200 millions de milliards d'opérations par seconde.Fugaku 415-PFLOPS (2020-2021): 415 pétaflops, soit 415 millions de milliards d'opérations par seconde.Cette puissance n'est pas à prendre au pied de la lettre. Elle précise surtout les ordres de grandeur en présence et leur évolution relativement rapide (jusqu'en 2018).L'intelligence artificielle n'avait donné que des résultats mitigés sur les ordinateurs typiques de 1970 effectuant 107 opérations logiques par seconde,. Le cerveau humain, formé de 1011 neurones ne pouvant chacun commuter plus de 100 fois par seconde en raison de leur temps de relaxation permettait beaucoup plus de traitements logiques par unité de temps (1013 opérations logiques par seconde). Ce handicap technique précis n'existe plus sur les ordinateurs depuis les années 2000, travaillant en 64 bits et avec des horloges cadencées à 4 GHz environ, pour des processeurs destinés aux particuliers. Concernant des supercalculateurs comme Summit ou Fugaku 415-PFLOPS, le rapport du nombre de comparaisons par seconde entre ordinateur et cerveau a même complètement changé de sens.Le matériel serait donc maintenant disponible, toutefois l'IA souligne la difficulté à expliciter toutes les connaissances utiles à la résolution d'un problème complexe. Certaines connaissances dites implicites sont acquises par l'expérience et mal formalisables. L'apprentissage de ces connaissances implicites par l'expérience est exploitée depuis les années 1980 (voir Réseau de neurones). Néanmoins, un autre type de complexité apparaît : la complexité structurelle. Comment mettre en relation des modules spécialisés pour traiter un certain type d'informations, par exemple un système de reconnaissance des formes visuelles, un système de reconnaissance de la parole, un système lié à la motivation, à la coordination motrice, au langage, etc. En revanche, une fois un système cognitif conçu et son apprentissage par l'expérience réalisé, l'« intelligence » correspondante peut être distribuée en un grand nombre d'exemplaires, par exemple sur les portables d'actuaires ou de banquiers pouvant ainsi, comme le rappelle un slogan, dire oui ou non, mais le dire tout de suite grâce à des applications dites de credit scoring.Les principales opinions soutenues pour répondre à la question d’une intelligence artificielle forte (c'est-à-dire douée d'une sorte de conscience) sont les suivantes :impossible : la conscience serait le propre des organismes vivants (supérieurs), et elle serait liée à la nature des systèmes biologiques. Cette position est défendue par certains philosophes et sociologues comme Harry Collins, pour qui l'intelligence requiert une immersion dans la société humaine, et donc un corps humain, et peut rappeler le courant du vitalisme.impossible avec des machines manipulant des symboles comme les ordinateurs actuels, mais possible avec des systèmes dont l’organisation matérielle serait fondée sur des processus quantiques. Des algorithmes quantiques sont théoriquement capables de mener à bien des calculs hors de l'atteinte pratique des calculateurs conventionnels (complexité en                               N                      3                                {\displaystyle N^{3}}   au lieu de                               2                      N                                {\displaystyle 2^{N}}  , par exemple, sous réserve d'existence du calculateur approprié). Au-delà de la rapidité, certains scientifiques comme Roger Penrose défendent que la conscience nécessiterait un fonctionnement non compatible avec les lois de la physique classique, et accessible uniquement avec des systèmes quantiques. Toutefois, l'état de la recherche en informatique quantique n'est pas encore suffisamment avancé pour permettre de l'utiliser dans des applications concrètes hors laboratoires, rendant difficile la vérification de ces hypothèses.impossible car la pensée n'est pas un phénomène calculable par des processus discrets et finis. Cette théorie est notamment avancée par le philosophe John Searle et son expérience de la chambre chinoise. Une conscience est donc nécessaire pour accéder à l'intelligence, mais un système informatique ne serait capable que d'en simuler une, sans pour autant la posséder, renvoyant au concept philosophique du zombie.possible avec des ordinateurs manipulant des symboles. La notion de symbole est toutefois à prendre au sens large. Cette option inclut les travaux sur le raisonnement ou l'apprentissage symbolique basé sur la logique des prédicats, mais aussi les techniques connexionnistes telles que les réseaux de neurones, qui, à "
informatique;"Le langage machine, ou code machine, est la suite de bits qui est interprétée par le processeur d'un ordinateur exécutant un programme informatique. C'est le langage natif d'un processeur, c'est-à-dire le seul qu'il puisse traiter. Il est composé d'instructions et de données à traiter codées en binaire.Chaque processeur possède son propre langage machine, dont un code machine qui ne peut s'exécuter que sur la machine pour laquelle il a été préparé. Si un processeur A est capable d'exécuter toutes les instructions du processeur B, on dit que A est compatible avec B. L'inverse n'est pas forcément vrai : A peut avoir des instructions supplémentaires que B ne connaît pas.Le code machine est aujourd'hui généré automatiquement, généralement par le compilateur d'un langage de programmation ou par l'intermédiaire d'un bytecode.Les « mots » d'un langage machine sont appelés instructions. Chacune d'elles déclenche une commande de la part du processeur (par exemple : chercher une valeur dans la mémoire pour charger un registre, additionner deux registres, etc.).Un processeur à architecture RISC ne reconnaît que peu d'instructions différentes, alors qu'un processeur à architecture CISC en possède un large éventail. Néanmoins certains processeurs CISC récents transforment en interne les instructions complexes en une suite d'instructions simples, qui sont alors exécutées.Un programme n'est qu'une longue séquence d'instructions qui sont exécutées par le processeur. Elles sont exécutées séquentiellement sauf quand une instruction de saut transfère l'exécution à une autre instruction que celle qui suit. Il existe également des sauts conditionnels qui sont soit exécutés (l'exécution continue à une autre adresse), soit ignorés (l'exécution continue à l'instruction suivante) selon certaines conditions.Chaque instruction commence par un nombre appelé opcode (ou code opération) qui détermine la nature de l'instruction.Par exemple, pour les ordinateurs d'architecture x86, l'opcode 0x6A (en binaire 01101010) correspond à l'instruction push (ajouter une valeur en haut de la pile).Par conséquent, l'instruction 0x6A 0x14 (01101010 00010100) correspond à push 0x14 (ajouter la valeur hexadécimale 0x14 , ou 20 en décimal, en haut de la pile).Certains processeurs codent toutes leurs instructions avec le même nombre de bits (par exemple : ARM, MIPS, PowerPC), tandis que chez d'autres la longueur de l'instruction dépend de l'opcode (exemple : x86). L'organisation des combinaisons de bits dépend largement du processeur. Le plus commun est la division en champs. Un ou plusieurs champs spécifient l'opération exacte (par exemple une addition). Les autres champs indiquent le type des opérandes, leur localisation, ou une valeur littérale (les opérandes contenus dans une instruction sont appelés immédiat). Avantages et inconvénients Lorsque toutes les instructions ont la même taille elles sont également alignées en mémoire. Par exemple si toutes les instructions sont alignées sur 32 bits (4 octets), alors les deux bits de poids faibles de l'adresse mémoire de n'importe quelle instruction sont à zéro. Cela permet notamment une implémentation plus aisée du cache des prédictions de branchement bimodales.En revanche le code machine prend moins de place en mémoire s'il ne possède pas de taille minimum, étant donné qu'on élimine les champs non utilisés.Alors que le langage machine était le seul disponible à l'aube des ordinateurs, il est aujourd'hui très long et fastidieux de développer en binaire : il faut passer par au moins un langage intermédiaire.De très nombreux langages de programmation sont transformés en langage machine lors de la compilation. Tous les programmes exécutables contiennent au moins une petite partie en langage machine.Le langage le plus facile à convertir en code machine est l'assembleur car il possède quasiment les mêmes instructions. L'assembleur (ou langage assembleur) diffère d'une machine à une autre, bien que les instructions soient au bout du compte très semblables. Les langages de plus haut niveau sont convertis en assembleur pendant la compilation. Les langages utilisant une machine virtuelle passent par un bytecode qui est converti à la volée par la machine virtuelle.Comme exemple spécifique, regardons l'architecture MIPS. Ses instructions ont toujours une longueur de 32 bits. Le type général de l'instruction est donné par les 6 bits de poids les plus forts (dans une représentation sur 32 bits, les 6 de gauche), qu'on appelle le champ op.Les instructions de type-J et de type-I sont pleinement spécifiées par le champ op. Les instructions de type-R ont un champ supplémentaire, fonct, pour déterminer la nature exacte de l'opération. Les champs de ces 3 types d'instructions sont :   6      5     5     5     5      6 bits[  op  |  rs |  rt |  rd |shamt| fonct]  type-R[  op  |  rs |  rt | adresse/immédiat ]  type-I[  op  |        adresse cible         ]  type-Jrs, rt, et rd indiquent des opérandes de type registre ; shamt indique un décalage (shift amount) ; et le champ adresse ou immédiat contient un opérande sous forme de valeur.Par exemple, ajouter les registres 1 et 2 et placer le résultat dans le registre 6 est codé :[  op  |  rs |  rt |  rd |shamt| fonct]    0     1     2     6     0     32     décimal 000000 00001 00010 00110 00000 100000   binaireCharger une valeur depuis la cellule mémoire 68 cellules après celle pointée par le registre 3 dans le registre 8 :[  op  |  rs |  rt | adresse/immédiat ]   35     3     8           68           décimal 100011 00011 01000  0000000001000100    binaireSauter à l'adresse 1025 (la prochaine instruction à exécuter se trouve à l'adresse 1025) :[  op  |        adresse cible         ]    2                 1025               décimal 000010   00000000000000010000000001     binaireLes processeurs de l'architecture ARM sont un cas particulier dans la mesure où toutes les instructions sont conditionnelles. Elles sont toutes d'une longueur de 32 bits, et leurs quatre premiers bits indiquent dans quelles conditions l'instruction doit être exécutée.Langage assembleur dit de deuxième générationLangage de haut niveau dit de troisième générationLangage de quatrième générationProcesseur, la composante qui exécute le langage machinePipeline (architecture des processeurs) Portail de la programmation informatique"
informatique;Une page web dynamique est une page web générée à la demande, par opposition à une page web statique. Le contenu d'une page web dynamique peut donc varier en fonction d'informations (heure, nom de l'utilisateur, formulaire rempli par l'utilisateur, etc.) qui ne sont connues qu'au moment de sa consultation. À l'inverse, le contenu d'une page web statique est a priori identique à chaque consultation.Lors de la consultation d'une page web statique, un serveur HTTP renvoie le contenu du fichier où la page est enregistrée.Lors de la consultation d'une page web dynamique, un serveur HTTP transmet la requête au logiciel correspondant à la requête, et le logiciel se charge de générer et envoyer le contenu de la page. La programmation web est le domaine de l'ingénierie informatique consacré au développement de tels logiciels. Les logiciels générant des pages web dynamiques sont fréquemment écrits avec les langages PHP, JavaServer Pages (JSP) ou Active Server Pages (ASP).Un site web dynamique peut ainsi fournir des informations aux utilisateurs en fonction de leur navigation sur celui-ci. Deux utilisateurs peuvent accéder simultanément à la même page web sans pour autant avoir le même contenu affiché à l'écran.Avec un site web dynamique, des modifications effectuées, par exemple via un système de soumission de commentaire ou bien une interface privée de gestion du site, pourront être directement visibles sur le site.Un site web dynamique peut permettre la mise en œuvre de différentes fonctionnalités, par exemples :- un système de gestion des accès, granulaire ou non, à certaines parties d'un site (administration du site, comptes utilisateurs, etc),- un système de soumission de commentaires publiques.En 2004, Le Journal du Net consacrait un article comparant les avantages des technologies statiques et dynamiques.Web profondProgrammation webFeuilles de style dynamiques en cascade Portail de l’informatique   Portail d’Internet
informatique;"PHP: Hypertext Preprocessor, plus connu sous son sigle PHP (sigle auto-référentiel), est un langage de programmation libre, principalement utilisé pour produire des pages Web dynamiques via un serveur HTTP, mais pouvant également fonctionner comme n'importe quel langage interprété de façon locale. PHP est un langage impératif orienté objet.PHP a permis de créer un grand nombre de sites web célèbres, comme Facebook et Wikipédia. Il est considéré comme une des bases de la création de sites web dits dynamiques mais également des applications web.PHP est un langage de script utilisé le plus souvent côté serveur : dans cette architecture, le serveur interprète le code PHP des pages web demandées et génère du code (HTML, XHTML, CSS par exemple) et des données (JPEG, GIF, PNG par exemple) pouvant être interprétés et rendus par un navigateur web. PHP peut également générer d'autres formats comme le WML, le SVG et le PDF.Il a été conçu pour permettre la création d'applications dynamiques, le plus souvent développées pour le Web. PHP est le plus souvent couplé à un serveur Apache bien qu'il puisse être installé sur la plupart des serveurs HTTP tels que IIS ou nginx. Ce couplage permet de récupérer des informations issues d'une base de données, d'un système de fichiers (contenu de fichiers et de l'arborescence) ou plus simplement des données envoyées par le navigateur afin d'être interprétées ou stockées pour une utilisation ultérieure.C'est un langage peu typé et souple et donc facile à apprendre par un débutant mais, de ce fait, des failles de sécurité peuvent rapidement apparaître dans les applications. Pragmatique, PHP ne s'encombre pas de théorie et a tendance à choisir le chemin le plus direct. Néanmoins, le nom des fonctions (ainsi que le passage des arguments) ne respecte pas toujours une logique uniforme, ce qui peut être préjudiciable à l'apprentissage.Son utilisation commence avec le traitement des formulaires puis par l'accès aux bases de données. L'accès aux bases de données est aisé une fois l'installation des modules correspondants effectuée sur le serveur. La force la plus évidente de ce langage est qu'il a permis au fil du temps la résolution aisée de problèmes autrefois compliqués et est devenu par conséquent un composant incontournable des offres d'hébergements.Il est multi-plateforme : autant sur Linux qu'avec Windows il permet aisément de reconduire le même code sur un environnement à peu près semblable (quoiqu'il faille prendre en compte les règles d'arborescences de répertoires, qui peuvent changer).Libre, gratuit, simple d'utilisation et d'installation, ce langage nécessite comme tout langage de programmation une bonne compréhension des principales fonctions usuelles ainsi qu'une connaissance aiguë des problèmes de sécurité liés à ce langage.La version 5.3 a introduit de nombreuses fonctions nouvelles : les espaces de noms (Namespace) — un élément fondamental de l'élaboration d'extensions, de bibliothèques et de frameworks structurés, les fonctions anonymes, les fermetures, etc.En 2018, près de 80 % des sites web utilisent le langage PHP sous ses différentes versions.Le langage PHP fait l'objet, depuis plusieurs années maintenant, de rassemblements nationaux organisés par l'AFUP (l'Association Française des Utilisateurs de PHP), où experts de la programmation et du milieu se retrouvent pour échanger autour du PHP et de ses développeurs. L'association organise ainsi deux évènements majeurs : le « Forum PHP », habituellement en fin d'année, et les « AFUP Day », qui ont lieu au cours du premier semestre, simultanément dans plusieurs villes.Le langage PHP a été créé en 1994 par Rasmus Lerdorf pour son site web. C'était à l'origine une bibliothèque logicielle en C dont il se servait pour conserver une trace des visiteurs qui venaient consulter son CV. Au fur et à mesure qu'il ajoutait de nouvelles fonctionnalités, Rasmus a transformé la bibliothèque en une implémentation capable de communiquer avec des bases de données et de créer des applications dynamiques et simples pour le Web. Rasmus a alors décidé, en 1995, de publier son code, pour que tout le monde puisse l'utiliser et en profiter. PHP s'appelait alors PHP/FI (pour Personal Home Page Tools/Form Interpreter). En 1997, deux étudiants, Andi Gutmans et Zeev Suraski, ont redéveloppé le cœur de PHP/FI. Ce travail a abouti un an plus tard à la version 3 de PHP, devenu alors PHP: Hypertext Preprocessor. Peu de temps après, Andi Gutmans et Zeev Suraski ont commencé la réécriture du moteur interne de PHP. C’est ce nouveau moteur, appelé Zend Engine — le mot Zend est la contraction de Zeev et Andi — qui a servi de base à la version 4 de PHP.En 2002, PHP est utilisé par plus de 8 millions de sites Web à travers le monde, en 2007 par plus de 20 millions et en 2013 par plus de 244 millions.De plus, PHP est devenu le langage de programmation web côté serveur le plus utilisé depuis plusieurs années :Enfin en 2010, PHP est le langage dont les logiciels open source sont les plus utilisés dans les entreprises, avec 57 % de taux de pénétration.Depuis juin 2011 et le nouveau processus de livraison de PHP, le cycle de livraison de PHP se résume à une mise à jour annuelle comportant des changements fonctionnels importants.La durée de vie d'une branche est de 3 ans, laissant trois branches stables et maintenues (cela signifie que lorsqu'une nouvelle version de PHP 5.x sort, la version 5.x-3 n'est plus supportée). Version 8.1 La version 8.1, sortie le 25 novembre 2021, introduit de nouvelles fonctionnalités comme : les énumérations ;les fibers ;la propriété Readonly. Version 8 Sortie le 26 novembre 2020, cette version majeure se démarque principalement par la fonctionnalité de « compilation à la volée » (Just-in-time compilation) qui permet un gain de vitesse d'exécution de plus de 45 % pour certaines applications Web. D'autres nouveautés sont également introduites comme : les weakmaps ;la Stringable Interface ;l'expression throw. Version 7.4 La version 7.4 est sortie le 20 février 2020. Elle vise à être maintenue jusqu'en novembre 2022.La version 7.4 se démarque de ses précédentes versions par :les propriétés typées 2.0 ;le pré-chargement ;l'opérateur d'affectation de coalescence nulle  ;improve openssl_random_pseudo_bytes ;les références faibles ;FFI (Foreign Function Interface) ;l'extension de hachage omniprésente ;le registre de hachage de mot de passe ;le fractionnement des chaînes multi-octets ;la réflexion sur les références ;le retrait de ext/wddx ;un nouveau mécanisme de sérialisation d'objets personnalisés. Version 7.3 Le 6 décembre 2018, la sortie de la version 7.3 mettait l'accent sur :l'évolution de la syntaxe Heredoc et Nowdoc ;la prise en charge de l'affectation de référence et de la déconstruction de tableau avec `list()` ;la prise en charge de PCRE2 ;l'introduction de la fonction High Resolution Time `hrtime()` function. Version 7.2 Le 30 novembre 2017, la version de PHP 7.2, qui utilise Zend Engine 2, a introduit une modélisation objet plus performante, une gestion des erreurs fondée sur le modèle des exceptions, ainsi que des fonctionnalités de gestion pour les entreprises. PHP 5 apporte beaucoup de nouveautés, telles que le support de SQLite ainsi que des moyens de manipuler des fichiers et des structures XML basés sur libxml2 :une API simple nommée SimpleXML ;une API Document Object Model assez complète ;une interface XPath utilisant les objets DOM et SimpleXML ;l'intégration de libxslt pour les transformations XSLT via l'extension XSL ;une bien meilleure gestion des objets par rapport à PHP 4, avec des possibilités qui tendent à se rapprocher de celles de Java. Version 7 (PHP7) Au vu des orientations différentes prises par le langage de celles prévues par PHP 6, une partie des développeurs propose de nommer la version succédant à PHP 5 « PHP 7 » au lieu de « PHP 6 ». Un vote parmi les développeurs valide cette proposition par 58 voix contre 24.PHP 7.0.0 est sorti en décembre 2015.La nouvelle version propose une optimisation du code et, d'après la société Zend, offre des performances dépassant celles de machines virtuelles comme HHVM,. Les benchmarks externes montrent des performances similaires pour HHVM et PHP 7, avec un léger avantage d'HHVM dans la plupart des scénarios. PHP 6 et Unicode En 2005, le projet de faire de PHP un langage fonctionnant d'origine en Unicode a été lancé par Andrei Zmievski, ceci en s'appuyant sur la bibliothèque International Components for Unicode (ICU) et en utilisant UTF-16 pour représenter les chaînes de caractères dans le moteur.Étant donné que cela représentait un changement majeur tant dans le fonctionnement du langage que dans le code PHP créé par ses utilisateurs, il fut décidé d'intégrer cela dans une nouvelle version 6.0 avec d'autres fonctionnalités importantes alors en développement. Toutefois, le manque de développeurs experts en Unicode ainsi que les problèmes de performance résultant de la conversion des chaînes de et vers UTF-16 (rarement utilisé dans un contexte web), ont conduit au report récurrent de la livraison de cette version. Par conséquent, une version 5.3 fut créée en 2009 intégrant de nombreuses fonctionnalités non liées à Unicode qui était initialement prévues pour la version 6.0, notamment le support des espaces de nommage (namespaces) et des fonctions anonymes. En mars 2010, le projet 6.0 intégrant unicode fut abandonné et la version 5.4 fut préparée afin d'intégrer la plupart des fonctionnalités non liées à l'unicode encore dans la branche 6.0, telles que les traits ou l'extension des fermetures au modèle objet.Le projet est depuis passé à un cycle de livraison prévisible (annuel) contenant des avancées significatives mais contenues tout en préservant au maximum la rétro-compatibilité avec le code PHP existant (5.4 en 2012, 5.5 en 2013, 5.6 prévue pour l'été 2014). Depuis janvier 2014, l'idée d'une nouvelle version majeure introduisant Unicode mais se basant sur UTF-8 (largement devenu depuis le standard du Web pour l'Unicode) et permettant certains changements pouvant casser la rétro-compatibilité avec du code PHP ancien est de nouveau discutée et les RFC sont maintenant triées selon leur implémentation en 5.x (évolutions ne causant pas ou marginalement de cassure de la rétro-compatibilité) ou dans la future version majeure (évolutions majeures du moteur et évolutions impliquant une non-compatibilité ascendante). À noter Il est à noter qu'historiquement, PHP disposait d'une configuration par défaut privilégiant la souplesse à la sécurité (par exemple register globals, qui a été activé par défaut jusqu'à PHP 4.2). Cette souplesse a permis à de nombreux développeurs d'apprendre PHP mais le revers de la médaille a été que de nombreuses applications PHP étaient mal sécurisées. Le sujet a bien été pris en main par le PHPGroup qui a mis en place des configurations par défaut mettant l'accent sur la sécurité. Il en résultait une réputation de langage peu sécurisé, réputation d'insécurité qui n'a plus de raison d'être[réf. nécessaire]. Détail de l'historique complet des versions PHP appartient à la grande famille des descendants du C, dont la syntaxe est très proche. En particulier, sa syntaxe et sa construction ressemblent à celles des langages Java et Perl, à ceci près que du code PHP peut facilement être mélangé avec du code HTML au sein d'un fichier PHP.Dans une utilisation destinée à l'internet, l'exécution du code PHP se déroule ainsi : lorsqu'un visiteur demande à consulter une page de site web, son navigateur envoie une requête au serveur HTTP correspondant. Si la page est identifiée comme un script PHP (généralement grâce à l'extension .php), le serveur appelle l'interprète PHP qui va traiter et générer le code final de la page (constitué généralement d'HTML ou de XHTML, mais aussi souvent de feuilles de style en cascade et de JS). Ce contenu est renvoyé au serveur HTTP, qui l'envoie finalement au client.Ce schéma explique ce fonctionnement :Une étape supplémentaire est souvent ajoutée : celle du dialogue entre PHP et la base de données. Classiquement, PHP ouvre une connexion au serveur de SGBD voulu, lui transmet des requêtes et en récupère le résultat, avant de fermer la connexion.L'utilisation de PHP en tant que générateur de pages Web dynamiques est la plus répandue, mais il peut aussi être utilisé comme langage de programmation ou de script en ligne de commande sans utiliser de serveur HTTP ni de navigateur. Il permet alors d'utiliser de nombreuses fonctions du langage C et plusieurs autres sans nécessiter de compilation à chaque changement du code source.Pour réaliser en Linux/UNIX un script PHP exécutable en ligne de commande, il suffit comme en Perl ou en Bash d'insérer dans le code en première ligne le shebang : #! /usr/bin/php. Sous un éditeur de développement comme SciTE, même en Windows, une première ligne <?php suffit, si le fichier possède un type .php.Il existe aussi une extension appelée PHP-GTK permettant de créer des applications clientes graphiques sur un ordinateur disposant de la bibliothèque graphique GTK+, ou encore son alternative WinBinder.PHP possède un grand nombre de fonctions permettant des opérations sur le système de fichiers, exécuter des commandes dans le terminal, la gestion des bases de données, des fonctions de tri et hachage, le traitement de chaînes de caractères, la génération et la modification d'images, des algorithmes de compression...Le moteur de Wikipédia, MediaWiki, est écrit en PHP et interagit avec une base MySQL ou PostgreSQLQuelques exemples du traditionnel Hello world :echo étant une structure du langage, il est possible – et même recommandé – de ne pas mettre de parenthèses.Il est aussi possible d'utiliser la version raccourcie :Résultat affiché :Le code PHP doit être inséré entre les balises <?php et ?> (la balise de fermeture est facultative en fin de fichier).Il y existe d'autres notations pour les balises :<?= et ?> (notation courte avec affichage) ;<? et ?> (notation courte sans affichage non disponible en PHP 8) ;<% et %> (notation ASP) ;<script language=""php""> et </script> (notation script).Les notations autres que la standard (<?php et ?>) et la notation courte avec affichage (<?= et ?>) sont déconseillées, car elles peuvent être désactivées dans la configuration du serveur (php.ini ou .htaccess) : la portabilité du code est ainsi réduite.Depuis PHP 7, les notations ASP et script ont été supprimées. La notation courte sans affichage reste déconseillée.Les instructions sont séparées par des ; (il n'est pas obligatoire après la dernière instruction) et les sauts de ligne ne modifient pas le fonctionnement du programme. Il serait donc possible d'écrire :Pour des raisons de lisibilité, il est néanmoins recommandé d'écrire une seule instruction par ligne. Il est aussi préférable d'écrire le dernier ;.Le code PHP est composé par des appels à des fonctions, dans le but d'attribuer des valeurs à des variables, le tout encadré dans des conditions, des boucles. Exemple :Une condition est appliquée quand l'expression entre parenthèses est évaluée à true, et elle ne l'est pas dans le cas de false. Sous forme numérique, 0 représente le false, et 1 (et tous les autres nombres) représentent le true.Le code précédent pourrait aussi être écrit de cette manière :Ici on teste l'égalité entre $lang et 'fr', mais pas directement dans le if : le test retourne un boolean (c'est-à-dire soit true, soit false) qui est stocké dans la variable $is_lang_fr. On entre ensuite cette variable dans le if et celui-ci, selon la valeur de la variable, effectuera ou non le traitement.Les blocs if, elseif et else sont généralement délimités par les caractères { et }, qui peuvent être omis, comme dans les codes précédents, lorsque ces blocs ne contiennent qu'une instruction.Il est également possible d'écrire else if en deux mots, comme en C/C++.On peut générer du code HTML avec le script PHP, par exemple :Il est également possible d'utiliser une syntaxe alternative pour la structure if/else :Une autre approche consiste à concaténer l'intégralité du code HTML dans une variable et de réaliser un echo de la variable en fin de fichier :Dans le cas où l'utilisateur aura préféré l'utilisation de la commande echo à la concaténation, il lui sera possible de capturer le flux en utilisant les fonctions ob_start() et ob_get_clean() :PHP, tout comme JavaScript, permet aussi de construire un modèle objet de document (DOM), ce qui permet de créer ou modifier un document (X)HTML sans écrire de HTML, comme le montre l'exemple suivant :Qui crée le code HTML suivant :Cette méthode est cependant peu utilisée pour générer un document complet, on l'utilise généralement pour générer un fichier XML.La commande phpinfo() est aussi utilisée pour générer un code HTML décrivant les paramètres du serveur ; elle est aussi très utilisée pour tester la bonne exécution du moteur d’exécution PHP.Comme en C++ et en Java, PHP permet de programmer en orienté objet, en créant des classes contenant des attributs et des méthodes, qui peuvent être instanciées ou utilisées en statique.Toutefois, PHP est un langage à héritage simple, c'est-à-dire qu'une classe ne peut hériter que d'au plus une seule autre classe (sinon il faut utiliser un trait pour simuler l'héritage multiple par composition). Cependant les interfaces peuvent en étendre plusieurs autres.Voici un exemple de création d'une classe :Comme de nombreux projets Open Source, PHP possède une mascotte. Il s'agit de l'éléPHPant, dessiné en 1998 par El Roubio.El Roubio s'est inspiré de la ressemblance des lettres PHP avec un éléphant et du fait que deux des lettres du langage soient déjà présentes dans ce mot, ce qui a permis de créer le néologisme éléPHPant. Toutes les œuvres d'El Roubio sont distribuées sous licence GNU GPL. Une peluche de l'ÉléPHPant bleu existe. D'autres versions ont vu le jour ces dernières années (rose, jaune, rouge, violet et orange) sous l'impulsion de sociétés (PHP Architect ou Zend Technologies) ou de groupes utilisateurs comme PHP Women ou PHP Amsterdam. Le site afieldguidetoelephpant.net recense tous les éléphpants existants.Wiki (MediaWiki, DokuWiki...)forum (phpBB, Vanilla, IPB, punBB...)FacebookSystèmes de gestion de blog (Dotclear)Systèmes de gestion de contenu (appelés aussi CMS) (WordPress, SPIP, ExpressionEngine, Drupal, Xoops, Joomla, K-Box...)Administration de bases de données (phpMyAdmin, phpPgAdmin, Adminer...)Frameworks (Laravel, Symfony, Zend Framework, CodeIgniter, CakePHP, etc.)Logiciel ECMLogiciel BPM, CRM et ou ERP (Dolibarr...)E-commerce (PrestaShop, WooCommerce, Magento, osCommerce, Sylius, etc.)Partis politiques (Parti chrétien-démocrate (France), etc.)Universités et formations supérieures alliant art et sciences (Ingénieur IMAC, UPEM, etc.)Un serveur Web en architecture trois tiers est composé d'un système d'exploitation, un serveur HTTP, un langage serveur et enfin un système de gestion de base de données (SGBD), cela constituant une plate-forme.Dans le cas de PHP comme langage serveur, les combinaisons les plus courantes sont celles d'une plateforme LAMP (pour Linux Apache MySQL PHP) et WAMP (Windows Apache MySQL PHP). Une plate-forme WAMP s'installe généralement par le biais d'un seul logiciel qui intègre Apache, MySQL et PHP, par exemple EasyPHP, VertrigoServ, WampServer ou UwAmp. Il existe le même type de logiciels pour les plates-formes MAMP (Mac OS Apache MySQL PHP), à l'exemple du logiciel MAMP.Il existe d'autres variantes, par exemple les plates-formes LAPP (le M de MySQL est remplacé par le P de PostgreSQL) ou encore le logiciel XAMPP (Apache MySQL Perl PHP ; le X indique que le logiciel est multiplate-forme), un kit de développement multiplate-forme.On peut décliner une grande variété d'acronymes sous cette forme. Des confusions peuvent parfois exister entre la plate-forme en elle-même et le logiciel permettant de l'installer, si elles ont le même nom. Il faut également remarquer que la grande majorité des logiciels « tout en un » sont destinés au développement d'applications Web en local, et non à être installés sur des serveurs Web. Une exception à cette règle est peut-être Zend Server, le serveur distribué par Zend Technologies, qui est prévu pour fonctionner aussi bien en environnement de développement que de production.PHP est à la base un langage interprété, ce qui est au détriment de la vitesse d'exécution du code. Sa forte popularité associée à son utilisation sur des sites Web à très fort trafic (Yahoo, Facebook) ont amené un certain nombre de personnes à chercher à améliorer ses performances pour pouvoir servir un plus grand nombre d'utilisateurs de ces sites Web sans nécessiter l'achat de nouveaux serveurs.La réécriture du cœur de PHP, qui a abouti au Zend Engine pour PHP 4 puis au Zend Engine 2 pour PHP 5, est une optimisation. Le Zend Engine compile en interne le code PHP en bytecode exécuté par une machine virtuelle. Les projets open source APC et eAccelerator fonctionnent en mettant le bytecode produit par Zend Engine en cache afin d'éviter à PHP de charger et d'analyser les scripts à chaque requête. À partir de la version 5.5 de PHP, le langage dispose d'un cache d'opcode natif (appelé OpCache) rendant obsolète le module APC.Il existe également des projets pour compiler du code PHP :Roadsend et phc compilent du PHP en C ;Quercus compile du PHP en bytecode Java exécutable sur une machine virtuelle Java ;Phalanger compile du PHP en Common Intermediate Language exécutable sur le Common Language Runtime du framework .NET ;HipHop for PHP transforme du PHP en C++ qui est ensuite compilé en code natif. Ce projet open source a été démarré par Facebook.(en) Luke Welling et Laura Thomson, PHP and MySQL Web development, Sams Publishing, 2008, 4e éd. (ISBN 978-0-672-32916-6 et 0-672-32916-6, OCLC 854795897)Damien Seguy et Philippe Gamache, Sécurité PHP 5 et MySQL, 3e édition, Eyrolles, 1er décembre 2011, 277 p. (ISBN 978-2-212-13339-4 et 2-212-13339-1, lire en ligne)Jean Engels PHP 5 Cours et Exercices, 3e édition, Eyrolles 2013, 631 pages  (ISBN 978-2-212-13725-5)Paamayim Nekudotayim : nom de l'opérateur :: en PHPListe de frameworks PHP : liste des cadres de développement (Frameworks) en PHPSuhosin: module de durcissement de PHP5(en) Site officiel Portail des logiciels libres   Portail de la programmation informatique"
informatique;"La programmation déclarative est un paradigme de programmation qui consiste à créer des applications sur la base de composants logiciels indépendants du contexte et ne comportant aucun état interne. Autrement dit, l'appel d'un de ces composants avec les mêmes arguments produit exactement le même résultat, quel que soit le moment et le contexte de l'appel.En programmation déclarative, on décrit le quoi, c'est-à-dire le problème. Par exemple, les pages HTML sont déclaratives car elles décrivent ce que contient une page (texte, titres, paragraphes, etc.) et non comment les afficher (positionnement, couleurs, polices de caractères…). Alors qu'en programmation impérative (par exemple, avec le C ou Java), on décrit le comment, c'est-à-dire la structure de contrôle correspondant à la solution.C'est une forme de programmation sans effets de bord, ayant généralement une correspondance avec la logique mathématique.Il existe plusieurs formes de programmation déclarative :la programmation descriptive, à l'expressivité réduite, qui permet de décrire des structures de données, comme HTML ou LaTeX ;la programmation fonctionnelle, qui perçoit les applications comme un ensemble de fonctions mathématiques, comme Lisp, Caml, Haskell et Oz ;la programmation logique, pour laquelle les composants d'une application sont des relations logiques, comme Prolog et Mercury ;la programmation par contraintes.Peter Van Roy, Seif Haridi. Concepts, Techniques, and Models of Computer Programming. MIT Press, 2004. Portail de la programmation informatique"
informatique;"En informatique, la programmation impérative est un paradigme de programmation qui décrit les opérations en séquences d'instructions exécutées par l'ordinateur pour modifier l'état du programme. Ce type de programmation est le plus répandu parmi l'ensemble des langages de programmation existants, et se différencie de la programmation déclarative (dont la programmation logique ou encore la programmation fonctionnelle sont des sous-ensembles).La quasi-totalité des processeurs qui équipent les ordinateurs sont de nature impérative : ils sont faits pour exécuter une suite d'instructions élémentaires, codées sous forme d'opcodes (pour operation codes). L'ensemble des opcodes forme le langage machine spécifique à l'architecture du processeur. L'état du programme à un instant donné est défini par le contenu de la mémoire centrale à cet instant.Les langages de plus haut niveau utilisent des variables et des opérations plus complexes, mais suivent le même paradigme. Les recettes de cuisine et les vérifications de processus industriel sont deux exemples de concepts familiers qui s'apparentent à de la programmation impérative ; de ce point de vue, chaque étape est une instruction, et le monde physique constitue l'état modifiable. Puisque les idées de base de la programmation impérative sont à la fois conceptuellement familières et directement intégrées dans l'architecture des microprocesseurs, la grande majorité des langages de programmation est impérative.La plupart des langages de haut niveau comporte cinq types d'instructions principales :la séquence d'instructionsl'assignation ou affectationl'instruction conditionnellela boucleles branchementsUne séquence d'instructions, (ou bloc d'instruction) désigne le fait de faire exécuter par la machine une instruction, puis une autre, etc., en séquence. Par exemple                                           ouvrirConnexion                          ;                              envoyerMessage                          ;                              fermerConnexion                          ;              {\displaystyle {\mbox{ouvrirConnexion}};{\mbox{envoyerMessage}};{\mbox{fermerConnexion}};}   est une séquence d'instructions. Cette construction se distingue du fait d'exécuter en parallèle des instructions.Les instructions d'assignation, en général, effectuent une opération sur l'information en mémoire et y enregistrent le résultat pour un usage ultérieur. Les langages de haut niveau permettent de plus l'évaluation d'expressions complexes qui peuvent consister en une combinaison d'opérations arithmétiques et d'évaluations de fonctions et l'assignation du résultat en mémoire. Par exemple:                     x        ?        2        +        3        ;              {\displaystyle x\leftarrow 2+3;}   assigne la valeur                     2        +        3              {\displaystyle 2+3}  , donc 5, à la variable de nom                     x              {\displaystyle x}  .Les instructions conditionnelles permettent à un bloc d'instructions de n'être exécuté que si une condition prédéterminée est réalisée. Dans le cas contraire, les instructions sont ignorées et la séquence d'exécution continue à partir de l'instruction qui suit immédiatement la fin du bloc. Par exemple                     s        i                                      connexionOuverte                                  a        l        o        r        s                                      envoyerMessage                          ;              {\displaystyle si\;{\mbox{connexionOuverte}}\;alors\;{\mbox{envoyerMessage}};}   n'enverra le message que si la connexion est ouverte.Les instructions de bouclage servent à répéter une suite d'instructions un nombre prédéfini de fois (voir Boucle_for), ou jusqu'à ce qu'une certaine condition soit réalisée. Par exemple                     t        a        n        t        q        u        e                                      connexionNonOuverte                                  a        l        o        r        s                                      attendreUnPeu                          ;              {\displaystyle tantque\;{\mbox{connexionNonOuverte}}\;alors\;{\mbox{attendreUnPeu}};}   bouclera jusqu'à ce que la connexion soit ouverte.Il se trouve que ces quatre constructions permettent de faire tous les programmes informatiques possibles, elles permettent de faire un système Turing-complet.Les branchements sans condition permettent à la séquence d'exécution d'être transférée à un autre endroit du programme. Cela inclut le saut, appelé « goto » (go to, /??? tu?/, « aller à ») dans de nombreux langages, et les sous-programmes, ou appels de procédures. Les instructions de bouclage peuvent être vues comme la combinaison d'un branchement conditionnel et d'un saut. Les appels à une fonction ou une procédure (donc un Sous-programme) correspondent à un saut, complété du  passage de paramètres, avec un saut en retour.Les langages impératifs les plus anciens sont les langages machine des premiers ordinateurs. Dans ces langages, le jeu d'instructions est minimal, ce qui rend la mise en œuvre matérielle plus simple — on maîtrise directement ce qui se passe en mémoire —, mais gêne la création de programmes complexes.Le premier compilateur – un programme destiné à vérifier un programme au préalable et à le traduire en langage machine – dénommé A-0, fut écrit en 1951 par Grace Murray Hopper.Fortran, développé par John Backus (prix Turing 1977) chez IBM à partir de 1954, fut le premier langage de programmation capable de réduire les obstacles présentés par le langage machine dans la création de programmes complexes. Fortran était un langage compilé, qui autorisait entre autres l'utilisation de variables nommées, d'expressions complexes, et de sous-programmes. Premier langage normalisé au milieu des années 60, il continue d'évoluer et est toujours utilisé dans le milieu scientifique pour la qualité de ses bibliothèques numériques et sa grande rapidité, ce qui en fait le langage informatique ayant eu la plus grande longévité. Les normes Fortran apparues depuis le début du XXIe siècle sont Fortran 2003, Fortran 2008 et Fortran 2018.Les deux décennies suivantes virent l'apparition de plusieurs autres langages de haut niveau importants. ALGOL, développé en 1958 par un consortium américano-européen pour concurrencer FORTRAN, qui était un langage propriétaire, fut l'ancêtre de nombreux langages de programmation d'aujourd'hui.COBOL (1960) est un langage pour la programmation des applications de gestion développé avec plusieurs objectifs : d'une part avoir un langage standardisé, avec des sources portables sur des matériels différents, d'autre part avoir des sources lisibles et vérifiables par des non-spécialistes de l'informatique. Dans cet objectif, il a été défini avec une syntaxe proche de l'anglais. Le langage a ensuite évolué pour intégrer la programmation structurée (COBOL 85), et la programmation orientée objet (2000). Le parc énorme d'applications COBOL existantes dans les grandes entreprises assure sa longévité.Le langage BASIC (1963) a été conçu comme une version simplifiée de FORTRAN à but éducatif, destinée aux débutants et interactive. Sa simplicité et le fait que BASIC soit interprété facilitaient grandement la mise au point des programmes, ce qui lui conféra rapidement une grande popularité, malgré la pauvreté de ses constructions. Malheureusement, cette pauvreté même devait mener à une quantité de programmes non structurés et donc difficilement maintenables. Après un article de Edsger Dijkstra dénonçant les ravages de BASIC, la réputation de BASIC comme langage pour l'enseignement de la programmation déclina, au profit de Pascal.Dans les années 1970, le Pascal fut développé par Niklaus Wirth, dans le but d'enseigner la programmation structurée et modulaire. Pascal dérivait d'une proposition faite par N. Wirth (et refusée) pour l'évolution du langage ALGOL. Il combine les constructions de base de la programmation structurée (boucles tant-que, répéter-jusqu'à et boucle avec compteur), la possibilité de définir ses propres types de donnée, dans un ensemble élégant (servi par un grand nombre de types prédéfinis : ensemble, énumérations, intervalle), qui lui assura un succès durable comme langage d'initiation (en remplacement de BASIC). Par la suite, Niklaus Wirth fut à l'origine de Modula-2, Modula-3, et d'Oberon, les successeurs de Pascal.À la même époque, Dennis Ritchie créa le langage C aux laboratoires Bell, pour le développement du système Unix. La puissance du C, permettant grâce aux pointeurs de travailler à un niveau proche de la machine, ainsi qu'un accès complet aux primitives du système, lui assura un succès qui ne s'est jamais démenti depuis.Une des raisons du succès du langage C par rapport aux autres langages procéduraux de la même génération vient de son mode de distribution : les universités américaines pouvaient acheter une licence au prix de 300 dollars pour toute l'université et tous ses étudiants[réf. nécessaire].En 1974, le Département de la Défense des États-Unis cherchait un langage dont le cahier des charges mettait l'accent sur la sûreté d'exécution, pour tous ses besoins futurs. Le choix se porta sur Ada, langage créé par Jean Ichbiah chez CII-Honeywell Bull, dont la spécification ne fut complétée qu'en 1983. Le langage a connu plusieurs révisions, la dernière en date remontant à 2012.Dans les années 1980, devant les problèmes que posaient la complexité grandissante des programmes, il y eut un rapide gain d'intérêt pour la programmation orientée objet. Smalltalk-80, conçu à l'origine par Alan Kay en 1969, fut présenté en 1980 par le Palo Alto Research Center de la compagnie Xerox (États-Unis).À partir des concepts objet, Bjarne Stroustrup, chercheur aux Bell Labs, conçut en 1985 une extension orientée objet de C nommée C++. Parallèlement, une extension à C moins ambitieuse, mais inspirée de Smalltalk avait vu le jour, Objective C. Le succès d'Objective C, notamment utilisé pour le développement sur les stations NeXT et Mac OS X, est resté faible par rapport à C++.Dans les décennies 1980 et 1990, de nouveaux langages impératifs interprétés ou semi-interprétés doivent leur succès au développement de scripts pour des pages web dynamiques et les applications client-serveur. On peut citer dans ces catégories Perl (Larry Wall, 1987), Tcl (John Ousterhout, 1988), Python (Guido van Rossum, 1990), PHP (Rasmus Lerdorf, 1994), Java (Sun Microsystems, 1995), JavaScript (Brendan Eich, Netscape Navigator, 1995).Les langages de programmation impératifs doivent être distingués d'autres types de langages, les langages fonctionnels et les langages de programmation logique. Les langages fonctionnels, tels que Haskell ou ML, ne sont pas des suites d'instructions et ne s'appuient pas sur l'idée d'état global, mais au contraire tendent à s'extraire de ce modèle pour se placer à un niveau plus conceptuel (qui a ses fondations dans le lambda-calcul). Les langages de programmation logiques, tels que Prolog, se concentrent sur ce qui doit être calculé, et non comment le calcul doit être effectué.Un synopsis de l'histoire des langages de programmationUn cours en ligne de l'Université Paris XIII Portail de la programmation informatique"
informatique;"La programmation orientée objet (POO), ou programmation par objet, est un paradigme de programmation informatique. Elle consiste en la définition et l'interaction de briques logicielles appelées objets ; un objet représente un concept, une idée ou toute entité du monde physique, comme une voiture, une personne ou encore une page d'un livre. Il possède une structure interne et un comportement, et il sait interagir avec ses pairs. Il s'agit donc de représenter ces objets et leurs relations ; l'interaction entre les objets via leurs relations permet de concevoir et réaliser les fonctionnalités attendues, de mieux résoudre le ou les problèmes. Dès lors, l'étape de modélisation revêt une importance majeure et nécessaire pour la POO. C'est elle qui permet de transcrire les éléments du réel sous forme virtuelle.La programmation par objet consiste à utiliser des techniques de programmation pour mettre en œuvre une conception basée sur les objets. Celle-ci peut être élaborée en utilisant des méthodologies de développement logiciel objet, dont la plus connue est le processus unifié (« Unified Software Development Process » en anglais), et exprimée à l'aide de langages de modélisation tels que le Unified Modeling Language (UML).La programmation orientée objet est facilitée par un ensemble de technologies dédiées :les langages de programmation (chronologiquement : Simula, LOGO, Smalltalk, Ada, C++, Objective C, Eiffel, Python, PHP, Java, Ruby, AS3, C#, VB.NET, Fortran 2003, Vala, Haxe, Swift) ;les outils de modélisation qui permettent de concevoir sous forme de schémas semi-formels la structure d'un programme (Objecteering, UMLDraw, Rhapsody, DBDesigner…) ;les bus distribués (DCOM, CORBA, RMI, Pyro…) ;les ateliers de génie logiciel ou AGL (Visual Studio pour des langages Dotnet, NetBeans ou Eclipse pour le langage Java).Il existe actuellement deux grandes catégories de langages à objets : les langages à classes, que ceux-ci soient sous forme fonctionnelle (Common Lisp Object System), impérative (C++, Java) ou les deux (Python, OCaml) ;les langages à prototypes (JavaScript, Lua).En implantant les Record Class de Hoare, le langage Simula 67 pose les constructions qui seront celles des langages orientés objet à classes : classe, polymorphisme, héritage, etc. Mais c'est réellement par et avec Smalltalk 71 puis Smalltalk 80, inspiré en grande partie par Simula 67 et Lisp, que les principes de la programmation par objets, résultat des travaux d'Alan Kay, sont véhiculés : objet, encapsulation, messages, typage et polymorphisme (via la sous-classification) ; les autres principes, comme l'héritage, sont soit dérivés de ceux-ci ou une implantation. Dans Smalltalk, tout est objet, même les classes. Il est aussi plus qu'un langage à objets, c'est un environnement graphique interactif complet.À partir des années 1980, commence l'effervescence des langages à objets : C++ (1983), Objective-C (1984), Eiffel (1986), Common Lisp Object System (1988), etc. Les années 1990 voient l'âge d'or de l'extension de la programmation par objets dans les différents secteurs du développement logiciel.Depuis, la programmation par objets n'a cessé d'évoluer aussi bien dans son aspect théorique que pratique et différents métiers et discours mercatiques à son sujet ont vu le jour :l'analyse objet (AOO ou OOA en anglais) ;la conception objet (COO ou OOD en anglais) ;les bases de données objet (SGBDOO) ;les langages objets avec les langages à prototypes ;ou encore la méthodologie avec MDA (Model Driven Architecture).Aujourd'hui, la programmation par objets est vue davantage comme un paradigme, le paradigme objet, que comme une simple technique de programmation. C'est pourquoi, lorsque l'on parle de nos jours de programmation par objets, on désigne avant tout la partie codage d'un modèle à objets obtenu par AOO et COO.La programmation orientée objet a été introduite par Alan Kay avec Smalltalk. Toutefois, ses principes n'ont été formalisés que pendant les années 1980 et, surtout, 1990. Par exemple le typage de second ordre, qui qualifie le typage de la programmation orientée objet (appelé aussi duck typing), n'a été formulé qu'en 1995 par Cook.Concrètement, un objet est une structure de données qui répond à un ensemble de messages. Cette structure de données définit son état tandis que l'ensemble des messages qu'il comprend décrit son comportement :les données, ou champs, qui décrivent sa structure interne sont appelées ses attributs ;l'ensemble des messages forme ce que l'on appelle l'interface de l'objet ; c'est seulement au travers de celle-ci que les objets interagissent entre eux. La réponse à la réception d'un message par un objet est appelée une méthode (méthode de mise en œuvre du message) ; elle décrit quelle réponse doit être donnée au message.Certains attributs et/ou méthodes (ou plus exactement leur représentation informatique) sont cachés : c'est le principe d'encapsulation. Ainsi, le programme peut modifier la structure interne des objets ou leurs méthodes associées sans avoir d'impact sur les utilisateurs de l'objet.Un exemple avec un objet représentant un nombre complexe : celui-ci peut être représenté sous différentes formes (cartésienne (réel, imaginaire), trigonométrique, exponentielle (module, angle)). Cette représentation reste cachée et est interne à l'objet. L'objet propose des messages permettant de lire une représentation différente du nombre complexe. En utilisant les seuls messages que comprend notre nombre complexe, les objets appelants sont assurés de ne pas être affectés lors d'un changement de sa structure interne. Cette dernière n'est accessible que par les méthodes des messages.Dans la programmation par objets, chaque objet est typé. Le type définit la syntaxe (« Comment l'appeler ? ») et la sémantique des messages (« Que fait-il ? ») auxquels peut répondre un objet. Il correspond donc, à peu de chose près, à l'interface de l'objet. Toutefois, la plupart des langages objets ne proposent que la définition syntaxique d'un type (C++, Java, C#…) et rares sont ceux qui fournissent aussi la possibilité de définir formellement sa sémantique (comme dans le langage Eiffel avec sa conception par contrats).Un objet peut appartenir à plus d'un type : c'est le polymorphisme ; cela permet d'utiliser des objets de types différents là où est attendu un objet d'un certain type. Une façon de réaliser le polymorphisme est le sous-typage (appelé aussi héritage de type) : on raffine un type-parent en un autre type (le sous-type) par des restrictions sur les valeurs possibles des attributs. Ainsi, les objets de ce sous-type sont conformes au type parent. De ceci découle le principe de substitution de Liskov. Toutefois, le sous-typage est limité et ne permet pas de résoudre le problème des types récursifs (un message qui prend comme paramètre un objet du type de l'appelant). Pour résoudre ce problème, Cook définit en 1995 la sous-classification et le typage du second ordre qui régit la programmation orientée objet : le type est membre d'une famille polymorphique à point fixe de types (appelée classe). Les traits sont une façon de représenter explicitement les classes de types. (La représentation peut aussi être implicite comme avec Smalltalk, Ruby, etc.).On distingue dans les langages objets deux mécanismes du typage :le typage dynamique : le type des objets est déterminé à l'exécution lors de la création desdits objets (Smalltalk, Common Lisp, Python, PHP…) ;le typage statique : le type des objets est vérifié à la compilation et est soit explicitement indiqué par le développeur lors de leur déclaration (C++, Java, C#, Pascal…), soit déterminé par le compilateur à partir du contexte (Scala, OCaml…).De même, deux mécanismes de sous-typage existent : l'héritage simple (Smalltalk, Java, C#) et l'héritage multiple (C++, Python, Common Lisp, Eiffel, WLangage).Le polymorphisme ne doit pas être confondu avec le sous-typage ou avec l'attachement dynamique (dynamic binding en anglais).La programmation objet permet à un objet de raffiner la mise en œuvre d'un message défini pour des objets d'un type parent, autrement dit de redéfinir la méthode associée au message : c'est le principe de redéfinition des messages (ou overriding en anglais).Or, dans une définition stricte du typage (typage du premier ordre), l'opération résultant d'un appel de message doit être la même quel que soit le type exact de l'objet référé. Ceci signifie donc que, dans le cas où l'objet référé est de type exact un sous-type du type considéré dans l'appel, seule la méthode du type père est exécutée :Soit un type Reel contenant une méthode * faisant la multiplication de deux nombres réels, soient Entier un sous-type de Reel, i un Entier et r un Reel, alors l'instruction i * r va exécuter la méthode * de Reel. On pourrait appeler celle de Entier grâce à une redéfinition.Pour réaliser alors la redéfinition, deux solutions existent :le typage du premier ordre associé à l'attachement dynamique (c'est le cas de C++, Java, C#…). Cette solution induit une faiblesse dans le typage et peut conduire à des erreurs. Les relations entre type sont définies par le sous-typage (théorie de Liskov) ;le typage du second ordre (duquel découlent naturellement le polymorphisme et l'appel de la bonne méthode en fonction du type exact de l'objet). Ceci est possible avec Smalltalk et Eiffel. Les relations entre types sont définies par la sous-classification (théorie F-Bound de Cook).La structure interne des objets et les messages auxquels ils répondent sont définis par des modules logiciels. Ces mêmes modules créent les objets via des opérations dédiées. Deux représentations existent de ces modules : la classe et le prototype.La classe est une structure informatique particulière dans le langage objet. Elle décrit la structure interne des données et elle définit les méthodes qui s'appliqueront aux objets de même famille (même classe) ou type. Elle propose des méthodes de création des objets dont la représentation sera donc celle donnée par la classe génératrice. Les objets sont dits alors instances de la classe. C'est pourquoi les attributs d'un objet sont aussi appelés variables d'instance et les messages opérations d'instance ou encore méthodes d'instance. L'interface de la classe (l'ensemble des opérations visibles) forme les types des objets. Selon le langage de programmation, une classe est soit considérée comme une structure particulière du langage, soit elle-même comme un objet (objet non-terminal). Dans le premier cas, la classe est définie dans le runtime ; dans l'autre, la classe a besoin elle aussi d'être créée et définie par une classe : ce sont les méta-classes. L'introspection des objets (ou « méta-programmation ») est définie dans ces méta-classes.La classe peut être décrite par des attributs et des messages. Ces derniers sont alors appelés, par opposition aux attributs et messages d'un objet, variables de classe et opérations de classe ou méthodes de classe. Parmi les langages à classes on retrouve Smalltalk, C++, C#, Java, etc.Le prototype est un objet à part entière qui sert de prototype de définition de la structure interne et des messages. Les autres objets de mêmes types sont créés par clonage. Dans le prototype, il n'y a plus de distinction entre attributs et messages : ce sont tous des slots. Un slot est un label de l'objet, privé ou public, auquel est attachée une définition (ce peut être une valeur ou une opération). Cet attachement peut être modifié à l'exécution. Chaque ajout d'un slot influence l'objet et l'ensemble de ses clones. Chaque modification d'un slot est locale à l'objet concerné et n'affecte pas ses clones.Le concept de trait permet de modifier un slot sur un ensemble de clones. Un trait est un ensemble d'opérations de même catégorie (clonage, persistance, etc.) transverse aux objets. Il peut être représenté soit comme une structure particulière du langage, comme un slot dédié ou encore comme un prototype. L'association d'un objet à un trait fait que l'objet et ses clones sont capables de répondre à toutes les opérations du trait. Un objet est toujours associé à au moins un trait, et les traits sont les parents des objets (selon une relation d'héritage). Un trait est donc un mixin doté d'une parenté. Parmi les langages à prototype on trouve Javascript, Self, Io, Slater, Lisaac, etc.Différents langages utilisent la programmation orientée objet, par exemple PHP, Python, etc.En PHP la programmation orientée objet est souvent utilisée pour mettre en place une architecture MVC (Modèle Vue Contrôleur), où les modèles représentent des objets.La modélisation objet consiste à créer un modèle du système informatique à réaliser. Ce modèle représente aussi bien des objets du monde réel que des concepts abstraits propres au métier ou au domaine dans lequel le système sera utilisé.La modélisation objet commence par la qualification de ces objets sous forme de types ou de classes sous l'angle de la compréhension des besoins et indépendamment de la manière dont ces classes seront mises en œuvre. C'est ce que l'on appelle l'analyse orientée objet ou OOA (acronyme de « Object-Oriented Analysis »). Ces éléments sont alors enrichis et adaptés pour représenter les éléments de la solution technique nécessaires à la réalisation du système informatique. C'est ce que l'on appelle la conception orientée objet ou OOD (acronyme de « Object-Oriented Design »). À un modèle d'analyse peuvent correspondre plusieurs modèles de conception. L'analyse et la conception étant fortement interdépendants, on parle également d'analyse et de conception orientée objet (OOAD). Une fois un modèle de conception établi, il est possible aux développeurs de lui donner corps dans un langage de programmation. C'est ce que l'on appelle la programmation orientée objet ou OOP (en anglais « Object-Oriented Programming »). Pour écrire ces différents modèles, plusieurs langages et méthodes ont été mis au point. Ces langages sont pour la plupart graphiques. Les trois principaux à s'imposer sont OMT de James Rumbaugh, la méthode Booch de Grady Booch et OOSE de Ivar Jacobson. Toutefois, ces méthodes ont des sémantiques différentes et ont chacune des particularités qui les rendent particulièrement aptes à certains types de problèmes. OMT offre ainsi une modélisation de la structure de classes très élaborée. Booch a des facilités pour la représentation des interactions entre les objects. OOSE innove avec les cas d'utilisation pour représenter le système dans son environnement. La méthode OMT prévaut sur l'ensemble des autres méthodes au cours de la première partie de la décennie 1990.À partir de 1994, Booch et Jacobson, rapidement rejoints par Rumbaugh, décident d'unifier leurs approches au sein d'une nouvelle méthode qui soit suffisamment générique pour pouvoir s'appliquer à la plupart des contextes applicatifs. Ils commencent par définir le langage de modélisation UML (Unified Modeling Language) appelé à devenir un standard de l'industrie. Le processus de normalisation est confié à l'Object Management Group (OMG), un organisme destiné à standardiser des technologies orientées objet comme CORBA (acronyme de « Common Object Request Broker Architecture »), un intergiciel (« middleware » en anglais) objet réparti. Rumbaugh, Booch et Jacobson s'affairent également à mettre au point une méthode permettant d'une manière systématique et répétable d'analyser les exigences et de concevoir et mettre en œuvre une solution logicielle à l'aide de modèles UML. Cette méthode générique de développement orienté objet devient le processus unifié (également connu sous l'appellation anglo-saxonne de « Unified Software Development Process »). Elle est itérative et incrémentale, centrée sur l'architecture et guidée par les cas d'utilisation et la réduction des risques. Le processus unifié est de plus adaptable par les équipes de développement pour prendre en compte au mieux les particularités du contexte.Néanmoins pour un certain nombre de concepteurs objet, dont Bertrand Meyer, l'inventeur du langage orienté objet Eiffel, guider une modélisation objet par des cas d'utilisations est une erreur de méthode qui n'a rien d'objet et qui est plus proche d'une méthode fonctionnelle. Pour eux, les cas d'utilisations sont relégués à des utilisations plutôt annexes comme la validation d'un modèle par exemple[réf. nécessaire].(en) Brad J. Cox et Andrew J. Novobilski, Object-Oriented Programming : An Evolutionary Approach, Addison-Wesley, 1986 (ISBN 0-201-54834-8).Grady Booch, James Rumbaugh et Ivar Jacobson, Le guide de l'utilisateur UML, EYROLLES, 2000 (ISBN 2-212-09103-6).Erich Gamma, Richard Helm, Ralph Johnson et John Vlissides (trad. de l'anglais par Jean-Marie Lasvergères), Design Patterns : Catalogue des modèles de conception réutilisables, Vuibert, 1999 (ISBN 2-7117-8644-7).Bertrand Meyer (2000). Conception et programmation orientées objet,  (ISBN 2-212-09111-7).De Hugues Bersini (2007). L'Orienté Objet,  (ISBN 978-2-212-12084-4).Francisco Bonito (2000). La programmation : l'orienté objet.Introduction à la POO Apprendre simplement la Programmation Orientée ObjetDes paradigmes « classiques » à l'orienté objetAnalyse et conception orientée objet avec UML et RUP, un survol rapide(en) The Theory of Classification de Anthony J.H. Simons sur le JOT (Journal of Object Technology) Portail de la programmation informatique"
informatique;"En informatique, la programmation procédurale est un paradigme  qui se fonde sur le concept d'appel procédural. Une procédure, aussi appelée routine, sous-routine ou fonction (à ne pas confondre avec les fonctions de la programmation fonctionnelle reposant sur des fonctions mathématiques), contient simplement une série d'étapes à réaliser. N'importe quelle procédure peut être appelée à n'importe quelle étape de l'exécution du programme, y compris à l'intérieur d'autres procédures, voire dans la procédure elle-même (récursivité).La programmation procédurale est un meilleur choix qu'une simple programmation séquentielle. Les avantages sont en effet les suivants :la possibilité de réutiliser le même code à différents emplacements dans le programme sans avoir à le dupliquer (principe « DRY »), ce qui a pour effet la réduction de la taille du code source et un gain en localité des modifications, donc une amélioration de la maintenabilité (compréhension plus rapide, réduction du risque de régression) ;une façon plus simple de suivre l'exécution du programme : la programmation procédurale permet de se passer d'instructions telles que goto, évitant ainsi bien souvent de se retrouver avec un programme compliqué qui part dans toutes les directions (appelé souvent « programmation spaghetti[réf. nécessaire] ») ; cependant, la programmation procédurale permet les « effets de bord », c'est-à-dire la possibilité pour une procédure qui prend des arguments de modifier des variables extérieures à la procédure auxquelles elle a accès (variables de contexte plus global que la procédure).La modularité est une caractéristique souhaitable pour un programme ou une application informatique, et consiste enle découpage du programme ou de l'application en unités sans effet de bord entre elles, c'est-à-dire dont le fonctionnement et le résultat renvoyé au module appelant ne dépend que des paramètres explicitement passés en argument (unités fonctionnelles). Un module est un ensemble de structure de données et de procédures, dont l'effet de bord est confiné à cet ensemble de données.De ce fait, un module offre un service. Un module peut avoir un contexte d'exécution différent de celui du code appelant : on parle alors de RPC (Remote Procedure Call) si ce contexte est un autre ordinateur, ou de communication inter-processus (légers ou système) s'il s'agit du même ordinateur. Lorsque la transmission des données ne se fait pas en mémoire mais par fichiers, les modules qui communiquent peuvent être compilés séparément et un script doit assurer l'enchainement des appels.On constate qu'il n'est pas contre-indiqué pour une procédure d'accéder en lecture et en écriture à des variables de contexte plus global (celui d'un module) : cela permet une réduction essentielle du nombre d'arguments passés, mais au détriment de la réutilisation telle quelle dans d'autres contextes d'une procédure. C'est le module en entier qui est réutilisable.Du fait de leur comportement sans effet de bord, chaque module peut être développé par une personne ou un groupe de personnes distinct de ceux qui développent d'autres modules. Les bibliothèques sont des modules. À noter que pour qu'une procédure puisse être considérée comme se comportant comme une « fonction pure » mathématique, il faut que la valeur de son résultat renvoyé au programme appelant prenne toujours la même valeur pour chaque valeur des arguments. Il faut donc qu'elle ne dépende pas d'une variable globale statique éventuelle du module, statique au sens qu'elle garde sa valeur après la fin de l'invocation du module (par une de ses procédures).La programmation objet et générique permet une mutualisation et une unicité de l'information et des traitements/procédures/méthodes (en théorie). En identifiant les variables globales à un module à une structure au sens C ou Pascal et à un type utilisateur, ces modules deviennent par définition des « classes » dont l'instanciation correspond à l'instanciation d'un type composé (une structure C ou Pascal).De plus, par le jeu du polymorphisme et de la généricité, les méthodes d'une classe, qui correspondent exactement aux procédures du module correspondant, en confiant l'effet de bord aux attributs de cette classe, peuvent accepter des arguments dont le type est variable (d'une manière contrôlée par le graphe d'héritage). De ce fait, la programmation objet va plus loin dans la factorisation des traitements que la programmation procédurale (en prolongeant celle-ci), et permet de répondre bien mieux à des besoins où des traitements similaires sont attendus dans des endroits différents d'une solution. Dans une programmation objet aboutie, les « procédures d'aiguillage » (routage de traitements en fonction du type d'une variable passée en argument) sont reléguées au compilateur par utilisation de la liaison dynamique. Le code source s'en trouve aussi réduit.Le plus vieil exemple de ce type de langage est l'ALGOL. D'autres exemples sont Fortran, PL/I, Modula-2 et Ada (dans sa première version). Portail de la programmation informatique"
informatique;La théorie algorithmique de l'information, initiée par Kolmogorov, Solomonov et Chaitin dans les années 1960, vise à quantifier et qualifier le contenu en information d'un ensemble de données, en utilisant la théorie de la calculabilité et la notion de machine universelle de Turing.Cette théorie permet également de formaliser la notion de complexité d'un objet, dans la mesure où l'on considère qu'un objet (au sens large) est d'autant plus complexe qu'il faut beaucoup d'informations pour le décrire, ou — à l'inverse — qu'un objet contient d'autant plus d'informations que sa description est longue. La théorie algorithmique de l'information est fondée sur cette équivalence : la description d'un objet est formalisée par un algorithme (autrement dit une machine de Turing), et sa complexité (autrement dit  son contenu en information) est  formalisé par certaines caractéristiques de l'algorithme : sa longueur ou son temps de calcul.  Ces fondements  sont différents de ceux de la théorie de l'information de Shannon : cette dernière n'utilise pas la notion de calculabilité et n'a de sens que par rapport à un ensemble statistique de données. Cependant, les deux théories sont compatibles et des liens formels entre elles peuvent être établis.Tandis que la théorie de l'information de Shannon a eu de nombreuses applications en informatique, télécommunications, traitement de signal et neurosciences computationnelles, la théorie algorithmique de l'information a été utilisée avec succès dans les domaines de la biologie, de la physique et même de la philosophie.L'idée principale de la théorie algorithmique de l'information est qu'une chose est d'autant plus complexe, ou contient d'autant plus d'information, qu'elle est difficile à expliquer, c'est-à-dire fondamentalement longue à expliquer. Voici par exemple trois descriptions d'objets :D1 : « un mur tout blanc de 1 m sur 1 m. »D2 : « un mur tout blanc de 1m sur 1m, avec une rayure rouge horizontale de 2 cm de large en bas, une autre 8 cm au-dessus, encore une autre 8 cm au-dessus, encore une autre 8 cm au-dessus, encore une autre 8 cm au-dessus, encore une autre 8 cm au-dessus, encore une autre 8 cm au-dessus, encore une autre 8 cm au-dessus, encore une autre 8 cm au-dessus, encore une autre 8 cm au-dessus et une dernière encore 8 cm au-dessus. »D2' : « un mur tout blanc de 1 m sur 1 m, avec des rayures rouges horizontales de 2 cm de large, de bas en haut tous les 8 cm. »En termes de longueur de description, D1 est plus courte que D2' qui est plus courte que D2. Que D1 soit la plus courte description semble normal, et est lié au fait que l'objet décrit est « plus simple ». Mais en ce qui concerne D2 et D2', les objets décrits sont identiques bien que D2' soit plus courte que D2. Ainsi la longueur brute d'une description n'est pas une mesure parfaitement adaptée.L'idée « algorithmique » est alors de considérer, comme complexité de l'objet décrit, sa plus courte description possible. Idée « algorithmique » dans le sens où la description n'est pas forcément extensive, mais peut — comme D2' dans l'exemple ci-dessus — décrire un procédé d'obtention de l'objet (ici : tracer des bandes horizontales à intervalles réguliers).Ainsi, un objet sera d'autant plus compliqué qu'on ne peut le décrire plus brièvement qu'une liste exhaustive de ses propriétés… Ce dernier cas constitue le cas limite d'une complexité maximale.Jean-Paul Delahaye Information, complexité et hasard [détail des éditions]Li, M., and Vitanyi, P. An Introduction to Kolmogorov Complexity and its Applications, Springer-Verlag, New York, 1997Analyse de la complexité des algorithmesThéorie de la complexitéThéorie de la simplicité Portail de l'informatique théorique
informatique;"Un serveur web est soit un logiciel de service de ressources web (serveur HTTP), soit un serveur informatique (ordinateur) qui répond à des requêtes du World Wide Web sur un réseau public (Internet) ou privé (intranet),,, en utilisant principalement le protocole HTTP.Un serveur informatique peut être utilisé à la fois pour servir des ressources du Web et pour faire fonctionner en parallèle d'autres services liés, comme l'envoi d'e-mails, l'émission de flux en streaming, le stockage de données dans des bases de données, le transfert de fichiers par FTP.Les serveurs web publics sont reliés à Internet et hébergent des ressources (pages web, images, vidéos, etc.) du Web. Ces ressources peuvent être statiques (servies telle quelles) ou dynamiques (construites à la demande par le serveur).Certains serveurs sont seulement accessibles sur des réseaux privés (intranets) et hébergent des sites utilisateurs, des documents, ou des logiciels, internes à une entreprise, une administration, etc.Techniquement il serait possible qu'un même ordinateur remplisse ces deux fonctions, mais c'est rarement le cas pour des raisons de sécurité[réf. nécessaire]. La fonction principale d'un serveur Web est de stocker et délivrer des pages web qui sont généralement rendues en HTML. Le protocole de communication Hypertext Transfer Protocol (HTTP) permet le dialogue via le réseau avec le logiciel client, généralement un navigateur web.Les deux termes sont utilisés pour le logiciel car le protocole HTTP a été développé pour le Web, et les pages Web sont en pratique toujours servies avec ce protocole. Cependant d'autres ressources du Web comme les fichiers à télécharger ou les flux audio ou vidéo sont parfois servis avec d'autres protocoles, telle que, par exemple, le protocole de transport Temps Réel (Real-time Transport Protocol), ainsi que son pendant sécurisé, le protocole de transport sécurisé Temps Réel (Secure Real-time Transport Protocol).CERN httpd est le premier serveur HTTP, inventé en même temps que le World Wide Web, en 1990 au CERN de Genève. Il est rapidement devenu obsolète en raison de l'évolution exponentielle des fonctionnalités du protocole.Quelques serveurs HTTP :Apache HTTP Server de la Apache Software Foundation, successeur du NCSA HTTPd ;Apache Tomcat de la Apache Software Foundation, évolution de Apache pour J2EE ;BusyBox httpd, utilisé dans le domaine de l'informatique embarquée, et notamment avec OpenWRT ;Google Web Server de Google ;Internet Information Services (IIS) de Microsoft ;lighttpd de Jan Kneschke ;Monkey web server de Eduardo Silva Pereira, dédié au noyau Linux, permettant d'utiliser pleinement ses fonctionnalités ;nginx d'Igor Sysoev ;Hiawatha de Hugo LeisinkNodeJS sous MIT Licence conçu par Ryan Lienhart Dahl en lignes de programmation en JavaScript ;Sun Java System Web Server de Sun Microsystems (anciennement iPlanet de Netscape, puis Sun ONE de Sun Microsystems) ;Tengine, fork de nginx, de Taobao (9e rang mondial Alexa en juillet 2014) ;Zeus Web Server de Zeus Technology ;Gunicorn est un serveur web HTTP WSGI écrit en Python pour Unix ;Zazouminiwebserver, serveur extrêmement léger (approx. 500 kilooctets), sous environnement Microsoft Windows.Abyss Web Server, un serveur gratuit, multi-plateforme (Linux, Windows, MacOS, BSD), permettant un paramétrage très facile via une interface graphique multilingue.Le serveur HTTP le plus utilisé est Apache HTTP Server qui sert environ 55 % des sites web en janvier 2013 selon Netcraft.Le serveur HTTP le plus utilisé dans les 1 000 sites les plus actifs est en revanche Nginx avec 38,2 % de parts de marché en 2016  selon w3techs et 53,9 % en avril 2017Historiquement, d'autres serveurs HTTP importants furent CERN httpd, développé par les inventeurs du Web, abandonné le 15 juillet 1996 et NCSA HTTPd, développé au NCSA en même temps que NCSA Mosaic, abandonné mi-1994, ainsi que WebObjects.Il existe aussi des serveurs HTTP qui sont des serveurs d'applications capables de faire serveur HTTP, comme Caudium et GlassFish. À l'inverse, on peut trouver des serveurs HTTP spécialisés dans un service distinct comme : HTTP File Server qui est uniquement destiné au partage de fichiersLe logiciel serveur HTTP ou daemon HTTP est le logiciel prenant en charge les requêtes client-serveur du protocole HTTP développé pour le World Wide Web. Ces logiciels intègrent généralement des modules permettant d'exécuter un langage serveur comme PHP pour générer des pages web dynamiques. Les plus connus sont Apache, Nginx, IIS, et Lighttpd.Le plus souvent, un serveur Web exécute continuellement d'autres logiciels qui fonctionnent en collaboration avec le logiciel de serveur HTTP. Selon les besoins, certains services gourmands en ressources, comme le serveur de base de données, peuvent être situés sur la même machine ou un serveur spécialisé.Certaines combinaisons de logiciels de base sont connues sous différents acronymes, notamment celle d'Apache (serveur HTTP) logiciel installé et exécuté sur le serveur web en parallèle de MySQL (serveur de base de données) et le script d'interprétation et d'exécution de PHP (voire PHP-FPM).Voir en PDF l'introduction « Qu'entend-t-on par serveur HTTP et serveur Web ? » d'Anthony Garcia (2008) - IBISC[source insuffisante] :LAMP pour « Linux, Apache, MySQL, PHP » ;WAMP pour « Windows, Apache, MySQL, PHP » ;MAMP pour « Macintosh, Apache, MySQL, PHP ».Il existe aussi la distribution de Microsoft nommée IIS pour « Internet Information Services » qui comprend plusieurs services : HTTP, FTP, SMTP et NNTP.L’équilibrage de charge des serveurs web, ou répartition de charge des serveurs Web, regroupe l’ensemble des mécanismes utilisés pour distribuer les requêtes sur de multiples serveurs Web. Cette pratique est devenue indispensable depuis l’explosion du trafic du Web qui a pour conséquence un accroissement important de la charge demandé au serveur. Cela a entraîné une évolution des architectures, destinée à apporter plus de scalabilité, de disponibilité et de performances. Portail des réseaux informatiques   Portail d’Internet   Portail de l’informatique"
informatique;"Un superordinateur ou supercalculateur est un ordinateur conçu pour atteindre les plus hautes performances possibles avec les techniques connues lors de sa conception, en particulier en ce qui concerne la vitesse de calcul. Pour des raisons de performance, c'est presque toujours un ordinateur central, dont les tâches sont fournies en traitement par lots.La science des superordinateurs est appelée « calcul haute performance » (en anglais : high-performance computing ou HPC). Cette discipline se divise en deux : la partie matérielle (conception électronique de l'outil de calcul) et la partie logicielle (adaptation logicielle du calcul à l'outil). Ces deux parties font appel à des champs de connaissances différents.Les premiers superordinateurs (ou supercalculateurs) apparaissent dans les années 1960.En 1961, IBM développe l'IBM Stretch ou IBM 7030, dont une unité est exploitée en France en 1963.À cette époque, et jusque dans les années 1970, le plus important constructeur mondial de superordinateurs est la société Control Data Corporation (CDC), avec son concepteur Seymour Cray. Par la suite, Cray Research, fondée par Seymour Cray après son départ de CDC, prend l’avantage sur ses autres concurrents, jusqu’aux alentours de l'année 1990. Dans les années 1980, à l’image de ce qui s’était produit sur le marché des micro-ordinateurs des années 1970, de nombreuses petites sociétés se lancèrent sur ce marché, mais la plupart disparaissent dans le « crash » du marché des superordinateurs, au milieu des années 1990.Ce que désigne le terme superordinateur varie avec le temps, car les ordinateurs les plus puissants du monde à un moment donné tendent à être égalés, puis dépassés, par des machines d’utilisation courante plusieurs années après. Les premiers superordinateurs CDC étaient de simples ordinateurs mono-processeurs (mais possédant parfois jusqu’à dix processeurs périphériques pour les entrées-sorties) environ dix fois plus rapides que la concurrence. Dans les années 1970, la plupart des superordinateurs adoptent un processeur vectoriel, qui effectue le décodage d’une instruction une seule fois pour l’appliquer à toute une série d’opérandes.C’est seulement vers la fin des années 1980 que la technique des systèmes massivement parallèles est adoptée, avec l’utilisation dans un même superordinateur de milliers de processeurs. De nos jours, certains de ces superordinateurs parallèles utilisent des microprocesseurs de type « RISC », conçus pour des ordinateurs de série, comme les PowerPC ou les PA-RISC. D’autres supercalculateurs utilisent des processeurs de moindre coût, de type « CISC », microprogrammés en RISC dans la puce électronique (AMD ou Intel) : le rendement en est un peu moins élevé, mais le canal d’accès à la mémoire — souvent un goulet d’étranglement — est bien moins sollicité.Au XXIe siècle, les superordinateurs sont le plus souvent conçus comme des modèles uniques par des constructeurs informatiques « traditionnels » comme International Business Machines (IBM), Hewlett-Packard (HP), ou Bull, qu’ils aient derrière eux une longue tradition en la matière (IBM) ou qu’ils aient racheté dans les années 1990 des entreprises spécialisées, alors en difficulté, pour acquérir de l’expérience dans ce domaine.Les superordinateurs sont utilisés pour toutes les tâches qui nécessitent une très forte puissance de calcul, comme les prévisions météorologiques, l’étude du climat (à ce sujet, voir les programmes financés par le G8-HORCs), la modélisation d'objets chimiques (calcul de structures et de propriétés, modélisation moléculaire, etc.), les simulations physiques (simulations aérodynamiques, calculs de résistance des matériaux, simulation d'explosion d'arme nucléaire, étude de la fusion nucléaire, etc.), la cryptanalyse ou les simulations en finance et en assurance (calcul stochastique).Les institutions de recherche civiles et militaires comptent parmi les plus gros utilisateurs de superordinateurs.En France, on trouve ces machines dans les centres nationaux de calculs universitaires, tels que l'Institut du développement et des ressources en informatique scientifique (IDRIS), le Centre informatique national de l'enseignement supérieur (CINES), mais aussi au Commissariat à l'énergie atomique et aux énergies alternatives (CEA) ou dans certaines grandes entreprises, comme Total, EDF ou encore Météo-France.Les superordinateurs tirent leur supériorité sur les ordinateurs conventionnels à la fois grâce à :leur architecture, en « pipeline » (exécution d’une instruction identique sur une longue série de données) ou parallèle (nombre très élevé de processeurs fonctionnant chacun sur une partie du calcul), qui leur permet d’exécuter plusieurs tâches simultanément ;des composants électroniques rapides (structure de type serveurs lame utilisant des processeurs multi-cœur ou des cartes graphiques dédiées au calcul scientifique de dernière génération, de la mémoire vive et des équipements de stockage de masse — disque dur — reliés à la fibre optique en grande quantité, etc.) associés à un système d'exploitation dédié (comme Linux, majoritairement utilisé actuellement).Ils sont presque toujours conçus spécifiquement pour un certain type de tâches (le plus souvent des calculs numériques scientifiques : calcul matriciel ou vectoriel) et ne cherchent pas de performance particulière dans d'autres domaines.L’architecture mémorielle des supercalculateurs est étudiée pour fournir en continu les données à chaque processeur afin d’exploiter au maximum sa puissance de calcul. Les performances supérieures de la mémoire (meilleurs composants et meilleure architecture) expliquent pour une large part l’avantage des superordinateurs sur les ordinateurs classiques.Leur système d’entrée/sortie (bus) est conçu pour fournir une large bande passante, la latence étant moins importante puisque ce type d’ordinateur n’est pas conçu pour traiter des transactions.Comme pour tout système parallèle, la loi d’Amdahl s’applique, les concepteurs de superordinateurs consacrant une partie de leurs efforts à éliminer les parties non parallélisables du logiciel et à développer des améliorations matérielles pour supprimer les goulots d'étranglement restants.D'une part, les superordinateurs ont souvent besoin de plusieurs mégawatts de puissance électrique. Cette alimentation doit aussi être de qualité. En conséquence, ils produisent une grande quantité de chaleur et doivent donc être refroidis pour fonctionner normalement. Le refroidissement (par exemple à air) de ces ordinateurs pose souvent un problème important de climatisation.D'autre part, les données ne peuvent circuler plus vite que la vitesse de la lumière entre deux parties d'un ordinateur. Lorsque la taille d’un superordinateur dépasse plusieurs mètres, le temps de latence entre certains composants se compte en dizaines de nanosecondes. Les éléments sont donc disposés pour limiter la longueur des câbles qui relient les composants. Sur le Cray-1 ou le Cray-II, par exemple, ils étaient disposés en cercle.De nos jours, ces ordinateurs sont capables de traiter et de communiquer de très importants volumes de données en très peu de temps. La conception doit assurer que ces données puissent être lues, transférées et stockées rapidement. Dans le cas contraire, la puissance de calcul des processeurs serait sous-exploitée (goulot d’étranglement).           En 1993, l'Institut de Physique du Globe de Paris (IPGP) opère un ordinateur CM-5/128 qui utilise des processeurs SuperSPARC, il est classé 25e au TOP500. Trois ans plus tard, en 1996, l'Institut du développement et des ressources en informatique scientifique (IDRIS) parvient à atteindre la 12e place mondiale avec le T3E construit par Cray.À la mi-2002, le plus puissant des supercalculateurs français se classe 4e au TOP500, c'est le TERA basé sur des processeurs Alpha à 1 GHz (AlphaServer SC45) et développé par Hewlett-Packard ; il appartenait au Commissariat à l'énergie atomique (CEA). En janvier 2006, le TERA-10 de Bull lui succède, il génère une puissance de calcul de 60 téraFLOPS et se placera au 5e rang mondial du TOP500.En juin 2008, l'IDRIS et son Blue Gene/P Solution d'IBM affiche, selon le test LINPACK, une puissance de 120 téraflops et remporte la 10e place.En novembre 2009, la première machine française a pour nom Jade. De type « SGI Altix (en) » elle est basée au Centre informatique national de l'enseignement supérieur (CINES) de Montpellier. Ce supercalculateur se classe au 28e rang mondial avec 128 téraflops au test LINPACK. Peu après, la configuration de la machine Jade est complétée pour atteindre une performance de 237 téraflops. La machine passe en juin 2010 au 18e rang du TOP500. C’est alors le troisième système informatique européen et le premier français, il est destiné à la recherche publique.En novembre 2010, le record français est détenu par le TERA-100 de Bull. Installé au CEA à Bruyères-le-Châtel pour les besoins de la simulation militaire nucléaire française, avec une performance de 1 050 téraflops, cette machine se hisse au 6e rang mondial et gagne le 1er rang européen. Elle est constituée de 17 296 processeurs Intel Xeon 7500 dotés chacun de huit cœurs et connectés par un réseau de type InfiniBand.En mars 2012, Curie, un système conçu par Bull pour le GENCI, installé sur le site du Très Grand Centre de Calcul (TGCC) à Bruyères-le-Châtel, dispose d'une puissance de 1,359 pétaflops. Il sera le supercalculateur le plus puissant de France en prenant la 9e place du classement mondial. Il est conçu pour délivrer 2 pétaflops.En janvier 2013, les systèmes Ada et Turing construits par IBM sont installés à l'IDRIS d'Orsay. La somme de leur puissance dépasse le pétaflops. Ces deux machines sont à la disposition des chercheurs. En mars 2013, le supercalculateur Pangea détenu par la société Total est inauguré, il devient le système le plus performant jamais installé en France. Sa puissance de calcul s'élève à 2,3 pétaflops. Équivalant à 27 000 ordinateurs de bureau réunis, il obtient la 11e place mondiale.En janvier 2015, le système Occigen, conçu par Bull, Atos technologies, pour le GENCI est installé sur le site du CINES ; il est doté d'une puissance de 2,1 pétaflops. Il se situait en 26e position au classement mondial du TOP500 de novembre 2014.En mars 2016, Total annonce avoir triplé la capacité de calcul de son supercalculateur Pangea, passant à une puissance de calculs de 6,7 pétaflops en pics de performance et de 5,28 pétaflops en puissance utilisable. Cela lui permet de retrouver le 11e rang au TOP500 et le place ainsi en tête du secteur industriel mondial.En juin 2022, le GENCI met en service Adastra, un superordinateur fourni par HPE-Cray hébergé au CINES. Ses 46,10 pétaflops lui permettent de gagner le 10e rang mondial en termes de performances de calcul.L'essor des supercalculateurs a vu Linux devenir le système d'exploitation équipant la majorité des 500 supercalculateurs les plus puissants de la planète,, Unix perdant progressivement du terrain face à Linux, mais occupant pendant un temps une place de choix sur le marché des supercalculateurs (5 %).[réf. souhaitée]Windows ne fut exécuté que par deux des 500 supercalculateurs les plus puissants de la planète, soit 0,4 %, tandis que BSD n'était présent que sur une seule machine du top 500, soit 0,2 %. Enfin, les autres configurations (« Mixed », soit un ensemble de plusieurs types de systèmes d'exploitation) représentaient 4,6 %.[réf. souhaitée]En novembre 2017, Linux équipe la totalité des 500 superordinateurs les plus puissants au monde.Georges Karadimas (Snecma), « Les superordinateurs dans le secteur aérospatial français », dans Nouvelle revue Aéronautique et Astronautique, no 2, juin 1994  (ISSN 1247-5793).Site HPC du commissariat à l'énergie atomique (CEA)Site officiel du centre informatique national de l'enseignement supérieur (CINES)Site officiel de l'institut du développement et des ressources en informatique scientifique (IDRIS) Portail de l’informatique"
informatique;"Un système de traitement de l'information est un système constitué d'un ensemble de composants (mécaniques, électroniques, chimiques, photoniques ou biologiques) permettant de traiter automatiquement des informations. Il est régi par la théorie de l'information et c'est l'élément central de tout appareil informatique.Les premiers systèmes permettant de traiter automatiquement des informations étaient des appareils mécaniques tel que la pascaline ou encore la machine d'Anticythère qui représentent les tout premiers calculateurs spécifiques à l'inverse de l'ordinateur qui est programmable et universel.Aujourd'hui[Quand ?] il existe une panoplie d'appareils en électronique numérique permettant de traiter automatiquement des informations : commutateur réseau, ordinateur, console de jeu, calculatrice, certaines cartes à puce, distributeur automatique de billets, enregistreur vidéo personnel, GPS, téléphone portable, etc.Avant d'être un appareil fonctionnel, le système de traitement de l'information est d'abord un objet d'étude conceptuelle qui propose une approche systémique à l'informaticien. Il permet d'élargir les champs de recherche en informatique et permet de catégoriser les systèmes de traitement par rapport à leurs capacités plutôt qu'à leurs fonctionnements.Un système de traitement de l'information se caractérise par un minimum de quatre unités :l'unité d'entrée (anglais : input), recueille l'information ;l'unité de stockage (anglais : storage) conserve toute ou une partie de l'information ;l'unité de traitement (anglais : processing), transforme l'état de l'information ;l'unité de sortie (anglais : output) présente le résultat de la modification.L'évolution de l'état d'une information ne peut être observée qu'en accord avec la notion de temps. L'horloge est le système de traitement de l'information le plus élémentaire. Tous les systèmes de traitement de l'information sont jusqu'à présent mus par un système de balancement ou de mouvement oscillatoire dont la source d'entraînement est naturelle (ex. : l'actionnement d'une manivelle) ou artificielle (rotation d'un moteur, alimentation électrique d'un quartz, etc.).L'information est représenté par un  signal de type analogique ou numérique et qui peut être véhiculé sous forme matériel (engrenage, molécule, etc.) ou énergétique (particule élémentaire).La Pascaline est le premier calculateur mécanique. Il a été construit par Blaise Pascal en 1642.La première conceptualisation d'un système de traitement de l'information programmable et universel est appelé machine de Turing. C'est la thèse de Turing qui est aujourd'hui considérée comme l'article fondateur de l'ordinateur.Le premier calculateur électronique à utiliser le système binaire est l'EDVAC. Construit en 1945, il occupait une salle de 45 m2, et pesait près de 8 tonnes.C'est l'invention du transistor en 1947 et celle du circuit intégré en 1958 qui ont permis la miniaturisation électronique des systèmes de traitement de l'information.La première console de jeu, l'Odyssey a été construite en 1973. C'était un système de traitement de l'information qui n'était pas Turing-complet.Le premier type d'informations que les systèmes étaient capables de manipuler étaient des nombres, puis des textes, jusqu'à l'arrivée dans les années 1980 des systèmes multimédia, c'est-à-dire capables de manipuler divers types d'informations: images, sons, vidéos…Exemples d'informations :nombres : prix, poids, volume, température, vitesse, pression…textes : courrier, publications, articles de presse…images : plans, dessins, graphiques, diagrammes, cartes géographiques, photos, images 3D…sons : paroles, chants, bruitages ou musique ,vidéo : prises de vue, clips ou films ;les instructions d'un programme informatique sont aussi des informations.Dans le système de traitement de l'information, les informations circulent sous forme de suite de bits (chiffres en base 2) et le octet groupés dans des fichiers ou des enregistrements (voir : électronique numérique).Un format désigne la manière dont les bits sont disposés à l'intérieur du fichier ou de l'enregistrement pour stocker l'information.Un protocole est un ensemble de règles normalisées qui, lorsqu'elles sont appliquées de manière commune par deux appareils, leur permettent de s'échanger des informations.Les règles établies par un protocole concernent autant le point de vue électronique (tensions, fréquences) que le point de vue informationnel (choix des informations, format) ainsi que le déroulement des opérations de communication (qui initie la communication, comment réagit le correspondant, combien de temps dure la communication…).Les protocoles sont utilisés en informatique et en télécommunication (téléphonie, télévision).Un système de traitement de l'information est composé de quatre unités :l'unité d'entrée (anglais : input), qui permet de faire entrer les informations dans le système ;l'unité de stockage (anglais : storage) qui permet de conserver les informations ;l'unité de traitement (anglais : processor) qui comme son nom l'indique va traiter les informations ;l'unité de sortie' (anglais : output) qui permet de faire sortir les résultats des traitements.Les informations peuvent être introduites par une personne à l'aide d'un clavier et d'une souris, enregistrées à l'aide de différents appareils — microphone, caméra, scanner, appareil photo, ou apportées par des dispositifs de télécommunication (voir: réseau informatique).Les premiers appareils permettant d'introduire des informations étaient des lecteurs de cartes perforées. Un dispositif semblable à celui des pianos mécaniques. Cette technologie datant du XVIIIe siècle a été utilisée en informatique jusque dans les années 1980.La première étape des traitements consiste en la réceptions des informations en provenance des différents appareils.La numérisation est le procédé qui consiste à transformer une information provenant du monde réel en une suite de chiffres qui seront utilisés dans le système de traitement de l'information.Les informations stockées peuvent être des informations qui viennent d'être entrées dans la machine, ou résultats d'un traitement.Les informations sont conservées dans des dispositifs de stockage tels que disque durs, DVD, CD-ROM ou mémoire flash.La possibilité de stocker des informations existe depuis les années 1960, auparavant les informations étaient traitées à mesure qu'elles étaient entrées.Le système de traitement des informations effectue les traitements en suivant scrupuleusement les  instructions d'un programme informatique.Les traitements peuvent consister à :à partir de certaines informations, d'obtenir d'autres informations, par exemple par calcul ;transformer les informations ;stocker les informations dans le système d'informations, en vue d'effectuer des traitements plus tard ;extraire des informations préalablement stockées.Exemples de traitements :tri, classement, recherche ;calculs de comptabilité, statistiques, analyses de physique ou d'économie ;correction orthographique ;reconnaissance de texte ;reconnaissance vocale ;traitement d'images tels que fausse couleur, négatif.Un composant électronique qui effectue des traitements s'appelle un processeur.La sortie est l'étape finale des traitements qui consiste à faire sortir les résultats du système d'informations.Selon leur nature, les informations des résultats peuvent être restituées sur un écran, du papier par une imprimante ou un traceur, des enceintes audio ou tout autre appareil. Les informations peuvent aussi être transportées vers d'autres systèmes par des moyens de télécommunication (voir : réseau informatique).Boîte noireInformatiqueOrdinateurSystème binaireMario Borillo, Informatique pour les sciences de l'homme : limites de la formalisation du raisonnement, Éditions Mardaga, 1984  (ISBN 2-8700-9202-4) Portail de l’informatique"
informatique;"La théorie de l'information, sans précision, est le nom usuel désignant la théorie de l'information de Shannon, qui est une théorie probabiliste permettant de quantifier le contenu moyen en information d'un ensemble de messages, dont le codage informatique satisfait une distribution statistique précise. Ce domaine trouve son origine scientifique avec Claude Shannon qui en est le père fondateur avec son article A Mathematical Theory of Communication publié en 1948.Parmi les branches importantes de la théorie de l'information de Shannon, on peut citer :le codage de l'information ;la mesure quantitative de redondance d'un texte ;la compression de données ;la cryptographie.Dans un sens plus général, une théorie de l'information est une théorie visant à quantifier et qualifier la notion de contenu en information présent dans un ensemble de données. À ce titre, il existe une autre théorie de l'information : la théorie algorithmique de l'information, créée par Kolmogorov, Solomonoff et Chaitin au début des années 1960.L'information est un concept physique nouveau qui a surgi dans un champ technologique. Le concept théorique d'information a été introduit à partir de recherches théoriques sur les systèmes de télécommunication. L'origine de ces recherches remonte aux études entreprises dès la fin du XIXe siècle, en physique et en mathématique par Boltzmann et Markov sur la notion de probabilité d'un événement et les possibilités de mesure de cette probabilité. Plus récemment, avant la Seconde Guerre mondiale, les contributions les plus importantes sont dues à la collaboration des mathématiciens et des ingénieurs des télécommunications, qui ont été amenés à envisager les propriétés théoriques de tout système de signaux utilisé par les êtres, vivants ou techniques, à des fins de communication.À la suite des travaux de Hartley (1928), Shannon (1948) détermine l'information comme grandeur mesurable, sinon observable — car nul n'a jamais vu l'information — et celle-ci devient la poutre maîtresse de la théorie de la communication qu'il élabore avec Warren Weaver.Cette théorie est née de préoccupations techniques pratiques. La société Bell cherche à transmettre les messages de la façon à la fois la plus économique et la plus fiable. Aussi le cadre originel de la théorie est celui d'un système de communications où un émetteur transmet un message à un récepteur à travers un canal matériel/énergétique donné. Émetteur et récepteur ont par hypothèse un répertoire commun, un code qui contient les catégories de signaux utilisables. Ainsi le message codé est transmis, de l'émetteur au récepteur à travers le canal, sous forme de signes ou signaux portés par de la matière/énergie.Ainsi, le concept d'information a été l'objet d'une théorie que la postérité a choisi d'appeler « théorie de l'information » alors qu'il s'agissait, à proprement parler, d'une théorie mathématique de la communication de l'information ; or cette expression est exactement celle de Shannon et Weaver. Cette source de confusion est régulièrement rappelée dans la littérature. On dit, en pareil cas, que l'expression abrégée a été retenue par l'usage ; l'emploi du sigle TMCI clarifierait pourtant bien la situation.Cette théorie mathématique appliquée aux techniques de la télécommunication a été élaborée plus spécialement par Claude Shannon, ingénieur à la Compagnie des Téléphones Bell et reste jusqu'à nos jours la base du concept dit scientifique d'information. Cependant, cette théorie ne pourrait s'appuyer ni sur la forme matérielle/énergétique, ni sur le contenu cognitif des messages émis : leur contenu sémantique est laissé de côté, de même que leur contenant physique, pour ne s'intéresser qu'aux aspects mathématiques et communicationnels.Dans sa conception originale, la théorie de l'information de Shannon s'est limitée à analyser les moyens à mettre en œuvre dans les techniques de télécommunication pour transmettre l'information le plus rapidement possible et avec le maximum de sécurité. Elle s'est donc efforcée de développer des méthodes susceptibles de minimiser la probabilité d'erreur dans la reconnaissance du message. Une notion fondamentale sera nécessaire pour développer ces méthodes : la mesure de l'information, au sens mathématique du terme.Pour Shannon, l'information présente un caractère essentiellement aléatoire. Un événement aléatoire est par définition incertain. Cette incertitude est prise comme mesure de l'information. Une information sera donc uniquement définie par sa probabilité (I = – log p). Donc l'information est la mesure de l'incertitude calculée à partir de la probabilité de l'événement. Shannon a donc confondu la notion d'information et de mesure d'incertitude. Il faut remarquer que dans cette définition l'information est bien synonyme de mesure d'incertitude. Dans cet ordre d'idée, plus une information est incertaine, plus elle est intéressante, et un événement certain ne contient aucune information. En théorie de l'information de Shannon, il s'agit donc de raisonner en probabilité et non en logique pure.L'information de Shannon se mesure en unités binaires dites bits. Le bit peut être défini comme un événement qui dénoue l'incertitude d'un récepteur placé devant une alternative dont les deux issues sont pour lui équiprobables. Plus les éventualités que peut envisager ce récepteur sont nombreuses, plus le message comporte d'événements informatifs, plus s'accroît la quantité de bits transmis. Il est clair que nul récepteur ne mesure en bits l'information obtenue dans un message. C'est seulement le constructeur d'un canal de télécommunication qui a besoin de la théorie, et mesure l'information en bits pour rendre la transmission de message la plus économique et la plus fiable.La notion d'information d'après Shannon est nécessairement associée à la notion de « redondance » et à celle de « bruit ». Par exemple, en linguistique l'information n'est ni dans le mot, ni dans la syllabe, ni dans la lettre. Il y a des lettres voire des syllabes qui sont inutiles à la transmission de l'information que contient le mot : il y a dans une phrase, des mots inutiles à la transmission de l'information. La théorie de Shannon appelle redondance tout ce qui dans le message apparaît comme en surplus. Aussi est-il économique de ne pas transmettre la redondance.L'information chemine à travers un canal matériel/énergétique : fil téléphonique, onde radio, etc. Or, dans son cheminement, l'information rencontre du bruit. Le bruit est constitué par les perturbations aléatoires de toutes sortes qui surgissent dans le canal de transmission et tendent à brouiller le message. Le problème de la dégradation de l'information par le bruit est donc un problème inhérent à sa communication. Ici, l'idée de redondance présente une face nouvelle ; alors qu'elle apparaît comme un surplus inutile sous l'angle économique, elle devient, sous l'angle de la fiabilité de la transmission un fortifiant contre le bruit, un préventif contre les risques d'ambiguïté et d'erreur à la réception.Très vite de multiples applications de la théorie de l'information de Shannon sont apparues dans le domaine des sciences humaines : les modèles mathématiques élaborés ont permis de préciser certains concepts utilisés couramment dans les analyses linguistiques structurales, en même temps qu'ils faisaient apparaître les limites inhérentes à ce type d'analyse et provoquaient des recherches nouvelles (en traduction automatique et en psycho-linguistique). Tandis que se développait un champ scientifique nouveau : la cybernétique.Cependant, une caractéristique majeure de la théorie de Shannon est de donner à la notion d'information (telle que définie par cette théorie) un statut physique à part entière. Effectivement, l'information acquiert les caractères fondamentaux de toute réalité physique organisée : abandonnée à elle-même, elle ne peut évoluer que dans le sens de sa désorganisation, c'est-à-dire l'accroissement d'entropie ; de fait, l'information subit, dans ses transformations (codage, transmission, décodage, etc.), l'effet irréversible et croissant de la dégradation. Par conséquent Shannon définit comme entropie d'information la mesure H (H = – K log p). De façon étonnante, l'équation par laquelle Shannon définit l'entropie de l'information coïncide, à un facteur multiplicatif près, avec l'équation de Boltzmann-Gibbs définissant l'entropie S en thermodynamique (S = – K log p). Cet épisode important a été abondamment commenté.Certains, comme Couffignal, ont soutenu que la coïncidence est sans signification : l'application de la fonction de Shannon à la thermodynamique et à l'information serait un hasard de rencontre de l'application d'une même formule mathématique, sans plus. Certes, il peut y avoir rencontre de deux équations de probabilité provenant d'univers différents.À l'inverse, Brillouin avait prétendu établir une relation logique entre le H de Shannon et le S de Boltzmann, ce que retiennent la plupart des chercheurs qui appliquent la théorie aux disciplines non mathématiques, la biologie en particulier. Selon ce point de vue, il est possible d'inscrire l'information telle que définie par Shannon dans la physique. En effet, il existe une dualité dans le concept d'information reliant l'information à la matière/énergie véhiculant cette information. L'information telle que définie par Shannon s'enracine ainsi dans la physique d'une part, dans les mathématiques d'autre part, mais sans qu'on puisse la réduire aux maîtres-concepts de la physique classique : masse et énergie. Comme le dit Wiener : « l'information n'est ni la masse, ni l'énergie, l'information est l'information », ce qui laisse la porte ouverte à des conceptions diverses, à commencer par celle d'un troisième constituant de l'univers, après la matière et l'énergie précisément !La théorie mathématique de l'Information résulte initialement des travaux de Ronald Aylmer Fisher. Celui-ci, statisticien, définit formellement l'information comme égale à la valeur moyenne du carré de la dérivée partielle (?) du logarithme naturel de la loi de probabilité étudiée.                                          I                          (        ?        )        =                  E                          {                                                                                    [                                                                                    ?                                                  ?                          ?                                                                                      ln                    ?                    L                    (                    X                    ;                    ?                    )                                    ]                                                  2                                            |                        ?                    }                      {\displaystyle {\mathcal {I}}(\theta )=\mathrm {E} \left\{\left.\left[{\frac {\partial }{\partial \theta }}\ln L(X;\theta )\right]^{2}\right|\theta \right\}}  À partir de l'inégalité de Cramer, on déduit que la valeur d'une telle information est proportionnelle à la faible variabilité des conclusions résultantes. En termes simples, moins une observation est probable plus elle est porteuse d'information. Par exemple, lorsque le journaliste commence le journal télévisé par la phrase « Bonsoir », ce mot, qui présente une forte probabilité, n'apporte que peu d'information. En revanche, si la première phrase est, par exemple « La France a peur  », sa faible probabilité fera que l'auditeur apprendra qu'il s'est passé quelque chose, et, partant, sera plus à l'écoute.D'autres modèles mathématiques ont complété et étendu de façon formelle la définition de l'information.Claude Shannon et Warren Weaver renforcent le paradigme. Ils sont ingénieurs en télécommunication et se préoccupent de mesurer l'information pour en déduire les fondamentaux de la Communication (et non une théorie de l'information). Dans Théorie Mathématique de la Communication en 1948, ils modélisent l'information pour étudier les lois correspondantes : bruit, entropie et chaos, par analogie générale aux lois d'énergétique et de thermodynamique. Leurs travaux complétant ceux d'Alan Turing, de Norbert Wiener et de John von Neumann (pour ne citer que les principaux) constituent le socle initial de la théorie du signal  et des « sciences de l'information ».Pour une source X comportant n symboles, un symbole xi ayant une probabilité pi = P(X = xi) d'apparaître, l'entropie H de la source X est définie comme :                    H        (        X        )        =        ?                  ?                      i                                n                                    p                      i                                    log                      2                          ?        (                  p                      i                          )              {\displaystyle H(X)=-\sum _{i}^{n}p_{i}\log _{2}(p_{i})}  Au départ, c'était le logarithme naturel, à base                     10              {\displaystyle 10}  , qui était utilisé. Mais la base                     2              {\displaystyle 2}   est justifiée par un étalonnage. On considère l'expérience probabiliste la plus élémentaire : le tirage aléatoire à deux issues équiprobables, pile ou face, chacune de probabilité                     1                  /                2              {\displaystyle 1/2}  . En imposant que la quantité d'information fournie par l'issue d'un tirage aléatoire à pile ou face soit de                     1              {\displaystyle 1}  , c'est-à-dire que l'on doive avoir                     ?                  log                      a                          ?        (        1                  /                2        )        =        1              {\displaystyle -\log _{a}(1/2)=1}  , on trouve que                     a        =        2              {\displaystyle a=2}  . La valeur de                     1              {\displaystyle 1}   avec cette base                     2              {\displaystyle 2}   du logarithme définit l'unité de mesure de l'information, le shannon (avec une minuscule), couramment appelé le bit (voir article shannon (unité)).Les considérations d'entropie maximale (MAXENT) permettront à l'inférence bayésienne de définir de façon rationnelle ses distributions a priori.L'informatique constituera une déclinaison technique automatisant les traitements (dont la transmission et le transport) d'information. L'appellation « Technologies de l'Information et de la Communication » recouvre les différents aspects (systèmes de traitements, réseaux, etc.) de l'informatique au sens large.Les sciences de l'information dégagent du sens depuis des données en s'appuyant sur les notions de corrélation, d'entropie et d'apprentissage (voir Fouille de données). Les technologies de l'information, quant à elles, s'occupent de la façon de concevoir, implémenter et déployer des solutions pour répondre à des besoins identifiés.Adrian Mc Donough dans Information economics définit l'information comme la rencontre d'une donnée et d'un problème. La connaissance est une information potentielle. Le rendement informationnel d'un système de traitement de l'information est le quotient entre le nombre de bits du réservoir de données et celui de l'information extraite. Les données sont l'aspect coût du système, l'information, l'aspect valeur. Il en résulte que lorsqu'un informaticien calcule la productivité de son système par le rapport entre la quantité de données produites et le coût financier, il commet une erreur, car les deux termes de l'équation négligent la quantité d'information réellement produite. Cette remarque prend tout son sens à la lumière du grand principe de Russell Ackoff qui postule qu'au-delà d'une certaine masse de données, la quantité d'information baisse et qu'à la limite elle devient nulle. Ceci correspond à l'adage « trop d'information détruit l'information ». Ce constat est aggravé lorsque le récepteur du système est un processeur humain, et pis encore, le conscient d'un agent humain. En effet, l'information est tributaire de la sélection opérée par l'attention, et par l'intervention de données affectives, émotionnelles, et structurelles absentes de l'ordinateur. L'information se transforme alors en sens, puis en motivation. Une information qui ne produit aucun sens est nulle et non avenue pour le récepteur humain, même si elle est acceptable pour un robot. Une information chargée de sens mais non irriguée par une énergie psychologique (drive, cathexis, libido, ep, etc.) est morte. On constate donc que dans la chaîne qui mène de la donnée à l'action (données ? information ? connaissance ? sens ? motivation), seules les deux premières transformations sont prises en compte par la théorie de l'information classique et par la sémiologie. Kevin Bronstein remarque que l'automate ne définit l'information que par deux valeurs : le nombre de bits, la structure et l'organisation des sèmes, alors que le psychisme fait intervenir des facteurs dynamiques tels que passion, motivation, désir, répulsion, etc. qui donnent vie à l'information psychologique.Une information désigne, parmi un ensemble d'événements, un ou plusieurs événements possibles.En théorie, l'information diminue l'incertitude. En théorie de la décision, on considère même qu'il ne faut appeler « information » que ce qui est « susceptible d'avoir un effet sur nos décisions ».En pratique, l'excès d'information, tel qu'il se présente dans les systèmes de messagerie électronique, peut aboutir à une saturation, et empêcher la prise de décision.Soit une source pouvant produire des tensions entières de 1 à 10 volts et un récepteur qui va mesurer cette tension. Avant l'envoi du courant électrique par la source, le récepteur n'a aucune idée de la tension qui sera délivrée par la source. En revanche, une fois le courant émis et reçu, l'incertitude sur le courant émis diminue. La théorie de l'information considère que le récepteur possède une incertitude de 10 états.Une bibliothèque possède un grand nombre d'ouvrages, des revues, des livres et des dictionnaires. Nous cherchons un cours complet sur la théorie de l'information. Tout d'abord, il est logique que nous ne trouverons pas ce dossier dans des ouvrages d'arts ou de littérature ; nous venons donc d'obtenir une information qui diminuera notre temps de recherche. Nous avions précisé que nous voulions aussi un cours complet, nous ne le trouverons donc ni dans une revue, ni dans un dictionnaire. Nous avons obtenu une information supplémentaire (nous cherchons un livre), qui réduira encore le temps de notre recherche.Il faut moins d'octets pour écrire « chien » que « mammifère ». Pourtant l'indication « Médor est un chien » contient bien plus d'information que l'indication « Médor est un mammifère » : le contenu d'information sémantique d'un message dépend du contexte. En fait, c'est le couple message + contexte qui constitue le véritable porteur d'information, et jamais le message seul (voir paradoxe du compresseur).Le mot même de « message » n'a d'ailleurs de sens qui si on postule un émetteur (conscient ou non, par exemple un phénomène créant des ondes gravitationnelles) et un récepteur soit réel (LIGO) soit hypothétique (par exemple message d'un naufragé glissé dans une bouteille), en plus des informations de contexte : langue, dictionnaire, grammaire.Considérons N boîtes numérotées de 1 à N. Un individu A a caché au hasard un objet dans une de ces boîtes. Un individu B doit trouver le numéro de la boîte où est caché l'objet. Pour cela, il a le droit de poser des questions à l'individu A auxquelles celui-ci doit répondre sans mentir par OUI ou NON. Mais chaque question posée représente un coût à payer par l'individu B (par exemple un euro). Un individu C sait dans quelle boîte est caché l'objet. Il a la possibilité de vendre cette information à l'individu B. B n'acceptera ce marché que si le prix de C est inférieur ou égal au coût moyen que B devrait dépenser pour trouver la boîte en posant des questions à A. L'information détenue par C a donc un certain prix. Ce prix représente la quantité d'information représentée par la connaissance de la bonne boîte : c'est le nombre moyen de questions à poser pour identifier cette boîte. Nous la noterons I.ExempleSi N = 1, I = 0.Il n'y a qu'une seule boîte. Aucune question n'est nécessaire.Si N = 2, I = 1.On demande si la bonne boîte est la boîte no 1. La réponse OUI ou NON détermine alors sans ambiguïté quelle est la boîte cherchée.Si N = 4, I = 2.On demande si la boîte porte le no 1 ou 2. La réponse permet alors d'éliminer deux des boîtes et il suffit d'une dernière question pour trouver quelle est la bonne boîte parmi les deux restantes.Si N = 2k, I = k.On écrit les numéros des boîtes en base 2. Les numéros ont au plus k chiffres binaires, et pour chacun des rangs de ces chiffres, on demande si la boîte cherchée possède le chiffre 0 ou le chiffre 1. En k questions, on a déterminé tous les chiffres binaires de la bonne boîte. Cela revient également à poser k questions, chaque question ayant pour but de diviser successivement le nombre de boîtes considérées par 2 (méthode de dichotomie).On est donc amené à poser I = log2(N), mais cette configuration ne se produit que dans le cas de N événements équiprobables.Supposons maintenant que les boîtes soient colorées, et qu'il y ait n boîtes rouges. Supposons également que C sache que la boîte où est caché l'objet est rouge. Quel est le prix de cette information ? Sans cette information, le prix à payer est log2(N). Muni de cette information, le prix à payer n'est plus que log2(n). Le prix de l'information « la boîte cherchée est rouge » est donc log2(N) – log2(n) = log2(N/n).On définit ainsi la quantité d'information comme une fonction croissante de N?n avec :N le nombre d'évènements possibles ;n le nombre d'éléments du sous-ensemble délimité par l'information.Afin de mesurer cette quantité d'information, on pose :                    I        =                  log                      2                          ?                  (                                    N              n                                )                      {\displaystyle I=\log _{2}\left({\frac {N}{n}}\right)}  I est exprimé en bit (ou « logon », unité introduite par Shannon[réf. nécessaire], de laquelle, dans les faits, bit est devenu un synonyme), ou bien en « nat » si on utilise le logarithme naturel à la place du logarithme de base 2.Cette définition se justifie, car l'on veut les propriétés suivantes :l'information est comprise entre 0 et ? ;un évènement avec peu de probabilité représente beaucoup d'information (exemple : « Il neige en janvier » contient beaucoup moins d'information que « Il neige en août » pour peu que l'on soit dans l'hémisphère nord) ;l'information doit être additive.Remarque : lorsqu'on dispose de plusieurs informations, la quantité d'information globale n'est pas la somme des quantités d'information. Ceci est dû à la présence du logarithme. Voir aussi : information mutuelle, information commune à deux messages, qui, dans l'idée, explique cette « sous-additivité » de l'information.Supposons maintenant que les boîtes soient de diverses couleurs : n1 boîtes de couleur C1, n2 boîtes de couleur C2…, nk boîtes de couleurs Ck, avec n1 + n2 + … + nk = N. La personne C sait de quelle couleur est la boîte recherchée. Quel est le prix de cette information ? L'information « la boîte est de couleur C1 » vaut log N/n1, et cette éventualité a une probabilité n1/N. L'information « la boîte est de couleur C2 » vaut log N/n2, et cette éventualité a une probabilité n2/N… Le prix moyen de l'information est donc n1/N log N/n1 + n2/N log N/n2 + … + nk/N log N/nk. Plus généralement, si on considère k évènements disjoints de probabilités respectives p1, p2…, pk avec p1 + p2 + … + pk = 1, alors la quantité d'information correspondant à cette distribution de probabilité est p1 log 1/p1 + … + pk log 1/pk. Cette quantité s'appelle entropie de la distribution de probabilité.L'entropie permet donc de mesurer la quantité d'information moyenne d'un ensemble d'évènements (en particulier de messages) et de mesurer son incertitude. On la note H :                    H                  (          I          )                =        ?                  ?                      i            ?            I                                    p                      i                                    log                      2                                            p                      i                                {\displaystyle H\left(I\right)=-\sum _{i\in I}p_{i}\log _{2}\;p_{i}}  avec                               p                      i                          =                                            n                              i                                      N                                {\displaystyle p_{i}={\frac {n_{i}}{N}}}   la probabilité associée à l'apparition de l'évènement i.On considère une suite de symboles. Chaque symbole peut prendre deux valeurs s1 et s2 avec des probabilités respectivement p1 = 0,8 et p2 = 0,2. La quantité d'information contenue dans un symbole est :                              p                      1                          ×                  log                      2                          ?                              1                          p                              1                                                    +                  p                      2                          ×                  log                      2                          ?                              1                          p                              2                                                    ?        0        ,        7219              {\displaystyle p_{1}\times \log _{2}{\frac {1}{p_{1}}}+p_{2}\times \log _{2}{\frac {1}{p_{2}}}\approx 0,7219}  Si chaque symbole est indépendant du suivant, alors un message de N symboles contient en moyenne une quantité d'information égale à 0,72N. Si le symbole s1 est codé 0 et le symbole s2 est codé 1, alors le message a une longueur de N, ce qui est une perte par rapport à la quantité d'information qu'il porte. Les théorèmes de Shannon énoncent qu'il est impossible de trouver un code dont la longueur moyenne soit inférieure à 0,72N, mais qu'il est possible de coder le message de façon que le message codé ait en moyenne une longueur aussi proche que l'on veut de 0,72N lorsque N augmente.Par exemple, on regroupe les symboles trois par trois et on les code comme suit :Le message s1s1s1s1s1s2s2s2s1 sera codé 010011110.La longueur moyenne du code d'un message de N symboles est :                                          N            3                          (        0        ,        512        +        3        ×        0        ,        128        ×        3        +        3        ×        0        ,        032        ×        5        +        0        ,        008        ×        5        )        =        0        ,        728        N              {\displaystyle {N \over 3}(0,512+3\times 0,128\times 3+3\times 0,032\times 5+0,008\times 5)=0,728N}  L'une des caractéristiques fondamentales de cette théorie est l'exclusion de la sémantique. La théorie de l'information est indifférente à la signification des messages. Le sens d'un message peut pourtant être considéré comme essentiel dans la caractérisation de l'information. Mais le point de vue de la théorie de l'information se limite à celui d'un messager dont la fonction est de transférer un objet.La théorie de l'information de Shannon est toujours relative à un ensemble de données, une famille de chaînes de caractères, caractérisée par une loi de distribution bien précise. Elle donne donc un contenu en information en moyenne, ce qui en fait une théorie probabiliste, particulièrement bien adaptée au contexte de la transmission de donnée, et dans ce cadre cette théorie a produit des résultats importants. En revanche, elle n'est pas en mesure de quantifier le contenu en information d'une chaine prise isolément, un brin d'ADN par exemple, alors que la théorie algorithmique de l'information en est capable jusqu'à un certain point. Mais cette dernière théorie possède également ses propres limitations. C'est pourquoi il ne faut pas considérer que la notion d'information est entièrement cernée par la théorie de l'information de Shannon, ou la théorie algorithmique de l'information, mais que cette notion a besoin d'une variété de modélisations formelles pour s'exprimer.L'information de Fisher semble ainsi parfois avantageusement remplacer l'information de Shannon dans la mesure où elle est une quantification locale et non globale de l'information contenue dans une distribution. Cela dit, les deux notions sont liées et peuvent dans diverses applications mener aux mêmes résultats.Léon Brillouin Science et théorie de l'information, J. Gabay, 2000  (ISBN 2876470365)Léon Brillouin Science and information theory (typographie plus lisible, mais version en anglais)(en) [PDF] C. E. Shannon « A Mathematical Theory of Communication », sur L’Institut d’électronique et d’informatique Gaspard-Monge (Reprinted with corrections from The Bell System Technical Journal, Vol. 27, p. 379–423, 623–656, July, October, 1948.)Claude Shannon et Warren Weaver, Théorie mathématique de la communication, Paris, Cassini, coll. « le sel et le fer », 2018 (ISBN 978-2-84225-222-9), compte rendu par Olivier Rioul, « Une théorie mathématique de la communication », Bibnum. Textes fondateurs de la science analysés par les scientifiques d'aujourd'hui,? 2018 (lire en ligne)Thomas M. Cover, Joy A. Thomas, Elements of Information Theory, Wiley-Interscience, 2006 (ISBN 978-0-471-24195-9) [détail des éditions](en) David MacKay, Information Theory, Inference, and Learning Algorithms, Cambridge University Press, 2003 (ISBN 0-521-64298-1) [détail des éditions]Théorie algorithmique de l'informationAutorégulationCodage de l'informationCompression de donnéesLoi de Zipf et sa généralisation par MandelbrotProjet:Sciences de l'information et des bibliothèquesSciences de l'information et de la communicationSciences de l'information et des bibliothèquesTechnologies de l'information et de la communicationTraitement de l'informationTransport de l'information« Un cours de théorie de l’information par Louis Wehenkel », sur Université de LiègeVidéo sur l'entropie dans la théorie de l'information Portail de l’informatique   Portail des mathématiques   Portail de l'informatique théorique   Portail des télécommunications"
informatique;"La théorie de la complexité est le domaine des mathématiques, et plus précisément de l'informatique théorique, qui étudie formellement le temps de calcul, l'espace mémoire (et plus marginalement la taille d'un circuit, le nombre de processeurs, l'énergie consommée …) requis par un algorithme pour résoudre un problème algorithmique. Il s'agit donc d'étudier la difficulté intrinsèque des problèmes, de les organiser par classes de complexité et d'étudier les relations entre les classes de complexité.Considérons l'exemple du problème du voyageur de commerce. La donnée du problème est un ensemble de villes et de distances séparant ces villes. L'objectif du problème est de trouver un plus court circuit qui passe une et une seule fois par toutes les villes. Il existe un problème de décision associé : étant donné un ensemble de villes, les distances entre villes et un entier k, déterminer s'il existe un circuit qui passe une et une seule fois par toutes les villes de longueur inférieure à k. Ainsi, on distingue deux types de problèmes.Les problèmes de décision posent une question dont la réponse est oui ou non ; dans le cas du problème du voyageur de commerce : existe-il oui ou non un circuit de longueur inférieure à k ?Les problèmes de recherche d'une solution comportent une question ou plutôt une injonction de la forme « renvoyer un élément tel que… » dont la réponse consiste à fournir un tel élément; dans le cas du problème du voyageur de commerce, exhiber un circuit de longueur minimale. Il s'agit donc d'un problème fonctionnel, et il sera donc catégorisé dans une classe fonctionnelle, par exemple FP si la solution est calculée en temps polynomial.La théorie de la complexité étudie principalement (mais pas uniquement) les problèmes de décision.Un exemple de problème de décision est :« Étant donné un entier n, est-il premier ? » (PRIME)Dans le cadre de la théorie de la complexité, la donnée d'un problème s'appelle une instance. Une instance du problème du voyageur de commerce est la donnée de villes et de distances séparant les villes. Comme on mesure la complexité en fonction de la taille d'une instance, la représentation (le codage) d'une instance joue un rôle important. Par exemple, un entier comme 13 peut être représenté en unaire (IIIIIIIIIIIII) ou en binaire (1101). En unaire, la taille de l'instance 13 est 13 et en binaire, la taille de l'instance est 4 (car il y a quatre chiffres dans 1101).Un problème de recherche peut parfois être transformé en un problème de décision équivalent. Par exemple, le problème du voyageur de commerce qui cherche, dans un graphe dont les arêtes sont étiquetées par des coûts, à trouver un cycle de coût minimum, passant une fois par chaque sommet, peut s'énoncer en un problème de décision comme suit : Existe-t-il un cycle hamiltonien (passant une fois par chaque sommet) de coût inférieur à k (donné dans l'instance) ? L'équivalence de ces deux problèmes doit être démontrée, elle montre que la démonstration d'existence repose sur un argument constructif, c'est-à-dire, par exemple, dans le cas du voyageur de commerce, fournissant effectivement un cycle. Ceci n'est pas vrai en général : par exemple le problème PRIME ci-dessus est soluble en temps polynomial, mais aucune solution (algorithme) polynomiale n'est connue pour le problème de recherche associé : la factorisation, fournir les facteurs d'un nombre naturel.Le problème de rechercher un cycle de coût minimum est équivalent au problème du voyageur de commerce, au sens où si l'on sait résoudre efficacement l'un, on sait aussi résoudre efficacement l'autre. Dans la suite de cet article, nous ne parlerons que de problèmes de décision, mais il existe une branche de la complexité dédiée aux problèmes fonctionnels.Si toute entrée est acceptée, autrement dit si le problème n'a pas de précondition, un problème de décision peut être vu comme l'ensemble des instances positives, c'est-à-dire les instances pour lesquelles la réponse est ""oui"". Par exemple, pour le problème de primalité, le problème de décision est l'ensemble des nombres premiers. Étant donné un codage des instances par des mots, on peut voir un problème de décision comme un langage formel : l'ensemble des mots qui représentent les instances positives.Considérons à présent un algorithme qui résout le problème du voyageur de commerce. L'algorithme exécute des étapes élémentaires de calcul et le nombre d'étapes dépend de la taille de l'entrée. Pour le problème du voyageur de commerce, la taille de l'entrée est la quantité de mémoire nécessaire pour représenter les villes et les distances qui les séparent. Pour mesurer le temps d'exécution d'un algorithme, on définit la complexité en temps qui représente le nombre d'étapes qui sont nécessaires pour résoudre le problème pour une entrée de taille donnée.Bien sûr, il existe de nombreux algorithmes qui résolvent le même problème. La théorie de la complexité s'attache à connaître la difficulté (ou la complexité) intrinsèque d'un problème algorithmique, c'est-à-dire celle de l'algorithme le plus efficace pour ce problème. On classifie les problèmes (et non pas les algorithmes) en termes de classes de complexité.Dans chaque catégorie de problèmes ci-dessus, on dit qu'un problème a une réponse algorithmique si sa réponse peut être fournie par un algorithme. Un problème de décision — donc un problème dont la réponse est soit « oui » soit « non » — est décidable si sa réponse peut être fournie par un algorithme. De la même façon, on dit qu'un problème fonctionnel est calculable si l'élément solution peut être calculé par un algorithme. Il existe aussi des classes de complexité pour les problèmes non-décidables, comme celles de la hiérarchie arithmétique.Pour les problèmes décidables, on cherche à évaluer les ressources – temps et espace mémoire – mobilisées pour obtenir algorithmiquement la réponse.La théorie de la complexité vise à savoir si la réponse à un problème peut être donnée très efficacement, efficacement ou au contraire être inatteignable en pratique, avec des niveaux intermédiaires de difficulté entre les deux extrêmes ; pour cela, elle se fonde sur une estimation — théorique — des temps de calcul et des besoins en mémoire informatique. Dans le but de mieux comprendre comment les problèmes se placent les uns par rapport aux autres, la théorie de la complexité établit des hiérarchies de difficulté entre les problèmes algorithmiques, dont les niveaux sont appelés des « classes de complexité ». Ces hiérarchies comportent des ramifications, suivant que l'on considère des calculs déterministes — l'état suivant du calcul est « déterminé » par l'état courant — ou non déterministes.L'analyse de la complexité est étroitement associée à un modèle de calcul. L'un des modèles de calcul les plus utilisés, car il permet de mesurer le temps de calcul et la mémoire utilisée, est celui de la machine de Turing proposé par Alan Turing en 1936. Un calcul est constitué d'étapes élémentaires. À chaque étape, la machine exécute une action élémentaire (changer d'état interne et déplacer la tête de lecture) en fonction de sa configuration courante (état interne et du symbole lu par la tête de lecture).D'autres modèles de calcul qui permettent d'étudier la complexité existent :machines de Turing non-déterministes (il peut y avoir plusieurs choix possibles d'actions à effectuer dans une configuration donnée) ;machines de Turing alternantes ;machines de Turing probabilistes ;la machine RAM (Random Access Machine) ;les circuits booléens et les programmes straight-line ;les fonctions récursives, dues à Kleene ;le lambda-calcul ;les automates cellulaires ;la logique linéaire ;les pebble games.Les ressources les plus classiques sont le temps et l'espace utilisés.Généralement, on mesure la quantité de ressources (temps, espace, etc.) requis en fonction de la taille de l'entrée (instance). La façon dont cette taille est mesurée joue un rôle crucial dans l'évaluation de la complexité de l'algorithme. Par exemple, pour le problème de tester si un nombre entier naturel est premier, une instance est un nombre entier naturel. La taille d'un nombre entier naturel se mesure généralement par le nombre de chiffres (par exemple le nombre de bits si le nombre est représenté en binaire). Ainsi, le nombre 1 024 peut être représenté avec seulement onze chiffres binaires et quatre chiffres décimaux et donc sa taille est 11 ou 4 ; la taille d'un entier p vaut alors O(log(p)). La théorie de l'information montre qu'on ne peut diminuer la taille davantage. On mesure la quantité de ressources en fonction de cette taille, qui sera notée n. L'évaluation des ressources requises permet de répartir les problèmes dans des classes de complexité.Pour les machines déterministes, on définit la classe TIME(t(n)) des problèmes qui peuvent être résolus en temps t(n), c'est-à-dire pour lesquels il existe au moins un algorithme sur une machine déterministe résolvant le problème en temps t(n). Le temps est le nombre de transitions sur machine de Turing ou le nombre d’opérations sur machine RAM. En fait, ce temps n'est pas une fonction précise, mais un ordre de grandeur. On parle aussi d'évaluation asymptotique. En particulier les constantes multiplicatives sont systématiquement ignorées grâce au théorème de speedup linéaire. Ainsi, pour un temps qui s'évalue par un polynôme, ce qui compte est le degré du polynôme. Si ce degré est 2, on dira que l'ordre de grandeur est en O(n²), que la complexité est quadratique et que le problème appartient à la classe TIME(n²).D'autre mesures existent :nombre de communications (voir la théorie de la complexité de la communication) ;nombre de portes dans un circuit booléen ;profondeur d'un circuit booléen ;nombre d'alternations (voir machines alternantes).Une classe de complexité regroupe les problèmes de même complexité, souvent à une réduction polynomiale près. Les classes usuelles sont définies en utilisant les machines de Turing comme modèles de calcul et les ressources sont le temps et l'espace. Le tableau suivant donne quelques exemples de classes de complexité :On a P ? NP car tout algorithme déterministe est un cas particulier d'algorithme non déterministe. En revanche, la réciproque : NP ? P, qui est la véritable difficulté de l'égalité P = NP, est un problème ouvert fondamental de l'informatique théorique. Il a été posé en 1970 indépendamment par Stephen Cook et Leonid Levin ; il fait partie des listes, établies en 2000, des problèmes du prix du millénaire et des problèmes de Smale.La plupart des spécialistes conjecturent que les problèmes NP-complets ne sont pas résolubles en temps polynomial (donc, que P ? NP). Cela ne signifie pas pour autant que toute tentative de résoudre un problème NP-complet est vaine (voir la section « Résolution » de l'article sur la NP-complétude). Il existe de nombreuses approches (qui se sont finalement révélées irrémédiablement erronées) attaquant le problème P ? NP ; le spécialiste de la théorie de la complexité Gerhard Woeginger maintient une liste de ces erreurs.La revendication de Vinay Deolalikar (6 août 2010), travaillant aux HP Labs (en), d'une démonstration de P ? NP, a été la première à faire l'objet d'une attention relativement importante de nombreux mathématiciens et informaticiens de renom, que ce soit sous la forme d'échanges dans des blogs,,, de journaux en ligne ou sous la forme plus construite d'un projet d'étude collaborative en ligne (du type projet Polymath, tel que promu par les médaillés Fields Terence Tao et Tim Gowers). Cette étude collaborative donne la liste des points où l'approche de Vinay Deolalikar achoppe actuellement.À l'instar du problème P = NP, on ne sait pas par exemple si :L = NL ?L = P ?NP = co-NP ?P = Pspace ?NP = Pspace ?Exptime = NExptime ?La complexité des opérations à réaliser a des conséquences sur leur déroulement concret, notamment la consommation d'énergie nécessaire à leur réalisation. Celle-ci peut varier considérablement suivant la performance des processus utilisés pour effectuer les calculs. En 1961, Rolf Landauer, de la société IBM, a proposé un modèle permettant d'estimer le coût en énergie minimal théorique en donnant une estimation du coût énergétique minimal de changement d'état d'un bit informatique. En date de 2013, cette relation entre complexité et énergie, appelée principe de Landauer, est confirmée par les études expérimentales.Complexité, article général sur la complexitéComplexité de KolmogorovComplexité descriptiveHiérarchie arithmétiqueHiérarchie de GrzegorczykExplosion combinatoireMachine de Turing non déterministeRéduction polynomialeComplexité des preuves, le domaine qui étudie la longueur des preuves des énoncés mathématiques selon les systèmes de preuvesSylvain Perifel, Complexité algorithmique, Ellipses, 2014, 432 p. (ISBN 9782729886929, lire en ligne)Olivier Carton, Langages formels, calculabilité et complexité, 2008 [détail de l’édition] (lire en ligne)(en) Sanjeev Arora et Boaz Barak, Computational Complexity : A Modern Approach, Cambridge University Press, 2009 (ISBN 0-521-42426-7)(en) Christos Papadimitriou, Computational Complexity, Addison-Wesley, 1993 (ISBN 978-0-201-53082-7)(en) Michael R. Garey et David S. Johnson, Computers and Intractability : A guide to the theory of NP-completeness, W.H. Freeman & Company, 1979  (ISBN 0-7167-1045-5)Richard Lassaigne et Michel de Rougemont, Logique et Complexité, Hermes, 1996  (ISBN 2-86601-496-0)Nicolas Hermann et Pierre Lescanne, Est-ce que P = NP ? Les Dossiers de La Recherche, 20:64–68, août-octobre 2005Ressource relative à la recherche : (en) Stanford Encyclopedia of Philosophy (en) Complexity Zoo consulté le 21 avril 2014(en) The Status of the P versus NP Problem : cet article, publié dans les Comm. of the ACM, a été cité par l'article New York Times ci-dessous.(en) « Prizes Aside, the P-NP Puzzler Has Consequences », The New York Times,  7 octobre 2009 Portail de l’informatique   Portail de l'informatique théorique   Portail des mathématiques"
informatique;"Le Langage de Modélisation Unifié, de l'anglais Unified Modeling Language (UML), est un langage de modélisation graphique à base de pictogrammes conçu comme une méthode normalisée de visualisation  dans les domaines du développement logiciel et en conception orientée objet.L'UML est une synthèse de langages de modélisation objet antérieurs : Booch, OMT, OOSE. Principalement issu des travaux de Grady Booch, James Rumbaugh et Ivar Jacobson, UML est à présent un standard adopté par l'Object Management Group (OMG). UML 1.0 a été normalisé en janvier 1997; UML 2.0 a été adopté par l'OMG en juillet 2005. La dernière version de la spécification validée par l'OMG est UML 2.5.1 (2017).UML est destiné à faciliter la conception des documents nécessaires au développement d'un logiciel orienté objet, comme standard de modélisation de l'architecture logicielle. Les différents éléments représentables sont :Activité d'un objet/logicielActeursProcessusSchéma de base de donnéesComposants logicielsRéutilisation de composants.Il est également possible de générer automatiquement tout ou partie du code, par exemple en langage Java, à partir des documents réalisés.UML est un langage de modélisation. La version actuelle, UML 2.5, propose 14 types de diagrammes dont sept structurels et sept comportementaux. À titre de comparaison, UML 1.3 comportait 25 types de diagrammes.UML n'étant pas une méthode, l'utilisation des diagrammes est laissée à l'appréciation de chacun. Le diagramme de classes est généralement considéré comme l'élément central d'UML. Des méthodes, telles que le processus unifié proposé par les créateurs originels de UML, utilisent plus systématiquement l'ensemble des diagrammes et axent l'analyse sur les cas d'utilisation (« use case ») pour développer par itérations successives un modèle d'analyse, un modèle de conception, et d'autres modèles. D'autres approches se contentent de modéliser seulement partiellement un système, par exemple certaines parties critiques qui sont difficiles à déduire du code.UML se décompose en plusieurs parties :Les vues : ce sont les observables du système. Elles décrivent le système d'un point de vue donné, qui peut être organisationnel, dynamique, temporel, architectural, géographique, logique, etc. En combinant toutes ces vues, il est possible de définir (ou retrouver) le système complet.Les diagrammes : ce sont des ensembles d'éléments graphiques. Ils décrivent le contenu des vues, qui sont des notions abstraites. Ils peuvent faire partie de plusieurs vues.Les modèles d'élément : ce sont les éléments graphiques des diagrammes.Une façon de mettre en œuvre UML est de considérer différentes vues qui peuvent se superposer pour collaborer à la définition du système :Vue des cas d'utilisation (use-case view) : c'est la description du modèle vu par les acteurs du système. Elle correspond aux besoins attendus par chaque acteur (c'est le quoi et le qui).Vue logique (logical view): c'est la définition du système vu de l'intérieur. Elle explique comment peuvent être satisfaits les besoins des acteurs (c'est le comment).Vue d'implémentation (implementation view) : cette vue définit les dépendances entre les modules.Vue des processus  (process view) : c'est la vue temporelle et technique, qui met en œuvre les notions de tâches concurrentes, stimuli, contrôle, synchronisation…Vue de déploiement (deployment view) : cette vue décrit la position géographique et l'architecture physique de chaque élément du système (c'est le où).Le pourquoi n'est pas défini dans UML.En UML 2.5, les diagrammes sont représentés sous deux types de vue : d'un point de vue statique ou structurelle du domaine avec les diagramme de structure (Structure Diagrams).D'un point de vue dynamique avec les diagrammes de comportement (Behavior Diagrams) et les diagrammes d’interactions (Interaction Diagrams).Les diagrammes sont dépendants hiérarchiquement et se complètent, de façon à permettre la modélisation d'un projet tout au long de son cycle de vie. Il en existe quatorze depuis UML 2.3. Diagrammes de structure ou diagrammes statiques Les diagrammes de structure (structure diagrams) ou diagrammes statiques (static diagrams) rassemblent :Diagramme de classes (class diagram) : représentation des classes intervenant dans le système.Diagramme d'objets (object diagram) : représentation des instances de classes (objets) utilisées dans le système.Diagramme de composants (component diagram) : représentation des composants du système d'un point de vue physique, tels qu'ils sont mis en œuvre (fichiers, bibliothèques, bases de données…)Diagramme de déploiement (deployment diagram) : représentation des éléments matériels (ordinateurs, périphériques, réseaux, systèmes de stockage…) et la manière dont les composants du système sont répartis sur ces éléments matériels et interagissent entre eux.Diagramme des paquets (package diagram) : représentation des dépendances entre les paquets (un paquet étant un conteneur logique permettant de regrouper et d'organiser les éléments dans le modèle UML), c'est-à-dire entre les ensembles de définitions.Diagramme de structure composite (composite structure diagram) : représentation sous forme de boîte blanche des relations entre composants d'une classe (depuis UML 2.x).Diagramme de profils (profile diagram) : spécialisation et personnalisation pour un domaine particulier d'un meta-modèle de référence d'UML (depuis UML 2.2). Diagrammes de comportement Les diagrammes de comportement (behavior diagrams) rassemblent :Diagramme des cas d'utilisation (use-case diagram) : représentation des possibilités d'interaction entre le système et les acteurs (intervenants extérieurs au système), c'est-à-dire de toutes les fonctionnalités que doit fournir le système.Diagramme états-transitions (state machine diagram) : représentation sous forme de machine à états finis du comportement du système ou de ses composants.Diagramme d'activité (activity diagram) : représentation sous forme de flux ou d'enchaînement d'activités du comportement du système ou de ses composants. Diagrammes d'interaction ou diagrammes dynamiques Les diagrammes d'interaction (interaction diagrams) ou diagrammes dynamiques (dynamic diagrams) rassemblent :Diagramme de séquence (sequence diagram) : représentation de façon séquentielle du déroulement des traitements et des interactions entre les éléments du système et/ou de ses acteurs.Diagramme de communication (communication diagram) : représentation de façon simplifiée d'un diagramme de séquence se concentrant sur les échanges de messages entre les objets (depuis UML 2.x).Diagramme global d'interaction (interaction overview diagram) : représentation des enchaînements possibles entre les scénarios préalablement identifiés sous forme de diagrammes de séquences (variante du diagramme d'activité) (depuis UML 2.x).Diagramme de temps (timing diagram) : représentation des variations d'une donnée au cours du temps (depuis UML 2.3).Un stéréotype est une marque de généralisation notée par des guillemets, montrant que l'objet est une variété d'un modèle.Un classeur est une annotation qui permet de regrouper des unités ayant le même comportement ou structure. Un classeur se représente par un rectangle conteneur, en traits pleins.Un paquet regroupe des diagrammes ou des unités.Chaque classe ou objet se définit précisément avec le signe « :: ». Ainsi l'identification d'une classe X en dehors de son paquet ou de son classeur sera définie par « Paquet A::Classeur B::Classe X ». Modèles d'éléments de type commun Symbolique des modèles d'éléments :                  Fourche (fork).             État initial (initial state).État final (final state).Interface (interface).O?--- sens du flux de l'interface.O)----- est un raccourci pour la superposition de ---?O et O?---. Modèles d'éléments de type relation Généralisation (generalisation).Association (association).    Réalisation.Utilisation. Autres modèles d'éléments Les stéréotypes peuvent dépendre du langage utilisé.Les archétypes.Les profils.UML n'est pas une norme en droit mais un simple standard « industriel » (ou norme de fait), parce que promu par l'OMG (novembre 1997) au même titre que CORBA et en raison de son succès. Depuis juillet 2005, la première version 2.x de UML est validée par l'OMG.Par ailleurs, depuis 2003, l'OMG a mis en place un programme de certification à la pratique et la connaissance d'UML OCUP qui recouvre trois niveaux de maîtrise.S'il existe de nombreux logiciels de modélisation UML, aucun ne respecte entièrement chacune des versions de UML, particulièrement UML 2, et beaucoup introduisent des notations non conformes. En revanche, de nombreux logiciels comportent des modules de génération de code, particulièrement à partir du diagramme de classes, qui est celui qui se prête le mieux à une telle automatisation.Grady Booch, James Rumbaugh, Ivar Jacobson, Le guide de l'utilisateur UML, 2000 (ISBN 2-212-09103-6)Laurent Audibert, UML 2, De l'apprentissage à la pratique (cours et exercices), Ellipses, 2009 (ISBN 978-2729852696)Franck Barbier, UML 2 et MDE, Ingénierie des modèles avec études de cas, 2009 (ISBN 978-2-10-049526-9)Craig Larman, UML 2 et les design patterns, Analyse et conception orientées objet et développement itératif (3e édition), Pearson Education, 2005  (ISBN 2-7440-7090-4)Martin Fowler et al., UML 2.0, Initiation aux aspects essentiels de la notation, 2004 (ISBN 2-7440-1713-2)Pascal Roques, UML 2, Modéliser une application Web, Eyrolles, 2007 (ISBN 2-212-12136-9)Pascal Roques, UML 2 par la pratique, Études de cas et exercices corrigés, Eyrolles, 2006 (ISBN 2-212-12014-1)Jim Conallen, Concevoir des applications web avec UML, Eyrolles, 2000, 288 p. (ISBN 978-2-212-09172-4)Unified ProcessIngénierie dirigée par les modèlesModel Driven ArchitectureATLAS Transformation LanguageObject Constraint LanguageTransformation de modèlesModeling and Analysis of Real Time and Embedded systems(en) UML.org(en) Dernière version de la spécification UML(en) OMG (Object Management Group)(en) Profil UML standardisé par l'ITU-T basé sur le Specification and Description Language Portail de l’informatique   Portail de la programmation informatique"
informatique;"Un informaticien ou une informaticienne est une personne qui exerce un métier dans l'étude, la conception, la production, la gestion ou la maintenance des systèmes de traitement de l'information.La définition générale désigne le technicien ou l'ingénieur spécialiste d'un système informatique. Comme pour le mathématicien, l'informaticien est plus strictement un scientifique diplômé d'un doctorat en informatique. Ses recherches se basent notamment sur la théorie de l'information.Le métier d'informaticien apparaît à la fin du XIXe siècle avec l’émergence de la mécanographie. Celle-ci consiste alors à traiter l'information à l'aide de systèmes électromécaniques. Les mécanographes sont ainsi employés à s'occuper de ces systèmes et ce n'est qu'au milieu du XXe siècle, avec l'arrivée du terme allemand « Informatik » créé par l'ingénieur Karl Steinbuch et repris en France par Philippe Dreyfus, que le terme d'informaticien voit le jour.Dans les années 1960 à 1980, on nomme ainsi informaticien toute personne exerçant un métier en rapport avec l'informatique. Le métier est alors peu valorisé, il est largement exercé par les femmes ingénieures. Depuis le milieu des années 1980 et l'arrivée de micro-ordinateurs dans les foyers, ce domaine est largement investi par les hommes, dans les pays occidentaux. Dans les écoles d'informatique, les femmes forment 15 % des effectifs en 2018. La variété et le peu de rapport des métiers du génie informatique fait tomber le terme informaticien en désuétude en France au profit de noms plus spécifiques : agent d'exploitation, administrateur système, responsable de sécurité, administrateur de sécurité, administrateur réseau, analyste, programmeur, architecte informatique, etc. Ces métiers peuvent concerner le domaine matériel et/ou le domaine logiciel. « Informaticien » est donc un terme générique désignant des métiers très éloignés les uns des autres et propre à une époque où on ne les distinguait pas toujours.Depuis le début du XXIe siècle, le terme est différencié de l’opérateur, qui se sert de l'informatique comme d'un outil destiné à son propre métier. La profession d’informaticien regroupe ainsi tous les corps de métier qui visent à concevoir, à coordonner ou à mettre en œuvre le développement ou le déploiement d'une solution informatisée qui est mise à la disposition des opérateurs appelés alors utilisateurs.Le travail d'un informaticien logiciel (programmeur) est d'utiliser ses connaissances en langages informatiques (Assembleur, Fortran, Pascal, Cobol, RPG, Python, C, C++, Java, C#, Visual Basic, Visual Basic for Applications, HTML, MAD...) afin de concevoir et de superviser le développement d'applications informatiques ou de logiciels.Ce travail se traduit concrètement par différentes activités, souvent liées à l'âge et à l'expérience de l'informaticien logiciel :le débutant, sorti de l'école ou de la faculté, fera généralement du développement ; il code les tâches décrites d'un programme selon les spécifications qui lui sont fournies. Il effectue aussi de la maintenance sur des programmes existants, ainsi que des évolutions (ajouts de fonctionnalités aux programmes, etc.) ;après quelques années, l'informaticien logiciel dispose d'une meilleure connaissance technique et métier. En commençant à encadrer de plus jeunes développeurs, l'une des évolutions possibles est de devenir chef de projet. La connaissance grandissante des techniques et du métier permet à l'informaticien logiciel de conseiller les utilisateurs ou clients afin de les aider à cerner leurs besoins, évoquer des fonctionnalités oubliées, etc. Il propose également des solutions techniques sur lesquelles il tranche avec le client : interface locale sur chacun des postes, interface web partagée, base de données… Il peut ensuite devenir architecte du système d'information ou responsable du système d'information, à la tête de plusieurs projets d'envergure, comme un progiciel de gestion intégré.Formations les plus courantes : école d'ingénieurs en informatique ;diplôme universitaire en informatique ;diplôme universitaire en génie logiciel ;brevet de technicien supérieur en informatique.L'évolution du métier d'informaticien logiciel est soumise à certaines turbulences depuis la fin du XXe siècle. Les fonctions d'un programmeur débutant ne comportent pas toujours de responsabilités de conception et impliquent rarement un rôle de direction de projet. Mais en même temps, les règles d'avancement, les grilles de rémunération en vigueur et les critères de reconnaissance sociale limitent considérablement les possibilités de carrière pour les programmeurs, provoquant un phénomène de « fuite des cerveaux » vers d'autres métiers. Les programmeurs de haut niveau sont donc extrêmement rares; on les trouve principalement dans les cellules de recherche et développement des constructeurs informatiques et éditeurs de logiciels (certains ont d'ailleurs une notoriété internationale). Il est à noter aussi que, dans les autres entreprises, la dévalorisation de la fonction se manifeste notamment par la quasi-disparition du mot programmeur dans les intitulés de fonction des informaticiens qui exercent ce métier (nommés de préférence ingénieurs d'étude, ingénieur de développement, etc.).Les métiers de l'informatique regroupent : Métiers techniques (informaticien) les administrateurs de bases de données ou DBA : chargés du bon fonctionnement d'une base de données et/ou d'un système de gestion de base de données  ;les administrateurs réseau : chargés de gérer les comptes et les machines d'un réseau ;les administrateurs système : chargés de la maintenance applicative des serveurs ;les analystes-programmeurs : chargés de spécifier techniquement les concepts définis par le Concepteur (ou analyste) en composants informatiques ;les architectes de systèmes d’information : chargés de définir la cartographie de systèmes informatiques (logicielle et matérielle) ;les chercheurs : chargés de formaliser les problèmes à résoudre, de développer des algorithmes permettant de les résoudre, de définir de nouvelles structures de données, de nouveaux concepts, de nouveaux langages de programmation ou de nouveaux systèmes informatiques  ;les développeurs : chargés de la programmation au sein du projet ;les pen-testeurs : chargés, avec l'accord de leurs propriétaires, d'attaquer les systèmes informatiques en vue d'évaluer l'efficacité des solutions de sécurité mises en place et d'en proposer des améliorations (voir test d'intrusion) ;les techniciens de maintenance : chargés de l'assistance technique, de la disponibilité des postes de travail, des sauvegardes de données, du déploiement des ordinateurs, etc. Ils doivent veiller au bon fonctionnement du parc informatique et faire de la maintenance ""préventive"" ;les techniciens en télécommunications ; Métiers non-techniques (employés du secteur informatique) les chefs de projets : chargés de la rédaction des cahiers de charges des applications manuelles ou innovantes et à élaborer les résultats informatiques exigés par les demandeurs. Une autre de leurs tâches consiste en la planification des projets, techniquement et en termes de ressources humaines  ;les concepteurs (ou analystes) : chargés d'identifier les besoins des utilisateurs et de les spécifier. Leur rôle consiste en particulier à expliquer les concepts à des experts non informaticiens ;les consultants : chargés par essence d'analyser un environnement, un besoin ou un problème informatiques sur les plans fonctionnel et technique et de proposer un ou plusieurs scénarios d'évolution ou de résolution adéquats. Leur champ d'activité est, en pratique, beaucoup plus variable et vaste : il s'étend du conseil à la gestion de projet, en passant par l'action commerciale ;les directeurs des systèmes d'information chargés d'encadrer l'ensemble de l'activité informatique d'une structure ;les ergonomes informatiques : chargés notamment d'améliorer la convivialité et l'efficacité des interfaces homme-machine ;les rédacteurs techniques : chargés de produire la documentation destinée aux métiers ;les responsables de la sécurité des systèmes d'information : chargés de la sécurité des systèmes d'information ;les techniciens helpdesk : chargés d'assister les usagers par téléphone ;les testeurs ou qualifieurs : chargés de tester le logiciel ou les chaines de programmes produit par les programmeurs, par exemple durant la période de VABF ;les urbanistes : chargés de redéfinir les projets sur le plan fonctionnel ;les Community manager ou les Gestionnaires de communautés, qui animent et fédèrent des communautés sur Internet pour le compte d'une société, d'une marque, d’une célébrité ou d’une institution.les webmasters : chargés de la ligne éditoriale et du contenu des sites Internet ; De nombreux métiers sont apparentés à l'informatique. Parmi ceux-ci, certains peuvent néanmoins être exercés par des autodidactes en informatique, ou par des personnes ne disposant pas spécialement de notions techniques :vendeur de produits informatiques : chargé de conseiller les acheteurs sur les produits informatiques (exemple : vendeur en micro-informatique) ;téléassistant : préposé à l'assistance aux utilisateurs ;commercial ;webmestre gérant le contenu : chargés du suivi éditorial d'un site web, sans responsabilité de maintenance technique ;Rédacteur de documentation : chargé de produire les documents écrits destinés aux utilisateurs ;infographiste : concepteur des outils d'infographie, puis graphiste utilisant l'ordinateur comme outil de création graphique chargés de produire :des icônes, dessins ou diagrammes dans le respect de la charte graphique d'un logiciel,un travail graphique ou artistique qui n'est pas propre au domaine informatique (exemples : mise en couleur, dessin assisté par ordinateur…) ;ingénieur du son : souvent appelé à utiliser l'informatique, en particulier dans les studios d'enregistrements pour l'enregistrement sonore, l'édition, le mixage audio et le mastering.Filières courtes : France : BTS ou BUT (anciennement DUT), licence. Belgique : bachelier en informatique et systèmes, bachelier en informatique de gestion. Suisse : école technique (équivalent au BTS français), Certificat fédéral de capacité d'informaticien, puis brevet fédéral et/ou diplôme fédéral. Québec : diplôme d'études collégiales (DÉC) en techniques de l'informatique (équivalent au BTS français).Filières longues : Tous pays : doctorat. France : master en informatique ou école d'ingénieurs. Belgique : Master en sciences informatiques, master en sciences de l'ingénieur industriel en informatique, ingénieur civil en informatique ou ingénieur civil en informatique et gestion. Suisse : Master en sciences informatiques, EPF (master), HES (master). Québec : baccalauréat en informatique (3 ans d'études), baccalauréat en génie informatique (4 ans d'études universitaires).Entreprises de formation privées, organismes de formation d'état.Isidore de Séville, saint patron des informaticiens.HackerListe d'informaticiens et précurseurs de l'informatiqueSSII - Société de services en ingénierie informatique.MUNCI - Association professionnelle en France fédérant les membres des professions des technologies de l'information.Humour :Dilbert - BD : L'informaticien dans l'entreprise.Commitstrip - BD : Le blog qui raconte la vie des codeurs.The IT Crowd - Série télévisée britannique, suivant la vie d'un service informatique. Portail de l’informatique   Portail du travail et des métiers"
économie;"Un agent économique est, en économie, une personne physique ou morale prenant des décisions qui participent à l'activité économique. Il est l'actant économique principal des modèles économiques. Le périmètre pertinent de définition de l'agent économique dépend des conceptions de l'économie : les courants de pensée économiques les définissent de manière différentes, ainsi que la comptabilité nationale. La question de la définition de l'agent économique est au centre des controverses économiques du XVIIIe siècle. Au sein de l'école physiocrate, François Quesnay crée un système de pensée où il définit l'économie nationale comme peuplée de trois agents économiques : les fermiers, les propriétaires fonciers et les artisans. Il qualifie cette dernière de « classe stérile ». Cette conception est plus tard critiquée par Adam Smith, qui considère les travailleurs et les commerçants comme les agents économiques majeurs.Le marxisme se fonde lui aussi à partir d'une remise en question de la définition des agents économiques. Karl Marx propose une analyse de l'économie avec une bipartition sociale, entre les capitalistes d'un côté, et les travailleurs de l'autre, représentatifs de la bourgeoisie et du prolétariat. Néanmoins, au sein de la classe des capitalistes, Marx distingue deux catégories d'agents économiques : ceux qui produisent des biens de production et ceux qui produisent des biens de consommation. Pour l'école du circuit, les agents économiques doivent être définis de manière proche de la définition de la comptabilité nationale, c'est-à-dire en étant regroupés en pôles fonctionnels. Cette école considère ainsi que les grands agents économiques sont les institutions financières (et notamment les banques), les entreprises, les ménages et les administrations publiques. Chez les keynésiens, chaque agent économique est classé par référence à sa fonction principale dans l'économie. Un entrepreneur appartement à l'agent "" entreprises "" en raison de sa fonction de base bien qu'il peut effectuer des opérations de consommation courante, à titre secondaire, qui concerne l'agent "" ménages "". Une banque effectue, à titre principal des opérations financières est classée donc dans l'agent "" banques "" bien qu'elle peut effectuer des opérations, à titre secondaire, relevant des agents "" ménages "" ou "" entreprises "". On peut faire un raisonnement analogue pour démontrer l'appartenance des individus à l'agent "" ménages "".La macroéconomie considère que l'agent économique pertinent peut être, ou bien un agent représentatif (le ménage moyen ou médian), ou bien une agrégation d'agents économiques, ou bien, dans le cadre d'une étude sectorielle, un groupe homogène. Dans tous les cas, les agents économiques sont agents car ils agissent dans le cadre d'échanges économiques avec d'autres agents,.L'objectif de l'étude menée par l'économiste oriente son choix dans la définition des agents économiques qu'il souhaite étudier.La microéconomie s'intéresse aux décisions prises par les agents économiques. Chaque agent possède des caractéristiques particulières qui permettent aux économistes de prévoir ses décisions. Plutôt qu'être un simple représentant de sa classe ou de son groupe d'appartenance, l'agent microéconomique arbitre entre les choix possibles, pour maximiser son utilité. L'hypothèse qui sous-tend cette conception est celle de la rationalité des agents, qui sont censés effectuer des choix optimaux en s'appuyant sur un calcul coût-avantage.Les néoclassiques considèrent qu'il faut s'intéresser à deux types d'agents économiques : le consommateur et le producteur. Le consommateur offre son travail en échange d'un salaire, qu'il va consommer sur le marché des biens et services. Le producteur achète la force de travail et les capitaux nécessaire à la production, et l'écoule ensuite sur le marché des biens et services. Comme le note la Direction générale du Trésor dans une note longue de 2021, « les modèles traitent les ménages et les entreprises de manière quasi-symétrique malgré leur profonde différence de nature, en les rassemblant dans le concept d'agents économiques ».Les organes de comptabilité nationale regroupent les agents économiques selon leurs fonctions, à l'instar de l'école du circuit. Les catégories les plus simples sont les ménages, les entreprises et le gouvernement. La fonction principale des entreprises et du gouvernement est de produire des biens et services, celle des ménages est de consommer.En France, l'INSEE catégorise les agents économiques en six catégories, aussi appelées unités institutionnelles. Chaque catégorie inclut des sous-divisions.Les modèles économiques théoriques sont le plus souvent basés sur des hypothèses comportementales homogènes, afin de décrire les ensembles économiques de la manière la plus simple. Ainsi les ménages consomment, les entreprises produisent.La théorie économique gagnant en complexité, les économistes affinent leur segmentation des agents économiques pour les différencier selon certains critères comme le revenu, le patrimoine ou l'âge.Agent (fonction publique)Unité institutionnelleÉconomie des institutions[PDF] http://www.newschooljournal.com/files/NSER01/82-94.pdf Duncan Foley, The strange history of the economic agent, 2002Les agents économiques et leurs opérations Portail de l’économie"
économie;"L'allocation des ressources est un concept économique qui concerne l'utilisation des ressources rares et notamment les facteurs de production (travail, capital, matières premières) pour satisfaire à court et long terme les besoins de consommation de la population. Cette allocation sert également à financer des services non marchands comme la justice, la police, certaines infrastructures communes (voirie…) indispensables au fonctionnement de la société.Dans le domaine de l'écologie et de l'évolution, l'allocation des ressources (correspondant à l'allocation de nutriments en nutrition microbienne, fongique, végétale et animale) destinées au développement (caractérisé par le taux de croissance, les patrons d'allocation de la biomasse produite aux différentes parties de l'organisme, la durée de développement et survie…), à la maintenance des fonctions somatiques (nutrition, respiration, régulation, défense, relation…) et reproductives (allocation des ressources à la reproduction (en) caractérisée par la fécondation et la fertilité), reflète l'existence de compromis évolutifs entre différents traits biologiques.L'économie elle-même, en tant que domaine du savoir, a pour rôle d'étudier la façon dont sont allouées (et créées) les ressources rares. Cette allocation demande, dès que l'activité économique atteint une certaine taille et complexité, de définir un mode d'arbitrage autre que la guerre ou la rapine, et donc des institutions sociales adaptées.Cet arbitrage se fait de façon plus ou moins libre Par le biais des prix de marché, le fonctionnement de ces marchés étant eux-mêmes formalisés par des règles de droit.Ou par les administrations d'État, à l'aide de règles ou de lois.Dans un sens plus étroit, l'allocation des ressources peut concerner L'arbitrage entre les divers facteurs de production,Voire les choix et dosages à faire à l'intérieur d'un type de facteur (par exemple allocation des capitaux entre divers investissements : voir ci-dessous ""l'allocation d'actifs"")L'allocation des ressources économiques et financières se double de l'allocation des risques propres à ces domainesDans le cadre de la transition écologique, l'économie circulaire promeut - dans toute la mesure des possibilités techniques et financières disponibles - le recyclage des ressources et en particulier des ressources rares, qui devraient en outre être économisées ou remplacées par des alternatives quand cela est possible ; ce qui pourrait être grandement facilité par une généralisation du principe des écotaxe et de l'écoconceptionL'allocation d'actifs est une technique de gestion de patrimoine ou de portefeuille consistant à revoir périodiquement la composition du portefeuille par ""classes d'actifs"" (autrement dit par types de placement : obligations, actions, produits dérivés…)CorruptionRichesseEvolution de l'allocation à la reproductionOptimisation (mathématiques) Portail de l’économie   Portail de l’écologie   Portail origine et évolution du vivant"
économie;Un bien de consommation est un produit fabriqué destiné au consommateur final. En économie, on le distingue d’un bien de production. Un service ne peut pas être considéré comme un bien.  Biens et services qui se consomment en une seule fois (pain, électricité du logement…)Biens semi-durables : ils durent quelque temps mais s'usent assez facilementBiens durables au plein sens du terme que l'on peut utiliser durant de nombreuses années (réfrigérateur, automobile…)Équipements (machines, moyens de transport…)Produits semi-finisMatières premièresÉnergie (électricité, pétrole…)Services rendus par une entreprise à une autre entrepriseBiens et services marchandsBien intermédiaireConsommation« Industrie des biens de consommation / Biens de consommation », sur le site de l'Insee Portail de l’économie
économie;"Les biens et services marchands sont tous les « produits », l'ensemble des biens matériels et immatériels, appelés « produits » en comptabilité nationale, qui sont destinés à être commercialisés sur un marché.Les biens marchands sont des produits matériels pouvant être vendus et achetés.Les services marchands sont des services payants par ex : coiffeur pour particuliers … services rendus par des entreprises contre le versement d'une rémunération monétaire qui leur permet de dégager un profit. On les oppose aux services non marchands qui sont rendus par les administrations publiques et les associations (au sens large d'institutions sans but lucratif au service des ménages, ou ISBLSM) : souvent payants, ces services ne sont pas considérés comme marchands car ils ne sont pas à l'origine d'un profit de la part de l'unité productive.Un bien matériel est tangible lorsqu'on peut le toucher, le voir.Un service est une ""aide"" donnée à une autre personne en échange de quelque chose. Un bien matériel est un objet que l'on peut acheter. Service exemple : le coiffeur (quelqu'un qui vous rend service : il vous coupe les cheveux (un coiffeur).Les services ne sont pas stockables et sont immatériels, contrairement aux biens matériels. On dit donc que les services sont intangibles.Cependant, fonder la distinction entre bien et service sur la matérialité du bien et l'immatérialité du service peut prêter à confusion, en effet pour fournir un service le producteur a recours souvent à des biens (consommations intermédiaires nécessaires à la production du service). Ainsi, qui irait acheter un service de restauration si on lui servait une assiette vide? Pour éviter toute confusion il convient donc de s'en référer à un critère de distinction plus sûr, les services ne sont pas stockables et donc leur effet sur le consommateur ne peuvent être différés dans le temps. Il ne peut ainsi pas y avoir de décalage dans le temps entre production et consommation. La réparation du plombier et les effets sur les canalisations du client sont synchronisés. Le cours du prof (en tant que service) nécessite la présence de l'élève (l'enregistrement ou la copie d'un cours imprimé crée donc un bien). De même, au restaurant, la fourniture du repas nécessite la présence du gourmand. Au contraire, un sandwich à emporter est un bien et non un service puisqu'un décalage temporel entre production du bien et consommation de celui-ci est possible.Autres exemples : l’hôpital (soins médicaux, opérations…), service de transport… Les trois principaux biens Les biens de consommation finale (répondent aux besoins des particuliers)Les biens intermédiaires (matière première transformée par l'entreprise industrielle)Les biens d'équipement ou de production (biens nécessaires à la production : énergie…) Les trois principaux services Les services aux particuliers (restaurants, administration, coiffeur, école, hôtels, musées, expositions…)La promotion commerciale / La gestion immobilière (agences…)Les services aux entreprises (recherche et développement…)Un service marchand est généralement produit par les entreprises, il est payant. Le producteur bénéficie d'une richesse.Par exemple : un concert, un opéra, un voyage en train…Un service non marchand est gratuit ou quasi gratuit, il est généralement produit par les administrations publiques et les associations : leur objectif est non lucratif, c'est-à-dire que leur objectif principal est de venir en aide aux citoyens et non réaliser des profits. Elles peuvent toutefois produire des biens marchands (T-shirt avec leur logo, badges…). La richesse supplémentaire sera utilisée pour améliorer cette association.Les services gratuits n'ont pas de prix de vente, alors que les services quasi-gratuits ont un prix non significatif, c'est-à-dire nettement inférieur au coût de la production.Exemples de services gratuits : commissariat, l'école publique…Exemples de services quasi-gratuits : l'école privée, l'inscription dans une bibliothèque publique, les transports publics…Le marché des biens et services est le marché sur lequel s'échangent les biens et services marchands, c’est-à-dire qu'il représente l'ensemble de l'offre (quantité des biens et services proposés, théoriquement le prix total de ce qui est à vendre) et de la demande (quantité réclamée par des acheteurs potentiels) de biens et services à un moment donné. Ce marché se subdivise bien sûr en marchés de différents types de produits (comme le marché de l'acier, ou le marché de l'automobile d'occasion).Bien de consommationComptabilité nationaleSecteur marchandServices non marchandsBien immatérielDéfinition de l'InseeProduction des services marchands Portail de l’économie"
économie;"Pour un agent économique (à savoir un individu, un ménage, une association, une entreprise, un État...) ou une entité (à savoir un équipement, un service, un établissement, un projet, une mission, une fonction...) le budget est un document récapitulatif des recettes et des dépenses prévisionnelles déterminées et chiffrées pour un exercice comptable à venir (généralement l'année).La démarche budgétaire peut concerner le domaine de la gestion privée ou publique. Le présent article ne traite que des concepts et des pratiques ayant trait à la gestion privée (voir par ailleurs les articles détaillés : le Budget de l'État ou le Budget de l'État français ou le Budget de l'Union européenne).Le mot provient du gaulois « bouge » puis de l'ancien français « bougette », qui désignait une petite bourse accrochée à la ceinture de l'habit d'une personne, contenant de la menue monnaie lui permettant de faire face aux dépenses prévisibles de la journée. Le mot est exporté au Moyen-Âge en Angleterre, où il désigne ensuite le sac dans lequel le chancelier de l'Échiquier présente au Parlement les comptes et autres pièces justificatives, et il prend progressivement le sens actuel de « budget ». Le mot avait disparu de la langue française depuis plusieurs siècles quand il y revient au XVIIIe siècle par l'intermédiaire de l'anglais, avec son nouveau sens.Le budget n'est pas un document normalisé et ne fait pas partie des documents prévus par la méthodologie de la comptabilité.Ce qui n'empêche pas l'outil « budget » d'être largement utilisé par les gestionnaires : la méthode et la procédure budgétaire se révèlent en effet très utiles dans le cadre d'une entreprise ou d'une organisation pour servir de point d'appui aux tâches de prévision, de pilotage ou de contrôle.Le budget d'exploitation traite les recettes-dépenses qui concernent l'exploitation.Le budget d'investissement traite les recettes-dépenses qui concernent l'investissement.Un budget peut être :indicatif (on fait une estimation, mais la réalisation pourra se révéler différente),impératif (les dépenses doivent être inférieures ou égales aux dépenses prévues, les recettes doivent être supérieures ou égales aux prévisions).Un budget comporte des données chiffrées qui peuvent être exprimées de façon plus ou moins poussée :Chiffrage exprimé uniquement en volume ou uniquement en valeur monétaire ;Double chiffrage comprenant à la fois un volet exprimé en volume, doublé d'un volet exprimé en valeur monétaire.La notion de budget est normalement inséparable d'une période de temps bien définie : soit généralement un exercice fiscal de douze mois (budget dit annuel).De manière à assurer une vision opérationnelle plus concrète pour les opérateurs impliqués dans la mise en œuvre, le budget est souvent assorti d'un calendrier de réalisation (parfois appelé « tableau de marche » ou « tableau de progress control »). Le budget est alors dit « phasé » : en fonction du degré de finesse visé, le chiffrage des recettes-dépenses est ventilé et détaillé par phase de réalisation (période mensuelle, trimestrielle ou semestrielle).Préparer un budget, c'est décaler le travail de la machine vers l'homme comme le préconisait l'économiste Zraffa, il faut constater, planifier et élaborer.Au moment de son établissement, un budget n'a de valeur que si les prévisions affichées sont conformes à la réalité qu'elles sont censées décrire. Les principes comptables généraux doivent être respectés :Principe d'unicitéRien ne doit être oublié : le budget doit récapituler à lui seul l'ensemble des dépenses et recettes concernées. Rien ne doit être ajouté : le budget n'a pas à supporter des dépenses qui ne concernent pas son objet ou se voir crédité de recettes auxquelles il ne peut prétendre.Principes de sincérité et d'intégritéLes évaluations données sont censées être les plus réalistes possible : aucune dépense ne doit être minorée, aucun revenu ne doit être majoré…Principe de prudenceL'erreur, l'incertitude, l'irréalisme et la force majeure font peser un risque sur tout exercice de prévision. Le chiffrage tant des dépenses que des recettes doit les prendre en compte de manière raisonnable.En début d'exercice sont fixées les Estimations Originales (EO). Celles-ci peuvent être établies de manière concertée ou non avec les services chargés de l'exécution. En cas de procédure concertée, la première version des EO est éditée par la direction en mai-juin précédant l'exercice annuel suivant. Les services disposent donc d'une période de deux mois (juillet-août) pour établir leurs propres prévisions dans le cadre général fourni par les EO. Toutes les prévisions des services remontent et sont consolidées en septembre par la direction. La consolidation étant rarement parfaitement en ligne avec les intentions de la direction, une procédure de négociation s'établit en octobre entre la direction et les services concernés. La consolidation définitive pouvant intervenir courant octobre, sinon début novembre.Les estimations originales (EO), une fois validées comme cadre de référence s'imposant à tous les membres de l'organisation, doivent être diffusées en temps utile et expliquées à tous les exécutants concernés et ce, suffisamment avant le démarrage de l'exercice concerné. Ceci implique que cette diffusion intervienne fin novembre - début décembre, de manière à être répercutée sur l'ensemble des parties prenantes présentes à tous les niveaux et dans toutes les unités de l'organisation.C'est l'objet du reporting (en particulier dans les grands groupes où sont mis en œuvre de nombreux budgets). Ceci afin d'avoir en temps presque réel une vision exacte de la réalisation du budget. Tout reporting suppose un suivi chronologique (généralement trimestriel) assez étoffé. Dans la pratique la plus répandue, les estimations originales sont mises à jour de manière « roulante » au terme de chaque trimestre. PERPremière Estimation Révisée, éditée à la fin du premier trimestre d'exécution. Les données réelles du premier trimestre -désormais connues remplacent les données prévisionnelles correspondantes : données TRIM 1 réel + données TRIM 2, 3 et 4 prévisionnel.SERSeconde Estimation Révisée, éditée à la fin du second trimestre d'exécution : données TRIM 1 et 2 réels + données TRIM 3 et 4 prévisionnels TERTroisième Estimation Révisée, éditée à la fin du troisième trimestre d'exécution : données TRIM 1,2 et 3 réels + données TRIM 4 prévisionnelet ainsi de suite, jusqu'à QER, la Quatrième Estimation Révisée, éditée en fin de période budgétaire, ne comprend que des données réelles de l'exercice.L'intérêt de cette actualisation progressive est de pouvoir comparer en base annuelle (sur 12 mois) les estimations initiales avec la part de réalisé tel que connu au fur et à mesure de l'avancement de l'exercice.L'analyse des écarts (écart-volumes ou écarts-prix) fournit des indications précieuses sur l'existence de décalages plus ou moins importants entre le prévu et le réalisé. Ces écarts sont mesurés et localisés. Le pilote du budget est en mesure de décider selon l'ampleur des éléments constatés et la meilleure prévision qu'il peut faire pour la suite des événements :soit des ajustements marginaux avec rappel à l'ordre des exécutants ;soit des corrections plus significatives lorsque les écarts constatés font apparaître que la réalisation d'un objectif est devenu hors de portée. Le pilote peut choisir de réviser ses prévisions. On parle alors d'un budget révisé. Il convient cependant de manier ces révisions avec précaution car un usage trop fréquent des révisions discrédite la notion de Budget en tant qu'indicateur d'objectifs anticipés et partagés.Dans la période suivant immédiatement la clôture d'un exercice budgétaire, un bilan-évaluation complet doit être tiré pour dégager et capitaliser toutes les leçons utiles pour les exercices ultérieurs.Certaines organisations — à des fins de « motivation » — pratiquent un double budget. Le premier (réservé à la direction) est le « vrai » budget. Le second (plus largement diffusé auprès des collaborateurs, notamment commerciaux) comporte des objectifs plus ambitieux que ceux contenus dans la version direction. Un tel usage de la démarche budgétaire relève d'une démarche manipulatoire et peut se révéler à terme contre-productif : les objectifs irréalistes finissent par démobiliser.Le suivi de budget implique une logistique particulière : la « comptabilité budgétaire » afin de s'assurer que les recettes et/ou dépenses prévues s'effectuent :dans les volumes et valorisations prévues ;selon le calendrier prévu (dans la mesure où le budget a été « phasé »).Ce suivi comptable ouvre la possibilité du « contrôle budgétaire » via l'analyse des écarts :Évaluation des « écarts-volume » ou des « écarts-prix » éventuellement constatés entre prévisions et réalisations ;Opportunité d'effectuer en temps utile des mesures correctrices.Les services budgétaires ont une importance très variable selon les organisations: Bien utilisé, le contrôle budgétaire fournit une aide très appréciable pour le pilotage de toute gestion. C'est la porte d'entrée au domaine plus large du contrôle de gestion particulièrement nécessaire dans les organisations importantes ou complexes.Le contrôle budgétaire peut perdre de son impact lorsqu'il est aux mains d'un scribe docile compilant des chiffres inutilisés ou inutilisables, d'une Cassandre pointant uniquement les risques et dérapages, ou d'un censeur exerçant un droit de veto tatillon ou systématiquement méfiant envers la moindre dépense.BBZ, Budget Base ZéroLes démarches de rationalisation budgétaire pratiquées dans la gestion privée sont à rapprocher de celles de la gestion publique. Des emprunts méthodologiques sont constatés. Voir en particulier :RCB, Rationalisation des choix budgétaires (en anglais, PPBS Planning and Programming Budget Setting)RGPP, Révision générale des politiques publiquesComme le disait Henri Fayol, gérer c’est administrer qui peut signifier prévoir et « prévoir, c’est déjà supputer l’avenir et le préparer : prévoir c’est agir ». Le budget est un outil essentiel de gestion prévisionnelle qui permet de repérer à l’avance les difficultés de choisir les programmes d’activité à partir de l’analyse de l’environnement pour assurer à l’entreprise la rentabilité souhaitée.Le système budgétaire est fondé sur l’idée d’une mise à la disposition des opérationnels d’un certain nombre d’outils utiles au processus de management. Il s’agit :de la définition d’objectifs clairs, précis et réalistes ;de la préparation de plans d’actions économes ;d'un suivi rapide centré sur l’objectif grâce à la communication régulière des résultats à la suite d'un rapprochement entre ce qui fait et ce qui devrait être fait.de moyens ;de la possibilité de contrôle a priori.Le budget est un outil qui permet à l’entreprise d’atteindre ses objectifs notamment en facilitant le couplage de celle-ci avec l’environnement et l’intégration des différentes fonctions.Mais pour que le budget soit efficace, il faut que les préalables ci-après soient remplis :une structure organisation adaptée ou procéder à la structuration de l’organisation ou découpage en centres de responsabilité ;la formulation d’une stratégie ;la prévision des objectifs ;le système de suivi des réalisations au jour le jour ;une possibilité de contrôle a posteriori.C’est ce dernier aspect qui fait du budget un outil d’évaluation. La constatation des écarts constituent ainsi l’évaluation qui est faite action par action. Alors que la nomenclature budgétaire est tout autre.Ressource relative à la santé : (en) Medical Subject Headings  Portail du management   Portail de la finance   Portail de l’économie"
économie;"Le capitalisme est un système économique caractérisé par la propriété privée des moyens de production et la liberté de concurrence. Par extension, le terme peut également désigner l'organisation sociale induite par ce système ou un système fondé sur l'accumulation du capital productif fondé sur la recherche du profit. Les acteurs du système capitaliste sont les individus, des entreprises, des associations, des fondations voire l’État quand il assume un rôle économique.Les économistes, les sociologues et les historiens ont adopté des perspectives différentes dans leurs analyses du capitalisme et en ont reconnu diverses formes dans la pratique dont le capitalisme de laissez-faire, l'économie sociale de marché ou le capitalisme d'État. Les différentes formes de capitalisme présentent des degrés variables de marché, de propriété privée, d'obstacles à la libre concurrence et d'implication de l'État à travers les politiques sociales et sont du ressort des politiques et de la loi. La plupart des économies capitalistes existantes sont des économies mixtes, qui combinent des éléments de libre marché avec l'intervention de l'État et, dans certains cas, une certaine forme de planification économique.Le système capitaliste a connu une diffusion croissante depuis la révolution industrielle et est actuellement le système économique de la plupart des pays de la planète à la suite de l'échec du modèle marxiste et communiste avec l'effondrement de l'URSS en 1991. Il s'est développé historiquement à partir de l'Italie à la fin du Moyen Âge avant de se diffuser en Europe, en Amérique du Nord et de se diffuser dans le reste du monde notamment à partir du XIXe siècle. Source de développement économique et de croissance, il a permis l'industrialisation précoce de l'Angleterre et ensuite d'autres pays européens et de l'Amérique du Nord. Plusieurs types de critiques ont été faites contre le capitalisme sur le plan de la morale, de la validité des théories économiques, sur le rôle de l'État, le pouvoir des propriétaires du capital, le partage de la valeur ajoutée et du profit, l'organisation du travail ou encore dans le domaine des relations internationales. En outre, l'implication du capitalisme dans de grandes questions sociétales, comme l'impérialisme ou l'environnement, font l'objet de controverses. Pour cette raison, le mot capitalisme est généralement employé à ses origines avec une connotation critique ou une volonté de s'opposer à ce système, notamment par les tenants du marxisme, du communisme et de l'anarchisme, tandis que les économistes libéraux préfèrent utiliser l'expression « économie de marché ».Sa définition diffère dans le temps, dans l'espace, et en fonction des sensibilités politiques des personnes qui l'emploient,.Le capitalisme repose sur deux éléments clés : la propriété privée et la liberté d'entreprendre comme source de revenu (dénommé « profit »). Compte tenu du fait que de nombreux systèmes politiques, philosophiques ou religieux, dont bien évidemment le marxisme, s'opposent à l'idée de « profit » soit en totalité, soit lorsqu'il est jugé excessif avec des conséquences critiquables, la compréhension du terme ne peut être dissociée de son contexte d'emploi qui renvoie à des réalités sous-jacentes qui peuvent différer : contextes sociaux et politiques, cadres idéologiques, théories de référence qui pondèrent, combinent et articulent de façon spécifique des concepts ou des mécanismes importants tels que : la recherche du profit ; l'accumulation du capital ; la dissociation de la propriété du capital et du travail ; le salariat ; la régulation par le marché.Le Larousse.fr en propose plusieurs définitions du capitalisme :Le capitalisme peut désigner une société où domine la propriété privée des moyens de production et des entreprises et au sein de laquelle les salariés ne sont pas les propriétaires des entreprises ;Le capitalisme peut aussi renvoyer à un type de production économique fondée sur l'entreprise privée et sur les différentes formes de libertés économiques ;Le capitalisme peut également caractériser un modèle économique où la finance et les grandes entreprises dominent ;Selon la théorie marxiste, le capitalisme désigne une société reposant sur la domination des travailleurs par les capitalistes c'est-à-dire par les propriétaires du capital et qui aboutit à l'exploitation de la plus-valeur par ces derniers au détriment des ouvriers.Le New Palgrave Dictionary of Economics définit le capitalisme comme un système dans lequel les moyens de production sont détenus par des particuliers.Dans son sens moderne, l'étymologie du terme « capitalisme » renvoie à plusieurs définitions :le capital qui peut désigne essentiellement a) le capital productif (entreprises, machines, brevets…), b) le capital financier (les liquidités d'une entreprises ou ses placements), c) le capital immobilier (domicile privé, locaux d'une entreprise) ;le capitaliste comme agent opérationnel ou comme vecteur social ;le capitalisme qui est la façon dont est conduit, pour des fins peu altruistes d'ordinaire, ce jeu constant d'insertion.Le mot capital apparaît au XIIe siècle et désigne alors une quantité d'argent à faire fructifier,,, il provient du mot latin « caput », qui signifie « la tête », à l'origine la tête de bétail (le cheptel),,. Au milieu du XVIIe siècle le mot capitaliste est utilisé par le Hollandische Mercurius pour désigner « une personne possédant du capital » puis en 1788, par le Français Étienne Clavier qui s'interroge : « L'Angleterre a-t-elle l'heureux privilège de n'avoir ni agioteurs, ni banquiers, ni faiseurs de services, ni capitalistes ? » ; ou encore en 1794, sous la plume d'Arthur Young dans son livre Travels in France (1792),.Adam Smith (1723-1790) et Turgot (1727-1781) parlaient surtout des capitalistes et des systèmes socio-économiques construits autour d'eux. Engels et Marx ont surtout utilisé les termes de « mode de production capitaliste » ou d'« économie bourgeoise ».Né à l'origine au XIXe siècle  comme un concept visant à désigner un système économique que ses adversaires voulaient détruire, le terme capitalisme prend très vite une connotation négative, tandis que ses défenseurs préfèrent alors parler d'« économie de marché ». Son usage moderne est attribué à Louis Blanc en 1850 et à Pierre-Joseph Proudhon en 1861 dans sa correspondance privée,. Il acquiert une dimension plus respectable au XXe siècle où il fait l'objet d'études académiques.L'usage des termes « capitaliste » et « capitalisme » fut employé par de nombreux auteurs au cours du XIXe siècle :Plusieurs fois par l'économiste David Ricardo, dans son livre Principles of Political Economy and Taxation (1817).Selon l'Enrichissement de la langue française ; dictionnaire des mots nouveaux de Jean-Baptiste Richard de Radonvilliers publié en 1842, « Capitalisme » signifie « système de capitalisation ».Werner Sombart, économiste allemand en fait usage dans sa description du système de production en 1902.Max Weber, ami proche et collègue de Sombart, utilise également le terme dans son livre L'Éthique protestante et l'esprit du capitalisme en 1904.Selon Braudel, on trouve dès le Moyen Âge des premières manifestations du capitalisme « commercial » en Italie et aux Pays-Bas. Au moment où Venise établit sur l’Europe sa suprématie matérielle à la fin du quatorzième siècle, Florence en devient le centre intellectuel : le commerce maritime, notamment avec l'Orient a enrichi les cités italiennes un siècle après les croisades, tandis que les Pays-Bas, à l'embouchure du Rhin, font le lien entre l'Italie et l'Europe du Nord dominée par la ligue hanséatique. La naissance supposée médiévale du capitalisme s'inscrit ainsi dans le commerce de la laine, achetée en Angleterre par des marchands flamands ou italiens, qui la revendent ensuite dans des ateliers des Pays-Bas, de Florence ou de Venise. Dans les grandes cités, les marchands de draps et de soieries développent ainsi des méthodes de gestion capitalistes. Ils effectuent des ventes en gros, établissent des comptoirs et vendent leurs produits dans l'ensemble des grandes foires européennes.La ville de Florence, en Italie, est un exemple du développement d'un système bancaire : on y trouve très tôt des banquiers qui développent des succursales à travers l'Europe. Parmi eux de grandes familles, telle celle des Médicis, renouvellent les rapports « privilégiés » entre le monde des affaires et le monde politique, nécessairement très liés à l'échelle d'une ville.La création en 1409 de la bourse de Bruges, un hôtel consacré à l'échange de marchandises, lettres de change et effets de commerce, marque un tournant dans le développement des activités financières. La place s'impose rapidement grâce à l'ouverture de son port, à la renommée de ses foires commerciales et au climat de tolérance et de liberté dont profitent marchands et investisseurs de toutes origines. Ce sont ces mêmes atouts qui permettront ensuite à la place d'Anvers (créée en 1460) de se développer au début de la Renaissance.Alors que la théorie économique n'existe pas, à partir du XVIe siècle va émerger un corps de doctrine qui va se formaliser progressivement dans les différents pays où le mouvement émerge : Espagne et Portugal (avec le bullionisme), France (avec le colbertisme), Hollande et Angleterre (avec le commercialisme).Cette pensée est précapitaliste : elle se soucie davantage de la puissance de l'État que du développement de la richesse privée. Souvent la création de monopoles (les concentrations) mis en place par les États ont constitué une plate-forme de compromis entre l'enrichissement des marchands et la mainmise de la puissance publique. Ce fut par exemple le cas des différentes compagnies commerciales telles que la Compagnie des Indes.Max Weber, dans l'Éthique protestante et l'esprit du capitalisme en 1905, considère que l'émergence du capitalisme moderne date de la Réforme. Sur la base d'un constat sociologique, il lie l'esprit du capitalisme moderne à la moralité protestante et le voit donc comme le résultat d'une évolution lente issue de la Réforme, et plus généralement de l'évolution religieuse se faisant dans le sens d'un « désenchantement du monde ».Cette nouvelle éthique se diffuse grâce à l'émergence de nouvelles valeurs : l'épargne, la discipline, la conscience professionnelle. Cette dernière permet par exemple l'apparition d'une élite ouvrière qui, au-delà du salaire, se soucie de la qualité de son œuvre. Le travail serait une fin en soi. En parallèle émerge un personnage emblématique, l'entrepreneur, qui recherche une réussite professionnelle profitable à la société dans son ensemble. L'esprit d'innovation D'après Lewis Mumford, le système technique de la Renaissance annonce le futur économique du monde occidental.Le XVe siècle vit par exemple la mise au point de l'imprimerie à caractères mobiles (la « typographie ») par Gutenberg. Soucieux de préserver autant qu'il se peut les secrets de ses recherches, contraint à des emprunts monétaires importants, il est en quelque sorte l'archétype de l'entrepreneur. Son objectif est de répondre à une demande insatisfaite : la demande de culture des esprits de moins en moins analphabètes de la Renaissance. Au besoin de publications à grande échelle de livres majeurs va rapidement suivre la demande d'une production plus diversifiée. La diffusion de Bibles à usage personnel contribue à l'essor de la Réforme, tandis que celle-ci accroît en retour la demande. En partie permise par les progrès de la métallurgie, la typographie lui fournit en retour des débouchés. Intérêt pour la mécanique, prémices de « standardisation », productions de grandes séries, souci de la « productivité » et esprit d'innovation. S'il faudra bien attendre des avancées similaires dans l'industrie textile pour connaître le décollage industriel, l'imprimerie montre bien que les mécanismes économiques du capitalisme sont plus anciens.Le nouveau système technique qui se met en place à la Renaissance montre certaines caractéristiques du capitalisme moderne comme l'amélioration de la productivité, l'économie de main d'œuvre, l'augmentation de la production en volume et sa diversification ou encore l'investissement. Il s'appuie sur quelques innovations de rupture comme le haut fourneau, l'imprimerie ou le système bielle-manivelle, la montée en puissance des grands secteurs industriels (métallurgie, exploitation minière) et l'utilisation courante d'une source d'énergie (hydraulique). Ce système persistera jusqu'au milieu du XVIIIe siècle. Les évolutions juridiques et monétaires Au XVIIe siècle, la Hollande acquiert d'importants comptoirs en Inde et développe le commerce des épices, du poivre en particulier ; elle s'établit au Japon et commerce avec la Chine. Elle devient le nouveau centre de l'« économie-monde » selon Braudel. En 1602, elle fonde la première Compagnie des Indes orientales : c'est la première grande « société par actions ». Sa durée est permanente (alors que les sociétés précédentes ont une durée de vie calée sur une expédition particulière) et la responsabilité des associés est limitée aux apports (alors qu'auparavant le patrimoine des associés peut être mis en cause intégralement). Ses dividendes s'élevaient parfois à 15 %, voire 25 %. De 3 100 florins, les actions montèrent à 17 000 florins à la fin du siècle. Elles étaient soumises à d'incessantes spéculations, alimentées par les rumeurs les plus infondées, voire des campagnes de désinformation organisées. La Compagnie émet aussi des obligations.En parallèle, l'afflux d'or depuis les colonies d'Amérique provoque à partir du XVIe siècle une stimulation des échanges, un perfectionnement des méthodes de paiement et des techniques monétaires. Les monnaies fiduciaires connaissent une importante expansion, les premiers billets apparaissent. Dans le reste du monde, les échanges restent limités par l'usage de « monnaies métalliques dans l'enfance ».Dans L'Ethique protestante et l'esprit du capitalisme, le sociologue Allemand Max Weber considère que l'émergence du capitalisme aux États-Unis est dû aux calvinistes au XVIIe siècle.L'émergence du capitalisme est plus souvent associée aux prémices de la révolution industrielle, et en particulier au XVIIIe siècle. Émergence du machinisme Les innovations des débuts de la révolution industrielle restent accessibles aux artisans et ne requièrent pas encore la concentration du capitalisme industriel. Ainsi dans l'activité textile :en 1725, Basile Bouchon utilise un ruban perforé pour programmer un métier à tisser ;en 1765, James Hargreaves invente la spinning-jenny qui décuple la productivité du fileur ; 20 000 exemplaires sont vendues avant 1790 ;en 1801, Joseph-Marie Jacquard met au point à Lyon le métier à tisser qui porte son nom, inspiré des idées de Jacques de Vaucanson et de Basile Bouchon. Le dispositif qu'il améliore comporte une bande perforée qui permet de faire jouer plus facilement les fils de chaîne et par là autorise la conduite du métier par une seule personne.On assiste pourtant à de premières grandes concentrations sporadiques, sans lien avec le machinisme mais liées à des productions particulières, comme l'impression sur toile. Cette dernière nécessite des terrains étendus afin de blanchir les toiles, des pièces immenses où les sécher. Elle requiert un outillage diversifié et complexe, et entraîne des stocks importants de toiles et de colorants. Enfin, elle nécessite le regroupement d'ouvriers spécialistes dans des tâches distinctes. Finalement, de nombreuses formes de productions, pas encore mécanisées, entraînent les premières concentrations de capitaux et de main-d'œuvre.Les progrès de l'agriculture capitaliste ont été nécessaires pour alimenter une population dont la croissance exponentielle (elle passe en Grande-Bretagne de 6 à 18 millions entre 1750 et 1850). Innovation juridique Le capitalisme prendra son véritable essor avec la révolution industrielle.Aux États-Unis, depuis la colonisation, la propriété privée des terres a été la règle. Toutefois, la législation américaine a pu se montrer très favorable envers les moins riches et a su, grâce à l'immensité du territoire, faire de la propriété privée de la terre une notion fondamentale défendue par les plus humbles (non esclaves). Une loi de 1862 accorde en effet la propriété privée de 160 arpents aux pionniers. Le Homestead Act, en offrant un jardin à cultiver aux Européens démunis, stimule les flux migratoires vers les États-Unis.En 1795, des juges de Speenhamland, un village de Grande-Bretagne, ont décidé d'accorder des compléments de salaires, voire un revenu minimum aux indigents. Cette décision inspira la Grande-Bretagne tout entière et l'instauration d'un marché du travail, fondé sur l'idée libérale que seul le travail doit être source de revenu, se heurtait à l'idée charitable que quiconque a un « droit de vivre ». Cet obstacle, critiqué par les économistes classiques dont notamment David Ricardo et certaines philosophes utilitaristes fut finalement levé en 1834 avec la disparition des poor laws, lois sur les pauvres[réf. incomplète].En France, le décret d’Allarde des 2 et 17 mars 1791, affirme le principe qu' « Il sera libre à toute personne de faire tel négoce ou d'exercer telle profession, art ou métier qu'elle trouve bon ». La constitution du marché du travail est encadrée en juin 1791 avec la Loi Le Chapelier qui interdit toutes formes de regroupement des travailleurs telles que les corporations, les associations et les coalitions (en langage d'aujourd'hui : les syndicats et grèves).Dans le domaine de la propriété intellectuelle, au Royaume-Uni, la première loi sur les brevets d'invention (statute of monopolies) fut votée par le Parlement anglais en 1623. Depuis la Renaissance, de nombreuses cités reconnaissaient des privilèges aux inventeurs. En France, l'Ancien Régime leur assure aussi des droits. C'est Beaumarchais qui fera, durant la Révolution française, voter des « droits d'auteur ». Le Royaume-Uni de la révolution industrielle se garantira l'exclusivité de ses innovations en empêchant la sortie de toute machine jusqu'en 1843. Croissance du niveau de vie et de la démographie Les historiens s’accordent sur le fait que le niveau de vie sur l’ensemble du globe a peu évolué de l’Antiquité jusqu’au XVIIIe siècle (entre l'an 1 et l'an 1000 l'économie mondiale aurait même décliné), mis à part une embellie en Europe occidentale entre les Xe et XIIIe siècles, annulée par les épidémies et les famines des XIVe et XVe siècles[source insuffisante]. C'est grâce à l'industrialisation que la croissance économique progresse et permet une augmentation du niveau de vie alors inédite dans l'histoire de l'humanité. Période coloniale Dans une thèse datée de 1984, Empire colonial et capitalisme français, histoire d'un divorce, Jacques Marseille se demande si l'empire colonial a été un frein ou un moteur pour le développement du capitalisme français. Selon lui, l'importance de l'empire pour le capitalisme français n'a été qu'une apparence statistique. Certains estiment donc que l'empire n'a pas été la source du progrès économique.Cette théorie s'oppose à celle de Karl Marx sur le rôle du pillage colonial. La troisième voie Il s'agirait de réhabiliter le rôle de l'état, et de définir les moyens de production (terres agricoles, éducation, santé, défense, banque, et autres secteurs clé de l'économie) qui doivent être placés sous contrôle démocratique afin qu'ils répondent aux besoins des populations, et ceux non essentiels qui peuvent être laissés au secteur privé.Selon Alberto Alesina (dans The future of Europe, Reform or Decline (2006)), entre capitalisme et communisme, l'existence d'une voie intermédiaire serait un leurre. Le capitalisme entraînerait à terme une concentration des richesses au sein d'une part de la population de plus en plus réduite, et une paupérisation du reste de la population. Un contrôle et une redistribution via l'intervention de l'État serait alors nécessaire. Échec du système soviétique Les communistes qui n'ont pas adhéré aux conceptions et aux pratiques qui ont instauré les régimes du « bloc communiste », ont dès la fin du XIXe siècle désigné par capitalisme d'État le cas où l'État est propriétaire de tout le capital (ou presque).Depuis la chute du mur de Berlin en 1989, les États communistes ayant pratiqué cette économie, fonctionnent sous un régime de libéralisme économique avec une privatisation pratiquement totale, en général avec une appropriation par les anciennes élites. Une économie mondialisée Au XXIe siècle, le capitalisme se développe encore plus avec la mondialisation à travers le monde qui permet un échange facile des capitaux et marchandises.Le dernier quart du XXe siècle  est marqué par le développement (ou la « libéralisation ») dans certains pays comme la France des marchés financiers et par un fort ralentissement de l’élévation des niveaux de vie dans les pays industriels (à la suite de la baisse de la croissance depuis 1973), mais une accélération de leur croissance surtout en Asie de l'Est (Chine). Les petits actionnaires retrouvent « le chemin de la bourse », l'actionnariat salarié se développe tout en restant très minoritaire, ainsi que les fonds de pension dans les pays anglo-saxons (ou de façon marginale pour la fonction publique française avec PREFON). Le capitalisme face à l'enjeu écologique La production intensive, la recherche de profit, les transactions très importantes dans le monde et la société de consommation sans limite confronte le capitalisme aux enjeux environnementaux. En effet, la forte production épuise les ressources naturelles rapidement.L'histoire des différentes nations depuis le XIXe siècle a mené l'économie de marché à prendre des formes différentes d'un pays à l'autre.Au cours de l'histoire le capitalisme et l'économie de marché prennent des formes différentes en fonction des pays .Selon Michel Albert (dans son ouvrage Capitalisme contre capitalisme, de 1991), « le capitalisme triomphant, après la disparition à l'Est des régimes collectivistes, redevient dangereux et notre avenir se joue désormais entre cette victoire et ce danger, entre les deux modèles résiduels ».La croissance du capitalisme britannique au XIXe siècle a été fortement marquée par un libre-échangisme mêlé à la tradition du mercantilisme commercial. Cette évolution a mené à la constitution d'un empire colonial important et à une insertion très précoce du pays dans la division internationale du travail (la part de la population agricole est devenue largement minoritaire au Royaume-Uni dès le XIXe siècle). Important depuis ses colonies les matières premières, le Royaume-Uni est devenu au XIXe siècle l'« atelier du monde ». Héraut du libéralisme à travers un monde encore protectionniste, le Royaume-Uni a toutefois connu une parenthèse marquée par l'émergence d'un État-providence important à la suite de la Seconde Guerre mondiale, avant de redevenir à partir des années 1980 un des exemples du capitalisme libéral anglo-saxon.Si capitalisme américain et capitalisme britannique sont parfois présentés sous l'étiquette « capitalisme anglo-saxon », leurs histoires respectives sont différentes. Le capitalisme américain a été jusqu'à la fin de la Seconde Guerre mondiale marqué par un protectionnisme important. À la fin du XIXe siècle, le capitalisme américain a connu une concentration importante dans la plupart des branches de l'économie, la constitution des trusts. Ainsi fusionnaient (ou s'alliaient) de leurs côtés les banques, de leurs côtés les compagnies pétrolières, et ainsi de suite. Bien qu'on considère le modèle américain contemporain comme proche de celui du Royaume-Uni, l'État joue un rôle important dans le soutien de ses entreprises comme le montrent les interventions protectionnistes récentes, les commandes importantes à certaines industries… ce qui pousse certains économistes à qualifier les États-Unis de pays mercantiliste.Le capitalisme rhénan se caractérise par la diversité des acteurs qui jouent un rôle économique directs ou indirects. Les entreprises sont en effet co-actrices des décisions économiques avec les syndicats qui sont représentés dans les conseils d'administration des entreprises.Le capitalisme rhénan, qui se pratique en Allemagne et – avec des variantes au Japon – valorise la réussite collective, le consensus et le souci du long terme. Système également caractérisé par un poids majeur des banques (détentrices de près de la moitié des actions des sociétés cotées, et très influentes sur les autres entreprises), et l'influence importante de syndicats puissants.Le modèle allemand est plus récent, du fait même de la constitution plus tardive de l'État allemand (1870). Il est depuis cette origine marqué par une forte prise en charge sociale (depuis Bismarck), une forte intervention de l'État dans les activités économiques, et une concentration importante des entreprises, qui deviennent ainsi des konzern. Cette concentration s'est faite dans une logique différente de celle de la concentration américaine. Elle a consisté dans le rapprochement de secteurs d'activité différents et complémentaires, comme une forte implication du secteur bancaire dans l'ensemble des grandes branches de l'économie. Depuis la fin de la Seconde Guerre mondiale, avec l'essor de la social-démocratie, l'Allemagne a aussi réussi à développer un système syndical où la collaboration entre patronat et représentants du personnel aboutit à un faible taux de grèves contrastant avec le taux de syndicalisation élevé.Dans son ouvrage de 1991, Capitalisme contre capitalisme, Michel Albert a analysé les évolutions des deux grands modèles, le modèle « néo-américain » (ou anglo-saxon) et le modèle « rhénan » (Allemagne, mais aussi pays scandinaves, Autriche, Suisse, et partiellement le Japon).Selon Michel Albert, les performances économiques américaines depuis l'arrivée au pouvoir de Ronald Reagan en 1981 sont à relativiser. La forte croissance du capitalisme américain est selon lui le fait d'acquis des années antérieures à la libéralisation de l'économie opérée par ce président. Par ailleurs, selon lui, l'économie est menacée par la prépondérance des marchés financiers et de leurs exigences.Au contraire le modèle rhénan accorde une part moins importante à l'économie de marché via l'intervention de divers organismes. Les salaires sont par exemple fixés par les conventions collectives, l'ancienneté… Les grandes entreprises ne sont pas considérées comme des biens marchands mais comme une communauté « industrialio-financière » où les banques prennent une responsabilité de long terme. L'économie sociale de marché allemande incarne par ailleurs une synthèse entre le capitalisme et le socialisme. Ce système est moins générateur d'inégalités sociales et fondé sur des équilibres économiques solides (on pense à la rigueur monétaire allemande).Au Japon, les origines du capitalisme se trouvent dans l'intervention vigoureuse de l'État. C'est, en effet, l'État qui, centralisant les anciens revenus des grandes familles féodales, va développer l'industrie sous l'ère Meiji (à partir de 1868) avant de la confier à ces dernières. Les principes du capitalisme nippon sont semblables à ceux du capitalisme allemand dans la façon dont sont concentrées les entreprises. L'État joue toujours un rôle important dans l'économie, notamment via l'intervention du ministère de l'Économie, du Commerce et de l'Industrie. Sur le plan social, les grandes firmes entretiennent avec leurs salariés des rapports fondés sur la sécurité de l'emploi et en retour le dévouement à l'entreprise de la part du salarié.Le capitalisme français s'est développé entre de grands événements collectifs : la Révolution ainsi que les guerres napoléoniennes (Napoléon Ier et Napoléon III) et les deux guerres mondiales, ce dont elle a sans doute plus souffert que d'autres grands pays industriels comme les États-Unis et l'Angleterre du fait que ses territoires du Nord et de l'Est, très importants au plan des mines et de l'industrie ont été le théâtre des combats. La France a ainsi été occupée entre 1815 et 1818 puis en 1871 avec en outre le paiement d'une énorme indemnité de guerre d'environ 5 milliards de francs-or et l'annexion de l'Alsace et de la Moselle par l'Allemagne entre 1871 et 1919 soit pendant près de 50 ans. La France industrielle et agricole a été une nouvelle fois occupée entre 1940 et 1945 avec un transfert massif de ses moyens de productions et de sa production industrielle (Peugeot par exemple) et agricole en Allemagne, ce qui lui a fait prendre un retard industriel correspondant.Par ailleurs, la France a connu deux premières vagues de nationalisations : en 1936 avec le Front populaire (chemin de fer notamment) et après la seconde guerre mondiale (le métro parisien en 1948, l'électricité, Renault, des banques, etc.) qui ont autorisé des auteurs à décrire une « économie mixte » qui comprend également des sociétés semi-publiques comme les SEM (logement ou chauffage urbain, par exemple), les concessions de service public et les délégations de service public, formes emblématiques de l'action publique française.Le capitalisme français s'est donc traduit par des mouvements d'industrialisation chaotiques et depuis les années 1980, une désindustrialisation, notamment en comparaison de l'Allemagne : Sur la période 1995-2012, l’emploi manufacturier est passé en Allemagne de 24 % à 19 % contre 17 % à 12 % en France (sur la population active soit 13,9 % en 2014 sur les emplois effectifs); en 37 ans (1980-2007), la France a perdu 36 % de ses effectifs manufacturiers, soit de 5,3 millions à 3,4 millions en 2007 ou encore une destruction nette de 1.9 million de salariés ; alors que la valeur ajoutée de l’industrie par rapport au PIB est resté stable sur la période outre-Rhin (21 %), cette part s’inscrit sur une tendance baissière en France (de 15 % à 12 %), après avoir culminé en 1980 à 24 % du PIB.Le capitalisme a été analysé par différents courants théoriques qui insistent pour certains sur le rôle des masses ouvrières, sur l'intervention de l'Etat, le marché ou sur le compromis entre intervention de l’État et économie de marché.Le libéralisme et le capitalisme sont distincts même si l'un et l'autre sont souvent confondus. Le libéralisme désigne en effet une théorie économique fondée sur le principe de la libre concurrence, le libre-échange, le respect du droit, l'esprit d'entreprise et sur l’État de droit. En revanche, le capitalisme désigne un système économique et social qui peut reposer sur une forte intervention de l’État, la collusion entre les responsables politiques et les grands dirigeants économiques.L'école néoclassique d'économie voit ainsi dans le capitalisme une coopération générale (la concurrence poussant les acteurs à se positionner au mieux compte tenu des positions des autres) qui inclut les générations passées et futures, et un accroissement de production général qui bénéficierait à tous.Pour les libéraux, il appartient aux entrepreneurs des biens qui sont utilisés comme moyens de production d'apprécier ce qu'ils font de ce capital. Pour les libéraux classiques, comme Tocqueville, par exemple, ou comme plus tard pour Raymond Aron, le marché est un moyen, en aucune façon une fin.Dans un régime l'autorisant, les biens qui forment le capital, peuvent appartenir à des personnes ou à des entreprises privées. Les rôles de détenteur de capitaux, d’apporteur de travail, d’entrepreneur et de consommateur sont dissociés et chacun cherche à satisfaire ses propres objectifs. Un même individu peut jouer simultanément ou successivement plusieurs de ces rôles, en plus de celui de consommateur.Pour les libéraux, le régime capitaliste existe dès lors que les individus ont le droit de posséder et de disposer librement des biens de production et des fruits de leur utilisation, et de pouvoir les échanger librement, sans aucune contrainte, avec d'autres agents. Les propriétaires de moyens de production peuvent déterminer librement leurs actions en réalisant les arbitrages qui leur paraissent les plus pertinents entre les différentes finalités qui leur sont ouvertes :souci de servir les consommateurs,rémunération des collaborateurs et salariés,recherche du profit,accumulation du capital ou d'un patrimoine.Dans cette conception, la recherche du profit et l'accumulation de capital ne sont pas – a priori et de façon dogmatique – les seules finalités offertes aux agents, et ne constituent pas pour tous une obligation ou une priorité exclusive et absolue. Si cette nuance mérite d'être prise en compte, force est de reconnaître que le mobile prioritaire, sinon principal, de l'activité économique n'en demeure pas moins la recherche du profit qui e"
économie;"La comptabilité nationale est une représentation schématique et quantifiée de l'activité économique d'un pays. Elle consiste en une mesure des flux monétaires représentatifs de l'économie d'un pays pendant une période donnée, en principe une année, et les regroupe dans des totaux nommés agrégats, dans un but analytique direct. La comptabilité nationale prend en compte de nombreux indicateurs macroéconomiques, dont le plus important est le PIB (produit intérieur brut), qui correspond à la somme des valeurs ajoutées — auxquels il faut ajouter les impôts nets des subventions sur les produits — des biens et services produits dans un pays donné au cours d'une année. La comptabilité nationale prend en compte de nombreuses informations, contenues dans les documents comptables des entreprises d'une part, mais aussi dans les rapports des institutions administratives. La comptabilité nationale classe ainsi les différents agents économiques en catégories, les secteurs institutionnels, afin de recenser au mieux les différentes informations relatives à l'économie.Les premiers systèmes de comptabilité nationale datent de la Seconde Guerre mondiale, tout d'abord avec l'économiste britannique Keynes qui développe dès 1941 des instruments de mesure de l'économie, puis avec Jan Tinbergen et Wassily Leontief, considérés comme les véritables inventeurs de la comptabilité nationale. La comptabilité nationale s'est ensuite développée dans la plupart des pays développés. Ainsi, dans le cadre du système monétaire européen (SME), les systèmes de comptes nationaux ont été harmonisés autour de normes communes, et les États européens utilisent le même plan comptable : le SEC (système européen de comptabilité).La comptabilité nationale est née de la volonté des États d'intervenir dans une régulation conjoncturelle de l'économie. Selon un article du Figaro en 2009, « l'invention de la comptabilité nationale a été une réponse à la Grande Dépression des années 1930. On ne disposait à l'époque d'aucune statistique générale, en dehors des cours boursiers ou des données de production établies plus ou moins bien par les professions. Dès 1932, avant même l'élection de Roosevelt et le New Deal, le Congrès américain avait demandé à l'économiste Simon Kuznets (couronné par le Prix Nobel en 1971) d'estimer le recul de l'activité globale. Il s'est alors avéré qu'elle avait chuté de 40 % entre 1929 et 1932. »Le premier vrai système de comptabilité nationale fut créé par John Maynard Keynes (qui dirigeait alors la délégation britannique chargée de rédiger les accords de Bretton Woods) en 1941 à la suite de la demande du parlement de Grande-Bretagne. Les collaborateurs de Keynes élaborèrent une série de tableaux illustrant les ressources produites et leur utilisation sous forme de consommation, dépenses publiques, subventions et investissements. En outre, les travaux menés par l'américain Wassily Leontief (« Prix Nobel » d'économie en 1973) et le néerlandais Jan Tinbergen, « Prix Nobel » d'économie en 1969 ont permis de développer des analyses plus proches de celles que nous connaissons aujourd'hui.Les travaux de Richard Stone et de Simon Kuznets sont à l'origine de ce que l'on a baptisé un « modèle normalisé de la comptabilité nationale ».En ce qui concerne les tableaux de synthèse, en particulier le tableau entrées-sorties (TES) le précurseur fut l'économiste d'origine russe naturalisé américain Wassily Leontief.En France, François Quesnay, chef de file de l'école physiocratique, apparaît comme le premier à avoir élaboré un modèle dynamique, en 1758, pour représenter, à une échelle macroéconomique, la comptabilité nationale dans son ensemble. Au xixe siècle, plusieurs économistes ou hommes politiques s'efforcent de quantifier l'activité économique : Lesur dresse un bilan économique de la France en 1817 et y évalue la somme des revenus à cinq milliards ; en 1819, Jean-Antoine Chaptal estime la valeur de la production agricole et manufacturière en s’appuyant sur les données statistiques des préfectures et du cadastre. Des économistes comme François Perroux (également auteur de la théorie des « pôles de croissance ») ont les premiers établi des modèles modernes de comptabilité nationale sous le régime de Vichy et à la Libération. Selon une étude sur le sujet, « ces pionniers aux vues anticipatrices élaborent des outils statistiques et amorcent la réflexion sur la comptabilité nationale, à partir de la fin des années trente, puis pendant l'occupation. Ces économistes non traditionnels (Jean Fourastié) et ces statisticiens de l'Insee (André Vincent, Jacques Dumontier) se joignent ensuite à l'équipe de Jean Monnet à partir de 1945. »La comptabilité nationale a deux vocations principales : modéliser et étudier l'activité économique d'un pays donné pendant une durée précise d'une part, et prévoir l'évolution d'une conjoncture d'autre part. Elle peut ainsi être un outil de prévision pour aider un gouvernement à trouver des solutions ou à relancer la consommation par exemple. Les comptes nationaux sont publiés par trimestre ou par année.La comptabilité nationale est ex-post, elle s'effectue une fois l'année écoulée. Elle se mesure à prix constants, c'est-à-dire qu'elle ne tient pas compte de l'inflation.L'information la plus connue utilisée par la comptabilité nationale est le PIB (Produit intérieur brut). Le PIB est un indicateur macroéconomique nommé agrégat, c’est-à-dire une grandeur globale qui mesure l'activité économique. Il est possible de proposer trois approches du PIB, cependant, on le considère la plupart du temps comme la somme des valeurs ajoutées produites par l'ensemble des unités résidentes, c’est-à-dire les agents économiques effectivement présents sur le territoire pendant au moins 183 jours sur une année.Le PIB a ainsi une triple optique basée sur les grands principes de la comptabilité nationale :la production : PIB = somme des VAB + IP - SUBV. L'approche par la production, met ainsi en relation la somme des valeurs ajoutées brutes, l'impôt sur la production ainsi que les différentes subventions ;la formation de revenu : PIB = RS + EBE + RMB - SUBV + IP, avec RS la rémunération des salariés, EBE l'Excédent brut d'exploitation, RMB les revenus mixtes bruts, SUBV les subventions et IP les impôts sur la production (liés à la production et aux importations) ;la demande : PIB = CF + FBCF + (X-M), avec CF la consommation finale, FBCF la formation brute de capital fixe (l'investissement), X les exportations et M les importations.Le PIB (Produit intérieur brut) ne doit pas être confondu avec le PNB (produit national brut) qui est la somme des revenus primaires reçus effectivement par les agents économiques d'une même nationalité, qu'ils soient situés sur le territoire ou non. On a ainsi la relation PNB = PIB + revenus des facteurs en provenance de l'extérieur - revenus des facteurs versés à l'extérieur.Les différents agents économiques sont regroupés dans différentes branches baptisées unités institutionnelles. Elles constituent les unités de base de la comptabilité nationale.Une unité institutionnelle est un centre de décision autonome pouvant être une personne (ou plusieurs) physique, les économistes disent alors qu'il s'agit d'un ménage, ou une personne morale, c'est-à-dire une entreprise, une administration publique ou une association. Elles sont susceptibles de posséder elles-mêmes des actifs, de souscrire des engagements, de s'engager dans des activités économiques et de réaliser des opérations avec d'autres unités.Ces unités institutionnelles doivent exercer des opérations économiques pendant un an au moins sur le territoire national pour être comptabilisées dans les secteurs institutionnels. Ce territoire est, si on prend l'exemple de la France, la métropole et les départements d'outre-mer, les enclaves territoriales françaises hors du territoire, l'espace aérien, les eaux territoriales et les espaces qui regroupent des ressources appartenant à la France. En revanche, les enclaves étrangères, à l'image de consulats et ambassades présents sur le sol français, ne sont pas considérées comme des unités résidentes.Les unités institutionnelles ayant la même activité principale et la même source principale de revenu sont regroupées en cinq secteurs institutionnels.On distingue cinq secteurs institutionnels résidents :les ménages ;les sociétés non financières (SNF) ;les sociétés financières (SF) ;les administrations publiques (APU) ;les institutions sans but lucratif au service des ménages (ISBLSM).L'ensemble des unités non-résidentes, dans la mesure où elles entretiennent des relations économiques avec des unités résidentes, sont regroupées dans une catégorie appelée reste du monde, parfois baptisée catégorie « plus-un ».La fonction principale des ménages est la consommation à partir de ressources principales obtenues de deux manières :d'une part par la rémunération des facteurs de production, à savoir le travail, la terre, le capital ;d'autre part, par les transferts effectués par d'autres secteurs institutionnels à destination des ménages.Au sein des ménages, on peut distinguer :le ménage « ordinaire » ou « pur », à savoir un ensemble de personnes vivant dans un logement ;le ménage « collectif » qui est constitué par les populations des maisons de retraite, des foyers de travailleurs, etc.On retrouve également dans ce secteur les entreprises individuelles qui sont des unités économiques dont la fonction principale est la production de biens et services pour leur usage final propre. On retrouve ainsi dans cette catégorie les agriculteurs, les artisans, les professions libérales, les petits commerçants, etc.Les sociétés non financières (SNF) regroupent l'ensemble des sociétés et quasi-sociétés dont la fonction principale est de produire des biens et services marchands, c'est-à-dire dont le prix de vente couvre au moins 50 % du coût de production.Les ressources des sociétés et quasi-sociétés non financières sont le résultat de la production et des éventuelles subventions versées par les administrations publiques (collectivités locales).La CN classe actuellement les SNF en trois catégories, selon le contrôle :Les SNF sous contrôle public, c'est-à-dire sous le contrôle de l'État : la SNCF, la RATP… ;Les SNF sous contrôle privé national : Bouygues, Total… ;Les SNF sous contrôle privé étranger : Google France, Toyota France…Les sociétés financières (ou SF) sont constituées par l'ensemble des sociétés et quasi-sociétés dont la principale fonction est d'offrir des services d’intermédiation financière et/ou d'exercer des activités financières auxiliaires. Leurs ressources sont des fonds provenant des engagements financiers.Cinq sous-secteurs institutionnels constituent le secteur institutionnel des sociétés financières :Les banques centrales ;Les autres institutions financières monétaires (la compatibilité nationale y exclut par convention les sociétés d'assurance et les fonds de pension) ;Les intermédiaires financiers ;Les auxiliaires financiers ;Les sociétés d’assurance et les fonds de pension.Les administrations publiques sont regroupées sous le sigle APU. La fonction principale de ces unités institutionnelles est de produire des services non marchands et/ou d'effectuer des opérations de redistribution des revenus ou du patrimoine national. Elles tirent la majeure partie de leurs ressources de contributions obligatoires (impôts).En France, les administrations publiques (APU) se regroupent en trois sous-secteurs :Les APU centrales (APUC) : composées de l'État et des organismes divers APUC (ODAC) ; les universités, le CNRS, l'ANPE… ;Les APU locales (APUL) : régions, départements, communes + OAL (régie de transport municipal, chambre de commerce…) ;Les ASSO (Administration de sécurité sociale) : unités qui distribuent des prestations sociales à partir de cotisations sociales obligatoires + ODASS ; les ressources proviennent des assurances sociales (ex. : hôpitaux publics).Les institutions sans but lucratif au service des ménages (ISBLSM) regroupent diverses structures dont certaines associations (ex. : association de consommateurs, parti politique, syndicat, Église, organisme de charité, etc.). Leurs points communs sont que, d'une part, elles produisent des services pour les ménages, d'autre part, elles sont financées par des cotisations volontaires et parfois par la vente de biens et services marchands, mais dont le but n'est pas d'en tirer de bénéfice.D'un point de vue économique et du fait de la façon dont la comptabilité nationale les prend en compte, les ISBLSM affichent un rôle négligeable ; il en résulte que dans les statistiques globales, leur consommation est ajoutée à celle des ménages. La majorité des organismes à but non lucratif, qui regroupent l'ensemble des entreprises de l'économie sociale, n'est cependant pas regroupée dans cette catégorie des ISBLSM, ce qui contribue à minorer leur importance. Les différentes études menées situent l'importance de l'ensemble du secteur non lucratif (ISBLSM et économie sociale) à environ 10 % des emplois en France.Ce n'est pas un secteur institutionnel et à ce titre on le qualifie parfois de faux secteur, dans la mesure où les opérations ne sont pas décomposées en distinguant des catégories d'agents : il n'y a pas de compte des ménages ou des SNF du reste du monde. Ce secteur « plus un » regroupe ainsi les unités non résidentes qui effectuent des opérations avec l'économie nationale.Les flux sont enregistrés au moment de la réalisation de l'opération. Les flux financiers sont comptabilisés en « flux nets d'acquisition d'actifs » et « flux nets d'engagements contractuels » alors que les autres flux le sont en « emplois » et « ressources ».Il s'agit de l'ensemble des opérations qui concernent la création et l'utilisation des biens et des services.Parmi elles on distingue :La production, qui a évolué dans le temps; les entreprises y jouent un rôle majeur, mais les ménages ainsi que les administrations sont eux aussi considérés comme des producteurs ;La consommation ;La formation brute de capital fixe — FBCF — (c'est-à-dire l'investissement) ;Les opérations avec l'extérieur (c'est-à-dire les importations et les exportations de biens et de services). Ces opérations sont regroupées dans le TRE (tableau des ressources et des emplois).Ce sont les opérations par lesquelles la valeur ajoutée créée par la production est distribuée entre les salariés, les propriétaires d'entreprises et les administrations publiques, puis redistribuée du fait de l'action des administrations publiques (versements d'allocations financées par des prélèvements…).Pour simplifier on peut considérer ici la valeur ajoutée (VA) comme l'ensemble des richesses créées.VA = P - CI : Production - Consommations IntermédiairesUn indicateur, le taux de marge, résume pour l'essentiel la répartition des richesses créées entre les salariés et les propriétaires d'entreprises. Il mesure la part des profits des entreprises (EBE, excédent brut d'exploitation) dans la VA : taux de marge = EBE / VA x 100. Comme la valeur ajoutée se répartit principalement entre salaires et profits, à une hausse du taux de marge correspond une baisse de la part des richesses créées qui revient aux salariés, et une hausse de celle qui revient aux propriétaires des moyens de production (capital).Ces opérations sont regroupées dans le TCEI (tableau des comptes économiques intégrés).Les opérations financières représentent les engagements pris par les agents économiques les uns envers les autres, en contrepartie de monnaie ou de produits. Par exemple les prêts faits par certains représentent des emprunts pour les autres. La comptabilité nationale retrace ces opérations entre les principaux secteurs institutionnels dans le cadre du TOF « tableau des opérations financières ».(Cette partie de l'article fait la liste des principaux comptes. C'est une ébauche à compléter car chacun d'eux reste à présenter). Le compte de production                      P        ?        C        I        =        V        A              {\displaystyle P-CI=VA}  Le compte de production décrit les flux qui composent le processus de production à savoir les consommations intermédiaires qui sont des opérations sur biens et services : son solde est la valeur ajoutée ou la richesse créée. Le compte d'exploitation EBE (excédentaire brut d'exploitation) = Valeur Ajoutée - Salaires - Impôt (production) + Subvention (exploitation)ouEBE= PIB - Salaires - Impôts (production + produit) + Subvention (exploitation + produit) Le compte d'affectation des revenus primaires EBE + Revenus de la propriété reçus + revenus salariés + impôts sur la production - subventions - revenus de la propriété versés = SRPCe compte s'intéresse aux ressources des secteurs c'est la répartition des revenus liés directement au processus de production (revenus primaires). En emploi on a les revenus de la propriété que les secteurs versent. Le compte de distribution secondaire du revenu Srp + Impôts sur le revenu reçus + impôts sur le patrimoine reçus + Prestations sociales reçues + Autres transferts courants reçus+ Cotisations reçues - impôts sur le revenu versés - impôts sur le patrimoine versé - prestations sociales versées - cotisations sociales versés - autres transferts courants versés = RDBCe compte de répartition des revenus secondaires décrit les flux entre les différents secteurs que sont les ménages et les administrations publiques. En ressource de compte les impôts et cotisations sociales sont versés aux administrations publiques. Les ménages reçoivent des prestations sociales. Les autres transferts courants sont versés à l'ensemble des secteurs. En emplois on a les impôts versés et reçus par l'ensemble des secteurs institutionnels. Les cotisations sociales sont versées par les ménages et les entreprises. Le solde obtenu est le revenu disponible brut. Le compte d'utilisation du revenu disponible                     R        D        B        ?        C        F        =        E        B              {\displaystyle RDB-CF=EB}  Ce compte permet de distinguer la part du revenu disponible (RDB=revenu disponible brut) qui sera consacrée à la consommation de biens finaux (CF = consommation finale) de celle qui sera réservée à l'épargne (EB = épargne brute). Ce compte constitue en fait la charnière entre les comptes de résultat (ceux qui représentent des flux) d'une part et les comptes d'accumulation (parfois appelés comptes patrimoniaux et qui représentent des stocks). En effet c'est au départ de l'épargne que se constituent les masses capitalistiques. Le compte de capital Emplois+ FBCF (P51)+ CCF+ VS (P52)+ OV (P53 acquis - cédés)+ AF (NP1 + NP2 acquis - cédés)Ressources+ EB(B8) [solde précédent]+ TC(D9 reçu - D9 versé)Solde : Capacité/Besoin de financement (B9A)FBCF : Formation Brute de Capital Fixe Le compte financier le compte financier mesure la variation de l'actif et le passif financier du secteur institutionnel et du reste du monde.Il permet d'évaluer le patrimoine financier des secteurs institutionnels, en dressant un état de la valeur des actifs détenus et des engagements contractés (passif) à un moment donné. Cette opération a souvent lieu au 31 décembre de l'année.Le TEE est un tableau de synthèse qui donne une présentation simultanée des comptes de flux des secteurs institutionnels et des comptes d'opérations. Il rassemble les opérations économiques et financières de l'économie nationale pour une année donnée. Le TEE permet ainsi de mesurer les résultats économiques globaux, la contribution de chaque secteur institutionnel à ces résultats, ainsi que l'importance des relations entre l'économie nationale et le reste du monde. Il constitue également un outil très important pour la prévision économique.La comptabilité nationale utilise le « tableau économique d’ensemble » (TEE) qui rassemble l’origine et l’utilisation des ressources de chaque secteur (sociétés non financières, instituts de crédit, entreprises d’assurance, administrations publiques, administrations privées, ménages et reste du monde).Il est construit en valeur d'une part, en brut, cvs (corrigé des variations saisonnières) et cjo-cvs (corrigé de l'effet des jours ouvrables et des variations saisonnières) d'autre part. Ainsi que pour le TES, les comptes du TEE ne sont pas publiés.Le TEE se décompose en une succession de lignes et de colonnes qui aboutissent chacune à la mesure d'un solde correspondant. Chaque compte est séparé en emplois (actif) et en ressources (passif). Excepté dans le compte de production, les soldes des différents comptes sont évalués dans les comptes trimestriels tout simplement par solde.Le tableau entrées-sorties distingue les branches et secteurs. La branche est constituée par l'ensemble des activités qui élaborent un produit donné. Ainsi, il y a autant de branches que de produits. Un secteur est constitué par l'ensemble des entreprises ayant la même activité principale. Le TES indique le montant de chaque produit utilisé par les diverses branches de l'économie. Il permet de retrouver l'équilibre pour chaque branche entre les emplois et les ressources. Il permet d'expliquer a posteriori et de simuler a priori les incidences d'une modification des conditions économiques générales.La comptabilité nationale utilise le « tableau entrées-sorties » (TES) qui décrit l’équilibre des opérations sur biens et services pour toutes les branches de l’économie. On entend par branche l’ensemble des unités de production qui fabriquent un même produit. Ainsi le TES permet pour chaque branche et pour l’ensemble de l’économie, de faire ressortir un équilibre entre les emplois et les ressources de la branche. Sa structure repose sur une division par branches et par produits. Il constitue un outil utile aux comptables nationaux. Dans une perspective keynésienne, s’inspirant du tableau économique de Quesnay, le TES a été mis en évidence par l'analyse entrée-sortie de Wassily Leontief pour représenter l’ensemble des opérations des agents économiques au cours d’une période donnée.On va donc tout d’abord rappeler l’égalité de base, puis voir la structure du TES, et enfin son utilité. Rappel de l’égalité de base Ressources=Production_(P) + Importation_(M) + Impôts_(M)Emplois = Consommation intermédiaire (CI) + Consommation finale (CF) + FBCF + Exportations (X) + Variation des stocks (VS)Le TES présente l’équilibre emploi/ressources : P + M = CI + CF + FBCF + X + VSCet équilibre est toujours vérifié dans les comptes en T. La structure du TES En ligne : répartition des produits entre les branches c’est-à-dire le volume de produits utilisés par chaque branche.En colonne : volumes des produits nécessaires à chaque branche pour sa production.Le total des ressources de chaque branche est égal au total des emplois des produits correspondants.Le TES se compose :d'un tableau des emplois intermédiairesd'un tableau des emplois finauxd'un tableau des comptes de productiond'un tableau total des ressources L’utilité du TES Le TES donne une représentation cohérente de la production nationale et permet de représenter les branches qui contribuent le plus à la production nationale. Il permet de faire apparaître le degré d’indépendance des branches en faisant le calcul : (Total des consommations intermédiaires de branche/Production de la branche)*100Ainsi, toute modification de la production dans une branche entraîne des répercussions dans les autres branches.Le TES est aussi un instrument de prévision économique. On peut calculer des coefficients techniques : (Consommation intermédiaire en produit x / Production de la branche y)*100.L’ensemble des coefficients techniques donne une matrice sur laquelle on peut baser des prévisions relativement fiables à court terme. Il est notamment possible de prévoir :l’effet d’entraînement d’une branche sur les autres ;les conséquences sur les branches d’une augmentation globale de la production, des exportations, de la consommation des ménages… ;les conséquences de l’interdépendance des branches (goulets d’étranglement).On peut bien entendu critiquer la difficulté de construction d’un tel tableau pour une économie nationale, ainsi que les erreurs de mesure des grandeurs économiques qu’il renferme.Le TES peut servir de base à la construction d'une matrice de comptabilité sociale, entrée utile pour un modèle d'équilibre général calculable.Abréviations :P : production ;M : importation ;C : consommation ;CI : consommation intermédiaire ;CF : consommation finale ;FBCF : formation brute de capital fixe ;X : exportation ;VS : variation des stocks.Le TOF réunit l'ensemble des statistiques financières relatives aux secteurs institutionnels (SI) et permet d'analyser les aspects financiers de l'économie.En dépit de leur taille et de la masse d'informations qu'ils contiennent, ces tableaux sont d'une structure très simple et leur lecture est assez facile et posée.Comptes nationaux et régionaux de la Belgique publiés par la BNBComptes économiques et financier du CanadaComptes nationaux de la FranceBalance des paiements de la France en 2002Comptes économiques du QuébecDes organismes spécialisés sont chargés de vérifier les comptes nationaux : les Cours des comptes.En France, la loi organique relative aux lois de finances (LOLF), promulguée en août 2001 et mise en œuvre depuis le 1er janvier 2006, modifie en profondeur les finances publiques..La comptabilité nationale est assujettie à un principe de sincérité.Le rapport de la Cour des comptes de juin 2006 fait état de manques de précisions dans le système français de comptabilité nationale :« II ne comprend pas les passifs implicites ; il ignore bon nombre d'actifs ayant une utilité sociale, mais qui ne sont pas valorisés faute d'une valeur marchande de référence ; peu d'actifs incorporels sont recensés ; enfin, il se fonde sur une notion d'actif restrictive, excluant la plus grande partie du capital immatériel – éducation, recherche, santé. »La comptabilité nationale, a été conçue dans les années de reconstruction qui ont suivi la Seconde Guerre mondiale. Il fallait vérifier que le pays retrouvait le niveau de production d'avant guerre, puis qu'il rattrapait celui de l'Amérique. L'attention était focalisée sur le quantitatif, et les contraintes environnementales étaient ignorées.Ainsi conçu, et rigidifié par les institutions de la comptabilité nationale, le PIB ne serait pas adapté à l'économie actuelle, dont le but est différent.Concevoir la comptabilité nationale qui répondrait à des objectifs de développement durable suppose un gros effort intellectuel.Du point de vue environnemental, la comptabilité nationale tient compte actuellement de la consommation de ressources naturelles en tant que consommations intermédiaires.Edith Archambault, La Comptabilité nationale, Economica, 2003,  (ISBN 271784712X)Jean-Paul Piriou, La Comptabilité nationale, Repères, La Découverte, 2004,  (ISBN 2707143367)André Vanoli, Une Histoire de la comptabilité nationale, La Découverte, 2002,  (ISBN 2707137022)Gilbert Abraham-Frois, Économie politique, Economica, 2001],  (ISBN 2717842675) (l'ouvrage comporte une annexe sur la comptabilité nationale, claire et synthétique)Dictionnaire d’économie, J-Y Capul, Olivier Garnier, Hatier, 2005,  (ISBN 2218740591)DJ. Muller, P. Vanhove, PECF. Économie, Dunod, 1999Michel Braibant, Vers un tableau « entrées-sorties » idéal et mondial, Edilivre, 2018  (ISBN 9782414267040)Histoire de la pensée économique Normalisation ÉconométrieComptabilitéPlan comptable Mesure économique Produit intérieur brutTaux d'investissementFormation brute de capital fixe Belgique StatbelBudget fédéral de BelgiqueBanque nationale de BelgiqueBureau fédéral du PlanCour des comptes (Belgique) France InseeBudget de l'État françaisCour des comptes (France)Abrégé de comptabilité nationaleComptes nationaux et régionaux belgesLes définitions de l'InseeUNSTATS Portail de l’économie"
économie;"La dette publique est, dans le domaine des finances publiques, l'ensemble des engagements financiers pris sous formes d'emprunts par un État, ses collectivités publiques et ses organismes qui en dépendent directement (certaines entreprises publiques, les organismes de sécurité sociale, etc.).Tous les pays ont des dettes publiques. Lorsqu'un déficit budgétaire apparaît, il est couvert par l'emprunt, qui, accumulé sur la longue période, se traduit en dette supplémentaire et croissante.La dette prend le plus souvent, de nos jours, la forme d'emprunt d'État auprès du public. Par ailleurs, des banques commerciales, des institutions internationales (Banque mondiale, Fonds monétaire international, banques régionales de développement, institutions) ou d'autres États peuvent accorder des prêts.Un titre de dette publique peut être de court terme (un an ou moins), à moyen terme (jusqu'à dix ans), ou encore à long terme (au-delà de dix ans). Des durées de 30 et 50 ans ne sont pas rares, et des emprunts perpétuels, traduits en rente, continuent d'exister dans certains pays.La capacité de remboursement des emprunts contractés au titre de la dette publique par les États et les collectivités publiques est évaluée par les agences de notation financière.Au sein de la dette publique, on distingue la dette publique intérieure, détenue par les agents économiques résidents de l'État émetteur et la dette publique extérieure, détenue par des prêteurs étrangers.La dette publique se distingue donc, en macroéconomie, de la dette des ménages ou de la dette des entreprises.La croissance des dettes publiques, inégalée en période de paix, nourrit la question de leur soutenabilité. Elle est l'objet de débats nombreux au sein de la science économique. Plusieurs théories, comme celle du supercycle de la dette, sont émises pour expliquer les variations de la dette publique dans le monde. La dette publique est constituée par l'ensemble des engagements financiers des administrations publiques. Les administrations publiques recouvrent un périmètre plus large que l'État, entendu au sens strict. Dans le cas de la norme européenne de comptabilité nationale (SEC 95), elles sont définies comme l’« ensemble des unités institutionnelles dont la fonction principale est de produire des services non marchands ou d'effectuer des opérations de redistribution du revenu et des richesses nationales. […] Le secteur des administrations publiques comprend les administrations publiques centrales, les administrations publiques locales et les administrations de sécurité sociale ».La dette publique n'est donc pas la dette du pays, entreprises et ménages compris, mais seulement celle de l'ensemble des administrations publiques.Elle n'est pas non plus la dette extérieure (ce que l'ensemble des agents économiques, publics et privés, doivent à des agents hors des frontières).La question de les compter ou non comme dette est importante, de par les sommes en jeu.Les comptables, au niveau international, se sont mis d'accord sur la norme IPSAS 25, qui considère les engagements à pension de retraite comme des dettes publiques. Même si le montant à verser est incertain et lointain, il n'y a pas de difficulté de principe ni technique à réajuster chaque année les provisions correspondantes en fonction des évolutions, démographiques ou réglementaires, qui l'imposent, ou des possibilités de meilleure évaluation.Toutefois, seuls une poignée de pays appliquent effectivement cette convention, dont les États-Unis et le Japon.En Europe, la question est posée, notamment pour que les comparaisons d'endettement des pays soient plus justes. Il est bien précisé que cette question comptable ne remet nullement en cause les prérogatives des États en matière d'organisation et de financement de leur système de retraite.En France, le choix a été fait d'attendre une coordination européenne.Plusieurs raisons sont avancées pour résister à cette convention comptable.la difficulté d'évaluation. Les charges de retraites par exemple dépendent de décisions libres des agents (à quel âge prendront-ils leur retraite ?), d'évènements aléatoires (le taux de mortalité des retraités, leur nombre d'enfants, etc.), etc. Toutefois, en comptabilité, un engagement certain mais dont la valeur ou l'échéance sont incertaine(s) se traite par une provision (voir Retraite (comptabilité)).une conception juridique et doctrinale des prérogatives de l’État : contrairement à un acteur ordinaire, qui doit se conformer aux règles légales, l'État a le pouvoir de fixer et de modifier les règles. Rien n'oblige ce dernier à appliquer plus tard les règles qu'il applique aujourd'hui. Il peut, par exemple, baisser à tout moment les pensions, ou allonger la durée de service pour bénéficier d'une pension. De plus, des États considèrent que les engagements à pension ne prennent juridiquement naissance qu'au moment de la mise à la retraite, voire au moment de la liquidation (i.e. la mise en paiement). En d'autres termes, on peut contester jusqu'à l'existence des engagements implicites, à l'égard des agents actuellement en service, voire à l'égard des actuels retraités. Là encore, le fait que l’État puisse modifier les règles n’empêche nullement de comptabiliser l'effet des règles actuelles, les retraites n'étant en rien spéciales à cet égard.lié à la précédente, une conception des pensions de retraites comme système de protection sociale pour tous sous l'égide de l’État), dans le cadre d'un fonctionnement en flux, ne préjugeant en rien du futur et indépendant du passé (avoir cotisé ou pas, par exemple), et qui donc n'a pas de dettes à proprement parler. Ceci par opposition à la conception des pensions comme salaire différé, encore non versé mais dû.une utilité douteuse : dans la mesure où les engagements correspondants sont d'ores et déjà tracés et évalués, et la question de la pérennité du système de retraite est prise en charge, il importerait peu d'en connaitre le stock au sens comptable.un problème de communication financière : on parle de plusieurs fois le PIB, dès lors, faire apparaître de telles sommes au total des dettes publiques (pour les pays que ne le font pas déjà) aurait de nombreuses conséquences politiques, financières et technique (notamment : le bilan doit rester équilibré, il faut donc mettre des actifs en face de ces dettes, mais lesquels ?).Ainsi, pour des engagements (et donc des problèmes de financement et de soutenabilité) strictement identiques, selon queles pensions de retraite publiques sont incluses dans la dette publique ;elles ne le sont pas, mais sont faibles (l'essentiel étant pris en charge par des systèmes privés) ;elles ne le sont pas et sont importantes.La dette publique sera apparemment à des niveaux radicalement différents. Il importe de garder à l'esprit cet aspect lorsqu'on compare les dettes publiques de pays dont les systèmes de retraite et les façons qu'ils ont de comptabiliser leurs engagements en la matière sont différents.Cette dette est généralement calculée de manière brute : les actifs des administrations ne sont pas soustraits au passif. Cela met l'accent sur les engagements pris, indépendamment de ce à quoi ils ont servi, des engagements reçus (mesurés par les actifs financiers), et du patrimoine. D'autres approches sont possibles, pour répondre à d'autres question. On peut considérer notammentla différence entre la dette brute et les actifs financiers (dette nette, bien qu'ils ne s'agisse pas nécessairement d'une dette : pour certains pays c'est une créance nette), qui mesure mieux les engagements financiers de l'État. Plusieurs organismes calculent la dette financière des États du monde en prenant la dette nette. L'OCDE le fait, par exemple.le patrimoine total des administrations. Sachant que les actifs les plus caractéristiques du patrimoine public sont extrêmement difficiles à cerner (comment compter l'environnement, la biodiversité, le capital culturel de la nation) à évaluer (quelle est la valeur des armes nucléaires ?), voire inestimables (que vaut le château de Versailles ou le contenu du Louvre ?) et très peu liquides.La dette publique est, souvent, exprimée en pourcentage du PIB, lorsqu'il s'agit de mesurer son importance économique. Une même dette brute est évidemment très différente selon la richesse totale du pays, tant pour les autorités qui auront plus de facilité à taxer, que pour les contribuables dont la charge sera moindre s'ils sont plus riches et plus productifs. Ce ratio est donc économiquement plus pertinent que le seul encours de la dette publique.La dette brute a plusieurs limites.Elle conduit à présenter comme importantes des opérations qui sont en fait neutres pour le patrimoine public : par exemple un remboursement de dette par une vente d'actifs publics (tels que des terrains ou des participations dans des entreprises), ou inversement une nationalisation payée par une émission de dette.Et par suite, elle mesure mal la soutenabilité de la dette et surestime le problème qu'elle peut poser. Dette explicite et dette implicite La dette ne retient que les engagements financiers « explicites », c'est-à-dire l'ensemble des engagements financiers que l'État s'est engagé explicitement à payer, notamment de manière contractuelle, comme ses emprunts. Cela met de côté :les engagements hors bilan, connus et recensés, mais qu'il est improbable (mais pas impossible) qu'ils se traduisent effectivement par une dépense, comme les garanties que l’État a des opérations économiques dont il espère une bonne fin, et qu'il ne garantit que pour en faciliter et réduire le coût de financement. Ce type d'engagement est recensé hors bilan.les engagements dits « implicites », qui n'ont fait l'objet d'aucune écriture, pas même hors bilan, mais que tout le monde s'attend à ce que l’État les prenne en charge en qualité d'« assureur en dernier ressort ».Cette convention statistique, à l'inverse de la précédente, a tendance à sous-estimer la dette publique et son poids éventuel. Comparaison des niveaux d'endettement public des pays en fonction des définitions retenues Comme on le voit sur ce tableau, le niveau et l'importance relative de la dette publique entre les pays changent en fonction du type de définition retenue. Ainsi, la différence dans les niveaux d'endettement public entre la France et le Royaume-Uni est fortement diminuée si l'on considère la dette nette, où il est de 8 % du PIB, contre près de 25 % pour la dette brute. L'endettement public du Japon est divisé par deux si l'on considère la dette nette plutôt que la brute, passant d'environ 160 % du PIB à environ 80 %, en raison de l'importance des actifs financiers détenus par les administrations publiques, en particulier comme réserves pour le financement des retraites.Une confusion commune consiste à confondre la dette publique avec la dette du pays. Or, l'État n'est qu'un des agents économiques et sa dette ne représente pas la dette du pays tout entier : les entreprises et les ménages s'endettent également. L'endettement intérieur total d'un pays est donc constitué de la somme des dettes des administrations publiques, des ménages et des entreprises.Par exemple, en France, en 2007, l'endettement intérieur total était de 3 600 milliards d'euros, soit 190 % du PIB, et se répartissait ainsi :De ce point de vue, la répartition dans la dette entre agents diffère entre les pays. L'Observatoire français des conjonctures économiques propose d'opposer deux modèles : un modèle anglo-saxon et un modèle européen. Dans le premier modèle, les ménages sont très endettés (leur dette représente 100 % du PIB au Royaume-Uni), épargnent peu et l'État est comparativement moins endetté. Dans le second modèle, les ménages ont un endettement limité, une épargne plus élevée et l'État est comparativement plus endetté : la France se rattache à ce modèle. Les différences dans la structure de l'endettement renverraient partiellement aux fonctions assurées par l'État : dans le second modèle, l'État assure des fonctions plus nombreuses, comme la construction de logements sociaux ou l'éducation, qu'il finance en partie par endettement. Au contraire, dans le modèle anglo-saxon, l'État intervient moins, et les ménages s'endettent pour financer les dépenses qui ne sont pas socialisées par l'État. Autrement dit, le niveau de dette publique dépendrait en partie de la répartition des activités économiques entre les agents.Parmi les objectifs de la gestion de la dette publique, la diminution des rendements obligataires, pour que l'État s'endette à moindre coût. Le rôle des gestionnaires de la dette publique n’est pas de développer ou de mettre en œuvre une « politique de la dette » : ils n’ont presque aucun pouvoir sur les recettes et dépenses des administrations publiques (APU). Mais leur rôle est de gérer au mieux cette dette, c’est-à-dire de minimiser le coût de financement des administrations tout en tenant compte du risque et en respectant un certain nombre de critères (transparence, etc.) :Minimiser le coût de la dette à court, moyen et long terme. Or les possibilités d’agir sur le coût de la dette étant d’autant plus limitées que l’horizon est court et d’autant plus aléatoires que l’horizon est long, le moyen terme est de fait privilégié. Ce coût est représenté par des flux de paiements d’intérêts et de remboursements étalés au cours du temps (de un jour (le lendemain) – sinon de quelques heures – jusqu’à 50 ans). Mais rembourser un million d’euros la semaine prochaine n’est pas du tout identique à rembourser un million d’euros dans 20 ou 40 ans, quand le PIB sera alors, en valeur, probablement beaucoup plus élevé qu’aujourd’hui : deux fois plus si la croissance et l’inflation sont très faibles ; ou trois fois ; ou peut-être dix fois si le pays connaît des crises d’inflation (et même vingt fois si le pays peut maintenir une « croissance à la chinoise » sur plusieurs décennies…).Minimiser les risques à court et moyen terme, en s’assurant que dans un an, deux ans ou ans, les APU puissent faire face à leurs obligations de paiement y compris le remboursement du capital et le paiement des intérêts de la dette, que les remboursements ne soient pas concentrés sur un mois ou une année particulière, etc. En théorie, il faudrait aboutir à un risque zéro de défaut (en réalité, de délai) de paiement.Or on ne peut minimiser à la fois les coûts et les risques. Par exemple, en émettant des obligations indexées sur l’inflation (par exemple, sur un indice des prix à la consommation), ce sont les créanciers, et non les APU, qui s’exposent au risque de poussées inflationnistes et ils demanderont un taux d’intérêt d’autant plus élevé que le risque perçu est grand. De même, en empruntant à court terme, les APU peuvent prévoir précisément ce que l’emprunt leur coûtera, mais s’exposent au risque de devoir emprunter à un taux supérieur plus tard. En allongeant les échéances des emprunts, les APU se mettent mieux à l’abri des chocs économiques et financiers, mais à un coût plus élevé. Etc.Plus la maturité d’un emprunt est longue, plus le taux d’intérêt est élevé. Une méthode utilisée pour réduire le coût de la dette à court terme, quitte à en accroître le risque, est d’échanger des instruments à long terme à « taux longs » contre des instruments à court terme à « taux courts », moins élevés mais plus volatils. Ces opérations de contrats d’échanges de taux d’intérêt ou swaps peuvent également concerner l’échange d’instruments à taux variables contre d’autres à taux fixes – ou l’inverse. C’est aux gestionnaires de la dette de juger de l’opportunité de ces swaps, selon la situation de la conjoncture, les variations de taux d’intérêt et les prévisions tant économiques que budgétaires,. Ces opérations sont cependant rendues risquées par le fait que les taux d'intérêt à court terme fluctuent et peuvent remonter rapidement.La gestion de la dette se distingue, dans de nombreux pays, de la gestion de la trésorerie de l’État (ou d’un secteur des APU), dont la « mission essentielle de s’assurer qu’il dispose à tout moment et en toutes circonstances des moyens nécessaires pour honorer ses engagements financiers ». Pour cela, les gestionnaires centralisent en permanence les prévisions de recettes et de dépenses à très court terme, pour les jours, semaines et mois suivants, parfois même à un horizon de quelques heures. Ils s’assurent que des réserves suffisantes sont disponibles, même en cas de perturbations et placent au mieux la trésorerie excédentaire. En France, les services de l’agence France Trésor (AFT) gèrent à la fois la trésorerie et la dette de l’État.Au niveau international, la Banque mondiale et le FMI fournissent une assistance à divers pays pour réformer et améliorer les techniques de gestion de leur dette. Dans ce but, ces organismes assurent des formations et ont publié des directives ; développé des outils de modélisation et de mesure des risques liés à la gestion de la dette publique, ainsi que des outils d’évaluation.Un groupe de travail sur la gestion de la dette, créé en 1979, permet aux gestionnaires de la dette des pays membres de l’OCDE de dialoguer et d’échanger leurs expériences. Depuis 1990, le dialogue s’est étendu aux pays en transition et émergents.Au Moyen Âge, la dette publique tend à se confondre de prime abord avec le patrimoine personnel des souverains. En réalité, les souverains dépendent de diverses formes d'impositions indirectes et donc d'intermédiaires (les suzerains, les charges, etc.) et surtout de banquiers marchands (les Fugger, Médicis, etc.), et ce dès le XIIIe siècle.En France, par exemple, où le Trésor royal est institué dès le début du XIVe siècle, l'équivalent aujourd'hui du ministère des Finances, apparaissent les États généraux, quand la question de la dette du royaume devient peu ou prou insoluble. Au XVIe siècle s'impose la notion d'intérêt face, entre autres, aux aléas du commerce maritime. Au XVIIe siècle, certains États deviennent plus unitaires mais se confrontent à la montée du mercantilisme et des grandes compagnies commerciales maritimes et coloniales. L’État doit faire face à des dépenses extraordinaires, à savoir, non budgétées, et liées essentiellement à des conflits armés de grande ampleur, qui n'ont pour but que de défendre et élargir les zones commerciales. C'est ainsi que Colbert créa la Caisse des emprunts : les sommes empruntées mettront plus de 25 ans à être plus ou moins remboursées.Avec l'apparition d’États parlementaires ou de droit, en particulier lors de la Révolution financière britannique, la dette publique devient un enjeu politique pouvant mener à des bouleversements de grande ampleur sur le plan interne. La notion de spéculation privée apparaît. Le début du XVIIIe siècle est le théâtre de nombreuses bulles spéculatives. En 1776, le ministre français Turgot et le banquier suisse Isaac Panchaud créent la Caisse d'escompte afin de faire baisser la surchauffe sur les taux d'intérêt. En effet, cette spéculation, qui visait les effets détenus par l’État, représentait la presque totalité de l'activité à la Bourse de Paris durant les 70 années précédant la Révolution française.Le marché financier est, à cette époque, déjà sophistiqué : les textes officiels parisiens parlent d'un « jeu de primes », c'est-à-dire d'options d'achat et de vente, permettant de renoncer finalement à la transaction si le cours n'a pas suivi une trajectoire aussi ample qu'espérée. Les créances sur l’État rapportent en moyenne 7,5 % des sommes prêtées en 1788, deux fois plus qu'en Angleterre (3,8 % en moyenne), pays dont la dette publique est pourtant plus élevée d'environ 50 % : 133 millions de livres sterling dès 1766, malgré une fiscalité britannique plus lourde depuis la création de la Land Tax. En revanche, la Bourse de Paris cote une obligation différente pour chaque taxe française, alors qu'il n'y a qu'une seule obligation à Londres, perpétuelle et à taux d'intérêt moindre.En 1800, à la suite de la création de la Banque de France, le Consulat met en place la Caisse de garantie et d'amortissement, destinée à permettre l'étalement de la Dette publique, lequel, amorcé en 1791, ne prendra fin qu'en 1825.Les emprunts contractés par un dirigeant politique ne constituent une dette publique que si deux conditions sont respectées. Ces conditions feront défaut, pour l'essentiel, avant l'avènement des États en Occident entre la fin du Moyen Âge et le début de la Renaissance[pas clair].Il faut tout d'abord que la dette de l'institution publique soit conçue comme une dette proprement publique, c'est-à-dire ne se confondant pas avec les engagements financiers personnels du chef de cette institution. Autrement dit, il est nécessaire que les individus opèrent un effort d'abstraction, qui les conduit à voir dans le souverain qui émet l'emprunt l'incarnation d'une institution transcendante, l'État, à laquelle appartient seule la dette. La dette publique naît ainsi en même temps que l'État moderne : celui-ci apparaît, en effet, grâce au processus par lequel les individus différencient la personne physique du souverain et sa personne « immortelle », c'est-à-dire incarnant l'institution étatique dans sa continuité. De ce point de vue, il n'existait pas de dette publique sous l'Empire romain, puisque les empereurs s'endettaient à titre personnel. De même, en Europe, les monarques se sont initialement endettés à titre personnel : la notion de dette publique ne se dégage que progressivement, pleinement à partir du XVIIe siècle.D'autre part, comme le notent Jean Andreu et Gérard Béaur, « si l’endettement des États est ancien, le principe d’une dette reconductible, dont les créances circulent dans le public à travers un marché officiel, anonyme et régulé, bref d’une véritable dette publique, n’a pas toujours été connu et admis ». Pour qu'il existe une dette publique, il faut donc que soit réunie une seconde condition : la dette publique doit être pérenne, dépasser le financement immédiat de besoins, de même que le décès éventuel du souverain qui l'a contractée. C'est en particulier le cas lorsque les emprunts étatiques prennent la forme d'emprunts publics, de long terme, durablement cessibles sur un marché secondaire.La dette publique apparaît ainsi véritablement dans les cités italiennes du XIIIe siècle, en particulier à Venise et Florence, en ce que les emprunts sont contractés au nom de la cité elle-même, et non au nom de ses dirigeants. D'autre part, ces emprunts sont rapidement échangés sur des marchés de la dette publique, notamment les emprunts « perpétuels », inventés à cette époque, et qui donnent droit à des intérêts jusqu'à ce que le principal soit racheté par la cité. Une étude récente montre d'autre-part que la fiscalité en vigueur dans l'Espagne de Charles Quint et de Philippe II était une généralisation de la dette publique dans les cités italiennes. Contrairement à l'idée attachée la Légende noire espagnole, les trois « banqueroutes » espagnoles du XVIe siècle ont été négociées tant au niveau des banques, qu'au niveau des dix-huit communes qui ont représenté le royaume espagnol - les Cortes - qui sont les administrateurs principaux des impôts, le gouvernement central n'ayant aucun contrôle direct sur une grande partie de l'administration fiscale. Les deux premières crises (1557-1560 et 1575-1577), ont conduit à une augmentation des impôts qui pourraient être utilisés pour le service de la dette à long terme. La résolution de la deuxième et de la troisième crise (1596-1597) a entraîné une réduction du taux d'intérêt,.La dette publique est caractérisée, si on l'envisage sur le long terme, par deux éléments forts et structurants.Premièrement, la dette publique est indissociable de l'activité guerrière des États : les États ont avant tout utilisé l'emprunt pour financer les guerres. Les conflits armés récurrents ont constitué un des éléments essentiels dans la construction historique des États occidentaux, en les contraignant à se structurer, à se bureaucratiser et à accroître leur capacité à prélever des ressources sur leur territoire. C'est de cette nécessité que naît la taxation moderne : le suzerain féodal ne pouvait se contenter de l'assistance que lui devait ses vassaux immédiats. Pour vaincre, il lui fallait des fonds, qu'il obtenait en imposant directement la population de son royaume. Comme le relève Michael Mann, « la croissance de la fiscalité étatique est le produit du coût croissant des guerres ».Or, les revenus obtenus par taxation s'avérant insuffisants, l'emprunt a constitué un moyen systématique de financement des guerres. L'emprunt a rendu ainsi possible le financement de conflits de plus grande ampleur. « Cette dynamique de la guerre et de l'endettement est donc au cœur de l'histoire européenne », comme le souligne Jean-Yves Grenier. L'importance des emprunts contractés a d'ailleurs eu pour conséquence de lisser les prélèvements fiscaux : « du XIVe siècle au XXe siècle, les États qui avaient fortement emprunté pour financer les guerres ont vu s'amoindrir les fluctuations de leurs dépenses publiques, car les dettes étaient payées durant de nombreuses années au-delà de la durée des hostilités. La taxation en temps de paix en était donc la conséquence inévitable ».Ainsi, jusqu'au XXe siècle, ce sont les guerres, en imposant la mobilisation immédiate de très importantes ressources, qui ont conduit aux principaux accroissements de la dette publique, comme le montrent les évolutions de la dette publique au Royaume-Uni et aux États-Unis (graphiques 1 et 2). La dette a pu atteindre des niveaux extrêmement élevés, sans commune mesure avec ceux des temps de paix : ainsi la dette publique britannique a représenté près de 300 % du PIB au sortir des guerres napoléoniennes, comme après la Seconde Guerre mondiale (graphique 2). De même, la dette publique américaine s'est nettement accrue à la suite des quatre principaux conflits qu'a connus le pays : la guerre d'indépendance, la guerre civile, la Première et la Seconde Guerre mondiale, où elle a atteint son sommet historique : 120 % du PIB.Deuxième élément structurant : la difficulté des États à pleinement assurer le remboursement de leur dette. « L'histoire financière des monarchies européennes est pleine de ce que l'on pourrait pudiquement appeler des discontinuités, les banqueroutes plus ou moins déguisées succédant aux tentatives des monarques de gagner non sans mal la confiance du public ». L'emprunt est, comme on l'a vu, la modalité complémentaire de l'impôt pour le financement des dépenses. Il est choisi quand l'État ne peut accroître les prélèvements fiscaux, le plus souvent en raison du refus des populations. Toutefois, le service de la dette peut atteindre des niveaux écrasants, représentant une grande part du budget de l'État : ainsi, la moitié du budget britannique est par exemple utilisé pour le service de la dette de 1820 à 1850. Ne pouvant accroître leurs ressources, incapables de rembourser leurs dettes, les États ont utilisé des techniques comme la création monétaire, la baisse imposée du taux d'intérêt de leurs emprunts, le refus de rembourser tout ou partie de leurs dettes. D'Édouard Ier expropriant puis chassant d'Angleterre en 1290 la communauté juive, se débarrassant par là d'une partie de ses créanciers, au défaut de l'Argentine sur sa dette en 2001 et à la crise de la dette publique grecque depuis 2010, l'histoire de la dette publique est en partie une histoire de son non-remboursement complet ou partiel. Moyen Âge La notion de dette publique apparaît au Moyen Âge dans les cités italiennes. Au XIIe siècle, ces cités, politiquement indépendantes, ont tout d'abord recours à des emprunts à court terme, à taux très élevé, et qu'elles dissimulent au public. Ces emprunts ne sont considérés que comme un moyen de faire face à un problème immédiat, et sont remboursés très vite. Toutefois, en raison du coût croissant des guerres, et de l'impossibilité d'augmenter les impôts face au refus des populations, une dette publique de long terme se développe dès le XIIIe siècle. Dès cette époque, les cités italiennes empruntent, souvent de manière forcée, des sommes importantes à long terme. Progressivement, les cités italiennes créent, durant la fin du Moyen Âge, un ensemble d'institutions financières de gestion de la dette publique, promis à un avenir durable. Tout d'abord, la dette publique devient négociable sur un marché secondaire. Elle est, d'autre part, rapidement consolidée (dès 1262 à Venise), et gérée par un établissement public spécialisé (comme le Monte de Florence). La cité émet également des emprunts « perpétuels », dont le principal n'est remboursé que lorsque la cité le souhaite. Ce type de dette fut utilisé en raison de la difficulté à rembourser le principal. Le poids de la dette et de son service devient également pour la première fois considérable : l'essentiel des taxes sert à son financement à Florence au XIVe siècle. Finalement, les cités italiennes ne payent, grâce à ce système, que des intérêts modérés, de l'ordre de 5 %, bien plus bas que ceux des monarchies européennes. Ancien Régime L'Ancien Régime voit la diffusion de ce système à l'ensemble des monarchies européennes. Ainsi, en 1522, François Ier, émet la première rente perpétuelle de la monarchie française. Ce nouveau système va permettre aux monarchies européennes d'accroître considérablement leur dette. Avec la consolidation des États, qui emploient du personnel beaucoup plus nombreux, les guerres sont de plus en plus coûteuses et se succèdent à un rythme soutenu. Les Habsbourg d'Espagne font banqueroute en 1557, 1575, 1596, 1607, 1627 : la légende veut que l'or et l'argent des Amériques ne suffit pas à financer les tentatives de constructions impériales mais la cause principale est que les dépenses militaires conduisent à une hausse de la dette flottante tandis que la dette à long terme est proche du maximum compatible avec les impôts autorisés par les Cortès. Banqueroutes et consolidations forcées deviennent des expédients habituels.L'endettement de la monarchie française s'accroît très fortement à partir du XVIIe siècle, avec l'affirmation de l'absolutisme. La monarchie française a lentement établi un système d'endettement fondé sur la vente d'offices vénaux, qui constituent en fait des dettes perpétuelles, et sur l'emprunt à travers les corps, auquel on prête à des taux beaucoup plus faibles que pour le roi directement, en raison de la confiance qu'ils inspirent. Privilèges, vénalités des offices, corps et dette publique font ainsi système. Ce système a néanmoins une efficacité limitée, notamment en raison des très nombreuses exemptions d'impôt. La monarchie n'a donc pas les moyens de financer des guerres répétées et prolongées contre de larges coalitions. Les rois ne renoncent toutefois pas à leurs ambitions et empruntent des sommes très importantes. Les dépenses s'accroissent au fil des guerres, parfois de manière considérable, alors que les recettes n'augmentent que faiblement, et parfois s'effondrent à la suite de prélèvements trop importants (graphique 3). La crise financière est telle, à la fin du XVIIIe siècle, qu'elle est une des causes de la Révolution française. Révolution financière britannique La Grande-Bretagne qui s'affirme alors comme l'autre grande puissance européenne accroît sa dette considérablement à partir de la dernière décennie du XVIIe siècle (graphique 2), après la création de la Banque d'Angleterre. Entre 1688 et 1702, la dette publique anglaise est passée de 1 à 16,4 millions de livres. Entre 1702 et 1714 elle triple pour atteindre 48 millions de sterling et en 1766 elle atteint 133 millions de sterling. Toutefois, l'Angleterre a su mettre en place un système d'endettement moderne et efficace qui lui permet d'emprunter sans délai et à des taux d'intérêt faibles. Elle emprunte à travers la banque d'Angleterre, créée en 1694, qui est soumise au contrôle vigilant du parlement, ce qui garantit le remboursement et inspire la confiance aux créanciers. Ce système est à l'origine d'une révolution financière, en favorisant le développement des marchés financiers. Les banques privées s'appuient sur le contrôle du marché de la dette publique par la Banque d'Angleterre pour y développer leur activité, y compris à destination du secteur privé, soutenant ainsi le développement économique de la Grande-Bretagne. Ce système favorise en outre l'expansionnisme militaire de la couronne britannique, avec le développement de la Royal Navy par les Navy bills, en particulier face à l'État français. XIXe siècle Une évolution essentielle intervient au XIXe siècle : les États, notamment ceux des puissances financières dominantes de l'époque, la France et la Grande-Bretagne, ne font plus banqueroute : ils assurent pleinement le remboursem"
économie;"En comptabilité nationale, la notion de déficit budgétaire s'utilise lorsque le budget de l'État est en déficit : les recettes de l’État (hors emprunt) sont inférieures à ses dépenses (hors remboursement d'emprunt) d'où un solde budgétaire négatif,.De même, les administrations publiques (ensemble composé de l’État, des ODAC, de l'administration territoriale et des administrations de sécurité sociale) connaissent un déficit public lorsque les dépenses publiques pour une année sont supérieures aux recettes publiques ; le solde des finances publiques est alors négatif.Le déficit budgétaire peut se traduire par de nouveaux emprunts contractés par l'État au cours de l’année, en plus de ceux destinés à amortir les emprunts antérieurs arrivés à échéance. Ces emprunts viennent alimenter la dette de l'État, de même que le déficit public augmente la dette publique. Ainsi, ces deux données sont liées mais se distinguent par leur nature : le déficit est un flux alors que la dette est un stock.Concrètement, les budgets publics (mesurés avec une périodicité annuelle) sont très souvent déficitaires, dans la majorité des pays ; dans le cas opposé, on parle d’excédent budgétaire. En France, dans le cadre de la loi organique relative aux lois de finances, le solde budgétaire de l'année à venir fait l'objet d'une prévision inscrite dans le projet de loi de finances.Pour équilibrer les comptes, le déficit peut être compensé :par l'emprunt (ce qui déplace le « problème » dans le temps, suppose la confiance des créanciers, et a de toute façon un coût puisqu'il faut payer des intérêts) ;par le recours à des réserves préalablement accumulées à partir d'excédents budgétaires réalisés les années antérieures ;plus généralement, en puisant dans le patrimoine ;par des hausses d'impôts, à supposer qu'elles n'atteignent pas le point de rupture où elles détruisent le gisement fiscal, ou par des baisses d'impôts, qui peuvent éventuellement augmenter les rentrées fiscales (voir courbe de Laffer) ;par une émission monétaire (« planche à billets »), qui ne déplace pas la difficulté dans le temps, mais en change la nature en modifiant la valeur de la monnaie. Cette méthode n'est plus utilisée depuis des décennies dans les pays développés (quoique l'assouplissement quantitatif, auquel les États-Unis ont notamment eu recours dans le cadre de la crise financière de la fin des années 2000, s'en rapproche), elle est même impossible dans les États qui ont confié la gestion de la monnaie à une banque centrale indépendante. C'est le cas par exemple des États de l’Union européenne avec la Banque centrale européenne.par une réduction des dépenses publiques.En macroéconomie, on distingue, au sein du solde public, deux éléments : la composante structurelle (dénommée solde structurel), et la composante conjoncturelle. Le solde conjoncturel est traditionnellement calculé à partir des sensibilités moyennes des dépenses et des recettes publiques à la position de l’économie dans le cycle économique (écart de production ou output gap en anglais). Le solde structurel est alors obtenu en retranchant du solde public le solde conjoncturel ainsi construit,. Une autre définition plus simple consiste à dire que le solde structurel est le solde budgétaire qui serait obtenu si la croissance atteint le PIB potentiel. Le solde conjoncturel étant alors obtenu par différence entre le solde réel et le solde structurel. Ainsi, le déficit public structurel est le solde négatif des finances publiques sans tenir compte de l’impact de la conjoncture sur la situation des finances publiques.La variation du solde structurel est aussi appelée « ajustement structurel », qui comprend :l'effort structurel, qui regroupe les effets des mesures prises par l'État concernant les dépenses (réduction du ratio structurel de dépenses publiques) et les recettes (mesures nouvelles en prélèvements obligatoires) ;une composante non discrétionnaire, qui dépend de l'évolution des élasticités des différents prélèvements obligatoires ainsi que la contribution des évolutions des recettes hors prélèvements obligatoires.Parallèlement, on qualifie de solde primaire la situation budgétaire d'un État endetté avant le paiement de la charge de la dette. On parlera donc de déficit primaire ou d'excédent primaire, même si au bout du compte, le budget accuse un déficit. Ce type de solde est utilisé comme révélateur de l'équilibre budgétaire réel de l'État à un moment donné, en lui retranchant le poids de ses déficits passés qu'incarne la dette.Il est important de comprendre le lien entre déficit public et dette publique. Le budget de l'État est en déficit lorsque l'excédent primaire des finances publiques ne suffit pas au paiement des intérêts sur la dette. La dette publique augmente donc en valeur.Dans tous les cas, augmenter le déficit budgétaire a un impact économique qui peut, selon certains économistes, être un stimulant pour l'activité économique par l'intermédiaire d'une politique de relance selon les principes du keynésianisme, ou simplement être un moindre mal dans certaines situations de récession. Pour d'autres économistes, un déficit budgétaire est toujours le signe d'une mauvaise gestion des fonds publics et de l'argent du contribuable et à ce titre il doit être évité (voir politique budgétaire).Si le taux de rendement des investissements publics est supérieur au taux d'intérêt payé sur la dette publique, il peut être rationnel de s’endetter. Toutefois, pour la France par exemple, le déficit actuel (en 2007) ne finance que des dépenses courantes.Lorsque la dette publique est mesurée en pourcentage du PIB, elle peut baisser d'une année à l'autre, même en présence d'un déficit budgétaire. En effet, lorsque la dette et le PIB augmentent, le ratio de la dette publique sur le PIB diminuera si le PIB croît plus vite que la dette.Les pays participant à la monnaie unique européenne sont soumis à une discipline économique et budgétaire visant à empêcher les déficits publics excessifs. Sont considérés comme excessifs les déficits cumulés des administrations publiques dépassant le seuil de 3 % du produit intérieur brut. Cette limite a été définie dans le cadre du traité de Maastricht (1992) et du pacte de stabilité et de croissance (Amsterdam, 1997 ; Bruxelles, 2005).Depuis 2004, des procédures visant à la réduction des déficits excessifs ont concerné dix États membres de l'Union européenne, dont quatre dans la zone euro (Grèce, France, Allemagne et Pays-Bas) et six autres hors zone euro (Hongrie, République tchèque, Slovaquie, Pologne, Chypre, Malte).La procédure a depuis été légèrement assouplie[Comment ?][Quand ?][Qui ?].Par un phénomène purement mécanique, le déficit budgétaire se réduit en période de forte croissance économique, dans la période faste du cycle économique (les recettes de l'État augmentent fortement, alors que ses dépenses ont une volatilité plus faible, donc augmentent moins rapidement).Les différents gouvernements ont également tendance à présenter un budget en fort déficit en début de mandat (application des programmes électoraux, dépenses mises au compte du précédent gouvernement), et à présenter un déficit budgétaire réduit en fin de mandat, à des fins électorales (report à l'année suivante, déplacement de créances sur des organismes publics divers, utilisation de jeux comptables).En France, les déficits publics sont plafonnés à 3 % du PIB depuis 1982 : c'est le ministre de l'Économie et des Finances Jacques Delors sous la présidence de François Mitterrand qui a voulu faire du non-dépassement des 3 % une règle pour le tournant de la rigueur adopté un an plus tard. 1982 voit en effet le déficit budgétaire dépasser les 100 milliards (signal négatif aux marchés économiques mais qui, ramené au ratio, fait 3 % du PNB français),. Le chargé de mission à la Direction du budget, Guy Abeille, explique que : « On a imaginé ce chiffre de 3 % en moins d’une heure, il est né sur un coin de table, sans aucune réflexion théorique. (…) Mitterrand [voulait] qu’on lui fournisse rapidement une règle facile, qui sonne économiste et puisse être opposée aux ministres qui défilaient dans son bureau pour lui réclamer de l’argent ». Selon le néokeynésianisme mis en place en 1981 (programme de relance par la croissance), « limiter le déficit à 3 % permettait de renouer avec la croissance sans pour autant accroître la dette en points de PIB ». Dès lors, ce chiffre revient comme une antienne et, sous l'influence française, l'Union européenne l'impose à l’ensemble des pays membres par le pacte de stabilité et de croissance adopté en 1997 (et révisé en 2005).Par exemple, en France, la crise économique de 1993 a contribué à creuser le déficit budgétaire, et la bonne conjoncture autour de l'année 2000 a réduit mécaniquement le déficit. En 2000, il a été question d'une « cagnotte budgétaire » alors que le déficit global n'était pas comblé.En 2010, le déficit public de la France « au sens de Maastricht » s'est élevé à 136,5 milliards d'euros, soit 7,1 % du produit intérieur brut (PIB).Déficit public de la Zone euro : +0,1 % en 2000,[réf. nécessaire]-1,8 % en 2001,[réf. nécessaire]-2,5 % en 2002,[réf. nécessaire]-3,1 % en 2003,[réf. nécessaire]-2,9 % en 2004.[réf. nécessaire]En 2016, le Luxembourg (+ 1,6 %), Malte (+ 1,0 %), la Suède (+ 0,9 %), l'Allemagne (+ 0,8 %), la Grèce (+ 0,7 %), la République tchèque (+ 0,6 %), Chypre et les Pays-Bas (+ 0,4 % chacun) ainsi que l’Estonie et la Lituanie (+ 0,3 % chacun) ont affiché un excédent public, tandis que la Bulgarie et la Lettonie avaient un solde budgétaire à l'équilibre.Les déficits publics les plus faibles, par rapport au PIB, ont été enregistrés en Irlande (- 0,6 %), en Croatie (- 0,8 %) et au Danemark (- 0,9 %). Quatre États membres ont affiché un déficit supérieur ou égal à 3 % du PIB : l'Espagne (- 4,5 %), la France (- 3,4 %) ainsi que la Roumanie et le Royaume-Uni (- 3,0 % chacun).Lorsque les dépenses dans le secteur privé sont jugées insuffisantes pour relancer l'activité économique et l'emploi, l'État intervient. En recourant au déficit budgétaire, il peut augmenter les revenus disponibles des fonctionnaires civils et militaires, augmenter ses achats et/ou relancer les travaux publics. L'augmentation de la consommation, de l'investissement et de la production nationaux qui en résultent permettent de stimuler la croissance économique et l'emploi.L'inflation. Lorsque les autorités publiques financent le déficit budgétaire par simple émission de monnaie fiduciaire sans tenir compte des besoins de l'activité économique en liquidités, les prix des biens et des services augmentent causant ainsi une baisse, du pouvoir d'achat de la population et de la compétitivité des produits nationaux.""L'effet d'éviction"" . À l'inverse, si l'État veut éviter l'inflation et ses effets néfastes, il peut financer son déficit en se faisant prêter auprès du grand public. Le drainage de l'épargne ainsi réalisé va le raréfier en augmentant son prix : le taux d'intérêt. La conséquence et la baisse de l'investissement privé provoquée par la baisse de l' effet de levier.Dette publiqueBudget de l'ÉtatConseil budgétaireEn FranceFinances publiques en FranceBudget de l'État françaisDéficit budgétaire et déficit publicDéficit public de la FranceDéficit budgétaire de la FranceDéficit de la Sécurité sociale en FranceDette publique de la FranceDirection du budgetAgence France TrésorDans l'UECrise de la dette souveraine | Dette publique de la zone euro | Budget de l'Union européenne Portail de l’économie   Portail de la finance   Portail de la politique"
économie;"Une entreprise, également appelée firme, compagnie ou société, ou encore familièrement boîte ou business, est une organisation ou une unité institutionnelle, mue par un projet décliné en stratégie, en politiques et en plans d'action, dont le but est de produire et de fournir des biens ou des services à destination d'un ensemble de clients, en réalisant un équilibre de ses comptes de charges et de produits.Pour ce faire, une entreprise fait appel, mobilise et consomme des ressources (matérielles, humaines, financières, immatérielles et informationnelles) ce qui la conduit à devoir coordonner des fonctions (fonction d'achat, fonction commerciale, fonction informatique, etc.). Elle exerce son activité dans le cadre d'un contexte précis auquel elle doit s'adapter : un environnement plus ou moins concurrentiel, une filière technico-économique caractérisée par un état de l'art, un cadre socio-culturel et réglementaire spécifique. Elle peut se donner comme objectif de dégager un certain niveau de rentabilité, plus ou moins élevé. Du point de vue légal, une entreprise est une personne morale.Depuis le début du XXIe siècle, les entreprises sont appelées à prendre en compte les exigences de développement durable, à travers la responsabilité sociétale des entreprises.L'entreprise est la plus petite combinaison d'unités légales qui constitue une unité organisationnelle de production de biens et de services jouissant d'une certaine autonomie de décision, notamment pour l'affectation de ses ressources courantes (définition consultée en novembre 2020).En droit français, il n'y a pas de reconnaissance de l'entreprise comme sujet, mais comme activité. Il y a plusieurs formes de sujets juridiques qui peuvent porter une entreprise. Les plus courantes sont :les sociétés : lorsque l'entreprise est portée par plusieurs associés (société anonyme, société par actions simplifiée, société à responsabilité limitée, société civile professionnelle) ;les associations ou coopératives : lorsque l'entreprise n'a pas de but lucratif ;les structures individuelles : lorsque l'entreprise est portée par un individu seul (auto-entrepreneur, profession libérale, artisan, entreprise individuelle, EURL).La forme juridique choisie doit faire l'objet d'un enregistrement auprès des autorités compétentes (registre du commerce et des sociétés ; répertoire des métiers pour les entreprises artisanales ; URSSAF pour les professions libérales). Cette forme juridique est associée à une identification distinctive et non ambiguë (en France par exemple, inscription au répertoire SIREN/SIRET).Lorsqu'il s'agit d'une société, cet enregistrement lui confère la personnalité morale et un statut juridique dont la forme dépend de l'objet social de la société, du nombre des apporteurs de capitaux, du montant des capitaux engagés, ainsi que du cadre législatif et réglementaire en vigueur. L'exercice de l'activité de l'entreprise peut également faire l'objet d'une autorisation préalable délivrée à titre permanent ou révisable, là encore dans le cadre des législations en vigueur (exemples des activités de banque, assurance, pharmacie, travail temporaire, etc.).La question de l'entreprise comme patrimoine juridique, comme propriété, est toujours débattue en doctrine. En l'état actuel du droit français seuls des aspects parcellaires de l'entreprise, comme le capital, la fidélité de la clientèle et les moyens de production, sont considérés comme des droits patrimoniaux qui reviennent à l'entité exploitante. Par contre, la liberté d'entreprendre est reconnue par le Conseil d’État comme principe général du droit à valeur constitutionnelle.Par le concept de société, le droit identifie donc l'entreprise avec ses dirigeants. Cependant, le droit encadre aussi la représentation des employés au sein de l'entreprise (voir Comité d'entreprise). La personnalité de l'entreprise en anthropologie La conception de l'entreprise comme une entité propre et capable d'agir par elle-même est une construction culturelle. L'attribution de décisions, de comportements, voire d'émotions, à une entreprise est une croyance qui l'assimile à une personne humaine. Cette personnalisation de l'entreprise se retrouve en droit des sociétés, qui utilise l'image de la personne morale. Elle se retrouve aussi en marketing avec le concept d'identité de l'entreprise auprès des clients.Cette assimilation culturelle a des effets juridiques et économiques. Ainsi, le concept de « responsabilité limitée » et sa mise en œuvre dans les lois au XIXe siècle (ex : en France, lois du 23 mai 1863 puis du 24 juillet 1867 ; en Angleterre lois de 1856 à 1862 sur les Joint-Stock Company limited) compte, d'après Y.N. Harari dans son ouvrage Sapiens, « parmi les inventions les plus ingénieuses de l’humanité » : « Peugeot est une création de notre imagination collective. Les juristes parlent de « fiction de droit ». Peugeot appartient à un genre particulier de fictions juridiques, celle des « sociétés anonymes à responsabilité limitée ». Harari explique : « Si une voiture tombait en panne, l’acheteur pouvait poursuivre Peugeot, mais pas Armand Peugeot. Si la société empruntait des millions avant de faire faillite, Armand Peugeot ne devait pas le moindre franc à ses créanciers. Après tout, le prêt avait été accordé à Peugeot, la société, non pas à Armand Peugeot, l’Homosapiens ».La « responsabilité limitée » est donc un transfert de la responsabilité pénale de l'actionnaire à la société-entreprise, et des risques économiques à son collectif de travail. Toutefois, ce transfert ne s'accompagne pas en retour d'un transfert de propriété du fait de la non-réalité juridique de l'entreprise : quel que soit le montant investi par l'actionnaire il a toujours le pouvoir et est propriétaire de fait (grâce à sa possession des actions) de tous les moyens de production (locaux, machines, moyens informatiques, etc.), y compris de ceux acquis grâce aux « millions » empruntés : c'est l'entreprise, qui acquiert en empruntant, qui rembourse, et qui entretient à ses frais les moyens de production en plus, bien entendu, de payer les salaires, charges et taxes.Grâce à cette « responsabilité limitée » conjuguée avec la non-réalité juridique de l'entreprise, plusieurs procédés permettent aux actionnaires d'accroître les moyens de production qu'ils contrôlent en minimisant au maximum leur mise (le capital social) : investissement par effet de levier, achat à effet de levier, rachat d'actions. Il est donc très compréhensible que les actionnaires recourent à ces procédés plutôt que d’émettre des actions supplémentaires provoquant l'arrivée d'autres actionnaires avec qui certes les risques sont partagés, mais également le pouvoir et la propriété. Si l'entreprise était comme une association 1901, sujet de droit, la « responsabilité limitée » serait remplacée par les « responsabilités et propriétés partagées » entre actionnaires et le collectif de travail de l'entreprise, chacun selon sa contribution.Le concept d'entrepreneur désigne celui qui entreprend, qui se trouve être à l'origine et concrétise un projet d'entreprise :sa démarche peut être innovatrice lorsqu'il anticipe un besoin, ou assemble et organise les outils et les compétences nécessaires pour satisfaire de manière inédite ce besoin. Ce type d'entrepreneur fait appel à des notions de création et d'innovation, et se distingue donc de celui de chef d'entreprise. Pourtant, ces deux termes bien que relevant de réalités différentes, caractérisent souvent les mêmes personnes : un entrepreneur est un chef d'entreprise s'il pilote lui-même son projet et un chef d'entreprise peut être qualifié d'entrepreneur à raison des objectifs intrinsèques de sa fonction ;la démarche peut être moins originale et plus conventionnelle lorsque l'entrepreneur considéré porte un projet qui s'inspire fortement, voire reproduit ou utilise des modèles d'activité ou d'entreprise déjà existants.Ce faisant, l'entrepreneur prend le risque que le besoin ne se matérialise pas ou que les moyens qu'il met en place pour le satisfaire se révèlent inadéquats.Historiquement, l'entrepreneur est un intermédiaire, un agent en travail : on lui passe des commandes fermes de biens ou de services, il recherche les ouvriers qui vont produire chacun une partie de cette commande et il s'assure de la bonne livraison. Ceci dans un contexte où la division du travail est trop peu marquée, où les ouvriers travaillent à domicile, et disposent de leurs outils et même de leurs machines (métier à tisser par exemple).Avant la révolution industrielle, un entrepreneur est surtout un « homme-orchestre » capable d'optimiser les besoins en capitaux et les ressources humaines pour mener une activité licite et rentable, les moyens de production et la force de travail n'étant pas encore regroupé au sein d'entreprise. On retrouve encore au XXIe siècle ce type d'organisation, par exemple, dans l'industrie du transport, les services (ingénierie, etc.) où à côté de grands groupes, des indépendants sont propriétaires de leur outil de travail (par exemple : camions, péniches ou barges) et trouvent leurs donneurs d'ordres par l'intermédiaire de courtiers.Avec la révolution industrielle, les entrepreneurs changent, ils regroupent des machines sur un même lieu de travail et conservent les mêmes ouvriers longtemps, ce qui donne naissance aux entreprises au sens traditionnel. On voit alors émerger la figure du chef d'entreprise (un exemple connu étant celui d'Henry Ford).Les prémisses de l'entreprise au sens moderne du terme n'apparaissent qu'au XVIIIe siècle, avant cela, les activités de production et d'échange sont presque exclusivement assurées au sein de familles ou de guildes. La place de l'entrepreneur y est alors essentielle, il dirige tous les maillons de la chaîne de valeur. Du fait principalement de l'industrialisation, au XIXe siècle, l'organisation des entreprises change considérablement. L'identité familiale de l'entreprise et l'exclusivité du pouvoir de l'entrepreneur-dirigeant s'affaiblissent progressivement. À partir de 1880 se développent les « grandes entreprises modernes » sous forme de sociétés anonymes où la contribution de chaque actionnaire aux pertes ne peut excéder sa part dans le capital social. Grâce à ce principe, l'offre de capitaux explose.Les entreprises peuvent être classées selon différents critères :La classification par secteur économique est déterminée par l'activité principale de l'entreprise :secteur primaire : il s'agit d'activités liées à l'extraction des ressources naturelles via l'agriculture, la pêche, l'exploitation forestière ou minière ;secteur secondaire : il s'agit d'activités liées à la transformation des ressources naturelles issues du secteur primaire (bâtiments et travaux publics, électroménager, aéronautique, etc.) ;secteur tertiaire : il regroupe toutes les activités économiques qui ne font pas partie du secteur primaire et secondaire. Il s'agit d'activités marchandes (vente de produit) et d'activités non marchandes (vente de services, non échangeables).Au-delà de ce découpage classique, un secteur quaternaire est parfois distingué, avec une définition variant selon les auteurs.Selon la définition de la Commission européenne en 2011, les entreprises sont classées comme :microentreprise : sous-catégorie des TPE définie en France par un chiffre d'affaires inférieur à 81 500 euros pour celles réalisant des opérations d'achat-vente et à 32 600 euros pour les autres ;très petite entreprise (TPE) : moins de 10 salariés avec soit un chiffre d'affaires inférieur à 2 millions d'euros par an, soit un total bilan inférieur à 2 millions d'euros ;petite et moyenne entreprise (PME), on distingue :petite entreprise (PE) : entre 10 salariés et 49 salariés avec soit un chiffre d'affaires inférieur à 10 millions d'euros par an, soit un total bilan inférieur à 10 millions d'euros,moyenne entreprise (ME) : entre 50 salariés et 250 salariés avec soit un chiffre d'affaires inférieur à 50 millions d'euros par an, soit un total bilan inférieur à 43 millions d'euros ;grande entreprise : plus de 250 salariés et à la fois un chiffre d'affaires supérieur ou égal à 50 millions d'euros par an et un total bilan supérieur ou égal à 43 millions d'euros ;groupe d'entreprises : comporte une société mère et des filiales ;entreprise étendue (ou en réseau, ou matricielle, ou virtuelle) : comprend une entreprise pilote travaillant avec de nombreuses entreprises partenaires.Le secteur : ensemble des entreprises ayant la même activité principale.La branche : ensemble d'unités de production fournissant un même produit ou service.Pour l'Insee, une entreprise est une unité économique, juridiquement autonome, organisée pour produire des biens ou des services pour le marché ; elle est identifiée par le numéro SIREN. Un établissement est une unité de production géographiquement individualisée mais juridiquement dépendante de l'entreprise, et où s'exerce tout ou partie de l'activité de celle-ci ; il est identifié par un numéro SIRET.Les entreprises individuelles (existence juridique à travers la personne physique de l'entrepreneur — EI, EIRL).Les sociétés civiles (exemple : société civile professionnelle).Les sociétés commerciales (de personnes ou de capitaux ; parfois unipersonnelles — EURL, SASU).Les groupements d'intérêt économique.Les associations, entreprises privées dont les bénéfices doivent être intégralement réinvestis.Les sociétés coopératives, dans lesquelles les associés coopérateurs n'ont chacun qu'une voix quel que soit le montant de leurs apports (salariés, consommateurs, habitants, bénéficiaires du service, etc.).Les sociétés mutuelles à but non lucratif, immatriculées au registre national des mutuelles et soumises aux dispositions du code de la mutualité.Une autre forme de classement distingue trois grands types d'entreprises[réf. nécessaire] existant dans tous les pays :les entreprises privées à but lucratif (exemple : TPE, PME, groupe d'entreprises) ;les entreprises privées à but non lucratif (sociétés coopératives, associations et sociétés mutuelles relevant de l'économie sociale) ;les entreprises chargées d'une mission de service public (exemple : régie des transports urbains, régie des eaux, établissements publics industriels et commerciaux).L'activité économique est, dans tous les pays, encadrée par une réglementation. La plupart des entreprises fonctionnent donc dans un cadre prédéterminé par la loi : le droit des sociétés. Entreprise individuelle Dans le contexte de l'économie capitaliste, il est possible d'avoir une entreprise à titre personnel. Il s'agit alors d'une entreprise individuelle, c'est-à-dire que l'entrepreneur exerce directement et en son propre nom l'activité économique. L'exercice d'une activité sous forme d'entreprise individuelle concerne en général les TPE. Entreprise personne morale Il est aussi possible de constituer une personne morale sous forme de société. Celle-ci peut grouper plusieurs participants à son capital et est apte à faire des actes de gestion. Les diverses formes de sociétés varient selon les pays.Il convient alors de distinguer la propriété effective de l'entreprise et le pouvoir d'accomplir des actes de gestion au nom de la société. Selon la forme sociale, le responsable de la marche courante de l'entreprise sera appelé un gérant, président-directeur général ou directeur général. Le titulaire de cette fonction peut être détenteur de parts sociales ou d'actions ou être mandaté pour cela par l'assemblée générale des associés.Le droit des sociétés français distingue notamment les statuts de société anonyme (SA), société à responsabilité limitée (SARL), société par actions simplifiée (SAS), société civile (SC), société d'exercice libéral à responsabilité limitée (SELARL) et société en nom collectif (SNC). Un statut spécial nommé Euro 2016 SAS a été créé en 2014 afin que l'UEFA puisse organiser la coupe d'Europe de Football de 2016 en France sans devoir payer des impôts autre que la TVA (étant une taxe réglementée à l'international).Le fait qu'une entreprise utilise une forme de société par actions n'implique pas nécessairement que ces titres soient cotés en bourse (ou même qu'elle soit considérée comme faisant un appel public à l'épargne). Si c'est le cas, des achats en bourse ou des offres publiques peuvent faire changer la majorité de contrôle de l'entreprise, et aboutir aussi au changement de sa direction.La fonction première d'une entreprise varie selon l'entreprise ou même selon les points de vue au sein d'une même entreprise (par exemple, point de vue de l'actionnaire, de l'employé, du syndicat, de la direction, etc.). Parmi les différentes fonctions opérationnelles habituellement observées, on trouve :servir le marché, en produisant et distribuant des biens et services correspondant à une demande solvable. C'est sa seule justification économique, aucune entreprise ne pouvant survivre sans en faire sa priorité, à moins d'être protégée et en dehors du champ de la concurrence (exemple : cas de certains services publics), ce qui, d'un point de vue purement économique, peut la conduire à consommer plus de ressources qu'elle ne présente d'utilité ;gagner de l'argent, c'est-à-dire extraire des bénéfices financiers en « récoltant plus d'argent que d'argent investi », notamment pour attirer les investisseurs institutionnels et les petits actionnaires ;produire un excédent de trésorerie, qui sera investi avec un plus grand profit dans le développement des activités ou une autre entreprise (dans le cadre d'un « groupe ») ;maximiser l'utilité sociale ou environnementale. Certaines sociétés (entreprises à mission) se donnent même statutairement l'utilité sociale comme finalité ;atteindre un but technique : réalisation d'un ouvrage (tunnel, pont, route, etc.), fabrication d'un produit manufacturé, la conception et réalisation d'un service donnant satisfaction à un client. Ce but technique peut lui-même être extrêmement varié, on citera notamment :les activités qui ne sont pas, pour l'entrepreneur, l'enjeu principal, mais un moyen au service d'une autre activité : par exemple, la possession d'un groupe de presse, de production de ressources stratégiques ou d'entreprises vecteurs d'images (à l'exemple de la présence des cigarettiers dans l'industrie du prêt-à-porter),les coopératives agricoles qui sont des entreprises qui visent à dégager un bénéfice non pour elles-mêmes, mais pour les coopérateurs adhérents,les « entreprises d'insertion » visent à rendre aptes leurs employés à occuper un travail « normal », sans chercher dans certains cas (atelier chantier d'insertion) à générer du bénéfice.Certaines sociétés peuvent être constituées pour détourner les fonctions premières de l'entreprise, notamment pour camoufler des activités légales ou illégales (exemple : certaines activités comme le jeu, le change, le lavage de voitures, l'immobilier sont connues pour permettre le « recyclage » ou le « blanchiment » de l'argent issu d'activités illégales).Divers points de vue politiques sur l'utilité fonctionnelle de l'entreprise privée ont été formalisés au cours de l'histoire et de l'élaboration de la pensée économique :de son inutilité totale, aboutissant à sa suppression ou sa collectivisation ;à sa complète utilité (notamment en termes de création d'emplois), aboutissant à son encouragement et au développement des PME, des TPE, des sociétés artisanales et des professions libérales.Les entreprises se soucient de plus en plus de relégitimer leur rôle dans la société à travers divers vecteurs, particulièrement notables à partir de la fin du XXe siècle :les rapports de développement durable rédigés par les grandes sociétés mettent en avant leur rôle social et environnemental. La communication sur les efforts en faveur de l'environnement est devenue un argument majeur au début du XXIe siècle. En France, elle est rendue obligatoire par la loi sur les nouvelles régulations économiques (article 116) ;le mécénat (artistique, humanitaire, social, etc.) constitue autant un moyen de légitimation de la place de l'entreprise qu'une action de communication institutionnelle en faveur de l'image de l'entreprise ;en France, le thème de l'« entreprise citoyenne », en vogue au tout début des années 2000, a fait avancer la réflexion sur la place de l'entreprise dans la société.L'évaluation de la triple performance économique, sociale et écologique (3P pour People Planet Profit) de l'entreprise se fait par des agences de notation sociétale, qui examinent les rapports de développement durable pour noter les entreprises. Les investissements socialement responsables permettent de s'orienter vers les entreprises les mieux notées sur le plan sociétal.Ainsi, une nouvelle forme d'entreprise émerge, appelée à prendre en compte les intérêts à long terme de l'ensemble des parties prenantes de l'entreprise, et non plus seulement le seul intérêt à court terme des seuls actionnaires. En effet, le développement durable fait intervenir non seulement le marché, mais aussi l'État et la société civile.Le mode de gouvernance des entreprises conforme au développement durable s'appelle la responsabilité sociétale des entreprises.Pour le droit de la concurrence, la forme juridique (personne morale de droit privé ou de droit public, société, association) et le but (lucratif ou pas) de l'entreprise sont indifférents. Ainsi pour le droit communautaire, « la notion d'entreprise comprend toute entité exerçant une activité économique, indépendamment du statut juridique de cette entité et de son mode de financement » (Cour de justice des communautés européennes (CJCE), arrêt Höffner, 1991).Néanmoins, n'exerce pas une activité économique, et n'est plus une entreprise soumise au droit de la concurrence, l'organisme qui remplit une fonction exclusivement sociale (CJCE, Poucet 1993) ou celui qui exerce des prérogatives de puissance publique (CJCE, Eurocontrol, 1994).Acquisition et cession d'entrepriseConcurrencedroit de la concurrenceFusionMonopoleOligopolePlan marketing Finalité : rémunérer le risque pris par l'apporteur de capital Parmi les différents buts possibles pour une entreprise, la recherche du bénéfice occupe une place importante. Le bénéfice de l'entreprise (différent du profit) sert avant tout à rémunérer le capital investi.Les entreprises peuvent prendre plusieurs formes juridiques correspondant à des caractéristiques différentes de l'apporteur de capital : entreprises individuelles, sociétés de personnes, sociétés de capitaux. Les grandes entreprises sont en général des sociétés de capitaux.Dans le cas des sociétés de capitaux, si un investisseur (une des personnes qui financent l'entreprise) décide de le placer dans une entreprise plutôt que de le conserver, c'est qu'il souhaite que l'argent ainsi placé dans l'entreprise lui rapporte plus. Si une entreprise ne génère pas un profit suffisant redistribué sous forme de dividendes, sa réputation ternit et elle n'attire plus les investisseurs. Sa capacité de développement (en général consommatrice de capitaux pour, par exemple, ouvrir des filiales à l'étranger ou démarrer de nouveaux programmes d'innovation), voire sa survie, s'en trouvent alors obérées, voire peuvent être remises en cause.Pour chaque secteur d'activité, il existe un niveau de profit « normal » attendu. Ainsi, par exemple, dans le secteur pharmaceutique des années 2000, le niveau moyen de profit attendu était de 15 % par an du capital investi. Si une entreprise génère moins de profit, les actionnaires qui y ont placé leurs économies (directement ou plus souvent indirectement via une banque ou une caisse de retraite) sont déçus, perdent éventuellement confiance dans l'investissement consenti et vendent leurs actions : le prix de l'entreprise (qu'elle soit en bourse ou non) diminue alors et les investisseurs restants y perdent.Une entreprise capitaliste dont les profits sont faibles trop longtemps n'a pas de justification économique : elle est en général fermée ou rachetée. Dans le cas d'entreprise de l'économie sociale, elle perdura si elle apporte une utilité sociale à la société (exemple : entreprise de réinsertion) et si elle trouve un bailleur de fonds apte à en financer les pertes éventuelles (exemple : collectivité territoriale). Enfin, les entreprises familiales, à la fois privées et non cotées, peuvent trouver un équilibre entre profits élevés et utilité sociale, tout en réussissant sur le long terme, notamment par leur taille à l'échelle humaine et la proximité du management vis-à-vis des salariés. L'origine du bénéfice De manière simplifiée, la rentabilité d'une activité s'obtient en vendant le plus cher possible un produit ou service et en dépensant le moins possible pour le produire.On distingue des revenus normaux et des revenus exceptionnels :les revenus normaux sont les produits des ventes et des opérations financières courantes sur l'année en cours (crédits clients et fournisseurs) ;les revenus exceptionnels ne font pas, par définition, partie des opérations courantes de l'entreprise. Il peut s'agir de vente d'actifs (bâtiments, machines, etc.), de vente de filiales ou d'opérations comptables diverses (exemple : réévaluation de la valeur financière d'un stock).La marge, calculée comme différence entre le prix de vente et le coût de revient des marchandises incorporées dans le produit vendue représente la principale contribution au bénéfice de l'entreprise.Pour augmenter cette marge, il existe uniquement deux leviers :augmenter le prix des produits ou services vendus (exemple : vendre un véhicule automobile à 15 000 €) ;diminuer le coût de production des produits ou services vendus (exemple : produire le véhicule avec 12 000 €).Les moyens d'action sur la réduction des coûts sont extrêmement divers, notamment :négocier avec les fournisseurs pour baisser les prix d'achat des marchandises incorporées ;améliorer la qualité pour produire avec moins de rebut ;améliorer la productivité des machines ;améliorer la productivité des hommes (amélioration de la qualification, ajustement du ratio entre la rémunération fixe et celle indexée sur les résultats, amélioration des conditions de travail, audit des pratiques dans le but de les améliorer, meilleure gestion du personnel, management des compétences, audit des outils) ;diminuer les taxes et prélèvements sur la production (impôt sur les profits, diminution des cotisations salariales des caisses sociales ou de retraites, bénéficier d'exonérations) ;réduire les stocks pour réduire le capital immobilisé ;négocier des conditions de règlement plus rapides vis-à-vis des clients afin d'avoir moins de frais financiers ;utiliser des logiciels libres pour réduire le capital immobilisé par les logiciels propriétaires payants ;s'implanter à côté des lieux de production des matières premières ;réduire la masse salariale et les avantages sociaux ;utiliser l'analyse de la valeur (c'est souvent le moyen le plus puissant puisqu'on peut réduire parfois les coûts dans des proportions considérables). Innovation technique et technologique La solution à ces déplacements mondiaux des centres de production de faible valeur ajoutée passe par l'innovation, la création d'activités à forte valeur ajoutée (exemple : Airbus A380, TGV, automobiles intelligentes, microprocesseurs, nouveaux matériaux, logiciels sophistiqués, biotechnologies, armements, centrales nucléaires, robot d'assistance aux personnes âgées, textiles intelligents, haute couture, etc.) demandant une main-d'œuvre créative et hautement qualifiée, ainsi que le développement de services de proximité.En 2008, les services représentent 70 % du PIB du monde occidental, ce qui consacre l’évolution des pays développés vers l’économie post-industrielle[réf. nécessaire]. L'entreprise dans la mise en œuvre de la Connaissance Il y a toujours des organisations, des hommes et des machines. Les entreprises sont de plus en plus globales (même petites) et connectées en réseaux leur permettant de réagir vite à des opportunités et associer des bonnes compétences pour accompagner des « idées au succès ». Les connaissances jouent un rôle prépondérant dans la façon de faire des affaires. On commence à prendre en compte non seulement le capital financier, mais aussi les capitaux immatériels qu'il faut fructifier. La santé et l'avenir des entreprises dépendent de leur capacité à innover et leur savoir-faire en transformation des idées en valeurs à partager pour tous les participants. Dans ce contexte les ordinateurs sous toutes leurs formes jouent le rôle d'assistant intelligent de l'humain,,.L'entreprise privée, en tant qu'entité de création et de partage des richesses, a fait l'objet de nombreuses critiques. La critique, provenant en particulier depuis le XIXe siècle de la pensée du socialisme et du christianisme social, s'est révélée plus profonde dans les pays de culture catholique (où les rapports de la morale avec l'argent sont complexes) que dans les pays de culture protestante, dans lesquels la position et la fonction sociale de chaque individu est considérée comme étant le fruit de la volonté divine (selon la thèse de Max Weber sur l'éthique protestante et le capitalisme).L'entreprise privée est considérée par certains détracteurs comme une entité faisant primer ses intérêts particuliers au détriment de l'intérêt général.La critique socialiste apparue au XIXe siècle s'est d'abord portée sur les conséquences économiques avec la question de la répartition inégalitaire des richesses créées par l'entreprise, au profit des capitalistes (la rémunération du capital) et au détriment des salariés (qui apportent leur travail). Elle a notamment été théorisée par Karl Marx.Les critiques concernant l'influence des entreprises sur le pouvoir politique se sont ajoutées. Dans la théorie marxiste, la « superstructure » sociale, qui comprend les pouvoirs politique et religieux est au service de l'« infrastructure » économique. Cette critique, sur le lien entre hommes politiques et entreprises, même en dehors du courant de pensée marxiste, est très vivace au début du XXIe siècle.Les entreprises sont accusées de mener un jeu géopolitique propre, dicté par leurs seuls intérêts, indépendant, voire contradictoire avec celui des politiques étrangères nationales ou internationales (par exemple, sur la question des droits de l'homme).Historiquement, les (ou des) entreprises privées ont été accusées d'avoir promu le colonialisme et l'impérialisme occidental et la guerre. C'est par exemple, la critique de Lénine sur l'impérialisme, stade suprême du capitalisme.À partir de la fin du XXe siècle, les entreprises ont été accusées de dégrader l'environnement dans le cadre de leur activité.D'autres critiques se sont focalisées sur le fonctionnement interne de l'entreprise privée. On relèvera notamment :la critique d'exploitation du salarié compte tenu de l'asymétrie des rapports de force entre employeurs et employés, notamment en période de chômage ;des critiques sur la ligne de partage de la richesse (des gains de productivité, des bénéfices) entre ceux qui apportent le capital et ceux qui apportent le travail ;la critique du pouvoir dans l'entreprise qui appartient traditionnellement aux agents apportant les capitaux et non à ceux qui fournissent leur travail. D'où des tentatives d'équilibrage à travers, par exemple, la cogestion en Allemagne ;la critique des formes de pression exercée sur le salarié et conduisant à des phénomènes de stress, évoqués notamment à partir de la fin du XXe siècle.Face aux critiques, les défenseurs des entreprises soulignent que l'intérêt privé va en fait dans le sens de l'intérêt général :l'entreprise privée constitue le moyen le plus efficace d'allocation des ressources (capital, travail, matières premières et énergie) compte tenu notamment de la contrainte de rentabilité ;l'entreprise privée constitue le moteur le plus efficace de la croissance économique et de l'innovation technique. Même quand elle n'est pas à sa source, l'entreprise est le vecteur d'application et de diffusion des innovations techniques ;l'entreprise, guidée par le souci de son développement et de sa rentabilité, ne tient pas compte des distinctions de nationalité, de race ou de sexe pour ne se baser que sur le mérite personnel. L'entreprise est alors considérée comme un facteur de paix et de rapprochement international et d'intégration des personnes différentes.En ce qui concerne le fonctionnement interne de l'entreprise, ses défenseurs ajoutent que l'entreprise peut au contraire être un lieu d'épanouissement personnel. Les cas les plus en pointe de cette tendance se situent dans les entreprises de nouvelles technologies, dans lesquelles les entrepreneurs sont souvent jeunes et les rapports humains moins formels (la culture de la startup"
économie;"En économie, un indicateur est une statistique construite afin de mesurer certaines dimensions de l’activité économique, ceci de façon aussi objective que possible. Leurs évolutions ainsi que leurs corrélations avec d'autres grandeurs sont fréquemment analysées à l'aide de méthodes économétriques.Les indicateurs sont construits par l'agrégation d'indices qui figurent dans un document appelé « tableau de bord ». La construction des indicateurs découle d'un choix de conventions qui traduisent plus ou moins bien certaines priorités et valeurs éthiques et morales. Le « Tableau économique » de François Quesnay, l'un des premiers physiocrates qui a vécu au XVIIIe siècle, constitue l'un des premiers exemples d'un tel indicateur visant à mesurer la richesse d'un pays. Depuis les développements des comptes nationaux après la Seconde Guerre mondiale, le produit intérieur brut (PIB) et le produit national brut (PNB) sont les indicateurs les plus courants.Par ailleurs, il existe d'autres indicateurs qui prennent en compte d'autres facteurs ignorés par le PNB et le PIB afin de mesurer le bien-être des habitants d'un pays ; en incluant par exemple des indicateurs de santé, d’espérance de vie, de taux d'alphabétisation. Le Programme des Nations Unies pour le développement (PNUD) a ainsi créé l'indice de développement humain (IDH) dans les années 1990.Des tentatives pour prendre en compte d'autres dimensions telles la sécurité ou pour inclure la « soutenabilité écologique » de l'activité économique dans des indicateurs ont aussi été menées plus récemment.Parmi les nombreux indicateurs économiques très souvent utilisés figurent en premier lieu le Produit intérieur brut (PIB), dont on surveille le taux de croissance afin de mesurer la croissance économique, et le Produit national brut qui permet de comparer les puissances économiques des différentes nations. Sont aussi souvent utilisés le taux d'inflation et des indices du niveau des revenus, de celui de la richesse, ou encore le salaire minimum, le salaire moyen et l'indice de Gini, lesquels fournissent divers aperçus de la répartition et de l'inégalité des revenus. De nombreux indicateurs financiers sont enfin d'usage de plus en plus courant avec l'essor de la mondialisation financière.La mesure de la production d'un pays se fait généralement par le Produit national brut (PNB) et le Produit intérieur brut (PIB). Le PIB est défini comme la valeur totale de la production interne de biens et services dans un pays donné au cours d'une année donnée par les agents résidents à l’intérieur du territoire national. C'est aussi la mesure du revenu provenant de la production dans un pays donné. Ces indicateurs correspondent au développement des comptes nationaux mis en place après la Seconde Guerre mondiale. Ils sont limités du fait des conditions historiques de leur apparition, à la fois dans leur mesure et au niveau conceptuel.Le Produit national brut (PNB) vise à évaluer la valeur des productions nationales réalisées aussi bien sur le territoire d'un pays qu'à l'étranger. Pour ce faire, il retranche du PIB les productions et services réalisés sur le territoire par les non-résidents (donnant lieu au versement de revenus hors du pays) et lui ajoute la valeur des produits et services effectués à l'étranger par des résidents (entreprises ou personnes qui ont donc reçu des paiements de revenus à l'étranger). En dehors de ces ajustements comptables correspondant à la balance des paiements, le PNB présente les mêmes défauts et qualités que le PIB.Pour évaluer la richesse, on utilise souvent le Revenu national brut (RNB) qui fournit une mesure des revenus monétaires acquis durant l'année par les ressortissants d'un pays. Cet agrégat comptable est, au niveau d'un pays, peu différent de la production nationale brute du fait que le PNB est égal à la somme des revenus bruts des secteurs institutionnels, à savoir de la rémunération des salariés, des impôts sur la production et des importations moins les subventions, de l'excédent brut d’exploitation (assimilé au revenu des entreprises) et du solde de revenu avec l'extérieur.Mais les données de patrimoine constituent de meilleures mesures de la richesse proprement dite. Il est difficile toutefois d'obtenir des évaluations comparables du patrimoine quand bien même on se limite seulement aux valeurs monétaires. Le problème devient encore plus ardu si on veut inclure des évaluations du patrimoine physique (immeubles, usines, outils de production, etc.), du patrimoine culturel (monuments, œuvres d'art présentes dans des musées, etc.), et plus encore du patrimoine social. Il serait nécessaire pour cela d'établir des conventions comptables et, si l'on veut effectuer des comparaisons internationales, de se mettre d'accord au niveau mondial sur leur utilisation. Or de telles opérations nécessitent des négociations et des accords internationaux très longs à réaliser.La montée de la mondialisation financière depuis les dérégulations impulsées par les administrations Reagan et Thatcher s’est traduite depuis les années 1980 dans un développement considérable des besoins d’information sur les évolutions des marchés financiers internationaux et les données financières relatives aux obligations d'information et aux états financiers des sociétés cotées. Depuis cette époque, les chiffres de la croissance et du PNB voisinent de plus en plus avec le spectacle des évolutions de l’euro, du dollar et du yen d’un côté, du Dow Jones, du NASDAQ, du Nikkei ou du CAC 40 de l’autre. En effet, les acteurs de la mondialisation que sont les cadres des entreprises financières ou non financières tournées vers l'exportation ont besoin de suivre quotidiennement ces variables de base de leurs arbitrages que sont les taux de change et les niveaux de valorisation boursière. Ainsi, avec la multiplication des expositions des firmes et des nations aux risques de change et aux risques financiers se développent des besoins d’indicateurs en tout genre, de « risque client » ou de « risque pays émergent », associés à chaque type de transactions. La Caisse des dépôts et consignations a créé ainsi des indicateurs synthétiques de libéralisation financière et de crise bancaire informant sur les vulnérabilités associées aux opérations financières mondialisées dans les pays émergents.Les critiques ont été historiquement nombreuses vis-à-vis des indicateurs économiques « classiques ». Marilyn Waring, première femme députée au Parlement néo-zélandais, a souligné que les tâches ménagères et le temps consacré par les parents à l’éducation des enfants, en particulier par les femmes et surtout les femmes dites « inactives », étaient occultés par les mesures de production par individu. En outre, des indicateurs comme le PIB mesurent mal l'économie informelle ou les services domestiques comme le faisait remarquer Alfred Sauvy. Enfin, ils se concentrent sur la valeur ajoutée, et non sur la richesse possédée (stock de capital). Dès lors, une catastrophe naturelle qui détruit de la richesse va pourtant contribuer au PIB à travers l'activité de reconstruction qu'elle va générer. Cette contribution ne reflète pas la destruction de capital, ni le coût de la reconstruction. Cette contradiction était dénoncée dès 1850 par l'économiste français Frédéric Bastiat qui, dans Sophisme de la vitre cassée, écrivait que « la société perd la valeur des objets inutilement détruits », ce qu'il résumait par : « destruction n'est pas profit. »Depuis la fin des années 1980, de multiples mouvements ont mis en cause les capacités du PNB à représenter toutes les dimensions du niveau de vie. Ainsi, au début des années 1990, certaines institutions internationales du système de l'Organisation des Nations unies ont fait un travail de pionnier en proposant de nouveaux indicateurs de développement. La collaboration d'économistes comme Amartya Sen, avec le Programme des Nations unies pour le développement (PNUD), a permis de proposer successivement toute une batterie de nouveaux indicateurs multidimensionnels du développement qui incluent, en plus du PNB, des critères sociaux. Le plus connu est l'Indice de développement humain (IDH). Depuis, de nombreuses autres initiatives se sont multipliées.Le PIB (défini ici comme ""la valeur monétaire des biens et services produits durant une certaine période dans un pays""), est un indicateur très superficiel car :- c'est un indicateur global, qui ne tient pas compte de la répartition de la richesse créée (même en le divisant par la population on obtient qu'un indicateur de la répartition potentielle de la richesse produite, et non de la répartition réelle) ? le PIB/population peut s'accroître avec la richesse d'une minorité de la population tandis qu'une majorité devient plus pauvre ;- c'est un indicateur à court terme, qui ne tient pas compte de l'impact de la production sur la déplétion du capital naturel.- il repose sur l'hypothèse que le prix donne une mesure ""exacte"" de la qualité de la richesse produite globalement, c'est-à-dire de la mesure dans laquelle elle répond aux besoins de l'ensemble de la population; or les sondages d'opinions montrent que les budgets militaires (qui représentent une part importante du PIB et qui sont financés par les impôts payés par la population) ont toujours été supérieurs au niveau souhaité par la population, qui préfère que l'on consacre plus de ressources au sport, à la culture ou aux transports en commun; d'autre part une partie importante du PIB résulte d'achats provoqués par le conditionnement imposé que constitue la publicité dans les lieux publics, et qui donc gonflent artificiellement le PIB.Indice de la puissance économique d'une nation, le PNB mesure la richesse d'un pays. Mais il ne fournit qu'une mesure très approximative du bien-être des habitants qui y vivent. Il ne fournit en effet qu'une agrégation comptable des valeurs des différents biens et services marchands produits, quelles que soient les utilités de ces productions. Par exemple, le PNB ne prend pas en compte les externalités négatives de la production (les dégâts causés à l'environnement, les prélèvements sur le patrimoine, etc.). Il ne mesure pas non plus l'impact de toutes les activités non monétarisées et réalisées hors du champ économique proprement dit (travaux domestiques, éducation des enfants, activités artistiques, etc. – ensemble théorisés par l'opéraïsme italien sous le nom de « travail social », et qui concerne souvent les femmes), lesquelles augmentent le bien-être général.Dans le cas des États-Unis, par exemple, le PNB agglomère indistinctement la production de biens qui ne contribuent pas directement au bien-être des habitants (aides au développement, etc.), avec celles des biens ou services produits et consommés par les américains.L'indice de développement humain (IDH) est le premier des indices créés par le Programme des Nations Unies pour le développement (PNUD). Utilisé depuis les années 1990, l'IDH combine trois facteurs permettant d'apprécier les « capacités » des résidents de ces pays (leurs capabilities selon l'économiste Amartya Sen) : l'espérance de vie à la naissance,l'accès à l'éducation, mesuré à partir de la durée moyenne de scolarisation des adultes (en années) et de la durée attendue de scolarisation des enfants en âge scolaire (en années).ainsi que le niveau de vie réel par habitant calculé à partir du logarithme du revenu national brut par habitant en parité de pouvoir d'achat (PPA).L'IDH classe les pays en établissant la moyenne entre ces trois indices principaux « normalisés » (c'est-à-dire ramenés à une échelle de 0 à 1).Le PNUD publie trois indicateurs synthétiques intégrant d'autres dimensions que l'activité économique :D'abord, à partir de 1995, l'Indicateur Sexospécifique (ou sexué) de développement humain (ISDH), qui permet de corriger l'IDH d'un facteur d'autant plus positif que les différences entre les situations des femmes et des hommes sont moins importantes du point de vue des trois critères pris en compte dans le développement humain.Puis, à partir de 1995 également, l'Indicateur de Participation des Femmes (IPF) à la vie économique et politique, lequel complète le précédent en faisant la moyenne d'un certain nombre de taux de participation des femmes à des postes politiques ou économiques valorisés.L'Indicateur de pauvreté humaine (IPH) est introduit à partir de 1997. Il est construit sous un autre principe que celui des capabilities de Amartya Sen. Il signale les manques, privations ou exclusions fondamentales d'une partie de la population en tenant compte de quatre facteurs : longévité, éducation, emploi et niveau de vie. Deux variantes de calculs sont distinguées :une variante 1 (IPH-1) pour les pays économiquement en développementune variante 2 (IPH-2) pour les pays économiquement développés. Pour les pays développés, l’IPH-2 tient compte de quatre critères auxquels il accorde le même poids : la probabilité de mourir avant soixante ans, l'illettrisme, le pourcentage de personnes en deçà du seuil de pauvreté, soit 50 % du revenu médian, le pourcentage de chômeurs de longue durée.Au début des années 2000, dans la lignée du mouvement impulsé par le PNUD, de nombreuses institutions se sont mises à discuter des limites du PNB pour tenter de les dépasser. En novembre 2004 à Palerme, l'Organisation de coopération et de développement économiques (OCDE) a organisé un premier Forum mondial de l’OCDE sur ce thème. Fin juin 2007, l'OCDE a organisé un second colloque à Istanbul portant sur « les statistiques, les connaissances et les politiques ». Il a débouché sur une déclaration énergique exhortant les bureaux statistiques du monde entier à « ne plus se limiter aux indices économiques classiques comme le produit intérieur brut (PIB) ».Face aux mises en cause multiformes de la mondialisation, il s’agissait d’abord, comme le déclarait le secrétaire général de l'OCDE Angel Gurria, de « mesurer en quoi le monde est devenu meilleur ». Afin de mettre en œuvre et généraliser cette déclaration signée par l’ONU et le PNUD, la Commission européenne a réuni les 19 et 20 novembre 2007 à Bruxelles un colloque international dénommé Beyond the GDP (Au-delà du PIB), durant laquelle son président, José Manuel Durão Barroso, défendait la mise en place de nouveaux indices pour mesurer les problèmes contemporains.Ces réunions institutionnelles ont rassemblé une large part des nombreux indicateurs alternatifs mis au point dans le monde entier afin d'évaluer le bien-être social et environnemental. Parmi ces indicateurs synthétiques alternatifs, certains concernent les problèmes sociaux contemporains, d'autres les inégalités et la pauvreté, la sécurité économique et sociale ou le patrimoine écologique.L’Indice de santé sociale (ISS) a été mis au point aux États-Unis par deux chercheurs, Marc et Marque-Luisa Miringoff. L’ISS est un indice social synthétique visant à compléter le PIB pour évaluer le progrès économique et social. C'est une sorte de résumé des grands problèmes sociaux présents dans le débat public aux États-Unis dans les années 1990. Il se traduit dans seize indicateurs sociaux dont il fait une sorte de moyenne. Sont ainsi regroupés dans cet indice des critères de santé, d'éducation, de chômage, de pauvreté et d'inégalités, d'accidents et de risques divers. L'ISS a acquis une grande réputation internationale en 1996, année de la parution d'un article majeur dans la revue économique Challenge montrant le décrochage des courbes de progression du PNB et de l'ISS aux États-Unis, le premier continuant à progresser alors que le second plongeait durablement après les années 1973-1975. Ce graphique montre ainsi en quoi les années Reagan et Bush père ont porté un rude coup à la santé sociale des États-Unis, laquelle se trouvait en 1996 à un niveau nettement inférieur à celui de 1959, en dépit d’une très belle courbe de croissance économique.Le BIP 40 est un indicateur synthétique de l'évolution des inégalités en France dont le nom est une référence ironique à la fois au PIB (inversé) et au CAC 40. Cet indicateur a été mis au point et présenté à la presse en 2002 par réaction au fait que la santé économique et la santé boursière ont droit à des indices synthétiques fortement médiatisés, alors que ce n'est pas le cas pour ceux de la « santé sociale ». Cela même si l’Insee publie de nombreuses études et indicateurs sur le sujet. L'équipe de militants syndicalistes, d'économistes et de statisticiens français qui ont agrégé des indicateurs pour former le BIP 40 est associée à un réseau associatif militant pour la réduction des inégalités, le Réseau d’alerte sur les inégalités (RAI).De façon récente, des chercheurs de grandes institutions internationales (comme Guy Standing au BIT à Genève) et de pays développés (tels Lars Osberg et Andrew Sharpe au Canada ou Georges Menahem en France) ont mis au point des indicateurs visant à cerner le degré de protection économique des personnes contre les principaux risques de perte ou de diminution forte de leurs revenus, par exemple en matière de chômage, de maladie, de retraite, etc.  L'indicateur de bien-être économique de Osberg et Sharpe Osberg et Sharpe prennent ainsi en compte quatre composantes caractérisant le bien-être des populations dans la construction d’un Indicateur du bien-être économique (IBEE) :les flux effectifs de consommation par habitant, qui incluent la consommation de biens et services marchands, les flux effectifs par habitant de biens et services non marchands et les changements dans la pratique des loisirs ;l’accumulation nette dans la société des stocks de ressources productives, y compris l’accumulation nette de biens corporels et de parcs de logements, l’accumulation nette de capital humain et des investissements en Recherche & Développement (RD), les coûts environnementaux et la variation nette du niveau de l’endettement extérieur ;la répartition des revenus, selon l’indice de Gini sur l’inégalité, ainsi que l’ampleur et l’impact de la pauvreté ;la sécurité économique contre le chômage, la maladie, la précarité des familles monoparentales et des personnes âgées.Grâce à leur indicateur ils sont en mesure de comparer les tendances d’évolution du bien-être économique dans six pays de l’OCDE : États-Unis, Royaume-Uni, Canada, Australie, Norvège et Suède. Des comparaisons sont ainsi données sur le site du laboratoire de ces deux chercheurs canadiens. Une application de l'IBEE au cas de la France a été proposée par Florence Jany-Catrice et Stephan Kampelmann en juillet 2007. L'indicateur de sécurité de Standing à l'OIT Dans les travaux de Guy Standing effectués dans le cadre du Bureau international du travail (BIT), la vision est centrée sur le travail et vise à cerner la sécurité économique dans sept domaines. Dans un deuxième temps, un indice synthétique permet d'effectuer une moyenne de ces sept domaines : les revenus (y compris les prestations sociales), la participation à l’activité économique, la sécurité d’emploi, la sécurité du travail (contre les risques d'accidents ou de maladies professionnels), la sécurité des compétences et qualifications, la sécurité de carrière, et enfin celle de la représentation syndicale et d’expression des salariés. Une série de grandes enquêtes ont ainsi été menées par les missions locales du BIT dans une vingtaine de pays. Les pays scandinaves sont à nouveau aux premières places pour cet indicateur. L'indicateur de sécurité de Menahem En France, Georges Menahem a mis au point en 2005 un indicateur baptisé taux de sécurité économique (TSE). Selon ses dernières publications, la sécurité économique peut être décomposée entre une partie « marchandisée » dépendante des relations salariales et de la vente des produits, et une partie ""démarchandisée"" relative aux prestations et aides auxquelles les individus ont droit indépendamment de leurs relations actuelles avec le marché (comme la retraite, les allocations familiales, de logement, de chômage ou le RMI). Ses estimations sur une trentaine de pays montrent que le taux de sécurité démarchandisée est un bon indicateur de l'efficacité du système de protection sociale : il est maximum en Suède et dans les pays Nordiques, il est encore important dans les pays continentaux tels l'Autriche, l'Allemagne ou la France, mais il est faible au Royaume-Uni et dans les pays Européens du Sud comme l'Italie, la Grèce ou l'Espagne et très limité dans les pays d'Europe Centrale et Orientale tels la Lettonie ou la Lituanie. Quant aux États-Unis, leur taux de sécurité démarchandisée est négatif, ce qui témoigne du mauvais état des protections sociales dans ce pays présenté comme un modèle de l'économie de marché. Ce taux n'est que faiblement positif dans deux autres exemples du modèle « libéral » selon le sociologue danois Gosta Esping-Andersen : en Australie et au Canada, à un niveau à peine plus élevé pour ce dernier car les programmes sociaux y sont plus étendus.L'Empreinte écologique est un indicateur visant à mesurer les pressions économiques sur l'environnement. L’empreinte écologique d’une population est la surface de la planète, exprimée en hectares, dont cette population dépend compte tenu de ce qu’elle consomme. Les principales surfaces concernées sont consacrées à l’agriculture, à la sylviculture, à la pêche, aux terrains construits et aux forêts capables de recycler les émissions de CO2. Il s’agit d’un indicateur synthétique, qui « convertit » en surfaces utiles de multiples pressions humaines sur l’environnement, mais pas toutes.On peut calculer cette empreinte pour une population allant d’un seul individu à celle de la planète, et par grands « postes » de la consommation. Par exemple, la consommation alimentaire annuelle moyenne d’un Français exige 1,6 hectare dans le monde ; son empreinte totale (alimentation, logement, transports, autres biens et services) est de 5,3 hectares. Pour un Américain, on obtient 9,7 hectares : le record du monde. Or l’empreinte par personne « supportable » par la planète aujourd’hui, compte tenu des rythmes naturels de régénération des ressources était de 2,9 hectares en 1970, et elle ne cesse de diminuer sous l’effet de la progression de la population, de la régression des terres arables, des forêts, des ressources des zones de pêche, etc. Elle est passée à 2 hectares en 1990 et elle n’est plus que de 1,8 hectare en 2001. Si tous les habitants de la planète avaient le mode de vie des Américains, il faudrait 5,3 planètes pour y faire face. Si tous avaient le niveau de vie moyen des Français, il en faudrait près de trois.De nombreux rapports ont déjà été produits, dont ceux particulièrement documentés et fiables du Global environmental conservation organization (soit le World Wide Fund for Nature ou WWF). Mais leurs conséquences sont limitées compte tenu de la faible visibilité dans la sphère publique de ce problème, ses conséquences négatives sur la vie quotidienne ne touchant pas encore vraiment les acteurs économiques, politiques et médiatiques dominants et les nations les plus favorisées même si leur empreinte écologique est pourtant de loin la plus importante. De ce fait, elles peuvent croire encore dans les bénéfices d’une croissance matérielle soutenue et indéfinie, les indicateurs des limites de notre planète matériellement finie étant difficiles à percevoir.L'empreinte écologique est un indicateur abstrait et synthétique qui ne traduit qu'une faible part des conséquences du dérèglement du climat et des dégradations des écosystèmes. La comparaison de l'empreinte de l'Afrique et de l'Europe montre certes que les pays les plus pauvres ont encore, pour quelque temps, une empreinte écologique par personne très supportable par la planète, ce qui permet aux pays favorisés d'utiliser bien plus que leur surface. Ainsi, les dommages restent au faible niveau des premiers signes que nous observons actuellement. Selon le WWF, ce résultat traduit une dette écologique des pays riches par rapport aux pays pauvres : les premiers « empruntant » aux seconds d’énormes surfaces de ressources naturelles, terres arables, forêts, aux pays du Sud. Tout se passe comme s'ils y exportaient leur pollution, au moins celle qui ne connait pas de frontière, à commencer par celle des gaz à effet de serre.Mais l'empreinte écologique est limitée car elle ne permet d'illustrer que très indirectement l'importance des conséquences du réchauffement climatique :L’accélération du réchauffement climatique dans la période récente directement liée aux émissions d’origine humaine de gaz à effet de serre.La dimension des catastrophes humaines mondiales prévisibles au-delà d’un réchauffement de deux degrés : sècheresses, inondations et tempêtes, élévation du niveau des mers, etc.L'importance et la diversité de ces catastrophes à venir suggère qu'il faudrait compléter l'empreinte écologique par une batterie d'indicateurs d'inégalités économiques et sociales afin d'évaluer en quoi certaines populations plus pauvres sont davantage touchées par ce que les populations riches nomment les « aléas » climatiques. La moitié de la population mondiale vit ainsi dans des zones côtières qui risquent d'être submergées si le niveau des mers s’élevait d’un mètre, évolution possible pour le siècle à venir selon le Groupe d'experts intergouvernemental sur l'évolution du climat (GIEC) si les tendances actuelles persistent.Jean Gadrey et Florence Jany-Catrice, Les nouveaux indicateurs de richesse, Édition La Découverte, 2005Dominique Méda,  Au-delà du PIB. Pour une autre mesure de la richesse, Champs-Flammarion, 2008Rapport mondial sur le développement humain 2005 : La coopération internationale à la croisée des chemins : l’aide, le commerce et la sécurité dans un monde marqué par les inégalités, New York, Programme des Nations unies pour le développement, janvier 2005, 385 p. (ISBN 2-7178-5114-3, lire en ligne), chapitre 2 « Inégalité et développement humain ».WWF: Global environmental conservation organization (ou Organisation de conservation de l'environnement mondial)PIB et développement durableHappy planet indexIndice rouge à lèvresIndicateur du développementLes grands indicateurs de l'économie française, site de l'InseeLes indicateurs économiques du Loiret , iD Loiret Portail de l’économie"
économie;"Selon le vocabulaire de la comptabilité nationale, l’investissement (mesuré par la formation brute de capital fixe, en abrégé FBCF), peut être le fait de différents agents économiques :pour les entreprises : c'est la valeur des biens durables acquis pour être utilisés pendant au moins un an dans leur processus de production. Il peut avoir trois formes : capacité, remplacement et productivité ;pour les ménages : la FBCF dans le cadre de leur activité domestique ne concerne que l'acquisition ou la production pour leur propre compte de logements ;pour les entrepreneurs individuels : la formation brute de capital fixe (FBCF) des ménages en tant qu'entrepreneurs est comptée dans la FBCF des entreprises.Les investissements financiers, les acquisitions de terrains et les investissements immatériels (publicité, etc.) ne sont pas comptabilisés dans la FBCF, bien que ces investissements aient pris depuis les années 1980 une grande importance dans la stratégies des sociétés.Le rendement d'un investissement fait l'objet d'un calcul prenant en compte sa durée de vie ou sa durée d'utilisation (avec dans ce cas la prise en compte de valeur finale résiduelle de l'investissement).L'investissement durable stratégique désigne un type d'investissement se voulant plus vertueux, dans la perspective de développement durable, ce qui implique de prendre en compte dans la prise de décision d'investissement de nouveaux paramètres comme la durabilité et la soutenabilité.L'impact d'un investissement sur une entreprise, qui produit et/ou met à disposition des biens et des services, peut être financier ou uniquement en propriété.Cet impact est financier si l'entreprise reçoit effectivement le montant de l'investissement. Cet investissement va donc augmenter son capital social.Seul un investissement sur le marché primaire (ex : lors de la fondation d'une société ou lors d'une émission d'actions d'une société existante) a un impact financier sur l'entreprise. Cet investissement sert en général à acquérir ou améliorer des moyens de production (machines, locaux, informatique, etc.).Un investissement sur le marché secondaire (ex : un produit financier d'une assurance vie composé d'un « panier » d'actions) est un échange financier (ex : entre un particulier et sa banque) dont pas un sous ne va aux entreprises dont les actions composent le panier.Dans tous les cas, que ce soit un investissement sur le marché primaire ou secondaire, il a un impact sur la propriété. Ainsi, les actions rachetées par d'autres lors d'une OPA peuvent provoquer des changements importants d'actionnaires qui, en vertu des pouvoirs que donne la propriété des actions, sont en mesure de modifier profondément les destinées de l'entreprise et de ses salariés, alors même que l'entreprise elle-même n'a pas reçu un sous : la transaction est uniquement entre investisseurs. Il en est de même lorsque des membres d'un C.A. de sociétés sont des gestionnaires de fonds d'investissement ou de pensions tenus de rentabiliser les placements de leurs petits ou gros épargnants.Ces deux marchés (primaire et secondaire) nous suggèrent une typologie des investissements :Investissements ayant une finalité d'accroissement du capital technique (ou capital fixe, ou capital productif) ;Investissement financier dont la finalité est de rechercher une contrepartie (placement) ou à plus ou moins long terme un gain financier (plus-value).Marx parle de cet investissement d'une manière que certains qualifieraient de manichéenne dans « Das Kapital, Band 2, Abschnitt 1, 1.4 Der Gesamtkreislauf » :« Geldmachen ist das treibende Motiv. Produktion erscheint nur als notwendiges Übel dazu. » soit « Gagner de l'argent est le motif moteur. Pour cela, la production n'apparaît que comme un mal nécessaire ». … à défaut de pouvoir s'en défaire ou d'en rêver comme il le précise dans la parenthèse ensuite : « Alle kapitalistischen Nationen ergreift periodisch ein Schwindel, den sie zur Geldmacherei frei von lästiger Produktion nutzen. » soit « Toutes les nations capitalistes ont périodiquement une chimère, celle de pouvoir faire du fric en se passant d'une production pesante ennuyeuse ».Ce rêve de légèreté et de vitesse des investissements se réalise justement dans la sphère financière, dans le marché secondaire, avec des « produits » financiers de toute sorte et les systèmes « électroniques » pour les transactions internationales. Il se réalise également dans la sphère de l'économie réelle, parfois au détriment de PdG trop adeptes d'une logique industrielle ou sociale et pas assez d'une logique « financière ».Ainsi, Pierre Suard, ancien PdG d'Alcatel, a été nommé par des investisseurs dont la finalité était productive. Il a créé un empire industriel à l'image de Siemens, son concurrent le plus semblable. Il a été débarqué et remplacé par Serge Tchuruk en 1995 après l'arrivée de nouveaux actionnaires dont les investissements étaient plutôt à finalité « financière ». Ce dernier a concentré Alcatel sur son « cœur de métier », escompté le plus rentable, et a vendu le reste. Le changement de slogan qui a suivi, même du « cœur de métier », est révélateur d'un changement de finalité des investissements, moins industriel et plus financier : le slogan « être un architecte d'un monde internet » est remplacé par « apporter de la valeur ajoutée aux actionnaires ».En mars 2021, la même mésaventure arrive à Emmanuel Faber, PdG de Danone débarqué sous l'impulsion d'actionnaires anglo-saxons insatisfaits des résultats financiers de leurs investissements, obérés, d'après eux, par la politique sociale de ce PdG.À la vue des réalités économiques actuelles, il semble que l'influence des investissements « financiers », y compris sur le marché primaire, soit de plus en plus grande.Cette domination des investissements « financiers » peut aussi s’apprécier en considérant les flux financiers : (1-) les flux financiers correspondant au marché primaire (à savoir investissements productifs) sont beaucoup moins importants que ceux correspondant au marché secondaire (à savoir investissements « financiers ») ; (2-) même au niveau du marché primaire, il semble que l'investisseur souhaite minimiser, rendre ""marginale"" son investissement et il a à sa disposition les outils juridiques pour le faire.En effet, le plus souvent, les entreprises investissent directement soit en recyclant une partie de leurs bénéfices, soit surtout en empruntant directement sur les marchés bancaires ou obligataires. La part d’investissement par le marché primaire (ex : par émission d'actions) est minime au regard de leur investissement direct : en 2016 investissement par émission d'actions : 22 M€ ; par emprunt des entreprises : 297 M€ (source : LaTribune et Insee). De plus, il faut déduire des investissements sur le marché primaire la part de plus en plus importante de « rachat d'actions » par l'entreprise sur ordre de ses « investisseurs », ce « rachat » consiste à reverser à ceux-ci les montants de la valorisation d'une partie de leurs actions pour les « annuler ». Souvent l'entreprise doit emprunter pour cela.Enfin, l'investissement, au regard des investissements directement faits par les entreprises, est à considérer en tenant compte du concept de « responsabilité limitée » conjugué avec la non réalité juridique de l'entreprise : les investisseurs d'une entreprise ont de fait la propriété et le contrôle de TOUS les moyens de production de celle-ci alors même qu'ils n'y ont que peu contribué par leur argent.Le concept de « responsabilité limitée » et sa mise en œuvre dans les lois au XIXe siècle (ex : en France, lois du 23 mai 1863 puis du 24 juillet 1867 ; en Angleterre lois de 1856 à 1862 sur les Joint-Stock Company limited) compte, d'après Y.N. Harari dans son célèbre ouvrage Sapiens, « parmi les inventions les plus ingénieuses de l’humanité » : « Peugeot est une création de notre imagination collective. Les juristes parlent de « fiction de droit ». Peugeot appartient à un genre particulier de fictions juridiques, celle des « sociétés anonymes à responsabilité limitée ». L’idée qui se trouve derrière ces compagnies compte parmi les inventions les plus ingénieuses de l’humanité ». Harari en explique les avantages : « Si une voiture tombait en panne, l’acheteur pouvait poursuivre Peugeot, mais pas Armand Peugeot. Si la société empruntait des millions avant de faire faillite, Armand Peugeot ne devait pas le moindre franc à ses créanciers. Après tout, le prêt avait été accordé à Peugeot, la société, non pas à Armand Peugeot, l’Homo sapiens » actionnaire !Cette explication montre que la « responsabilité limitée » est en fait non pas une limitation des risques mais un véritable transfert de responsabilité et des risques de l'investisseur-actionnaire à la société-entreprise, à son collectif de travail, responsabilité pénale et économique. Toutefois ce transfert ne s'accompagne pas en retour d'un transfert de propriété du fait de la non réalité juridique de l'entreprise : quel que soit le montant investi par l'investisseur-actionnaire il a toujours le pouvoir et est propriétaire de fait (de par sa possession des actions) de tous les moyens de production (locaux, machines, moyens informatiques, etc.), y compris de ceux acquis grâce aux « millions » empruntés : c'est l'entreprise, qui acquiert en empruntant, qui rembourse, et qui entretient à ses frais les moyens de production en plus, bien entendu, de payer les salaires, charges et taxes.Grâce à cette « responsabilité limitée » conjuguée avec la non-réalité juridique de l'entreprise, plusieurs procédés permettent aux investisseurs-actionnaires d'accroître les moyens de production qu'ils contrôlent en minimisant au maximum leur mise (le capital social) : investissement par effet de levier, achat à effet de levier, rachat d'actions. Il est donc très compréhensible que les investisseurs-actionnaires recourent à ces procédés plutôt que d’émettre des actions supplémentaires provoquant l'arrivée d'autres investisseurs-actionnaires avec qui certes les risques sont partagés mais également le pouvoir et la propriété. Si l'entreprise était, comme une association 1901, sujet de droit, la « responsabilité limitée » serait remplacée par les « responsabilités et propriétés partagées » entre actionnaires et le collectif de travail de l'entreprise, chacun selon sa contribution. Les procédés « à effet de levier » et autres au profit de certains ne seraient plus et beaucoup d'autres s'en réjouiraient.Sous la finalité générale d'accroissement du capital technique (ou capital fixe, ou capital productif) des objectifs plus précis peuvent être visés :l'investissement de remplacement ou de renouvellement, a pour but de maintenir l'activité à son niveau actuel ;l'investissement de modernisation ou de productivité, a pour but d'accroître la productivité en introduisant des équipements modernes et perfectionnés ;l'investissement de capacité ou d'expansion, a pour but d'augmenter la capacité de production de l'entreprise en ajoutant par exemple des unités de production que ce soit d'un produit déjà existant, il s'agit alors d'une expansion quantitative, ou d'un nouveau produit - on parle alors d'expansion qualitative ;l'investissement total.L'investissement peut être qualifié de :productif : attention double sens possible :soit renvoie à l'idée qu'il s'agit d'un investissement de nature directement productive,soit renvoie à l'idée de l'efficacité de son rendement : la valeur cumulée des biens et des satisfactions obtenues est supérieure voire très supérieure au coût investi ;non directement productif (voire improprement qualifié d'improductif): il concerne des biens et des services d'utilité publique (écoles, hôpitaux, etc) ;matériel : il se traduit par la création d'un bien ou actif réel (un bien de production, par exemple) ;immatériel : il concerne des services : formation, recherche-développement, innovation, marketing, technologies de l'information, publicité, etc., susceptibles d'apporter un développement futur ;financier : il doit être considéré à part compte tenu de ce que sa finalité est de rechercher une contrepartie (placement) ou à plus ou moins long terme un gain financier (plus-value) ;stratégique, lorsqu'il est jugé essentiel pour la survie ou l'avenir de l'investisseur ;réputationnel, lorsqu'il contribue ou est nécessaire à la réputation de l'entreprise ou à son maintien, avec par exemple la publicité, l'acquisition de certains labels et certifications, certaines formes de mécénat, le rappel de produit.L'investissement « productif » se décompose d'abord en bâtiments puis en équipements.La manière dont sont enregistrées et répertoriées les dépenses d'investissement peut conduire à des difficultés pratiques :Par exemple, les dépenses en technologies de l'information sont habituellement rattachées à des centres de coût dans les entreprises. Or dans ce type de dépenses, 50 % en moyenne[réf. nécessaire] concerne la maintenance d'applications existantes, (dépenses d'exploitation) les 50 % restant concernent les développements (dépenses d'investissement). Or la distinction est souvent perdue dans la comptabilité des entreprises (avec un impact fâcheux sur l'évaluation objective de l'effort d'investissement et/ou d'innovation).On parle d'investissement brut quand le flux d'investissement comprend l'investissement neuf et l'investissement de remplacement.Le calcul de l'Investissement net s'obtient par différence entre : Capital technique de fin de période - Capital technique en début de période. Il représente l'investissement brut moins l'amortissement. Selon la théorie économique L'investissement doit être fait jusqu'au point où son bénéfice marginal égale son coût marginal. Ceci suppose évidemment que les biens d'investissements nécessaires soient disponibles. Selon le critère de la rentabilité Investir revient à engager de l'argent dans un projet, en renonçant à une consommation immédiate ou à un autre investissement (coût d'opportunité) et en acceptant un certain risque, pour accroître ses revenus futurs.La rentabilité est mesurable selon différentes méthodes qui ne donnent pas toutes toujours exactement le même résultat, tout en restant globalement cohérentesle retour sur investissement, qui peut s'exprimer en taux ou en temps, mesure le ratio des sommes rapportées par l'investissement sur le montant investi ;la valeur actuelle nette : l'investissement rapporte la différence entre son coût et la VAN, qui dépend du taux d'actualisation retenu ; elle diffère du retour sur investissement en ce qu'elle tient compte du montant total investi : par exemple, le retour sur investissement peut être meilleur pour l'achat d'une bicyclette que d'une maison (rendement respectif de 100 % et 1 %) et la VAN en sens inverse (VAN respective de 200 € et 100 000 €) ;le taux de rentabilité interne (TRI) : l'investissement est d'autant plus rentable que ce taux est élevé (cependant, pour un taux d'actualisation donné et connu, la VAN est un indicateur plus significatif, alors qu'on peut trouver deux investissements A et B tels TRIA > TRIB et VANB > VANA).On peut également assimiler à la rentabilité des critères tels que le temps nécessaire pour atteindre le point mort (durée nécessaire pour que les flux générés soient égaux au montant de l’investissement initial).Le risque pris par l'investisseur est aussi un critère important, dont un indicateur est le ratio de la capacité d'autofinancement par rapport au montant investi ; il est souvent fait à titre prévisionnel pour déterminer si un investissement proposé est adapté, et dans quelle mesure il satisfera l'investisseur.Quelle que soit la méthode utilisée, les paramètres suivants doivent être convenablement appréciés et intégrés dans le calcul :le capital investi a une durée prévue d'utilisation à la fin de laquelle il peut encore présenter une valeur résiduelle ;le prix relatif du capital par rapport à celui du travail influe sur l'investissement. Lorsque le prix du capital baisse par rapport à celui du travail, il est intéressant d'engager des investissements de productivité, qui permettent de substituer du capital (moins cher) au travail (plus cher) ;les taux d'intérêt déterminent le coût des emprunts contractés pour effectuer un investissement et peuvent donc freiner l'investissement s'ils sont élevés ;le niveau d'endettement de l'entreprise joue aussi : une entreprise endettée devra consacrer ses profits à son désendettement au risque de disparaître ;les entreprises cherchent à anticiper la demande avant d'investir pour savoir s'il est nécessaire d'augmenter leurs capacités de production. Ainsi, des anticipations favorables où l'on prévoit une hausse de la demande, favorisent l'investissement tandis que les anticipations défavorables qui prévoient une stagnation ou une baisse de la demande, le freinent. C'est le principe de la demande anticipée ou effective évoquée par Keynes. C'est la demande anticipée des entrepreneurs qui va déterminer l'offre. Autres méthodes financières D'autres méthodes existent qui s'inscrivent dans la mouvance des théories financières intégrant davantage l'incertitude future liée aux valorisations découlant du marché :James Tobin a proposé un critère, appelé le Q de Tobin qui compare la valeur boursière de l'investissement avec son coût de remplacement ;Dixit et Pindyck(1994) proposent de faire l'analogie avec les options: pour la décision d'investissement, l'entrepreneur a le choix entre ne rien faire (attendre) ou investir tout de suite, choix dont l'irréversibilité joue un rôle important dans la productivité.Dans sa décision d'investir, l'entrepreneur compare le cout de l'investissement (I) et la somme des valeurs, actualisées et pondérées par les risques, des rentrées de trésorerie obtenues grâce à l'investissement (R). Le projet d'investissement sera réalisé si R > I. Dans l'analyse keynésienne, l'efficacité marginale du capital désigne le taux de rendement interne de l'investissement. Elle sert de taux d'actualisation des recettes tirées de l'investissement. À savoir, l'investissement est d'autant plus important que le taux d'intérêt est faible. Pour Keynes, l'investissement dépend de la comparaison entre l'efficacité marginale r de l'investissement et le taux d'intérêt pratiqué sur le marché des capitaux, i. Si r > i, la décision de réaliser l'investissement est justifiée. Il peut être financé soit à partir de fonds dont dispose l'entreprise, soit à partir d'emprunt dont le coût est inférieur au taux de rendement de l'investissement. La formule de Keynes n’est valable que pour un investissement financé uniquement par la dette. Si une partie du financement est apportée en fonds propres, il est nécessaire d’en calculer le coût puis de calculer le coût moyen pondéré du capital, qui sera substitué à i. Par ailleurs, l’entrepreneur prendra une marge de sécurité car en pratique, le rendement de l’investissement ne sera pas égal à celui anticipé.Dans l'analyse macro-économique, le terme d'investissement est réservé à la seule création de biens capitaux nouveau (machines, immeubles...). Pour Keynes, l'investissement dépend de l'efficacité marginale du capital et du taux d'intérêt. En fait, les dépenses en biens d'investissement dépendent principalement de deux variables :le rendement attendu de l'investissement, dit ""efficacité marginale du capital"",le taux d'intérêt i ou coût d'emprunt contracté pour financer l'acquisition de biens d'investissement.Pour une efficacité marginale donnée, l'investissement apparaît comme une fonction décroissante du taux d'intérêt. Le niveau du taux d'intérêt est donc la variable incitatrice ou désincitatrice privilégiée du processus d'investissement. Dans l'analyse Keynésienne, l'investissement est considéré comme autonome, c'est-à-dire indépendant du revenu.Avant toute chose, le dirigeant doit faire tout d'abord son métier en restituant l'investissement dans la stratégie d'entreprise et l'organisation d'entreprise. À défaut, il risque de prendre des décisions hâtives en matière de moyens mais sans chemin pertinent et/ou dans une facilité trompeuse qui juge inutile la nécessité de cette réflexion.Avant d'engager ses ressources propres à l'investissement, l'entreprise doit en effet examiner toutes les solutions possibles pour financer son besoin de financement : autofinancement, recours à l'emprunt, leasing, aides publiques (pour la R&D), augmentation de capital ou financement par prélèvement sur fonds propres. Ces sources de financement peuvent être combinées.Il faut aussi noter que les investissements peuvent aussi être financés par cession d'actifs , (dans l'hypothèse où l'entreprise désinvestit dans le cadre d'une stratégie de réorientation ou de recentrage de ses activités).Le législateur offre des possibilités de réduction d'impôt sur le revenu et impôt de solidarité sur la fortune (ISF) pour les particuliers qui investissent dans les PME. La PME doit répondre à des critères quantitatifs (CA < 50M€, emploie de moins de 250 salariés) et à la définition de PME communautaire.Pour l'ISF la réduction fiscale ne concerne que la souscription au capital initial de la société ou la souscription à une augmentation de capital de celle-ci. La possibilité de réduire son ISF risque donc de ne profiter principalement qu'à un cercle réduit de contribuables, sollicités par leur entourage pour participer à ce type d'opérations souvent réalisée en cercle restreint.L'autofinancement est le financement des investissements par des moyens internes à l'entreprise. L'autofinancement se mesure de deux manières : le taux de marge qui donne une indication sur les ressources de l'entreprise (excédent brut d'exploitation / valeur ajoutée) et le taux d'autofinancement : EB/FBCF (Formation Brute de Capital Fixe) qui mesure la part de l'investissement qui est financée par l'épargne brute (partie de l'EBE, hors dividendes, intérêts et impôts, servant à financer la FBCF).Cela consiste à lever des capitaux sous forme de prêt auprès de tiers. La durée de l'emprunt doit être en accord avec la durée d'amortissement du bien acheté (en général l'emprunt est un peu plus court que celle-ci). L'emprunt peut être de 2 types : bancaire ou obligataire.Il s'agit d'augmenter les capitaux propres de l'entreprise en faisant souscrire de nouvelles parts (SARL) ou actions (SA). Il est demandé, via une opération d'augmentation de capital en numéraire,aux actionnaires de mettre la main à la poche pour financer les investissements ;et/ou à de nouveaux actionnaires d'entrer dans le capital de l'entreprise.Cette méthode a l'avantage de renforcer la solvabilité de l'entreprise, laquelle de toute façon ne peut dépasser un certain montant de recours à l'emprunt sans perdre la confiance de ses banques et fournisseurs. Cela dit, cette opération est assez souvent mal vue par les actionnaires, car l'émission de nouvelles actions va « diluer » la valeur de leurs actions actuelles.Cette méthode n'est donc utilisable que si les actionnaires acceptent de remettre de l'argent dans la société. Cela dépendra en grande partie :de la rentabilité des fonds propres affichée ou visée par l'entreprise. Cette rentabilité et le risque qui lui est associé doit être comparée aux autres couples rentabilités/risques disponibles par ailleurs ;et, pour les sociétés cotées, du cours de bourse, qui doit être supérieur au prix d'émission des nouvelles actions pour qu'il y ait intérêt à souscrire celles-ci ;ou encore, facteur plus négatif mais qui entraîne une pression forte sur les actionnaires, d'une situation d'endettement critique risquant de faire sombrer l'entreprise si elle ne trouve pas de l'argent frais pour conforter ses capitaux propres.L'augmentation de capital en numéraire ne doit pas être confondue avec celle par incorporation de réserves (il ne s'agit alors que d'un transfert de poste comptable à l'intérieur des capitaux propres) ni celle par échange de titres (cas de fusion-acquisition)On parle de mal-investissement lorsque l'investissement est inadéquat : trop élevé (sur-investissement), trop faible (sous-investissement), ou les deux à la fois (i.e. : mal orienté).La décision d'investir ou de ne pas le faire, est toujours une forme de pari sur l'avenir : il n'est donc pas étonnant de rencontrer des investissements inadéquats. Lorsqu'une accumulation d'investisseurs se trouvent commettre la même erreur, plus ou moins simultanément, celle-ci peut générer - au niveau macro-économique, dans une filière d'activité ou dans une zone géographique - des situations pouvant aller de la simple récession à la crise économique de plus grande ampleur (voir l'analyse du cycle économique).En régime d'économie libre, la variable essentielle en la matière est le taux d'intérêt. Trop élevé, il rend impossible l'investissement même dans des projets a priori rentables. Trop bas, il favorise l'investissement dans des projets à la rentabilité trop faible.Des agents économiques trop optimistes peuvent sur-investir et créer des capacités de production excédentaires par rapport à la demande effective exprimée par le marché. À l'échelle d'un pays, ou d'une branche d'activité, l'insuffisance constatée des débouchés par rapport à l'offre ainsi créée va provoquer un effet déflationniste et la faillite des entreprises marginales (celles dont le prix de revient est le plus élevé).Goal-based investingDirection générale des entreprises (France)Oséo (France)Small Business Administration (États-Unis)Direction générale des entreprises (Espagne)Ressource relative à la santé : (en) Medical Subject Headings Ressource relative aux beaux-arts : (en) Grove Art Online  Portail de l’économie   Portail de la finance"
économie;"La macroéconomie est une discipline de l'économie qui étudie le système économique au niveau agrégé à travers les relations entre les grands agrégats économiques que sont le revenu, l'investissement, la consommation. La macroéconomie constitue l'outil essentiel d'analyse des politiques économiques des États ou des organisations internationales. Il s'agit d'expliquer les mécanismes par lesquels sont produites les richesses à travers le cycle de la production, de la consommation, et de la répartition des revenus au niveau national ou international.La macroéconomie est une branche de la science économique. Elle se consacre à l'étude des grandes variables économiques nationales ou internationales, et aux relations entre ces variables. Elle repose sur une approche globale, quoiqu'elle puisse se fonder sur des comportements microéconomiques et micro-ondes.Parce qu'elle fonctionne par la comparaison d'agrégats, la macroéconomie est avant-tout une représentation hiérarchisée de l'économie, articulée entre ses agents, via des flux. Elle cherche à expliciter ces relations et à prédire leur évolution face à une modification de certaines variables. La macroéconomie permet par exemple d'estimer la réaction d'un système économique possédant telles caractéristiques face à un choc pétrolier, ou à une politique économique particulière.Contrairement à la microéconomie, qui favorise les raisonnements en équilibre partiel, la macroéconomie se place toujours dans une perspective d'équilibre général, ce qui l'amène à accorder plus d'attention au bouclage des modèles et à la dynamique de création et de maintien d'institutions essentielles, comme les marchés, la monnaie.La macroéconomie a évolué à travers le temps pour devenir plus précise et plus sûre. Ses origines se trouvent dans les premiers travaux économiques du XVIIIe siècle (voir Histoire de la pensée économique), mais elle prend véritablement forme grâce aux travaux de John Maynard Keynes. Sa théorie, le keynésianisme, se fonde sur ce qui fut appelée la macroéconomie keynésienne, qui est l'interprétation keynésienne de la macroéconomie. Elle crée des outils de base de la macroéconomie (le modèle IS/LM, la courbe de Phillips, etc.).La macroéconomie n'est aujourd'hui plus rattachée à une quelconque école de pensée. Elle a évolué vers la construction de modèles économiques complexes, incluant à la fois des relations supposées entre variables et des relations comptables servant à définir les agrégats. Très utilisés pour analyser et prévoir les résultats des politiques économiques, ces vastes modèles qualifiés, le plus souvent, d'économétriques (les plus frustes comportent une dizaine d'équations, les plus complexes dépassent les 1 500) sont à l'heure actuelle employés par la plupart des gouvernements, institutions statistiques (comme l'INSEE), organisations internationales (OCDE) et certains acteurs privés voulant disposer de leurs propres prévisions quant à la conjoncture.Les penseurs de la Grèce antique, tels que Platon, Aristote et Xénophon, avancent des idées économiques. Celles-ci sont toutefois souvent liées à la gestion de la maisonnée, et l'économie désigne l'art de bien administrer sa maison. La microéconomie primitive naît ainsi avant la macroéconomie. Certains auteurs font toutefois remarquer que, chez Platon notamment, la place de l'économie de la cité est parfois pensée dans le cadre des relations géopolitiques,. Il y aurait donc des bribes de pensée macro chez les Grecs.Au XVIIe siècle, des premières réflexions sur la monnaie et l'économie agricole émergent. Les pensées développées sont assez primitives, et il faut attendre le XVIIIe siècle, avec son courant physiocrate, pour qu'un premier système macroéconomique soit ébauché. Cette représentation se trouve dans l'ouvrage de François Quesnay appelé le Tableau économique. Quesnay, médecin de la famille royale, cherchait à représenter l'économie sur les bases de la circulation du sang, comme un système cohérent et dynamique,. Certains auteurs rechignent toutefois à utiliser le terme de macroéconomie pour désigner la pensée de Quesnay, et préfèrent celui de circuit économique.Les Classiques pensent à la fois le niveau micro et le niveau macro. Adam Smith étudie les questions de répartition des richesses et de fonctionnement global de l'économie, tandis que David Ricardo marque l'histoire de la théorie du commerce international. Ils posent donc les jalons de la macroéconomie, conceptualisant le marché et sa concurrence, et mobilisant une loi générale (la loi des débouchés élaborée par Jean Baptiste de Say) qui explique l'économie comme un système. Ils ont toutefois également une pensée micro, car ils réfléchissent à la théorie de la valeur. La pensée des Classiques va jusqu'à créér des relations entre la micro et la macroéconomie. Adam Smith considère, ainsi, que la satisfaction des intérêts particuliers (niveau micro) permet la réalisation de l'intérêt général (niveau macro).Karl Marx propose à son tour une représentation schématique de l'économie industrielle de son époque. Il conceptualise le cycle monnaie-marchandise-monnaie et la baisse tendancielle du taux de profit. Il raisonne en classes sociales, comme ses prédécesseurs, quoiqu'il les redéfinisse. S'attachant à traiter d'agrégats afin de ne pas se perdre dans l'individualité, il adopte une approche macro à l'économie. Il est à ce titre cité dans les manuels de macroéconomie de référence comme un précurseur.Parallèlement, les fondateurs de l'école néoclassique ont utilisé la théorie marginaliste, pour agréger les comportements des agents économiques, c'est-à-dire les consommateurs et les producteurs. Cette microéconomie agrégée, approche souvent à la base de certaines théories macroéconomiques, est à la base de la théorie de l'équilibre général de Léon Walras, et complété par Kenneth Arrow et Gérard Debreu. Cette vision de l'économie ne peut toutefois pas se confondre avec la macroéconomie, étant donné qu'elle ne se base que sur des comportements individuels, et n'analyse pas l'économie dans son ensemble.La distinction systématique entre la microéconomie et la macroéconomie n'émerge vraiment qu'au cours des années 1930. Les travaux de John Maynard Keynes, dont notamment la Théorie générale de l'emploi, de l'intérêt et de la monnaie de 1936, indiquent un intérêt renouvelé pour l'étude agrégée des grandes variables, détachées des questionnements propres à l'économie du niveau micro. Il est question pour cette macroéconomie d'étudier les grandes variables économiques agrégées afin de fournir aux décideurs les clefs de compréhension du monde et des moyens d'action.Keynes écrit ainsi, en 1939, dans la préface à l'édition française de la Théorie générale : « J'ai appelé ma théorie une théorie générale. Par là, j'ai voulu signifier que j'étais principalement intéressé par le comportement du système économique dans son ensemble, avec des revenus globaux, des profits globaux, un produit global, un emploi global, un investissement global, une épargne globale, plutôt qu'avec des revenus, des profits, un produit, l'emploi, l'investissement d'industries, d'entreprise set d'individu particuliers. » Il continue : « je soutiens que l'on a commis d'importantes erreurs en étendant au système dans son ensemble ce qui a été établi correctement pour une de ses parties prise isolément. »L'après-guerre voit l'extension du keynésianisme et sa mise à jour. Repris aux États-Unis, la synthèse néoclassique, macroéconomique, forme le socle des politiques économiques occidentales des Trente Glorieuses. Le monde académique accepte la séparation nette entre microéconomie et macroéconomie. La microéconomie se spécialise alors sur les problèmes d'allocation des ressources par le moyen des prix relatifs (prix d'un produit, ou de plusieurs produits, exprimé par un ou plusieurs autres produits), alors que la macroéconomie étudie la production globale et le niveau des prix.Les échecs de la synthèse néoclassique à prévoir et à enrayer la stagflation (hausse des prix et baisse de la croissance) conduit, au XXe siècle, à l'émergence de nouvelles écoles de pensée qui rénovent les théories macroéconomiques dominantes. Le monétarisme, la nouvelle économie classique et la nouvelle économie keynésienne font partie de ces mouvements de pensée.Robert E. Lucas de la nouvelle économie classique critique la macroéconomie pour son manque de fondements microéconomiques. Cette critique dite de Lucas souligne que la synthèse néoclassique a formé un cadre macroéconomique qui a trop gommé la microéconomie, c'est-à-dire l'étude de la réaction des agents. En effet, l'école de la synthèse les considère généralement comme passifs et incapables d'anticiper les effets des politiques économiques.Parallèlement, les progrès réalisés par le monde académique permet la construction de modèles de plus en plus complexes et élaborés. Rendue possible par l'augmentation des capacités de calcul des ordinateurs ainsi que la généralisation des techniques d'optimisation dynamique, l'explosion des données économiques permettent à la macroéconométrie d'affiner ses capacités prédictives. La critique de Lucas mène aussi à la création de la microéconométrie.Au début du XXIe siècle, des économistes cherchent à dépasser la distinction entre microéconomie et macroéconomie. La plupart des modèles macroéconomiques actuels font l'hypothèse qu'ils ne constituent qu'une simplification de la réalité, dont ils étudient un aspect particulier, comme l'effet de l'innovation sur la croissance, ou des structures monétaires sur l'investissement. De ce fait, ils mélangent relations macroéconomiques et extensions au niveau macroéconomique de relations microéconomiques pour autant que ces extensions soient compatibles avec les faits stylisés qu'on cherche à analyser.La croissance désigne l'augmentation de la richesse produite dans un système économique. Ses causes sont l'un des sujets majeurs de la macroéconomie, qui a proposé différents modèles explicatifs. Le premier grand modèle fut le modèle de la croissance exogène. Il a été dépassé par les modèles de la croissance endogène. Au niveau de ces derniers, le prix Nobel d'économie, l'américain Paul Romer a apporté des contributions majeures.La consommation est l'une des variables majeures étudiées en macroéconomie. Elle est l'une des composantes de la croissance. Elle peut être issue des ménages, des entreprises ou de la puissance publique. La consommation peut être issue de l'extérieur, auquel cas il s'agira d'exportations. En cas d'un manque d'autosuffisance, la consommation peut s'adresser au reste du monde et correspond à des importations.La macroéconomie s'intéresse aux stocks et aux flux. Le stock est une quantité à un moment donnée. Le flux est exprimé en unité de temps (par exemple, sur un an). La dette est un stock constitué de l'ensemble des flux de déficits.La macroéconomie étudie les grands déséquilibres internationaux. Les déficits commerciaux, les déficits budgétaires, les situations de déséquilibres sur les différents marchés internationaux..Toutes les pages avec « macroéconomie » dans le titre(fr) Bernard Guerrien, Une brève histoire de la macroéconomie [lire en ligne (page consultée le 11/09/2015)]Paul Krugman et Robin Wells, Macroéconomie, De Boeck, coll. « Ouvertures économiques », 2009, 2e éd., 998 p.Gregory N. Mankiw, Macroéconomie, De Boeck, coll. « Ouvertures économiques », 2003, 3e éd., 655 p.Olivier Blanchard, Daniel Cohen, Macroéconomie, Pearson éducation, 2002,  (ISBN 2-84211-121-4)David Romer, Macroéconomie approfondie, McGraw Hill / EdiscienceMichel De Vroey, Pierre Malgrange, La théorie et la modélisation macroéconomiques, d’hier à aujourd’hui Document de travail PSE Lire en ligne(en) Gregory Mankiw,""The Macroeconomist as Scientist and Engineer"", NBER Working Paper 12349, juin 2006 Lire en ligneMichael Wickens, Analyse Macroéconomique Approfondie, De Boeck, 2010,  (ISBN 2-8041-6193-5)Gilbert Abraham-Frois, « La macroéconomie en l’an 2000 », Revue économique, vol. 52, no 3,? 2001 (lire en ligne, consulté le 28 juin 2011)Pascal Salin, Macroéconomie, PUF, coll. « Premier cycle », 1991, 400 p. (ISBN 978-2-13-043530-3, lire en ligne) Portail de l’économie"
économie;Le marché primaire est le marché financier où les agents économiques peuvent acheter et vendre des actifs financiers qui viennent d'être émis. Il s'agit d'un « marché du neuf », contrairement au marché secondaire, qui est un marché de l'occasion. Le marché primaire est le marché sur lequel a lieu l'émission d'actifs financiers (actions, des obligations, warrants, etc.). Par opposition au marché secondaire, le marché primaire est parfois appelé « marché du neuf ». Les actions d'une entreprise qui vient d'être introduite en bourse sont pour la première fois émises sur le marché primaire. Il en va de même en ce qui concerne l'augmentation de capital d'une société existante, ou encore d'une émission d'obligations souveraines qui permettent à celui-ci de se financer.C'est l'un des métiers de la banque d'investissement que d'être « arrangeur » de ces émissions, en organisant et en centralisant les souscriptions des épargnants et des organismes financiers.Le marché primaire a pour rôle d'assurer la rencontre entre l'offre et la demande de capitaux. Le marché primaire contribue à ce titre au financement de l'économie.Marché secondaireHistoire des bourses de valeurs Portail de la finance
économie;Le marché secondaire est le marché financier où les agents économiques peuvent acheter et vendre des actifs financiers déjà existants. Il s'agit d'un « marché de l'occasion », contrairement au marché primaire, qui est un marché de neuf. Les marchés secondaires sont des marchés où s'échangent des actifs financiers déjà émis et achetés une première fois. Il s'agit à ce titre d'un marché d'occasion, où un actif change de main. Les marchés secondaires comportent à la fois un segment organisé (la bourse), et un segment de gré à gré (marchés privés, over-the-counter). On peut acheter sur les marchés secondaires des instruments financiers comme des actions, des obligations, des options et des contrats à terme. Ces instruments ont déjà été émis et achetés une première fois sur le marché primaire. Toute revente se fait sur les marchés secondaires.Le cours de chaque instrument sur les marchés secondaires fluctue selon l'offre et la demande sur ce marché. La valeur des titres peut ainsi s'écarter de la valeur initiale du titre lors de l'émission.Le marché secondaire a plusieurs rôles. Son premier est un rôle de liquidité, car le marché permet de liquider aisément des actifs financiers. Le deuxième est un rôle de veille, car le marché secondaire agit comme une sorte de thermomètre de la santé financière.Marché primaireHistoire des bourses de valeurs Portail de la finance
économie;"La macroéconomie est une discipline de l'économie qui étudie le système économique au niveau agrégé à travers les relations entre les grands agrégats économiques que sont le revenu, l'investissement, la consommation. La macroéconomie constitue l'outil essentiel d'analyse des politiques économiques des États ou des organisations internationales. Il s'agit d'expliquer les mécanismes par lesquels sont produites les richesses à travers le cycle de la production, de la consommation, et de la répartition des revenus au niveau national ou international.La macroéconomie est une branche de la science économique. Elle se consacre à l'étude des grandes variables économiques nationales ou internationales, et aux relations entre ces variables. Elle repose sur une approche globale, quoiqu'elle puisse se fonder sur des comportements microéconomiques et micro-ondes.Parce qu'elle fonctionne par la comparaison d'agrégats, la macroéconomie est avant-tout une représentation hiérarchisée de l'économie, articulée entre ses agents, via des flux. Elle cherche à expliciter ces relations et à prédire leur évolution face à une modification de certaines variables. La macroéconomie permet par exemple d'estimer la réaction d'un système économique possédant telles caractéristiques face à un choc pétrolier, ou à une politique économique particulière.Contrairement à la microéconomie, qui favorise les raisonnements en équilibre partiel, la macroéconomie se place toujours dans une perspective d'équilibre général, ce qui l'amène à accorder plus d'attention au bouclage des modèles et à la dynamique de création et de maintien d'institutions essentielles, comme les marchés, la monnaie.La macroéconomie a évolué à travers le temps pour devenir plus précise et plus sûre. Ses origines se trouvent dans les premiers travaux économiques du XVIIIe siècle (voir Histoire de la pensée économique), mais elle prend véritablement forme grâce aux travaux de John Maynard Keynes. Sa théorie, le keynésianisme, se fonde sur ce qui fut appelée la macroéconomie keynésienne, qui est l'interprétation keynésienne de la macroéconomie. Elle crée des outils de base de la macroéconomie (le modèle IS/LM, la courbe de Phillips, etc.).La macroéconomie n'est aujourd'hui plus rattachée à une quelconque école de pensée. Elle a évolué vers la construction de modèles économiques complexes, incluant à la fois des relations supposées entre variables et des relations comptables servant à définir les agrégats. Très utilisés pour analyser et prévoir les résultats des politiques économiques, ces vastes modèles qualifiés, le plus souvent, d'économétriques (les plus frustes comportent une dizaine d'équations, les plus complexes dépassent les 1 500) sont à l'heure actuelle employés par la plupart des gouvernements, institutions statistiques (comme l'INSEE), organisations internationales (OCDE) et certains acteurs privés voulant disposer de leurs propres prévisions quant à la conjoncture.Les penseurs de la Grèce antique, tels que Platon, Aristote et Xénophon, avancent des idées économiques. Celles-ci sont toutefois souvent liées à la gestion de la maisonnée, et l'économie désigne l'art de bien administrer sa maison. La microéconomie primitive naît ainsi avant la macroéconomie. Certains auteurs font toutefois remarquer que, chez Platon notamment, la place de l'économie de la cité est parfois pensée dans le cadre des relations géopolitiques,. Il y aurait donc des bribes de pensée macro chez les Grecs.Au XVIIe siècle, des premières réflexions sur la monnaie et l'économie agricole émergent. Les pensées développées sont assez primitives, et il faut attendre le XVIIIe siècle, avec son courant physiocrate, pour qu'un premier système macroéconomique soit ébauché. Cette représentation se trouve dans l'ouvrage de François Quesnay appelé le Tableau économique. Quesnay, médecin de la famille royale, cherchait à représenter l'économie sur les bases de la circulation du sang, comme un système cohérent et dynamique,. Certains auteurs rechignent toutefois à utiliser le terme de macroéconomie pour désigner la pensée de Quesnay, et préfèrent celui de circuit économique.Les Classiques pensent à la fois le niveau micro et le niveau macro. Adam Smith étudie les questions de répartition des richesses et de fonctionnement global de l'économie, tandis que David Ricardo marque l'histoire de la théorie du commerce international. Ils posent donc les jalons de la macroéconomie, conceptualisant le marché et sa concurrence, et mobilisant une loi générale (la loi des débouchés élaborée par Jean Baptiste de Say) qui explique l'économie comme un système. Ils ont toutefois également une pensée micro, car ils réfléchissent à la théorie de la valeur. La pensée des Classiques va jusqu'à créér des relations entre la micro et la macroéconomie. Adam Smith considère, ainsi, que la satisfaction des intérêts particuliers (niveau micro) permet la réalisation de l'intérêt général (niveau macro).Karl Marx propose à son tour une représentation schématique de l'économie industrielle de son époque. Il conceptualise le cycle monnaie-marchandise-monnaie et la baisse tendancielle du taux de profit. Il raisonne en classes sociales, comme ses prédécesseurs, quoiqu'il les redéfinisse. S'attachant à traiter d'agrégats afin de ne pas se perdre dans l'individualité, il adopte une approche macro à l'économie. Il est à ce titre cité dans les manuels de macroéconomie de référence comme un précurseur.Parallèlement, les fondateurs de l'école néoclassique ont utilisé la théorie marginaliste, pour agréger les comportements des agents économiques, c'est-à-dire les consommateurs et les producteurs. Cette microéconomie agrégée, approche souvent à la base de certaines théories macroéconomiques, est à la base de la théorie de l'équilibre général de Léon Walras, et complété par Kenneth Arrow et Gérard Debreu. Cette vision de l'économie ne peut toutefois pas se confondre avec la macroéconomie, étant donné qu'elle ne se base que sur des comportements individuels, et n'analyse pas l'économie dans son ensemble.La distinction systématique entre la microéconomie et la macroéconomie n'émerge vraiment qu'au cours des années 1930. Les travaux de John Maynard Keynes, dont notamment la Théorie générale de l'emploi, de l'intérêt et de la monnaie de 1936, indiquent un intérêt renouvelé pour l'étude agrégée des grandes variables, détachées des questionnements propres à l'économie du niveau micro. Il est question pour cette macroéconomie d'étudier les grandes variables économiques agrégées afin de fournir aux décideurs les clefs de compréhension du monde et des moyens d'action.Keynes écrit ainsi, en 1939, dans la préface à l'édition française de la Théorie générale : « J'ai appelé ma théorie une théorie générale. Par là, j'ai voulu signifier que j'étais principalement intéressé par le comportement du système économique dans son ensemble, avec des revenus globaux, des profits globaux, un produit global, un emploi global, un investissement global, une épargne globale, plutôt qu'avec des revenus, des profits, un produit, l'emploi, l'investissement d'industries, d'entreprise set d'individu particuliers. » Il continue : « je soutiens que l'on a commis d'importantes erreurs en étendant au système dans son ensemble ce qui a été établi correctement pour une de ses parties prise isolément. »L'après-guerre voit l'extension du keynésianisme et sa mise à jour. Repris aux États-Unis, la synthèse néoclassique, macroéconomique, forme le socle des politiques économiques occidentales des Trente Glorieuses. Le monde académique accepte la séparation nette entre microéconomie et macroéconomie. La microéconomie se spécialise alors sur les problèmes d'allocation des ressources par le moyen des prix relatifs (prix d'un produit, ou de plusieurs produits, exprimé par un ou plusieurs autres produits), alors que la macroéconomie étudie la production globale et le niveau des prix.Les échecs de la synthèse néoclassique à prévoir et à enrayer la stagflation (hausse des prix et baisse de la croissance) conduit, au XXe siècle, à l'émergence de nouvelles écoles de pensée qui rénovent les théories macroéconomiques dominantes. Le monétarisme, la nouvelle économie classique et la nouvelle économie keynésienne font partie de ces mouvements de pensée.Robert E. Lucas de la nouvelle économie classique critique la macroéconomie pour son manque de fondements microéconomiques. Cette critique dite de Lucas souligne que la synthèse néoclassique a formé un cadre macroéconomique qui a trop gommé la microéconomie, c'est-à-dire l'étude de la réaction des agents. En effet, l'école de la synthèse les considère généralement comme passifs et incapables d'anticiper les effets des politiques économiques.Parallèlement, les progrès réalisés par le monde académique permet la construction de modèles de plus en plus complexes et élaborés. Rendue possible par l'augmentation des capacités de calcul des ordinateurs ainsi que la généralisation des techniques d'optimisation dynamique, l'explosion des données économiques permettent à la macroéconométrie d'affiner ses capacités prédictives. La critique de Lucas mène aussi à la création de la microéconométrie.Au début du XXIe siècle, des économistes cherchent à dépasser la distinction entre microéconomie et macroéconomie. La plupart des modèles macroéconomiques actuels font l'hypothèse qu'ils ne constituent qu'une simplification de la réalité, dont ils étudient un aspect particulier, comme l'effet de l'innovation sur la croissance, ou des structures monétaires sur l'investissement. De ce fait, ils mélangent relations macroéconomiques et extensions au niveau macroéconomique de relations microéconomiques pour autant que ces extensions soient compatibles avec les faits stylisés qu'on cherche à analyser.La croissance désigne l'augmentation de la richesse produite dans un système économique. Ses causes sont l'un des sujets majeurs de la macroéconomie, qui a proposé différents modèles explicatifs. Le premier grand modèle fut le modèle de la croissance exogène. Il a été dépassé par les modèles de la croissance endogène. Au niveau de ces derniers, le prix Nobel d'économie, l'américain Paul Romer a apporté des contributions majeures.La consommation est l'une des variables majeures étudiées en macroéconomie. Elle est l'une des composantes de la croissance. Elle peut être issue des ménages, des entreprises ou de la puissance publique. La consommation peut être issue de l'extérieur, auquel cas il s'agira d'exportations. En cas d'un manque d'autosuffisance, la consommation peut s'adresser au reste du monde et correspond à des importations.La macroéconomie s'intéresse aux stocks et aux flux. Le stock est une quantité à un moment donnée. Le flux est exprimé en unité de temps (par exemple, sur un an). La dette est un stock constitué de l'ensemble des flux de déficits.La macroéconomie étudie les grands déséquilibres internationaux. Les déficits commerciaux, les déficits budgétaires, les situations de déséquilibres sur les différents marchés internationaux..Toutes les pages avec « macroéconomie » dans le titre(fr) Bernard Guerrien, Une brève histoire de la macroéconomie [lire en ligne (page consultée le 11/09/2015)]Paul Krugman et Robin Wells, Macroéconomie, De Boeck, coll. « Ouvertures économiques », 2009, 2e éd., 998 p.Gregory N. Mankiw, Macroéconomie, De Boeck, coll. « Ouvertures économiques », 2003, 3e éd., 655 p.Olivier Blanchard, Daniel Cohen, Macroéconomie, Pearson éducation, 2002,  (ISBN 2-84211-121-4)David Romer, Macroéconomie approfondie, McGraw Hill / EdiscienceMichel De Vroey, Pierre Malgrange, La théorie et la modélisation macroéconomiques, d’hier à aujourd’hui Document de travail PSE Lire en ligne(en) Gregory Mankiw,""The Macroeconomist as Scientist and Engineer"", NBER Working Paper 12349, juin 2006 Lire en ligneMichael Wickens, Analyse Macroéconomique Approfondie, De Boeck, 2010,  (ISBN 2-8041-6193-5)Gilbert Abraham-Frois, « La macroéconomie en l’an 2000 », Revue économique, vol. 52, no 3,? 2001 (lire en ligne, consulté le 28 juin 2011)Pascal Salin, Macroéconomie, PUF, coll. « Premier cycle », 1991, 400 p. (ISBN 978-2-13-043530-3, lire en ligne) Portail de l’économie"
économie;"La monnaie est définie par Aristote par trois fonctions : unité de compte, réserve de valeur et intermédiaire des échanges. À la période contemporaine, cette définition ancienne persiste mais doit être amendée, entre autres par la suppression de toute référence à des matières précieuses (à partir du IVe siècle en Chine) avec la dématérialisation progressive des supports monétaires, et les aspects légaux de l'usage de la monnaie — et notamment les droits juridiques qui sont attachés au cours légal et au pouvoir libératoire —, qui sont plus apparents. Ces droits sont fixés par l'État et font de la monnaie une institution constitutionnelle et la référence à un territoire marchand sous la forme d'un marché national (lié par une unité monétaire, de compte commun).La monnaie est l'instrument de paiement en vigueur en un lieu et à une époque donnée :du fait de la loi : on parle de cours légal ;du fait des usages : les agents économiques l'acceptent en règlement d'un achat, d'une prestation ou d'une dette.La monnaie est censée remplir trois fonctions principales :intermédiaire dans les échanges : la capacité d'éteindre les dettes et les obligations, notamment fiscales, constitue le « pouvoir libératoire » de la monnaie ;réserve de valeur ;unité de compte pour le calcul économique ou la comptabilité.Une monnaie se caractérise par la confiance qu'ont ses utilisateurs dans la persistance de sa valeur et de sa capacité à servir de moyen d'échange. Elle a donc des dimensions sociales, politiques, psychologiques, juridiques et économiques. En période de troubles, de perte de confiance, une monnaie de nécessité peut apparaître.La monnaie a pris au cours de l'histoire les formes les plus diverses : bœuf, sel, nacre, ambre, métal, papier, coquillages, etc. Après une très longue période où l'or et l'argent (ainsi que divers métaux) en ont été les supports privilégiés, la monnaie est aujourd'hui principalement dématérialisée : les espèces, ou monnaie fiduciaire, ne constituent plus qu'une petite partie de la masse monétaire.Chaque monnaie est définie, sous le nom de devise, pour une zone monétaire. Elle y prend la forme principalement de crédits qui font les dépôts et accessoirement de billets de banque et de pièces de monnaie. Les devises s'échangent entre elles dans le cadre du système monétaire international.En raison de l'importance de la monnaie, les États cherchent très tôt à s'assurer le maximum de pouvoir monétaire. Ils définissent la devise officielle en usage sur leur territoire et font en sorte que cette devise soit symbole et marque de leur puissance. Ils s'arrogent progressivement le monopole de l'émission des billets et des pièces et exercent un contrôle sur la création monétaire des banques via la législation et la politique monétaire des banques centrales.L'origine et l'histoire de la monnaie sont largement développées dans les articles suivants :Voir également Proto-monnaie Création du terme La notion de paléomonnaie a été proposée par Jean-Michel Servet, qui a « inventé (le) terme » vers 1976-1977, avant d'en « justifi(er) l'emploi » dans son Essai sur les origines de la monnaie paru en 1979. Il désigne une monnaie primitive. Le mot «monnaie» indique que les paléomonnaies remplissent les fonctions qui sont dévolues à une monnaie au sens élargi. Le préfixe « paléo- » signifie qu'aux yeux de Servet ces monnaies ne sont pas des antécédents des monnaies actuelles, mais des formes monétaires simples, répondant aux besoins du milieu qui les produit. En cela, il rejoint les points de vue de Karl Polanyi ou de Bronislaw Malinowski, selon lequel la culture est un tout indivisible dans lequel prend place l'ensemble des institutions. Usages Les paléomonnaies ont pour fonction de satisfaire des obligations sociales ou rituelles. Elles servent à régler des naissances, des mariages et des deuils, à déclarer la guerre ou à faire la paix et à compenser des meurtres, des injures, des offenses et des dommages physiques ou moraux. Elles sont amenées à changer de mains selon les circonstances. Certaines obligations rituelles nécessitent la détention de paléomonnaies. Il est alors possible de les acheter ou de les emprunter. Les paléomonnaies consacrent des hiérarchies dans la société. Elles peuvent constituer des moyens de pouvoir. Les formes et les usages sont variés d'une société à l'autre, voire à l'intérieur d'une société. Formes Les formes sont très diverses et variables : coquillages (travaillés ou non), dents de chien ou de marsouin, plumes collées, pierres polies, etc. Dans tous les cas les paléomonnaies ont un caractère inutile, précieux et rare.Une forme particulière de ces proto-monnaies (monnaie de commodité) est la monnaie de pierre des Îles Yap (Océanie, États fédérés de Micronésie) Caractère monétaire Jean-Michel Servet estime que les paléomonnaies sont une genèse de la monnaie. Nombre de leurs caractéristiques les rapprochent d'une monnaie. Elles ont un caractère inutile, précieux et rare. Elles sont standardisées et codifiées. Leurs techniques de fabrication sont parfois très sophistiquées. Elles sont fondées sur la confiance.Les paléomonnaies indispensables à l'exécution d'un rite ou à la réalisation d'obligations sociales sont parfois prêtables. Elles peuvent alors être soumises à intérêt.Les trois fonctions des monnaies de nos jours, unité de compte, instrument de paiement et instrument de réserve, se retrouvent dans les paléomonnaies soit seules, soit réunies. Elles codifient et rythment des activités et des biens à la manière d'unités de compte. Étant standardisées, elles préfigurent des moyens de paiement. Vu leurs rituels de conservation et le jeu des dettes et des créances, elles préfigurent un instrument de réserve.Jean-Michel Servet relève dans les rites et mythes de nombreuses sociétés une correspondance entre excréments et paléomonnaies. Il rejoint en cela les interprétations psychanalytiques de l'argent développées notamment par Sandor Ferenczi.Voir dans les paléomonnaies une genèse de la monnaie relève d'une théorie évolutionniste de la monnaie. D'autres chercheurs, notamment Jacques Mélitz, contestent cette vision. Ils estiment que le fait monétaire est le résultat d'une diffusion.L'utilisation d'un type d'objet privilégié (comme des coquillages) servant de référence pour l'établissement des prix et utilisé comme moyen d'échange, et l'utilisation d'une unité de compte par les scribes des civilisations antiques pour établir une comptabilité précise de leur empire, est considéré depuis Adam Smith comme marquant le passage d'une économie de troc à une économie de marché. La plus ancienne monnaie connue — au sens actuel du terme — fut créée par le roi de Lydie, Gygès, qui en, 687 av. J.-C., substitua aux lingots d'or des morceaux d'électrum (alliage naturel d'or et d'argent provenant de filons locaux, notamment de la rivière Pactole) dotés des caractéristiques suivantes : poids invariable, formes identiques, et marqués d'un signe authentifiant leur étalonnage.Le développement de la monnaie métallique est parallèle au développement de vastes territoires politiquement unifiés et centralisateurs tels l'Empire romain et la Chine Qin. La monnaie permet en effet de gouverner à distance, de payer les soldats et l'administration : cette gouvernance passe nécessairement par le biais d'instruments de crédit ou « lettre de change » : un document authentifié permet de débloquer une masse de métal précieux en échange d'un service.Après la chute de l'Empire romain, l'usage de la monnaie connaît une régression dans l'Europe du Haut Moyen Âge avec les restrictions au commerce et la mise en place presque partout de systèmes féodaux laissant peu de place aux libertés économiques.Au Moyen Âge, toutes les unités monétaires locales sont définies partout en référence à leur poids d'or ou d'argent. En France, les seigneurs qui parfois créent des monnaies locales sont régulièrement rappelés à l'ordre par des ordonnances ou règlements royaux, dont par une ordonnance de 1315. Le monde musulman, s'inspirant du monnayage parthe (IIIe siècle), met en place un système monétaire trimétallique.Avec le développement du commerce international, la banque, au sens moderne, fait son apparition en Europe. Venise, républicaine et indépendante, devient la plateforme monétaire du monde. Son succès repose principalement sur l'arbitrage entre les cours respectifs de l'or et de l'argent entre Orient et Occident. Elle assèche l'argent existant en Europe provoquant de nombreuses difficultés monétaires et, par ricochet, favorisant les manipulations monétaires. En contrepartie les rois de France, par exemple, usent de tous les artifices pour fausser en leur faveur le rapport entre valeur nominale des monnaies et teneur en métal. L'histoire monétaire devient celle de la production relative de l'or et de l'argent et des conséquences de la variation des taux d'échange entre ces deux métaux. Ils varieront dans des proportions de 1 à 7 et 1 à 12 entre le XIVe siècle et la fin du XIXe siècle.En 1550, dans le Royaume et territoires assujettis, le privilège de battre monnaie est limité aux villes de Paris, Rouen, Troyes, Digeon, Lyon, Grenoble, Turin, Marseille, Montpellier, Toulouse, Bayonne, Bordeaux, La Rochelle, Limoges, Poitiers, Bourges, Tours, Angers et Rhennequi sont également limité dans le fonctionnement de leur cour et parlements sur le sujet de la monnaie.La Première Guerre mondiale marque la fin des monnaies indexées sur les métaux précieux : les états européens continentaux sont dans l'incapacité de rembourser leurs dettes en or. Après la Seconde Guerre mondiale, la plupart des monnaies sont indexées sur le dollar, qui seul reste théoriquement convertible en or. La guerre du Viêt Nam mettra fin à l'étalon-or. En 1976, avec les accords de Kingston, le cours des devises devient flottant. C'est l'explosion du système monétaire international qui se traduit par la fin des parités fixes en Asie une quinzaine d'années plus tard.Par le passé, les historiens de l'anthropologie économique considéraient que la monnaie avait quatre fonctions principales (moyen d'échange — notion la plus familière —, unité de compte, réserve de valeur et norme de paiement différé). Les manuels d'économie modernes ne distinguent plus que trois fonctions, celle de norme de paiement différé (impôts, amendes) étant englobée dans les autres.Il y a eu de nombreux débats historiques sur la distinction entre ces différentes fonctions, d'autant plus que la monnaie, actif généralement accepté comme moyen de paiement, est dominée par des actifs plus rentables (tels les Bons du Trésor) aussi le terme « capital financier » est plus général pour désigner les liquidités et la fusion de l'ensemble des fonctions de la monnaie.Selon une conception élargie de la monnaie (conception substantive de Karl Polanyi), il suffit qu'un objet réponde à une de ces fonctions pour qu'il puisse être qualifié d'« objet monétaire ». Intermédiaire des échanges En l'absence de monnaie, les échanges commerciaux et relations professionnelles ne peuvent se réaliser que sous forme de troc d'un bien ou d'un service contre un autre. Pour que deux agents A et B échangent des biens X et Y, il faut que celui qui possède X préfère Y et que celui qui possède Y préfère X. C'est ce qu'on appelle la condition de « double coïncidence des désirs ». Cette condition limite le nombre de situations où le troc est immédiatement possible pour ces échanges et relations.La monnaie permet de s'affranchir des limitations du troc en constituant une valeur échangeable contre biens et services dans la mesure où les autres acteurs de l'économie l'acceptent aussi. La monnaie a pour valeur la convention collective de l'utiliser pour tous les échanges qui nécessiteraient sinon du troc ou une autre comptabilité pour des échanges différés dans le temps.Un échange d'un bien contre un autre utilise alors la monnaie comme un intermédiaire qui dissocie deux opérations distinctes : d'abord la vente du bien possédé contre de la monnaie, et ensuite l'achat du bien désiré. La fonction de moyen de paiement, quelquefois présentée comme une quatrième fonction de la monnaie est de servir d'intermédiaire commun comme moyen d'échange immédiat. En facilitant les échanges par rapport au troc, la monnaie est un outil essentiel du commerce libre[réf. souhaitée]. Contrats La monnaie facilite aussi le paiement de rémunérations de travailleurs libres qui autrement ne peut se faire qu'au pair ou plus généralement par compensation. Ces dernières méthodes sont lourdes, potentiellement arbitraires et sujettes à contentieux.La monnaie facilite l'emploi salarié, la division du travail et l'établissement des contrats. Elle donne une expression commode aux obligations privées nées de toutes les sortes de contrat, ou publiques (amendes, taxes, impôts) dès lors que la puissance publique lui donne un pouvoir libératoire.C'est une institution fondamentale pour l'économie des sociétés modernes fondées sur la liberté du travail, des productions, de la consommation et de l'épargne.Par réserve de valeur ou d'épargne, on entend la capacité que possède un instrument financier ou réel de transférer du pouvoir d'achat dans le temps. Ainsi, un bien immobilier constitue une réserve de valeur puisqu'il peut être acheté aujourd'hui et revendu dans le futur en procurant un pouvoir d'achat à son détenteur. On appelle cela un actif réel par opposition à la notion d'actifs financiers ou de titres, dont les actions et les obligations font partie.La capacité de la monnaie est pratiquement garantie à court terme : il est rare qu'elle soit amputée fortement de sa valeur du jour au lendemain, même si cela s'est déjà produit. À plus long terme le pouvoir d'achat de l'unité monétaire est réduit par l'inflation. Pour échapper à ce phénomène, les épargnants cherchent à placer leur épargne plutôt qu'à la conserver sous forme de monnaie, sauf en cas de panique.La thésaurisation de la monnaie est le placement le plus liquide. La propension collective à conserver plus ou moins « liquide » son épargne conditionne tous les marchés financiers et est suivie avec attention par les autorités monétaires. Lorsque les agents économiques accroissent leurs encaisses, c'est qu'ils se détournent des placements et la conséquence la plus fréquente est une restriction du crédit. Les paniques financières se manifestent par des ruées vers les espèces (monnaie de banque centrale) qui déstabilisent gravement le système bancaire.La monnaie est une unité de compte, un moyen standardisé d'expression de la valeur des flux et des stocks. On parle de calcul économique quand cette évaluation est faite a priori et de comptabilité quand elle est faite a posteriori. Il existe des unités de compte qui ne sont pas de la monnaie. Monnaie fiduciaire (ou corporelle) Une monnaie fiduciaire (du latin fides, la confiance) est une monnaie (ou plus généralement un instrument financier) dont les supports sont dépourvus de valeur intrinsèque et qui ne peuvent être convertis en or. Ce n'est plus la valeur des métaux précieux qui servent de gage à la monnaie mais la confiance du public. Cette confiance peut porter sur l'émetteur et lorsque l'émetteur est une banque centrale publique, la confiance se porte sur la société tout entière. L'expression de monnaie fiduciaire a été utilisée pour caractériser les monnaies de billon d'alliage métallique qui n'avaient pas de valeur intrinsèque. Mais lorsque les unités monétaires ont perdu leur définition en or — soit leur convertibilité — (le franc français en 1936, le dollar américain en 1976), c'est toute la monnaie émise par une banque centrale qui est devenue fiduciaire. Aussi, c'est le corps - d'où l'expression de monnaie corporelle - qui caractérise les billets et les pièces et non la confiance puisque, de nos jours, toutes les monnaies reposent sur la confiance. Monnaie scripturale La monnaie scripturale, littéralement écrite, est constituée des dépôts bancaires sur les comptes courants dans les banques commerciales. Ces écritures longtemps tenues dans des registres sont maintenant gérées par informatique. Ils forment l'essentiel de la masse monétaire, très loin devant les billets et les pièces. Support électronique Avec le développement des outils informatiques on assiste à une numérisation de la monnaie. Alors que la carte de paiement a déplacé la banque sur le lieu de transaction, la monnaie électronique entraîne la suppression de l'organisme de contrôle lors de l'échange. Aussi le droit limite fortement l'usage de la monnaie électronique à cause des risques de fraude qu'elle pose. Quasi monnaies et mesures de la masse monétaire La masse monétaire est une mesure de la quantité de monnaie en circulation. À l'origine la masse monétaire correspondait aux réserves d'or disponibles dans le coffre de la banque centrale. Mais l'abandon de l'étalon or et le développement de la monnaie scripturale nécessitent une nouvelle mesure. De fait il existe plusieurs masses monétaires selon les types de compte qui sont comptabilisés. En effet si un compte courant créditeur représente une dette d'une banque vis-à-vis d'une personne (la banque est engagée par la loi à fournir au détenteur du compte la somme créditée en billets de banque), il existe d'autres types de compte bancaire comme le livret A et plus généralement d'autres types de dette. Ainsi on distingue les masses monétaires :M0 appelée aussi base monétaire ou monnaie centrale représente l'ensemble des engagements monétaires d'une banque centrale (pièces et billets en circulation, avoirs en monnaie scripturale comptabilisée par la banque centrale) ;M1 correspond aux billets, pièces et dépôts à vue ;M2 correspond à M1 plus les dépôts à terme inférieurs ou égaux à deux ans et les dépôts assortis d'un préavis de remboursement inférieur ou égal à trois mois (par exemple, pour la France, le livret jeune ou le CODEVI, les livrets A et bleu, le compte d'épargne logement, le livret d'épargne populaire...) ;M3 correspond à M2 plus les instruments négociables sur le marché monétaire émis par les institutions financières monétaires (IFM), et qui représentent des avoirs dont le degré de liquidité est élevé avec peu de risque de perte de capital en cas de liquidation (ex. : OPCVM monétaire, certificat de dépôt, créance inférieure ou égale à deux ans) ;M4 correspond à M3 plus les bons du Trésor, les billets de trésorerie et les bons à moyen terme émis par les sociétés non financières.La création monétaire est le processus par lequel la masse monétaire d'un pays ou d'une région (comme la zone euro) est augmentée. Dans le système des réserves fractionnaires (réserves obligatoires déposées auprès des banques centrales) la création monétaire résultait essentiellement de l'effet multiplicateur du crédit, i.e. de la création de dette. Les banques centrales créent de la monnaie en achetant des actifs financiers comme des bons du trésor ou en prêtant de l'argent aux banques commerciales en échange d'une reconnaissance de dette. Les banques commerciales peuvent pour leur part créer aussi de l'argent en prêtant à des particuliers ou à des entreprises. Les réserves obligatoires exigées par les banques centrales étant devenues symboliques ou nulles pour ne pas nuire à la liquidité bancaire, la quantité de monnaie qui peut être créée par les banques commerciales est désormais limitée essentiellement par des règles prudentielles de solvabilité et liquidité fixés dans des traités internationaux comme Bâle III.La création de monnaie permanente peut pallier, dans certains cas, l'incapacité de la monnaie d'endettement à atteindre le niveau souhaitable du PIB.La politique monétaire est l'action par laquelle l'autorité monétaire, en général la banque centrale, agit sur l'offre de monnaie dans le but de remplir ses objectifs. Ceux-ci, définis dans le mandat de la banque centrale, peuvent recouvrir la stabilité des prix, le plein emploi, ou encore des objectifs environnementaux. Dans les pays à dominance monétaire, la stabilité des prix est son objectif principal ou unique.On formalise souvent les objectifs de la banque centrale sous la forme d'un triangle dit keynésien : la croissance, le plein emploi, l'équilibre extérieur. Depuis le début de la crise économique de 2008, les Banques centrales ont de plus en plus recours à des politiques dites non conventionnelles dont l'assouplissement quantitatif (en anglais, quantitative easing).La politique monétaire se distingue de la politique budgétaire. Ces deux politiques interagissent et forment ensemble le policy-mix.Le marché monétaire désigne le marché informel où les institutions financières – Trésors nationaux, banques centrales, banques commerciales, gestionnaires de fonds, assureurs, etc. – et les grandes entreprises (marché des billets de trésorerie), placent leurs avoirs ou empruntent à court terme (moins d'un ou deux ans). De plus avec l'adoption des changes flottants, les devises sont devenues des commodités comme les autres, un bien qui s'achète et se vend. Le marché monétaire est un élément essentiel au fonctionnement des marchés financiers.La monnaie a eu une profonde influence sur l'évolution du droit et réciproquement. L'émission de monnaie de crédit est strictement encadrée par le droit bancaire et des institutions étatiques de contrôle.En l'absence de monnaie, la sanction publique ne peut prendre que des formes physiques : confiscation de bien ; travail forcé. Elle est relativement difficile à étager. La monnaie permet de simplifier le système des amendes et de proposer des sanctions nuancées qui peuvent pour les délits sans trop d'importance ne pas entraver la vie courante des contrevenants.Dans le domaine civil l'absence de monnaie impose la compensation, c'est-à-dire la recherche d'une indemnisation en nature systématique et souvent très difficile à mettre en œuvre de façon juste et simple. L'indemnisation pécuniaire a été un grand progrès.Les pouvoirs publics sont seuls capables de donner un pouvoir libératoire à une monnaie, c'est-à-dire une capacité d'éteindre toute dette y compris les dettes fiscales et les dettes pénales ou civiles, en tout lieu et à tout moment dans la zone où un moyen de paiement a cours légal. Toutes les formes monétaires n'ont pas nécessairement cours légal. Généralement n'en sont dotés que seuls les billets émis par une Banque centrale et les pièces de monnaie. Le chèque n'a généralement pas cours légal. Il peut être refusé par les commerçants.Pourtant, inversement, il n'est pas possible d'effectuer tous les paiements avec une forme monétaire ayant cours légal. Par exemple en France, alors que l'article R642-3 du Code pénal prévoit que « le fait de refuser de recevoir des pièces de monnaie ou des billets de banque ayant cours légal est puni de l'amende prévue pour les contraventions de deuxième classe », la Cour de Cassation s'appuie sur l'article L112-5 du Code monétaire et financier qui dispose qu'« en cas de paiement en billets et pièces, il appartient au débiteur de faire l'appoint ».Les théories économiques cherchent à établir des liens entre les grandeurs comme les prix, la croissance, le chômage, l'inflation, les taux d'intérêt, les salaires...La pensée économique sur la monnaie est multiple.La théorie quantitative de la monnaie résulte du constat que les prix sont influencés par la quantité de monnaie en circulation. Cette théorie a été développée par différents auteurs dans différents pays. Son précurseur est Martin d'Azpilcueta, illustre Dominicain de l'École de Salamanque, et nous pouvons citer aussi Nicolas Copernic au XVIe siècle. Jean Bodin est le premier à la formuler, David Ricardo développe ses travaux, et c'est Irving Fisher qui formule en 1911 l'équation de la théorie quantitative de la monnaie (MV=PT) en. Elle a été reformulée par les théories monétaristes au cours des années 1970, dans une version restrictive, pour attaquer les théories keynésiennes.Pour John Keynes la monnaie n'est pas neutre, mais au contraire joue un rôle actif dans le fonctionnement de l'économie. Dans son livre Tract on Monetary Reform de 1923, il souligne que l'inflation peut conduire à la révolution, qu'une réforme monétaire est nécessaire pour reconstruire l'Europe et qu'il vaut mieux dévaluer que recourir à la déflation. Récemment, certains modèles nouveaux Keynésiens ont montré que la monnaie a un rôle à court terme sur les dynamiques économiques en fonction du niveau d'aversion au risque des ménages.Le monétarisme est un courant de pensée économique pour lequel l'action de l'État en matière monétaire est inutile voire nuisible. La réflexion sur ce thème est ancienne (cf. les écrits de Jean Bodin, David Hume, ou plus récemment Irving Fisher). Mais le rénovateur de ce courant est sans conteste l'économiste Milton Friedman (chef de file de l'École de Chicago), qui a contribué à réhabiliter et à relancer la théorie quantitative de la monnaie contre le paradigme dominant de l'époque, le keynésianisme. Ainsi la politique monétaire est réapparue sur le devant de la scène pour figurer depuis quelques années parmi les instruments essentiels de la politique économique.Le Chartalisme est une théorie monétaire. Selon cette théorie, la monnaie est une émanation de l'état. L'état crée de la monnaie en payant les personnes à son service comme les soldats et en exigeant que les dettes fiscales de ses sujets soient soldées par une certaine somme de monnaie. Les sujets sur le territoire contrôlé par l'état sont obligés de travailler ou d'échanger avec les personnes qui possèdent de la monnaie afin de payer les taxes réclamées par l'état. La valeur de la monnaie découle selon cette théorie directement des taxes qu'elle permet de solder. La monnaie représente donc une fraction du pouvoir de l'état.La quantité de monnaie est conservée lors d'un échange économique (voir aussi Atomicité (économie)). La conservation de la monnaie lors des échanges économiques implique que la monnaie tend à se répartir entre les agents économiques suivant une distribution exponentielle indépendamment de la nature des échanges. En l'absence de dette, cette distribution ne dépend que de la quantité de monnaie moyenne par agent et en présence de dette (monnaie négative) elle dépend aussi du niveau de dette autorisée.La question est : quelles sont les règles à appliquer à l'émission des billets de banque ? La querelle se produit en Angleterre, d'abord en 1810 quand la banque d'Angleterre suspend la convertibilité en métal de ses billets, puis dans les années 1840 à la suite d'une crise bancaire qui a vu la faillite de plusieurs banques, puis encore, aux États-Unis, dans les années 1870 à propos des greenbacks (Demand Note et United States Note).Le currency principle dispose que les billets remplacent les monnaies métalliques 1 pour 1. Tout billet émis peut donc être converti sans aucune difficulté ce qui assiéra la confiance et permettra de bénéficier des avantages du billet sans les risques d'insolvabilité des banques que l'on constate.Le banking principle considère que l'émission des billets doit être ajustée au besoin de l'économie qui, si elle est contrainte par le faible accroissement des ressources en métal, ne sera pas optimale. Selon cette doctrine, le fait que le public a toujours la faculté d'exiger le remboursement en or des billets suffit à en garantir la valeur, pourvu toutefois que les actifs de la banque, non seulement en or, mais aussi sous n'importe quelle autre forme (doctrine des effets réels) restent suffisants.La loi de 1844, le Banking Act, tranche la querelle au profit du currency principle, du moins en théorie puisqu'en pratique à chaque crise des mesures d'exceptions seront adoptées.La démonétisation de l'or et de l'argent a rendu cette querelle très inactuelle, elle subsiste néanmoins sous la forme de la question de la garantie des dépôts et du niveau de réserve (en monnaie banque centrale) qu'on exige des banques.L'argent métal est démonétisé aux États-Unis en 1873, dans le cadre d'un mouvement international qui verra la fin du bimétallisme au profit de l'étalon-or. La question agite fortement la vie politique américaine au point qu'un « parti de l'argent » est constitué qui aura un rôle dans toutes les élections présidentielles et législatives de la fin du XIXe siècle appuyé par les états producteurs de ce métal.La querelle durera jusque dans les années trente où Roosevelt remonétise partiellement l'argent, provoquant une raréfaction en Asie qui mettra en difficulté le régime chinois de Tchang Kai Check et favorisera involontairement la révolution communiste.Milton Friedman donnera raison rétrospectivement aux partisans du bimétallisme en montrant que la raréfaction de monnaie due à la disparition de l'argent monétaire explique pour une partie importante la récession qui a suivi.Les questions monétaires ont toujours agité les États-Unis. Après l'épisode d'hyperinflation des billets du Congrès on ressent le besoin d'une émission monétaire un peu mieux contrôlée. Une banque des États-Unis est créée en 1791 par Alexander Hamilton, dont la charte, temporaire, dure 20 ans. Elle ouvrit huit succursales, servit de dépôt pour les fonds de l'État, assura les transferts d'un bout à l'autre des États-Unis et joua le rôle de payeur général des dépenses publiques. Elle émit des billets convertibles en or ou en argent. Ces billets ne perdirent pas de leur valeur et « connurent l'estime générale ».La Constitution américaine définit strictement la monnaie et donne au Congrès (Sénat et Chambre des représentants réunis) la responsabilité des questions monétaires. Une grande querelle politique s'installa lorsqu'il s'agit de renouveler ou non la franchise de la banque. Menée par Thomas Jefferson, l'opposition au renouvellement gagna. Une seconde Banque des États-Unis vit le jour peu de temps après. Cette fois là c'est le Président Andrew Jackson qui l'étouffa.L'idée d'une banque centrale s'effaça pour longtemps (80 ans).L'avis de Jefferson était sans nuance : « J'ai toujours été l'ennemi des banques : non de celles qui acceptent des dépôts mais bien de celles qui vous refilent leurs billets de papier, écartant ainsi les honnêtes espèces de la circulation. Mon zèle contre ces institutions était tel qu'à l'ouverture de la Banque des États-Unis je m'amusais comme un fou des contorsions de ces bateleurs de banquiers cherchant à arracher au public la matière de leur jongleries financières et de leurs gains stériles. »Les banques se développeront à un rythme très rapide, surtout dans la seconde partie du XIXe siècle. Par exemple la Wells Fargo ouvrit 3 500 succursales entre 1871 et 1900. Les Westerns rendent compte de cette frénésie bancaire en montrant que dans tout village qui se crée se monte aussitôt un relais de diligence, un saloon et… une banque. Il est vrai que les colons qui accédaient à un lopin de terre n'avaient pas de ressources. La banque les leur fournissait, avec la terre comme garantie et les résultats d'exploitation comme source de remboursement. Il fallut attendre la crise de 1907 qui vit de nombreuses faillites de banques pour que l'idée d'une banque centrale assurant la fonction de « prêteur de dernier ressort » prît corps à nouveau.Mais les préventions étaient telles qu'on lui donna un nom neutre (Système Fédéral de Réserve, dit familièrement FED) et on créa dans plusieurs régions (states) un établissement similaire avec de larges pouvoirs. Ce n'est que bien après le déclenchement de la crise de 1929 et la faillite de plus de 9000 banques que la FED obtint de Roosevelt, en 1935, tous les pouvoirs d'une véritable banque centrale (1929 : 659 faillites de banque, 1930 : 1352, 1931 : 2294 ; fin 1933, près de la moitié des banques avaient disparu car 4004 banques firent faillite cette année-là). Mais ce n'est pas à la FED que l'on doit l'arrêt des faillites bancaires mais à la Société Fédérale D'assurances des dépôts, « Federal Deposit Insurance Corporation » (FDIC), qui offrit une garantie d'État aux déposants. En 1934, 62 banques cessèrent leur paiement. La crise bancaire était terminée.Note : Cette situation se répéta en 2008 où après la crise de confiance suivant la chute des bourses et la faillite de Lehman Brothers, ce sont les États qui déclarèrent garantir les déposants, et non les banques centrales.Le projet, historiquement entièrement nouveau, de créer une zone monétaire unifiée plurinationale en Europe a été une source de tensions politiques extrêmement fortes. Celles-ci ont suscité de très vives dissensions au sein des partis de gouvernement dans tous les pays concernés.Les souverainistes expliquèrent que la monnaie était un attribut fondamental de la nation qui ne pouvait être transféré et que l'abandon de la souveraineté monétaire signifi"
économie;"La parité de pouvoir d'achat (PPA) (on parle de valeurs mesurées en parité de pouvoir d'achat) est une méthode utilisée en économie pour établir une comparaison, entre pays, du pouvoir d'achat des devises nationales, ce qu’une simple utilisation des taux de change ne permet pas de faire.Le pouvoir d'achat d’une quantité donnée d’argent dépend en effet du coût de la vie, c’est-à-dire du niveau général des prix. La PPA permet de mesurer combien une devise permet d’acheter de biens et services dans chacune des zones que l’on compare.Les économistes forment un « panier » normalisé de biens et de services, dont le contenu peut être sujet à discussion (à ce sujet, voir en:Discussion and clarification of PPP).La monnaie couramment utilisée comme référence est le dollar américain, pris à une année donnée.Dans un marché global et unifié, sans coûts de transport, les produits identiques ont tous le même prix au même instant et à tous les endroits de ce marché : c'est la loi de prix unique.Cette loi, de nature microéconomique, est théorique. Elle se définit produit par produit, manufacturé ou non (par exemple le cuivre, le café, le ciment, les pneus d'une dimension donnée, une canette de boisson, un hamburger, ce dernier parfois utilisé comme indice rudimentaire de PPA à lui tout seul — l'indice Big Mac). Le monde réel fournit des exemples d'autant plus proches de cette situation théorique que les produits considérés sont mieux standardisés et moins coûteux à transporter.Pour la plupart des produits au contraire, les hypothèses sur lesquelles cette loi repose ne sont pas vérifiées, car le monde est loin d'être un marché unique : les coûts de transport ne sont pas nuls, les réglementations diffèrent en fonction des pays, les droits de douane appliqués aux importations augmentent leurs prix de vente. Par ailleurs, les coûts de fabrication varient fortement en fonction des pays : certaines ressources naturelles sont plus ou moins abondantes, le climat varie, le coût de la main-d'œuvre varie fortement. Les prix sont donc différents d'un endroit à l'autre.Cependant on peut considérer que le consommateur d'un pays substitue à certains produits plus chers dans son pays certains autres moins chers[réf. souhaitée]. Il y a donc non pas un seul produit, mais un ensemble de produits nécessaires à la vie du consommateur moyen. C'est le « panier », qui reflète les habitudes de consommation : au Japon la quantité de protéines nécessaire à la vie est apportée par du soja et du poisson alors qu'en France elle est apportée par de la viande de volaille ou de bovidés.La loi de parité du pouvoir d'achat exprime un coût égal du panier dans tous les pays ayant un niveau de vie raisonnablement comparable. C'est une loi de nature macroéconomique.Les taux de change PPA sont utilisés avant tout dans les comparaisons internationales de niveau de vie. La comparaison internationale des PIB conduit à ne pas prendre en compte les différences de prix existant entre les pays. Les écarts entre les taux de change réels et les taux de change PPA peuvent être significatifs. Ainsi, lorsque le yen, la monnaie japonaise, est surévalué, comme en 1999, le PIB par habitant paraît beaucoup plus élevé que son équivalent américain, alors que mesuré en PPA, il est en réalité beaucoup plus bas.Cette méthode permet de s'affranchir de trois problèmes :les taux de change des devises peuvent connaître des variations subites et brutales sans qu'il y ait modification des conditions économiques. Une comparaison internationale des évolutions à court terme serait faussée par une utilisation des taux de change du marché ;les devises des pays pauvres sont systématiquement sous-évaluées sur le marché des changes du fait de leur moindre productivité (c’est l’effet Balassa-Samuelson) ;certains pays fixent administrativement le taux de change de leur devise. Cela a pour effet de fausser les statistiques et les comparaisons internationales. C'était en particulier le cas des pays d’Europe de l'Est avant 1989.L'utilisation des PPA permet de s'affranchir de ces trois effets.La PPA est parfois utilisée comme un indicateur de la sous-évaluation ou surévaluation d'une devise par rapport à une autre sur le marché des changes. L'exercice est hasardeux, compte tenu des incertitudes inhérentes à cet instrument de mesure.La PPA absolue définit un cours de change entre deux monnaies. Elle est déterminée en définissant un panier de consommation dans un pays A et en évaluant le prix d’un panier « semblable » dans un pays B par la formule :                    P        P                  A                      t                          =                                            P                              t                                                    P                              t                                            ?                                                    .              {\displaystyle PPA_{t}={\frac {P_{t}}{P_{t}^{*}}}.}  où PPAt est la PPA absolue entre les deux pays sur la période t, et Pt est le prix sur la période t du panier de référence dans le pays A. L'autre pays, B, est marqué par un astérisque.Pour prendre un exemple chiffré, fictif, si un panier de produits évalués à 1 000 $ aux États-Unis a un coût moyen de 900 € en France, alors le taux de change en PPA du dollar par rapport à l'euro sera de 0,90. Ce taux est calculé indépendamment du cours de l’euro en dollar sur les marchés des changes, qui peut fortement fluctuer.La PPA relative mesure la variation de la PPA entre deux périodes. Elle s'exprime ainsi :                                                        P              P                              A                                  t                                                                    P              P                              A                                  t                  ?                  1                                                                    =                                                            P                                  t                                                            /                                            P                                  t                  ?                  1                                                                                    P                                  t                                                  ?                                                            /                                            P                                  t                  ?                  1                                                  ?                                                                          {\displaystyle {\frac {PPA_{t}}{PPA_{t-1}}}={\frac {P_{t}/P_{t-1}}{P_{t}^{*}/P_{t-1}^{*}}}}  où PPAt est le taux de change et Pt est le prix à la période t. Le pays étranger est marqué par un astérisque.Une variation de la PPA relative permet de mettre en évidence un différentiel d’inflation entre deux régions du monde.Plusieurs arguments limitent la pertinence et l’usage des PPA :les PPA peuvent varier de façon très importante suivant le choix du panier de produits. En ce sens, il est soumis aux mêmes limitations que les indices des prix ;les habitudes de consommation et les choix sont parfois très variables entre pays. Les produits consommés par les populations en dépendent et construire deux paniers équivalents est un travail très subjectif ;les différences de qualité pour deux produits mis en équivalence sont difficiles à évaluer ;les prix peuvent beaucoup varier à l’intérieur d’un même pays. Le prix d’un verre de bière est beaucoup plus élevé dans un bar sur les Champs-Élysées que dans un village du Massif central ;les prix des produits importés dépendent du taux de change. Une modification du taux de change a donc une influence sur la PPA alors que celle-ci est construite pour définir une parité de change décorrélée du marché des changes.Des indicateurs comme l’indice Big Mac, construit initialement par The Economist, ou l’indice iPod, assez frustes, sont parfois utilisés à des fins pédagogiques.Liste des pays par PIB (PPA)Liste historique des régions et pays par PIB (PPA)Programme de comparaison internationale(en) Penn World Table: tableaux de référence sur la parité de pouvoir d'achat et les revenus nationaux convertis en prix internationaux pour plus de 169 pays sur la période 1950 jusqu'à récemment. Site de l'université de Groningue.(fr) Le marché ou la PPA : Quelle base de comparaison choisir ? - Étude du FMI, mars 2007 [PDF](fr) Parités de pouvoir d’achat : mesure et utilisations - Cahier statistique de l'OCDE sur les PPA, mars 2002 [PDF](fr) Prix et salaires 2015 - Est-ce que je gagne assez pour me permettre la vie que je veux ? [PDF] Portail de l’économie"
économie;"Le prix, exprimé en un montant de référence (en général monétaire), est la traduction de la compensation qu'un opérateur est disposé à remettre à un autre en contrepartie de la cession d'un bien ou un service. Le prix mesure la valeur vénale d'une transaction et en constitue l'un des éléments essentiels.Le mécanisme de formation des prix est un des concepts centraux de la microéconomie, spécialement dans le cadre de l'analyse de l'économie de marché, où les prix jouent un rôle primordial dans la recherche et la définition d'un prix dit « d'équilibre » (alors qu'ils jouent un rôle plus mineur dans une économie administrée).Les niveaux de prix possibles sont en nombre potentiellement infini, selon les acteurs économiques, selon leurs estimations de la valeur de la chose pour eux-mêmes et pour les autres (spéculation). Si une transaction se réalise effectivement, le prix traduit le compromis entre les estimations de l'acheteur et celles du vendeur (reflet de l'offre et la demande).Le mécanisme de détermination des prix peut être affecté par d'autres facteurs :éventuelles imperfections régnant sur le marché (monopole, oligopole, pénurie, marché noir, etc.),contraintes légales lorsqu'il en existe (les prix n'étant pas toujours libres : « prix imposés » ou « administrés »),considérations techniques, telles que la méthode de mise en marché (commerce national et international, commerce de gros, commerce de détail, enchères, etc.) ou les contraintes que cela implique (délais de transmission des offres, définitions des priorités entre offres, ...).Selon l'objet concerné, le périmètre et la méthode de détermination du prix varie. On rencontre ainsi différentes sortes de prix :le prix d'achat.le prix de vente, qui indique le prix auquel un commerçant déclare être disposé à céder la chose et qui ne doit pas être inférieur au coût de revient (interdiction légale de la vente à perte) ;le prix de revient, censé refléter l'ensemble des dépenses liées aux intrants et à la fabrication d'un produit ou d'un service ; Le prix de revient ou coût de revient est égal au coût de production majoré des frais de transport ;le prix d'acceptabilité ou prix psychologique, qui définit le prix qu'une grande partie de la clientèle trouve justifié pour l'acquisition d'un bien ou d'un service ;le prix de cession, qui indique le prix auquel est facturée une cession entre deux services d'une même entreprise ou entre deux filiales d'un même groupe. En matière de comptabilité des entreprises, les prix de cession concernent les biens immobiliers (qui ont une longue durée de vie à l'instar des constructions ou des terrains non bâtis) par opposition aux prix de vente qui concernent, eux, les produits courants c'est-à-dire ceux qui sont relatifs à l'activité normale de l'entreprise.Raymond Barre distingue plusieurs types de prix en fonction du degré de liberté du marché (prix libres, prix administrés), du stade d'élaboration du produit (prix du gros ou de détail) ou encore de la nature des produits vendus (prix des produits agricoles, industriels ou des services)L'importance du système de prix libres a été mise en avant et débattue en particulier dans les années 1920-1930.Une vive controverse sur la question du calcul économique oppose les économistes de l'école autrichienne d'économie, Ludwig von Mises puis, ultérieurement Friedrich Hayek, aux tenants du socialisme de marché, Oskar Lange au premier chef. Pour Ludwig von Mises, le système de prix libres est le seul moyen de coordination des actions des millions d'individus qui composent l'économie d'un pays. Friedrich Hayek relaie cette idée et insiste pour sa part sur le rôle des prix comme vecteur de transmission de l'information disponible aux individus.L'économiste Milton Friedman résume cela en écrivant que le système de prix libres remplit trois fonctions :transmission de l'information sur l'offre et la demande ;.incitation pour les producteurs à s'orienter vers les secteurs aux prix élevés et, partant, à permettre un retour à l'équilibre ;répartition des revenus.Dans une économie planifiée, les prix n'ont pas la même importance. L'appareil productif peut s'en passer : au lieu de chercher à maximiser la valeur ajoutée de sa production comme il le ferait dans une économie de marché, un producteur peut se voir attribuer un quota de matières premières et un objectif de production ; les prix sont fixés par les pouvoirs publics à un niveau considéré comme « souhaitable », mais ils ne sont pas directement connectés aux décisions d'allocations des matières premières ou d'objectif de production, qui sont fixés par ailleurs. Il peut en résulter une pénurie (file d'attente et marché noir) ou un rationnement, si le prix est inférieur à l'utilité pour les consommateurs, ou des excès de production dans le cas contraire.En outre, certaines situations (par exemple, la guerre) incitent les autorités à recourir au contrôle des prix (ou du moins du prix de certains produits jugés nécessaires), ou à influer sur l'offre (protectionnisme, subvention...) et la demande.En réalité, la liberté totale des prix est rarement constatée, même dans les économies réputées les plus libérales, notamment à cause de l'impact de la fiscalité, de lois anti-dumping, des subventions, des engagements pris dans le cadre de contrats pluri-annuels, etc.Sur un marché libre le prix reflète l'équilibre entre l'offre et la demande. Mais les auteurs classiques (Adam Smith, David Ricardo, John Stuart Mill, ...) et Karl Marx considèrent qu'il est soumis plus aux influences de l'offre (coût exprimé par une certaine quantité de travail) que de la demande. Pour Karl Marx l'équilibre tend à se fixer autour de la valeur du travail incorporé. Ricardo estime également que le ""prix réel"" correspond à la quantité de travail incorporé mais constate que le ""prix courant"" est fonction de l'offre et de la demande. Le prix courant aurait tendance à se rapprocher du prix naturel. Selon Adam Smith le prix se dissocie de la ""valeur réelle"" car il tient compte de la valeur de la monnaie qui, elle, est variable. À partir de la fin du dix-neuvième siècle, les auteurs marginalistes (Léon Walras, Stanley Jevons, ...) estiment que le prix ou ""la valeur d'échange"" ne dépend pas de l'offre mais de la demande et donc de l'utilité exprimée par le consommateur. Malgré les influences de la demande sur la détermination du prix du marché admises par les marginalistes, Alfred Marshall considère que, de toute façon, on ne peut pas se passer du concours des deux (i.e l'offre et la demande) pour la fixation du prix. André Orléan estime que la fixation d'un prix peut s'établir par mimétisme et non en fonction du travail incorporé (côté offre) ou de l'utilité (côté demande),. Pour Jacques Perrin, les institutions jouent ou doivent jouer un rôle dans la constitution des prix en prenant en compte l'utilité sociale. Le prix n'est pas donc déterminé par l'unique confrontation de l'offre et de la demande qui sont exprimées par des agents économiques ""rationnels"". Elles subissent d'autres influences (psychologiques, sociologiques et institutionnelles). Par ailleurs, l'offre et la demande sont exprimées dans le temps. Ce dernier peut avoir une influence capitale dans les décisions des producteurs et des acheteurs. Avant de produire, de vendre ou d'acheter, ils procèdent notamment à des études futures du marché pour exploiter les opportunités avantageuses et éviter les menaces sources de risques majeurs.Les libéraux, en faisant appel au concept du consommateur-roi de Paul A. Samuelson, considèrent que les consommateurs, par leur pouvoir d'augmenter ou de baisser librement la demande exprimée sur le marché des biens de consommation, déterminent les prix et donnent le signal aux entreprises d'augmenter ou de baisser l'offre conséquente. Par conséquent, toute chose étant égale par ailleurs, les entreprises vont augmenter ou diminuer les demandes portant sur les marchés du travail, des biens de production et des capitaux déterminant ainsi les taux de salaire, d'intérêt et les prix sur le marché des biens de production. Cependant, John K. Galbraith a montré, dans les années 1960, que le fonctionnement réel de l'économie contemporaine ne correspond pas à ce schéma théorique. Les entreprises, en agissant sur le marché des biens de consommation (études des besoins du consommateur, étude de la concurrence, promotion des ventes) conditionnement la demande du consommateur aussi bien sur ce marché que sur ceux des biens de production, du travail et des capitaux et le privent de toute initiative. De plus, la liberté du consommateur est contrariée par la dépenses budgétaires de l'État visant à accroître les investissements publics pour augmenter la croissance économique. Cette nouvelle stratégie des entreprises est appelée par John K. Galbraith la "" filière inversée "".L'évolution des prix n'est pas l'inflation (l'augmentation du niveau général des prix), qui ne mesure le prix que par de la monnaie, alors que l'évolution des prix en général dépend du fonctionnement de l'économie, qui modifie le prix relatif des biens (i.e le prix d'un bien exprimé par d'autres biens). Cependant la mesure du prix de la monnaie ne peut être fait qu'indirectement, par mesure du prix d'un panier représentatif de biens : si le prix de ce panier augmente, c'est que la valeur (relative) de la monnaie diminue, et inversement.Il existe différents indices de prix pour différentes classes de biens et pour différents usages :les prix à la consommation sont mesurés par l'Indice des prix à la consommation (IPC ou, en anglais, CPI) ;les prix à la production sont mesurés séparément, et correspondent aux Coûts de production ;l'indice du coût de la construction ou l'indice de référence des loyers mesurent l'évolution du prix du logement ;etc.Pour un bien, on parle de « prix nominal » lorsque l'on fait référence au prix exprimé dans une monnaie donnée. On parle de « prix réel » lorsque l'on extrait du prix nominal la part due à l'évolution de la valeur de la monnaie, c'est-à-dire l'inflation.Les prix définissent une distance dans l'espace des commodités préservant la valeur des échanges.Soit A et B, Alphonse et Brigitte, deux agents économiques qui possèdent chacun les vecteurs commodités a et b, par exemple                     a        =        (        1        ,        1        ,        3        ,        0        ,        10        )              {\displaystyle a=(1,1,3,0,10)}   et                     b        =        (        0        ,        1        ,        4        ,        1        ,        100        )              {\displaystyle b=(0,1,4,1,100)}   avec l'ordre (voiture,table,chaises,machine à laver,monnaie), et p le vecteur des prix, la distance comparant la richesse des deux agents est définie par                    d        (        A        ,        B        )        =        d        (        a        ,        b        )        =                  |                          ?                      i                                    p                      i                          (                  a                      i                          ?                  b                      i                          )                  |                      {\displaystyle d(A,B)=d(a,b)=|\sum _{i}p_{i}(a_{i}-b_{i})|}  Cette pseudo-distance définit une relation d'équivalence dans l'espace des commodités qui préserve les écarts de richesse lors d'un échange. Par exemple si d(A,B) est de 100 et A et B échangent une même valeur de commodités d(a',b')= 0, i.e. A donne a' à B et B donne b' à A, après l'échange d(A,B) est toujours égale à 100. Cette préservation de la valeur ne serait pas vérifiée si d était une autre distance.Contrôle des loyersJuste prixLoi du maximum généralMarginalismeOffre et demandeParadoxe de l'eau et du diamantPrix prédateursValeurDistinction entre l'acompte et les arrhes(en) Price Theory par Milton Friedman(en) Theory of Price par George StiglerAndré Orléan, L'empire de la valeur, Seuil, 2011(en) Le système de prix libres, Henry Hazlitt(en) Four Thousand Years of Price Control, Ludwig von Mises Institute Portail de l’économie   Portail du management   Portail du commerce"
économie;"La production est l'action d'un sujet qui transforme une matière première  pour faire exister un nouvel objet. On rencontre ce phénomène de production dans la société, mais aussi bien dans la nature. C'est pourquoi on peut l'étudier soit sous l'angle économique et sociologique, soit sous l'angle biologique.Le terme « production » dérive du latin classique qui signifie « prolonger, mettre en avant ». Dans l'Antiquité, il désigne aussi bien les créations de la nature (l'arbre producteur de fruits) que celles de l'homme (l'artisan producteur d'objets utiles). Ce n'est qu'au début de l'ère industrielle qu'il entre dans le discours économique.Selon John Stuart Mill, « l'économie décrit les lois des phénomènes de société qui se produisent du fait des opérations conjointes de l'humanité pour la production de richesses ». L'économie est donc la discipline scientifique qui étudie la production comme élément fondamental, mais aussi l'échange, la distribution et la consommation des biens et des services. C'est ainsi qu'on étudie la production selon les méthodes, les lieux et les marchés. On compare la production d'un même produit à partir de modèles différents d'organisation. On calcule le volume de production par pays et par époques. On sépare l'analyse par secteurs économiques. On distingue la production marchande de la production non marchande. La production marchande est celle qui est réalisée et vendue essentiellement par les entreprises sur le marché des biens de consommation achetés par les ménages ou sur celui des biens de production achetés par les entreprises. La période de référence est généralement l'année. Elle est différente de la production annuelle. Grâce à la variation des stocks (stockage lorsque la production annuelle n'est pas totalement vendue ou déstockage dans le cas où celle-ci est insuffisante), la production permet de répondre au besoin annuel du marché national. Cette production est celle réalisée sur le territoire national (par des entreprises nationales ou étrangères) et ne tient donc pas compte de la production réalisée par des entreprises nationales dans le reste du monde. Par contre, la production est dite non marchande lorsque le prix payé par l'utilisateur est inférieur à la moitié de son coût de production. La production non marchande est réalisée essentiellement par l'État et accessoirement par les administrations privées (syndicats et partis politiques, par exemple) et les ménages. Les services concernés sont, essentiellement, de défense nationale, de sécurité, de justice, religieux et de spectacle public. La production non marchande sous forme d'autoconsommation des ménages n'est pas prise en compte par la comptabilité nationale car elle n'est pas justifiée par des documents (factures ou bulletins de paie, par exemple) justifiant son existence. Par contre, les travaux domestiques (services de jardinage ou d'éducation des enfants) sont comptabilisés lorsque le paiement des domestiques est prouvé par des pièces justificatives et non effectué uniquement gratuitement ou par remise d'une somme d'argent de la main à la main.La première approche économique de la production fut celle des physiocrates au XVIIIe siècle, qui considéraient que seule l'agriculture était vraiment productrice puisque le végétal apporte plus de graines qu'il n'en consomme, les autres activités ne faisant que transformer les produits de la terre. Au siècle suivant, David Ricardo va mettre l'accent sur la théorie de la valeur fondée sur le travail, approfondissant la distinction entre valeur d'usage et valeur d'échange. Henry Charles Carey est un célèbre économiste américain qui s'est opposé à Ricardo et au libre-échange en faisant l'éloge du capitalisme protectionniste et interventionniste américain,.Aujourd'hui, la production est l'activité socialement organisée exercée par une unité institutionnelle qui combine des facteurs de production (facteur travail et facteur capital) afin de transformer les consommations intermédiaires en biens ou en services s'échangeant sur le marché.Depuis les travaux de Colin Clark, on regroupe les activités économiques de production  selon trois grands secteurs :le secteur primaire : l'ensemble des activités qui exploitent les ressources naturelles : agriculture, mines, pêche...le secteur secondaire : toutes les activités de transformation d'une matière première : industries manufacturières, construction...le secteur tertiaire : principalement marchand : commerce, transports, hébergement-restauration... ; ou non-marchand : administration publique, enseignement...Selon une enquête de 2016, en France, le secteur primaire représente 2,8 % des 26 millions de personnes possédant un emploi (au sens du Bureau international du travail) ; le secteur secondaire 20,6 % et le secteur tertiaire 75,7 %. La France est le pays européen où le poids du tertiaire est le plus élevé.Selon l'INSEE, l'industrie regroupe « les activités économiques qui combinent des facteurs de production (installation, approvisionnement, travail, savoir) pour produire des biens matériels destinés au marché. » En France, l'industrie représente 12,4 % du PIB (20,3 % en Allemagne, 8,7 % au Royaume-Uni). La part de l'industrie manufacturière dans l'économie française a diminué de moitié depuis 1970 (5,7 millions de salariés contre 2,7 millions aujourd'hui).On distingue la production marchande de la production tout court.La production marchande peut se subdiviser en deux catégories :la production marchande simple où le producteur vend son produit sur le marché ou rend un service marchand à titre individuel ;la production marchande capitaliste où le produit ou le service créé par des salariés est propriété du capitaliste. Il est ensuite vendu en tant que marchandise dans le but de réaliser un bénéfice.La production non-marchande se définit comme la production de biens ou services proposés gratuitement ou à un prix inférieur au coût de production, par des organisations publiques, ou des associations.La production réelle d'une entreprise ne correspond pas normalement à sa production vendue. Celle-ci comprend en effet, en plus de la production propre de l'entreprise, celle issue d'autres entreprises, qui correspond aux matières premières et aux autres produits achetés (appelés consommations intermédiaires) pour fabriquer le produit vendu. La production réelle de l'entreprise, appelée « valeur ajoutée », est donc sa production vendue, de laquelle il faut retrancher les consommations intermédiaires.Lorsque la production n'est pas vendue sur le marché (l'essentiel de la production des administrations), sa valeur correspond, par définition, à son coût de production. Comme dans le cas de la production marchande, la valeur ajoutée des administrations est obtenue après avoir retranché les consommations intermédiaires de la production.Différentes organisations permettent de produire un bien ou un service. Certaines sont des espaces où sont concentrés les moyens de production et les ressources humaines pour produire à grande échelle, en grande quantité et d'une manière répétitive avec une division des tâches poussée. D'autres sont des structures plus éclatées et plus mobiles comme l'entreprise en réseau, (l'entreprise étendue) mise en place dans le cadre de l'économie post-industrielle.Trois grands modes d’organisation de la production peuvent être observés : organisation de type « série unitaire »,  les industries process, la production manufacturière.La sociologie économique considère que la production est une activité de création, de rencontre, d'échange et de partage de nombreux éléments tels que le temps, l'espace, les biens, les idées et les émotions.Les économistes ont modélisé la production en identifiant les éléments qui contribuent à sa réalisation, à savoir les facteurs de production. L'un des facteurs de production est constitué par le travail, ce qui représente la dimension sociale de la production du point de vue des théories économiques.Depuis les années 1970 environ, où sont apparus et se sont développés les mouvements écologistes, on se rend compte que la production, surtout industrielle, est grosse consommatrice de ressources naturelles, ce qui pose le problème de la rareté ou de l'épuisement de ces ressources, et qu'elle peut engendrer d'importantes pollutions. C'est pourquoi est apparue la notion de développement durable, qui combine deux aspects : ne pas abuser des ressources naturelles ; régler la production pour qu'elle ne détruise ni ne pollue l'environnement.Du point de vue biologique, tous les êtres vivants, végétaux comme animaux, sont des producteurs : ils produisent de la matière vivante en prélevant des éléments dans leur milieu de vie. L'animal comme le végétal produit sa propre matière à partir des aliments qu'il consomme. On distingue deux types de producteurs :les producteurs primaires : ce sont les végétaux verts qui contiennent de la chlorophylle grâce à laquelle en présence de lumière et uniquement à partir de matières minérales, ils fabriquent de la matière organique carbonée ;les producteurs secondaires : ce sont tous les autres êtres vivants qui fabriquent leurs substances organiques à partir de la matière d'un autre être vivant végétal ou animal.L'histoire de la production est marquée par deux grandes ruptures. La première est la Révolution  néolithique caractérisée par la transition de tribus de chasseurs-cueilleurs vers des communautés  d'agriculteurs. La première émergence eut lieu au Proche-Orient, il y a 5000 ans environ, où les hommes passèrent graduellement de la cueillette de céréales sauvages, à la production de plantes et d'animaux domestiqués. Les hommes ne se contentent plus de prendre ce que la nature leur offre, ils modifient radicalement leur environnement par des techniques agricoles nouvelles pour obtenir d'importants surplus de production. Une société sédentaire remplace progressivement les groupes nomades.La seconde rupture majeure, à partir du XVIIe siècle, est la Révolution industrielle qui transforme une société à dominante agraire et artisanale en une société commerciale et industrielle. Le caractère dominant de cette mutation est le passage de l'outil (prolongement de la force musculaire de l'ouvrier) à la Machine (dispositif autonome mû par une énergie naturelle), ce qui permet la mise en place de la production en série, c'est-à-dire d'une production de masse, production d'objets tous identiques à  grande échelle.Dans le cadre du capitalisme, la production est généralement conçue comme l'activité destinée à satisfaire non plus les besoins du producteur (autoconsommation), mais à être vendue sur le marché. Cette dernière est appelée « production marchande ». De plus, la vente n'est pas effectuée pour satisfaire les besoins jugés nécessaires ou urgents. Ceux-ci doivent être armés d'un pouvoir d'achat ; autrement dit, la production est destinée aux consommateurs qui sont capables de payer.Dans le second Discours, Jean-Jacques Rousseau cherche à cerner l'origine de la civilisation, qui est aussi selon lui l'origine du malheur de l'homme. Il affirme : « La métallurgie et l'agriculture furent les deux arts dont l'invention produisit cette grande révolution ». Au XIXe siècle les archéologues et les historiens ont parlé de ""révolution néolithique"" pour caractériser ""la période de la préhistoire marquée par l'émergence des premières sociétés agricoles sédentaires (...) qui ont éliminé, en quelques millénaires les sociétés de chasseurs-cueilleurs"", et qui ont installé ""une économie de la production"".C'est aussi au XIXe siècle que Karl Marx a élaboré une philosophie qui donne une grande importance à la production :d'une part, il en fait la base de la compréhension de l'homme : les hommes ""commencent à se distinguer des animaux dès qu'ils commencent à produire leurs moyens d'existence, pas en avant qui est la conséquence même de leur organisation corporelle"". La production n'est donc pas seulement une action économique; elle a un sens plus profond car elle est ""la façon dont les individus manifestent leur vie"". ""Ce qu'ils sont coïncide donc avec leur production"" (ibid.). À la question philosophique : ""qu'est-ce que l'homme ?"", Marx répond donc : ""l'homme c'est le monde de l'homme, l'État, la société"". Il faut donc dire que l'homme se produit lui-même dans l'histoire : ""par son activité historique, l'homme se donne une valeur humaine, il produit ses propriétés d'homme"", il se met en valeur. Marx écrivait : « Tout ce qu'on appelle l'histoire universelle n'est rien d'autre que l'engendrement de l'homme par le travail humain. »d'autre part, l'étude de la production fournit la base scientifique qui permet de comprendre la structure et l'évolution des sociétés humaines. C'est la théorie du matérialisme historique selon laquelle les rapports de production (relations entre les classes sociales) sont liés  aux forces productives (techniques, outillage et machines): ""les rapports sociaux sont intimement liés aux forces de production. En acquérant de nouvelles forces productives, les hommes changent leur mode de production et en changeant leur mode de production,ils changent la manière de gagner leur vie, ils changent tous leurs rapports sociaux"". Dans un fameux raccourci, il écrit : « Prenez le moulin à bras et vous aurez la société féodale avec le suzerain; prenez le moulin à vapeur et vous aurez la société avec le capitaliste industriel». Les rapports de production  sont d'abord en accord avec l'état de développement des forces productives, mais l'évolution de ces dernières finit par créer le besoin de nouveaux rapports de production, ""alors commence une ère de révolution sociale"" qui se conclut par l'apparition d'un nouveau mode de production et donc d'un nouveau type de société. L'histoire de l'humanité se définit par celle des modes de production. Il distingue les suivants : asiatique, antique, féodal et capitaliste auxquels devrait succéder le mode de production communiste débarrassé de la lutte entre les classes sociales qui a caractérisé les précédents.D'une manière plus générale, Michel Henry crédite Marx d'avoir pensé ""l'activité productive des hommes"" comme une praxis: ""C'est dans la pratique qu'il faut que l'homme prouve la vérité"". Selon Adolfo Sanchez-Vasquez, le concept de praxis signifie : ""activité orientée vers la transformation d'un objet (nature ou société) en tant que fin tracée par la subjectivité consciente et agissante des hommes et, par conséquent, activité objective et subjective à la fois"", et en ce sens, il s'oppose à toutes les philosophies précédentes, car comme le dit la XIe thèse des thèses sur Feuerbach : « Les philosophes n'ont fait qu'interpréter le monde de diverses manières, il s'agit maintenant de le transformer ».Biens et services marchandsCapital productifConsommationÉconomie post-industrielleEntrepriseEntreprise étendueFacteur de productionMatérialisme historiqueMode de productionProduction audiovisuelleProductivismeProductivité Portail de l’économie   Portail des entreprises   Portail de la philosophie   Portail de la sociologie   Portail de la production industrielle"
économie;En économie, le produit national brut (PNB) correspond à la production annuelle de richesses (valeur des biens et services créés, moins valeur des biens et services détruits ou transformés durant le processus de production) créées par un pays, que cette production se déroule sur le sol national ou à l'étranger.En comptabilité nationale française, le PNB n'est plus mesuré depuis 1993 : le choix a été fait de mesurer le revenu national brut, qui en est très proche. Le produit intérieur brut (PIB) est également beaucoup plus utilisé.Le PNB est la valeur totale de la production finale de richesses (valeur des biens et services créés - valeur des biens et services détruits ou transformés durant le processus de production) des acteurs économiques d'un pays donné au cours d'une année donnée. À la différence du PIB, il inclut les produits nets provenant de l'étranger, c'est-à-dire le revenu sur les investissements nets réalisés à l'étranger (cet élément étant négatif si les revenus des investissements de l'étranger sur le territoire national sont supérieurs aux revenus des investissements du pays à l'étranger).Le terme « national », dans « produit national brut », reflète ainsi la prise en compte de la valeur ajoutée produite par les résidents du pays en question (principe de nationalité) mais il n'est pas intérieur parce qu'une partie de cette valeur ajoutée est produite à l'étranger (le PIB est basé sur le principe de territorialité). Le PNB, de même que le PIB, inclut la TVA du pays, ce dont la légitimité est contestée.PNB = PIB + (revenus des facteurs du travail et du capital en provenance de l'extérieur - revenus des facteurs du capital et du travail versés à l'extérieur)PNB = PIB + revenus nets des facteurs versés au reste du mondeUne des questions pièges posées par des professeurs d'économie à leurs étudiants est la suivante : « Si l'on transforme un parking gratuit en parking payant, l'effet est-il d'augmenter, de diminuer ou de laisser inchangé le PNB du pays ? »La première réponse à laquelle on pense est que le PNB augmente en raison de l'apparition d'une nouvelle valorisation d'échanges. C'est oublier que les budgets ne sont pas élastiques et que ce qui est payé sur le parking par ses utilisateurs ne le sera pas par eux-mêmes ailleurs… mais le sera peut-être par les bénéficiaires de ces nouveaux revenus. Après analyse, la réponse est que le problème posé de cette façon ne permet pas de savoir si le PNB va augmenter, diminuer, ou éventuellement rester stationnaire.Sur le plan comptable, les échanges valorisés ne sont plus les mêmes. De nouveaux échanges apparaissent (le prix payé par les utilisateurs du parking), mais d'autres échanges s'en trouvent diminués et d'autres encore augmentés.Sur le plan économique, si on fait l'hypothèse d'un comportement similaire des bénéficiaires de ces nouveaux revenus, alors le PNB aura augmenté à hauteur de cette nouvelle valorisation. Si leur comportement est radicalement différent, alors l'analyse devient plus complexe : dans certains cas, le PNB étudié diminue : délocalisation du PNB local vers un autre PNB (rémunération de capitaux étrangers), investissements moins productifs, ou encore dépenses dans des cycles d'échanges plus lents.Les indices d'évolution des prix, (sur lesquels reposent par exemple les taux d'inflation) sont censés tenir compte des augmentations de prix, mais ce n'est pas toujours possible. Ainsi, le parking nouvellement payant fait probablement nouvellement l'objet d'une surveillance, d'une organisation, d'un nettoyage, d'une publicité, d'une signalisation, toutes choses qui contribuent à sa valeur et ne peuvent être distinguées de façon certaine d'une vulgaire augmentation de prix.Revenu national brutRevenu par têteProduit intérieur brut (PIB)Indicateur économique Portail de l’économie
économie;Le revenu national (RN) au prix du marché représente l’ensemble des revenus primaires reçus par les différents secteurs institutionnels du pays.Le revenu national = Produit intérieur brut + (Revenu Net du travail, de propriété, des entreprises reçues du reste du monde) - la consommation de capital fixe - les impôts (net de subvention versé aux institutions de l'Union européenne).Le revenu national est le revenu perçu par les agents économiques nationaux du fait de leur participation à l'activité de production.Produit intérieur brutProduit national brut Portail de l’économie
économie;"En économie, le secteur primaire est le premier secteur défini dans la loi des trois secteurs. Il est parmi les trois secteurs économiques définis dans la comptabilité nationale et regroupe les activités liées à l’exploitation de ressources naturelles : agriculture, sylviculture, pêche et activités minières. Le secteur primaire rassemble l'ensemble des activités qui produisent des matières premières non transformées.Le secteur primaire comprend l'agriculture, la pêche, l'exploitation forestière et l'exploitation minière. On désigne parfois les trois dernières par le terme « autres industries primaires ». Les industries primaires sont liées à l'extraction des ressources de la terre.Selon Fortune, le secteur de l’extraction représenterait 27 % de l'économie mondiale comprenant notamment les activités relatives à l’énergie ou les minières. Les vingt plus gros négociants du secteur ont engrangé $191 milliards de profit entre 2003 et 2012.C'est un processus par lequel les gens aménagent leurs écosystèmes pour satisfaire les besoins de leurs sociétés. Elle désigne l’ensemble des savoir-faire et activités ayant pour objet la culture des terres, et, plus généralement, l’ensemble des travaux sur le milieu naturel (pas seulement terrestre) permettant de cultiver et prélever des êtres vivants (végétaux, animaux, voire champignons ou microbes) utiles à l’être humain. En France En 1700, il fallait environ trois heures pour produire un kilogramme de blé, d'où la malnutrition et les famines. À cette période, 80 % de la population active travaillait dans l'agriculture; en 1880, il fallait encore un peu plus d'une heure ; en 1950, 30 minutes. Aujourd'hui, environ une minute. Cela explique l'évolution de la part de l'agriculture : en 1995, l'agriculture représentait en France 6 % de part de la population active ayant un emploi, contre 40 % en 1913[réf. souhaitée]. En 2008, l'agriculture en France pesait 3,5 % du PIB (2008), soit 66,8 milliards d'euros. En 2012, elle ne serait plus que de 2 % du PIB français. En Belgique En 1846, les cultivateurs représentent encore 52 % de la population économiquement active :en 1880, 22 % ;en 1913, 16 % ;actuellement 2 %.En 1846, intervient encore pour plus de 50 % dans le PNB :en 1880, 29 % ;en 1913, 15 % ;actuellement, 0,7 %. En Europe L'emploi dans le secteur agricole est en forte régression pour l'amont de la filière (agriculture) depuis plus d'un siècle, et dans l'UE27 il a encore diminué au XXIe siècle sous l'effet de l'industrialisation et de l'augmentation de la productivité ; selon Eurostat. Ceci correspond à la perte de 3,7 millions d’emplois à temps plein en 10 ans. L'emploi a ainsi baissé de 17 % dans l’UE152 et de 31 % dans les 12 États-membres (NEM122) ayant rejoint l’UE en 2004 et en 2007. En 2009, le secteur de l’agriculture employait dans l’UE27 l’équivalent de 11,2 millions de personnes travaillant à temps plein, dont 5,4 millions dans l’UE15 et 5,8 millions dans les NEM12. Dans le même temps (2000 ? 2009), le revenu réel moyen par actif a augmenté de 5 % (il a même doublé en Lettonie, Estonie et Pologne) de 2000 à 2009. Dans le monde En 2011, la production agricole mondiale était estimée à 4 949 milliards, soit 6,2 % de l'économie mondiale.Dans le secteur primaire, la section des mines est définie comme l'exploitation de différents roches ou minéraux.Secteur économiqueSecteur secondaireSecteur tertiaireSecteur quaternaire Portail de l’économie"
économie;"En économie, le secteur secondaire ou secteur industriel est le second secteur défini dans la loi des trois secteurs. Il est parmi les trois secteurs économiques définis dans la comptabilité nationale et regroupe les activités liées à la transformation des matières premières issues du secteur primaire (industrie manufacturière, construction)Ce secteur, même s’il représente une part relativement modeste du PIB des pays développés (par exemple 13,2 % aux États-Unis en 2006, 20,6 % en France en 2006 et 26,3 % en Suisse en 2005), est considéré comme stratégique ; il fournit des emplois d’ingénieur et fournit du travail de recherche et développement à des entreprises du secteur tertiaire.Selon la CIA, le secteur industriel représentait 30,7 % de l'économie mondiale en 2012. Mais, selon Fortune, le secteur industriel représenterait 13,2 % de l'économie mondiale en 2012 si l'on intègre les activités d'extraction au secteur primaire.Secteur économiqueIndustrieSociété industrielleSecteur secondaire en FranceLe secteur tertiaireSecteur quaternaire Portail de la production industrielle   Portail de l’économie"
économie;"Le secteur tertiaire produit des services, il fait partie du domaine de l'économie. C'est le troisième secteur défini dans la loi des trois secteurs. Il est parmi les trois secteurs économiques définis dans la comptabilité nationale et est de fait défini par complémentarité avec les activités agricoles et industrielles (secteurs primaire et secondaire respectivement).Le secteur tertiaire est composé du :tertiaire principalement marchand (commerce, transports, activités financières, services rendus aux entreprises, services rendus aux particuliers, hébergement-restauration, immobilier, information-communication) ;tertiaire principalement non marchand (administration publique, enseignement, santé humaine, action sociale).Dans les pays développés, c’est de loin le secteur le plus important en nombre d'actifs occupés. En 2012, le secteur tertiaire représentait près de 60 % de l'économie mondiale.La « tertiarisation » de l’économie pose des problèmes statistiques, conceptuels et méthodologiques : les notions de volume, de qualité et de productivité du travail sont probablement à revoir dans le « tertiaire moderne ». L'objet des études futures sera aussi d'examiner si les trois critères d'homogénéité (une part croissante de l'emploi, une relative insensibilité aux crises économiques, et surtout un progrès technique faible), sont aujourd'hui respectés dans un ensemble qui comprend 79 %[réf. nécessaire] de la population active française.Le secteur tertiaire de l'économie, généralement appelé secteur des services, est le troisième des trois secteurs économiques de la théorie des trois secteurs. Les autres sont le secteur secondaire (à peu près identique à celui de la fabrication) et le secteur primaire (matières premières).Le secteur des services consiste en la production de services au lieu de produits finis (en) . Les services (également appelés «biens immatériels») comprennent les soins, les conseils, l'accès, l'expérience et le travail affectif. La production d'informations a longtemps été considérée comme un service, mais certains économistes l'attribuent désormais à un quatrième secteur, le secteur quaternaire .Le secteur tertiaire de l'industrie implique la fourniture de services à d'autres entreprises ainsi qu'aux consommateurs finaux. Les services peuvent impliquer le transport, la distribution (en) et la vente de marchandises du producteur au consommateur, comme cela peut se produire dans la vente en gros et au détail, la lutte contre les ravageurs ou le divertissement. Les produits peuvent être transformés au cours du processus de fourniture du service, comme cela se produit dans l'industrie de la restauration. Cependant, l'accent est mis sur les gens en interagissant avec les gens et servant le client plutôt que des transformations les biens physiques.Il est parfois difficile de définir si une entreprise donnée fait partie intégrante du secteur secondaire ou tertiaire. Et ce ne sont pas seulement les entreprises qui ont été classées comme faisant partie de ce secteur dans certains régimes; le gouvernement et ses services tels que la police ou l'armée, et les organisations à but non lucratif telles que les organismes de bienfaisance et les associations de recherche peuvent également être considérés comme faisant partie de ce secteur.Afin de classer une entreprise en tant que service, on peut utiliser des systèmes de classification tels que la norme de classification industrielle standard internationale des Nations Unies, le système de codes de la classification industrielle standard (SIC) des États-Unis et son nouveau système de remplacement, le système de classification des industries de l'Amérique du Nord. (SCIAN), la nomenclature statistique des activités économiques dans la Communauté européenne (NACE) dans l'UE et des systèmes similaires ailleurs. Ces systèmes de classification gouvernementaux ont un premier niveau de hiérarchie qui indique si les biens économiques sont tangibles ou intangibles.Aux fins de la finance et des études de marché, des systèmes de classification basés sur le marché tels que le Global Industry Classification Standard et l'Industry Classification Benchmark (en) sont utilisés pour classer les entreprises qui participent au secteur des services. Contrairement aux systèmes de classification gouvernementaux, le premier niveau des systèmes de classification basés sur le marché divise l'économie en marchés ou industries fonctionnellement liés. Le deuxième ou le troisième niveau de ces hiérarchies indique alors si des biens ou des services sont produits.Les groupes sociaux non productifs les plus anciens sont les propriétaires fonciers, marchands, militaires et le clergés, etc. Ils établissent leur assise sur le foncier, moyen fondamental de production, sur la régulation, le contrôle et la commercialisation de ses produits, les derniers sur le spirituel, « l'établissement d'une relation au cosmos à travers les religionse »:« Les activités de services sont nées à partir du moment où les activités de survie de l'espèce humaine ont été assurées de façon suffisante pour permettre de dégager un surplus, même temporaire, susceptible d'être attribué à d'autres fonctions et, conjointement, à d'autres individus ou groupes humains, qui ont ainsi acquis la possibilité de s'abstraire, partiellement ou en totalité, des contraintes de la production. En fait, il y a une liaison directe entre le développement d'activités non directement productives et l'organisation d'une société permettant l'appropriation de ces activités par certaines classes sociales. »L'assise du pouvoir de ces classes sociales est toujours justifiée par le « service rendu », et par l'extraction d'une fonction confiée à un corps de « spécialistes », détenteurs d'une compétence qui est de fait déniée aux autres groupes sociaux. Les activités non directement productives se font dans une hiérarchisation croissante.Au cours des 100 dernières années, il y a eu un glissement substantiel des secteurs primaire et secondaire vers le secteur tertiaire dans les pays industrialisés. Ce changement s'appelle la tertiarisation. Le secteur tertiaire est désormais le plus grand secteur de l'économie du monde occidental, et c'est aussi le secteur qui connaît la croissance la plus rapide. En examinant la croissance du secteur des services au début des années 90, le mondialiste Ken'ichi Ohmae a noté le secteur tertiaire représenterait « 70 % de la force de travail aux États-Unis, 60 % au Japon et 50 % à Taïwan »:« In the United States 70 percent of the workforce works in the service sector; in Japan, 60 percent, and in Taiwan, 50 percent. These are not necessarily busboys and live-in maids. Many of them are in the professional category. They are earning as much as manufacturing workers, and often more. »Les économies ont tendance à suivre une progression de développement qui les fait passer d'une forte dépendance à l'agriculture et à l'exploitation minière au développement de l'industrie manufacturière (par exemple, automobiles, textiles, construction navale, acier) et finalement à une structure davantage axée sur les services. La première économie à suivre cette voie dans le monde moderne a été le Royaume-Uni. La vitesse à laquelle d'autres économies ont fait la transition vers des économies de services (ou « post-industrielles ») a augmenté avec le temps.Historiquement, le secteur manufacturier avait tendance à être plus ouvert au commerce international et à la concurrence que les services. Cependant, avec une réduction spectaculaire des coûts et des améliorations de la vitesse et de la fiabilité dans le transport des personnes et la communication de l'information, le secteur des services comprend désormais une partie de la concurrence internationale la plus intense, malgré un protectionnisme résiduel.Vous trouverez ci-dessous une liste de pays par production de services aux taux de change du marché en 2016:Les prestataires de services sont confrontés à des obstacles dans la vente de services auxquels les vendeurs de biens sont rarement confrontés. Les services sont intangibles, ce qui rend difficile pour les clients potentiels de comprendre ce qu'ils recevront et quelle valeur cela représentera pour eux. En effet, certains, comme les consultants et les prestataires de services d'investissement, n'offrent aucune garantie de la valeur pour le prix payé.Étant donné que la qualité de la plupart des services dépend en grande partie de la qualité des personnes qui fournissent les services, les « coûts de personnel » représentent généralement une fraction élevée des coûts des services. Alors qu'un fabricant peut utiliser la technologie, la simplification et d'autres techniques pour réduire le coût des produits vendus, le fournisseur de services est souvent confronté à un schéma constant d'augmentation des coûts.La différenciation des produits est souvent difficile. Par exemple, comment peut-on choisir un conseiller en placement plutôt qu'un autre, étant donné qu'ils sont souvent perçus comme fournissant des services identiques ? La facturation d'une prime pour les services n'est généralement une option que pour les entreprises les plus établies, qui facturent des frais supplémentaires en fonction de la reconnaissance de la marque.Des exemples d'industries tertiaires peuvent inclure:TélécommunicationHôtellerie TourismeMédias de masseSoins de santé/hôpitauxSanté publiquePharmacieTechnologie de l'informationGestion des déchetsConsultantJeu d'argentCommerce de détailBien de grande consommation (FMCG)FranchisageImmobilierÉducationServices financiersBanqueAssuranceInvestment management (en)Services professionnels (en)Services juridiques (en)Conseil en managementTransportÉducationAprès s'être développé jusqu'en 1960 selon un rythme annuel moyen de 1 % en France, l'emploi des branches tertiaires progresse très vivement de 1960 à 1980 (2 % par an), puis encore assez fortement de 1980 à 2000 (+1,7 %). Il ralentit ensuite entre 2000 et 2011 (+0,9 %), avec une quasi-stagnation de 2008 à 2011, voire une légère baisse dans certains services traditionnels aux ménages ou les télécommunications. Ceci est le résultat de deux tendances : une accélération de la croissance de la demande intérieure tertiaire (+ 4,3 % par an en volume entre 1959 et 2012) et des gains de productivité du travail plus faibles que dans le reste de l'économie (+ 2,5 % par an dans les services marchands contre + 4,5 % dans l’industrie). Face à une demande croissante, un secteur dont la productivité progresse relativement plus lentement ne peut que se développer en terme d'emploi. Depuis le début du siècle dernier, le progrès technique a toujours été plus faible dans le tertiaire que dans les autres secteurs. Expliquer « l'explosion » récente de l'emploi tertiaire revient donc à analyser les raisons de l'accélération de la demande tertiaireLa balance commerciale des services est excédentaire en France. Finance On parle dans ce cas là, des garages de voitures ou encore des magasins de ventes qui comprend en particulier le secteur bancaire et celui de l'assurance au point d'être parfois désigné par « Secteur Banque Assurance ». Information  Sécurité Cette branche du secteur tertiaire comprend notamment les activités de police, de milice et d'armée assurant la sécurité des biens et des personnes. Justice Aux États-Unis, 2 % du PIB est obtenu en justice. Bénévolat Selon une étude de l’Insee parue en 2004, le bénévolat représentait aux alentours de 1 point de PIB.Le tertiaire « supérieur » (ou mixte) regroupe les « métiers du savoir » qui fournissent aux entreprises et aux particuliers des prestations intellectuelles complexes. Essentiels au fonctionnement de l’économie et au développement stratégique des entreprises, élément clef du rayonnement et de l’attractivité du territoire, ces métiers constituent également par eux-mêmes un secteur économique majeur, en fort développement. Recherche et éducation Le secteur tertiaire, privé et public, via ses infrastructures (bâtiments tels que bureaux, hôtels, commerces, d'enseignement et les bâtiments administratifs, réseaux internet, serveurs..) est devenu un grand consommateur d'énergie et de foncier ; En France, à la suite de la loi Grenelle 2 de 2010, un décret annoncé pour fin 2012 mais un temps repoussé par le Conseil d’État avant d'être publié en mai 2017 et entrant en vigueur le 11 mai 2017, afin de « favoriser l'efficacité et la sobriété énergétiques », impose aux « collectivités territoriales services de l'Etat, propriétaires et occupants de bâtiments à usage tertiaire privé, professionnels du bâtiment, maîtres d'ouvrage, maîtres d'œuvre, bureaux d'études thermiques, sociétés d'exploitation, gestionnaires immobiliers, fournisseurs d'énergies » une « obligation d'amélioration de la performance énergétique dans les bâtiments à usage tertiaire », sur la base d'une « étude énergétique portant sur tous les postes de consommations du bâtiment », avec un niveau d'économie d'énergie à atteindre avant 2020 et « un ou plusieurs scénarios permettant de diminuer, d'ici 2030 » la consommation d'énergie. Un observatoire doit recueillir les données permettant d'évaluer les résultats et de mettre à jour les données. L'étude énergétique est accompagnée de propositions de travaux d'économie d'énergie et de « recommandations hiérarchisées selon leur temps de retour sur investissement » avec présentation des « interactions potentielles entre ces travaux ». Ce décret permet de mutualiser l'obligation sur l'ensemble d'un patrimoine, et prévoit le cas d'un changement de propriétaire ou de preneur (un dossier dédié sera annexé au contrat de vente ou de bail). Il demande aussi une sensibilisation des occupants au thème des économies d'énergie. La « non-atteinte » des objectifs malgré les actions et travaux entrepris devra pouvoir être justifiée auprès des services de l’État. Un propriétaire d'un ensemble de bâtiments ou de parties de bâtiments concernés peut remplir globalement ses obligations sur l'ensemble de son patrimoine.Michel Braibant, De la désindustrialisation à la tertiairisation, vers un mélange des genres, Paris, Société des écrivains, 2 mai 2015, 188 p. (ISBN 9782342034738).Secteur économiqueBranche d'activitéSecteur quaternaireTertiarisation du travailSecteur bénévoleSociété post-industrielleEffet de structure de fret Portail de l’économie"
économie;"Un système économique est le mode d’organisation de l'activité économique d’une société ou d'une aire géographique donnée, qui détermine la production, la consommation, l'utilisation des ressources, etc. Dans un sens élargi, il peut être également compris comme l'organisation sociale induite par le système. Le système économique influence de nombreux facteurs, comme le niveau de vie des habitants, le niveau des inégalités, les relations avec les autres pays, ou la puissance économique.Un système économique est un mode d'organisation de la société analysé par le prisme économique. Un tel système regroupe l'ensemble des institutions, sociales comme normatives, qui agissent comme une courroie ou un récipiendaire de l'activité économique au niveau micro. Le système forme le niveau macroéconomique de la vie du pays. Les normes du système (via le droit du pays) détermine également la répartition des richesses.Ce système n'est pas neutre économiquement, dans le sens où ses structures conditionnent l'affectation des ressources et la productivité (production unitaire d'un facteur de production utilisé dans la combinaison productive) des facteurs. Sa modulation joue donc un rôle essentiel dans le cadre des politiques publiques visant à générer de la croissance ou à modifier le niveau d'inégalités.Les systèmes économiques varient en fonction des régions et des époques. Les pays occidentaux suivent une organisation fondée sur le capitalisme caractérisé par la propriété privée des moyens de production. Le système économique des pays de l'ancien bloc de l'Est était fondé sur le principe de l’économie communiste dont l'objectif est la propriété collective des moyens de production.Un système économique est lui-même composé d'une multitude de sous-systèmes, plus petits. Les agents qui le composent peuvent y être spécialisés : le système économique vigneron dispose de spécificités face au système économique de l'agroalimentaire auquel il appartient, qui, lui-même, n'est qu'un sous-système d'un ensemble plus grand.Du fait des combinaisons politiques et des modalités que peuvent prendre les institutions, il n'y a pas un seul modèle de système économique. Dans Les trois mondes de l'Etat-providence, Gøsta Esping-Andersen identifie trois types de systèmes économiques, correspondant à un système continental, un système nordique social, et un système anglo-saxon. Des recherches ultérieures ont identifié des systèmes économiques arabo-musulmans et asiatiques.Raymond Aron transpose par ailleurs le concept à la société internationale, en écrivant : « les États, par leurs politiques, contribuent à former le système économique mais celui-ci, inégalement déterminé par les États selon les pesanteurs de chacun d'eux, constitue un système différent du système interétatique, qu'on devrait plutôt qualifier de transnational que d'interétatique ».Un système économique est composé par des relations entre ses acteurs. Pierre-Joseph Proudhon note ainsi en 1850 que le système économique capitaliste d'Europe de l'Ouest est dominé par la figure du travailleur et celle du patron. Les relations entre les agents sont modulées par les politiques publiques, qui peuvent par exemple décider d'un niveau de centralisation plus ou moins élevé de l'activité économique.Le passage à un mode de développement durable suppose d'évoluer vers un nouveau modèle économique. Un système économique peut être refusé par certains de ses agents, voire remplacé. Le communisme est ainsi un système économique concurrent au système économique capitaliste. Un système économique se fond dans les institutions sociales ; dès lors, le changement de système économique requiert une modification desdites institutions.Le concept de souveraineté économique permet de qualifier un système économique dont les principales sources d'approvisionnement de matériaux et de biens stratégiques sont endogènes au système. Un système économique peut, à l'extrême, être autarcique, c'est-à-dire fonctionner sans relations avec d'autres systèmes économiques.Certains auteurs comme Jean-Marie Albertini constatent que le mot système ou modèle est utilisé par les économistes pour déterminer les différences théoriques essentielles entre les pays appartenant au même système (Japon et États-Unis, par exemple) ou, a fortiori entre des pays qui appartiennent à des systèmes économiques et sociaux totalement opposés (URSS et États-Unis lors de l'époque de la Guerre froide). Toutefois, ce mot ne permet pas de décrire l'évolution historique et réelle d'un pays ou d'un système. Pour cela, Jean-Marie Albertini propose d'utiliser le mot « régime ».Développement économique et social Portail de l’économie"
économie;"L'organisation industrielle (ou économie industrielle ou Concurrence imparfaite et industrial organization  en anglais) est la branche de la microéconomie qui étudie le fonctionnement des marchés et les comportements des entreprises sur ces marchés. Elle traite notamment des situations dans lesquelles les entreprises disposent d'un pouvoir de marché, ce que les économistes appellent la concurrence imparfaite. Elle ne se réduit toutefois pas à l'analyse de la concurrence imparfaite. Un de ses objectifs est d'évaluer la performance des marchés en matière d'efficacité et de bien-être collectif. À cet égard, l'économie industrielle comporte une dimension importante d'aide à la décision publique, pour tout ce qui touche à la régulation des marchés.L’économie industrielle a pour objet l’étude de « l’organisation et du fonctionnement des entreprises et des marchés dans le monde réel » (Médan et al, 2000). L'organisation industrielle tire l'essentiel de ses outils de la microéconomie et de la théorie des jeux.Les questions posées par l'organisation industrielle visent à ouvrir un certain nombre de boîtes noires de la microéconomie néoclassique. Elle se demande ainsi pourquoi il existe des entreprises (plutôt qu'un monde de travailleurs indépendants) ?pourquoi la taille et la structure varient en fonction des produits, des marchés et du temps ?pourquoi la prédiction du prix au coût marginal n'est que très rarement vérifiée ?Dans ce cadre, l'organisation industrielle a absorbé l'étude des monopoles et des oligopoles, ainsi que la problématique schumpetérienne du lien entre la capacité d'extraire des profits et la capacité à supporter des dépenses liées à la recherche et à l'innovation. Elle s'interroge ainsi également sur les raisons de la diversité des biens et par voie de conséquence à la dynamique de l'innovation.En 1988, Richard Schmalensee définit ainsi l'économie industrielle par trois thèmes essentiels :L'étude des déterminants du comportement, de la taille, de l'échelle et de l'organisation des entreprises privées ;La concurrence imparfaite, c'est-à-dire dans quelle mesure le fonctionnement et la performance du marché (en tant que moyen d'allocation des ressources entre agents) est affecté lorsque les conditions de la concurrence pure et parfaite (CPP) ne sont pas respectées ? Ce thème couvre en particulier les questions de choix de prix, de quantité et de capacité, ainsi que la concurrence hors-prix : sélection des produits, publicité, changement technique.L'étude des politiques publiques concernant l'activité économique, en particulier en matière de droit de la concurrence, de dérégulation, et de privatisations, ainsi que des politiques industrielles affectant le progrès technique.Dans son ouvrage Théorie de l’organisation industrielle, Jean Tirole souligne la difficulté empirique qu’il y a à définir un marché en écrivant que « la notion de marché est loin d’être simple,… [si] la définition d’un marché ne saurait être trop étroite, la définition ne doit pas être [non plus] trop large. La « bonne » définition dépend de l’usage qui en sera fait. Il n’y a pas de recette facile pour définir un marché ». Au-delà de cette plaisante réflexion, l’idée est de décrire une norme (que l’on pourrait présenter comme un idéal-type au sens de Max Weber) pour examiner, ensuite, comment et pourquoi la réalité s’écarte de cette référence. Au sens économique, un marché est le lieu où se rencontrent une offre et une demande, où s’établissent des contrats (qui peuvent porter sur les quantités, la qualité ou le prix) et où se concluent des échanges.En économie industrielle, il convient aussi, pour rester pertinent, de définir « l’étendu d’un produit », c’est-à-dire ses caractéristiques qui font que les biens entre eux ne sont pas parfaitement substituables mais similaires.  Par exemple définir, selon les cas, si une chemise et un tee-shirt sont similaires (ce sera alors le même marché) ou s’il faut considérer deux marchés distincts.En pratique, il faut s’intéresser à l’organisation de la production qui va influer sur la nature du marché. En raison des spécificités sectorielles, il s’agit aussi souvent d’adopter une approche au cas par cas pour étudier la concurrence. L’organisation industrielle s’intéresse donc à des équilibres partiels et non à l’équilibre général, cher aux macroéconomistes.L’économie industrielle s’intéresse aux interactions entre les différents acteurs du marché (i.e., entreprises, consommateurs et l'Etat).  L'entreprise Selon Jean Tirole, « une entreprise doit être capable de produire (ou vendre) plus efficacement que ne le pourraient ses parties constituantes en agissant séparément ». Une entreprise doit optimiser en permanence pour maximiser son profit (ou minimiser ses pertes), mettre en œuvre différentes combinaisons d’activité, s’adapter à son environnement. Ce comportement peut l’amener à rechercher un pouvoir de marché allant jusqu’au monopole, par l'élimination progressive de ses concurrents, ou à profiter des interactions avec les autres producteurs pour s’inscrire dans un oligopole. Le profit étant défini comme l’écart entre le chiffre d’affaires et les coûts engagés (fixes et variables), sa maximisation suppose de minimiser les coûts sous la contrainte de la fonction de production (pour vendre, il faut produire un bien ou un service). La fonction de coût d’une entreprise est la donnée clé de l’économie industrielle. L'entreprise comme processus de production Dans l'analyse microéconomique traditionnelle, ou standard, l'entreprise est abordée au travers de ses caractéristiques techniques. Elle est une organisation dont le but est de produire certains biens ou services. Pour produire ces biens, elle combine des facteurs de production, tels que la force de travail, le capital matériel (locaux, machines, etc.) et immatériel (savoir-faire, connaissance, etc.), des matières premières ou des biens intermédiaires. Les contraintes techniques de l'entreprise sont représentées par une fonction de production qui détermine les niveaux de production accessibles pour différentes combinaisons des facteurs de production.  L'entreprise comme organisation L'entreprise remplit deux fonctions.Elle est une instance de coordination du processus de production. Dans La Richesse des nations (1776), Adam Smith présente la firme moderne comme une réponse à la complexité croissante des activités, en particulier à la division du travail.Elle repartit la valeur créée entre les parties impliquées dans le processus, ainsi que les risques liés aux aléas de la production.Cependant, cela n'explique pas la forme particulière d'organisation qu'est la firme et pourquoi ces deux fonctions doivent être remplies au sein de celle-ci. Comme le souligne Ronald Coase dès 1937, la nature fondamentale des échanges qui s'opèrent au sein d'une entreprise n'est pas différente de celle des échanges qui s'opèrent sur les marchés. Telle entreprise peut faire effectuer la même opération en interne ou, en externe, en faisant appel à un sous-traitant. Seul le mode de transaction change. Selon Coase, la question est donc de comprendre pourquoi tel type d'échange se fait au sein d'une entreprise, et tel autre sur les marchés. Il fonde alors son approche sur le fait que tout échange implique des coûts de transaction (principalement des coûts de recherche du meilleur partenaire et de négociation des contrats), L'échange se fera au sein d'une entreprise si le coût de transaction (i.e., en externe) de cette opération est moins élevé dans l'entreprise que sur le marché. Les travaux de Coase ont eu une profonde influence sur l'analyse moderne de l'entreprise, telle qu'elle apparaît, dès le milieu des années 1970, en particulier dans les travaux d'Oliver Williamson qui a cherché à définir la nature de ces coûts de transaction.Au-delà des coûts directs (la négociation des échanges par exemple), cette notion de coûts englobe tous les facteurs qui limitent les parties dans leur capacité d'améliorer l'efficacité des échanges. Une des sources d'inefficacité, mise en avant par Herbert Simon (1976), provient des limites aux capacités cognitives des individus, la rationalité limitée (par opposition à la rationalité illimitée chère aux économistes) des acteurs. Une deuxième limitation provient de ce que Williamson appelle l'opportunisme des acteurs. Une entreprise est une organisation mettant en présence des individus aux intérêts multiples et parfois contradictoires. Les objectifs des cadres dirigeants (managers, en anglais), des propriétaires et des employés sont différents. Dans un tel contexte, les divers acteurs ne sont enclins à révéler l'information dont ils disposent et à agir dans le sens de l'intérêt général que si cela sert leur objectif propre. La bonne marche de la coopération nécessite la mise en place de mécanismes visant à limiter les comportements opportunistes, c'est-à-dire à fournir de bonnes incitations. Et cela induit nécessairement une certaine perte d'efficacité.Le système de prix constitue le mécanisme utilisé par le marché tandis que l'entreprise se fonde sur une forme d'organisation plus hiérarchisée. À cet égard, la firme est vue comme un ensemble de contrats liant les parties dans le but de mettre en place la production.  Le consommateur La fonction de demande reflète le comportement du consommateur qui, en tenant compte de sa contrainte budgétaire, maximise son utilité. Ce comportement a un effet sur le prix d’équilibre et donc sur le chiffre d’affaires de l’entreprise. La fonction de demande est déterminée par un « prix de réserve » au-dessus duquel le consommateur a décidé qu’il n’achèterait pas. Si ce prix est proposé, le consommateur A achète ; si le prix est inférieur A et B, qui a un « prix de réserve » inférieur, achètent et A voit sa satisfaction (son bien-être) augmenter. Plus le prix est bas, plus nombreux et contents sont les consommateurs. Le « surplus social » augmente. Les pouvoirs publics En économie industrielle, les pouvoirs publics mettent en place des politiques de règlementation ou de régulation pour corriger les imperfections de marché.L'organisation industrielle étudie le pouvoir de marché des entreprises qui peut découler des différentes structures de marché et des interventions publiques mises en oeuvre pour limiter la capacité des entreprises à abuser de leur pouvoir de marché. Dans une première approche, la taille et le nombre d'entreprises dans un secteur d'activité reflètent l'ampleur des économies d'échelle, terme utilisé pour indiquer le lien entre le niveau global de la production d'une entreprise et son coût unitaire (coût total rapporté au volume de la production). On dit qu'il y a des économies d'échelle si le coût unitaire baisse lorsque la production totale augmente. Dans ce cas, il est plus efficace de concentrer la production sur un nombre restreint d'entreprises que de la répartir entre de nombreuses entreprises produisant peu, puisque ainsi on réduit la dépense par unité produite.Tout démarrage d'une nouvelle activité industrielle nécessite des dépenses initiales spécifiques : recherche et développement, mise en place des capacités de production et du circuit de distribution, information des consommateurs et construction d'une image de marque. Le concept de coûts d'entrée regroupe ces dépenses. À la différence des coûts fixes, les coûts d'entrée ne sont engagés qu'une fois. Lorsqu'elle envisage d'entrer sur un marché, une entreprise doit évaluer si les perspectives de profit futur justifient l'engagement de ces coûts. Cela signifie que si les coûts d'entrée sont élevés, il y aura peu d'entreprises actives sur le marché.L'intervention des pouvoirs publics peut aussi freiner l'entrée sur un marché en bloquant par des barrières à l'entrée les nouveaux entrants ou les produits substituables au sens de M. Porter. Ces barrières peuvent être tarifaires (impôts et taxes, droits de douane) ou non tarifaires (quotas, normes). Dans ce cas, elle a un coût social (prix élevés, qualité insuffisante, etc.) qu'il convient de mettre en balance avec les justifications de l'intervention, qu'elles soient d'ordre économique, social, juridique ou prudentiel.Voir monopole pour l'article détaillé.Les oligopoles s’observent en pratique dans de nombreux secteurs d’activité comme la construction automobile ou l’industrie du tabac. Dans la plupart des cas, un oligopole est non-coopératif et chaque entreprise prend ses décisions (sur le prix ou sur les quantités) en fonction de ce qu’elle suppose être le comportement des autres membres de l’oligopole. En langage économique, il s’agit de faire des hypothèses sur la fonction de réaction des concurrents. Cette étape vient s’ajouter à la connaissance de la fonction de demande des consommateurs. Voir oligopole pour l'article détaillé.Les biens offerts sur un même marché sont rarement homogènes, rigoureusement interchangeables. Leurs différences résultent d'une politique de production délibérée. Les offreurs souhaitent souvent se distinguer les uns des autres pour capter une clientèle, la fidéliser. Et les concurrents vantent leurs produits en matière de qualité et de performances d'utilisation pour conquérir et défendre des micro-monopoles.La différenciation du bien est verticale si, à prix égal, les acheteurs adressent unanimement leurs demandes au bien de qualité présumée supérieure. La qualité du bien est, en ce cas, synonyme d'excellence. La différenciation est horizontale si, à prix égal, les acheteurs se répartissent entre les différentes versions du produit, en fonction de leurs qualités respectives et des goûts personnels des consommateurs. Dans ce cas, la qualité définit une étroite conformité aux attentes de la clientèle visée.Lorsqu'il existe des barrières technologiques à l'entrée, les firmes en place sur un marché peuvent en plus développer des stratégies industrielles qui dissuadent les concurrents en puissance de venir sur le marché. Puisqu'une entreprise n'entre sur un marché que si elle anticipe des profits suffisants pour compenser ses coûts d'entrée, toute stratégie qui permet de réduire les profits futurs des concurrents permet de barrer l'entrée. Dans ce contexte, on dit que la firme en place met en place des barrières stratégiques à l'entrée. Ces stratégies soulèvent deux difficultés : elles doivent d'abord être crédibles, et elles ne doivent pas être trop coûteuses (pour la firme en place).On parle de comportement de prédation lorsqu'une entreprise « agresse » des concurrents afin de les évincer du marché ou tout au moins de les fragiliser pour accroître son pouvoir de marché. Les stratégies de surinvestissement déjà discutées peuvent être adoptées dans ce but. Une alternative consiste à baisser fortement ses prix de façon à diminuer la rentabilité des concurrents. Les relations verticales entre entreprises fait également l'objet de travaux en économie industrielle. Le problème de double marginalisation est une illustration.Les régulateurs cherchent à contrôler le comportement des firmes disposant d’un pouvoir de marché (monopoles, monopoles naturels, oligopoles). Il s’agit d’aligner l’objectif de la firme et l’intérêt collectif. La tâche est rendue délicate du fait de l’asymétrie d’information entre régulateur et régulé, lequel par essence connaît mieux sa propre situation (fonction de coûts, technologie utilisable pour abaisser les coûts, effort de réduction des coûts…) que le régulateur. C’est une information privée qu’il n’a pas toujours intérêt à révéler.Laffont et Tirole (1986) ont construit un modèle qui montre que le problème peut être résolu par l’offre d’un menu de contrats. En choisissant un contrat, le régulé révèle son information privée (auto révélation). Dans la réalité cela se concrétise à l’occasion de processus de négociation entre régulateur et régulé (généralement pas explicitement par un menu de contrats). Après avoir rappelé la nature et les conséquences essentielles de l’asymétrie d’information, on examinera la logique qui préside à l’offre d’un menu de contrats pour enfin présenter brièvement les principes du modèle fondamental de Laffont et Tirole.Les relations étudiées supposent un déficit informationnel chez l'autorité (le principal). Généralement, l'opérateur (l'agent) dispose d'une connaissance privée de la technologie utilisée, de l'état des coûts d'exploitation ou de la demande du marché. La nouvelle théorie de la régulation propose donc de considérer systématiquement les rapports entre régulateur et opérateur à travers le cadre normatif principal-agent. S'inspirant des techniques de la conception de mécanismes, cette approche élabore les mécanismes régulateurs optimaux et requiert pour cela une définition précise des objectifs de l'autorité et de la firme et une prise en compte rigoureuse des contraintes économiques et informationnelles. Les contraintes informationnelles constituent l'enjeu majeur de cette théorie. Leur existence gêne le contrôle de l'autorité et empêche celle-ci de mettre en place la politique réglementaire qui s'avère être la meilleure pour la société.L’économie industrielle est le fruit d’une longue tradition, initiée par des ingénieurs économistes français, dont Cournot et Dupuit sont les grands noms. Elle s’est ensuite tournée vers les politiques publiques, avec l’entrée en vigueur du Sherman Act aux Etats-Unis et la construction du droit de la concurrence. L'économie industrielle s'est imposée en tant que discipline à partir des années 1940, sous l'impulsion d'économistes tels qu'Edward Mason et Joe Bain. Ce courant initial, baptisé Tradition de Harvard, a adopté une approche descriptive, et sa fameuse « Structure-Conduct-Performance » qui conforte et affine l’intervention publique dans l’organisation des marchés,. On l'oppose généralement à la Tradition de Chicago, plus théorique et non- interventionniste. Cette dernière adopte une approche sceptique critiquant l’absence de fondement théorique de l’économie industrielle, mais qui, réticente face à la régulation en général, n’a pas développé d’autres doctrines en la matière.[1]En effet, avant les années 1980, pour l’économiste, une entreprise était, de fait, une boîte noire (qui permet de transformer divers entrants en produit final et dont la fonction de coût permet d’établir un prix de vente). La portée des études systématiques était de ce fait limitée : il est possible de constater une corrélation entre, par exemple, le taux de concentration d’une industrie et les profits réalisés par les entreprises du secteur sans saisir les causalités à l’œuvre. Les conclusions qui pouvaient être tirées de telles études étaient souvent rudimentaires (lutter contre les cartels, casser les monopoles) et sans véritable dimension prospective, ce qui réduisait leur efficacité.C'est avec les avancées en théorie des jeux et théorie de l'information, au début des années 1980, que l'économie industrielle se dote d'outils pour modéliser les comportements complexes des entreprises. Cela donna naissance à une littérature très majoritairement théorique. Cette approche permet de tenir compte des asymétries d’information et de comprendre de quelle façon un changement de règlementation peut les corriger. La comparaison d'un texte de référence des années 1970, Industrial Market Structure and Economic Performance de F. Michael Scherer, et du texte de Jean Tirole, Théorie de l'organisation industrielle (1988), montre cette évolution sensible, due à l'irruption de la théorie des jeux. La théorie de l’organisation industrielle a permis de passer du constat comportementaliste statique (telle industrie se trouve dans telle situation) à une approche cognitive de l’entreprise (relations avec les concurrents, politique de recherche et développement, différenciation des produits). La compréhension des choix stratégiques (passés et futurs) permet de mieux spécifier les caractéristiques particulières du secteur et de rendre ainsi la réglementation plus efficace (ouverture à la concurrence, régulations de firmes dominantes).  En permettant, toujours à travers des modèles, ces études rigoureuses des interactions stratégiques permettent à l’économiste d'apporter des conseils plus pertinents (à l’entreprise pour son organisation ou à l’Etat pour la règlementation) qui seront adaptés à une industrie spécifique.   L’économie industrielle — et plus encore l’économie de l’innovation — ne se sont développées que tardivement en France, par rapport à l’émergence de l’industrial organization aux États-Unis. Il faut en effet attendre les années 1970 pour que les premiers manuels soient publiés en français et que cet enseignement ne soit introduit dans le cursus de licence ou maîtrise de sciences économiques des universités. L'école d'économie de Toulouse a contribué fortement à cette discipline.   Après un développement important de la théorie, la discipline semble maintenant s'orienter vers une phase de travaux plus appliqués (empirical industrial organization).   Définitions L'organisation industrielle étudie les comportements stratégiques entre acteurs économiques, et donc n'entre pas dans le cadre de la concurrence pure et parfaite (CPP). Ce champ est parfois nommé ""concurrence imparfaite"". Ces deux dénominations correspondent aux titres des versions française et anglaise de l'ouvrage de Jean Tirole, Concurrence imparfaite, Economica, Paris, 1986 et Industrial Organization, MIT press 1988, qui constituent le point de départ de ce champ en France.Par ailleurs, l'utilisation d""industrielle"" dans la dénomination de cette discipline peut être trompeuse. Elle n'est pas spécifique à l'industrie et concerne tous les acteurs et secteurs économiques dont celui des services. Cela provient du fait qu'en anglais, le terme ""industry"" désigne plutôt un secteur ou une filière qu'une structure proprement industrielle (au sens du secteur industriel, secondaire, transformatif par opposition aux secteurs primaire - extractif - et tertiaire - services). En particulier, l'étude économique du fonctionnement d'une industrie se rapproche plutôt de la recherche opérationnelle, de l'ingénierie de production et de la logistique que de l'économie industrielle. Industrial organization versus Industrial Dynamics L’économie industrielle se diffuse en France selon deux principales orientations correspondant aux deux voies suivies par la discipline:Un premier courant prolonge l’approche américaine où l’économie industrielle reste centrée sur les questions des structures de marché et sur l’évaluation de leur efficacité en matière d’allocation des ressources. L’Institut d’économie industrielle (IDEI) à Toulouse constitue le fleuron de cette démarche, grâce à l’implication de deux universitaires français, Jean-Jacques Laffont et Jean Tirole, qui feront le lien entre leurs deux attaches : l’une au Massachusetts Institute of Technology (Boston) aux États-Unis et l’autre à Toulouse, où ils vont créer l'un des meilleurs pôles européens en matière de recherches et de formation de docteurs dans le champ de l’industrial organization, connu aujourd’hui sous le nom de Toulouse School of Economics (TSE). Pour une grande part, l’économie industrielle concerne la microéconomie en situation d’information imparfaite.Un deuxième courant, prenant appui sur les différentes approches structuralistes très présentes parmi les économistes français et parmi certains gestionnaires travaillant sur la firme, va développer une approche de l’économie industrielle plus orientée vers la question productive que sur celle du marché. Prenant appui sur une association, l’Association pour le développement d’études sur la firme et l’industrie (ADEFI, association aujourd’hui en sommeil) et sur une revue scientifique (la Revue d’économie industrielle), proposant des outils de politique industrielle (notamment les politiques de filière des années 1980) et, plus tard, sur une école d’été (l’école méditerranéenne d’économie industrielle, Cargèse), l’étude des systèmes productifs devient centrale dans ces travaux. La publication du volumineux Traité d’économie industrielle en 1988 constitue le point d’orgue de cette démarche.Ces travaux s’efforcent de fédérer un ensemble d’équipes dispersées (Paris, Nice, Strasbourg, Grenoble, Montpellier, Lyon, Bordeaux). Par opposition au courant précédent, ces analyses entendent se concentrer sur les processus de création des ressources en les replaçant dans un cadre contextuel large : stratégies des acteurs, analyses organisationnelles des firmes, prise en compte des technologies et de leurs modifications mais aussi place centrale des Institutions dont l’État. La montée des questionnements scientifiques associés aux différentes formes de l’informatisation de la société va réactualiser l’analyse de l’innovation, longtemps négligée, comme le propose Jean-Luc Gaffard (1990).Quand Bo Carlsson (1987, 1992) proposera d’opposer l’Industrial Organization (IO) centré sur la question de l’allocation des ressources existantes à l’Industrial Dynamics (ID) orienté vers la création de ressources et l’innovation technologique, le second courant se revendiquera clairement du champ de la Dynamique industrielle (Arena, 1990). Ce dernier se décline cependant dans des approches hétérodoxes diversifiées (évolutionnisme, régulationisme, institutionalisme, etc.) et veut s’opposer à la « nouvelle économie industrielle ».En français, il est assez classique de reprendre cette opposition en proposant les termes d'organisation industrielle et d'économie des organisations.Les années 1980 et début des 1990 correspondent à l’âge d’or de l’économie industrielle et de l’innovation, dans la diversité des approches développées. Plusieurs facteurs expliquent pourquoi elle va à cette époque prendre une position dominante dans l’ensemble du champ de la science économique et participer à son renouvellement.Premièrement, elle est productrice de concepts novateurs et de nouvelles approches (économie de l’innovation, théorie des coûts de transaction, théorie des marchés parfaitement disputables, économie de la connaissance, etc.).Deuxièmement, elle est en prise avec les problèmes auxquels sont alors confrontées les économies en matière de concurrence par l’innovation ou de reconversion industrielle (retour à Schumpeter et développement de l’évolutionnisme) : il s’agit de problèmes productifs.Troisièmement, la crise du keynésianisme a laissé la place libre aux approches néoclassiques en macro-économie et les courants hétérodoxes ont trouvé dans la méso-économie un espace et des outils pour penser dans leur propre champ (jusqu’au retour de l’institutionnalisme).Les approches initiées dans le champ de l’économie industrielle et de l’innovation vont trouver des applications dans de nouveaux champs disciplinaires qu’ils vont contribuer à renouveler : économie bancaire, nouvelle économie internationale, l’économie spatiale et, plus tard, l’économie géographique, l’économie du développement, etc. On peut même parler d’une banalisation tant aujourd’hui il est communément admis que la rivalité concurrentielle ou l’avantage compétitif (ou concurrentiel cher à Michael Porter) passe par l’aptitude des firmes et des économies à innover.L'économie industrielle reste aussi très proche de la pratique par son influence sur la conduite de la politique de la concurrence et sur celle de la politique de réglementation des monopoles.Cet article est partiellement ou en totalité issu de l'article intitulé « Économie industrielle » (voir la liste des auteurs).Bruno JULLIEN, « ÉCONOMIE INDUSTRIELLE », dans Encyclopædia Universalis (lire en ligne)Jean Tirole (trad. de l'anglais), Théorie de l'organisation industrielle, Paris, Economica, 1993, 419 p. (ISBN 2-7178-2218-6 et 2-7178-2217-8)Yves Morvan, Fondements d'économie industrielle, Paris, Economica, 1991(en) Luis Cabral, Introduction to industrial organization, MIT Press, 2000, 354 p. (ISBN 0-262-03286-4, lire en ligne)Les trois volumes du Handbook of Industrial Organization constituent la référence de ce champ à un niveau recherche (M1 et au-delà).(en) Handbook of Industrial Organization, vol. 1, Amsterdam/London/New York etc., Elsevier Science Pub. Co., coll. « Handbooks of Economics », 1989, 947 p. (ISBN 0-444-70434-5)(en) Handbook of Industrial Organization, vol. 2, Elsevier Science Pub. Co., coll. « Handbooks of Economics », 1989, 1555 p. (ISBN 0-444-70435-3)(en) Handbook of Industrial Organization, vol. 3, Elsevier Science Pub. Co., coll. « Handbooks of Economics », 2007, 2440 p. (ISBN 978-0-444-82435-6 et 0-444-82435-9, lire en ligne)(en) Richard Schmalensee, « Industrial Economics: An Overview », The Economic Journal, vol. 98, no 392,? septembre 1988, p. 643 - 681 (ISSN 0013-0133, résumé)Carlsson B., 1987, Reflections on industrial dynamics: the challenges ahead, International Journal of Industrial Organization, 5(2)Carlsson B., 1992, Industrial Dynamics: A Framework for Analysis of Industrial Transformation. Revue d’économie industrielle, no.61, 7-32.Arena R., 1990, La dynamique industrielle : tradition et renouveau, Revue d’économie industrielle, no.53, 5-17.Arena R., de Bandt, J., Benzoni L., Romani P.M. (coord.), 1988, Traité d’économie industrielle, Economica, Paris. *Gaffard J.L., 1990, Économie industrielle et de l’innovation. Précis Dalloz, Paris(en) Dennis W. Carlton, Modern Industrial Organization, Boston, Pearson / Addison Wesley, 2005, 4e éd., 822 p. (ISBN 9780321223418)Revue d'économie industrielleIJIO Portail de l’économie   Portail de la production industrielle"
économie;"Une économie du marché est un système économique où les décisions de produire, d'échanger et d'allouer des biens et services rares sont déterminées majoritairement à l'aide d'informations résultant de la confrontation de l'offre et de la demande telle qu'établie par le libre jeu du marché. Confrontation qui détermine les informations de prix, mais aussi de qualité, de disponibilité. Au cœur de l'économie de marché, le mécanisme de l'offre et de la demande concourt à la découverte et à l'établissement des prix. Ce mécanisme opère par arbitrage pour un horizon donné et pour une qualité donnée entre des valeurs représentatives du bien ou du service concerné : d'une part la valeur de son coût intrinsèque (prix de revient) mais aussi d'autre part sa valeur d'échange (prix relatif, c'est-à-dire du prix d'un produit ou d'un service par rapport aux autres).Pour Robert Gilpin la dynamique de l'économie de marché fait intervenir également d'autres facteurs comme la concurrence et l'aptitude à la survie des acteurs dans l'activité économique .Cette dynamique propre au marché représente un facteur expliquant la diffusion de la croissance économique et l'extension géographique des échanges dans un espace plus large, au-delà des frontières politiques des États.Pour Roger Guesnerie « À l'aune de l'esquisse qui est faite ici d'une économie de marché -des marchés appuyés sur la monnaie et le droit-, nombre d'économies historiquement datées ont droit au label d'économies de marché ».D'une manière générale, il serait plus exact de parler des économies de marchés plutôt que de l'économie de marché, tant le système est dépendant des contextes et institutions très diverses qui accompagnent et soutiennent les marchés.Dans cette perspective, la volonté de prendre en compte les aspects sociaux en Europe après la Seconde Guerre mondiale a conduit à l'émergence du concept d'économie dite « sociale de marché », qui a été décliné selon différentes variantes propres aux pays concernés.Aujourd'hui, l'importance croissante accordée à l'environnement peut laisser entrevoir une évolution vers une « économie durable de marché » voire une « économie sociale et durable de marché » .Certains auteurs posent clairement une distinction entre économie de marché et capitalisme.Pour Fernand Braudel, les régimes de production/répartition des biens et services ont évolué selon trois formes historiques successives : celle de la vie matérielle primitive où le processus d'auto-suffisance et d'auto-consommation se déroule de manière très locale, à l'échelle de l'individu, de la famille ou de petits groupes. Ici on produit pour se suffire, uniquement, à soi-même. L'échange et donc le marché n'existent pas.celle de l'économie de marché, telle qu'elle découle des échanges rendus nécessaires par une plus grande spécialisation et une plus large division du travail : chacun produit une catégorie spécifique de bien et doit fatalement échanger avec les autres pour se procurer les biens qu'il ne produit plus et ainsi satisfaire l'ensemble de ses besoins.celle du capitalisme, amorcée par les entreprises de « commerce ou de négoce au long cours » et qui se financiarise inéluctablement pour engendrer un système où l'échange commercial n'est plus que le support ou le prétexte de gains financiers. Pour lui, « le capitalisme dérive par excellence des activités économiques au sommet ou qui tendent vers le sommet. En conséquence, ce capitalisme de haut vol flotte sur la double épaisseur sous-jacente de la vie matérielle et de l'économie cohérente de marché, il représente la zone de haut profit ». D'une façon générale, Braudel distingue deux types d'échanges : « l'un terre à terre, concurrentiel puisque transparent » qui relève de l'économie de marché et « l'autre supérieur, sophistiqué, dominant » qui relève du capitalisme.Pour Robert Gilpin, l'essence du marché réside dans le rôle des prix relatifs dans le processus d'allocation des ressources tandis que celle du capitalisme réside dans la propriété privée des moyens de production. Au niveau théorique, une économie socialiste de marché composée d'acteurs publics et de travailleur non libres est pour lui concevable comme cela est envisagé dans le concept d'économie socialiste de marché.Issue d'un concept et d'une pratique liée à l'ordo-libéralisme, l'expression recouvre aujourd'hui un sens plus large. Ainsi, Mario Monti, le commissaire européen, distingue les économies de marché de type anglo-saxon des économies sociales de marché allemande ou française. Pour lui, l'économie de marché doit non seulement être compatible, mais aussi être en mesure de financer la protection sociale par une imposition redistributive, de même que promouvoir un certain volontarisme des gouvernements et des institutions européennes en faveur de l'économie dans le respect des règles européennes de la concurrence. En effet, dans l'optique libérale la concurrence entraîne la baisse des prix. Cela protège le pouvoir d'achat des individus et favorise l'innovation,. Le capitalisme, lui, encouragerait en réalité des comportements criminels, crapuleux et opportunistes .Il n'y a pas sur le plan théorique d'unanimité quant à la définition précise de l'économie de marché. On constate en revanche l'existence et la pratique de modèles les plus divers où le mécanisme d'économie de marché est amené à coexister et à composer avec des logiques ou contraintes plus ou moins compatibles.Le régime de la concurrence pure et parfaite n'étant pas concrétisé dans la réalité,il n'existe pas « un » marché général où se produit la confrontation de toutes les offres et de toutes les demandes, mais « des » marchés plus ou moins inter-connectés sinon cloisonnés qui donnent lieu à des confrontations partielles. (ex : marché des biens, des matières premières, des services, du travail, des changes, des capitaux, marché monétaire, marché immobilier, etc.)les imperfections de marché prospèrent :Monopole, duopole, oligopole, cartel, entente, position dominante, etc.asymétrie de l'information, délai et effet retard, goulet d'étranglement, etc.la demande exprimée sur un marché est perçue la plupart du temps à travers le prisme déformant de la demande solvable, c'est-à-dire celle qui émane des opérateurs disposant du pouvoir d'achat monétaire suffisant.Des actions collectives peuvent être organisées pour promouvoir ou défendre des valeurs positives ou des règles sociales, culturelles, morales, voire religieuses :l'économie publique ayant une place complémentaire (ex : les services publics), ou centrale (ex: le capitalisme d’État) ;l'économie sociale pour pallier les insuffisances ou déficiences du marché (ex : les mécanismes de protection sociale, d'assistance, de solidarité, d'État providence) ;l'interventionnisme d'État ou dirigisme via l'économie planifiée ou l'économie administrée ;le commerce équitable comme projet d'organisation visant à faire une meilleure place à certains producteurs en danger d'être marginalisés ou évincés du marché courant ;l'élaboration de normes - à caractère volontaire, incitatif ou obligatoire - peuvent contribuer à préciser ou encadrer la définition des pratiques de conception, de production ou de distribution des biens et services pour des motifs de protection de la qualité ou de la sécurité dues au consommateur/usager.l'économie dite non marchande ou domestique ne donne pas lieu à échange rémunéré(ex : jardins familiaux, babysitting non rémunéré, femmes au foyer, aide des grands-parents…).l'économie de troc et/ou l'économie de subsistance se situe relativement à l'écart des flux économiques (ex. : pratiques de certaines zones rurales du tiers monde ou très déshéritées)l'économie spéculative ou exclusivement financière qui introduisent des logiques de type « argent ? marchandise ? argent » dénoncées par certains comme représentant des déconnexions forcées de l'économie réelle. (ex. : spéculation sur les matières premières)l'économie souterraine ou les « trafics » opérés sur des marchés parallèles ou occultes (ex: le trafic de drogue, le travail ou le marché au noir, ou le proxénétisme…).les phénomènes de corruption ou de délit d'initié qui visent par leur nature à fausser le libre jeu des forces du marché.L'économie de marché s'arrête difficilement au niveau d'un seul pays, si vaste soit-il. Au niveau international, elle est d'autant plus développée que les divers pays pratiquent le libre-échange.Cela dit, en pratique beaucoup de pays revendiquent pour leurs exportations les règles applicables à l'économie de marché (sinon la clause de la nation la plus favorisée), en organisant par ailleurs vis-à-vis des importations des règles fort peu réciproques (protectionnisme) :Certains pays issus du collectivisme — comme la Chine — se veulent « économie socialiste de marché », alors qu'ils sont encore des économies marquées par l'empreinte du Capitalisme d'État.D'autres pays bénéficient de conditions de couts qui leur permettent de pratiquer une concurrence jugée « déloyale » par leurs rivaux plus développés.D'autres pays se trouvent dans une situation où la structure des flux échangés les entraine vers la détérioration des termes de l'échange.Un débat donne lieu à un fort questionnement assorti de multiples prises de position :Échanges entre socialistes « réformistes » et socialistes « fondamentalistes » Michel Rocard se targue souvent « d'avoir mis des décennies à apprendre l'économie de marché aux socialistes ».le tournant de la rigueur survenu en 1983 motive la gauche réformiste pour accepter de facto l'économie sociale de marché.L'économie de marché non régulée n'est pas forcément compatible avec les exigences du développement durable. En effet, la recherche de la maximisation du profit par les entreprises ne va pas spontanément dans le sens d'un développement durable, car elle conduit à des raisonnements de court terme (voire de spéculation), et elle tend à la satisfaction des intérêts des seuls actionnaires des entreprises.L'encyclique Caritas in Veritate de Benoît XVI (juillet 2009) indique que les acteurs de la vie économique ne peuvent se limiter au marché seul, mais « que l'économie doit aussi impliquer l'État et la société civile » :« La vie économique a sans aucun doute besoin du contrat pour réglementer les relations d’échange entre valeurs équivalentes. Mais elle a tout autant besoin de lois justes et de formes de redistribution guidées par la politique, ainsi que d’œuvres qui soient marquées par l’esprit du don. L’économie mondialisée semble privilégier la première logique, celle de l’échange contractuel mais, directement ou indirectement, elle montre qu’elle a aussi besoin des deux autres, de la logique politique et de la logique du don sans contrepartie. »« Mon prédécesseur Jean-Paul II avait signalé cette problématique quand, dans Centesimus annus, il avait relevé la nécessité d’un système impliquant trois sujets : le marché, l’État et la société civile. »— Encyclique Caritas in Veritate, chapitre III, § 37 et 38L'intervention de l'État, qui représente les « intérêts publics » (notion à définir), est considérée par certains comme nécessaire. Elle se fait actuellement de la façon suivante :Être exemplaire en matière de développement durable :Définir une stratégie nationale de développement durable,Mettre en œuvre cette stratégie en mettant en place des organisations dédiées au développement durable dans les ministères et les collectivités territoriales (ex. : ministère de l'environnement ou du développement durable),Lancer des actions concrètes comme le Grenelle de l'environnement.Encadrer le marché par l'application forcée des règles le concernant (concurrence, propriété privée, droit du travail, ...).Participer aux différentes initiatives qui ont lieu au niveau international sur le développement durable, sommets de la Terre, sommets de l'eau, protocole de Kyoto et ses suites, réunions sur la biodiversité…Définir de nouvelles règles du jeu :En France, les entreprises cotées en bourse doivent rendre compte des conséquences sociales et environnementales de leur activité (article 116 de la loi sur les Nouvelles Régulations Économiques),Principe du bonus-malus écologique pour les véhicules automobiles,Mise en place d'une finance du carbone.Toutefois, les évaluations portant sur la mise en œuvre des Nouvelles Régulations Économiques en France montrent qu'assez peu d'entreprises se conforment réellement aux exigences de la loi. En effet, le non-respect de la loi n'entraîne aucune sanction vis-à-vis des entreprises. Il s'agit d'un droit mou.On peut imaginer d'autres actions des États :Changer les règles de comptabilisation de la richesse. Par exemple, des études ont montré que le produit intérieur brut (PIB) ne prend pas en compte la diminution du capital naturel. L'INSEE a retenu le PIB comme indicateur de développement durable, alors qu'en réalité les effets à long terme de la croissance économique sur l'environnement ne sont pas pris en compte par le PIB. Il est clairement de la responsabilité des États de définir des instruments de mesure de la croissance économique, en l'occurrence des indices macroéconomiques, qui rendent compte efficacement de la conformité des agents économiques par rapport aux principes de développement durable (PIB vert).Mettre en place une fiscalité favorable aux produits durables (taxe carbone),Adapter l'enseignement,Sensibiliser la société civile en donnant le feu vert à tous les moyens permettant de montrer les dangers sur l'homme des activités qui menacent l'environnement et le développement durable.etc.La société civile intervient par l'intermédiaire de ses représentants, organisés en parties prenantes (organisations professionnelles, organisations syndicales, organisations non gouvernementales…). Par exemple, en matière environnementale, les parties prenantes représentatives sont les ONG (organisations non gouvernementales) (environnementales (WWF, Greenpeace, Amis de la Terre…).Les parties prenantes peuvent se concevoir par rapport aux autorités politiques, ou bien par rapport aux entreprises.Par rapport à la perspective catholique, les sociaux-libéraux s'interrogent sur la notion même d'État. La distinction société civile/État leur pose un problème car elle suppose, à la manière de ce qui existe dans l'Église, une prépondérance donnée à la hiérarchie de l'État sur les citoyens.L'Organisation mondiale du commerce (OMC) octroie le « statut d’économie de marché » (SEM) aux États. Un pays qui importe des produits depuis un pays qui n'en bénéficie pas est autorisé à ne pas tenir compte du prix pratiqué sur le marché intérieur de l’État exportateur.La Chine s'est ainsi vue attribuer ce statut en 2016, conformément à l'accord convenu lors de son adhésion en 2001,. Avant même cette décision de l'OMC, plus de 80 pays dans le monde avaient reconnu le statut d'économie de marché à la Chine. Cependant, les États-Unis s'y opposent,. De son côté, l'Union européenne a mis en place une nouvelle méthodologie anti-dumping qui ne cible plus spécifiquement la Chine : Jean Quatremer estime ainsi qu'« en clair, l’Union va continuer à considérer que la Chine n’est pas un pays à économie de marché, mais sans le proclamer et en évitant les foudres de l’OMC »,. D'autre part, un rapport détaillé de la Commission européenne émettait des doutes en décembre 2017 sur la nature d'« économie de marché » de l'« économie socialiste de marché » de la Chine,.Robert Gilpin, 1987, The Political Economy of International Relation, Princeton University Press.Fernand Braudel, 1985, La Dynamique du capitalisme, Paris, Arthaud, 1985  (ISBN 2080811924)Roger Guesnerie 2006, L'Économie de marché, Le Pommier.John Kenneth Galbraith, Et le système fut rebaptisé. dans Les Mensonges de l'économie, traduction française de Paul Chemla, Paris, Grasset, 2004Michel Lafitte, 2007, Développement durable et économie de marché  (ISBN 2863254782) Portail du libéralisme   Portail de l’économie"
économie;La conjoncture est la situation économique d’un pays à un moment donné.Le terme de conjoncture fait référence aux évolutions économiques de court terme d’un ensemble économique, en général un pays. Elle s’apprécie à l'aide d'indicateurs économiques tels que le taux de croissance du PIB, le taux d'inflation, l’évolution du taux de chômage, la balance commerciale, etc.La conjoncture est liée aux cycles économiques, et ses durées types des changements conjoncturels vont de quelques mois à quelques années. La conjoncture est étudiée au niveau d'un pays, d'une région, ou, à un niveau plus fin, au niveau d'un secteur économique particulier. On peut parler alors de la conjoncture économique d'une entreprise.Le gouvernement et la banque centrale d'un pays tentent généralement de limiter l'influence néfaste des trop grandes variations de conjoncture économique sur l'activité économique au moyen de politiques conjoncturelles.Par exemple, en période de « surchauffe économique », la banque centrale peut augmenter ses taux directeurs pour éviter les bulles spéculatives, l’accélération de l'inflation et le sur-investissement, c'est-à-dire les investissements dans des projets non-rentables ou l'accumulation de capacités excédentaires de production dans certains secteurs économiques. À l'inverse quand se présente un risque de récession économique elle peut abaisser ses taux, dans la mesure toutefois où cela ne risque pas de provoquer des tensions inflationnistes.Inversement, le gouvernement et la banque centrale encourageront l'activité dans les périodes de creux économique.ÉconomieCycle économique Portail de l’économie
économie;"La croissance économique désigne la variation positive de la production de biens et de services dans une économie sur une période donnée, généralement une longue période. En pratique, l'indicateur le plus utilisé pour la mesurer est le produit intérieur brut (PIB). Il est mesuré « en volume » ou « à prix constants » pour corriger les effets de l'inflation. Le taux de croissance, lui, est le taux de variation du PIB. On utilise souvent la croissance du PIB par habitant comme indication de l'augmentation de la richesse individuelle, assimilée au niveau de vie (à distinguer de la qualité de vie).La croissance est un processus fondamental des économies contemporaines, reposant sur le développement des facteurs de production, lié notamment à la révolution industrielle, à l'accès à de nouvelles ressources minérales (mines profondes) et énergétiques (charbon, pétrole, gaz, énergie nucléaire, etc.) ainsi qu'au progrès technique. Elle transforme la vie des populations dans la mesure où elle crée davantage de biens et de services. À long terme, la croissance a un impact important sur la démographie et le niveau de vie des sociétés qui en sont le cadre. De même, l'enrichissement qui résulte de la croissance économique peut permettre de faire reculer la pauvreté, à condition que les richesses créées soient redistribuées vers les plus bas revenus.Certaines conséquences de la croissance économique comme la pollution et les atteintes à l'environnement, l'accentuation des inégalités sociales ou l'épuisement des ressources naturelles (pétrole, métaux notamment) sont souvent considérés comme des effets pervers qui obligent à distinguer croissance et progrès.Le rapport commandé en 1970 par le Club de Rome à une équipe du Massachusetts Institute of Technology (MIT), intitulé The Limits To Growth met en évidence l'impossibilité d'une croissance illimitée dans un monde fini. Les tenants de la décroissance estiment que la poursuite de la croissance amènerait inévitablement à un effondrement de la civilisation.Les économistes utilisent le terme de croissance conventionnellement pour décrire une augmentation de la production sur le long terme (une durée supérieure à un an). Selon la définition de François Perroux, la croissance économique correspond à « l'augmentation soutenue pendant une ou plusieurs périodes longues d’un indicateur de dimension, pour une nation, le produit global net en termes réels ». La définition de Simon Kuznets va au-delà et affirme qu'il y a croissance lorsque la croissance du PIB est supérieure à la croissance de la population[réf. nécessaire].À court terme, les économistes utilisent plutôt le terme d'« expansion », qui s'oppose à « récession », et qui indique une phase de croissance dans un cycle économique. La croissance potentielle est le niveau de croissance qui serait obtenu avec une pleine utilisation de tous les facteurs de production (travail, capital et savoir) ; l'écart entre la croissance effective (celle effectivement mesurée) et la croissance potentielle est minimal au plus fort d'une expansion.Au sens strict, la croissance décrit un processus d'accroissement de la seule production économique. Elle ne renvoie donc pas directement à l'ensemble des mutations économiques et sociales propres à une économie en développement. Ces transformations au sens large sont, conventionnellement, désignées par le terme de développement économique. Selon François Perroux, « le développement est la combinaison des changements mentaux et sociaux d'une population qui la rend apte à faire croître, cumulativement et durablement, son produit réel global ». Le terme de « croissance » s'applique alors plus particulièrement aux économies déjà développées.La Commission du développement durable (en) du gouvernement britannique souligne qu'il est important de distinguer trois notions qui « ne sont absolument pas les mêmes choses » :la croissance des flux biophysiques (énergie et matériaux) ;la croissance de la valeur monétaire de la production (PIB) ;la croissance du bien-être économique de la population.Le croissantisme économique est considéré comme étant l'idéologie de la croissance par opposition à la philosophie décroissantiste.La croissance économique est généralement mesurée par l'utilisation d'indicateurs économiques dont le plus courant est le produit intérieur brut (PIB). Il offre une certaine mesure quantitative du volume de la production. Afin d'effectuer des comparaisons internationales, on utilise également la parité de pouvoir d'achat, qui permet d'exprimer le pouvoir d'achat dans une monnaie de référence. Au niveau international, cette monnaie de référence est le dollar américain. Pour comparer la situation d'un pays à des époques différentes on peut également raisonner à monnaie constante en se référant aux prix d'une date antérieure appelée date de référence (afin d'eviter les effets de l'inflation).L'indicateur du PIB reste cependant imparfait comme mesure de la croissance économique. Il est pour cela l'objet de plusieurs critiques :le PIB ne mesure que partiellement l'économie informelle. Une part importante des transactions, non déclarée, était perdue pour les statistiques comme pour l'administration fiscale. Mais en 2014, plusieurs pays (l'Italie, le Royaume-Uni, l'Espagne et la Belgique) ont décidé d'intégrer dans leur PIB des estimations de l'économie souterraine (drogue, prostitution, trafics divers) en application des nouvelles normes comptables européennes publiées par Eurostat ; les États-Unis l'avaient déjà fait en 2013 ; la comptabilité nationale française, qui effectuait déjà des redressements pour prendre en compte les activités dissimulées (travail au noir, contrebande), a décidé d'intégrer des estimations du trafic de drogue, mais pas de la prostitution clandestine ;le PIB ne mesure que de façon imparfaite les productions qui ne sont pas commercialisées : ainsi, la production des administrations est supposée égale aux salaires des fonctionnaires ; une évaluation des productions agricoles auto-consommées est intégrée au PIB. D'autre part, même s'il prend en compte la production des activités non marchandes, le PIB ne mesure pas l'activité de production domestique (ménage, cuisine, bricolage, éducation des enfants, etc.), des activités majoritairement réalisées par des femmes,. En 2009, la commission Stiglitz estime que la valeur du travail domestique équivaut à 35 % du PIB en France sur la période 1995-2006 ;le PIB ne mesure que les apports de valeur ajoutée dans l'immédiat (sur une année). Les effets de long terme, notamment dans des services tels que l'éducation ou la santé, ne sont pas ou mal comptabilisés à travers leur impact sur la production ;le PIB ne mesure que la valeur ajoutée produite par les agents économiques résidents. Il ne prend donc pas en compte les transferts de ressources internationaux (les sorties de fonds du pays par les résidents étrangers vers leurs pays d'origine et les entrées en provenance de l'étranger qui correspondent aux envois de fonds vers leur pays d'origine par les résidents nationaux à l'étranger), alors que ces derniers représentent souvent une part importante de leur richesse nationale. Il est possible d'utiliser un outil plus pertinent tel que le revenu national brut ;enfin, le PIB ne prend en compte que les valeurs ajoutées, et non la richesse possédée par un pays, sans distinguer les effets positifs ou négatifs sur le bien-être collectif. Une catastrophe naturelle (l'ouragan Katrina détruisant La Nouvelle-Orléans, par exemple), qui détruit de la richesse, va pourtant contribuer au PIB à travers l'activité de reconstruction qu'elle va générer. Cette contribution ne reflète pas la destruction antérieure, ni le coût du financement de la reconstruction. Cette contradiction était dénoncée dès 1850 par l'économiste français Frédéric Bastiat qui, dans son Sophisme de la vitre cassée, écrivait que « la société perd la valeur des objets inutilement détruits », ce qu'il résumait par : « destruction n'est pas profit ».Cette contradiction apparente provient probablement du fait que le PIB ne mesure pas réellement le développement, le progrès en lui-même ; il ne mesure pas non plus l'activité économique, pourvoyeuse d'emploi, car l'activité peut fort bien croître sans augmentation de valeur ajoutée, si l'on remplace du capital ou des matières premières par du travail. Dans ce cas, la croissance économique tiendrait compte aussi bien de la production formelle que celle informelle (cf. supra). Mais pour parvenir à cet objectif, tant la comptabilité des entreprises que la comptabilité nationale doivent complètement quitter le critère de mesure par la valeur ajoutée (i.e. production vendue moins consommations intermédiaires) et adopter celui du travail. La croissance ne mesure en fait que l'augmentation de la consommation de facteurs de production : travail, capital et ressources naturelles (matières premières, potentiel productif des terres agricoles, énergie...). La société peut progresser[pas clair] sans croissance, en modifiant la répartition des facteurs.Dans son acception classique, le développement économique ne se résume pas à la seule croissance économique et des indicateurs ont été proposés pour mesurer plus finement celui-ci, comme l'indice de développement humain, mis au point par l'économiste du développement Amartya Sen, prix Nobel d'économie.Dans un certain nombre de cas, les données de la comptabilité nationale ne sont pas disponibles ou sont de mauvaise qualité. C'est notamment un problème lorsqu'on s'intéresse à des périodes anciennes, à des pays en voie de développement avec une mauvaise comptabilité nationale ou encore lorsqu'on s'intéresse au développement économique à un niveau infra-national, par exemple au niveau d'une ville ou d'une région. Dans ce cas, plusieurs indicateurs ont été proposés.Les économistes Daron Acemoglu, Simon Johnson et James Robinson utilisent des données sur l'urbanisation et sur la densité de population pour approximer le degré de développement en l'an 1500.Les économistes David Weil, Vernon Henderson et Adam Storeygard (2011) proposent d'utiliser des images satellites pour mesurer l'augmentation de l'intensité lumineuse de nuit et utiliser cette donnée pour estimer la croissance économique.Grâce au développement des statistiques nationales, les économistes, les historiens et les démographes ont constaté qu'avant la Révolution industrielle, la croissance économique est essentiellement liée à celle de la population : on produit plus parce qu’il y a plus d'individus pour produire, mais le niveau de vie reste le même. Cela peut s'expliquer par le fait que la croissance démographique est strictement supérieure à la croissance économique (cf. graphe). Au sens du prix Nobel d'économie, l'américain Simon Kuzsnets, à ce stade il n'y a pas de croissance économique. Pour lui, la croissance n'est possible que lorsque le taux de ce dernier est strictement supérieur à celui de la croissance démographique.À partir du XVIIIe siècle, la croissance économique se déconnecte de celle de la population (et devient supérieure à celle-là, après qu'elle lui était égale sur plusieurs décennies) et l’augmentation du niveau de vie devient exponentielle, mais très irrégulière. Cela s'explique essentiellement par le progrès technique (amélioration des facteurs de production, capital et  travail, notamment) devenu plus important depuis la révolution industrielle (renouvellement colossal à la fois économique, culturel et social) de la fin du dix-huitième siècle. Après les très forte croissance mondiale des années 1830 et croissance mondiale des années 1850, la Grande Dépression (1873-1896) donne un sérieux coup de frein. De même, la grande dépression des années 1930 fait suite à la croissance économique de la Belle Époque et à la puissante expansion des années 1920. Plus généralement, les périodes de reconstruction suivant une guerre sont favorables, comme lors de la très forte croissance des années 1950, socle des Trente Glorieuses (1945-1973).Les historiens[Qui ?] s’accordent sur le fait que le niveau de vie sur l’ensemble du globe a peu évolué de l’Antiquité jusqu’au XVIIIe siècle (entre l'an 1 et l'an 1000 l'économie mondiale aurait même décliné), mis à part une embellie en Europe occidentale entre les Xe et XIIIe siècles, annulée par les épidémies et les famines des XIVe et XVe siècles. Ils s'accordent aussi à constater qu'il y a de grandes disparités selon les peuples et selon les époques. Sachant qu'on a affaire à des sociétés où presque toute la population est rurale, il est de toutes façons presque impossible d'obtenir la statistique de leur production, puisque celle-ci est presque complètement locale, voire familiale (bâtiment, mobilier, confection, alimentation, services…), et très marginalement commerciale, de telle sorte qu'il est impossible de reconstituer un standard moyen de consommation et de l'évaluer en monnaie.Selon l'économiste français Thomas Piketty, du XVIIIe siècle au XXe siècle, « la production mondiale a progressé en moyenne de 1,6 % par an, dont 0,8 % par an au titre de la population et 0,8 % au titre de la production par habitant », soit « un rythme très rapide, dès lors qu'il se prolonge durablement ».La croissance économique, aussi bien comme phénomène que comme donnée objectivable, est donc quelque chose de récent, lié à l'urbanisation (création des villes et déplacement de la population vers celles-là) des sociétés et à l'apparition de statistiques nationales. Selon la théorie du déversement, l'industrialisation provoque un déplacement des activités des campagnes vers les villes, les effectifs suivent. Jusqu'aux années 1970, c'était aussi un phénomène géographiquement limité, qui concernait surtout les pays occidentaux et le Japon[réf. nécessaire]. Même avec l'ouverture du commerce International à de nouveaux entrants et notamment à la Chine (Doha, 2002), la part de la croissance des pays de la triade (l'Union européenne, les États-Unis et le Japon) dans la croissance mondiale est encore la plus importante jusqu'à présent.Les Pays-Bas sont la première société à connaître un phénomène de croissance, au XVIIe siècle. Comme le note Henri Lepage en reprenant les analyses de Douglass North, « pour la première fois dans l'histoire connue de l'humanité, un pays se trouvait en mesure d'offrir un niveau de vie croissant à une population croissante, et cela un siècle avant que se manifestent les premiers signes réels de la Révolution industrielle ».Le phénomène s'est ensuite progressivement étendu. La phase de développement économique depuis la Révolution industrielle n'a aucun précédent historique. Après le XVIe siècle, lorsque différentes parties du monde développent des relations commerciales, on constate des périodes de croissance économique, mais éphémères et marginales. Les écarts entre conditions de VIe au XVIIIe siècle étaient réduits, pour certains auteurs comme Paul Bairoch : l'Inde possédait même un niveau de vie supérieur à l'Europe. On estime que la croissance globale de l'économie entre 1500 et 1820 n'est que d'un trentième de ce qu'elle a été depuis (de 247 milliards de dollars internationaux en 1500 à 695 en 1820, puis 33 725 en 1998). Les revenus en Europe ont été multipliés par 20 entre 1820 et les années 1990. L'Asie accélère aussi son rythme de croissance depuis un demi-siècle[Quand ?] : le niveau de vie en Chine a été multiplié par six et celui du Japon par huit[réf. nécessaire].Cependant, au XIXe siècle le développement économique entraîne des bouleversements sociaux comme l'exode rural (cf. supra, théorie du déversement). Le niveau de vie et le développement n'ayant commencé à être étudiés rigoureusement qu'au XIXe siècle, il est cependant difficile, faute de données, de faire une comparaison entre le XVIIIe et le XIXe siècle[réf. nécessaire].En 1913, le PIB/hab français était de 3 485 dollars internationaux (base 1990). En 1998, il était de 19 558 $. Le taux de croissance moyen du PIB/hab était donc de 2,0 % sur cette période. S'il avait été de 1,0 %, le niveau de vie aurait été de 8 200 $ en 1998, soit un peu moins que le niveau de vie réel de l'Uruguay (8 314 $).L'évolution en pourcentage du PIB en volume d'une année à l'autre. Les données sont mesurées en monnaie constante de 2005 d’après les données de l’OCDE (organisation du commerce et du développement en Europe).Source : Banque Mondiale.Selon Jean-Marie Albertini, si dans les économies capitalistes la croissance est réalisée en recourant au progrès technique et l'exploitation du travail devenu libre grâce à la séparation du travailleur de ses moyens de production qui a donné lieu à la propriété privée de ces moyens, elle est le résultat de la prépondérance du facteur politique sur le facteur économique dans les sociétés socialistes. La propriété collective des terres agricoles à permis de réaliser une production agricole à bas prix. Les produits alimentaires obtenus sont vendus dans les villes à des prix plus élevés (les revenus de la population urbaine étaient plus élevés que dans les compagnes). Les profils obtenus permettent de financer l'industrie lourde et donc de réaliser la croissance.Dans Le Capital au XXIe siècle, Thomas Piketty fait l'hypothèse que la période de forte croissance économique est terminée (depuis la fin des trente glorieuses et le début de la crise en 1973) et qu'il y a toutes les raisons de penser que la croissance devrait revenir à un niveau plus faible dans un régime stationnaire.Dans The Rise and Fall of American Growth (2016), l'économiste Robert J. Gordon défend la thèse que la forte croissance aux États-Unis et dans les pays développés entre 1870 et 1970 a été une exception et que les innovations qui ont eu lieu depuis 1970 génèrent moins de croissance que par le passé.En 2021, le quotidien français Le Monde observe que la croissance fait l'objet, en France, d'un « nouveau clivage politique ». Frédéric Dabi, directeur général de l’Institut français d’opinion publique (IFOP), indique : « En 2021, la moitié des Français se disent favorables à plus de croissance et à ce qu’une priorité absolue soit donnée à la création d’emplois, tandis que l’autre moitié défend un autre modèle de développement ayant pour objectif la préservation des ressources. En 2017, les premiers étaient majoritaires ».On peut distinguer plusieurs types de déterminants à la croissance : les richesses naturelles, l'environnement extérieur, la population, l'innovation au sens de Joseph A. Schumpeter (concept qui ne concerne pas seulement le progrès technique), l'investissement, la connaissance, la cohérence du développement. Les principales conclusions des travaux de Xavier Sala-i-Martin, économiste espagnol spécialiste de la croissance, confirment qu'il n'y a pas qu'un seul déterminant simple de la croissance économique.Xavier Sala-i-Martin avance que le niveau initial est la variable la plus importante et la plus robuste. C'est-à-dire que, dans la plupart des cas, plus un pays est riche, moins il croît vite. Cette hypothèse est connue sous le nom de convergence conditionnelle. Il considère également que la taille du gouvernement (administration, secteur public) n'a que peu d'importance. Par contre la qualité du gouvernement a beaucoup d'importance : les gouvernements qui causent l'hyperinflation (taux d'inflation extrêmement élevé), la distorsion des taux de change, des déficits excessifs (ceux de la balance des paiements et du budget de l'État) ou une bureaucratie inefficace ont de très mauvais résultats. Il ajoute également que les économies plus ouvertes tendent à croître plus vite (cf. croissance de la Chine depuis 2002). Enfin, l'efficience des institutions est très importante : des marchés efficients, la reconnaissance de la propriété privée et l'état de droit sont essentiels à la croissance économique. Il rejoint en cela les conclusions d'Hernando de Soto,.Sur une plus longue période, l'expérience historique[réf. nécessaire], notamment celle du XVIIIe siècle, suggère que l'extension des libertés économiques (liberté d'entreprendre, de circulation des idées, des personnes et des biens) est une condition de la croissance. En occident, la culture, la religion et la société ont connu des changements très profonds. Tout d'abord, le conformisme de l'homme précapitaliste a été vaincu par une volonté de rendre l'environnement dépendant de l'homme plutôt que de lui être dépendant. Ensuite, le puritanisme et le calvinisme (qui est une variante principale du premier courant) ont fait du travail une vertu divine et considèrent la misère comme un péché. Enfin, les restrictions sociales sont vigoureusement combattues : les vielles corporations qui constituent l'une des stratégies fondamentales du mercantilisme et qui limitent la liberté des hommes (et l'innovation) sont levées et aussi bien le "" laisser-faire "" que le "" laisser-passer "" sont devenus des moyens au service de plus de croissance économique. Au XXe siècle, il existe plusieurs cas où une population partageant les mêmes antécédents historiques, la même langue et les mêmes normes culturelles a été divisée entre deux systèmes, l'un étant une économie de marché et l'autre une économie planifiée : les deux Allemagne, les deux Corée, la république populaire de Chine et Taïwan. Dans chaque cas, les zones ayant pratiqué l'économie de marché ont obtenu une croissance nettement supérieure sur le long terme. Cependant, l'enrichissement de l'Allemagne de l'Ouest s'explique par l'aide des États-Unis, l'enrichissement de la Corée du Sud et de Taïwan par l'aide des États-Unis et du Japon et que Taïwan a attiré les Chinois les plus qualifiés. Les États-Unis et l'Europe de l'Ouest étant beaucoup plus développés que l'URSS, leurs pays alliés ont été beaucoup plus aidés. La très forte croissance de l'URSS avant les années 1960 et la très forte croissance de la Chine depuis les années 1980 sont des exemples de pays dont l'économie planifiée a augmenté la croissance. Aucun pays n'a eu une croissance telle que celle de la Chine et l'URSS sans bénéficier d'aide extérieure ou d'une exploitation massive de ressources naturelles très lucratives, telles le pétrole, par rapport au nombre d'habitants. L'effondrement de l'URSS témoigne également des meilleurs résultats des économies de marché par rapport aux économies de type collectiviste[Interprétation personnelle ?].Sur le très long terme, Angus Maddison identifie trois processus interdépendants qui ont permis l'augmentation conjointe de la population et du revenu : la conquête ou la colonisation d'espaces fertiles et relativement peu peuplés, le commerce international et les mouvements de capitaux, et enfin l'innovation technologique et institutionnelle.Quant à Daron Acemoglu, dans An Introduction to Modern Economic Growth (2008), il distingue quatre causes fondamentales de la croissance : l'environnement naturel, la culture, les institutions et la chance.Une étude empirique publiée en 2010 affirme avoir établi un lien entre un manque de croissance économique et la consanguinité.Pour Jean-Marie Albertini, maître de recherche au CNRS français, les déterminants de la croissance économique, sont, au moins, au nombre de trois. Le chercheur, précise, cependant, que ces paramètres ne sont pas les mêmes selon qu'il s'agisse d'une économie capitaliste développée ou d'un régime socialiste. Dans les sociétés industrialisées modernes, la croissance est le fruit, à la fois, de l'exploitation du travail des femmes et des enfants à la fin de la révolution industrielle de 1789 et du progrès technique d'une part, et de l'exploitation des pays colonisés de l'autre. Dans une société marxiste, la croissance économique est la conséquence d'une socialisation des moyens de production. La production collective des terres agricoles a permis au régime stalinien en URSS de réaliser une production agricole à bas prix, de vendre, ensuite, le produit ainsi obtenu à la population urbaine à prix élevé. Les bénéfices obtenus permettent de financer la croissance (production militaire, construction de logements sociaux, de barrages d'irrigation, ...).Les théories explicatives de la croissance sont relativement récentes dans l'histoire de la pensée économique. Ces théories, sans négliger le rôle de l'ensemble des facteurs de production tendent à mettre en avant parmi ceux-ci le rôle primordial du progrès technique dans la croissance. Sur le long terme, seul le progrès technique est capable de rendre plus productive une économie (et donc de lui permettre de produire plus, c'est-à-dire d'avoir de la croissance)[réf. nécessaire]. Toutefois, ces théories expliquent encore mal d'où provient ce progrès, et en particulier en quoi il est lié au fonctionnement de l'économie.La plupart des économistes de l'école classique, écrivant pourtant au commencement de la révolution industrielle, pensaient qu'aucune croissance ne pouvait être durable, car toute production devait, selon eux, inexorablement converger vers un état stationnaire. C'est ainsi le cas de David Ricardo pour qui l'état stationnaire était le produit des rendements décroissants des terres cultivables, ou encore pour Thomas Malthus qui le liait à son « principe de population », mais aussi pour John Stuart Mill.Toutefois, Adam Smith, à travers son étude des effets de productivité induits par le développement de la division du travail, laissait entrevoir la possibilité d'une croissance ininterrompue. Et Jean-Baptiste Say écrivait « Remarquez en outre qu’il est impossible d’assigner une limite à la puissance qui résulte pour l’homme de la faculté de former des capitaux ; car les capitaux qu’il peut amasser avec le temps, l’épargne et son industrie, n’ont point de bornes. ».Nikolai Kondratiev est un des premiers économistes à montrer l'existence de cycles longs de 50 ans, et Joseph Schumpeter développe la première théorie de la croissance sur une longue période. Il considère que l'innovation portée par les entrepreneurs constitue la force motrice de la croissance. Il étudie en particulier le rôle de l'entrepreneur dans Théorie de l'évolution économique en 1913.Pour Schumpeter, les innovations apparaissent par « grappes », ce qui explique la cyclicité de la croissance économique. Par exemple, Schumpeter retient les transformations du textile et l'introduction de la machine à vapeur pour expliquer le développement des années 1798-1815, ou le chemin de fer et la métallurgie pour l'expansion de la période 1848-1873. De façon générale il retient trois types de cycles économiques pour expliquer les variations de la croissance :les cycles longs ou cycles Kondratieff, d'une durée de cinquante ans ;les cycles intermédiaires ou cycles Juglar, d'une durée de dix ans environ ;les cycles courts ou cycles Kitchin, d'une durée de quarante mois environ.Schumpeter introduit enfin le concept de « destruction créatrice » pour décrire le processus par lequel une économie voit se substituer à un modèle productif ancien un nouveau modèle fondé sur des innovations. Il écrit ainsi :« L'impulsion fondamentale qui met et maintient en mouvement la machine capitaliste est imprimée par les nouveaux objets de consommation, les nouvelles méthodes de production et de transport, les nouveaux marchés, les nouveaux types d'organisation industrielle - tous éléments créés par l'initiative capitaliste. […] L'ouverture de nouveaux marchés nationaux ou extérieurs et le développement des organisations productives, depuis l'atelier artisanal et la manufacture jusqu'aux entreprises amalgamées telles que l’US Steel, constituent d'autres exemples du même processus de mutation industrielle — si l'on me passe cette expression biologique — qui révolutionne incessamment de l'intérieur la structure économique, en détruisant continuellement ses éléments vieillis et en créant continuellement des éléments neufs. Ce processus de destruction créatrice constitue la donnée fondamentale du capitalisme : c'est en elle que consiste, en dernière analyse, le capitalisme et toute entreprise capitaliste doit, bon gré mal gré, s'y adapter. »Après la Seconde Guerre mondiale, les économistes Harrod et Domar, influencés par Keynes, cherchent à comprendre les conditions dans lesquelles une phase d'expansion peut être durable. Ainsi, s'il ne propose pas à proprement parler une théorie de la croissance (expliquant son origine sur une longue période), le modèle de Harrod-Domar permet, néanmoins, de faire ressortir le caractère fortement instable de tout processus d'expansion. En particulier, il montre que pour qu'une croissance soit équilibrée — c'est-à-dire que l'offre de production augmente ni moins (sous-production) ni plus (surproduction) que la demande —, il faut qu'elle respecte un taux précis, fonction de l'épargne et du coefficient de capital (quantité de capital utilisée pour produire une unité) de l'économie. Or, il n'y a aucune raison que la croissance, qui dépend de décisions individuelles (en particulier des projets d'investissement des entrepreneurs), respecte ce taux. De plus, si la croissance est inférieure à ce taux, elle va avoir tendance non pas à le rejoindre, mais à s'en éloigner davantage, diminuant progressivement (en raison du multiplicateur d'investissement). La croissance est donc, selon une expression d'Harrod, toujours « sur le fil du rasoir »[réf. nécessaire].Ce modèle, construit après guerre et marqué par le pessimisme engendré par la crise de 1929, a toutefois été fortement critiqué. Il suppose, en effet, que ni le taux d'épargne, ni le coefficient de capital ne sont variables à court terme, ce qui n'est pas prouvé[réf. nécessaire].Robert Solow propose un modèle néoclassique de croissance. Ce modèle repose essentiellement sur l'hypothèse d'une productivité marginale décroissante du capital dans la fonction de production. Le modèle est dit néoclassique au sens où les facteurs de production sont utilisés de manière efficace et rémunérés à leur productivité marginale. Solow montre que cette économie tend vers un état stationnaire. Dans ce modèle, la croissance de long terme ne peut provenir que du progrès technique (et non plus de l'accumulation du capital).Si on pense que tous les pays convergent vers le même état stationnaire, alors le modèle de Solow prédit un phénomène de convergence : les pays pauvres devraient croître plus vite que les pays riches.L'une des faiblesses théoriques du modèle de Solow vient du fait qu'il considère le progrès technique comme exogène. Autrement dit, il ne dit rien sur la façon dont le progrès technique apparaît.Les théories de la croissance endogène cherchent à endogénéiser le progrès technique, c'est-à-dire à construire des modèles qui expliquent son apparition. Ces modèles ont été développés à partir de la fin des années 1970 notamment par Paul Romer, Robert E. Lucas et Robert Barro. Ils se fondent sur l'hypothèse que la croissance génère par elle-même le progrès technique. Ainsi, il n'y a plus de fatalité des rendements décroissants : la croissance engendre un progrès technique qui permet que ces rendements demeurent constants. Si tel est le cas, la croissance n'a donc plus de limite. À travers le progrès technique, la croissance constitue un processus qui s'auto-entretient.Ces modèles expliquent que la croissance engendre du progrès technique par trois grands mécanismes. Premièrement, le learning by doing : plus on produit, plus on apprend à produire de manière efficace. En produisant, on acquiert en particulier de l'expérience, qui accroît la productivité. Deuxièmement, la croissance favorise l'accumulation du capital humain, c'est-à-dire les compétences possédées par la main d'œuvre et dont dépend sa productivité. En effet, plus la croissance est forte, plus il est possible d'accroître le niveau d'instruction de la main-d'œuvre, en investissant notamment dans le système éducatif. D’une manière générale, la hausse du niveau d'éducation de la population – par des moyens publics ou privés – est bénéfique. Troisièmement, la croissance permet de financer des infrastructures (publiques ou privées) qui la stimulent. La création de réseaux de communication efficaces favorisent, par exemple, l'activité productive.« La principale [des] conclusions [de ces nouvelles théories] est qu'alors même qu'[elles] donnent un poids important aux mécanismes de marché, elles en indiquent nettement les limites. Ainsi il y a souvent nécessité de créer des arrangements en dehors du marché concurrentiel, ce qui peut impliquer une intervention active de l'État dans la sphère économique. »En particulier ce « retour de l'État » se traduit par le fait qu'il est investi d'un triple rôle : encourager les innovations en créant un cadre apte à coordonner les externalités qui découlent de toute innovation (par exemple"
médecine;"La cellule — du latin cellula « chambre de moine » — est l'unité biologique structurelle et fonctionnelle fondamentale de tous les êtres vivants connus. C'est la plus petite unité vivante capable de se reproduire de façon autonome. La science qui étudie les cellules est appelée biologie cellulaire.Une cellule est constituée d'une membrane plasmique contenant un cytoplasme, lequel est formé d'une solution aqueuse (cytosol) dans laquelle se trouvent de nombreuses biomolécules telles que des protéines et des acides nucléiques, organisées ou non dans le cadre d'organites. De nombreux êtres vivants ne sont constitués que d'une seule cellule : ce sont les organismes unicellulaires, comme les bactéries, les archées et la plupart des protistes. D'autres sont constitués de plusieurs cellules : ce sont les organismes multicellulaires, comme les plantes et les animaux. Ces derniers contiennent un nombre de cellules très variable d'une espèce à l'autre ; le corps humain en compte ainsi de l'ordre de cent mille milliards (1014), mais est colonisé par un nombre de un à dix fois plus grand de bactéries, qui font partie de son microbiote et sont bien plus petites que les cellules humaines. La plupart des cellules des plantes et des animaux ne sont visibles qu'au microscope, avec un diamètre compris entre 10  et   100 µm.L'existence des cellules a été découverte en 1665 par le naturaliste anglais Robert Hooke. La théorie cellulaire a été formulée pour la première fois en 1839 par le botaniste allemand Matthias Jakob Schleiden et l'histologiste allemand Theodor Schwann : elle expose que tous les êtres vivants sont constitués d'une ou plusieurs cellules, que les cellules sont les unités fondamentales de toutes les structures biologiques, qu'elles dérivent toujours d'autres cellules préexistantes, et qu'elles contiennent l'information génétique nécessaire à leur fonctionnement ainsi qu'à la transmission de l'hérédité aux générations de cellules suivantes. Les premières cellules sont apparues sur Terre il y a au moins 3,7 milliards d'années, et peut-être dès 4 Ga.Le nom de « cellule » est dû à son découvreur Robert Hooke, qui leur a donné le nom latin cellula en référence aux petites chambres occupées par les moines dans les monastères. Cellula est dérivé de cella, qui désignait en latin une pièce, ou un cellier — qui vient de son dérivé cellarium (« garde-manger »).Cella est issu de l’indo-européen commun *k?el (« couvrir ») dont proviennent aussi, indirectement, le français celer (« cacher ») ou l'anglais hell (« monde souterrain, enfer »)...On considère généralement deux types fondamentaux de cellules selon qu'elles possèdent ou non un noyau enveloppé d'une membrane nucléaire :les procaryotes, dont l'ADN est libre dans le cytoplasme, comprenant les bactéries et les archées, et qui, bien qu'ils puissent former des colonies très nombreuses, sont toujours des organismes unicellulaires ;les eucaryotes, qui ont une organisation interne complexe, de nombreux organites, et dont le noyau est délimité par une membrane nucléaire. Les eucaryotes comprennent un grand nombre de formes d'organismes unicellulaires et organismes multicellulaires.Les procaryotes sont la première forme de vie apparue sur Terre, définie comme étant autosuffisante et pourvue de tous les processus biologiques vitaux, y compris les mécanismes de signalisation cellulaire. Plus petites et plus simples que les cellules d'eucaryotes, les cellules de procaryotes sont dépourvues de système endomembranaire et des organites qui le constituent, à commencer par le noyau. Les bactéries et les archées sont les deux domaines du vivant regroupant les procaryotes. L'ADN d'un procaryote forme un unique chromosome en contact direct avec le cytoplasme. La région nucléaire du cytoplasme est appelée nucléoïde et n'est pas nettement séparée du reste de la cellule. La plupart des procaryotes sont les plus petits des êtres vivants connus, avec un diamètre compris entre 0,5 et 2 µm.Une cellule de procaryote contient trois régions distinctes :l'extérieur de la cellule porte des flagelles et des pili qui sortent de la surface cellulaire chez un grand nombre de procaryotes. Ces structures permettent à ces cellules de se déplacer (motilité cellulaire) et de communiquer entre elles ;la surface de la cellule est délimitée par l'enveloppe cellulaire (en), formée généralement d'une paroi cellulaire recouvrant une membrane plasmique, certaines bactéries étant recouvertes d'une couche supplémentaire appelée capsule. L'enveloppe confère sa rigidité à la cellule et sépare l'intérieur de la cellule de l'environnement de cette dernière en jouant le rôle d'un filtre protecteur. La plupart des procaryotes sont dotés d'une paroi cellulaire, mais il existe des exceptions telles que les genres Mycoplasma (une bactérie) et Thermoplasma (une archée). La paroi cellulaire des bactéries est constituée de peptidoglycane et agit comme barrière supplémentaire contre les agents extérieurs. Elle empêche également la cellule de se dilater et d'éclater (cytolyse) sous l'effet de sa pression osmotique interne dans un environnement hypotonique. Certains eucaryotes sont également pourvus d'une paroi cellulaire, par exemple les cellules végétales et les cellules fongiques ;l'intérieur de la cellule possède une région qui contient le génome (ADN) et contient également des ribosomes et divers types d'inclusions. Les ribosomes sont responsables de l'aspect granulaire du cytoplasme des procaryotes. Le matériel génétique est libre dans le cytoplasme. Les procaryotes peuvent porter des éléments extrachromosomiques appelés plasmides, qui sont généralement circulaires. Ces plasmides encodent des gènes supplémentaires, comme des gènes de résistance aux antibiotiques. Des plasmides bactériens linéaires ont été identifiés chez plusieurs espèces de spirochètes, notamment le genre Borrelia, et particulièrement Borrelia burgdorferi, responsable de la maladie de Lyme. À la place du noyau des eucaryotes, les procaryotes possèdent une région nucléaire diffuse appelée nucléoïde. Les plantes, les animaux, les mycètes, les protozoaires et les algues sont des eucaryotes. Ces cellules sont en moyenne 15 fois plus grandes qu'un procaryote typique, et peuvent être jusqu'à mille fois plus volumineuses. La principale caractéristique qui distingue les eucaryotes des procaryotes est leur compartimentation en organites spécialisés au sein desquels se déroulent des processus métaboliques spécifiques. Parmi ces organites se trouve le noyau, qui héberge l'ADN de la cellule. C'est la présence de ce noyau qui donne son nom à ce type de cellules, eucaryote étant forgé à partir de racines grecques signifiant « à vrai noyau ». Par ailleurs :la fonction biologique de la membrane plasmique des eucaryotes est semblable à celle des procaryotes, mais avec une structure qui peut être un peu différente. Il existe des eucaryotes avec et sans paroi cellulaire ; dans la cellule végétale, il s'agit d'une paroi pectocellulosique constituée de polysaccharides, notamment de cellulose, d'hémicellulose et de pectine ;le matériel génétique des eucaryotes est organisé en une ou plusieurs molécules d'ADN linéaires associées à des histones pour former des chromosomes. Tout l'ADN chromosomique d'un eucaryote est stocké dans le noyau, séparé du cytoplasme par la membrane nucléaire. Certains organites des eucaryotes contiennent également de l'ADN, par exemple les mitochondries ;de nombreuses cellules d'eucaryotes sont pourvues de cils primaires, qui jouent un rôle important dans la perception par la cellule d'informations chimiques, mécaniques et thermiques sur son environnement. Les cils primaires doivent donc être vus comme des antennes cellulaires sensibles qui coordonnent un grand nombre de voies métaboliques de signalisation cellulaire, parfois en couplant la signalisation à la motilité ou à la division cellulaire, voire à la différenciation cellulaire ;certaines cellules d'eucaryotes peuvent se déplacer par motilité au moyen de cils vibratiles ou de flagelles. Ces derniers forment des systèmes plus simples chez les eucaryotes que chez les procaryotes.Toutes les cellules, qu'il s'agisse de procaryotes ou d'eucaryotes, possèdent une membrane plasmique qui les enveloppe, régule les flux de matière entrants et sortants (transport membranaire) et maintient un potentiel électrochimique de membrane. Contenu dans cette membrane se trouve le cytoplasme, qui est une solution aqueuse riche en sels dissous occupant l'essentiel du volume de la cellule. Toutes les cellules possèdent un matériel génétique constitué d'ADN, ainsi que de l'ARN qui intervient essentiellement dans la biosynthèse des protéines et des enzymes, ces dernières étant responsables du métabolisme de la cellule ; les érythrocytes (globules rouges du sang) font exception, car leur cytoplasme est dépourvu de presque tous les organites constituant normalement une cellule d'eucaryote, ce qui leur permet d'accroître la quantité d'hémoglobine qu'ils peuvent contenir, et ne possèdent donc pas de noyau, dans lequel se trouverait l'ADN. Il existe une très grande variété de biomolécules dans les cellules.La membrane plasmique, ou membrane cellulaire, est une membrane biologique qui entoure et délimite le cytoplasme d'une cellule. Chez les animaux, la membrane matérialise la surface de la cellule, tandis que, chez les plantes et les procaryotes, elle est généralement recouverte d'une paroi cellulaire. Ainsi, chez les plantes, les algues et les mycètes, la cellule est incluse dans une paroi pectocellulosique, qui fournit un squelette à l'organisme. Des dépôts de composés tels que la subérine ou la lignine modulent les propriétés physico-chimiques de la paroi, la rendant plus rigide ou plus imperméable, par exemple.La membrane a pour fonction de séparer le milieu intracellulaire de l'environnement de la cellule en le protégeant de ce dernier. Elle est constituée d'une bicouche lipidique chez les eucaryotes, les bactéries et la plupart des archées, ou d'une monocouche d'étherlipides chez certaines archées. Chez les eucaryotes, il s'agit essentiellement de phospholipides, qui ont la propriété d'être amphiphiles, c'est-à-dire de posséder une tête polaire hydrophile et des queues aliphatiques hydrophobes. Une très grande variété de protéines, dites protéines membranaires, sont incluses dans la membrane plasmique, où elles jouent le rôle de canaux et de pompes assurant le transport membranaire entrant et sortant de la cellule. On dit que la membrane plasmique est semiperméable car sa perméabilité est très variable en fonction de l'espèce chimique considérée : certaines peuvent la traverser librement, d'autres ne peuvent la traverser que de façon limitée ou dans un seul sens, d'autres enfin ne peuvent pas la traverser du tout. La surface cellulaire contient également des récepteurs membranaires qui assurent la transduction de signal dans le cadre de mécanismes de signalisation cellulaire, ce qui permet à la cellule de réagir par exemple à la présence d'hormones.Le cytosquelette intervient pour définir et maintenir la forme de la cellule (tenségrité), positionner les organites dans le cytoplasme, réaliser l'endocytose d'éléments extracellulaires, assurer la cytokinèse lors de la division cellulaire, et déplacer certaines régions du cytoplasme lors de la croissance et de la mobilité cellulaires (transport intracellulaire). Le cytosquelette des eucaryotes est composé de microfilaments, de filaments intermédiaires et de microtubules. Un grand nombre de protéines sont associées à ces structures, chacune d'entre elles contrôlant la structure de la cellule en orientant, liant et alignant les filaments. Le cytosquelette des procaryotes est moins connu mais intervient pour maintenir la forme et la polarité ainsi que pour assurer la cytokinèse de ces cellules. La protéine constitutive des microfilaments est une petite protéine monomérique appelée actine, tandis que celle constitutive des microtubules est une protéine dimérique appelée tubuline. Les filaments intermédiaires sont des hétéropolymères dont les monomères varient en fonction du type de cellule et du tissu ; ce sont notamment la vimentine, la desmine, les lamines A, B et C, les kératines et les protéines des neurofilaments (NF-L et NF-M).Le matériel génétique des cellules peut se trouver sous la forme d'ADN ou d'ARN (cellule sans noyau). C'est la séquence nucléotidique de l'ADN qui porte toute l'information génétique (génotype) d'une cellule. Cet ADN est transcrit en ARN, un autre type d'acide nucléique, qui assure diverses fonctions : transport de l'information génétique de l'ADN vers les ribosomes sous forme d'ARN messager, et traduction de l'ARN messager en protéines sous forme à la fois d'ARN de transfert et d'ARN ribosomique, ce dernier agissant comme un ribozyme.Le matériel génétique des procaryotes est généralement constitué d'une molécule d'ADN circulaire unique formant un chromosome dans une région diffuse du cytoplasme appelée nucléoïde. Celui des eucaryotes est réparti sur plusieurs molécules d'ADN linéaires formant des chromosomes contenus dans un noyau cellulaire différencié. Les cellules d'eucaryotes contiennent également de l'ADN dans certains organites tels que les mitochondries et, chez les plantes, les chloroplastes.Une cellule humaine contient de ce fait de l'ADN dans son noyau et dans ses mitochondries. On parle respectivement de génome nucléaire et de génome mitochondrial. Le génome nucléaire humain est réparti sur 46 molécules d'ADN linéaires formant autant de chromosomes. Ceux-ci sont organisés par paires, en l'occurrence 22 paires de chromosomes homologues et une paire de chromosomes sexuels. Le génome mitochondrial humain est contenu sur un chromosome circulaire et possède 38 gènes : 14 gènes encodent des sous-unités constituant cinq protéines (NADH déshydrogénase, cytochrome b, cytochrome c oxydase, ATP synthase et humanine), deux gènes encodent des ARN ribosomiques mitochondriaux (ARNr 12S et ARNr 16S), et 22 gènes encodent vingt ARN de transfert mitochondriaux.Du matériel génétique exogène peut également être introduit dans une cellule par transfection. Ceci peut être permanent si l'ADN exogène est inséré de façon stable dans le génome de la cellule, ou transitoire dans le cas contraire. Certains virus insèrent également leur matériel génétique dans le génome de leur cellule hôte : c'est la transduction.Les organites sont des compartiments cellulaires réalisant des fonctions biologiques spécialisées, de façon analogue aux organes du corps humain. Les cellules d'eucaryotes et de procaryotes possèdent des organites, mais ceux des procaryotes sont plus simples et ne sont généralement pas matérialisés par une membrane.Il existe différents types d'organites dans une cellule. Certains sont généralement uniques, comme l'appareil de Golgi, tandis que d'autres sont présents en de très nombreux exemplaires — des centaines, voire des milliers — comme les mitochondries, les chloroplastes, les peroxysomes et les lysosomes. Le cytosol est le fluide gélatineux qui entoure les organites dans le cytoplasme. Organites présents chez tous les êtres vivants Ribosomes — Il s'agit de complexes moléculaires formés de protéines et d'ARN. Chaque ribosome est constitué de deux sous-unités et fonctionne comme une chaîne d'assemblage réalisant la biosynthèse des protéines à partir d'acides aminés. Les ribosomes peuvent se trouver ou bien en suspension dans le cytoplasme ou bien liés à la membrane plasmique chez les procaryotes ou au réticulum endoplasmique rugueux chez les eucaryotes. Organites des cellules d'eucaryotes Noyau — C'est l'organite le plus visible des cellules d'eucaryotes et qui contient leur matériel génétique. C'est dans le noyau que se trouvent les chromosomes et que se déroulent la réplication de l'ADN ainsi que la transcription de l'ADN en ARN. Le noyau est de forme grossièrement sphérique et est séparé du cytoplasme par une double membrane appelée membrane nucléaire. Celle-ci a notamment pour fonction d'isoler et de protéger le matériel génétique cellulaire des agents chimiques susceptibles de l'endommager ou d'interférer avec ses fonctions biologiques. La biosynthèse des protéines commence dans le noyau par la transcription de l'ADN en ARN messager, lequel quitte ensuite le noyau en direction du cytoplasme, où des ribosomes assurent sa traduction en protéines. Les ribosomes sont assemblés dans une région particulière du noyau appelée nucléole avant de gagner le cytoplasme. Les procaryotes étant dépourvus de noyau, l'ADN et toutes ses fonctions biologiques prennent place directement dans le cytoplasme.Réticulum endoplasmique — Cet organite est une extension cytoplasmique de la membrane nucléaire. Il assure la biosynthèse et le transport de molécules marquées pour des transformations et des destinations spécifiques, contrairement aux molécules non marquées, qui dérivent librement dans le cytoplasme. Le réticulum endoplasmique rugueux est recouvert de ribosomes qui produisent des protéines essentiellement destinées à être sécrétées hors de la cellule où à demeurer dans la membrane plasmique ; il produit également des lipides destinés au système endomembranaire dans son ensemble, dont le réticulum endoplasmique fait partie. Le réticulum endoplasmique lisse est dépourvu de ribosomes à sa surface et intervient essentiellement dans la production de lipides, dans la détoxication de certains xénobiotiques ainsi que dans diverses fonctions sécrétrices selon les types de cellules.Appareil de Golgi — Cet organite est le lieu de transformation finale des protéines nouvellement synthétisées — modifications post-traductionnelles — essentiellement par glycosylation et phosphorylation.Vacuoles — Elles concentrent les déchets cellulaires et, chez les plantes, stockent l'eau. Elles sont souvent décrites comme des volumes remplis de liquide et délimités par une membrane. Certaines cellules, notamment les amibes du genre Amoeba, possèdent des vacuoles contractiles qui peuvent pomper l'eau hors de la cellule si celle-ci en contient trop. Les vacuoles des cellules d'eucaryotes sont généralement plus grandes chez les plantes que chez les animaux.Centrosome — Cet organite produit les microtubules de la cellule, qui sont un composant essentiel du cytosquelette. Il organise le transport à travers le réticulum endoplasmique et l'appareil de Golgi. Les centrosomes sont constitués chacun de deux centrioles, qui se séparent lors de la division cellulaire et contribuent la formation du fuseau mitotique. Les cellules animales possèdent un centrosome unique ; on en trouve également chez certaines algues et certains mycètes.Lysosomes et peroxysomes — Les lysosomes contiennent essentiellement des hydrolases acides, qui sont des enzymes digestives. Ces organites ont donc pour fonction de dégrader les éléments cellulaires endommagés ou inutilisés, ainsi que les particules alimentaires, les virus et les bactéries phagocytés. Les peroxysomes contiennent des enzymes qui éliminent les peroxydes nocifs pour la cellule. Une cellule ne pourrait contenir ce type d'enzymes destructrices si elles n'étaient pas contenues dans des organites délimités par un système de membranes.Mitochondries et chloroplastes — Ce sont les organites assurant la production d'énergie métabolique de la cellule. Les mitochondries sont des organites qui se reproduisent par réplication et sont présents dans le cytoplasme de toutes les cellules d'eucaryotes sous des formes, des tailles et en nombre très variables. C'est dans les mitochondries que se déroule la respiration cellulaire, produisant de l'énergie métabolique sous forme d'ATP et du pouvoir réducteur sous forme de NADH et de FADH2 à travers un ensemble de voies métaboliques — ?-oxydation, cycle de Krebs, chaîne respiratoire, phosphorylation oxydative — qui convertissent en énergie les nutriments cellulaires tels que les acides gras et les oses. Les mitochondries se multiplient par scissiparité (division simple) comme les procaryotes. Les chloroplastes ne se trouvent que chez les plantes et les algues et assurent la production de glucides à partir du rayonnement solaire par photosynthèse. Structures extracellulaires De nombreuses cellules possèdent également des structures entièrement ou partiellement situées à l'extérieur de la membrane plasmique. Ces structures ne sont donc pas protégées de l'environnement de la cellule par une membrane semiperméable. L'assemblage de ces structures implique que leurs constituants soient transportés hors de la cellule par des processus spécifiques. Paroi cellulaire De nombreux types de cellules de procaryotes et d'eucaryotes possèdent une paroi cellulaire. Celle-ci protège la cellule des actions chimiques et mécaniques de son environnement et ajoute une couche protectrice supplémentaire par-dessus la membrane plasmique. Les différents types de cellules tendent à produire des parois de nature chimique différente : la paroi pectocellulosique des plantes est constituée essentiellement de cellulose, la paroi des mycètes est constituée essentiellement de chitine, et la paroi bactérienne est constituée essentiellement de peptidoglycane. Structures spécifiques aux procaryotes Capsule — Il s'agit d'une capsule gélatineuse présente chez certaines bactéries par-dessus la paroi bactérienne et la membrane plasmique. Elle peut être constituée de polysaccharides comme chez les pneumocoques et les méningocoques, de polypeptides comme chez le bacille du charbon, ou encore d'acide hyaluronique comme chez les streptocoques. Elles ne sont pas colorées par les procédés standard et peuvent être marquées à l'encre de Chine ou au bleu de méthyle.Flagelle — C'est l'organite essentiel de la motilité cellulaire. Le flagelle bactérien prend naissance dans le cytoplasme, traverse la membrane plasmique et les différentes parois qui peuvent éventuellement la recouvrir, et s'étend largement dans le milieu extracellulaire. Les flagelles sont de nature intégralement protéique. Ils sont de types différents chez les bactéries, les archées et les eucaryotes.Fimbriae — Il s'agit de l'appellation des pili en bactériologie. Elles se présentent comme des cils à la surface de la bactérie. Les fimbriae sont constituées d'une protéine appelée piline et interviennent dans les processus d'adhérence cellulaire. Il existe des types de pili spécifiques impliqués dans la conjugaison des bactéries.Entre deux divisions successives du cycle cellulaire, les cellules se développent grâce à leur métabolisme. Le métabolisme cellulaire est le processus par lequel chaque cellule exploite les nutriments qu'elle absorbe afin de se maintenir en vie et de se reproduire. Le métabolisme se divise en deux grandes parties : d'une part le catabolisme, par lequel les cellules dégradent les molécules complexes en espèces chimiques plus simples afin de produire de l'énergie métabolique sous forme par exemple d'ATP et du pouvoir réducteur sous forme par exemple de NADH et de FADH2 ; d'autre part l'anabolisme qui utilise l'énergie et le pouvoir réducteur produits par le catabolisme afin de synthétiser des biomolécules et de réaliser d'autres fonctions biologiques.Le cycle cellulaire est l'ensemble des processus biologiques conduisant à la division d'une cellule mère en deux cellules filles. Chez les procaryotes, qui ne possèdent pas de noyau, la réplication des cellules se fait par scissiparité, c'est-à-dire par division simple. Chez les eucaryotes, en revanche, le cycle cellulaire est divisé en trois grandes phases : l'interphase, la mitose et la cytokinèse. Au cours de l'interphase, les cellules grossissent en accumulant les substances nécessaires à la préparation de la division cellulaire et à la réplication de l'ADN. Puis le noyau se scinde en deux au cours de la mitose, et enfin le cytoplasme achève de se scinder à son tour en deux, avec un noyau dans chacune des deux parties, au cours de la cytokinèse. Des mécanismes appelés points de contrôle (en) assurent que la division se déroule de façon conforme.La division cellulaire est le processus par lequel une cellule unique, dite cellule mère, donne naissance à deux cellules, dites cellules filles. Ceci permet la croissance des organismes multicellulaires et la multiplication des organismes unicellulaires. Les cellules de procaryotes se divisent par scissiparité (division simple) tandis que les cellules d'eucaryotes se divisent d'abord au niveau de leur noyau — phase de mitose — puis au niveau de l'ensemble du cytoplasme — phase de cytokinèse. Une cellule diploïde peut également donner des cellules haploïdes, généralement au nombre de quatre, à travers le processus de méiose ; les cellules haploïdes interviennent comme gamètes chez les organismes multicellulaires en fusionnant avec d'autres gamètes pour redonner des cellules diploïdes.La réplication de l'ADN, qui est la base moléculaire de la réplication du génome d'une cellule, intervient toujours lorsqu'une cellule se divise par mitose ou par scissiparité ; elle a lieu à la phase S du cycle cellulaire. Au cours de la méiose, l'ADN n'est répliqué qu'une seule fois alors que la cellule se divise deux fois : la réplication de l'ADN intervient lors de la première division de la méiose, mais pas lors de la division subséquente. La réplication, comme tous les autres processus cellulaires, requiert des protéines et des enzymes spécialisées pour être menée à bien.Dans le cas des organismes unicellulaires, il est généralement accepté que les cellules prolifèrent spontanément, sans avoir besoin de stimulation. Dans le cas des organismes multicellulaires, cette question fait l'objet d'un débat. De nombreux auteurs défendent l'idée que ces cellules requièrent une stimulation pour proliférer, d'autres au contraire considèrent que la quiescence est le résultat de contraintes agissant sur les cellules quiescentes,. Pour la modélisation du comportent des cellules, les deux points de vue sont couramment utilisés.L'une des activités biochimiques principales des cellules est de produire de nouvelles protéines. Ces dernières sont essentielles à la régulation et à la maintenance de l'activité cellulaire. La synthèse des protéines se décompose en plusieurs étapes : transcription de l'ADN en ARN messager, modifications post-transcriptionnelles de l'ARN messager, traduction de l'ARN messager en protéines, modifications post-traductionnelles des protéines nouvellement synthétisées, et enfin repliement des protéines dans leur conformation fonctionnelle, appelée état natif.Lors de la transcription, des ARN polymérases produisent un brin d'ARN complémentaire au brin d'ADN codant. L'information génétique est portée par la séquence nucléotidique de l'ADN, reproduite sur l'ARN messager lors de la transcription. Cette séquence est ensuite lue par des ribosomes afin de polymériser les acides aminés dans l'ordre spécifié par la succession de groupes de trois nucléotides sur l'ARN messager, chacun de ces triplets, appelés codons, correspondant à un acide aminé donné ; c'est cette correspondance entre codons et acides aminés qu'on appelle code génétique.Les unicellulaires peuvent se déplacer afin de trouver de la nourriture ou d'échapper à leurs prédateurs. Flagelles et cils sont les principaux moyens de motilité cellulaire.Chez les organismes multicellulaires, les cellules peuvent se déplacer par exemple lors de la cicatrisation des plaies, lors de la réponse immunitaire, ou encore lors de la formation de métastases tumorales. Ainsi, les leucocytes (globules blancs) se déplacent jusqu'à la plaie afin d'y tuer les micro-organismes susceptibles d'y provoquer des infections. La motilité cellulaire fait intervenir de nombreux récepteurs, des mécanismes de réticulation, d'assemblage, de liaison ou encore d'adhérence de protéines, ainsi que des protéines motrices, parmi d'autres types de protéines. Le processus se déroule en trois temps : saillie de la pointe avant de la cellule, adhérence de l'avant de la cellule et « désadhérence » du reste de la surface cellulaire, et contraction du cytosquelette pour tirer la cellule vers l'avant. Chacune de ces étapes est gérée par des forces produites par des segments particuliers du cytosquelette. Comme pour la prolifération, la question de savoir si la motilité des cellules d'un multicellulaire est spontanée, comme pour les unicellulaires, ou doit faire l'objet de stimulations fait l'objet d'un débat.Un organisme multicellulaire est constitué de plusieurs cellules, par opposition à un organisme unicellulaire.Au sein des organismes multicellulaires, les cellules se spécialisent en différents types cellulaires (en) adaptés chacun à des fonctions physiologiques particulières. Chez les mammifères par exemple, on trouve par exemple des cellules de la peau, des myocytes (cellules musculaires), des neurones (cellules nerveuses), des cellules sanguines, des fibroblastes (cellules des tissus conjonctifs), ou encore des cellules souches. Des cellules de types différents d'un même organisme ont une fonction physiologique et une apparence propres, mais partagent le même génome. Des cellules ayant le même génotype peuvent présenter des phénotypes différents en raison d'une expression génétique différenciée : les gènes qu'elles contiennent ne sont pas exprimés de la même façon les uns par rapport aux autres, certains le sont davantage dans un type cellulaire que dans un autre.Tous les types cellulaires d'un organisme donné dérivent d'une cellule unique dite totipotente, c'est-à-dire capable de se différencier en n'importe quel type cellulaire lors du développement de l'organisme. La différenciation cellulaire est influencée par divers facteurs environnementaux (par exemple les interactions cellule-cellule (en)) et des différences intrinsèques (par exemple la distribution non uniforme des molécules lors de la division).La multicellularité a émergé à partir d'organismes unicellulaires un grand nombre de fois au cours de l'évolution et ne s'observe pas uniquement chez les eucaryotes : certains procaryotes tels que des cyanobactéries, des myxobactéries, actinomycètes, Magnetoglobus multicellularis ou encore des archées du genre Methanosarcina, présentent des organisations multicellulaires. Cependant, ce sont bien chez les eucaryotes que des organismes multicellulaires sont apparus, et ce parmi six groupes : les animaux, les mycètes, les algues brunes, les algues rouges, les algues vertes et les plantes. La multicellularité peut être apparue à partir de colonies d'organismes interdépendants, voire à partir d'organismes en symbiose.Les plus anciennes traces de multicellularité ont été identifiées chez des organismes apparentés aux cyanobactéries qui vivaient il y a entre 3 et 3,5 milliards d'années. Parmi d'autres fossiles d'organismes multicellulaires, on compte également Grypania spiralis,, dont la nature biologique exacte demeure cependant débattue, ainsi que les fossiles des schistes paléoprotérozoïques du groupe fossile de Franceville, au Gabon.L'évolution d'organismes multicellulaires à partir d'ancêtres unicellulaires a été reproduite au laboratoire à travers des expériences d'évolution expérimentale utilisant la prédation comme vecteur de pression de sélection.L'origine des cellules est intimement liée à l'origine de la vie, aux origines de l'histoire évolutive du vivant.Il existe plusieurs théories expliquant l'origine des petites molécules ayant conduit à l'apparition de la vie sur Terre. Elles auraient pu être apportées depuis l'espace par des météorites (météorite de Murchison), être formées dans les monts hydrothermaux sous les océans ou sous l'effet de la foudre dans une atmosphère réductrice (expérience de Miller-Urey). On dispose de peu de données expérimentales permettant de savoir ce qu'étaient les premières substances capables de se reproduire à l'identique. On pense que l'ARN a été la première molécule capable d'auto-réplication, car elle est capable à la fois de stocker l'information génétique et de catalyser des réactions chimiques (ribozymes), ce qui a été formulé dans le cadre de l'hypothèse du monde à ARN ; il existe cependant d'autres substances capables d'auto-réplication qui auraient pu précéder l'ARN dans cette fonction, par exemple des argiles comme la montmorillonite, qui sont capables de catalyser la polymérisation de l'ARN et la formation de membranes lipidiques,, ou encore les acides nucléiques peptidiques.Les premières cellules sont apparues il y a au moins 3,5 milliards d'années,. On considère actuellement que ces premières cellules étaient hétérotrophes. Les premières membranes cellulaires étaient probablement plus simples et plus perméables que les membranes actuelles. Les lipides forment spontanément des bicouches lipi"
médecine;"Un agent infectieux est un agent biologique pathogène responsable d'une maladie infectieuse.Les agents infectieux sont majoritairement des micro-organismes, notamment des bactéries et des virus. Cependant, certains agents pathogènes ne sont pas des organismes (les prions), d'autres ne sont pas microscopiques (les vers parasites).Le pouvoir pathogène d'un agent infectieux mesure sa capacité à provoquer une maladie chez un organisme hôte.La virulence d'un agent infectieux mesure sa capacité à se développer dans un organisme (pouvoir invasif) et à y sécréter des toxines (pouvoir toxique).Chez les humains, 1 415 espèces infectieuses ont été inventoriées,. Environ 600 sont connues pour le bétail et environ 400 pour les chats et les chiens. Les données concernant la faune sauvage sont trop parcellaires pour qu'un chiffre puisse être donné. Parmi ces agents infectieux on compte :des protéines : les prions (agents dits non conventionnels car exempts d'acide nucléique) ;des virus (217 espèces infectieuses) ;des organismes unicellulaires :procaryotes : certaines bactéries (538 espèces infectieuses),eucaryotes : les protozoaires parasites et certaines levures (66 espèces infectieuses) ;des organismes pluricellulaires :certaines moisissures (307 espèces infectieuses),les vers parasites ou helminthes (287 espèces infectieuses),certains arthropodes (acariens, pou).Le pouvoir pathogène (grec ancien ????? [pathos], « souffrance » ; id. ????? [genos], « naissance ») — ou pathogénicité — d'une bactérie mesure sa capacité à provoquer des troubles chez son hôte. Il varie selon la souche (sérovar) et dépend de son pouvoir invasif (capacité à se répandre dans les tissus et à y établir un ou des foyers infectieux), de son pouvoir toxicogène (capacité à produire des toxines) et de sa capacité à se reproduire.On distingue trois catégories de bactéries pathogènes :les bactéries pathogènes strictes (ou spécifiques), qui provoquent des troubles quel que soit le patient (à l'exception des porteurs sains) ; par exemple : Salmonella Typhi et Vibrio cholerae ;les bactéries pathogènes opportunistes, qui provoquent des troubles lorsque les défenses immunitaires de l'hôte sont affaiblies ou que la personne est âgée (on parle aussi de sujets immunodéprimés) ; par exemple : Pseudomonas aeruginosa.les bactéries pathogènes occasionnelles, qui sont le plus souvent inoffensives mais dont certaines souches sont pathogènes. On retrouve dans cette catégorie des bactéries commensales comme Escherichia coli ou Staphylococcus aureus.Le pouvoir invasif d'une bactérie (ou d'une souche bactérienne) est sa capacité à se multiplier et à se répandre dans un organisme hôte, malgré les défenses immunitaires. Facteurs exogènes La température de l'eau, de l'air et du sol, le pH, le taux d'oxygène, la teneur en certains nutriments de l'environnement, l'exposition aux UV solaires, à certains toxiques ou polluants ou à la radioactivité, la présence d'un vecteur biologique, d'une lésion ou d'une primo-infection, etc. peuvent ou non favoriser l'agent infectieux.Par exemple le réchauffement climatique pourrait permettre à des microbes d'être plus présents et infectieux plus haut en altitude, plus près des pôles nord et sud ou plus fréquemment dans les eaux douces, estuariennes ou salines.La radioactivité ambiante, l'augmentation des UV induite par le trou de la couche d'ozone, la dispersion de biocides et d'antibiotiques dans l'environnement, ou encore l'exposition à l'ozone troposphérique pourraient être de nouveaux facteurs de mutation et donc d'apparition de souches plus agressives ou plus résistantes, ou de maladies émergentes. De même de nombreux agents mutagènes dispersés par l'Homme dans l'environnement (radionucléides, certains métaux lourds et divers produits chimiques) pourraient favoriser l'apparition de nouvelles souches pathogènes. Facteurs liés à la bactérie La constitution et le métabolisme de la bactérie définissent en partie son pouvoir invasif ; ainsi, celui-ci dépend :des facteurs d'adhésion : la présence de fimbriae (ou pili), d'adhésines, et/ou de glycocalyx facilite la fixation de la bactérie sur sa cellule cible ;de la résistance à la phagocytose grâce à la présence d'une capsule, mais aussi de la résistance aux enzymes lysosomales (censées détruire la bactérie phagocytée) ;de la production d'enzymes :les collagénases, qui dégradent le collagène des tissus conjonctifs (chez Clostridium perfringens par exemple),les coagulases, qui permettent la formation d'un caillot autour du corps bactérien, qui le protège des cellules du système immunitaire (chez S. aureus par exemple),les hyaluronidases, qui dégradent l'acide hyaluronique, constituant des tissus conjonctifs,les DNases (ou ADNases) qui dégradent l'ADN des cellules infectées,les kinases (ou fibrinolysines). Facteurs liés à l'hôte Le pouvoir invasif dépend également du terrain infecté (c'est-à-dire le milieu environnant la bactérie), à savoir :l'état du système immunitaire de l'hôte (une diminution des défenses immunitaires due à l'âge, à la fatigue, à la maladie, etc. favorise l'invasion) ;les facteurs physico-chimiques de l'environnement.Une toxine est une molécule synthétisée par un organisme vivant, ayant un effet nocif ou létal pour l'organisme hôte.Les toxines protéiques sont les poisons les plus actifs : 250 g de toxine (tétanique ou botulinique) suffirait à tuer toute la population humaine.Le pouvoir pathogène peut être quantifié par trois données : la dose minimale mortelle (DMM), la dose létale 50 (DL50) et la dose minimale infectante (DMI).La DMM est la dose la plus faible qui tue dans un délai déterminé un groupe expérimental.La DL50 est la dose qui tue dans un délai déterminé 50 % d'un groupe expérimental.La DMI est la dose minimale permettant la contamination et le développement de la maladie.Les toxines peuvent agir de plusieurs manières : sur le système immunitaire, en provoquant une allergie (effet allergène), ou encore un choc septique ; sur le système nerveux (effet neurotoxique) ; sur le système musculaire (effet myotoxique) ; sur le système reproductif (effet reprotoxique) ; etc. Une toxine peut agir seule ou en synergie avec d'autres. Selon la bactérie en cause et le mode de contamination, la production et l'action de la toxine se feront différemment.Dans ou autour d'une plaie, une bactérie peut se multiplier et libérer sa toxine protéique. Celle-ci peut agir localement et éventuellement à distance, au niveau de la moelle épinière, en diminuant la quantité de neuromédiateurs libérés, et au niveau des synapses neuromusculaires en augmentant la libération d'acétylcholine. Elle provoque une paralysie de contracture. Exemple : toxine tétaniqueÀ la suite d'une ingestion ou inhalation ou de son développement dans le tube digestif, la toxine passe dans le sang et diminue la quantité d'acétylcholine au niveau des jonctions neuromusculaires. Elle provoque une paralysie flasque.Exemple : toxine botuliniqueÀ la suite d'une ingestion, la bactérie adhère à l'épithélium intestinal et produit la toxine qui se fixe sur les entérocytes. Elle empêche l'absorption des ions Na+ et Cl?, et provoque donc une fuite hydrominérale.Exemple : toxine cholériqueLes toxines protéiques ont souvent un pouvoir toxique très élevé. Elles provoquent l'apparition d'anticorps dans l'organisme : les anti-toxines.Certaines peuvent être transformées en anatoxines par un traitement au formol, et une incubation à 40 °C (Méthode de Ramon). Ces anatoxines sont utilisées pour :fabriquer des vaccins ;fabriquer des sérums utilisés en sérothérapie.Certaines souches d'Escherichia coli sont sources de diarrhées, d'infections urinaires, d'infections nosocomiales, de septicémies, de la méningite du nouveau-né ; la souche EHEC O157 est une source du syndrome hémolytique et urémique.Staphylococcus aureus, Pseudomonas aeruginosa sont sources d'infections des yeux, de plaies et de la gastro-entérite aiguë).La salmonelle, Yersinia enterocolitica provoquent des gastro-entérites.Campylobacter jejuni est devenue la première source de pathologies entériques bactériennes dans les pays industrialisés.Vibrio cholerae cause le cholera.Shigella dysenteriae cause des dysenteries.L'eau épurée doit donc être débarrassée de ces germes pathogènes lorsqu’elle est rejetée dans le milieu naturel pour ne pas contaminer celui-ci et causer une épidémie pouvant être mortelle au sein des populations en aval.Groupe 1 : non pathogènes pour l'homme.Groupe 2 : pathogènes pour l'homme, contagiosité faible, prophylaxie et traitement.Groupe 3 : pathogènes pour l'homme, contagiosité, prophylaxie ou traitement.Groupe 4 : pathogènes pour l'homme, forte contagiosité, sans prophylaxie ni traitement.(en) « Que devient une bactérie pathogène après avoir tué son hôte ? » Gazettelabo 13 avril 2012, sur PLOSToxinePlasmide et Conjugaison bactérienneCulture axéniqueConservation de la viandeVarioleRessource relative à la santé : (no + nn + nb) Store medisinske leksikon  Portail de la médecine   Portail des maladies infectieuses   Portail de la microbiologie"
médecine;"Une anesthésie locale  consiste à inhiber de façon réversible la propagation des signaux le long des nerfs. Si cette anesthésie est réalisée au niveau de voies nerveuses spécifiques, elle est susceptible de produire des effets tels que l'analgésie (diminution de la sensation de douleur) et la paralysie (perte de puissance du muscle). Elle s'oppose à l'anesthésie générale où le patient est endormi.En 1884, Carl Köller utilise déjà la cocaïne pour l’anesthésie par contact en ophtalmologie et en otorhinolaryngologie. La même année, Richard Hall inaugure son emploi en chirurgie dentaire et William Halsted introduit la technique du « bloc nerveux ».En le greffant sur d’autres alcaloïdes, comme la quinine ou la morphine, Filehne démontre que c’est le noyau benzoyle de la cocaïne qui est responsable de son activité anesthésique. Mais ces esters benzoïques, tous actifs, restent trop irritants pour être utilisables.Entrepris sur la base du modèle moléculaire proposé pour la cocaïne par Alfred Einhorn en 1892, les travaux de Georg Merling à Berlin aboutissent à la commercialisation de la bêta-eucaïne par Schering AG. Mais c’est Richard Willstätter, élève d’Einhorn, qui, en 1898, élucide définitivement la structure de la cocaïne, qu’il synthétise en 1901. La fonction phénolique liée à une fonction carboxylique estérifiée s’étant révélée essentielle, Paul Ehrlich met alors au point l’orthoforme puis le néoorthoforme, auxquels leur fonction phénolique prête aussi une action antiseptique. En 1902, E. Ritsert, cherchant à sa benzocaïne (Anesthésine) des dérivés plus solubles, parvient à la Nirvanine, immédiatement rendue obsolète par l’arrivée de nouvelles molécules.En effet, dès l’année suivante Ernest Fourneau, directeur des recherches chez Poulenc frères, revenant d’Allemagne où il a travaillé avec Willstätter, synthétise la Stovaïne, premier substitut non irritant de la cocaïne en anesthésie locale. Un an plus tard, les laboratoires Hoechst commercialisent la Novocaïne synthétisée par Einhorn et qui sera pendant des décennies le principal des anesthésiques locaux.Parallèlement à ces découvertes, des étapes essentielles dans le développement de l’anesthésie locale ont été franchies. Elles ont abouti aux techniques d’anesthésie locorégionale.À la suite des travaux de l’Allemand Heinrich Braun, puis des Anglais George Oliver et Edward Sharpey-Schafer en 1894, et enfin des Américains John Jacob Abel et Albert Cornelius Crawford en 1898, l’adrénaline est introduite en anesthésie locale comme vasoconstricteur pour ralentir l’élimination du médicament et compléter ainsi les effets du garrottage, pratiqué par James Leonard Corning dès 1885.Mais l’adrénaline reste insuffisamment efficace et le garrottage n’est utilisable que sur les territoires facilement accessibles. S’appuyant sur les observations faites par Edward Feinberg en 1886, et que François-Franck a reprises en 1887 pour établir que « le contact direct d’une solution de cocaïne avec un tronc nerveux détermine l’abolition des propriétés fonctionnelles de ce nerf », Corning et Oberst inaugurent alors la technique de l’anesthésie locorégionale : au lieu d’agir dans la région concernée, ils opèrent sur le nerf correspondant. Enfin, les Français Jean Anasthase Sicard et Fernand Cathelin mettent au point, en 1901, l’anesthésie péridurale en injectant le médicament dans le Liquide cérébrospinal,.Il existe différents types d'anesthésies locales :anesthésie topique = de surface : l'anesthésique sous forme de gel ou pommade est déposé sur la muqueuse.anesthésie par infiltration : l'anesthésique est déposé à proximité du ou des nerfs à endormir, grâce à une aiguille.Les anesthésies locorégionales, plus efficaces que la simple anesthésie locale, anesthésient un nerf ou un territoire donné, souvent plus large que la zone chirurgicale concernée. Ces techniques permettent d'effectuer des chirurgies de plus grande envergure. Elles nécessitent des doses d'anesthésiques locaux modérées pour une grande efficacité. Les différents types d'anesthésies locorégionales sont:bloc tronculaire : consiste à infiltrer un tronc nerveux pour obtenir l'anesthésie de son territoire ; par exemple le bloc du nerf cubital entraine l'anesthésie du bord interne de la main.bloc plexique : consiste à infiltrer un plexus (ensemble de nerfs) pour obtenir une anesthésie d'une région entière. Par exemple, l'infiltration du plexus brachial entraine une anesthésie de tout le membre supérieur.blocage épidural (ou infiltration de l'espace péridural). En fonction du niveau infiltré peut donner une anesthésie de la moitié inférieure du corps, ou simplement de plusieurs métamères (péridurale suspendue)rachianesthésie : injection d'anesthésique local dans le liquide céphalorachidien, donne une anesthésie de la moitié inférieure du corps.Les techniques d'anesthésie locorégionale font appel à l'utilisation de neurostimulateurs afin de faciliter le repérage des nerfs et d'améliorer le pourcentage de succès de ces anesthésies. L'utilisation du repérage des nerfs par échographie (technique indolore et beaucoup plus confortable pour le patient) est en pleine expansion et semble être la technique d'avenir.Il existe de nombreux anesthésiques locaux. Jusqu'à la mise au point de la Stovaïne et de la Novocaïne, la cocaïne avait été pratiquement seule en usage. En 1946, Löfgren introduisit la lidocaïne. Puis vinrent la scandicaïne, la prilocaïne, l'étidocaïne et la bupivacaïne. Les plus modernes sont la ropivacaïne, la lévobupivacaïne, l'articaïne et la mépivacaïne.péri-apicales, para apicales ou supra-périostée : les plus fréquentes. L'anesthésique est déposé en regard de l'apex de la dent sur la face externe du périoste. Le produit diffuse au travers de la corticale pour rejoindre l'apex. Cette anesthésie se fait à la jonction gencive attachée/gencive libre. Il faut piquer avec une angulation de 45° et faire glisser le biseau en direction apicale jusqu'au-dessus de l'apex. Le point d'infiltration sera toujours distal par rapport à la dent. Utilisé pour toutes les dents maxillaires, et les dents mandibulaires antérieures (jusqu'à la première prémolaire). Au-delà son efficacité est relative. Elle se caractérise par un engourdissement plus ou moins important de la lèvre au niveau du site d’injection. Une para- apicale n'est, si elle est bien faite, pas douloureuse.intraseptales : l'anesthésique est déposé à l'intérieur de l'os alvéolaire, dans la crête osseuse. La solution diffuse, par capillarité, dans l'os spongieux et atteint l'extrémité radiculaire. On vise le milieu de la papille pour entrer dans la table osseuse. Il faut tarauder l'os avec une aiguille courte, bipointe car il faut forcer pour atteindre le septum. Cette technique peut être indifféremment utilisée à la mandibule et au maxillaire. Elle pose quelques problématiques :il faut passer la corticale,il y a risque de torsion et d'obstruction de l'aiguille,c'est une anesthésie de courte durée,s'il y a peu d'os spongieux et beaucoup d'os cortical, la vasoconstriction peut générer une nécrose du septum.Il faut y avoir recours en dernier lieu.intraligamentaires : l'anesthésique est déposé à l'intérieur du desmodonte ou ligament alvéolodentaire. l'aiguille va passer entre le septum inter-dentaire et la racine. L'anesthésie arrive par toutes petites doses mais sous pression. Le biseau de l'aiguille est toujours tourné vers la racine. C'est une anesthésie de courte durée (15 minutes) donc pour des soins conservateurs courts. C'est une technique contre-indiquée pour les gros actes chirurgicaux (exodontie) et lésions parodontales car la vascularisation locale est perturbée et il y a un risque d'alvéolite.intrapulpaires : l'anesthésique est déposé dans la pulpe de la dent. Utilisée en complément, notamment lors d'actes endodontiques (dévitalisation). Cette technique a pour caractéristique d’être douloureuse.anesthésie loco-régionale (ou « tronculaire » ou « anesthésie à l'épine de Spix ») : permet d'endormir les molaires mandibulaires (impossibles à endormir par une para-apicale). On anesthésie le nerf mandibulaire (V3) avant qu'il n'entre dans l'os mandibulaire. On obtient une anesthésie des molaires et prémolaires, qui s’accompagne généralement d’une perte de sensibilité et de motricité de plusieurs heures de la lèvre du côté où a été faite l'infiltration. Cette anesthésie est souvent utilisée pour l’extraction des dents de sagesse. Elle nécessite un peu d’attente pour faire effet.anesthésie transcorticale : l'anesthésique est déposé dans l’os spongieux qui entoure la dent. Cette technique est immédiate et permet d’anesthésier 2 à 6 dents sans gêne postopératoire pour le patient.anesthésie ostéocentrale : l’anesthésique est également déposé dans l’os spongieux, mais à proximité immédiate des apex. De ce fait, l’anesthésie est instantanée et très efficace. Elle peut être utilisée sur tous les secteurs et permet d’obtenir l’anesthésie de 2 à 8 dents, sans engourdissement de la lèvre et sans suite postopératoire.Repose sur le principe de modification des perméabilités membranaires de l'axone.Dans la carpule, l'anesthésique est sous forme non ionisée donc inactif mais diffusible. Il ne s'active qu'en milieu acide (intérieur de l'axone).Donc lorsque le produit est injecté, il est inactif (car le milieu n'est pas acide) mais se diffuse jusqu'à l'axone qu'il pénètre grâce aux canaux sodiques. Une fois rentré, il est en milieu acide, s'ionise donc s'active mais n'est plus diffusible. Il bloquera les récepteurs sodiques, ce qui entraînera la perte de l'excitabilité de la fibre nerveuse et l'abolition de la conduction du potentiel d'action.Lors de phénomènes inflammatoires, le milieu dans lequel on injecte l'anesthésique est déjà acide, le produit est donc immédiatement ionisé, activé et ne diffuse pas jusqu'à l'axone, donc l'anesthésie ne prend pas. D'où l’intérêt dans ces cas là de passer de l'anesthésie locale à locorégionale.La section « Histoire » a pour source principale :François Chast, « De Freud à la péridurale : Anesthésie locale », dans Histoire contemporaine des médicaments, La Découverte, 2002 Portail de la médecine"
médecine;"La douleur est une « expérience sensorielle et émotionnelle désagréable », une sensation subjective normalement liée à un message, un stimulus nociceptif transmis par le système nerveux. D'un point de vue biologique et évolutif, la douleur est une information permettant à la conscience de faire l'expérience de l'état de son corps pour pouvoir y répondre. On distingue principalement deux types de douleur, aiguë et chronique :la douleur aiguë correspond à un « signal d'alarme » de l'organisme pour inciter à une réaction appropriée en cas de remise en cause de son intégrité physique, soit par un traumatisme (brûlure, plaie, choc), soit par une maladie ;la douleur chronique, l'installation durable de la douleur, est considérée comme une maladie qui peut notamment être le signe d'un dysfonctionnement des mécanismes de sa genèse, on parle alors de douleur neurogène ou psychogène.Cette sensation, de désagréable à insupportable, n'est pas nécessairement exprimée. Pour l'identifier chez autrui on peut faire le diagnostic de la douleur en se référant à des effets observables, par exemple les mouvements réflexes de retrait au niveau des membres et des extrémités pour les douleurs aiguës, ou des changements de comportement, d'attitudes et de positions du corps pour les douleurs chroniques.Les traitements de la douleur sont multiples, les études sur le sujet pour une meilleure compréhension se poursuivent, en particulier pour la reconnaître quand elle n'est pas exprimée. Ainsi la douleur de l'enfant ne l'est pas toujours, la douleur chez le nouveau-né étant même officiellement inexistante jusqu'à la démonstration du contraire en 1987, et son identification dans le Règne animal reste un sujet de recherche.Une définition de référence de la douleur a été donnée en 1979 par L'IASP (International Association for the Study of Pain) :La douleur apparait ainsi comme une expérience subjective. C'est un événement neuropsychologique pluridimensionnel. Il convient alors de distinguer :la composante sensori-discriminative qui correspond aux mécanismes neurophysiologiques de la nociception. Ils assurent la détection du stimulus, sa nature (brûlure, décharges électriques, torsion, etc.), sa durée, son évolution, son intensité, et l’analyse de ses caractères spatiaux ;la composante affective qui exprime la connotation désagréable, pénible, rattachée à la perception douloureuse. La représentation mentale de la douleur chronique (les états mentaux aversifs provoqués par les émotions causées par la douleur) serait chargée d'une valeur négative capable de transformer les états neuronaux ;la composante cognitive référant à l’ensemble de processus mentaux qui accompagnent et donnent du sens à une perception en adaptant les réactions comportementales comme les processus d’attention, d’anticipation et de diversion, les interprétations et valeurs attribuées à la douleur, le langage et le savoir sur la douleur (sémantique) avec les phénomènes de mémoire d’expériences douloureuses antérieures personnelles (mémoire épisodique) décisifs sur le comportement à adopter.Beecher en 1956 a démontré l’influence de la signification accordée à la maladie sur le niveau d’une douleur. En étudiant comparativement deux groupes de blessés, militaires et civils, qui présentaient des lésions identiques en apparence, il a observé que les militaires réclamaient moins d’analgésiques. En effet, le traumatisme et son contexte revêtent des significations tout à fait différentes : comparativement positives pour les militaires (vie sauve, fin des risques du combat, bonne considération du milieu social, etc.), elles sont négatives pour les civils (perte d’emploi, pertes financières, désinsertion sociale, etc.) ;la composante comportementale qui correspond à l’ensemble des manifestations observables :physiologiques (paramètres somato-végétatifs ex. : pâleur),verbales (plaintes, gémissements…),motrices (immobilité, agitation, attitudes antalgiques).En 1994 L'IASP propose cinq critères distincts de classification :la région du corps impliquée : l'abdomen, membre inférieur…le système dont la dysfonction cause la douleur : digestif, nerveux…la durée et la fréquence ;l'intensité et la durée depuis le début ;l'étiologie.Dans les voies nerveuses de la nociception on distingue le circuit de la perception et celui de la régulation :les voies nociceptives ascendantes, comme toutes les voies nerveuses sensitives, véhiculent l'information de la périphérie du corps vers le cortex cérébral en passant par la moelle épinière ;les voies descendantes, à l'inverse, portent un message depuis le cortex vers la périphérie, à la rencontre du message nociceptif dont elles peuvent alors limiter l'intensité en agissant sur les voies ascendantes.Le rôle de ces circuits descendant est le rétrocontrôle, ici la régulation de l’intensité du message sensitif afin de moduler la sensation douloureuse. Ce mécanisme inhibiteur est aussi appelé théorie de la porte ou gate control et est notamment utilisé dans le contrôle inhibiteur diffus.Ces voies nociceptives transmettent l'information du stimulus nociceptif grâce à des mécanismes électrobiochimiques faisant intervenir de nombreuses molécules, dont des acides aminés. La douleur est véhiculée en premier lieu par les fibres A-delta qui conduisent le message nocicepteur à une vitesse de 15 à 30 m/s.La vulnérabilité à la douleur ou la sensibilité à l'« effet placebo » dépendent en partie de facteurs génétiques qui contrôlent le système dopaminergique du cerveau, lequel est en cause dans l'anticipation de la douleur et de la confiance en la guérison. De même pour la production par le cerveau lui-même de certains opiacés naturels (les endorphines) jouant un rôle de neurotransmetteur.La douleur compte trois grands mécanismes de genèse (qui peuvent se combiner) : la douleur de nociception, la douleur neurogène et la douleur psychogène.La douleur nociceptive est générée par un récepteur spécifique, un nocicepteur, dont le rôle est de signaler les atteintes à l'intégrité de l'organisme. C'est un signal d'alarme normal, et même souhaitable dans la mesure où il induit une attitude appropriée dont l'absence est potentiellement dangereuse pour l'organisme.La douleur neurogène est générée par le nerf lui-même et non un récepteur spécifique, c'est donc une pathologie nerveuse classée parmi les neuropathie. Elle est ressentie comme des décharges électriques, des élancements, des sensations de brûlures, des sensations de froid douloureux et des picotements dans le territoire des nerfs atteints. C'est aussi la douleur que ressentent les malades amputés et en particulier la sensation perçue dans un membre qui a disparu (membre fantôme).La douleur psychogène est générée par le psychisme, mais n'est pas imaginaire, elle est réellement ressentie par l'individu mais existe en l'absence de lésion. Les mécanismes physiologiques de ces douleurs, étudiés par la psychopathologie, ne sont pas encore clairement compris. L'utilisation d'antalgique semble dans ce cas inefficace.Outre le sentiment de souffrance, la douleur peut provoquer un malaise vagal par stimulation des nerfs vagues (nerfs pneumogastriques). Les symptômes de cette excitation vagale sont tout ou partie des signes incluant notamment une baisse du débit sanguin par bradycardie et hypotension ; une syncope ; un myosis (diminution du diamètre des pupilles par contraction de l'iris) ; une transpiration aux extrémités des membres ; une sécrétion excessive de salive ; une hyperchlorhydrie (excès de sécrétion d'acide chlorhydrique par la muqueuse de l'estomac) ; une constipation ou des diarrhées ; des spasmes et des troubles de la respiration.La douleur prolongée est inhibée par le corps par sécrétion d'endorphines (ou endomorphines). La production d'endorphine se fait initialement aux niveaux des nerfs proches du siège de la douleur ; lorsque cette production ne suffit plus (douleur prolongée), c'est un site plus proche du cerveau qui prend le relais dans la sécrétion. La douleur revient ainsi par vagues.Les états de douleur sont le résultat de la sélection naturelle. La souffrance peut être un trait adaptatif et améliorer la capacité de survie d’un individu.L'évaluation et le diagnostic de la douleur étant complexe, l'IASP précise que « L'incapacité à communiquer verbalement n'infirme pas la possibilité que l'individu éprouve de la douleur et nécessite un traitement approprié pour soulager la douleur. La douleur est toujours subjective… » L'OMS (Organisation Mondiale de la Santé) le précise bien dans ces recommandations en ce qui concerne la douleur chez l'enfant car elle est communément sous estimée.Divers organismes définissent le cadre sémantique, répertorient les connaissances physiologiques, et livrent des recommandations de traitement souvent liées aux différentes classes d'âges. Par exemple dans la francophonie, on peut citer l'INSERM sur l'aspect scientifique, le CNRD pour l'archivage des informations, la SFETD pour l’exploration médicale des voies de traitement, ou encore l'AQDC au sujet de la douleur chroniqueCependant, malgré l'émergence de moyens techniques, le diagnostic reste malaisé car il existe une tendance naturelle[Information douteuse] à se protéger de la perception de la douleur d'autrui[réf. nécessaire], c'est entre autres la raison de la mise en place d'échelles d'évaluation de la douleur.En 2014, un moyen technique pour « mesurer » la douleur relativement à la dilatation réflexe de la pupille est en cours d'évaluation. La pupillométrie permettrait d'adapter au mieux les traitements anti-douleur, en particulier chez les personnes endormies où la dilatation de la pupille n'est pas sensible à d'autres facteurs, comme le stress, mais son évaluation sur les enfants semble donner de bons résultats.D'après Nicolas Danziger,la vision de la douleur chez l'autre crée une « émotion aversive » par un mécanisme dit de « résonance émotionnelle ». Mais il précise que ce mécanisme connaît des lacunes, par exemple en cas de différence ethnique ou religieuse, que d'autre part il peut donner lieu à « une volonté de fuite ou d’éloignement de celui qui souffre », et que « de nombreux travaux scientifiques ont démontré, ces dernières années, que le corps médical avait encore tendance à mésestimer la douleur des patients ».On trouve ainsi aisément les preuves d'un déni collectif de la douleur chez autrui en particulier en ce qui concerne la douleur chez l'enfant qui est largement sous-estimée en milieu hospitalier. Daniel Annequin affirme même : « Chez l'enfant on revient de loin, pendant des années on a voulu ignorer que l'enfant ressentait de la douleur […] On disait que les fibres C n’étaient pas myélinisées, mais elles ne sont jamais myélinisées, on avait comme ça tout une série d'argumentaires pseudo-scientifiques ». Et en effet la démonstration scientifique de la capacité du nourrisson à ressentir la douleur n'a été faite qu'en 1987, et donc sa prise en charge au préalable n'existait qu’exceptionnellement, même pour les interventions les plus lourdes.Ce relatif refus de voir la douleur de l'autre n'est ni propre au milieu médical ni universel comme le montre une étude issue du plan douleur 2006 qui distingue deux types d'attitudes réparties aussi bien chez les soignants que chez les parents : les « réservés » et les « sensibilisés », chacun reprochant respectivement à l'autre groupe le trop ou le trop peu de prise en charge, par « sensiblerie » ou par « déni ». Cette distinction entre en résonance avec celle d'une autre étude sociologique qui divise les médecins également en deux groupes : les « compatissants » et les « négateurs »,.La perception de la douleur, de son intensité, est subjective. Le même phénomène (traumatisme, maladie) sera ressenti différemment selon la personne et selon la situation. La douleur peut aller d'une simple incommodation jusqu'à un malaise, voire la mise en danger du pronostic vital ou psychiatrique de la personne.L'évaluation pour l'autre est donc complexe, c'est pourquoi on s'appuie de préférence sur le témoignage grâce à un support d'auto-évaluation et des échelles d'évaluation de la douleur spécifiques quand c'est impossible ou insuffisant. Auto-évaluation L'auto-évaluation consiste à demander directement à la personne souffrante le niveau de sa douleur. Elle nécessite une coopération et une bonne compréhension, et s'appuie sur des échelles médicales standardisées (numériques, visuelles analogiques, verbales simples et verbales relatives…).L'auto-évaluation n'est pas qu'une évaluation de la douleur, c'est également une manière de communiquer avec l'équipe médicale. Dans le cas de douleurs chroniques notamment, la cotation de la douleur n'indique pas uniquement la douleur ressentie, mais globalement l'altération de la qualité de vie et la détresse émotionnelle. Hétéro-évaluation Il existe aussi des échelles d'évaluation spécifiques fondées sur l'observation du comportement du patient. Contrairement aux échelles d'auto-évaluation elles ne nécessitent pas la participation du patient et sont de ce fait recommandées dans l'évaluation de la douleur chez les personnes chez qui l'auto-évaluation pose problèmes pour différentes raisons.En plus du témoignage de l'entourage qui peut évaluer les différences au quotidien, les changements survenus, il existe des échelles d'évaluations spécifiques comme l'échelle San Salvadour. Enfants et nourrissons Le signe habituel de l'expression de la douleur pour le petit enfant est le cri que le ou les parents arrivent souvent à distinguer des autres cris (peur, faim…), mais à un stade supérieur de douleur, le nourrisson est souvent prostré.Plusieurs échelles existent, bien que peu utilisées en pratique, il s'agit de la grille DESS (Douleur enfant San Salvadour), de l'échelle NCCPC (Non communicating children’s pain checklist) ou GED-DI (Grille d’évaluation de la douleur déficience intellectuelle) et de l'échelle EDINN (Échelle de douleur et d'inconfort du nouveau-né et du nourrisson). Le problème principal de ces échelles et qu'elles comportent des items longs à répertorier et ne sont pas utilisables en urgence. Personne âgée Chez les personnes âgées, et notamment atteintes de troubles cognitifs comme la maladie d'Alzheimer, on peut utiliser l’échelle Algoplus, et fréquemment utilisée, l'échelle ECPA.Il existe deux grands types de douleurs : les douleurs par excès de nociception, et les douleurs neuropathiques.La douleur par excès de nociception est déclenchée par une stimulation des récepteurs nociceptifs. L'examen neurologique est par ailleurs normal.La douleur neuropathique est une douleur causée par une lésion des voies sensitives du système nerveux, central ou périphérique. Elle est généralement associée à une dysesthésie et une allodynie. Il peut exister une zone gâchette déclenchant les douleurs. Les douleurs neuropathiques peuvent être à type de brûlure, de froid, ou de sensations de piqûres, et s'accompagner de sensations de fourmillement.On parle de douleur discrète ou aiguë, éventuellement chronique ou récidivante, mais « habituellement, la douleur est divisée en deux catégories en fonction de la durée ».La douleur aiguë est une douleur vive immédiate, et généralement brève. Elle est causée par une stimulation nociceptive de l'organisme, telle une lésion tissulaire, pouvant se produire sous la forme d'un stimulus thermique (contact de la peau avec du feu) ou mécanique (un pincement, un coup). « La douleur aiguë joue donc un rôle d’alarme qui va permettre à l’organisme de réagir et de se protéger face à un stimulus mécanique, chimique ou thermique. »Sa fonction d'alerte est alors justifiée, ce qui n'est plus nécessairement le cas avec une douleur chronique.« La douleur est dite chronique ou pathologique, lorsque la sensation douloureuse excède trois mois et devient récurrente. »La douleur chronique est une maladie grave et invalidante. Les conséquences des douleurs chroniques sont autant organiques (hypertension artérielle secondaire, atrophie musculaire) que psychologiques, avec une modification comportementale pouvant aller de l'anxiodépression jusqu'à des troubles de la dépersonnalisation avec risque suicidaire accru.Plusieurs sociétés savantes, dont la Société française d'étude et de traitement de la douleur (SFETD), l'Association internationale d'étude de la douleur ou la Société internationale de neuromodulation, soulèvent l'importance de la douleur chronique dans la population générale ; de 15 à 25 % de la population seraient victimes de douleurs chroniques.Les douleurs chroniques sont principalement des douleurs neuropathiques dans le cadre de maladies générales avec une atteinte du système nerveux. Par exemple le diabète insulinique génère principalement une destruction des nerfs périphériques avec une hypoesthésie, mais dans certains cas, l'atteinte des nerfs périphériques va tendre vers un état d'hyperesthésie. Les atteintes post-opératoires des nerfs périphériques sont aussi parmi les principales causes de douleurs neuropathiques. En fait, toute atteinte d'un nerf périphérique ou atteinte d'une structure du système nerveux central peut s'exprimer par des douleurs neuropathiques chroniques. Le mécanisme de ces douleurs est actuellement basé sur la perte du gate control (Le gate control est schématiquement l'inhibition des voies nociceptives A? et C par les grosses fibres sensitives-motrices). Pathologies en cause Il est difficile de dresser une liste complète des syndromes douloureux chroniques comptant par exemple :les migraines et les syndromes migraineux réfractaires ;les céphalées cervico-géniques ;le Failed Back Surgery Syndrome ou « syndrome d'échec de chirurgie du dos » ou encore syndrome post-laminectomie (en) ;les douleurs neuropathiques post-opératoires chroniques ;les douleurs neuropathiques ;les douleurs de l'artérite ;Les syndromes douloureux régionaux complexes de type I et II ;les douleurs du membre fantôme ;les douleurs des pathologies ostéo-articulaires chroniques ;les lombalgies chroniques ;l'algie vasculaire de la face ;etc.Par exemple on dénombre 150 000 personnes en France qui souffrent de migraines réfractaires ou rebelles au traitement et à peu près le même nombre de personnes souffrant de céphalées cervicogéniques.Les autres mécanismes de douleur chronique sontdes douleurs inflammatoires par hyperstimulation des voies nociceptives sans atteinte directe de celles-ci ;les douleurs mécaniques par destruction des articulations ;l'ischémie d'origine vasculaire avec une composante neuropathique par ischémie des nerfs des membres. Effets Les douleurs chroniques, quelles que soient leurs origines qui peuvent être multiples, vont amputer de façon plus ou moins profonde et intense la sphère comportementale par atteinte de l'activité physique, le sommeil, la concentration et les fonctions cognitives[réf. souhaitée] (schématiquement par manque de sommeil réparateur). Progressivement le comportement va être modifié vers des signes de dépression avec anxiété, agressivité envers l'entourage, pouvant aller jusqu'à de réels troubles dépressifs majeurs et une dépersonnalisation de la personne. Parallèlement la personne atteinte de douleur chronique peut se désocialiser (arrêts de travail itératifs, fin de droits…) tout en ayant éventuellement l'image de quelqu'un ayant acquis certains « bénéfices secondaires » durant la période de chronicisation de la douleur. Douleur cancéreuse Une autre forme de douleur dite chronique est la « douleur cancéreuse » qui est liée soit au cancer lui-même soit aux conséquences des traitements, qui peuvent induire des douleurs neuropathiques ou compressives en fonction du mécanisme.La forme la plus rare de douleur chronique est la douleur sine materia qui est un diagnostic d'élimination. C'est une douleur qui n'a pas d'origine organique apparente. Ce diagnostic ne devrait être évoqué que devant une douleur dont les explorations complémentaires morphologiques (IRM, TDM) et neurophysiologiques (électromyogrammes, électroneurogrammes, potentiels évoqués somesthésiques) sont et restent normales.Lors de l'examen médical des muscles, en particulier en médecine du sport, ces différents temps de l'examen permettent de faire la distinction entre les différentes pathologies possibles.Le médecin examinant recherchera par l'interrogatoire ainsi que par l'examen clinique à individualiser certains types particuliers de douleurs musculaires qui peuvent orienter vers leurs causes qui peuvent être des accidents sportifs, ou bien certaines maladies bien individualisées qui se manifestent par différentes types de douleurs musculaires.Si la douleur musculaire est présente à l'effort. L'arrêt de l'effort physique ou la baisse de son intensité fait diminuer ou disparaître la douleur. Elle est présente au repos, lorsque les muscles sont ""froids"". La palpation du muscle concerné provoque ou augmente la douleur : rictus douloureux sur le visage du sujet examiné, réaction de retrait. La contraction volontaire provoque ou augmente la douleur. L'étirement du muscle provoque ou augmente la douleur[réf. nécessaire].L'inflammation : la douleur inflammatoire est plus importante le soir et en début de nuit (lorsque le taux sanguin de cortisol naturel est au plus bas). Elle diminue ou disparaît après échauffement et à l'effort (activité professionnelle ou sportive) : douleur de dérouillage.La douleur mécanique est constante, ne diminue pas voire s'accentue à l'effort. Elle n'augmente pas le soir, ni en début de nuit, et diminue lorsque la mobilisation s'arrête.Certaines toxines bactériennes, végétales, fongiques ou animales (venins) peuvent être sources de vives douleursIl existe différents types de traitements tels que médicamenteux, chirurgicaux, psychologiques.Le traitement inadéquat de la douleur est très répandu à travers le domaine chirurgical et dans le domaine hospitalier et d'urgence en général,,,,,,. Cette négligence s'étend depuis toute époque. Les Africains et Latino-américains seraient les plus nombreux à souffrir entre les mains d'un médecin,; et la douleur chez les femmes est moins traitée que chez les hommes. Quant à la douleur chez l'enfant, et en particulier chez les plus petits, elle a été niée officiellement et scientifiquement jusqu’au milieu des années 1980, ces derniers étant régulièrement opérés sans anesthésie.La réaction à la douleur est utilisée pour évaluer l'état neurologique d'un patient, et notamment son état de conscience. Il fait partie du bilan des secouristes ainsi que de l'échelle de Glasgow. Si la victime n'a pas de réaction spontanée, ni au bruit ou au toucher, sa réaction à la douleur est testée. Il convient d'exercer une stimulation qui ne cause pas de blessure ni d'aggravation de l'état. Plusieurs méthodes peuvent être employées.Un pincement de la peau a longtemps été pratiqué ; celui-ci doit être évité. Sur une personne consciente, un léger pincement aux extrémités est utilisé (dos de la main ou dessus du pied, face interne du bras) pour vérifier si la personne ressent ce qui lui est fait, mais pas comme méthode de stimulation d'une personne sans réaction. Une pression avec les doigts sur l'arrière de la mâchoire inférieure (nomenclature internationale = mandibule), sous les oreilles et une pression appuyée au niveau sus-orbitaire.La douleur est la principale cause de visite dans les milieux hospitaliers dans 50 % des cas, est une pratique de visite présente dans 30 % des familles. De nombreuses études épidémiologiques de différents pays rapportent une prévalence élevée de douleur chronique présente chez 12-80 % de la population. Elle devient plus évidente à l'approche du décès chez les individus. Une étude de 4 703 patients affirme que 26 % des patients souffrant de douleurs durant les deux dernières années de leur vie, guérissent à 46 % le mois d'après.Une enquête de 6 636 enfants (âgés entre 0–18 ans) affirme que, sur 5 424 enfants interrogés, 54 % ont fait l'expérience de douleurs durant les trois derniers mois. Un quart d'entre eux rapportent qu'ils font l'expérience de douleurs présentes ou prolongées depuis trois mois voire plus, et un tiers d'entre eux rapportent qu'ils font l'expérience de douleurs fréquentes et intenses. L'intensité des douleurs chroniques était plus élevée chez les filles, et la douleur chronique augmente chez les filles âgées entre 12 et 14 ans.La perception de la douleur peut être augmentée ou diminuée selon le contexte et l'état d'esprit et/ou par certains médicaments. Sans médication, la nociception dépend fortement du type de douleur, du contexte et de la culture du patient. Dans un contexte rassurant, ou au contraire très difficile (situation de guerre, d'accident, de catastrophe) l'intensité de la douleur peut diminuer.Expérimentalement, on montre que la simple présence de plantes vertes dans une chambre diminue l’intensité perçue d'une douleur et de l'état psychologique du patient.Une hypothèse est qu'en fonction de la personnalité et du contexte, chacun peut plus ou moins dramatiser sa douleur (c'est-à-dire tendre à décrire son expérience de douleur en termes plus exagérés que ne le ferait une personne moyenne), ce qui influe durablement sur la perception de la douleur par le sujet qui modifi l'attention qu'il y porte, l'anticipation de la douleur attendue, en augmentant les réponses émotionnelles à cette douleur. La douleur n’est pas du tout considérée ni prise en compte de la même manière selon les cultures, les religions ou les époques. Chaque peuple a sa propre conception de la douleur, et plus généralement de la souffrance. Cette notion s’applique aussi bien aux bénéficiaires de soins qu’aux valeurs des soignants. En effet, « ce ne sont pas seulement les malades qui intègrent leur douleur dans leur vision du monde, mais également les médecins et les infirmières qui projettent leurs valeurs, et souvent leurs préjugés, sur ce que vivent les patients dont ils ont la charge. ».La prise en charge de la douleur peut s’expliquer par le fait que « (…) la pratique quotidienne d’actes douloureux oblige le soignant à mettre en œuvre un certain nombre de mécanismes de défense visant à le protéger, à le prémunir contre l’enlisement et la contamination par la souffrance de l’autre… ». Un aspect intéressant de l’écho que peut produire la douleur de l’enfant est noté chez le soignant : le déni. « Reconnaître, admettre la réalité de la douleur de l’enfant est un exercice difficile pour beaucoup d’équipes accueillant des enfants. D’autant que la non-reconnaissance de la douleur est plus facile chez l’enfant car ses moyens d’expression sont plus limités. » (…) « Ce déni est souvent le reflet d’un malaise chez les soignants, d’une incompréhension de l’attitude de l’enfant, d’un dysfonctionnement au sein d’un service. » . Dans les services[Lesquels ?], il est dit que : « Ce n’est pas de la douleur, c’est de la peur ou de l’anxiété… », ou bien : « C’est de la douleur mais il oubliera… », ou bien encore : « C’est dans la tête, c’est psychologique… ». Le déni de la réalité est un mécanisme de défense des soignants qui nient totalement une part plus ou moins importante de la réalité externe. « Le déni est un mécanisme psychologique où la personne réagit comme si sa pensée était toute puissante et qu’il suffisait de refuser la pensée d’une chose pour que cette chose n’existe pas. Mécanisme pathologique quand il est prévalent et rigide mais qui se retrouve sous une forme atténuée chez tout un chacun sous la forme : « il ne faut pas penser au malheur, à la mort, etc. » ; héritage de la pensée magique chez les jeunes enfants. Dans la relation de soin, ce déni se manifeste rarement de façon ouverte mais plutôt de manière inconsciente qui peut se traduire par la persistance d’attitudes nocives (le déni favorise les conduites à risque)… ».Il existe une autre notion qui peut entrer en ligne de compte dans ce déni des soignants face à la douleur de l’enfant : le concept d’amnésie infantile qui fait partie du développement psychologique de l’enfant. Il est vrai « que nous avons tous été des enfants ». Mais cette période de notre vie que nous avons tous en commun est recouverte « d’un voile d’étrangeté », peu, voire aucun souvenir de cette époque ne nous revient consciemment à la mémoire. « Qu’il est donc difficile de comprendre ce que veut, ce que cherche, ce que demande un enfant ! » : cela explique cette facilité des soignants à ne pas prendre en compte la douleur de l’enfant qu’il soigne, ne se souvenant pas eux-mêmes de ce qu’ils ont ressenti et vécu à cette période de leur vie. Un autre concept intéressant concernant le vécu de la douleur par les soignants est le transfert. Les soignants adultes résistent mieux à la douleur en général, et donc transfèrent leurs ressentis et leurs émotions sur la personne qu’ils soignent. Ils pensent que l’enfant supporte la douleur de la même façon qu’ils le feraient[réf. nécessaire].Pousser un juron peut également avoir un effet anti-douleur.Il existe une association internationale pour l'étude de la douleur (International Association for the Study of Pain ou IASP), basée à Seattle puis à Washington. Elle soutient la recherche dans ce domaine, publie une lettre mensuelle et a notamment publié une nouvelle classification des douleurs chroniques pour permettre aux chercheurs et cliniciens traitant la douleur d'utiliser un vocabulaire commun, codifié et approuvé, incluant une taxonomie des formes de la douleur et leurs abréviations (en 1986, actualisé en 1994 2011). Cette classification inclut des syndromes douloureux régionaux complexes (SDRC) et des sections spécialisées sur la douleur abdominale, pelvienne, et urogénitales (révisée en 2012).Les connaissances concernant la nociception et la douleur chez les animaux invertébrés sont encore très fragmentaires.L'une des méthodes pour repérer la douleur chez les humains est de poser une question : une personne peut exprimer une douleur qui ne peut être détectée par des mesures physiologiques connues. Cependant, comme chez les nourrissons, les animaux non-humains ne peuvent poser de question sur ce qu'ils ressentent ; ainsi les critères définis aux humains ne peuvent être attribués aux animaux. Les philosophes et scientifiques se sont penchés sur ces difficultés d'expression. René Descartes, par exemple, explique que les animaux manquent de conscience et font l'expérience d'une douleur différente de celle ressentie par les humains. Bernard Rollin de l'Université d'État du Colorado, principal auteur de deux lois fédérales concernant la douleur animale, rédige que les chercheurs, durant les années 1980, restaient incertains concernant l'expérience de la douleur ressentie par les animaux, et que les vétérinaires, formés aux États-Unis avant 1989, apprenaient à ignorer la douleur chez les animaux. Lors de ses discussions avec des scientifiques et autres vétérinaires, il lui était demandé de « prouver » que les animaux sont conscients et de fournir des preuves « scientifiquement acceptables » qui permettraient de mettre en avant la douleur animale. Carbone rédige que la perception dans laquelle les animaux souffrent différemment des humains reste peu répandue. La capacité des espèces invertébrées chez les animaux, telles que les insectes, à ressentir la douleur et la souffrance reste également incertaine,.La présence de la douleur chez les animaux reste incertaine pour quelques-uns, mais elle peut être repérée à l'aide de réactions comportementales ou physiques. Les spécialistes croient actuellement que tout animal vertébré peut ressentir la douleur, et que certains invertébrés, comme la pieuvre, le peuvent également,. Quant aux autres animaux, plantes et autres entités, la capacité physique à ressentir la douleur reste une énigme dans la communauté scientifique, car aucun mécanisme par lequel la douleur peut être ressentie n'a été détecté. En particulier, il n'existe aucun nocicepteur connu dans des groupes tels que les plantes, champignons et la plupart des insectes.L'évaluation relève parfois de la gageure. Suivant l'espèce animale et le type de douleur, l'évaluation peut être relativement facile ou impossible.En général, les douleurs chroniques sont silencieuses et se manifestent par des troubles fonctionnels plus ou moins marqués (position antalgique, comportements d'évitement, irritabilité, anorexie et parfois apathie). Les douleurs aiguës sont plus visibles et faciles à mettre en évidence par une palpation-manipulation appropriée.Il existe des grilles de notations pour certaines affections et espèces mais elles sont surtout employées en recherche.La douleur animale a longtemps été négligée pour diverses raisons : sous médicalisation de plusieurs espèces, un sondage Insee a donné il y a quelques années un taux de médicalisation d"
médecine;"L'endocrinologie est une discipline de la médecine qui étudie les hormones. Son nom signifie : l'étude (logos) de la sécrétion (crine) interne (endo). Elle étudie de très nombreux phénomènes physiologiques, car les hormones interviennent dans de nombreuses fonctions chez de nombreux organismes dont l'Homme :la nutrition :les hormones régulatrices de la glycémie comme l'insuline et le glucagon,la leptine qui régule les réserves de graisses dans l'organisme,la ghréline qui stimule l'appétit, et la PYY-36 qui donne une sensation de satiété pendant plusieurs heures ;la croissance, avec les différentes hormones de croissance ;la reproduction : la puberté, mais aussi les cycles menstruels de la femme, la grossesse et la lactation (fabrication de lait) ;la régulation de la température corporelle, avec les hormones thyroïdiennes ;la régulation des cycles circadiens avec la mélatonine.Chez certaines autres espèces animales, les hormones ont encore d'autres effets : le changement de sexe chez certains poissons, le changement de comportement chez les abeilles, la mue chez certains insectes (régulée par l'ecdysone)…Les hormones sont un moyen pour l'organisme de communiquer des informations en son sein, grâce principalement à des molécules transportées par le sang.L'endocrinologie a commencé en Chine au IIe siècle av. J.-C. Les Chinois isolaient à des fins thérapeutiques des hormones sexuelles et pituitaires à partir de l'urine humaine. Ils ont utilisé des méthodes nombreuses et complexes, comme la sublimation des hormones stéroïdiennes. Une autre méthode décrite par des textes chinois, le plus ancien datant de 1110, évoque l'utilisation de saponine, issue de fèves de Gleditsia sinensis (en), pour extraire les hormones. Le gypse, contenant du sulfate de calcium, a également été utilisé.La plupart des tissus et des glandes du système endocrinien avaient déjà été identifiés par les premiers anatomistes. Mais les Grecs anciens et penseurs romains, tels Aristote, Hippocrate, Lucrèce, Celse, Galien, etc. ont privilégié la théorie humorale pour expliquer le fonctionnement biologique et la maladie. Cette approche est restée dominante jusqu'au XIXe siècle, où la théorie microbienne, la physiologie et les bases de la physiopathologie se sont développées.En 1849, Arnold Berthold remarque que la crête et les barbillons ne se développent pas chez les coqs castrés et qu'ils adoptent un comportement moins masculin. Il constate que le fait de réintroduire des testicules dans la cavité abdominale du même coq ou d'un autre coq castré permet un développement comportemental et morphologique normal. Il en conclut que les testicules sécrètent une substance, transportée par le sang, qui agit sur la physiologie du coq. Mais deux autres hypothèses sur l'action des testicules étaient également envisageables : la modification ou l'activation d'un constituant du sang, ou bien le retrait d'un facteur inhibiteur contenu dans le sang. L'existence d'une substance qui engendre les caractéristiques masculines a été démontrée ultérieurement avec de l'extrait de testicules sur des animaux castrés. Puis la testostérone cristalline pure a été isolée en 1935.La maladie de Basedow reprend le nom d'un médecin allemand, Carl von Basedow, qui a décrit un cas de goitre avec exophtalmie en 1840. Cependant, des rapports antérieurs sur cette maladie avaient déjà été publiés par le médecin irlandais Robert James Graves, avec la même constellation de symptômes en 1835, par les Italiens Giuseppe Flajani et Antonio Giuseppe Testa, en 1802 et 1810 respectivement, et par le médecin anglais Caleb Hillier Parry (un ami d'Edward Jenner) à la fin du XVIIIe siècle. Thomas Addison a été le premier à décrire la maladie d'Addison en 1849.En 1902, William Bayliss et Ernest Starling ont observé dans une expérience que l'instillation d'un acide dans le duodénum provoquait un début de sécrétion du pancréas, et ce même après avoir supprimé toutes les connexions nerveuses entre les deux. Le même résultat pouvait être obtenu par l'injection d'un extrait de muqueuse du jéjunum dans la veine jugulaire, ce qui montrait que certains facteurs contenus dans cette muqueuse en étaient la cause. Ils ont appelé cette substance ""sécrétine"" et ont inventé le terme d'""hormone"" pour désigner les facteurs chimiques qui agissent de cette façon.Joseph von Mering et Oskar Minkowski ont observé en 1889 que le retrait du pancréas conduit à une augmentation de la glycémie, suivi d'un coma et enfin de la mort, symptômes de diabète sucré. En 1922, Frederick Banting et son assistant Charles Best ont remarqué que l'homogénéisation d'un pancréas et l'administration par injection de l'extrait qui en dérive améliore au contraire l'état du sujet. La mystérieuse hormone responsable de cet effet, l'insuline, n'a été séquencée par Frederick Sanger qu'en 1953.Les neurohormones ont été identifiés pour la première fois par Otto Loewi en 1921. Il a fait incuber le cœur d'une grenouille (innervé, avec son nerf vague) dans un bain d'eau salée, et a laissé reposer la solution pendant un certain temps. Un second cœur non innervé était ensuite introduit dans cette solution. Si le nerf vague du premier cœur était stimulé, l'activité inotrope (amplitude de battement) et chronotrope (fréquence cardiaque) était observable sur les deux cœurs. Si le nerf vague n'était pas stimulé, aucun des deux cœurs ne réagissait. L'effet pouvait être bloqué en utilisant de l'atropine, un inhibiteur connu de la stimulation du nerf vague. De toute évidence, quelque chose était sécrété par le nerf vague dans la solution saline et agissait sur les deux cœurs. Ce qui provoquait cet effet régulateur (myotropique) a été identifié plus tard: l'acétylcholine et la noradrénaline. Loewi a reçu un prix Nobel pour cette découverte.Des travaux plus récents en endocrinologie se sont axés sur les mécanismes moléculaires responsables du déclenchement des effets produits par les hormones. Earl Sutherland a mené les premiers travaux sur ce sujet en 1962, pour savoir si les hormones pénètrent dans les cellules pour provoquer une action, ou si elles restent en dehors. Il a étudié la noradrénaline, qui agit sur le foie pour convertir le glycogène en glucose grâce à l'activation de l'enzyme phosphorylase. Il a homogénéisé le foie en deux fractions, l'une membranaire et l'autre soluble (le phosphorylase est soluble), puis il a ajouté de la noradrénaline à la fraction membranaire, en a extrait les produits solubles, et les a ajoutés à la première fraction soluble. Le phosphorylase s'est activé, ce qui indiquait que le récepteur à noradrénaline cible se trouvait sur la membrane cellulaire et non dans la cellule. Il a plus tard nommé ce composé ""AMP cyclique"" (AMPc). Cette découverte a provoqué la naissance du concept de médiation par un messager secondaire. Il a reçu également un prix Nobel.À la fin du XXe siècle apparaissent des recherches sur les perturbateurs endocriniens, des molécules d'origine parfois naturelle mais le plus souvent artificielle — plastifiants, détergents, pesticides, médicaments, phénols, PCB, dioxines, agents ignifuges bromés, plomb, mercure, etc. — qui imitent ou altèrent le fonctionnement des hormones et qui peuvent avoir un effet néfaste sur la physiologie, le fonctionnement cognitif, et les capacités reproductives des espèces animales, y compris l'espèce humaine. Certains travaux montrent une augmentation récente et rapide de troubles d'origine endocrinienne liés au mode de vie et à la pollution de l'environnement,,.L'hypothalamusL'hypophyseL'épiphyseLa thyroïdeLes parathyroïdesLe thymusLe pancréasLes surrénalesLes ovairesLes testiculesLes revues scientifiques ci-dessous sont les principales revues spécialisées en endocrinologie clinique et fondamentale (en anglais) :EndocrinologyDiabetesJournal of Clinical Endocrinology and MetabolismMolecular EndocrinologyJournal of NeuroendocrinologyEndocrine ReviewsEuropean Journal of Endocrinology : document utilisé comme source pour la rédaction de cet article.Jean-Didier Vincent La biologie des passions. Un exposé clair du fonctionnement hormonal et de son importance dans la détermination des passions. L'homme humoral en quelque sorte distinct de l'homme neuronal.Jean Gautier (docteur), L'enfant ce glandulaire inconnu, 1961. Exposé des différentes étapes du développement endocrinien.Emmanuelle Lecornet - Sokol et Caroline Chaminadour, Et si c'était hormonal ?, Hachette, 20 février 2019, 240 p. (ISBN 978-2017077725)Robert Temple (trad. de l'anglais), Le génie de la Chine : 3 000 ans de découvertes et d'inventions, Arles, P. Picquier, 2007, 288 p. (ISBN 978-2-87730-947-9) Chirurgie endocrinienneCentre scientifique d'endocrinologie« Minimum vital » en endocrinologie (polycopié de la Pitié-Salpêtrière) Portail de la médecine"
médecine;"Un examen médical est une procédure de diagnostic réalisée pour des motifs de santé. Par exemple :pour diagnostiquer des maladiespour mesurer la progression, la régression ou la guérison des maladiespour confirmer chez quelqu'un l'absence de maladieQuelques-uns se composent d'un simple examen physique, appelé également examen clinique : il ne requiert que de simples instruments entre les mains d'un médecin, et peuvent être réalisés dans son cabinet. D'autres requièrent un équipement plus sophistiqué et/ou l'usage d'un environnement stérile.Quelques examens requièrent des tests sur échantillons de tissu ou des liquides corporels qui seront envoyés à un laboratoire médical pour analyse. Quelques tests chimiques simples (comme la mesure du pH de l'urine) peuvent être mesurés directement dans le cabinet du médecin. Parfois la possibilité d'un faux-négatif ou faux positif doit être confirmé par une mise en culture à partir d'échantillons.Certains examens peuvent également être effectués sur une personne morte dans le cadre d'une autopsie.Les examens médicaux peuvent être classés en deux catégories :examen invasifexamen non invasif.En règle générale, le premier requiert une effraction de la peau plus importante qu'une simple ponction veineuse, peut être désagréable, nécessite parfois une anesthésie locale  ou générale, peut  nécessiter une hospitalisation et comporte un certain nombre d'effets secondaires, voire, de risque d'accident. La distinction entre ces deux types d'examen n'est pas si tranchée dans un certain nombre de cas.Il existe également des examens ionisants. Généralement les examens de radiologie.La compilation de l'ensemble des données de l'examen médical constitue le dossier médical qui peut être manuscrit ou informatisé.Tout examen clinique débute par un interrogatoire du patient, permettant de déterminer :les antécédents :personnels : anciennes maladies, anciens examens,familiaux : à la recherche de maladies héréditaires,chez la femme : antécédents gynéco-obstétricaux, utilisation ou non d'une méthode contraceptive en faisant préciser laquelle (cette information est importante pour éviter de prescrire certains examens ou traitements contre-indiqués en cas de grossesse débutante).le motif de la consultation ;les symptômes actuels et  leur évolution, retraçant ainsi l' histoire de la maladie ;les traitements actuels et passés, médicaux et chirurgicaux, y compris les vaccinations ;le mode de vie : prise ou non de toxiques (tabac, alcool…), travail, situation familiale, prise en charge par un organisme de sécurité sociale et/ou une mutuelle.L'examen clinique comprend  classiquement 4 phases : l'inspection, la palpation, la percussion et l'auscultation.L'inspection : le médecin regarde le patient déshabillé.La palpation recherche d'éventuels points douloureux, masses anormales d'adénopathies (gros ganglions), augmentation de volume de certains organes comme le foie ou la rate… Elle permet également la ""prise du pouls"" du patient, habituellement au niveau de l'artère radiale, afin de mesurer la fréquence cardiaque, de dépister une anomalie du rythme cardiaque et de reconnaitre d'éventuelles anomalies de la pulsatilité artérielle (abolie, faible ou au contraire exagérée).La percussion permet de détecter d'éventuelles modifications au sein du thorax ou de l'abdomen (anormalement ""mat"" ou ""tympanique"")L'auscultation, peut être pratiquée de façon ""immédiate"", le médecin collant son oreille directement sur le corps du patient, ou ""médiate"" par l'intermédiaire d'un stéthoscope (cas le plus fréquent). Elle permet d'analyser les bruits provoqués par certains organes ou appareils : le cœur, l'appareil respiratoire, l'appareil digestif et les vaisseaux dans diverses localisations (crâne, cou, abdomen, aines...).Il est en règle générale complété par :la détermination du poids et de la taillela mesure de la pression artérielleEt de façon moins systématique par : des tests respiratoires ;une étude des examen des réflexes ;un examen de la vue :ophtalmoscopie (ophtalmoscope) ;un examen de l'ouïe ;un toucher rectal ;un examen du vagin et du col de l'utérus chez la femme.Voir articles : biopsiesponction lombaireVoir articles :examens sanguins, dontvitesse de sédimentation(hémogramme)test de dépistage du VIH/SIDA (le Western Blot est un test de détection d'anticorps spécifiques du VIH dans le sang, par une technique d’électrophorèse spéciale)tests urinairestest d'ADN, test d'ARNanalyse des sellesgaz sanguinsVoir articlesmicrobiologieVoir articlesradiologielavement barytéurographie intra-veineuseultrasonséchographiedopplerÉchographie Dopplerélectrocardiographie (ECG)électroencéphalographie (EEG)scannertomographie à émission de positronimagerie par résonance magnétique (RNM, IRM)IRM fonctionnelleendoscopiecoloscopiecystoscopiesigmoïdoscopiehystéroscopiecœlioscopiecolposcopieexamen de l'appareil respiratoirepléthysmographie Portail de la médecine"
médecine;"La glande thyroïde ou thyroïde est une glande endocrine régulant, chez les vertébrés, de nombreux systèmes hormonaux par la sécrétion de triiodothyronine (T3), de thyroxine (T4) et de calcitonine. Dans l'espèce humaine, elle est située à la face antérieure du cou, superficiellement.Ses déformations (on parle de goitre quand le volume de la thyroïde est augmenté) sont visibles sous la peau. Elle peut être le siège de diverses affections : hyperthyroïdie, hypothyroïdie, tumeur maligne ou tumeur bénigne. On peut l'étudier grâce à l'échographie et à la scintigraphie.La thyroïde, moulée sur l'axe trachéo-laryngé, est de consistance ferme, de couleur rosée, et pèse de 25 à 30 grammes généralement mais en cas de goitre sa masse peut augmenter jusqu'à 100-150 grammes. Elle est entourée d'une capsule avasculaire (ou gaine viscérale péri-thyroïdienne) qui lui est propre et qui est différente de la loge thyroïdienne.La thyroïde se compose de deux lobes droit et gauche situés verticalement de part et d'autre du larynx. Une partie intermédiaire horizontale, l'isthme thyroïdien, forme un pont entre les deux lobes. Généralement, la glande thyroïde répond aux 2e et 3e anneaux trachéaux ; mais elle peut avoir une position haute : 1er et 2e anneaux trachéaux, ou une position basse : 3e et 4e anneaux trachéaux. Les deux lobes ont un sommet supérieur, ainsi qu'une grande base inférieure. On leur décrit trois faces : médiale, postérieure et antéro-latérale. Sa hauteur est d'environ 6 cm pour une longueur de 6  à   8 cm. On trouve souvent entre les deux lobes, donc au niveau de l'isthme, le lobe pyramidal de Lalouette, souvent déporté vers la gauche : c'est un reliquat du canal thyréoglosse.Il existe des variations morphologiques, s'expliquant par l'embryologie : en effet les deux lobes sont parfois éloignés l'un de l'autre sans qu'il n'y ait d'isthme, ou au contraire peuvent être soudés donnant une thyroïde en forme de V. Provenant d'un bourgeon de cellules endodermiques naissant près de la racine de la langue, différentes positions de la glande thyroïde peuvent cependant survenir durant l'ontogenèse : une mauvaise migration de cette ébauche conduit alors à la détection de cette glande (fonctionnelle ou non fonctionnelle) dans la région linguale, cervicale, voire endo-thoracique.La thyroïde présente les rapports anatomiques suivants :ventralement : muscles cervicaux superficielslatéralement : nerfs récurrents et axes vasculaires jugulo-carotidiensdorsalement : larynx au pôle supérieur et trachée cervicale au pôle inférieurLes quatre parathyroïdes ont des positions variables, mais se situent généralement aux quatre pôles thyroïdiens.La thyroïde est un organe richement vascularisé. En effet on retrouve :Deux artères principales :artère thyroïdienne supérieure, première branche de l'artère carotide externe ; elle se divise en 3 branches (latérale, médiale et postérieure) une fois la glande atteinte.artère thyroïdienne inférieure, naissant du tronc thyro-cervical, branche collatérale de l'artère subclavière. Se divise également en trois branches (mêmes situations) dans la thyroïde.Dans de très rares cas il est possible qu'une 3e artère vienne vasculariser la thyroïde dans sa portion basse appelée artère de Neubauer qui est une branche de la crosse de l'aorte.Les deux artères principales de la thyroïde sont anastomosées ; l'ATS droite avec l'ATS gauche et l'ATI droite, et l'ATI droite avec l'ATS droite et l'ATI gauche.Il existe néanmoins d'autres artères, moins volumineuses, inconstantes, naissant directement de l'arc aortique. Par exemple l'artère thyroïdea ima vascularisant la partie isthmique. Celle-ci est présente chez environ 5 à 10 pour cent des sujets et peut provoquer une hémorragie en cas de trachéotomie.Trois veines principales :veine thyroïdienne supérieure, résultant de la confluence de trois veines dans la glande, et formant avec les veines linguale et faciale le tronc thyro-lingo-facial qui se jette dans la veine jugulaire interne.veine thyroïdienne moyenne, réunion de plusieurs branches pas très volumineuses se jetant dans la veine jugulaire interne.veine thyroïdienne inférieure, formée par la confluence de trois veines dans la glande et se jetant dans le tronc veineux brachio-céphalique.De même que pour les artères, certaines veines accessoires vascularisant préférentiellement l'isthme vont rejoindre les troncs veineux brachio-céphaliques droit et gauche.Chez nos mammifères domestiques, on parle de 2 glandes thyroïdes (droite et gauche), car contrairement à l'homme, les 2 lobes thyroïdiens ne sont pas réunis par un isthme.  Chez les autres vertébrés, la glande thyroïde est diffuse, formée de groupes dispersés de follicules, et situés latéralement à des distances variables de l'œsophage.Chez les agnathes et la plupart des téléostéens (poissons), les groupes de follicules se distribuent sur toute la partie ventrale de la tête.Malgré cette diversité morphologique, la structure histologique folliculaire de la thyroïde est hautement conservée chez tous les vertébrés, ce qui témoigne d'un processus original et commun de production hormonale.L'unité morpho-fonctionnelle de la glande thyroïde est le follicule thyroïdien (ou vésicule thyroïdienne), composé d'un épithélium unistratifié de cellules folliculaires (les thyréocytes), produisant les hormones thyroïdiennes, disposées autour d'une lumière centrale contenant la colloïde : la colloïde est principalement constituée du précurseur des hormones thyroïdiennes, la thyroglobuline. Le follicule thyroïdien est un véritable piège à iode (ion iodure), élément rare à la surface de la terre, et indispensable au fonctionnement de l'organisme; l'iode sera ainsi capté et stocké dans la colloïde : la biosynthèse des hormones thyroïdiennes pourra alors  se dérouler, l'iode venant se coupler à la thyroglobuline ; la thyroglobuline iodée est ensuite réintégrée dans le follicule thyroïdien, et sécrétée dans le courant sanguin.Le follicule thyroïdien, en dehors d'une majorité de cellules folliculaires, contient 1 à 2 % de cellules dites parafolliculaires (ou cellules C, ou cellules claires), produisant la calcitonine : elles n'ont cependant jamais de contact avec la colloïde.On trouve aussi des amas de cellules (ilots de Woffler), cellules jointives pouvant se transformer en vésicule thyroïdien.La thyroïde est issue de trois ébauches :deux ébauches latérales issues du 4e sillon branchial interne et qui forme une partie des lobes latérauxet une ébauche centrale issue de l'évagination du pharynx buccal constituant ainsi le tractus thyréoglosse qui forme l'isthme ainsi  que la majeure partie des lobes latéraux. Rappelons que le tractus thyréoglosse est l'axe de migration de la thyroïde chez l'embryon.La thyroïde sécrète :la T3 ou triiodothyronine en très faible quantité ;la T4 ou thyroxine ;la calcitonine intervenant dans le métabolisme du calcium.La production de ces hormones est régie par la thyréostimuline (TSH, « thyroid-stimulating hormone »), produite par l'hypophyse et nécessite un apport en iode. La plus grande production de la T3 est obtenue par la conversion de la T4 au niveau du foie, pour la plus grosse quantité et les intestins pour le reste. La thyroïde ne produit, elle, de la T3 directement que pour à peine 10 à 20 %.De par sa position superficielle, la thyroïde est explorée en premier lieu par une échographie cervicale, qui recherchera des nodules ou un goitre. L'image ultrasonore permet d'évaluer le volume de la thyroïde ; à l'échelle du diagnostic individuel ou de population (suivi épidémiologique).La tomodensitométrie avec injection de produit de contraste iodé est peu utilisée, généralement dans le cadre du bilan pré-opératoire des goitres volumineux, notamment des goitres plongeants.La scintigraphie thyroïdienne à l'iode 123 est un examen fonctionnel. L'injection d'un traceur d'iode radioactif mettra en évidence des zones du parenchyme plus ou moins actives, et permettra la distinction entre un nodule hypersécrétant et un nodule « froid ».Un bilan thyroïdien standard comporte le dosage de la TSH et de la T3 ou de la T4. La calcitonine n'est pas systématiquement dosée.Les dysthyroïdies peuvent avoir des origines génétiques, être liées à des carences nutritionnelles en iode, mais aussi être induites par des toxiques (plomb, ou iode radioactif par exemple - on parle alors de « thyrotoxicoses »). Hypothyroïdie Situation d'imprégnation insuffisante de l'organisme en hormones thyroïdiennes, le plus souvent à cause d'un mauvais fonctionnement de la glande thyroïde.Les symptômes de l'hypothyroïdie découlent d'un ralentissement métabolique général : fatigue, difficultés de concentration, troubles de la mémoire, frilosité, myxœdème, prise de poids malgré un appétit stable voire diminué, diminution de la pilosité avec perte de cheveux ou cheveux devenant cassants, éclaircissement des sourcils, sécheresse ou épaississement cutané, pâleur, crampes musculaires, fourmillement ou engourdissement des extrémités, inappétence, tendance à la dépression, insomnies, tendance à la constipation.L'examen clinique recherche une augmentation de la taille de la thyroïde qui peut être importante (goitre), un ralentissement de la fréquence cardiaque, la bradycardie, et parfois, de la tachycardie ou des symptômes ressemblant à ceux de l'hyperthyroïdie.Le traitement est une substitution journalière à vie en hormones thyroïdiennes, par voie orale. Hyperthyroïdie Symptomatologie due à un excès de production d'hormones thyroïdiennes :cardio-vasculaire : tachycardie, éréthisme cardio-vasculaire (frémissement du choc de la pointe du cœur) ;digestif : syndrome polyuro-polydipsique (boit et urine en grande quantité), amaigrissement, diarrhée, flatulences ;neuro-psy : tremblement, agitation, trouble de l'humeur (irritabilité allant à la dépression), trouble du sommeil, trouble du comportement alimentaire (mange en quantité excessive, perte de poids) ;généraux : hypersudation (mains souvent moites, transpiration), hyperthermie, thermophobie (température élevée et n'apprécie pas les températures élevées) ;musculaire et articulaire : douleur et fatigue musculaire, ostéoporose, augmentations des glandes lactogènes.Le tabac multiplie par dix le risque de survenance de la maladie de Basedow, la forme la plus fréquente de l'hyperthyroïdie, et augmente les risques de complications. Thyroïdites La thyroïdite est une inflammation de la glande thyroïde.Il existe plusieurs types de thyroïdites: Maladie d'Hashimoto La cause la plus fréquente d’hypothyroïdie.Elle consiste en une destruction de la glande thyroïde causée par des taux d’anticorps antithyroïdiens anormalement élevés dans le sang et des globules blancs. La thyroïde ne sécrète alors plus suffisamment d’hormones thyroïdiennes.Cette maladie nécessite donc très souvent un supplément hormonal.Pour confirmer le diagnostic, il faut réaliser une prise de sang qui dosera les hormones thyroïdiennes et la capacité du corps à les gérer (T4, T3, TSH), ainsi que les auto anticorps thyroïdiens (AC anti TPO, et AC anti thyroglobuline).  Thyroïdite du post-partum La thyroïdite du post-partum peut survenir dans l'année qui suit un accouchement. Dans ce cas, la glande a tendance à récupérer, et le traitement de remplacement des hormones thyroïdiennes n’a besoin d’être administré que durant quelques semaines. Une évolution vers une hypothyroïdie permanente est possible. Thyroïdite silencieuse Elle porte ce nom, car elle n’entraîne aucun signe ni symptôme d’inflammation de la thyroïde. De prime abord, le patient présente une hyperthyroïdie pouvant donner lieu aux mêmes symptômes que ceux de la maladie de Basedow-Graves, qui laisse place à une phase d’hypothyroïdie aboutissant à une guérison complète. La présence d’anticorps antithyroïdiens comparables à ceux décelés au cours de la maladie de Hashimoto est un facteur de risque de persistance de l’hypothyroïdie. Thyroïdite subaiguë dite de Quervain Il s’agit d’une forme passagère de thyroïdite provoquant une hypothyroïdie. La thyroïdite subaiguë serait causée par une infection virale, car la majorité des patients atteints ont présenté une infection de la gorge dans les semaines précédant son apparition. Cette affection se manifeste sous forme d’épidémies de faible envergure et est généralement associée à des infections virales connues. Dépression et thyroïde L'hypothyroïdie peut parfois être confondue avec un état de dépression et l'hyperthyroïdie pour un état d'excitation. Le diagnostic thyroïdien permettra d'éliminer ces faux diagnostics.Cependant des travaux récents ont montré qu'une hypothyroïdie traitée uniquement avec de la thyroxine peut davantage encore se rapprocher d'un état dépressif. Plutôt que de prescrire un antidépresseur, le médecin peut parfois proposer une simple substitution d'une partie de la dose de thyroxine (T4) par de la tri-iodo-thyronine (T3) ou de flavinine (substitut générique du B52).Un goitre est une thyroïde globalement augmentée de volume. Il est dit toxique lorsqu'il sécrète des hormones thyroïdiennes de façon excessive, entraînant une hyperthyroïdie. Les goitres sont rarement homogènes et le plus souvent multinodulaires ; chaque lobe thyroïdien présente un nombre important de nodules bénins de volume variable.Les goitres peuvent être en situation cervicale normale ; ils sont dits « plongeants » lorsque le pôle inférieur d'au moins un lobe pénètre dans le médiastin à travers l'orifice supérieur du thorax. Plongeants ou non, les goitres peuvent être (rarement) compressifs, lorsque le volume trop important de la thyroïde comprime les organes de voisinage, principalement la trachée, mais aussi l'œsophage et parfois étirant les nerfs récurrents, entraînant alors une paralysie de la corde vocale homolatérale.Les tumeurs de la thyroïde se présentent sous la forme d'un nodule thyroïdien, qui peut être bénigne (adénome de la thyroïde) ou maligne (carcinome de la thyroïde). Tumeurs bénignes Adénome vésiculaire de la thyroïdeAdénome oncocytaire de la thyroïdeLipoadénome de la thyroïdeAdénome à cellules claires de la thyroïdeAdénome vésiculaire à cellules en bague à chaton de la thyroïdeAdénome vésiculaire mucosécrétant de la thyroïdeAdénome vésiculaire à noyaux bizarres de la thyroïdeAdénome vésiculaire atypique de la thyroïde Tumeurs malignes Il existe deux types histologiques principaux de cancers thyroïdiens :carcinomes différenciés folliculairescarcinomes différenciés papillaires (le plus fréquent, 80 % des cas)La thyroïde est généralement abordée par une cervicotomie médiane, qui peut être élargie latéralement en cervicotomie en U s'il existe une nécessité de curage ganglionnaire cervical. En cas de volumineux goitre plongeant, un refend cutané en Y en regard de l'extrémité crâniale du sternum sera souvent pratiqué. Au maximum, une simple manubriotomie (on parle alors de cervicomanubriotomie) ou une sternotomie médiane pourra être pratiquée.livret d'information ""Cancer de la thyroïde"" ; Institut Gustave Roussy.ARC, Monographie Volume 79 (2001) Some Thyrotropic AgentsFini J.B et Demeneix B (2019) Les perturbateurs thyroïdiens et leurs conséquences sur le développement cérébral. Biologie Aujourd’hui, 213(1-2), 17-26.Remaud S et Demeneix B (2019) Les hormones thyroïdiennes régulent le destin des cellules souches neurales. Biologie Aujourd’hui, 213(1-2), 7-16 (résumé). Portail de l’anatomie   Portail de la physiologie"
médecine;"Les hormones thyroïdiennes, c'est-à-dire la thyroxine (T4) la triiodothyronine (T3) et la diiodothyronine (T2), sont des hormones produites par les cellules folliculaires de la thyroïde à partir de la thyroglobuline et d'iodure. Il existe également la thyrocalcitonine, hormone produite par les cellules parafolliculaires de la thyroide et qui joue un rôle dans le métabolisme phospho-calcique. La thyrocalcitonine induit une hypocalcémie et une hypophosphoremie. Celles-ci sont produites majoritairement sous forme de T4. La T4 agit comme une prohormone, relativement peu active, qui est convertie en T3, plus active. La conversion de la T4 en T3 a lieu dans les cellules cibles, sous l'effet d'une enzyme, la thyroxine 5'-désiodase.Les hormones thyroïdiennes sont essentielles à la croissance et au développement corrects, à la multiplication et à la différenciation de toutes les cellules de l'organisme, notamment dans le système nerveux central, le squelette et les bourgeons dentaires. À divers degrés, elles régulent le métabolisme basal des protéines, des lipides et des glucides. Toutefois, c'est sur l'utilisation des composés riches en énergie que leur impact sur les cellules est le plus prononcé. Elles ont également un effet permissif sur l'action d'autres hormones et de neurotransmetteurs.De nombreux stimuli physiologiques et pathologiques influent sur la synthèse des hormones thyroïdiennes. L'hyperthyroïdie est le syndrome clinique causé par un excès de thyroxine libre ou de triiodothyronine libre circulante, ou des deux. Une carence en iode provoque une augmentation de la taille de la thyroïde, d'où l'apparition d'un goitre, en réponse au ralentissement de la biosynthèse des hormones thyroïdiennes.Les hormones thyroïdiennes sont biosynthétisées dans la thyroïde. Cette biosynthèse est stimulée indirectement par l'hormone thyréotrope (TRH, de l'anglais : thyrotropin-releasing hormone), un tripeptide de structure (pyro)Glu–His–Pro–NH2 synthétisé par l'hypothalamus. La TRH induit la synthèse de la thyréostimuline (TSH, de l'anglais : thyroid-stimulating hormone) par l'anté-hypophyse, lobe antérieur de l'hypophyse. La TSH agit en augmentant l'expression du gène de la thyroperoxydase (TPO, de l'anglais : thyroid peroxidase).La thyroïde est très vascularisée. Les cellules de la thyroïde sont organisées en follicules autour de vésicules thyroïdiennes qui contiennent une substance gélatineuse qu'on appelle généralement la colloïde. Ces cellules sont orientées, c’est-à-dire qu'elles possèdent un pôle apical du côté de la colloïde et un pôle basal du côté des vaisseaux sanguins. Le noyau des cellules folliculaires est relativement actif, la présence d'un réticulum endoplasmique rugueux, riche en ribosomes, démontre une forte activité de biosynthèse des protéines, et l'appareil de Golgi est lui-même très actif car on peut observer de nombreuses vésicules au pôle apical.Les cellules folliculaires permettent l'échange de molécules entre le sang et la colloïde. Le sang fournit les acides aminés nécessaires à la synthèse, dans le réticulum de ces cellules, de la thyroglobuline (Tg), une protéine dimérique de 660 kDa qui contient environ 120 résidus de tyrosine. La thyroglobuline passe ensuite dans l'appareil de Golgi pour être internalisée dans les vésicules, lesquelles fusionnent avec la membrane apicale des cellules folliculaires en libérant la thyroglobuline dans la colloïde par exocytose.L'iode absorbé par l'alimentation est présent dans l'organisme sous forme d'anions iodure I?, qui sont concentrés dans les cellules folliculaires à partir du sang à l'aide du symport Na/I (NIS), qui utilise le gradient électrochimique en cations sodium Na+ pour accumuler les ions I?. Ces derniers traversent ensuite la membrane apicale grâce à la pendrine, qui joue le rôle d'antiport Cl?/I?, pour rejoindre la colloïde, où ils sont oxydés par la thyroperoxydase (TPO) à l'aide de peroxyde d'hydrogène H2O2 pour former du diiode I2 susceptible de réagir directement avec les résidus de tyrosine de la thyroglobuline : ceux-ci peuvent être iodés une fois pour former des résidus de monoiodotyrosine (MIT), ou deux fois pour former des résidus de diiodotyrosine (DIT).La condensation de deux résidus de DIT donne — outre un résidu d'alanine — un résidu de thyroxine (T4), tandis que la condensation d'un résidu de MIT sur un résidu de DIT donne un résidu de triiodothyronine (T3) ; la condensation d'un résidu de DIT sur un résidu de MIT donne en revanche un résidu de 3,3',5'-triiodothyronine (rT3 ou « T3 inverse »), qui est biologiquement inactive.La thyréostimuline (TSH) se lie au récepteur de la TSH, un récepteur couplé à la protéine Gs, ce qui provoque l'endocytose de fragments de colloïde dans des vésicules qui fusionnent avec des lysosomes. Les hormones thyroïdiennes T4 et T3 sont libérées par digestion de la colloïde par des peptidases, à raison de seulement cinq ou six molécules d'hormone thyroïdienne libérées par molécule de thyroglobuline digérée, le ratio étant d'environ une molécule de T3 pour 20 molécules de T4.En raison de leur caractère lipophile, la T4 et la T3 sont transportées dans le sang en étant liées à des protéines telles que les globulines liant la thyroxine (TBG, des glycoprotéines qui fixent préférentiellement la T4), la transthyrétine (TTR, une autre glycoprotéine, qui ne transporte pratiquement que la T4) et des albumines sériques, qui ont une affinité relativement faible pour les T3 et T4 mais sont abondantes et donc contribuent significativement à leur diffusion dans l'organisme.Les molécules de T4 et T3 présentes dans l'organisme sont généralement liées à une protéine transporteuse, seuls les 0,03 % libres de T4 et les 0,3 % libres de T3 étant biologiquement actives. Ce mode de transport a pour effet d'accroître la demi-vie des hormones thyroïdiennes dans le sang — environ 6,5 jours pour la T4 et 2,5 jours pour la T3 — et de réduire la vitesse à laquelle elles sont absorbées dans les tissus. C'est la raison pour laquelle la mesure de la concentration en hormones thyroïdiennes libres, désignées par T4L et T3L, revêt une grande importance clinique, tandis que la concentration totale, incluant les hormones liées aux protéines transporteuses, n'est pas significative.Malgré leur nature lipophile qui devrait leur permettre de passer les membranes cellulaires, les hormones T3 et T4 ne diffusent pas passivement à travers la bicouche de phospholipides de la membrane plasmique des cellules cibles, et font appel pour cela à des transporteurs membranaires spécifiques.Parmi les deux hormones thyroïdiennes, la prohormone T4 doit en fait être désiodée en T3 par une thyroxine 5'-désiodase dans les cellules cibles pour être pleinement active : la T3 est typiquement entre trois et cinq fois plus active que la T4, qui sert in fine essentiellement au transport de cette hormone dans le sang. Il existe deux isozymes de cette iodothyronine désiodase :le type 1 (D1), présent dans le foie, les reins, la thyroïde et, dans une moindre mesure, l'hypophyse, dont le rôle exact dans l'organisme n'est pas entièrement compris ;le type 2 (D2), présent dans l'hypophyse, le muscle squelettique, le cœur (artères coronaires), le système nerveux central et le tissu adipeux brun, responsable de l'essentiel de la formation de T3 dans la thyroïde, mais capable également de désioder la 3,3',5'-triiodothyronine, ou T3 inverse, en 3,3'-diiodothyronine, ou T2.Pour mémoire, il existe également un troisième type d'iodothyronine désiodase, la thyroxine 5-désiodase (D3), qui convertit respectivement la T4 et la T3, qui sont biologiquement actives, en T3 inverse et en T2, biologiquement inactives, ce qui a pour effet d'inactiver globalement les hormones thyroïdiennes.Une fois dans le cytoplasme, les hormones thyroïdiennes se lient aux récepteurs des hormones thyroïdiennes, qui sont des récepteurs nucléaires. Les récepteurs thyroïdiens se lient, sur l'ADN des cellules cibles, à des éléments de réponse des promoteurs de certains gènes dont ils régulent la transcription. Ces récepteurs thyroïdiens conditionnent la sensibilité relative des différents tissus aux hormones thyroïdiennes.Les hormones thyroïdiennes agissent sur l'organisme pour augmenter le métabolisme de base, agir sur la biosynthèse des protéines et rendre le corps plus réceptif aux catécholamines (telles l'adrénaline, d'où l'intérêt des bêta-bloquants dans l'hyperthyroïdie). L'iode est un composant important dans leur synthèse.Les hormones thyroïdiennes accélèrent le métabolisme de base et, par conséquent, accroissent la consommation de l'organisme en énergie et en oxygène. Elles agissent sur presque tous les tissus, hormis la rate. Elles accélèrent le fonctionnement de la pompe sodium-potassium et, d'une manière générale, raccourcissent la demi-vie des macromolécules endogènes en activant leur biosynthèse et leur dégradation.Les hormones thyroïdiennes stimulent la production de l'ARN polymérase I et II, et, par conséquent, augmentent l'activité de biosynthèse des protéines. Elles augmentent également la vitesse de dégradation des protéines, et, lorsqu'elles sont trop abondantes, la dégradation des protéines peut être plus rapide que leur biosynthèse ; dans ce cas, le corps peut tendre vers un équilibre ionique négatif.Les hormones thyroïdiennes potentialisent les effets des récepteurs adrénergiques ? sur le métabolisme du glucose. Par conséquent, elles accélèrent la dégradation du glycogène et la biosynthèse du glucose par la néoglucogenèse[réf. nécessaire].Les hormones thyroïdiennes stimulent la dégradation du cholestérol et augmentent le nombre de récepteurs de LDL, ce qui accélère la lipolyse.Les hormones thyroïdiennes accélèrent le rythme cardiaque (Chronotrope positif - Tachycardie) et accroissent la force des systoles (Inotrope positif), augmentant ainsi le débit cardiaque à travers une augmentation du nombre de récepteurs adrénergiques ? dans le myocarde et aussi de la sensibilité aux catécholamines. Il en résulte une augmentation de la pression artérielle systolique et une diminution de la pression artérielle diastolique.Les hormones thyroïdiennes ont un effet profond sur le développement de l'embryon et les nourrissons. Elles affectent les poumons et influencent la croissance postnatale du système nerveux central. Elles stimulent la production de myéline, de neurotransmetteurs, et la croissance des axones. Elles sont également importantes dans la croissance linéaire des os.Les hormones thyroïdiennes peuvent accroître le taux de sérotonine dans le cerveau, en particulier au niveau du cortex cérébral, et inhiber les récepteurs 5-HT2, comme l'ont montré des études sur la réversibilité, sous l'effet de la T3, de comportements d'impuissance apprise chez des rats, et des études physiologiques de cerveaux de rats[pas clair] .Les hormones thyroïdiennes sont prescrites dans les cas d'hypothyroïdie ou de thyroïdectomie (ablation chirurgicale de la glande thyroïdienne). Les hormones thyroïdiennes utilisées sont la T3 et la T4. L'hormone T3 est plus efficace que l'hormone T4 au niveau des récepteurs mais la T4 est transformée en T3 par les tissus périphériques et un traitement par la T4 permet d'avoir un taux normal de T3. La demi-vie de la T3 n'est que de 24 heures et elle nécessiterait deux à trois prises quotidiennes, alors que la demi-vie de la T4 est de 6 à 8 jours et autorise une seule prise quotidienne, ce qui explique son utilisation préférentielle.Le diagnostic du fonctionnement thyroïdien se fait en médecine nucléaire par injection d'iode 123 (isotope radioactif de l'iode) produit dans un cyclotron. Sa période radioactive est relativement faible puisqu'elle est de 13,21 h (c'est-à-dire 13 heures, 12 minutes et 36 secondes). Sa désintégration radioactive émet des rayons ? d'énergie caractéristique équivalent à 159 keV et 27 keV. La dose injectée pour le diagnostic ne dépasse pas les 10 mégabecquerels (MBq).La thyroxine (T4) a été isolée par l'Américain Edward Calvin Kendall en 1910 à partir de trois tonnes de thyroïde de porc, tandis que la triiodothyronine (T3) a été découverte en 1952 par le Français Jean Roche. Portail de la biologie   Portail de la chimie   Portail de la médecine"
médecine;"L'hyperthyroïdie (appelée aussi dans des cas très prononcés — graves et rares — thyréotoxicose ou thyrotoxicose) est le syndrome clinique causé par un excès de thyroxine libre circulante (FT4) ou de triïodothyronine libre (FT3), ou les deux. Chez les humains, les causes principales sont la maladie de Basedow (cause la plus fréquente : 70-80 % des cas), l'adénome toxique de la thyroïde, le goitre multinodulaire toxique, et la thyroïdite sub-aiguë.La glande thyroïde, stimulée par la TSH (thyroid-stimulating hormone), secrète deux hormones, la thyroxine (= tétraiodothyronine) ou T4 et la triiodothyronine (T3). La première est une prohormone, transformée en la seconde qui constitue la forme active.L'hyperthyroïdie consiste en l'augmentation des taux de T3 et de T4 dans le sang. Si cette hypersecrétion est secondaire à une maladie de la thyroïde (ce qui est vrai dans la quasi-totalité des cas), la TSH est effondrée (par rétrocontrôle)L'incidence annuelle est de 0,6 pour 1 000 femmes. Elle est quatre fois moindre chez les hommes. La prévalence aux États-Unis est de 1,3 %.La cause la plus fréquente chez le sujet jeune est la maladie de Basedow et chez le sujet âgé, le nodule toxique ou le goitre multinodulaire, surtout si l'apport iodé de la nourriture est pauvre. Les thyroïdites, entraînant le relargage d'hormones thyroïdiennes à la suite de la destruction cellulaire, comptent pour 10 % des hyperthyroïdies. Les autres causes sont rares.C'est la première cause d'hyperthyroïdie en termes de fréquence. Elle est plus fréquente chez la femme jeune. On retrouve de manière non constante un souffle à l'auscultation de la glande thyroïde qui est augmenté de volume, un discret gonflement des parties molles de la jambe (myxœdème prétibial) ou des globes oculaires légèrement proéminents (exophtalmie). Le diagnostic est fait en présence de TSI (Thyroid stimulating immunoglobulins) dans le sang des patients. La structure de cette TSI est proche de celle de la TSH et stimule ainsi la production d'hormones thyroïdiennes par la glande.Le nodule toxique de Plummer est évoqué devant le nodule isolé de la glande thyroïde qui peut parfois être palpé et surtout, par la fixation d'iode radioactif de ce dernier de manière exclusive à la scintigraphie thyroïdienne, le reste de la glande n'étant plus visualisé. Il devient une cause importante d'hyperthyroïdie chez la personne âgée. Son traitement demande l'éradication du nodule, que cela soit par chirurgie ou par iode radioactif.Elle peut être :infectieuse (thyroïdite de De Quervain dans un contexte grippal) ou post opératoire ;auto-immune comme lors de la thyroïdite de Hashimoto avec la présence d'anticorps anti-TPO ;survenir après un accouchement (assez fréquente puisqu'elle concerne jusqu'à 10 % des parturientes, le plus souvent très discrète et guérissant sans séquelle).Elle évolue parfois vers une hypothyroïdie (diminution des hormones thyroïdiennes) régressive.La scintigraphie montre alors l'absence totale de fixation de l'iode radioactif (scintigraphie blanche).Parmi les autres causes possible, on distingue :le goitre multinodulaire : le goitre est révélé à l'examen clinique de la glande, il peut être suffisamment important pour causer des compressions des structures adjacentes. La fonctionnalité des nodules est affirmée par la scintigraphie thyroïdienne. Le traitement est essentiellement chirurgical : l'utilisation d'iode radioactif peut faire disparaître l'hyperthyroïdie clinique mais ne parvient pas, en règle générale, à faire diminuer le goitre ;l'association d'une maladie de Basedow et de nodules fonctionnels (syndrome de Marine-Lenhart) ;le cancer de la thyroïde évolué ;l'adénome hypophysaire à TSH ;la prise d'hormone thyroïdienne en quantité trop élevée ;effet secondaire de la prise de certains médicaments, surtout du fait de leur richesse en iode dans le principe actif ou les excipients : antiseptiques contenant de l'iode (polyvidone), produits de contraste de radiologie, etc. L'amiodarone peut donner également des hypothyroïdies. L'hyperthyroïdie de l'amiodarone est plus fréquente dans les régions avec apports iodés insuffisants. Elle impose l'arrêt de ce médicament lorsque c'est possible, en sachant que sa demi-vie prolongée (plus de 100 jours) fait que l'imprégnation en médicament va persister très  longtemps.La plupart des signes restent non spécifiques ou peuvent être discrets. La sévérité des signes est corrélée avec les taux hormonaux. Ils sont toutefois plus frustes chez la personne âgée.L'hyperthyroïdie peut se manifester par tout ou partie des signes ci-dessous.Une perte de poids malgré un appétit conservé ou accru (polyphagie).Une prise de poids dans environ 10 % des cas.Une chaleur ressentie comme insupportable (thermophobie).Une polydipsie, soif excessive.Une asthénie, fatigue, à l'instar de l'hypothyroïdie, pouvant avoir comme conséquence des troubles de l'érection dans la moitié des cas, chez l'homme, réversible sous traitement.Une fréquence cardiaque élevée (tachycardie) avec des palpitations ou des extrasystoles auriculaires.Un essoufflement (dyspnée) ;Un pouls irrégulier pouvant correspondre à une fibrillation auriculaire, cette dernière pouvant être présente même en cas d'hyperthyroïdie dite sub-clinique.Des tremblements fins des extrémités, conséquence de l'excès de circulation sanguine rapide du sang (Attention, ce tremblement n'est pas d'origine neurologique !).Le tout peut se compliquer soit :d'une insuffisance cardiaque typiquement à haut débit, régressive le plus souvent après normalisation des hormones thyroïdiennes mais pouvant aboutir à des séquelles dans un tiers des cas ;de douleurs thoraciques pouvant évoquer une angine de poitrine.Diarrhée chronique.Nausées ou vomissements.Il existe une diminution de la force musculaire (myopathie endocrinienne) avec parfois diminution de la taille des muscles (atrophie musculaire).La maladie peut se présenter sous forme de dépression ou irritabilité.Dans les formes graves, l'hyperthyroïdie peut entraîner un coma, des mouvements anormaux sous forme de chorée, des troubles du comportement pouvant ressembler à une psychose.Peau luisante, chaude et humide.Démangeaison isolée.Plusieurs symptômes sont décrits :impuissance ;augmentation de la taille des seins (gynécomastie) ;infertilité ;absence totale ou partielle de menstruations.L'hyperthyroïdie, même modérée (dite sub-clinique) peut se compliquer d'une décalcification osseuse (ostéoporose secondaire).Certaines thyrotoxicoses peuvent ainsi faciliter un saturnisme inattendu, via une contamination de l'organisme par relargage du plomb antérieurement stocké dans les os. Dans ce dernier cas, l'augmentation de la plombémie est accompagnée d'une augmentation du taux sérique d'ostéocalcine qui reflète l'augmentation du remodelage osseux qui accompagne souvent l'hyperthyroïdie.Inversement ou en retour le plomb pourrait affecter la thyroïde en inhibant la captation d'iode, phénomène d'abord observé chez l'animal puis confirmé chez l'Homme dans les années 1960,, y compris dans un cas d'intoxication saturnine liée à la présence d'une balle en plomb non extraite de l'organisme.On observe une élévation de l'hormone TSH ou une chute de la thyroxine sérique et libre, en cas d'exposition chronique et plutôt quand la plombémie dépasse 60 µg/100 mL.Diminution de la concentration de cholestérol sanguin (hypocholestérolémie).Anémie (diminution de la concentration d'hémoglobine dans le sang).Neutropénie (diminution du nombre de polynucléaire neutrophiles sanguin).Selon la cause de l'hyperthyroïdie on observe un goitre, un nodule thyroïdien, une hypertrophie thyroïdienne...Le diagnostic est établi par un examen sanguin : mesure du taux de TSH dans le sang. Un taux effondré de TSH est spécifique d'une hyperthyroïdie périphérique (l'immense majorité des hyperthyroidies, secondaire à une atteinte de la thyroïde). Le diagnostic est confirmé par une mesure du taux de T3 libre et T4 libre sanguin que l'on retrouve augmenté. L'augmentation de ces deux hormones peut cependant être dissociée avec des cas rares d'hyperthyroïdie à T3, la T4 étant normale. Si la TSH est basse et la T4 et T3 sont normales sur des dosages répétées, on parle d'« hyperthyroïdie infraclinique ».Une fois le diagnostic fait, il reste à rechercher la cause. Il est indispensable de doser les anticorps spécifiques (Anticorps anti-récepteur de la TSH, anti-thyroglobuline, anti-thyropéroxydase « anti-TPO ») et de réaliser un examen d'imagerie de la thyroïde : échographie (par ultrasons) ou  scintigraphie (par injection d'un isotope radioactif qui se fixe sur la glande thyroïde et dont le rayonnement est détecté par une caméra à scintillations). Ces examens précisent l'aspect de la glande et la répartition géographique de son activité (fixation à la scintigraphie).La prise en charge de l'hyperthyroïdie a fait l'objet de la publication de recommandations par l'American Thyroid Association en 2011.Le choix du traitement dépend de la cause, de la sévérité et du terrain.Un traitement d'urgence est l'ingestion de solution saturée d'iodure de potassium (SSKI), un fort taux d'ion iodure permettant de stopper temporairement la sécrétion de thyroxine par la thyroïde.Elle consiste en l'ablation de la totalité ou d'une grande partie de la glande thyroïdienne. La chirurgie se doit de respecter les glandes parathyroïdes de petite taille et situées en arrière de la thyroïde. Elle doit également passer en dehors du nerf récurrent qui remonte en arrière de la glande. La section de ce nerf peut entraîner un changement de la voix (dysphonie) du fait qu'il innerve les cordes vocales.Dans le cas de l'hyperthyroïdie, une radiothérapie métabolique peut être prescrite. Il s'agit de l'ingestion d'iode 131 radioactif qui va se fixer sur la glande thyroïde et la détruire. Ce traitement n'est proposé qu'à certaines formes de maladie de Basedow et est naturellement inefficace en cas de non fixation de l'iode sur la glande (scintigraphie blanche). Elle expose à un risque d'hypothyroïdie (comme la chirurgie par ailleurs) qui est facilement traitée par la prise d'hormones thyroïdiennes.Il s'agit de  médicament inhibant la production d'hormones thyroïdiennes, comme le méthimazole (alias thiamazole), le carbimazole ou le propylthiouracile. Le délai d'efficacité peut être long. Deux techniques de prescription peuvent être utilisés : la titration (ou recherche de la dose minimale efficace) ou la prescription à doses importantes conduisant à une hypothyroïdie et à une substitution hormonale associée.En cas de douleurs, il est possible de donner un antalgique et un antipyrétique en cas de fièvre.Les bêta-bloquants ralentissent le cœur et diminuent les palpitations ainsi que les tremblements.Elle est définie par un taux bas de TSH et un taux normal de T4 libre et de T3 totale. Plus de la moitié des hyperthyroïdies sont sub cliniques et leur prévalence serait de 0,7 % aux États-Unis.Ce syndrome est associé avec un risque majoré d'ostéoporose chez la femme âgée mais aussi de maladies cardiovasculaires, de mortalité cardiaque et de fibrillation auriculaire.Un traitement systématique d'emblée n'est pas recommandé : une surveillance régulière du taux des hormones doit être faite et le traitement débuté à l'élévation de ces dernières.C'est l'une des maladies hormonales les plus fréquentes chez le chat, souvent provoquée par une tumeur bénigne (non cancéreuse) de la thyroïde. Cette maladie a été décrite pour la première fois dans les années 1970. Portail de la médecine"
médecine;"L'hypothyroïdie est une situation pathologique d'imprégnation insuffisante de l'organisme en hormones thyroïdiennes (normalement produites par la glande thyroïde). Les symptômes, d'intensité variable, sont notamment une fatigue, une somnolence, une frilosité, une constipation, une prise de poids, une pâleur cutanée, une raideur musculaire, des œdèmes (« myxœdème »).Elle peut se compliquer d'insuffisance cardiaque ou de dépression et classiquement lorsque l'évolution est avancée, d'un coma myxœdémateux. In utero et chez le nouveau-né elle peut entraîner un retard mental (autrefois dénommé « crétinisme »).La thyroïde étant sous le contrôle de la glande hypophysaire, les causes de l'hypothyroïdie relèvent de deux mécanismes principaux. Elle est dite primitive lorsque seule la thyroïde est atteinte, et secondaire lorsque c'est l'hypophyse qui est atteinte. Le diagnostic d'hypothyroïdie est établi par une prise de sang qui montre un taux anormalement bas d'une part de la thyroxine (taux normal dans les formes frustes) et d'autre part de thyréostimuline (TSH), dans des proportions variant selon mécanisme. L'hypothyroïdie est fréquente et le plus souvent d'origine primaire, d'expression fruste et affectant une femme. Les causes sont alors généralement une carence en iode, une pathologie auto-immune (telles que la thyroïdite de Hashimoto ou la thyroïdite atrophique), ou médicamenteuse (par exemple en rapport avec l'amiodarone). La plupart des hypothyroïdies secondaires sont dues à des tumeurs de la région hypophysaire comprimant la glande, ou à des séquelles locales de chirurgie ou de radiothérapie.Le traitement est celui de la cause, lorsqu'il est possible. Le traitement substitutif de l'hypothyroïdie est la lévothyroxine, prescrite par un médecin et dont la surveillance est à la fois clinique (signes d'hypothyroïdie et d'hyperthyroïdie) et biologique (dosage de TSH). La prévention repose en premier lieu sur la supplémentation alimentaire en iode dans les zones déficitaires, notamment sous la forme de sel iodé.Cette maladie affecte plus souvent les femmes, surtout après 50 ans ; et des personnes qui ont des antécédents personnels ou familiaux de maladie de la thyroïde ou de maladie auto-immune (diabète de type 1, maladie cœliaque, etc.), et les femmes qui ont enfanté au cours de l’année.Son incidence est de 0,3 % chez la femme et sa prévalence est de près de 3 % de la population (étude réalisée dans la population anglaise).La grossesse peut causer une affection auto-immune transitoire de la glande thyroïde. L’hypothyroïdie peut alors survenir dans l’année suivant un accouchement (auquel cas elle dure de 6 à 12 mois en moyenne).Les causes sont multiples. La grande majorité est représentée par l'hypothyroïdie primaire, autrement dit un dysfonctionnement au niveau de la glande thyroïde même. L'hypothyroïdie secondaire est due à un dysfonctionnement de l'hypophyse qui secrète alors en quantité insuffisante la TSH ou « hormone de stimulation de la thyroïde ». Enfin, cas très rare, l'hypothyroïdie peut être due à une résistance périphérique aux hormones thyroïdiennes.À l’origine, l’hypothyroïdie était due essentiellement à une carence en iode. Depuis l’ajout de l’iode dans le sel de table, cette cause est devenue rare dans les pays industrialisés (mais reste fréquente dans les pays en voie de développement).En 1986, la catastrophe nucléaire de Tchernobyl rappelait au monde les dangers du nucléaire. Mais le lien entre cette pollution et l'augmentation des cancers de la thyroïde n'est toujours pas établi. Un nouveau rapport officiel dresse un premier bilan 20 ans après la catastrophe.[réf. souhaitée]carence chronique en iode ;thyroïdite qu'elle soit auto-immune (thyroïdite de Hashimoto) ou infectieuse (thyroïdite de De Quervain) ;cause iatrogène (thyroïdectomie au radioiode, effets secondaires de certains médicaments comme l'amiodarone, l'hypothyroïdie pouvant être transitoire, cédant à l'arrêt du traitement, ou définitive) ;maladie infiltrative (sarcoïdose, amyloïdose, hémochromatose…) ;trouble enzymatique de la thyroïde (génétique) ;dysgénésie thyroïdienne congénitale ;consommation d'aliments goitrogènes.insuffisance hypophysairelésion hypothalamiquedéficit en TSHC'est un syndrome rare caractérisé par des anomalies des récepteurs tissulaires des hormones thyroïdiennes qui altèrent la liaison entre l'hormone et sa cible.Cette anomalie est partiellement compensée par une hyperproduction hormonale, ce qui limite l'hypothyroïdie.Les premiers signes d'une hypothyroïdie sont souvent asymptomatiques et très légers. Cette affection peut être associée a un grand nombre de symptômes. Ils peuvent être dus à une pathologie causale sous-jacente à l'hypothyroïdie, à un effet de masse dû à un goitre, ou directement à la carence en hormones thyroïdiennes. Les signes sont listés ci-après,,.Dans les formes débutantes, on peut rencontrer des signes :généraux : ongles cassants, peau sèche, démangeaisons, gain de poids, rétention d'eau,,, myxœdème, chute de cheveux ;neuropsychiques : dépression, pensées incessantes et rapides, insomnie ;neuromusculaires : réflexes déprimés, hypotonie, des crampes musculaires, douleurs articulaires, l'instabilité de l'humeur, l'irritabilité, constipation ;métaboliques : fatigue, somnolence, frilosité et intolérance au froid, sudation diminuée, ralentissement métabolique général ;cardiovasculaires : bradycardie ;endocriniens : infertilité (féminine), règles irrégulières et galactorrhée.Lorsque la maladie est plus évoluée, peuvent exister des signes :généraux : amincissement des sourcils (signe de Hertoghe), dessèchement de la peau du visage ;neuropsychiques : élocution ralentie et dysphonie, migration de la voix dans les graves à cause de l'œdème de Reinke (pseudomyxome des cordes vocales) ;neuromusculaires : syndrome du canal carpien et paresthésie bilatérale (fourmillement, picotements, engourdissement des extrémités tactiles) ;métaboliques : hypothermie ;cardiovasculaires : hypotension ;endocriniens : goître (dépendant de la cause de l'hypothyroïdie), baisse de la libido principalement chez les hommes en raison d'une insuffisance de synthèse de la testostérone testiculaire.Plus rarement, peuvent être présents des signes :généraux : jaunissement de la peau causé par une déficience de conversion du bêta-carotène en vitamine A (caroténodermie), visage/mains/pieds bouffis, grossissement du volume de la langue ;neuropsychiques : troubles de la mémoire, psychose aiguë/schizophrénie dysthymique[réf. nécessaire], déficit d'attention, diminution du sens du goût et de l'odorat, surdité ;neuromusculaires : difficultés à avaler, essoufflement avec un rythme respiratoire profond et lent. Examens généraux Il peut exister une hyperlipémie, plus rarement, une hypoglycémie.Il peut exister une anémie due à une anomalie de synthèse de l'hémoglobine en rapport avec la diminution des niveaux d'érythropoïétine (EPO), la diminution de l'absorption du fer et du folate au niveau intestinal, ou à une carence en vitamine B12 consécutivement à l'anémie.Il peut exister une altération de la fonction rénale avec diminution du débit de filtration glomérulaire. Examens spécifiques Le diagnostic repose sur le dosage de la TSH qui est augmentée dans les formes primaires (de loin les plus courantes).Le dosage des hormones thyroïdiennes montre des taux bas, mais peut être normal dans les formes débutantes.La recherche d'anticorps anti-peroxydase et anti-thyroglobuline est utile afin de détecter un mécanisme auto-immun.Échographie du cou.Pas de scintigraphie thyroïdienne en cas d'hypothyroïdie.Les formes graves peuvent aller jusqu'à des troubles de la conscience, voire un coma, appelé « coma myxœdémateux ». L'hypothyroïdie peut également se compliquer d'une péricardite, d'épanchements pleuraux.L'hypothyroïdie, même fruste, semble être corrélée avec le risque de survenue de maladies cardio-vasculaires. La correction de celle-ci n'entraîne cependant pas une modification du risque.Le diagnostic est affirmé par un dosage hormonal guidé par la clinique. La T4 est abaissée sauf en cas de forme fruste (avec peu de symptômes). La TSH peut être basse ou augmentée selon la cause. En pratique seul le dosage de la TSH est réalisé dans un premier temps, et c'est en cas d'anomalie de celui-ci ou de forte conviction clinique que le dosage de T4 est fait.Le contexte clinique peut orienter vers une cause. Cependant, le dosage des anticorps (anti-thyroperoxydase voire anti-thyroglobuline) est souvent réalisé, une échographie n'est quant à elle, et contrairement au bilan d'hyperthyroidie,  pas nécessaire au bilan initial.La plupart du temps, le traitement doit faire appel à une substitution journalière en hormones thyroïdiennes, par voie orale. Il s'agit d'un traitement à vie qui exige un suivi médical impliquant également un dosage annuel de la TSH.Le médicament le plus utilisé est la lévothyroxine qui doit être donnée à doses progressives chez le patient âgé ou porteur d'une maladie cardiaque. Il entraîne la normalisation du taux de TSH, mais qui peut être différée de plusieurs mois.L'ajout d'iode dans le sel de cuisine est une mesure efficace pour prévenir la carence en iode, en particulier dans les pays en voie de développement.L'hypothyroïdie congénitale est dépistée systématiquement à la naissance en France, ce qui permet, avec un traitement approprié, d'éviter une affection autrefois dénommée crétinisme. Ce dépistage a été mis en place en 1975.Le dépistage d'une hypothyroïdie chez l'adulte est conseillé en France :pour les femmes de plus de 60 ans avec antécédent de pathologie thyroïdienne ;pour toute personne avec antécédent de traitement à risque : chirurgie, radiothérapie, médicament (amiodarone, lithium, interféron).(en) Cet article est partiellement ou en totalité issu de l’article de Wikipédia en anglais intitulé « hypothyroidism » (voir la liste des auteurs).HyperthyroïdieThyroxineThyroïdite d'HashimotoRessources relatives à la santé : ICD-10 Version:2016 Orphanet (en) Diseases Ontology (en) DiseasesDB (sv) Internetmedicin (en) Medical Subject Headings (en + es) MedlinePlus (en) NCI Thesaurus (no + nn + nb) Store medisinske leksikon (cs + sk) WikiSkripta Association française des malades de la thyroïde(en) « Hypothyroidism Booklet », sur American Thyroid Association, 2003 Portail de la médecine"
médecine;"L'immunologie est la branche de la biologie qui s'occupe de l'étude du système immunitaire. Apparu très tôt au cours de l'évolution, ce système a évolué pour distinguer le non-soi du soi. Les réactions de défense de l'organisme face à un organisme pathogène — quelle que soit la nature de celui-ci, virus, bactéries, champignons ou protozoaires. Les maladies auto-immunes, les allergies et le rejet des greffes forment l'aspect médical de cette science. Les mécanismes de synthèse et de maturation des anticorps, d'activation du système du complément, la mobilisation et la coordination des cellules de défense, en forment l'aspect fondamental et mécanistique.Les plus anciens témoignages connus d’observations d’ordre immunologique datent de 430 av. J.-C. lorsque l’historien Thucydide relata un épisode de « peste ». À cette date, pendant l’épidémie de fièvre typhoïde qui sévit à Athènes durant la guerre du Péloponnèse, Thucydide nota que seules les personnes ayant déjà supporté et survécu à l’infection étaient aptes à s’occuper des malades.Aux alentours de 6000 av. J.-C., il existe en Chine des pratiques de transmission volontaire de la variole en vue de prévention. Cette technique, appelée « variolisation », consiste à prélever du pus sur un malade peu atteint par la maladie pour l’inoculer avec une aiguille chez un sujet sain. Ce procédé se répandit à partir du XVe siècle, surtout en Chine, en Inde et en Turquie. Par l’entremise de l’épouse de l’ambassadeur britannique à Constantinople, qui fit vacciner son fils de cette manière, la variolisation s’est fait connaître en Angleterre vers 1722, puis s’est propagée dans les années suivantes dans toute l’Europe.À la même époque, le médecin de campagne Edward Jenner constatait que les fermières en contact régulier, lors de la traite, avec la variole de la vache (vaccine ou Cowpox), qui est inoffensive pour les humains, étaient épargnées par les épidémies de variole, alors fréquentes, ou ne montraient que de faibles symptômes. Après avoir intensivement étudié le phénomène, il préleva le 14 mai 1796 du pus sur une pustule d’une jeune fille contaminée par la vaccine, et l’injecta à un jeune garçon de huit ans. Après que le garçon eut guéri de la maladie bénigne induite par la vaccine, Jenner lui injecta de la variole véritable. Le garçon surmonta également cette infection sans symptômes sérieux. Par rapport à la variolisation, le procédé de Jenner offrait certains avantages majeurs : les personnes vaccinées par la vaccine ne présentaient pas les boutons et les cicatrices typiques induites par la variolisation ; il n’y avait aucun risque de mortalité contrairement à la variolisation ; et les personnes vaccinées ne représentaient aucun risque de contagion. Le virus de la vaccine est la l’origine des noms de « vaccin » et « vaccination », et Edward Jenner est considéré aujourd’hui comme le fondateur de l’immunologie.Une autre étape majeure dans le développement de l’immunologie est la conception d’un vaccin contre la rage par Louis Pasteur en 1885. Le 6 juillet 1885, il vaccine Joseph Meister, un garçon de neuf ans qui avait été mordu deux jours plus tôt par un chien enragé. Joseph Meister devint alors le premier être humain à survivre à la rage dans l’histoire de la médecine. En une année, le vaccin fut administré à 350 personnes contaminées, et aucune ne mourut de son infection rabique.Deux ans auparavant, Robert Koch avait découvert le responsable de la tuberculose, le bacille qui porte son nom, et peu de temps après, le test à la tuberculine, qui permet de prouver l’infection par la tuberculose, et qui se fonde sur la réponse immunitaire. Ces travaux servirent de base aux travaux de Calmette et Guérin, qui décrivirent le bacille qui porte leur nom (BCG pour bacille de Calmette et Guérin) et menant à la vaccination contre la tuberculose. Le vaccin permettant de lutter contre les maladies infectieuses se développa à partir de cette époque. Max Theiler reçu le prix Nobel de médecine en 1951 pour la mise au point d’un vaccin contre la fièvre jaune.En 1888, Émile Roux et Alexandre Yersin ont découvert la toxine diphtérique. Deux ans plus tard, Emil Adolf von Behring et Shibasaburo Kitasato mettent en évidence une antitoxine dans le sérum des patients qui avaient survécu à la diphtérie. Emil von Behring fut le premier à utiliser ces anti-sérums pour la prise en charge des malades diphtériques dans le cadre de la séroprophylaxie. Pour ces travaux, il reçut en 1901 le prix Nobel de physiologie ou médecine.Le bactériologue belge Jules Bordet découvre en 1898 que chauffer le sérum au-dessus de 55 °C bloque sa capacité de coller à certaines substances chimiques. La capacité du sérum à tuer les bactéries était également perdue. Il posa le postulat suivant : il existe dans le sérum une substance, sensible à la chaleur, nécessaire à l’action du sérum sur les bactéries, et il nomma ce composé « Alexin ». Ehrlich étudia ce composé dans les années suivantes, et introduisit le concept de complément encore utilisé de nos jours.Au début du XXe siècle, la recherche en immunologie prend deux directions distinctes. L’immunologie humorale, dont les principales figures étaient Paul Ehrlich et Emil Adolf von Behring, partait du principe que la base de la défense contre les infections devait se trouver dans une substance contenue dans le sérum, comme les antitoxines. Cette théorie prédomina vers les années 1900 et pendant plusieurs dizaines d’années. En parallèle, et à partir des années 1883/1884, se développa le point de vue de l’immunité cellulaire, qui se base sur les travaux de George Nuttall ainsi que Ilya Ilitch Metchnikov. Metchnikov put prouver l’implication et l’importance de l’action des cellules du corps dans la lutte contre les pathogènes en étudiant l’action des globules blancs sur des bactéries. Ses travaux sur la phagocytose lui valurent le prix Nobel de médecine en 1908, conjointement avec Paul Ehrlich. Comme il sera montré plus tard, ces deux types de phénomènes sont les deux facettes de l’action du système immunitaire et de la réponse immunitaire. Il fallut cependant attendre les années 1940 pour que l’hypothèse de l’immunité cellulaire soit généralement reconnue, et que l’hypothèse selon laquelle les anticorps seraient les acteurs principaux de la réponse immunitaire soit abandonnée.En 1901, Karl Landsteiner mit en évidence l’existence des groupes sanguins et par cette découverte permit de franchir une nouvelle étape importante dans la compréhension du système immunitaire. Il reçut en 1930 le prix Nobel de médecine. En 1906, Clemens Peter Freiherr von Pirquet observa que les patients à qui il administrait du sérum de cheval avaient une forte réaction à la deuxième injection. Il nomma cette réaction d’hypersensibilité « allergie ». Le phénomène d’anaphylaxie fut découvert par Charles Richet, qui reçut pour cela le prix Nobel de médecine en 1913. Emil von Dungern et Ludwik Hirszfeld publient en 1910 leurs recherches sur la transmission des groupes sanguins, et ainsi les premiers résultats sur la génétique d’une partie du système immunitaire. Dans ce travail, ils proposent la nomenclature « ABO », qui deviendra un standard international en 1928. En 1917, Karl Landsteiner décrit le concept d’haptènes, qui après s’être conjuguées à une protéine sont capables d’induire une réponse immunitaire avec production d’anticorps spécifiques. Lloyd Felton réussit en 1928 la purification des anticorps à partir du sérum. De 1934 à 1938, John Marrack développa la théorie de la reconnaissance spécifique d’un antigène par un anticorps.En étudiant le rejet de greffes, Peter Gorer découvrit l’antigène H-2 de la souris, et ainsi, sans le savoir, le premier antigène de ce qu’on appellera ensuite le complexe majeur d'histocompatibilité (MHC pour l’anglais major histocompatibility complex). Toujours par l’étude du rejet de greffe, Peter Medawar et Thomas Gibson découvrirent d’importantes fonctions des cellules immunitaires. C’est par ces travaux que l’acceptation générale de l’immunité cellulaire se fit. En 1948, Astrid Fagraeus découvrit que les anticorps sont produits dans le plasma sanguin par les lymphocytes B. L’année suivante, Frank Macfarlane Burnet et Frank Fenner publiaient leur hypothèse de la tolérance immunologique, qui fut validée quelques années plus tard par Jacques Miller, qui découvrit l’élimination des lymphocytes T auto-réactifs dans le thymus. Burnet et Fenner reçurent le prix Nobel de médecine en 1960 pour leurs travaux sur la tolérance. En 1957, Frank Macfarlane Burnet décrivit le principe fondamental de l’immunité adaptative comme étant la sélection clonale.L’Anglais Alick Isaacs et le Suisse Jean Lindenmann, en étudiant l’infection de cultures cellulaires par des virus, découvrirent en 1957 que les cellules, au cours de l’infection par un virus, étaient en grande partie résistantes à une autre infection par un deuxième virus. Ils isolèrent à partir des cellules infectées une protéine qu’ils nommèrent interféron. À la fin des années 1960 et au début des années 1970, John David et Barry Bloom découvrirent le facteur d’inhibition de la migration des macrophages (MIF) ainsi que de nombreuses autres substances sécrétées par les lymphocytes. Dudley Dumonde proposa pour ces substances le nom de « lymphokine ». Stanley Cohen, qui reçut en 1986 le prix Nobel de médecine pour sa découverte des facteurs de croissances NGF et EGF, commença, au début des années 1970, à travailler avec Takeshi Yoshida sur les fonctions des lymphokines. Ils mirent en évidence que ces substances, produites de nombreux types différents de cellules, étaient capables d’action à distance, comme des hormones. À la suite des nombreuses découvertes dans ce domaine, Stanley Cohen proposa en 1974 le terme « cytokine » qui s’imposa rapidement. Entretemps, plus de cent cytokines différentes étaient identifiées, et leurs structures et activités étudiées en détail.Les années soixante sont en général considérées comme le début de l’époque moderne de l’immunologie. Jacques Oudin découvre en 1956 l’allotypie des protéines, puis en 1963 l’idiotypie des anticorps. Rodney Porter et Gerald Edelman réussirent à élucider la structure des anticorps entre 1959 et 1961, et furent lauréats du prix Nobel de médecine en 1972. En même temps, Jean Dausset, Baruj Benacerraf et George Snell découvraient le complexe majeur d'histocompatibilité, également appelé système HLA (de l’anglais Human Leukocyt Antigen) chez l’être humain, découverte qui leur permit de recevoir le prix Nobel de médecine en 1980. En 1959, Joseph Murray réalise la première allogreffe en transplantant un rein. Avec Donnall Thomas, ils étudient l’immunosuppression artificielle qui permet la tolérance des patients vis-à-vis de leur greffe ; ils reçurent le prix Nobel de médecine en 1990 pour ces études. Vers 1960 également, la communauté scientifique découvrait, grâce aux travaux de Jacques Miller, d’autres caractéristiques fondamentales des cellules immunitaires, en particulier la description des fonctions et de la différenciation des lymphocytes B et T. Après cette percée, la théorie selon laquelle l’immunité est divisée en une partie cellulaire et une autre humorale s’imposa, et les deux théories ne furent plus mises en concurrence. Dans les décennies suivantes, les différents sous-types (appelés isotypes) d’anticorps furent identifiés et leurs fonctions respectives étudiées. En 1975, Georges Köhler, Niels Kaj Jerne et César Milstein décrivent la méthode de production des anticorps monoclonaux. Cette découverte eut un impact majeur sur la recherche fondamentale, ainsi que pour le diagnostic et le traitement de maladies, et ils reçurent en 1984 le prix Nobel de médecine. D’autres découvertes majeures furent faites dans les années suivantes : en 1973, Ralph Steinman et Zanzil Cohn découvrent les cellules dendritiques. Ralph Steinman obtiendra le prix Nobel en 2011 pour cette découverte. En 1974, Rolf Zinkernagel et Peter Doherty découvrent la restriction de la présentation de l’antigène par les molécules du MHC, découverte qui lui valut le prix Nobel de médecine en 1996 ; En 1985, Susumu Tonegawa identifie les gènes des immunoglobulines, et reçoit pour cela en 1987 le prix Nobel ; la même année, Leroy Hood fait de même pour les gènes du récepteur des cellules T.Un autre concept émerge en 1986 : celui de l'orientation de la réponse immunitaire. Basé sur le rôle des lymphocytes T CD4+ (CD pour cluster de différenciation), ce concept, développé par Robert Coffman et Tim Mosmann, présente la dichotomie entre une « Th1 », réponse orientée contre des cellules d'une part, qui produira des lymphocytes cytotoxiques spécifiques, comme dans le cas du cancer ou d'une infection intracellulaire; et une réponse « Th2 » contre un agent soluble, qui produira des anticorps spécifiques, comme dans le cas d'une bactérie extracellulaire ou d'une toxine. La balance Th1/Th2 est toujours un intense champ de recherche.La notion de tolérance induite par des lymphocytes fut pour la première fois évoquée en 1969 par Nishizuka et Sokakura. Ils présentaient leurs résultats concernant une sous-population de lymphocytes T suppresseurs capables d'empêcher une réaction de lymphocytes naïfs. Très controversés, ces résultats seront oubliés jusqu'à la redécouverte du phénomène par Sakaguchi en 1982 sous le nom de T régulateur, sujet activement étudié actuellement.Depuis les années 1950, la théorie qui domine en immunologie est celle de la reconnaissance du « soi » et du « non-soi » par le système immunitaire adaptatif. Cependant, ce modèle ne permet pas d'expliquer de manière satisfaisante les phénomènes de tolérance, de rejet de greffe, ni la nécessité de la présentation de l'antigène, et en 1989, Charles Janeway propose un modèle selon lequel ce serait l'immunité innée qui serait la véritable gardienne des clefs du déclenchement d'une réponse immunitaire. La décision de réagir ou non face à un agent étranger reposerait sur la reconnaissance de motifs par des récepteurs putatifs qu'il nomme les récepteurs de reconnaissance de motifs moléculaires. Ce modèle est approfondi à partir de 1994 par Polly Matzinger, qui développe la théorie du danger. D'après Matzinger, le déclenchement de la réponse immunitaire se ferait sur la base de motifs moléculaires associés aux pathogènes par les récepteurs de reconnaissance de motifs moléculaires. Ce modèle fut validé expérimentalement depuis par l'identification de récepteurs de signaux de danger et de certains de leurs ligands.De nos jours, la multiplication des cytokines, chimiokines, sous-types et marqueurs cellulaires rend difficile d'avoir une vue d'ensemble du domaine.Du fait de la complexité des phénomènes étudiés et de leur intime imbrication, les immunologistes sont souvent réduits à utiliser des concepts plus ou moins abstraits pour interpréter les informations disponibles. Au fil du temps, de plus en plus de nouveaux concepts, se recoupant plus ou moins, se font jour dans la communauté scientifique, la plupart du temps en opposant deux notions. La liste ci-dessous ne peut pas être exhaustive, mais donne un aperçu de quelques-unes de ces grandes notions. Elle reprend naturellement certains points déjà vu dans l'historique, mais les développe sous un aspect simplifié et plus pragmatique.Le concept de base de l'immunologie de la réponse adaptative est celui d'antigène. Globalement, on qualifie d'antigène toute substance qui est reconnue par le système immunitaire adaptatif. Tous les antigènes ne déclenchent pas de réactions immunitaires; ils sont dits non immunogènes. Autrement dit tous les antigènes ne sont pas immunogènes.Il est important de dire qu'il n'y a pas de réponse immunitaire sans antigène.Il existe différentes dénominations des antigènes :xénoantigène : antigène étranger à l'espèce ;alloantigène : molécule variable selon les individus d'une même espèce (exemple : système ABO) ;néoantigène : antigène normalement non exprimé dans l'organisme (antigène induit par des tumeurs) ;autoantigène : antigène du soi normalement non reconnu par le système immunitaire.Un antigène peut comporter un à plusieurs épitopes (chaque épitope peut être reconnu par un paratope spécifique d'un anticorps ou d'un récepteur des cellules T (en anglais T cell receptor ou TCR).Le complexe épitope + paratope forme un complexe type : clef-serrure. Néanmoins, un épitope n'est pas spécifique (dans l'absolu) à un antigène. C'est pourquoi il peut exister des réactions croisées (AG 1 reconnu par anticorps mais si un AG 2 porte également l'épitope de AG 1 alors AG 2 peut être reconnu par cet anticorps).Concept important, celui du système inné et du système adaptatif (ou acquis, bien que ce terme soit de moins en moins utilisé). Il s'agit ici d'opposer des phénomènes « non spécifiques » à des événements « spécifiques », sous-entendu « de l'antigène ».Dans le premier cas, il s'agit d'une réaction suivant l'introduction d'un nouvel élément, quel qu'il soit, et qui repose sur une réaction globale d'un type cellulaire. Toutes les cellules blessées, quelle qu'en soit la cause, ont des réactions similaires, et les cellules du système immunitaire réagissent de manières stéréotypées également. Cette réponse innée est rapide, sans mémoire et indépendante de l'antigène. Une multitude de situation (blessure, infection virale ou bactérienne, etc.) mènent à des réactions innées similaires.La réponse adaptative concerne des phénomènes liés aux antigènes, et consiste en la sélection de clones de lymphocytes, capables de cibler ce qui est perçu comme une menace. Cette réponse adaptative est lente, strictement dépendante des antigènes, et possède une mémoire immunitaire. Chaque situation différente mènera à la sélection de quelques clones lymphocytaires qui prendront en charge le danger.Un des plus anciens concepts oppose une composante cellulaire à une composante soluble (« humorale ») de l'immunité. Elle tient du fait que le sérum, donc débarrassé des cellules sanguines et du fibrinogène, peut produire des phénomènes rapides et très efficaces de destruction (lyse) d'organismes cibles, d'une part et que les effets de certaines cellules immunitaires sont plus difficiles à observer, car sont plus lents et imposent des conditions d'expérimentation très strictes. Les deux types de phénomènes furent pendant longtemps impossibles à observer concomitamment. Cette opposition n'aura plus lieu d'être dès que les techniques permettront de prouver que ce sont bien des cellules immunitaires qui produisent ces facteurs solubles.La découverte du rôle des cellules T CD4+ helper (Th), à savoir d'aider la réponse immunitaire, fit se dégager assez vite un fait expérimental : dans certaines conditions, les Th peuvent favoriser une réponse à médiation cellulaire, avec génération de cellules cytotoxiques, ou une réponse humorale, avec production d'anticorps. En d'autres termes, un même antigène dans des situations différentes induira parfois une réponse à médiation cellulaire, parfois une réponse à médiation humorale. Reprenant l'ancienne dichotomie cellulaire/humorale, le concept Th1/Th2 permet d'opposer les conditions dans lesquelles les T CD4+ réagissent en produisant des signaux dirigeant la réponse vers une cytotoxicité cellulaire, avec formation de cellules T CD8+ cytotoxiques (CTL pour cytotoxic T lymphocytes) en grand nombre; ou au contraire la formation d'une réponse soluble, avec différenciation de lymphocytes B en plasmocytes, produisant des anticorps en grande quantité.La réponse cellulaire fut pendant longtemps considérée comme résultant d'une reconnaissance directe par les cellules immunitaires des cellules étrangères. Autrement comment expliquer que des substances produisent une réaction forte chez un organisme et aucune chez un autre ? L'introduction d'un élément étranger (infection ou greffe) doit être suivie d'une acceptation ou d'un rejet par le système immunitaire. Lors d'une greffe de peau par exemple, la peau prélevée sur le donneur était bien acceptée par le système immunitaire du donneur. Or, après la greffe, le système immunitaire du receveur peut bien décider de considérer la nouvelle peau comme étrangère, et la rejeter, alors qu'elle ne constitue en rien un danger (mais sera considéré comme un danger par le système immunitaire). Ce concept reste très actuel, bien que ses mécanismes aient été en grande partie élucidés par l'étude des interactions entre les récepteurs des cellules T (TCR) et les molécules du complexe majeur d'histocompatibilité (CMH).Une autre question peut se poser : comment se fait-il que certains corps étrangers ne soient « pas reconnus ? » Autrement dit : lors de la reconnaissance d'un antigène donné, qu'est-ce qui active ou non les défenses immunitaires ?La première notion-clé est celle de tolérance centrale : aucun organisme n'est censé produire de lymphocytes auto-réactifs, c'est-à-dire des lymphocytes réagissant contre les antigènes du soi non modifié. La seconde notion-clé est celle de tolérance périphérique : elle repose sur une inhibition conditionnelle de la réponse des cellules immunitaires face à un antigène du « non-soi ».La difficulté est de comprendre dans quelles conditions un antigène induit :soit une réaction de défense de la part du système immunitaire, auquel cas on dit que l'antigène est immunogène ;soit une tolérance pour cet antigène, auquel cas on dit que l'antigène est tolérogène.La théorie du danger repose sur un constat simple: dans certaines situations, un même antigène peut être perçu comme sans danger (tolérogène) ou dangereux (immunogène), et, dans le second cas, entraîner une réponse très différente : réponse cellulaire ou réponse anticorps de divers types, allant jusqu'à l'allergie. La théorie du danger stipule que ce sont les conditions dans lesquelles l'antigène est perçu qui déterminent le type de réponse immunitaire qui sera développé. Ces conditions particulières impliquent des signaux de danger en plus ou moins grande quantité et plus ou moins variés, et qui accompagnent l'antigène. La combinaison des signaux de danger (ou leur absence) oriente la réponse immunitaire.L'ensemble des organes du système immunitaire s'appelle le système lymphoïde. La moelle osseuse C'est là que les cellules du système immunitaire sont produites, par un processus appelé hématopoïèse. C'est également le lieu de l'acquisition de l'immunocompétence des lymphocytes B.Lors de la production des lymphocytes B, ces derniers avant de quitter la moelle doivent avoir acquis certaines caractéristiques comme : les chaînes légères (? ou ?) et lourdes (?) des BCR. Il y a au sein de la moelle osseuse une sélection négative : 90 % des lymphocytes B sont détruits. Les 10 % restant rejoindront la circulation systémique pour poursuivre leur maturation dans les organes lymphoïdes secondaires. Le thymus C'est là qu'a lieu la maturation et la sélection des lymphocytes T.Les cellules (futurs lymphocytes) entrent dans le thymus et rejoignent le cortex. Il y aura apparition de caractéristiques de lymphocyte « delta-gamma » et « alpha beta » :l'apparition de récepteurs TCR gamma-delta va entraîner le lymphocyte T dans la medulla, puis ce dernier va rejoindre la circulation systémique ;l'apparition de sous-unités alpha et béta va entraîner par la suite une apparition des molécules membranaires CD4+ et CD8+. On dit alors que ces lymphocytes sont doublement positifs. Mais un lymphocyte T est soit CD4+ soit CD8+ donc il doit perdre soit la molécule de surface CD4 soit CD8.Pour cela, il va y avoir des cellules épithéliales du thymus qui vont présenter les complexes CMH I et CMH II. Si la molécule de surface CD4+ reconnaît le CMH II, le lymphocyte T se différencie en lymphocyte T CD4+ ; si la molécule de surface CD8+ reconnait le CMH I, le lymphocyte T se différencie en lymphocyte T CD8+. Cette étape s'appelle la sélection positive. Nous avons donc à la sortie du cortex du thymus des lymphocytes CD4+ et CD8+. À la jonction cortico-médullaire du thymus, il va y avoir l'étape de la sélection négative. En effet, il est dangereux de laisser quitter du thymus des lymphocytes ayant une trop grande affinité pour les cellules du soi (qui les détruirait ?). Pour cela, les thymocytes (lymphocytes T CD4+ et lymphocytes T CD8+) vont rencontrer des cellules du soi. Après la sélection positive dans le compartiment médullaire, il va y avoir la sélection négative. Les thymocytes vont rencontrer des cellules du soi.CD8 + CMH I : si trop forte reconnaissance : destruction du thymocyteCD4 + CMH II : si trop forte reconnaissance : destruction du thymocyteGrâce à cela, on élimine les lymphocytes T auto-réactifs dangereux pour l’organisme.Tous les lymphocytes T « en vie » vont former les lymphocytes T naïfs : car n’ont pas encore rencontré les antigènes du non-soi. Ils vont sortir du thymus et vont aller dans les organes secondaires. C’est là qu’ils rencontreront les antigènes externes du non-soi.Au niveau du système sanguin, il y a des échappées de protéines. Ces protéines se retrouvent dans le liquide interstitiel et doivent retourner dans le sang afin de contrôler son osmolarité. Les capillaires lymphatiques récupèrent ces protéines et captent aussi les agents pathogènes, cellules du système immunitaire et débris de cellules mortes. Le système lymphatique entraîne la lymphe au niveau d'un centre intégrateur qui correspond aux ganglions lymphatiques. Après le passage de la lymphe dans le ganglion, la lymphe est épurée. La lymphe circule vers le cœur à sens unique. Elle rejoint la circulation sanguine au niveau du cœur par le canal thoracique et se jette dans la veine sous-clavière gauche.Les ganglions lymphatiques ont une structure plus ou moins globuleuse. Ils se décomposent en plusieurs zones.Un sinus capsulaire qui permet l'arrivée des vaisseaux lymphatiques afférents. La lymphe traverse le sinus entre dans le ganglion par l'intermédiaire de travées.Le cortex du ganglion est occupé par les lymphocytes B. Les cellules B sont regroupées en amas. Ce sont ces follicules qui grossissent en cas d'infection.Le paracortex abrite les lymphocytes T et les cellules dendritiques.Au centre, on a une zone de sortie avec autant de lymphocytes B que de lymphocytes T. C'est le hile par lequel sortent les vaisseaux lymphatiques efférents.Les appendices secondaires (formations lymphoïdes agrégées) ont des zones particulières d'épuration. Ce sont l’anneau de Waldeyer au carrefour aérodigestif (amygdales et végétations adénoïdes), l'appendice et les plaques de Peyer.La rate fait également partie du système immunitaire car elle épure le sang vis-à-vis des pathogènes qui pourraient s'y trouver.Les organes lymphoïdes tertiaires comprennent tous les tissus et organes où la réponse immunitaire a lieu. Ils contiennent peu de cellules lymphoïdes dans les conditions physiologiques normales mais peuvent en importer une grande quantité lors de la présence d'un pathogène. Ils comprennent :la peau ;le système respiratoire – voir tissu lymphoïde associé aux muqueuses ;le tube digestif – voir tissu lymphoïde associé aux muqueuses ;le tractus génital – voir tissu lymphoïde associé aux muqueuses ;le reste du corps.Il faut noter l'existence de sanctuaires immunitaires. Ce sont des tissus où les cellules immunitaires ne pénètrent pas ; il s'agit des testicules et de la chambre antérieure de l'œil. Les lymphocytes naïfs ne peuvent pas franchir la barrière hémato-encéphalique.Il s'agit des mécanismes de défense impliquant des facteurs solubles. Elle est de deux types : défense innée et défense adaptative. Immunité humorale innée Les défenses innées correspondent à des molécules présentes spontanément dans l'organisme et qui préexistent à la menace. Il s'agit des anticorps naturels, des défensines et du système du complément. Les tissus agressés produisent également des molécules de l'inflammation, tels que le facteur tissulaire et les dérivés de l'acide arachidonique: leucotriènes et prostaglandines Immunité humorale adaptative Elle est supportée par la présence d'anticorps circulants. Les anticorps sont produits par les plasmocytes, issus de la différenciation terminale d'un clone de lymphocyte B. Ce sont des molécules de type immunoglobuline de différents types :les IgM qui sont les premiers produits lors d'une infection. Ils sont décavalents et leur avidité pour les antigènes est très grande. Ils ont un rôle majeur dans la formation de complexes immuns ;les IgG de haute affinité, ayant un rôle essentiel dans la cytotoxicité liée aux anticorps ;les IgE supports de l'allergie immédiate (réaction d'hypersensibilité type 1) ;les IgA sécrétés au niveau des muqueuses, jouent un rôle majeur dans la neutralisation des pathogènes présents sur les épithéliums (bronches, tube digestif).De manière générale, les anticorps agissent de deux manières différentes: soit par l'activation du complément, soit par fixation du complexe immun sur une cellule immunitaire possédant un récepteur pour le fragment constant des anticorps (tels que les macrophages, les lymphocytes NK par exemple).Les phénomènes immunitaires à médiation cellulaire impliquent différents types de cellule, regroupés dans deux concepts: les cellules de l'immunité innée et celles de l'immunité adaptative. Cellules de l'immunité innée Ce sont des cellules qui sont capables de réagir à un phénomène sans éducation préalable. Elles réagissent à des stimuli présents sur une variété de pathogènes, et indépendamment des antigènes. Il s'agit :des lymphocytes NK ;des granulocytes, anciennement appelés polynucléaires ;des macrophages ;des cellules dendritiques, qui sont les meilleures cellules présentatrices d'antigènes. Cellules de l'immunité adaptative Il s'agit de réactions qui mettent en jeu des cellules de type lymphocyte T. Leur maturation dépend d'un stimulus antigénique et d'une éducation par une cellule présentatrice d'antigène. Leur activation face à une cible dépend de la présentation de l'antigène par la cellule cible. Les lymphocytes T ne sont donc capables de reconnaître que des cellules transformées (c'est-à-dire infectées par un pathogène intracellulaire, ou une cellule tumorale). Il y a deux types principaux de lymphocytes T :les lymphocytes TCD8+ reconnaissent un antigène porté par une molécule de CMH de type I. Ils se différencient généralement en lymphocytes cytotoxiques et produisent relativement peu de cytokines ;les lymphocytes TCD4+ reconnaissent un antigène porté par une molécule de CMH de type II. Leur action principale est la sécrétion de cytokines, qui orientent et amplifient la réponse immunitaire, c'est ce qu'on nomme le help (en français : aide), d'où le surnom de helper donnés à ces lymphocytes T. Le paradigme actuel est de différencier deux types de CD4+: les lymphocytes helpers qui orientent vers une réponse cytotoxique (Th1) et ceux qui orientent vers une réponse plus humorale (Th2).On retrouve également les lymphocytes B via la diversité de leurs BCR, qui sont des AC.Congénitaux :à prédominance humorale, touchant les anticorps ;à prédominance cellulaire ;combinés.Acquis :iatrogènes (chimiothérapie du cancer, traitement immunosuppresseur) ;viraux (VIH SIDA) ;liés à une pathologie sévère : leucémie, lymphome, cancer évolué, etc.Les différents types d'allergie ou hypersensibilité.De type 1, dite « immédiate » : Médiée par les IgE, rapide voire foudroyante.De type 2 : cytotoxicité directe des immunoglobulines.De type 3 : complexes immuns circulants.De type 4 dite « retardée » : allergie retardée d'immunité cellulaire.Les maladies auto-immunes, au cours desquelles l'organisme attaque ses propres constituants (le « soi »)Les réactions à la transfusion sanguine.Le rejet de greffe d'organe.La connaissance des mécanismes immunologiques a permis le développement de nombreuses techniques d'analyses aussi bien quantitatives que qualitatives, utilisant notamment les anticorps, vecteurs de l'immunité humorale, mais aussi parfois des tests cellulaires. La maitrise de production des anticorps a ouvert le champ à de nombreuses techniques de purification par « affinité », mais aussi des applications thérapeutiques. Techniques La plupart de ces techniques utilisent les propriétés des anticorps monoclonaux ou polyclonaux purifiés par affinité. Leur affinité et leur spécificité de liaison à leur cible fait d'eux des outils incontournables de détection ou capture spécifique. Ils permettent de déterminer la présence dans un échantillon complexe d'un antigène ou même d'une de ses parties particulière(épitope). De nombreuses modalités de mise en œuvre existent. Techniques avec anticorps non marqués Les premières techniques d'analyse proposées utilisaient des anticorps non marqués, par exemple dans les réactions de précipitation, les ré"
médecine;"L'inflammation est la réaction stéréotypée du système immunitaire, face à une agression externe (infection, trauma, brûlure, allergie, etc.) ou interne (cellules cancéreuses) des tissus. C'est un processus dit ubiquitaire ou universel qui concerne tous les tissus, faisant intervenir l'immunité innée et l'immunité adaptative. Elle est cependant inhibée dans le système immunitaire des muqueuses, dont le mécanisme d'action est spécifique.L'inflammation est identifiée en médecine par le suffixe -ite. Traditionnellement, les symptômes associés à l'inflammation sont décrits en latin par « dolor, calor, rubor, tumor, et functio laesa » (douleur, chaleur, rougeur, œdème et perte de fonctionnalité). Le plus gros problème qui découle de l'inflammation est que la défense de l'organisme attaque à la fois les agents nocifs et non nocifs, d'une manière qui endommage les tissus ou les organes sains.L'inflammation chronique est une réponse à de nombreuses transformations de l'environnement et du comportement modernes (elle est favorisée par la sédentarité, la mauvaise hygiène alimentaire (malbouffe), la pollution, les altérations du microbiote humain) et un facteur important dans le développement de maladies de civilisation telles que la résistance à l'insuline, l'obésité, les maladies cardiovasculaires, les maladies immunitaires, et même les troubles de l'humeur et du comportement.Dès les premières civilisations, on trouve des témoignages de sa connaissance et de sa guérison. Les premiers écrits sont apparus dans des papyrus égyptiens datant de 3000 av. J.-C. En Grèce et à Rome, un livre a été conservé, l'un des nombreux écrits de Aulus Cornelius Celso, encyclopédiste, ""De Medicinae"", dans lequel quatre signes cardinaux d'inflammation sont identifiés.Traditionnellement, à sa suite, les symptômes associés à l'inflammation sont décrits en latin par « dolor, calor, rubor, tumor, et functio laesa » (douleur, chaleur, rougeur, œdème et perte de fonctionnalité — la formulation des quatre premiers étant attribués à Aulus Cornelius Celsus et celle du cinquième souvent attribuée à Claude Galien). Si l’inflammation est connue depuis l’Antiquité, l’impotence fonctionnelle a été rajoutée à sa définition par Rudolf Virchow en 1858.En 1793, le chirurgien écossais John Hunter a souligné ce qui est aujourd'hui considéré comme une évidence : « L'inflammation n'est pas une maladie, mais une réponse non spécifique qui produit un effet curatif sur le corps dans lequel elle se produit ».Le pathologiste Julius Cohnheim a été le premier chercheur à utiliser le microscope pour examiner les vaisseaux sanguins enflammés dans des membranes minces et translucides, telles que le mésentère et la langue de la grenouille. Il observa la réorganisation initiale du flux sanguin, la formation de l'œdème après augmentation de la perméabilité vasculaire,la migration des leucocytes. En 1867, il démontra que l'émigration des globules blancs était à l'origine du pus. La contribution de Cohnheim fut fondamentale pour comprendre l'ensemble du processus inflammatoire.Le biologiste russe Ilya Ilitch Metchnikov découvrit le processus de phagocytose, en observant l'ingestion d'épines de rose par les amibocytes de larves d'étoiles de mer, et de bactéries par les leucocytes de mammifères (1882) ; la conclusion de ce chercheur était que l'objet de l'inflammation était de faire en sorte que des cellules ayant une capacité phagocytaire atteignent la zone lésée afin qu'elles puissent phagocyter des agents infectieux. Cependant, il est vite devenu clair que les facteurs cellulaires (phagocytes) et les facteurs sériques (anticorps) étaient essentiels à la défense contre les micro-organismes. En reconnaissance de cela, Metchnikoff et Paul Ehrlich (qui a développé la théorie humorale) ont partagé le prix Nobel de médecine en 1908. .A ces noms il faut ajouter celui de Sir Thomas Lewis qui, par de simples expériences sur la réponse inflammatoire de la peau, a établi le concept que divers produits chimiques induits localement par la stimulation d'une lésion, comme l'histamine, sont des facteurs médiateurs des altérations vasculaires. d'inflammation. Ce concept fondamental est à la base des importantes découvertes de médiateurs chimiques de l'inflammation et de la possibilité d'utiliser des médicaments anti-inflammatoires.L'inflammation aiguë peut être considérée comme la première ligne de défense contre les blessures infligées à un tissu. Elle fait partie du système immunitaire inné.Le processus d'inflammation aiguë est initié par les cellules immunitaires résidentes déjà présentes dans le tissu impliqué, principalement les macrophages résidents, les cellules dendritiques, les histiocytes, les cellules de Kupffer et les mastocytes. Ces cellules possèdent des récepteurs de surface appelés récepteur de reconnaissance de motifs moléculaires (PRR), qui peuvent reconnaître (c'est-à-dire se lier à) deux sous-classes de molécules, qui agissent comme signal déclencheur :les motif moléculaire associé aux pathogènes (PAMP). Les PAMP sont des composés qui sont associés à divers agents pathogènes, mais qui se ne sont pas présent dans les cellules de l'organisme. Ils signent donc la présence d'un agent pathogène ;les motif moléculaire associé aux dégâts (DAMP). Les DAMP sont des signaux de danger émis par les cellules de l'hôte, associés à des blessures ou à des dommages cellulaires.Au début d'une infection, d'une brûlure ou d'autres blessures, ces cellules reconnaissent un signal de danger pour l'organisme (l'un des PRR reconnaît un PAMP ou un DAMP) et s'activent, libérant des médiateurs inflammatoires responsables des signes cliniques de l'inflammation du tissu concerné.Les cellules immunitaires réagissent aux stress physiques détectés dans les tissus (chaleur, froid, pression) et produisent les médiateurs sérotonine et histamine, qui sont de puissants agents vasoactifs qui agissent sur la contraction et la perméabilité des vaisseaux artériels et veineux.Telle que définie, l'inflammation aiguë est une réponse immunovasculaire à des stimuli inflammatoires. Cela signifie que l'inflammation aiguë peut être largement divisée en une phase vasculaire, qui se produit en premier, suivie d'une phase cellulaire impliquant des cellules immunitaires (plus spécifiquement des granulocytes myéloïdes dans le cadre aigu).La réponse inflammatoire aiguë nécessite une stimulation constante pour être soutenue. Les médiateurs inflammatoires sont de courte durée et se dégradent rapidement dans les tissus. Par conséquent, l'inflammation aiguë commence à se résorber dès que le stimulus est supprimé.L'inflammation est déclenchée par l'action de médiateurs chimiques, qui déclenchent la phase vasculaire, ou vasculo-exsudative. On constate :La libération d'amines vaso-actives préformées par les mastocytes (histamine et sérotonine) ;L'activation de protéines plasmatiques inactives (facteur XII (Hageman), bradykinine, kallikréine, complément) ;La sécrétion de médiateurs lipidiques (prostaglandines dont prostacycline, leucotriènes, facteur d'activation plaquettaire (PAF)).En plus des médiateurs dérivés des cellules, plusieurs systèmes de cascade biochimiques acellulaires - constitués de protéines plasmatiques préformées - agissent en parallèle pour initier et propager la réponse inflammatoire. Ceux-ci incluent le système du complément, activé par les bactéries, et les systèmes de coagulation et de fibrinolyse, activés par la nécrose (par exemple, brûlure, traumatisme).Les facteurs chimiques produits durant l'inflammation (histamine, bradykinine, sérotonine, leucotrienes et prostaglandines) augmentent la sensation de douleur, induisent localement la vasodilatation des vaisseaux sanguins et le recrutement de phagocytes, en particulier les neutrophiles[réf. nécessaire]. Les neutrophiles peuvent également produire des facteurs solubles contribuant à la mobilisation d'autres populations de leucocytes. Les cytokines produites par les macrophages et les autres cellules du système immunitaire inné constituent un relais de la réponse immunitaire. On compte, parmi ces cytokines, le TNF?, HMGB1, et l'interleukine-1.Les trois cytokines majeures de l'inflammation sont l'interleukine-1, l'interleukine-6 et le facteur de nécrose tumorale,. On les nomme le trio pro-inflammatoire.Sous l'influence de médiateurs chimiques, les cellules endothéliales (formant les vaisseaux sanguins) s'activent. Cela entraîne une vasodilatation locale artériolaire puis capillaire qui provoque :une augmentation de l'apport sanguin ;une diminution de la vitesse du flux sanguin.Ce gonflement local des vaisseaux sanguins provoque la rougeur (rubor) et la sensation de chaleur (calor).L’augmentation de l'apport sanguin permettra d’évacuer les cellules mortes et les toxines (détersion), et d’apporter les éléments nécessaires à la guérison, notamment des globules blancs pour combattre les corps étrangers.Les molécules médiatrices modifient également les vaisseaux sanguins pour permettre la migration des leucocytes, principalement des neutrophiles et des macrophages, hors des vaisseaux sanguins (extravasation) et dans les tissus. Dans des conditions normales, l'endothélium ne permet pas la sortie des protéines et l'échange se fait par pinocytose. Au cours de l'inflammation, les bases morphologiques de l'endothélium sont altérées par l'action de médiateurs chimiques, produisant une altération des jonctions cellulaires et des charges négatives de la membrane basale. Généralement, cet effet se produit dans les veinules, mais s'il est très intense, il atteint les capillaires et une extravasation se produit en raison de la rupture.Parallèlement, les cellules endothéliales activées expriment des molécules d'adhésion (nécessaires à la diapédèse). De son côté, la diminution de la vitesse (stase) permet aux leucocytes de se marginaliser le long de l'endothélium, un processus essentiel à leur recrutement dans les tissus.Le vaisseau devenant plus perméable, l’eau du plasma sanguin s'épanche par osmose vers les tissus.La perméabilité accrue des vaisseaux sanguins entraîne une exsudation (fuite) de liquide dans les tissus. La fuite de liquide provoque une augmentation de la viscosité du sang, ce qui augmente la concentration des globules rouges (congestion veineuse).L’œdème inflammatoire (tumor) est donc la conséquence du passage du plasma (plus précisément d'un exsudat) dans la zone lésée. Il se traduit par un gonflement du tissu touché, et comprime les nerfs alentour, provoquant la sensation douloureuse et les démangeaisons (dolor). Certains des médiateurs libérés comme la bradykinine augmentent la sensibilité à la douleur (hyperalgésie). La perte de fonction (functio laesa) est probablement le résultat d'un réflexe neurologique en réponse à la douleur.L'œdème a plusieurs fonctions : il permet l'apport jusqu'à la lésion de moyens de défense (immunoglobulines, protéines du complément…), la dilution de l'agent pathogène, et la limitation du foyer inflammatoire.Le mouvement du liquide plasmatique entraîne avec lui des protéines importantes, telles que la fibrine et les immunoglobulines (anticorps), dans le tissu enflammé.Ce fluide tissulaire exsudé contient divers médiateurs antimicrobiens du plasma tels que le système du complément, le lysozyme, des anticorps, qui peuvent immédiatement endommager les microbes et opsoniser les microbes en vue de la phase cellulaire.Si le stimulus inflammatoire est une plaie lacérée, les plaquettes exsudatives, les coagulants, la plasmine et les kinines peuvent coaguler la zone blessée et assurer l'hémostase dans un premier temps.Ces médiateurs de la coagulation fournissent également un cadre structurel de mise en scène au site du tissu inflammatoire sous la forme d'un réseau de fibrine - comme le ferait un échafaudage de construction sur un chantier de construction - dans le but de faciliter le débridement phagocytaire et plus tard la réparation des plaies.Une partie du liquide tissulaire exsudé sera également acheminée par les vaisseaux lymphatiques vers les ganglions lymphatiques régionaux. Dans des conditions normales, le système lymphatique filtre et contrôle les petites quantités de liquide extravasculaire qui ont été perdues par les capillaires. Au cours de l'inflammation, la quantité de liquide extracellulaire augmente, et le système lymphatique participe à l'élimination de l'œdème. De plus, dans ce cas, une plus grande quantité de leucocytes, de débris cellulaires et de microbes passe dans la lymphe, pour lancer la phase de reconnaissance et d'attaque du système immunitaire adaptatif. Comme pour les vaisseaux sanguins, les lymphatiques prolifèrent également dans les processus inflammatoires, pour répondre à la demande accrue. Les vaisseaux lymphatiques peuvent devenir secondairement enflammés (lymphangite) ou les ganglions lymphatiques peuvent devenir enflés (lymphadénite), en raison d'une hyperplasie des follicules lymphoïdes et d'un nombre accru de lymphocytes et de macrophages.Les molécules d’adhésion (CAM, intégrines, sélectines) libérées par les cellules endothéliales sont un signal pour les leucocytes présents dans les vaisseaux sanguins, qui dans la région inflammatoire ont tendance à quitter le milieu du courant pour s’accoler à la paroi de l’endothélium du vaisseau, par « margination », favorisée par le ralentissement du flux sanguin.La diapédèse leucocytaire est le phénomène permettant le passage des leucocytes de la circulation sanguine jusqu'au foyer de l'inflammation. La traversée de l'endothélium par les leucocytes ou diapédèse intervient dans un segment particulier du système circulatoire : les veinules post-capillaires. On peut distinguer différentes étapes :Margination leucocytaire ;Rolling : interaction des leucocytes et des cellules endothéliales par l'intermédiaire de sélectines, adhérence faible entre une sélectine E capillaire et un carbohydrate de la cellule immunitaire ;Activation (activation) par interaction entre une chimiokine capillaire et une intégrine cellulaire ;Adhérence ferme (adhesion) entre l'intégrine cellulaire (LFA-1) et son ligand vasculaire (ICAM-1). La cellule est fixée sur l'endothélium ;Diapédèse : passage de la paroi endothéliale par les leucocytes, qui commence par l'adhésion cellulaire par l'intermédiaire des intégrines et de molécules d'adhésion (ICAM, en anglais : intercellular adhesion molecule).Après la migration des leucocytes hors des vaisseaux sanguins (extravasation), les cellules inflammatoires, dont les leucocytes, se dirigent ensuite de façon unidirectionnelle par chimiotaxie, le long d'un gradient créé par les cellules locales, pour atteindre le site de la lésion.À l'issue de la phase vasculaire ou vasculo-exsudative, la phase cellulaire fait suite à la diapédèse, lorsque les leucocytes sont amassés dans le tissu interstitiel.Elle correspond à la formation du granulome inflammatoire. Il participe à la détersion (rôle des granulocytes et des macrophages) et permet le développement de la réaction immunitaire adaptative. Les cellules composant le granulome ont également un rôle de sécrétion de médiateurs chimiques.Les leucocytes engloutissent les microbes et les détruisent, générant la production de pus. Le pus sera éliminé vers l'extérieur si la lésion est en contact avec l'extérieur, ou il générera un abcès si la zone où s'est formé le pus est à l'intérieur d'un organe.La réparation des lésions tissulaires s'effectue grâce aux macrophages, qui stimulent les fibroblastes pour synthétiser le collagène et les cellules endothéliales pour générer de nouveaux vaisseaux, grâce à la sécrétion de facteurs de croissance.L 'Inflammation systémique implique trois organes (le foie, le système nerveux central et les glandes surrénales) et le trio pro-inflammatoire (l'interleukine-1, l'interleukine-6 et le facteur de nécrose tumorale).Les molécules inflammatoires sensibilisent les terminaisons nerveuses. Les neurones relarguent la substance P et la CGRP des peptides qui ont des actions vaso-dilatatrices puissantes.L'inflammation peut :aboutir à la guérison de l'individu ;donner une cicatrice ;être limitée dans une partie du corps et persister sous forme de granulome comme le granulome pulmonaire de la tuberculose ;se diffuser dans tout l'organisme sous forme de septicémie aboutissant à un choc toxique souvent mortel.L’inflammation bien contrôlée est une réponse normale du corps qui nait s’amplifie et s’éteint. Elle est consécutive à une agression interne (comme un cancer) ou externe (comme une infection). Lorsque le corps n’arrive plus à maîtriser l’inflammation, celle-ci peut engendrer des maladies diverses comme le diabète, le cancer ou devenir chronique comme l’arthrite, la maladie de Crohn par exemple.Des efforts importants ont été déployés pour comprendre les mécanismes moléculaires inflammatoires et comment les combattre. En effet, une inflammation de trop longue durée ou trop intense peut avoir des effets délétères sur l’organe où elle siège et potentiellement entraver sa fonction. Les mécanismes de la phase d’initiation de l’inflammation sont maintenant bien compris. En revanche, les mécanismes de la phase d’arrêt de l’inflammation n’étaient jusque récemment pas connus. Ces dernières années, les travaux de l’équipe du professeur Charles Serhan, de l’École de Médecine de Harvard CETRI (Center for Experimental Therapeutics and Reperfusion : injury), ont permis de comprendre cette phase appelée résolution caractérisée par l’arrêt de la réponse inflammatoire, la réparation du tissu enflammé pour permettre finalement le retour à l’état initial du tissu appelé homéostasie.De nombreuses études scientifiques démontrent ainsi que le corps humain dispose de mécanismes naturels pour contrôler et programmer l’arrêt de l’inflammation. Ces mécanismes portent le nom de Résolution, processus associé à la synthèse d’une famille de molécules spécifiques appelée SPM (en) pour médiateurs spécialisés de la résolution ou Specialized Pro-resolving Mediators en anglais.On peut définir la résolution de l’inflammation comme le processus biologique naturel et indispensable pour stopper naturellement l’inflammation. Ce mécanisme est piloté par des médiateurs appelés SPM issus des acides gras polyinsaturés (AGPI) comme les oméga 3.Les AGPI qui donnent naissances aux SPM sont l’acide arachidonique (AA), l’acide docosahexahenoique (DHA), l’acide eicosapentaénoique (EPA) et l’acide docosapentaénoique (DPA). Ainsi l’AA va donner naissance aux lipoxines, l’EPA au résolvines de type E, le DHA aux résolvines de type D, aux marésines, aux protectines et le DPA aux résolvines de la famille n-3.Dans certains cas, le corps ne produit pas ces molécules en quantité suffisante ou au bon moment. L’arrêt de l’inflammation est alors altéré et peut s’accompagner de complications telles que fibrose, cicatrices ou perdurer de façon chronique.De nombreux travaux ont ainsi permis de mieux comprendre la finesse des mécanismes mis en place naturellement par notre organisme et de démontrer que les réponses inflammatoires chroniques semblent être dues à un défaut de résolution.A la faveur de ces découvertes, l’enjeu pour arrêter l’inflammation n’est donc plus de la bloquer, mais de la réguler en favorisant sa phase de résolution. Ce nouveau champ de recherches porte le nom de pharmacologie de la résolution.Les SPM agissent de façon différente aux anti-inflammatoires et représentent donc une alternative thérapeutique très prometteuse pour arrêter de façon programmée l’inflammation sans la bloquer.Ils agissent en :Contrebalançant l’effet des médiateurs pro-inflammatoires,Diminuant la pénétration des polynucléaires dans les tissus inflammés,Stimulant la défense innée par phagocytose (cellules apoptotiques, pathogènes…),Atténuant la douleur,Favorisant la régénération des tissus,Les SPM présentent des activités biologiques très bénéfiques car ils sont synthétisés au niveau du site inflammatoire et passent dans la circulation sanguine pour exercer leur activité à distance (activité autacoïde). Ils vont permettre ainsi d’arrêter l’inflammation (en inhibant les voies dépendantes de NF-kB par exemple). En permettant un arrêt programmé de l’inflammation, ils évitent le versant fibrotique d’une mauvaise cicatrisation et favorisent les capacités de défense de l’organisme (ils sont non immunosuppresseurs).Il existe actuellement deux méthodes principales pour mesurer les SPM :Les tests EIA (Enzyme Immunosorbant assay). Cette méthode est limitée car peu de kits sont disponibles et ils ne permettent de quantifier qu’un marqueur à la fois alors que les SPM sont une famille de molécule.La chromatographie liquide associé à la spectrométrie de masse (ou LC/MS/MS). Il s’agit de la technique de référence qui permet de coupler sensibilité et robustesse pour l’identification de plusieurs SPM en une analyse unique. Des méthodes de quantification des SPM par LC/MS/MS ont été décrites dans la littérature.Des études cliniques montrent que l’augmentation des SPM dans le corps est corrélée à une amélioration clinique de l’état inflammatoire.C’est le cas par exemple de l’étude Barden publiée en 2016qui a montré une corrélation négative entre la douleur perçue par le patient et la présence de RVE2 dans le liquide synovial suggérant que la production de SPM peut être associée à la gestion naturelle de l’inflammation et de la douleur par le corps humain.En effet dans les modèles animaux d’autres SPM ont également montré des effets atténuateurs sur la douleur,. Ces effets analgésiques sont médiés par des récepteurs spécifiques couplés aux protéines G.Pour aller plus loin dans les recherches sur le rôle des SPMs dans les maladies inflammatoires, l’Union européenne via son programme H2020, a sélectionné et financé le projet immunAID (H2020-EU.3.1.1. Grant agreement no: 779295). Ce projet est coordonné par l’INSERM et il est composé de 24 partenaires dans 12 pays.L’inflammation peut se manifester par :une rougeur (érythème correspondant à une vasodilatation locale) ;un gonflement (œdème) : augmentation de la formation de liquide interstitiel et d'œdème. ;une sensation de chaleur : Augmentation de la température de la zone enflammée. Elle est due à une vasodilatation et à une augmentation de la consommation locale d'oxygène ;une douleur qui semble pulser : La douleur apparaît comme une conséquence de la libération de substances capables de provoquer l'activation de nocicepteurs, telles que les prostaglandines ;une éventuelle altération du fonctionnement de l’organe concerné (ex. : difficulté à bouger (impotence fonctionnelle) dans le cas d'une articulation).On fait parfois référence aux noms latins, notamment dans les langues étrangères, pour décrire les manifestations de l’inflammation. Ces manifestations ont été décrites il y a 2 000 ans par Celsus : rubor (rougeur), calor (chaleur), tumor (gonflement), dolor (douleur), functio laesa (impotence fonctionnelle). Si l’inflammation est connue depuis l’Antiquité, l’impotence fonctionnelle a été rajoutée à sa définition par Rudolf Virchow en 1858.Le phénomène inflammatoire s'accompagne de modifications biologiques telles que l'augmentation de la concentration sanguine de plusieurs protéines dont l'haptoglobine, la céruloplasmine, des globulines, ou la protéine C réactive (CRP). Une électrophorèse des protéines plasmatiques permet d'objectiver ces changements dans leur globalité[pas clair].L'élévation des « protéines inflammatoires » accroît la vitesse de sédimentation.La ferritine augmente, reflétant la séquestration tissulaire du fer sérique. Cette séquestration est secondaire à l'augmentation de la sécrétion d'hepcidine, médiée notamment par l'interleukine 6. Cette séquestration est un des facteurs concourant à l'installation d'une anémie sur le long terme (anémie inflammatoire).Dans certains cas, une polynucléose neutrophile est présente.Des études récentes ont lié l'inflammation chronique à plusieurs types de pathologies, dites « maladies de civilisation » : maladies cardio-vasculaires, diabète et obésité...L'état inflammatoire chronique est reconnu favoriser le développement des tumeurs et a fortiori des tumeurs cancéreuses,.Inflammation aiguëInflammation chroniqueInflammation non spécifiqueInflammation spécifiqueInflammation granulomateuseL’inflammation, est une réaction de défense généralement bénéfique, mais pose parfois problème, par la douleur qu'elle engendre ou lorsqu'elle perdure et devient chronique, risquant alors de nuire à la structure ainsi qu'à la fonction de l'organe touché.Le froid (glace à travers un tissu par exemple) suffit parfois à combattre l’inflammation (il induite une vasoconstriction, diminuant l’œdème et calme la douleur).Des médicaments anti-inflammatoires peuvent calmer les symptômes ou limiter les effets délétères de l'inflammation sur l'organisme. On distingue les anti-inflammatoires non stéroïdiens et les glucocorticoïdes. Ces médicaments existent sous de nombreuses formes (orale, suppositoire, inhalation, perfusion ou bien locale par pommade, collyre...) selon les indications.Des thérapies récentes (biothérapies) bloquent spécifiquement certains médiateurs de l'inflammation (anti-TNF?, anti-IL4…). Elles ont révolutionné la prise en charge de maladies inflammatoires telles que la polyarthrite rhumathoïde ou les spondylarthrites ankylosantes mais avec des effets secondaires.Certains aliments contribuent à réduire l'inflammation - ou ses marqueurs sanguins -, en particulier les omega-3,,, (contenus dans les poissons gras et l'huile de lin ou de colza par exemple), les anthocyanes (contenus dans les fruits rouges et la betterave par exemple), le bêta-glucane (contenu par l'avoine et les grains entiers par exemple), le riz complet, le thym, le curcuma, le gingembre, le chou, l'ananas, l'huile d'olive, les noix, l'ail, les oignons. À l'inverse, les aliments à fort indice glycémique ou à forte charge glycémique (sucre, amidon par exemple) contribuent à augmenter ces marqueurs sanguins.La restriction calorique semble réduire l'inflammation. Dans une étude de restriction calorique portant sur 218 personnes pendant 2 ans, dénommée CALERIE, le taux de Protéine C réactive a baissé de 47 %.Basée sur l’approche développé par le Professeur Charles Serhan, Professeur à l’école de médecine de Harvard, qui a établi le concept de résolution de l’inflammation à la suite de la découverte des SPM, une approche micronutritionnelle peut également être développée pour lutter contre l’inflammation.Elle consiste en premier lieu à apporter à notre corps le substrat qui lui permet d'augmenter la synthèse de SPM lors d’une réponse inflammatoire, qu’elle soit aiguë, chronique ou de bas grade. Cet apport peut se faire via l’ingestion d’acides gras polyinsaturés spécifiques enrichis en SPM ou favorisant la production de SPM.Selon l’endroit où est située l’inflammation, elle peut prendre différents noms, en général en -ite :(en) « Inflammation », Science, vol. 374, no 6571 (numéro spécial),? 26 novembre 2021 (lire en ligne, consulté le 26 novembre 2021)InflammasomeMacrophageCytokineRessources relatives à la santé : (en) Medical Subject Headings (no + nn + nb) Store medisinske leksikon (cs + sk) WikiSkripta  Portail de la biologie   Portail de la médecine"
médecine;Une antibiothérapie est un traitement par antibiotique.Les indications à l'antibiothérapie sont les infections bactériennes.Il existe deux types d'antibiothérapie, la curative et la prophylactique ou préventive (antibioprophylaxie).Si besoin est, un prélèvement à visée bactériologique (afin de déterminer quel est le germe responsable de l'infection) est fait avant le début de l'antibiothérapie. Le choix du ou des antibiotiques est fait par le médecin en fonction du germe en cause, du foyer infectieux, de l'état du malade (terrain), des antibiotiques éventuellement déjà prescrits.Une antibiothérapie peut être le plus souvent une monothérapie à base d'un seul antibiotique ou parfois une bithérapie (deux antibiotiques) voire une trithérapie (trois antibiotiques) dans certains cas. L'association de plusieurs antibiotiques permet dans certains cas d'être plus efficace sur un germe identifié, ou d'avoir un maximum de chance d'être efficace si le germe n'est pas identifié.La durée de l'antibiothérapie varie selon le germe, sa localisation, le terrain, l'évolution de la maladie. Elle doit être décidée par le médecin et est poursuivi très souvent au-delà de l'amélioration des symptômes du malade afin d'éviter toute récidive.La surveillance de l'antibiothérapie se fait sur sa tolérance et l'apparition éventuelle d'effets secondaires, sur l'évolution clinique et biologique, sur les prélèvements devenus négatifs ou non, sur les dosages sanguins d'antibiotiques.Antibiotique | Résistance aux antibiotiques Portail de la médecine   Portail de la pharmacie
médecine;"Une maladie auto-immune est consécutive à une anomalie du système immunitaire conduisant ce dernier à s'attaquer aux composants normaux de l'organisme (le « soi », d'où la racine auto- pour parler de ce trouble de l'immunité).Parmi ces maladies peuvent être citées la sclérose en plaques, le diabète de type 1 — jadis appelé « diabète juvénile » ou « diabète insulino-dépendant » —, le lupus, les thyroïdites auto-immunes, la polyarthrite rhumatoïde, le syndrome de Goujerot-Sjögren, la maladie de Crohn, etc. On distingue classiquement les maladies auto-immunes spécifiques d'organes, qui touchent un organe en particulier (comme par exemple les maladies auto-immunes de la thyroïde), et les maladies auto-immunes systémiques, telles que le lupus, qui peuvent toucher plusieurs organes. Au début du XXIe siècle en Occident, les maladies auto-immunes sont devenues la 3e cause de mortalité/morbidité après le cancer puis les maladies cardiovasculaires et à peu près dans les mêmes proportions.Le système immunitaire est un ensemble de cellules et voies métaboliques conduisant à l'élimination d'une grande variété de pathogènes. Ce système repose sur la notion très centrale du soi opposé au non-soi ainsi qu'au soi modifié. Cette distinction s'effectue grâce à des marqueurs chimiques du soi (c'est-à-dire la reconnaissance de motifs antigéniques plus ou moins spécifiques) mais elle n'est pas véritablement innée : les cellules immunitaires naïves sont d'abord sensibilisées et sélectionnées en fonction de leur réactivité vis-à-vis de ces marqueurs du soi,. Cela explique notamment le fait que les individus chimériques n'expriment pas forcément « plus » d'auto-immunité que des individus monozygotes.Il existe donc chez tous les vertébrés une auto-immunité latente, laquelle est en temps normal inhibée par les mécanismes de régulation de la maturation des cellules immunitaires.On connaît ou suspecte diverses causes possibles :prédispositions génétiques ;facteurs environnementaux diffus :manque d'exposition-immunisation à des entités du non-soi (flore, pathogènes, parasites intestinaux, etc. ; par exemple les personnes non exposées aux parasites intestinaux ont plus de risques de développer une maladie de Crohn et les enfants parasités par des helminthes et dont l'hygiène est moins rigoureuse risquent moins de développer des allergies),dérégulation par des toxiques inhalés, ingérés ou acquis en passage percutané. L'exposition chronique et/ou périodiquement intense par inhalation aux poussières de silice (par exemple issues du sciage, ponçage ou meulage de ciment/béton en est une cause reconnue, confirmée par l'ANSES en 2019) ;séquelles infectieuses ;séquelles allergiques.Dans les pays industrialisés, les maladies auto-immunes touchent environ 8 % de la population, dont 78 % de femmes. Une forte prévalence de maladies auto-immunes (lupus érythémateux disséminé (SLE pour les anglophones) est constatée, sclérose en plaques (SEP),, cirrhose biliaire primitive, polyarthrite rhumatoïde (PR), et thyroïdite de Hashimoto notamment) chez les femmes. L'évolution de nombreuses maladies auto-immunes, leur gravité et leur pronostic varie aussi selon le sexe. Ceci n'est pas encore clairement expliqué, bien qu'il ait été prouvé que les taux d'hormones sont liés à la gravité de certaines maladies auto-immunes dont la sclérose en plaques.Chez les humains, et dans le modèle animal, le système hormonal semble avoir une importance majeure dans plusieurs phénomènes liés à ces maladies ; par exemple, les maladies auto-immunes sont plus fréquentes chez les personnes ayant une dysthyroïdie que dans la population générale, ce qui peut laisser supposer des mécanismes physiopathologiques communs et « justifie une surveillance des patients ayant une dysthyroïdie auto-immune et la réalisation d'un bilan initial et d'un suivi thyroïdien régulier chez les patients ayant une maladie auto-immune »,.En laboratoire chez le modèle murin, les femelles sont également plus touchées que les mâles par des maladies telles que le lupus érythémateux disséminé spontané (souris de souches (NZB×NZW)F1 et NZM.2328), l'encéphalomyélite allergique expérimentale (EAE, pour Experimental autoimmune encephalomyelitis) chez la souris SJL, la thyroïdite, le syndrome de Sjögren chez les souris de souche MRL/Mp-lpr/lpr, et pour le diabète chez les souris NOD. Les hormones sexuelles et/ou le patrimoine génétique hérité lié au sexe semblent donc être responsables de la sensibilité accrue des femmes à ces maladies auto-immunes.Chez l’animal, certains œstrogènes, la progestérone et les androgènes préviennent ou atténuent les signes cliniques des maladies auto-immunes alors que la castration chez le mâle les aggrave.Du fait de leurs propriétés immunologiques, promyélinisantes, neurotrophiques et neuroprotectrices des œstrogènes, progestatifs et androgènes, une régulation hormonale pourrait peut-être moduler l'évolution de maladies telles que la sclérose en plaques (qui est plus rare et plus tardive chez l'homme que chez la femme, mais plus grave). Chez les femmes, le rythme des poussées de cette maladie diminue en fin de grossesse, puis progresse après l'accouchement, alors que les sécrétions hormonales chutent. D'autres maladies auto-immunes semblent pouvoir répondre à une médication de type hormonal. L'influence de perturbateurs endocriniens pourrait possiblement être l'un des facteurs explicatifs de la récurrence croissante de certaines maladies auto-immunes.D'autre part — de manière générale — les femmes ont une réponse immunitaire différente que celle des hommes,, ; elles répondent notamment à l'infection, à une vaccination ou à des traumatismes avec une production plus importante d'anticorps et une production accrue de lymphocytes T auxiliaires Th2 (réponse immunitaire humorale prédominante), alors qu'une réponse par les lymphocytes T auxiliaires Th1 et l'inflammation sont généralement plus sévères chez les hommes. Cette différence d'intensité et de qualité de réponse immunitaire semble au moins en partie responsable de la plus grande vulnérabilité des femmes à un nombre important de maladies auto-immunes. À l'importance du sexe sur la réponse immunitaire s'ajoutent parfois les additionnels de l'importance du sexe sur les organes cibles de ces maladies auto-immunes, telles que le CNS dans la MS (Cerghet et al. 2006 ; Spring et al. 2007).Chez les deux sexes, les maladies auto-immunes commencent par une phase aiguë associée à une réponse immunitaire inflammatoire pour évoluer vers une phase chronique associée à la fibrose, mais des différences marquées existent selon le sexe :les maladies auto-immunes qui sont plus fréquentes chez les hommes se manifestent habituellement cliniquement avant cinquante ans. Et elles sont caractérisées par une inflammation aiguë, l'apparition d'autoanticorps, et une réponse immunitaire pro-inflammatoires de type Th1 ;les maladies auto-immunes qui prédominent chez les femmes se manifestent avec une phase aiguë (ex. : maladie de Basedow, lupus érythémateux systémique, sont des maladies connues pour être des pathologies médiées par des anticorps. Et les maladies auto-immunes qui ont une incidence accrue chez les femmes semblent cliniquement actives après l'âge de 50 ans et associés à une maladie chronique, fibrosique et « Th2-médiée ». Les Th17 augmentent l'inflammation par les neutrophiles et la fibrose chronique.Le sexe du sujet est donc un facteur à considérer comme particulièrement important dans les études sur l'auto-immunité, concernant les processus physiopathologiques du système immunitaire et des organes-cibles concernés.Les auto-anticorps sont des anticorps (Ac) dirigés contre des éléments de l'organisme qui les a fabriqués ; leur nombre est élevé.Certains de ces auto-anticorps sont plus fréquemment retrouvés dans certaines maladies appelées maladie auto-immune.Ces maladies sont :Anémies hémolytiques auto-immunes ;Anémie de Biermer (anémie pernicieuse) : Ac anti-facteur intrinsèque, anti-cellule pariétale ;Cholangite sclérosante primitive : Ac anti-cytoplasme des polynucléaires neutrophile ;Dermatite herpétiforme : Ac anti-gliadine, Ac anti-endomysium ;Diabète de type 1 : Ac anti-cellules Bêta du pancréas ;Épidermolyse bulleuse acquise : Ac anti-collagène VII ;Hypothyroïdie : Ac anti-thyropéroxydase ;Lupus érythémateux : Ac anti-ADN natif, Ac anti-Sm ;Maladie cœliaque : Ac anti-endomysium, Ac anti-gliadine, Ac anti-transglutaminase ;Maladie de Berger : Ac anti-glomérule rénal ;Maladie de Basedow : Ac anti-récepteurs de la TSH ;Maladie de Crohn : Ac anti-Saccharomyces cerevisiae ;Myasthénie : Ac anti-récepteur de l'acétylcholine (Ac anti-RACh) ;Pemphigoïde bulleuse : Ac anti-glycoprotéine intégrine ;Pemphigus profond : Ac anti-desmogléine ;Polymyosite : Ac anti-Jo 1, PL7, PL12, OJ, EJ ;Purpura thrombopénique idiopathique : Ac anti-plaquettes.Rectocolite hémorragique : Ac anti-cytoplasme des polynucléaires neutrophilesSyndrome de CREST (Le terme CREST tend à disparaître on parle actuellement de sclérodermie limitée) : Ac anti-centromères ;Syndrome de Stiff Man (ou maladie de l'homme raide) : Ac anti-GAD 65 et Ac anti-GABA ;Sclérodermie systémique : Ac anti-Scl 70 ;Syndrome myasthénique de Lambert-Eaton : Ac anti-canaux calciques voltage-dépendants (Ac anti-VGCC) ;Syndrome de Goujerot-Sjögren : Ac anti-SSA, Ac anti-SSB. ;Thyroïdite d'Hashimoto : Ac anti-thyroglobuline, Ac anti-thyropéroxydase ;Les maladies suspectées d'être des maladies auto-immunes sont :L'hidradénite suppurée ou Maladie de VerneuilÀ ce jour, il n'existe pas de traitement curatif. Il existe plusieurs traitements « suspensifs », qui limitent l'expression des symptômes mais qui ont leurs limites en raison de leur toxicité pour le système immunitaire et certaines cellules.De nouveaux traitements sont envisagés pour bloquer l'effecteur même. Ce sont souvent les mêmes médicaments que ceux utilisés pour éviter les rejets de greffe d'organes.Les principales molécules utilisées visent à supprimer l'activation de cellules à problème et/ou à les tuer ; ce sont ;les glucocorticoïdes (traitement de 1re ligne, et le plus ancien). Il est peu ciblant : le médicament aussi dit anti-inflammatoire freine toutes les réponses immunitaires, en affectant la prolifération des lymphocytes T ou B à dose normale et pouvant même les tuer à forte dose ; le médicament sert ici in fine à inhiber la production de cytokines. Les effets secondaires des corticoïdes sont nombreux, ce qui pousse les chercheurs à trouver de nouvelles molécules immunosuppressives, dont :le cyclophosphamide (qui tue les cellules en train de se diviser, et qui agit sur les lympohcytes B et moindrement T) ;l'Azathioprine ;l'acide mycophénolique ;des molécules agissant au stade de l'activation (plus précocement, en inhibant la calcineurine au moment de la reconnaissance des antigènes) ; elles sont utilisées depuis le début des années 1980 ; elles présentent l'avantage de ne pas tuer les cellules et d'avoir des effets plus réversibles, mais l'inconvénient d'être moins efficace sur des patients ayant déjà une maladie auto-immune avancée ; le traitement doit être précoce (exemple : Tacrolimus...) ;des inhibiteurs de la voie M-Tor 1 (ex. : Sirolimus) qui ont un effet antiprolifératif sur les lyphocytes T surtout.des inhibiteurs du protéasome (ex. : Bortézomib) ; uniquement actif contre les lymphocytes B et les plasmocytes (initialement créé pour traiter le cancer du plasmocyte) ;des anticorps (essentiellement monoclonaux, mais aussi polyclonaux), sous forme de « sérums anti-lymphocitaires », mais ils tuent tous les lymphocytes, y compris ceux qui ne sont pas impliqués dans l'autoimmunité ;des anticorps ; ce sont par exemple l'Alemtuzumab qui cible la molécule CD52 pour détruire des lymphocytes T. On utilise aussi des anticorps ciblant la molécule CD20 pour détruite des lymphocytes B, ou encore l'anti-IL2R contre les lymphocytes T activés (prévu pour traiter les greffes d'organes), ou des anticorps ciblant la cytokine TNF (utile pour certaines maladies de Crohn ou la polyarthrite rhumatoïde), des anticorps ciblant des cytokines telles que l'IL17 (interleukine 17), ou l'IL23, ou l'IL6R et/ou leur récepteur. On utilise aussi un anticorps ciblant la CTLA4 Ig (limitent les molécule régulatrice CD80 eet CD 86 en cause dans certains phénomènes auto-immuns).La recherche vise à utiliser des anticorps ciblant mieux leurs cibles (lymphocytes T et/ou B), ce qui permettrait de traiter certaines maladies auto-immunes avec moins de toxicité pour le patient.Les effets iatrogènes (secondaires) liés à la toxicité des médicaments sont un problème majeur du traitement des maladies auto-immunes, car ils sont par exemple source d'hypertension, de risque de cancer (si traitement au long-cours) ; l'immunosuppression facilite les infections virales, moindre production de cellules sanguines, neurotoxicité.4 approches se dessinent au début du XXIe siècle :La première vise à neutraliser des cellules effectrices telles que la TH17 (qui produit l'IL17) qui semblent impliquées et retrouvées dans les lésions de l'arthrite rhumatoide, du syndrome de Sjögren (ou syndrome de Gougerot-Sjögren), de la sclérose en plaques ou de la maladie de Crohn. Encore mieux serait de convertir des cellules effectrices en cellules normales, ce qui semble possible d'après quelques données expérimentales sur le modèle animal ;Une seconde approche consiste à tenter de moduler la présentation des antigènes ;La troisième approche vise à inhiber les effets des interférons ;La quatrième voie cherche à induire et amplifier (le nombre et/ou la fonction) des lymphocytes T régulateurs ; sur le modèle murin en laboratoire on a montré en 2015 que dans un même organisme malade (y compris au pic de la maladie), on peut trouver certaines TH17 sont des effectrices pathogènes et responsables de la maladie, alors que d'autres sont non-pathogènes et même régulatrices.(en) Ronald Asherson (éd.): Handbook of Systemic Autoimmune Diseases, Elsevier, en 10 volumes [3] :Ronald Asherson, Andrea Doria, Paolo Pauletto: The Heart in Systemic Autoimmune Diseases, Volume 1, 2004,  (ISBN 978-0-444-51398-4),  (ISBN 0-444-51398-1)Ronald Asherson, Andrea Doria, Paolo Pauletto: Pulmonary Involvement in Systemic Autoimmune Diseases, Volume 2, 2005,  (ISBN 978-0-444-51652-7),  (ISBN 0-444-51652-2)Ronald Asherson, Doruk Erkan, Steven Levine: The Neurologic Involvement in Systemic Autoimmune Diseases, Volume 3, 2005,  (ISBN 978-0-444-51651-0),  (ISBN 0-444-51651-4)Michael Lockshin, Ware Branch (éd.): Reproductive and Hormonal Aspects of Systemic Autoimmune Diseases, Volume 4, 2006,  (ISBN 978-0-444-51801-9),  (ISBN 0-444-51801-0)Piercarlo Sarzi-Puttini, Ronald Asherson, Andrea Doria, Annegret Kuhn, Giampietro Girolomoni (éd.): The Skin in Systemic Autoimmune Diseases, Volume 5, 2006,  (ISBN 978-0-444-52158-3),  (ISBN 0-444-52158-5)Rolando Cimaz, Ronald Asherson, Thomas Lehman (ed.): Pediatrics in Systemic Autoimmune Diseases, Volume 6, 2008,  (ISBN 978-0-444-52971-8),  (ISBN 0-444-52971-3)Justin Mason, Ronald Asherson, Charles Pusey (éd.): The Kidney in Systemic Autoimmune Diseases, Volume 7, 2008,  (ISBN 978-0-444-52972-5),  (ISBN 0-444-52972-1)Ronald Asherson, Manel Ramos-Casals, Joan Rodes, Josep Font: Digestive Involvement in Systemic Autoimmune Diseases, Volume 8, 2008,  (ISBN 978-0-444-53168-1),  (ISBN 0-444-53168-8)Ronald Asherson, Sara Walker, Luis Jara: Endocrine Manifestations of Systemic Autoimmune Diseases, Volume 9, 2008,  (ISBN 978-0-444-53172-8),  (ISBN 0-444-53172-6)R. Cervera, Ronald Asherson, Munther Khamashta, Joan Carles Reverter (éd.): Antiphospholipid Syndrome in Systemic Autoimmune Diseases, Volume 10, 2009,  (ISBN 978-0-444-53169-8),  (ISBN 0-444-53169-6)Carole Émilea: Comment faire le diagnostic de maladie auto-immune systémique ?, in: ""Immunologie"", mai 2009, volume 20, numéro 418-9, page 29 DOI:10.1016/S0992-5945(09)70132-1 [4](en) Vinay Kumar, Abul K. Abbas, Nelson Fausto, Jon Aster: Robbins and Cotran Pathologic Basis of Disease, Elsevier, 8e édition, 2010, 1464 pp.,  (ISBN 978-1-4160-3121-5)Stephani Sutherland, « Une nouvelle vision de l'auto-immunité », Pour la science, no 531,? janvier 2022, p. 24-32Melinda Wenner Moyer, « Les femmes surexposées ? », Pour la science, no 531,? janvier 2022, p. 34-39Frédéric Rieux-Laucat et Loïc Mangin, « Il y a un fort parallèle entre auto-immunité et cancer », Pour la science, no 531,? janvier 2022, p. 40-45Alain Fischer (2016), Comment traiter les maladies autoimmunes ?, Cours/conférence du 31 mai 2016 15:00 16:30 Cours Amphithéâtre Guillaume Budé - Marcelin BerthelotSystème immunitaireMaladie autoinflammatoire Portail de la biologie   Portail de la médecine"
médecine;"La maladie de Basedow ou Graves-Basedow est une hyperthyroïdie auto-immune (maladie de la thyroïde). La personne atteinte produit des anticorps anormaux (stimulant le recepteur de la TSH) dirigés contre les cellules folliculaires de la thyroïde. Plutôt que de détruire ces cellules, comme le ferait tout anticorps normal, ces anticorps reproduisent les effets de la TSH et stimulent continuellement la libération d'hormones thyroïdiennes, provoquant une hyperthyroïdie accompagnée de signes cliniques spécifiques. La maladie de Basedow ou de Graves, plus fréquente chez la femme que chez l'homme, se manifeste le plus souvent par une accélération du métabolisme basal, une diaphorèse, des pulsations cardiaques rapides et irrégulières, une augmentation de la nervosité et une perte pondérale. Il s'agit de sa forme la plus fréquente.Elle doit son nom à Carl von Basedow.La maladie de Basedow peut toucher tout le monde, mais essentiellement les individus entre 40 et 60 ans et plus rarement à l'adolescence. Elle est cinq à dix fois plus fréquente chez les femmes. Elle est la cause de près des trois quarts des hyperthyroïdies.Il existe un facteur génétique comme l'atteste une atteinte concomitante chez de vrais jumeaux ainsi que la présence d'antécédents familiaux.Comparativement au tabagisme actif persistant, la cessation de consommation de tabac diminue le risque de développer une maladie de Basedow, notamment dans sa forme oculaire, et plus particulièrement chez les femmes.Décrit à de nombreuses reprises, ce syndrome doit son nom à Carl von Basedow mais peut avoir plusieurs dénominations :goitre exophtalmique ;goitre toxique diffus ;hyperthyroïdie auto-immune ;maladie de Graves ;maladie de Graves-Basedow.La maladie est probablement de cause multifactorielle. Il existe une participation génétique et plusieurs gènes sont impliquées : CD40, CTLA4, PTPN22, FCRL3, gènes de la thyroglobuline et au récepteur à la TSH.Le stress peut avoir un rôle provocateur. Le tabagisme est un facteur de risque.La maladie est plus fréquente chez la femme, faisant suspecter une participation génétique et/ou hormonale. D'autres facteurs ont été identifiés : infection à Yersinia enterocolitica, déficit en sélénium ou en vitamine D.L'auto-immunité se développe à partir des anticorps anti-récepteurs de la TSH, dans lequel le corps fabrique des anticorps pour le récepteur de la thyroïde-stimulant hormone (TSH-R). Ces anticorps se lient aux récepteurs TSH qui se trouvent sur les cellules qui produisent des hormones thyroïdiennes, ce qui entraîne une production anormalement élevée de T3 et T4. C'est une hypersensibilité de type V.Ils comprennent l'association de signes d'hyperthyroïdie et de signes plus spécifiques de la maladie.Elle se caractérise par une asthénie, un amaigrissement contrastant avec un appétit conservé voire augmenté, une hypersudation, des attitudes d'évitement de la chaleur. Il peut exister des troubles psychologiques, une agitation, une nervosité, un tremblement, une soif permanente avec augmentation des mictions (polyurie-polydipsie)L'examen clinique montre une fréquence cardiaque accélérée (tachycardie), voire un rythme irrégulier.Les signes et symptômes sont les suivants :goitre (augmentation de volume de la thyroïde) ;exophtalmie (déplacement de l'œil hors de son orbite) ;myxœdème (infiltration cutanée) au niveau des tibias ;gynécomastie, (développement excessif des glandes mammaires chez l'homme) ;augmentation du rythme cardiaque ;augmentation de l'activité métabolique ;plus rarement, hippocratisme digital (déformation du doigt et des ongles).L'hyperthyroïdie est démontrée biologiquement par l'effondrement du taux de TSH dans le sang, couplé à un taux élevé de triiodothyronine (T3) et de thyroxine (T4). En cas de maladie de Basedow, il existe une élévation du taux d'anticorps anti-récepteur de la TSH (« TRAK »), hautement spécifique et sensible, et des TSI (Thyroid stimulating immunoglobulins).La scintigraphie thyroïdienne consiste en la visualisation du captage (fixation) par cet organe d'un composé radioactif. Elle montre typiquement une thyroïde augmentée de volume et hyperfixante. Elle permet de différencier la maladie de Basedow d'autres causes d'hyperthyroïdie, comme un nodule, dit « toxique », par exemple.La vascularisation du goitre est augmentée et peut être démontrée par un doppler de la thyroïde. L'échographie de la glande, couplée au doppler, a des résultats comparables à ceux de la scintigraphie. L'échographie permet par ailleurs de détecter les nodules et distinguer une thyroïdite d'un Basedow (baisse du débit dans le premier cas).L'imagerie peut ainsi mettre en évidence dans près de 35 % des cas de maladie de Basedow des nodules thyroïdiens associés, généralement non fonctionnels. Dans environ 1% des cas, il est retrouvé un ou des nodules fonctionnels en plus de la maladie de Basedow (syndrome de Marine-Lenhart).Insuffisance surrénalienneMyasthénieThyroïdite d'HashimotoDiabète de type 1Polyendocrinopathie auto-immuneSyndrome de MeansUne rémission spontanée se fait dans près d'un tiers des cas. Le risque de rechute à court terme après arrêt des antithyroïdiens de synthèse peut être prédit par la persistance d'un taux élevé d'anticorps anti-récepteur de la TSH. Il reste toutefois élevé, dépassant 50 %.Les trois options sont la prescription de médicaments antithyroïdiens de synthèse (méthimazole et propylthiouracile), l'ablation chirurgicale de la thyroïde et l'emploi d'iode radioactif (iode 131 : irathérapie), détruisant ainsi sélectivement la glande. Ces trois traitements ont une efficacité comparable mais le taux de rechute est plus élevé avec les médicaments (près de 40 %) qu'avec les deux autres méthodes. Le choix de l'une ou l'autre des options dépend d'un certain nombre de paramètres, dont font partie les habitudes locales (les États-Unis recourant de manière beaucoup plus fréquente que l'Europe à l'iode radioactif en première intention).Pour les antithyroïdiens de synthèse, deux techniques de prescription peuvent être utilisés : la titration (ou recherche de la dose minimale efficace) ou la prescription à doses importantes conduisant à une hypothyroïdie et à une substitution hormonale associée. dans tous les cas, un dosage des hormones thyroïdiennes doit être fait au premier mois, puis tous les trois mois si l'hyperthyroïdie est équilibrée. Le traitement est prolongé jusqu'à 18 mois, voire plus.La prise en charge de l'atteinte oculaire a fait l'objet de la publication de recommandations par l'European Group on Graves' Orbitopathy en 2008. L'exophtalmie nécessite une prévention des lésions oculaires, secondaire à une couverture insuffisante des paupières avec un risque de lésion de la cornée. Plus rarement, elle doit être traitée pour elle-même.Louis-Ferdinand Céline en 1933, met au point un produit, La Basedowine, enregistré au Laboratoire National de contrôle des médicaments sous le no 343-4 et commercialisé par les Laboratoires Gallier jusqu'en 1971.Glafira Ziegelmann, première femme interne de Montpellier et la première femme admissible à l'agrégation de médecine, consacre en 1898 sa thèse au traitement de la maladie de Basedow. Portail de la médecine"
médecine;"Une maladie infectieuse (ou infection) est une maladie provoquée par l'invasion d'un ou plusieurs micro-organismes ou agent infectieux (bactéries, champignons, parasites, protozoaires, virus) dans un tissu où ils se multiplient, et par une réaction générale des cellules et des tissus infectés pour éliminer ces agents pathogènes ou leurs toxines (processus impliquant notamment le système immunitaire des plantes et des animaux).L'étude des agents infectieux relève de la biologie, de la microbiologie médicale, de l'épidémiologie et de l'écoépidémiologie. Dans la nature, des maladies infectieuses se développent chez tous les organismes vivants (animaux, végétaux, fongiques, micro-organismes… il existe également des virus de virus). En tant qu'interactions durables, les maladies infectieuses font partie des boucles de rétroaction qui entretiennent la stabilité relative (équilibre dynamique) des écosystèmes, la plupart des pathogènes coévoluant avec leur hôte depuis des millions d'années. Leur mode de transmission est variable et dépend de leur réservoir (humain, animal, environnemental) et parfois de vecteurs (maladies vectorielles).Elles sont plus ou moins contagieuses. Par exemple, le tétanos est une toxi-infection causée par Clostridium tetani, une bactérie qui se trouve dans la terre. Il n’y a pas de transmission interhumaine, l’infection se produit lorsque la bactérie entre dans l’organisme par une plaie souillée. Un vaccin existe contre cette affection et est obligatoire en France pour tous les enfants d’âge scolaire. Autre exemple, le paludisme est dû à un parasite, le Plasmodium falciparum (il existe d’autres Plasmodii), transmis d’homme à homme par l’intermédiaire d’un moustique, l’anophèle. Le réservoir du parasite est humain mais il n’y a pas de transmission interhumaine. Il n’existe à l'heure actuelle pas de vaccin. La tuberculose se transmet d’homme à homme par mécanisme aéroporté : le réservoir est humain et c’est une maladie contagieuse. Les infections sexuellement transmissibles (ou encore MST pour maladies sexuellement transmissibles) se transmettent à l’occasion de rapports sexuels ou par le sang.De nombreux microbes vivent normalement et nécessairement dans notre tube digestif et sur notre peau, et ne deviennent infectieux qu'à certaines occasions. Le contact avec les microbes est nécessaire à l'entretien et au bon fonctionnement de la digestion et du système immunitaire.L'infection est le terme désignant soit une maladie infectieuse en général, soit la contamination par un germe. C'est la conséquence pathologique au niveau d'un tissu ou d'un organisme de la présence anormale et/ou de la réplication d’un germe bactérien, viral ou mycosique. La contamination est la pénétration du germe dans un organisme.L'infectiologie est la branche de la médecine concernant les maladies infectieuses. Le médecin spécialiste est un infectiologue. Suivant le type de germe, il est également question de bactériologie, de virologie, de parasitologie ou de mycologie.Un sepsis est une infection grave. L'adjectif septique se rapporte à un organisme ou un objet contaminé par un germe (fosse septique par exemple). Une septicémie est la contamination grave et durable (sans traitement) du sang par un germe. Une bactériémie est une contamination transitoire du sang par un germe. Lorsque les cas se multiplient dans un lieu et une période limitée, il est question d’épidémie. Si la diffusion est beaucoup plus généralisée, il est alors question de pandémie. Lorsque l'épidémie concerne le milieu animal, il est question d'épizootie. Lorsque le germe se transmet de l’animal à l’homme, il est question d'anthropozoonose ou plus simplement de zoonose.Le contage désigne la contamination par le germe.La période d’incubation est le délai entre le contage et la première manifestation de la maladie. Le malade peut être contagieux durant ce temps.La période de contagion est le temps pendant lequel le patient excrète le germe et peut le transmettre. Elle dépend de chaque maladie infectieuse.Les infections nosocomiales (ou iatrogènes) sont des infections attrapées à l’hôpital. Elles sont particulièrement complexes et dangereuses car elles surviennent chez des sujets affaiblis et concernent souvent des germes résistants aux antibiotiques. Il s’agit d’un problème de santé publique majeur.Comme le résumait en 1935 le bactériologiste français Charles Nicolle : « Malheureusement, les signes des maladies infectieuses sont presque tous les mêmes : fièvre, maux de tête, agitation ou stupeur, éruption. Seuls leur groupement, leur succession, une observation minutieuse ont pu, après de longs tâtonnements, permettre d'établir des tableaux symptomatiques particuliers et les distinguer entre eux. »Les maladies infectieuses sont responsables dans le monde de 17 millions de décès par an, soit un tiers de la mortalité et 43 % des décès dans les pays en voie de développement (contre 1 % dans les pays industrialisés). Les six maladies suivantes représentent 90 % des décès par maladies infectieuses dans le monde.Depuis les années 2000, de nombreuses urgences sanitaires reliées à l’émergence de nouveaux agents étiologiques responsables de maladies respiratoires sévères sont survenues : le syndrome respiratoire aigu sévère (SRAS), les infections d’influenza aviaire A (H5N1) chez les humains dans plusieurs pays de l’Asie, la pandémie de grippe A (H1N1) et, plus récemment, le virus influenza aviaire A (H7N9) en Chine, le coronavirus du syndrome respiratoire du Moyen-Orient (MERS-CoV) et la pandémie de Covid19. La pathogénicité et la létalité élevées de la plupart de ces virus génèrent des répercussions sociales et une pression importante sur les services de santé.La population mondiale infectée par le VIH continue de croître : rien qu’en 2000, 5,3 millions de nouveaux cas se sont déclarés dans le monde, dont la moitié parmi les jeunes de plus de 25 ans.Après une phase de forte régression (époque pastorienne et hygiéniste), les maladies infectieuses sont revenues ou sont devenues plus résistantes (antibiorésistance). Des maladies infectieuses émergentes ou réémergentes inquiètent périodiquement les épidémiologistes et les autorités sanitaires en raison de leurs impacts sanitaires, économiques et socio-politiques actuels ou potentiels. Le Haut Conseil de la santé publique (HCSP) a récemment fait 25 recommandations (sur la recherche et l'enseignement, la surveillance sanitaire et la gestion raisonnée des crises sanitaires notamment).Les progrès de l'hygiène et de la vaccination ont fourni un espoir de pouvoir les éradiquer, mais elles sont encore en France, la troisième cause de mortalité :Il est également noté que certaines infections sont aussi à l’origine de maladies inflammatoires chroniques (telles que l’asthme) et de cancers.Les maladies infectieuses entravent la santé de base des individus et ont une influence négative sur chaque indice du développement humain et plus particulièrement sur l'espérance de vie à la naissance, l'éducation et le PIB réel. Elles sont responsables d'une forte mortalité dans les régions où l'hygiène connaît un déficit et où l’accès aux soins est difficile. La malnutrition ainsi qu'un accès limité à l'eau potable sont autant de facteurs aggravants qui diminuent les chances de survie des malades mais aussi des enfants en bas âge de même que leurs conditions de développement. Ces deux facteurs désarment le système immunitaire et peuvent être vecteurs de maladies infectieuses.Ces maladies ont des conséquences négatives importantes sur le développement cognitif et les performances scolaires chez l’enfant. La malaria, entre autres, peut causer de graves séquelles, dont des troubles comportementaux, des problèmes moteurs et un manque d’autonomie. Une telle infection est donc un frein à l’éducation. Dans le cas des épidémies, il peut arriver que les enseignants soient eux aussi touchés par la maladie. Un manque de corps enseignant réduirait de façon directe la qualité de l’éducation en affaiblissant le système scolaire. Par ailleurs, si dans une famille, les responsables de l'éducation des enfants (souvent la mère) sont touchés par la maladie, c’est l’éducation dans son ensemble qui va être affectée. Le coût du traitement réduit le budget qui aurait pu être accordé à la scolarisation mais également les conditions de vie de l’enfant. Ce qui crée un cercle vicieux : les couches les plus éduquées de la population sont de moins en moins atteintes par des maladies infectieuses telles que le sida. En effet ces personnes qui sont les plus éduquées sont les mieux informées sur les modes de transmission et de prévention. Or, plus de 80 % des personnes atteintes par ces maladies vivent dans les pays en développement.D'un point de vue macroéconomique, les maladies infectieuses ont un impact sur la croissance économique et le PIB. Dans les pays en développement, la main d’œuvre est le facteur-clé de la production et donc du PIB. Néanmoins, le bon fonctionnement des entreprises et la possibilité d'être concurrent sur le marché international nécessitent avant tout une bonne santé et une éducation de base. Lorsque la santé de la personne génératrice de revenu pour la famille est affectée, toute la famille en souffre. Les maladies infectieuses aggravent donc la pauvreté, réduisent la croissance économique, le capital humain et contribuent à l’augmentation des inégalités entre les pays en voie de développement et les pays riches.La prévention des maladies infectieuses vise à limiter le risque infectieux (y compris professionnel, notamment pour les métiers de la santé, de contact avec les animaux, des déchets, des cadavres, des eaux usées, des échantillons à analyser en laboratoires de biologie, etc.).Elle s’articule en trois volets : éviter l’infection, renforcer les défenses immunitaires et prendre des traitements préventifs (prophylaxie) en cas de risque d’exposition.La maladie infectieuse est provoquée par la pénétration dans l’organisme d’une bactérie ou d’un virus. La première précaution consiste donc à « fermer les portes d’entrée », à savoir :les voies respiratoires : tousser ou éternuer dans un mouchoir, dans le coude, ou dans les mains (en se les lavant immédiatement après) pour éviter de contaminer l’entourage ; porter un masque facial lorsque des personnes vulnérables sont rencontrées (par exemple dans certaines zones des milieux hospitaliers, personnes immunodéprimées) ou porteuses de virus très contagieux (comme le sras) ; pour la ventilation artificielle, utiliser un filtre antibactérien ;les voies digestives : se laver les mains avant de manger ou de préparer un repas, ou après une exposition à des liquides biologiques (par exemple en sortant des toilettes), voire les désinfecter lorsqu’il s’agit de liquides d’une autre personne (par exemple accident d'exposition au sang) ; porter des gants fins (latex, ou pour les personnes allergiques en PVC ou nitrile) lorsqu’une telle exposition est probable ; en général laver les mains régulièrement pendant la journée ;effraction cutanée : toute plaie grave devra être montrée à un médecin qui prendra les mesures nécessaires ; toute plaie simple doit être nettoyée, ou mieux désinfectée (voir l’article bobologie) ; mais la première précaution est bien sûr d’éviter de se faire une plaie, en respectant les règles de sécurité de certaines activités et en portant des protections adaptées (gants de travail…) ;voie oculaire : éviter de se frotter les yeux et se laver les mains avant au cas où cela arriverait ; en cas de risque d’exposition à des liquides biologiques, porter des lunettes de protection ;sexualité : utiliser un préservatif pour réduire les risques de transmission des maladies sexuellement transmissibles.Le port d'équipements de protection individuelle dépend de l’évaluation des risques. Au travail outre des gants de protection, un appareil de protection respiratoire et des lunettes masques ou une visière sont parfois nécessaires, voire un vêtement de protection intégral.Les gants fins sont recommandés en cas de risque d’exposition à des liquides biologiques ou chimiques, mais déconseillé pour les activités courantes : en effet, la peau est alors dans une atmosphère chaude et humide propice au développement de germes, et par ailleurs, il vaut mieux des mains propres que des gants sales. À noter qu’au bout d’une vingtaine de minutes, certains gants fins deviennent poreux ou sont incompatibles avec certaines substances.Il faut aussi limiter le développement de germes pathogènes sur et dans le corps et dans l’habitation, par une hygiène suffisante :hygiène corporelle : se laver, se brosser les dents ;hygiène ménagère : avoir un réfrigérateur créant un froid suffisant, décongelé et nettoyé régulièrement, laver les couverts, assiettes et verres après utilisation, stocker les ordures dans des poubelles dédiées et ramassées régulièrement par les services municipaux, évacuation des eaux usagées vers une fosse septique vidangée régulièrement ou vers les égouts, rangement et nettoyage de l’habitation, aérer pour limiter la pollution intérieure (acariens, composés organiques volatils) et donc les allergies et les maladies respiratoires ;surveiller et traiter les parasitoses (certaines facilitent les maladies infectieuses, virales ou bactériennes). Par exemple, chez le porc, l'ascaris augmente le risque de bronchopneumonie, la trichocéphalose l'entérite hémoragique, l'oesophagostomum les salmonelloses, les strongyloides le rouget, les metastrongylus la grippe porcine, etc.Les collectivités territoriales jouent un rôle important en ce qui concerne l’hygiène collective, avec la gestion des eaux pour fournir de l’eau potable, l’organisation de la collecte et du traitement des ordures, l’équarrissage des cadavres d’animaux et la police des funérailles et des lieux de sépulture (condition de transport et de conservation des corps avant crémation ou inhumation, gestion des cimetières et crématoriums).La première mesure consiste à avoir une bonne hygiène de vie : alimentation saine, exercice physique régulier, sommeil suffisant, éviter les comportements à risque (tabagisme, excès d’alcool), ce qui permet d’avoir un meilleur état de santé général donc de mieux résister aux infections.Par ailleurs, il convient de respecter les vaccinations préventives obligatoires, ou recommandées comme la vaccination des personnes âgées contre la grippe.Il faut aussi prendre précautionneusement les médicaments prescrits par un médecin, en lisant systématiquement les notices accompagnatrices, riches en informations (effets secondaires, interactions avec d’autres médicaments, recommandations…) et ne pas hésiter à questionner le médecin ou le pharmacien en cas de doute. Les effets peuvent ne pas être immédiats, et il faut continuer le traitement jusqu’à la fin même en cas d’amélioration et disparition des symptômes, notamment dans le cas des antibiotiques : la disparition des symptômes signifie la diminution du nombre de germes, mais pas leur disparition, si le traitement est interrompu trop tôt, ceux-ci peuvent se redévelopper, et devenir résistants à l’antibiotique.Il ne faut pas que le médecin prescrive systématiquement d’antibiotique : ils ne sont pas efficaces contre les maladies virales.Les mesures d’hygiènes simples sont les meilleurs traitement préventifs : lavage des mains, pour éviter la transmission des infections alimentaires, éternuer dans ses coudes lors d'un Éternuement et non pas dans ses mains afin de ne pas les « contaminer » par d'éventuels microbes… Il est parfois nécessaire de prendre des médicaments à titre préventif, comme les médicaments contre le paludisme lors d’un voyage dans un pays impaludé.La détection précoce d’une maladie permet de démarrer son traitement plus tôt et donc de réduire la mortalité ; il est recommandé de faire au moins une visite médicale par an. En cas de doute sur une infection (par exemple plaie souillée, accident d’exposition au sang, rapport sexuel non protégé), le médecin pourra mettre en place un traitement préventif pour diminuer les risques de développement d’une maladie. Pour les maladies sexuellement transmissibles, il existe en France des centres anonymes et gratuits de dépistage.Certains patients doivent être isolés (voire mis en quarantaine) pour éviter la dissémination du germe : ainsi, lors d’une varicelle, l’enfant ne doit pas aller à l’école pendant 15 jours à partir de la première éruption. Il s’agit de l'éviction scolaire. La prévention hospitalière des infections nosocomiales est un sujet complexe. Elle repose essentiellement sur l’hygiène des soignants et des soignés (lavage des mains), sur l’isolement des patients porteurs de germes résistants aux antibiotiques, mais aussi sur une antibiothérapie ciblée et adaptée.Une nouvelle approche en phase d'étude est d'utiliser la phagothérapie à des fins préventives pour la santé humaine comme cela se fait déjà dans l'agriculture et l'industrie alimentaire.Leur étude relève de l'épidémiologie et pour les zoonoses ainsi que de l'écoépidémiologie.Certaines situations (crises sanitaires ou alimentaires…) ou lieux (ports, aéroports) sont des facteurs de risques.Le traitement par antibiotiques est le traitement qui a permis de vaincre les maladies infectieuses jusqu'à l'apparition des bactéries multi-résistantes.Il présente de nombreux avantages dont la possibilité d'une fabrication en masse, rapide et bon marché des médicaments antibiotiques.Il trouve ses limites avec l'apparition de bactéries de plus en plus résistantes.La phagothérapie est apparue au début du XXe siècle avec le développement par le Français Félix d'Hérelle de médicaments bactériophagiques réalisés à partir de virus bactériophages (simplement appelés bactériophages ou même phages) lytiques afin de traiter certaines maladies infectieuses d’origine bactérienne. D'Hérelle a ainsi traité des épidémies de peste et de choléra avec succès.La phagothérapie a été largement utilisée dans le monde avant la découverte des antibiotiques. Si elle a été progressivement abandonnée par les pays occidentaux séduits par les avantages de l’antibiothérapie, la phagothérapie traditionnelle est toujours employée et développée dans les pays de l'ancienne Union Soviétique. Dans les pays occidentaux, des patients victimes d'infection par bactéries multi-résistantes se regroupent pour faciliter l'accès aux traitements bactériophagiques étrangers,,.Elle connaît un regain d'intérêt en Occident avec l'émergence de l'antibiorésistance. Elle fait l'objet de recherches à l'Institut Pasteur mais son utilisation demeure soumise à ATUn par l'ANSM.Antoine van Leeuwenhoek (1632-1723) voit pour la première fois des agents bactériens en microscopie.Louis Pasteur permet le rapprochement entre maladie et agents infectieux. Première vaccination contre la rage.Robert Koch est célèbre pour sa découverte du bacille de la tuberculose qui porte son nom : le bacille de Koch.Jonas Salk et Albert Sabin assurent le développement de la vaccination anti-polio.Charles Nicolle, Destin des maladies infectieuses, PUF 1939Brown L. (2010), ""Le plan B pour un pacte écologique mondial"", Paris, Calmann-Lévy Souffle Court Editions, 509 pages.Contrepois A. L'invention des maladies infectieuses. Édition des Archives Contemporaines. 2001. Naissance et développement institutionnel de la bactériologie médicale en France et en Allemagne au XIXe siècle.Flahaut A. et Zylberman P. Des épidémies et des hommes. Édition de la Martinière. 2008. Une bonne vulgarisation par deux experts de la question, avec nombreuses photos et illustrations.INSTITUT PASTEUR , ""Le défi des maladies infectieuses"", http://www.pasteur.fr/ip/easysite/pasteur/fr/presse/dossiers-de-presse/sante-en-voyage/le-defi-des-maladies-infectieuses, dernière visite le 8 mars 2014.Nicolle C. Le destin des maladies infectieuses. Édition France Lafayette. 1993. Réédition d'un grand classique de 1933. Conférences au Collège de France par Charles Nicolle, Prix Nobel de Médecine 1928. Toujours d'actualité.Orth G. et Sansonetti P. (sous la direction de). La maitrise des maladies infectieuses. Académie des Sciences. EDP Sciences. 2006. État des lieux et recommandations adressées aux pouvoirs publics et à l'ensemble des acteurs de santé. Un ouvrage collectif à l'aspect sévère, mais une actualisation pointue de tous les aspects (médico-scientifiques, socio-culturels, etc.) du problème.Raoult D. (1999), ""Les nouvelles maladies infectieuses, que sais-je ?"", Presses universitaires de France, 128 pages.Dossier documentaire Société Française de Santé PubliqueRessources relatives à la santé : ICD-10 Version:2016 ICD9Data.com (en) Diseases Ontology (en) Medical Subject Headings (en) NCI Thesaurus (no + nn + nb) Store medisinske leksikon (cs + sk) WikiSkripta Infectiologie.com Portail de la médecine   Portail de la microbiologie   Portail des maladies infectieuses"
médecine;Les maladies inflammatoires chroniques de l’intestin (MICI) regroupent les maladies liées à l'inflammation de l'intestin à caractère chronique.Leur origine est inconnue, mais qui dans un certain nombre de cas au moins pourraient être liées à une prédisposition génétique (conséquence de l'évolution humaine et de l'aseptisation de l'environnement actuel) et aux faibles doses de nombreux résidus de produits toxiques contenus dans l'alimentation moderne. Des études de 2015 ont montré un accroissement du nombre de bactériophages du virobiote qui implique une baisse de la diversité microbienne.La maladie peut se définir comme une inflammation chronique de l'intestin qui induit à la fois une modification du microbiote qui en retour entretient l'inflammation. La modification du microbiote (déficit dans certaines bactéries) est par ailleurs multifactorielle: génétique, alimentation, interaction avec le virobiote (phages), traitements antibiotiques.Parmi les maladies concernées, il faut distinguer ses deux principales formes que sont :la maladie de Crohn pouvant concerner tout le tube digestifla rectocolite hémorragique (ou colite ulcéreuse) limitée aux régions du rectum et parfois du côlonBien que différentes maladies existent, différents symptômes sont similaires :douleurs abdominalesdiarrhéevomissementrectorragie ou hémorragie rectale (hématochézie)perte ou gain de poidsainsi que des manifestations extradigestives dans 25 % des cas, comme l'arthrite par exemple.L'âge moyen de découverte d'une MICI se situe entre 15 et 35 ans, en général à la suite de différents examens, tels que :IRM fonctionnelleendoscopie digestive, telle que coloscopie (ou colonoscopie), anuscopie ou rectoscopie par exemplebiopsiedéfécographiemanométrie anorectaleendosonographieélectromyographieéchographie endoanalepet scancapsule vidéo endoscopiqueDifférent traitements sont prescrits, selon le niveau de gravité de la maladie. Dans les cas les plus graves les MICI peuvent exiger une immunosuppression afin de contrôler les symptômes, via des médicaments tels que l'azathioprine, le méthotrexate ou la mercaptopurine, voire une forme de mésalazine.L'inflammation du tube digestif amène des complications nutritionnelles et en particulier des carences que le traitement cherchera à compenser. Chez 126 patients atteints de maladies inflammatoires de l'intestin les carences concernaient l'hémoglobine (40 %), la ferritine (39,2 %), la vitamine B6 (29 %), le bêta-carotène (23,4 %), la vitamine B12 (18,4 %), la vitamine D (17,6 %), l'albumine (17,6 %) et le zinc (15,2 %). Toutes ne sont pas liées aux apports alimentaires inadéquats (fréquents dans ces maladies amenant à des choix alimentaires particuliers) : vitamine E (63 %), vitamine D (36 %), vitamine A (26 %), calcium (23 %), acide folique (19 %), fer (13 %), et vitamine C (11 %).Chez les enfants récemment diagnostiqués, une carence en zinc est aussi observée, tout particulièrement chez les enfants atteints de la maladie de Crohn.Des compléments alimentaires et un bilan régulier des teneurs en vitamine B6 sont recommandées.L'anémie, fréquente dans les MICI, doit être avérée avant la prise de compléments de fer.L'Inserm a mis en évidence un déficit en Elafine dans la pathologie, et envisage une bactérie probiotique génétiquement modifiée. Une étude anglaise révèle que la consommation de fructose peut aggraver l'inflammation intestinale des personnes atteintes de maladies inflammatoires chronique de l'intestin.Les MICI peuvent limiter grandement la qualité de vie en raison des douleurs et autres conséquences pouvant nécessiter une hospitalisation. Toute personne atteinte d'une MICI ne peut guérir, mais doit suivre un traitement à vie et fait l'objet d'un suivi médical pour surveiller et traiter l’évolution de la maladie : risque de fistule intestinale et risque accru de cancer colorectal.Microbiote intestinalRessources relatives à la santé : Orphanet (en) Diseases Ontology (en) DiseasesDB (en) Medical Subject Headings (en) NCI Thesaurus «MICI, chronique d’un intestin malade», La Méthode scientifique, France Culture, 20 janvier 2020 Portail de la médecine
médecine;"La maladie est une altération des fonctions ou de la santé d'un organisme vivant.On parle aussi bien de la maladie, se référant à l'ensemble des altérations de santé, que d'une maladie, qui désigne alors une entité particulière caractérisée par des causes, des symptômes, une évolution et des possibilités thérapeutiques propres.Un ou une malade est une personne souffrant d'une maladie, qu'elle soit déterminée ou non. Lorsqu'elle fait l’objet d'une prise en charge médicale, on parle alors de patient(e).La santé et la maladie sont liées aux processus biologiques et aux interactions avec le milieu social et environnemental. Généralement, la maladie se définit comme une entité opposée à la santé, dont l'effet négatif est dû à une altération ou à une désharmonisation d'un système à un niveau quelconque (moléculaire, corporel, mental, émotionnel…) de l'état physiologique ou morphologique considérés comme normal, équilibré ou harmonieux. On peut parler de mise en défaut de l'homéostasie.Les termes maladie et malade proviennent du latin male habitus signifiant qui est en mauvais état.Ce terme est unique en français, italien et espagnol, alors que l'anglais et l'allemand disposent de doublons tels que illness et disease, Erkrankung et Krankheit qui expriment des distinctions particulières de sens.Il n'existe pas de terme commun désignant la maladie dans le groupe des langues indo-européennes, on note l'existence de nombreux synonymes dont la signification étymologique appartient à quatre champs sémantiques :la faiblesse, la perte de force, l'incapacité à travailler ;la difformité et la laideur ;la gêne, le trouble, le malaise ;la souffrance et la douleur.Le concept initial d'état morbide ou de maladie s'appuie sur un critère objectif (incapacité de fournir un travail pour soi ou pour la société), et un critère subjectif (de la gêne ou indisposition à la douleur aiguë).Ce concept n'est pas socialement neutre, car il implique un jugement moral et esthétique : il y a la maladie, mais aussi le mal, le mauvais, et le laid. Disease, illness, sickness En français, les termes « maladie » et « malade » sont utilisés de façon indistincte pour signifier « avoir une maladie » (reconnue par un médecin), « être malade » (se sentir mal), « être un malade » (être reconnu comme tel par l'entourage ou la société).L'anglais utilise trois termes, plus ou moins interchangeables, mais en principe utilisés le plus souvent dans un contexte spécifique. Disease se rapporte à une perturbation biomédicale, objectivée par une maladie reconnue par un médecin, dans le cadre d'une pathologie référencée (nosologie).Illness se rapporte à l'expérience vécue, personnelle et intime, de la maladie : « je me sens, ou je suis, malade ».Sickness se rapporte à la perception de la maladie dans le cadre de l'entourage non-médical (social ou culturel) : «je suis un malade» (reconnu comme tel). Limites et extensions Il a été montré en 1989 que plus les étudiants en médecine étaient avancés dans leur cursus plus ils avaient tendance à qualifier de maladie les conditions parmi 38 qui leur étaient présentées, sans que cette qualification n'ait de lien fort avec les propriétés de gravité, curabilité, responsabilité du patient ou causalité externe. L'idée de maladie, plutôt qu'être parfaitement définie, évolue donc chez l'étudiant en fonction de son avancement dans le cursus.Classifier un certain état comme une maladie est aussi un fait social d'évaluation. Ainsi, certains états ne sont reconnus comme des maladies que dans certaines cultures, ou à certaines époques, et pas dans d'autres. On parle alors de syndromes culturels. Parfois la catégorisation d'un état comme une maladie est controversé au sein d'une même société. L'hyperactivité et l'obésité sont par exemple des états de plus en plus considérés comme des maladies par l'opinion publique dans les pays occidentaux mais n'étaient pas ainsi considérés il y a encore quelques décennies, et ne le sont toujours pas dans certains pays.La maladie est à différencier des blessures, handicaps, syndromes et affections.Une blessure est une lésion, physique ou psychique.Un handicap est une déficience qui peut aussi bien être due à une maladie qu'à une blessure.Un syndrome est un ensemble de signes ou de symptômes qui apparaissent simultanément. Ainsi l'usage médical distingue une maladie, qui a une cause spécifique connue, d'un syndrome, qui ne se préoccupe pas des causes.Une affection désigne une altération de fonctions qui est rattachée à un organe spécifique et qui ne prend en compte ni les causes, ni les symptômes, ni le traitement. Tout comme les syndromes, elle est parfois distinguée d'une maladie.Par extension, on peut associer la maladie à des entités non biologiques pour signifier qu'elles sont altérées ou que leur fonctionnement n'est plus considéré comme bon. Il est ainsi habituel d'entendre les termes « société malade » ou « entreprise malade » par exemple.La maladie humaine est le noyau fondateur de la médecine, une grande partie de la connaissance médicale étant orientée vers la maladie et ses solutions.La science vétérinaire concerne les maladies qui affectent les animaux, dont les zoonoses.La phytopathologie est la science qui concerne les maladies qui affectent les plantes et autres sujets botaniques.La pathologie est la branche de la médecine traitant des causes et des symptômes des maladies dans leur ensemble. Le terme est souvent utilisé fautivement pour désigner la maladie elle-même, ou ses manifestations, y compris par des médecins.La pathogénie est l'étude des mécanismes responsables du déclenchement et du développement d'une maladie.L'ontologie est l'étude de la genèse des entités médicales telles que les maladies, les signes cliniques et les syndromes.L'étiologie est l'étude spécifique des causes et des facteurs d'une maladie.La séméiologie, ou sémiologie médicale, est la branche de la médecine qui traite des signes cliniques et des symptômes des maladies et de la façon de les présenter.Le diagnostic est la réflexion menant à l'identification de la nature d'une maladie à partir des symptômes relevés par les observations.Le pronostic est la prévision de la progression de la maladie et des chances éventuelles de guérison.La prophylaxie désigne le processus ou l'ensemble de mesures visant à prévenir la propagation ou l'apparition d'une maladie.La thérapeutique est la section de la médecine s'occupant de l'étude des traitements.La nosologie est la branche de la médecine qui étudie les critères de classification systématique des maladies.Les facteurs des maladies sont le domaine d'étude de l'étiologie et physiologie. Catégorisation des facteurs  Facteurs intrinsèques et extrinsèques Il existe de nombreux facteurs différents pouvant entraîner l'apparition d'une maladie.Ces facteurs peuvent être aussi bien intrinsèques qu'extrinsèques à l'organisme concerné par la maladie.La présence d'un facteur intrinsèque n'exclut pas celle d'un facteur extrinsèque, et inversement. Ainsi, de nombreuses maladies résultent d'une combinaison de facteurs intrinsèques et extrinsèques. Liste Les facteurs peuvent être répartis dans les catégories suivantes :Facteurs chimiquesFacteurs économiquesFacteurs sociauxFacteurs psychologiquesFacteurs biologiquesFacteurs environnementauxLes facteurs environnementaux incluent les produits chimiques toxiques (par exemple les acétaldéhydes dans la fumée de cigarette et les dioxines relâchées lors de l'utilisation d'Agent orange) et les agents infectieux (par exemple les virus de la varicelle ou de la polio).Certains facteurs peuvent faire partie de plus d'une catégorie. Facteurs biochimiques C'est le cas des causes biochimiques de maladies qui peuvent être considérées comme un spectre où à l'une des extrémités la maladie est causée exclusivement par des facteurs génétiques (par exemple les répétitions CAG dans le gène HD (ou gène huntingtine ou encore gène IT15) qui cause la maladie de Huntington) et à l'autre causée entièrement par des facteurs environnementaux.Entre ces deux extrêmes, gènes et facteurs environnementaux interagissent pour causer la maladie comme c'est le cas pour la maladie inflammatoire appelée maladie de Crohn où les gènes NOD2/CARD15 et la flore intestinale jouent chacun un rôle. L'absence de facteur génétique ou environnemental dans ce cas a pour résultat l'absence de manifestation de la maladie. Étude des facteurs environnementaux Les postulats de Koch peuvent être utilisés pour déterminer si une maladie est causée par un agent infectieux. L'émergence de nouvelles maladies infectieuses est liée aux activités humaines perturbant l'équilibre des écosystèmes.Par exemple, l'Institut de recherche pour le développement indique que « le déboisement des forêts primaires reste l'une des causes principales de l'apparition de nouveaux agents infectieux et de leur circulation épidémique dans les populations humaines ». En effet, les forêts jouent un rôle essentiel pour la biodiversité terrestre, élément stabilisateur des agents pathogènes. Étude des facteurs génétiques Pour déterminer si une maladie est causée par un facteur génétique, les chercheurs étudient la présence de la maladie dans l'arbre généalogique familial.Cela fournit des informations qualitatives à propos de la maladie, c'est-à-dire comment elle est héritée.Un exemple classique de cette méthode de recherche est l'héritage de l'hémophilie dans la famille royale britannique. Plus récemment cette méthode a été utilisée pour identifier le gène Apoliprotéine E (ApoE) comme un gène susceptible d'être lié à la maladie d'Alzheimer, bien que certaines formes de ce gène (ApoE2) en soient moins susceptibles.Pour déterminer jusqu'à quel point une maladie est causée par des facteurs génétiques, c'est-à-dire pour obtenir des informations quantitatives, des études sur des jumeaux sont effectuées. Les jumeaux monozygotes sont génétiquement identiques alors que les jumeaux dizygotes sont seulement génétiquement similaires. De plus des jumeaux, qu'ils soient monozygotes ou dizygotes, partagent souvent un environnement similaire. Ainsi en comparant l'incidence de la maladie (nommée taux de concordance) chez des jumeaux monozygotes avec l'incidence de la maladie chez des jumeaux dizygotes, la contribution de chaque gène à la maladie peut être déterminée.Les gènes suspects peuvent être identifiés grâce à plusieurs méthodes. L'une d'entre elles est la recherche de mutation d'un organisme modèle (par exemple les organismes Mus musculus, Drosophila melanogaster, Caenhorhabditis elegans, Brachydanio rerio et Xenopus tropicalis) qui possèdent un phénotype similaire à la maladie étudiée. Une autre approche est la recherche de ségrégation de gènes ou l'utilisation de marqueurs génétiques (par exemple les polymorphismes nucléotidiques et marqueurs de séquences exprimées). Maladies complexes Les maladies complexes sont dues à l'interaction entre un profil génétique particulier et un environnement particulier. Quelques exemples :ObésitéDiabète sucréHypertension artérielleAthérome et athéroscléroseAsthmeMaladies dysimmunitaires ou auto-immunesMaladies neurodégénératives (maladie d'Alzheimer, maladie de Parkinson, sclérose latérale amyotrophique)Un symptôme se distingue d'un signe. Le symptôme est l'expression subjective des effets ressentis par le malade alors que les signes en sont l'expression objective déduite par le médecin, ou plus généralement de la personne réalisant un diagnostic.Certaines maladies sont contagieuses ou infectieuses, comme c'est le cas par exemple de l'influenza (ou grippe). Les maladies infectieuses peuvent être transmises par un grand nombre de mécanismes, incluant l'expulsion de particules dans l'air lors d'un éternuement ou d'une toux, les fomites (objets contaminés par des pathogènes), les morsures et piqûres d'insectes ou autres animaux vecteurs porteurs de la maladie, et l'absorption d'eau ou de nourriture contaminée.Il existe également des infections ou maladies sexuellement transmissibles (MST ou IST). Ce sont des maladies infectieuses qui se transmettent au cours de rapports sexuels, ou de contacts sanguins. Au début du XXIe siècle, un des principaux représentants de ces maladies est le SIDA. Un représentant plus ancien est la syphilis.Certaines maladies sont dites non transmissibles, elle ne se transmettent pas directement. Il y a par exemple les maladies liées à l'environnement.Une des principales mesures permettant d'éviter la propagation d'une maladie parmi une population ou seulement le développement d'une maladie chez un individu est la prévention.Elle peut se décomposer en trois parties :La prévention, qui a pour but de réduire la probabilité d'apparition de la maladie (ex : vaccination).La prévision, qui doit prévoir des mesures pour combattre le sinistre si celui-ci survient.La protection, qui a pour but de limiter l’étendue et la gravité de la maladie ou de l'épidémie, lorsqu'elle est déjà présente (ex : amputation, quarantaine).En médecine, on parle plus particulièrement de prophylaxie, le processus qui vise à prévenir les épidémies et la propagation d'une maladie. La prophylaxie est, plutôt qu'un traitement médical, une promotion de la prise de conscience générale des bonnes conduites à adopter face à la maladie.Les principales mesures de prévention de la maladie sont l'amélioration de l'hygiène et la vaccination.La thérapeutique est la section de la médecine s'occupant de l'étude des traitements.Les traitements consistent souvent, suivant le niveau évolutif de la société humaine concernée, en la prise de médicaments à base de molécules de synthèse ou bien de remèdes produits à partir de l'environnement naturel. Il existe toutefois de nombreuses autres thérapies, telles la radiothérapie ou la kinésithérapie, n'ayant pas recours à l'ingestion et à l'injection de substances extérieures.L'identification d'un état comme une maladie, plutôt que comme une simple variation de la structure humaine ou de fonctions, peut avoir des implications sociales et économiques significatives et peut changer le statut social de l'être concerné.La maladie peut parfois entraîner l'exclusion sociale des personnes touchées. Un exemple est l'exclusion des lépreux, courante en Europe depuis le Moyen Âge, et leur regroupement dans des établissements appelés léproseries dans le but de limiter la propagation de la maladie par contagion.La peur de la maladie a été et est encore un phénomène social très répandu, bien que toutes les maladies, notamment les plus bénignes, n'aient pas ce genre de répercussions sociales.Dans certains pays, les maladies infectieuses les plus dangereuses, du point de vue du risque épidémique, sont des maladies à déclaration obligatoire, c'est-à-dire qu'elles doivent être déclarées aux autorités dès qu'elles sont diagnostiquées par le médecin ou le vétérinaire.Certains dispositifs ont également été mis en place dans de nombreux pays pour éviter ou compenser les effets néfastes de la maladie. C'est dans cette optique qu'est apparue l'assurance maladie, qui est un dispositif chargé d'apporter une compensation financière à un individu subissant ou ayant subi une maladie.Une dérive consiste à élargir les descriptions nosographiques des maladies tout en y sensibilisant le grand public afin d'augmenter le marché de certains fournisseurs de traitements contre ces mêmes maladie. Cette pratique est appelée le disease mongering.L'étude des différentes classifications de la maladie concerne la branche de la médecine appelée « nosologie ».Il existe différentes tentatives de classification des maladies. Toutefois, du fait de la constante évolution de la médecine, elles ne sont pas figées. Les maladies peuvent être catégorisées en fonction de leurs causes et facteurs, de leurs symptômes ou des fonctions et organes touchés. On parle alors respectivement de classification étiologique, nosographique et fonctionnelle. Classification étiologique Maladies par agents physiques (froid, chaleur, etc.)Maladies toxiques (produits chimiques, poisons, etc.)Maladies parasitaires (champignons, vers, etc.)Maladies infectieuses (virus, bactéries, etc.)Maladies traumatiques (chocs psychologiques ou physiques, brûlures, etc.)Maladies dyscrasiques (troubles des métabolismes, troubles génétiques, etc.)Maladies psychiques (facteurs psychiques, bien que ces maladies puissent aussi avoir les mêmes facteurs que les maladies précédentes) Classification fonctionnelle Dysfonctionnements moléculaires (au niveau de la molécule)Dysfonctionnements cellulaires (au niveau de la cellule)Dysfonctionnements organiques (au niveau de l'organe)Dysfonctionnements corporel (au niveau d'un système d'organes)Dysfonctionnements mental (au niveau psychologique)On peut également séparer les maladies en :maladies aiguës et maladies chroniques, suivant qu'elles aient un développement rapide ou étalé ;en maladies bénignes et maladies malignes, suivant leur gravité ;en maladies locales et maladies générales, suivant l'étendue de la zone touchée ;en maladies évitables et inévitables.L'Organisation mondiale de la santé publie et est responsable de l'évolution de la Classification internationale des maladies, poursuite des travaux de Jacques Bertillon. Cette classification permet le codage des maladies, des traumatismes et de l'ensemble des motifs de recours aux services de santé grâce aux codes CIM (ou ICD en anglais). Elle permet également l'analyse systématique et l'interprétation des causes de morbidité et de mortalité dans le monde entier. Son but est notamment l'organisation et le financement des services de santé.De nombreuses cultures ont tenté de donner une signification et une origine à la maladie.Dans la mythologie grecque, l'apparition de la maladie est expliquée par l'ouverture de la boîte de Pandore. Zeus, qui voulait se venger des hommes à la suite du vol du feu par Prométhée, ordonne la création de Pandore, femme qu'il envoie auprès du frère de ce dernier. Pandore apporte avec elle une boîte qu'il lui est interdit d'ouvrir. La curiosité la pousse à le faire tout de même et c'est ainsi qu'elle libère la maladie et les autres maux de l'humanité que la boîte contenait.Au Proche-Orient ancien, l'origine naturelle de la maladie est concevable, mais elle se rajoute à une origine surnaturelle, par exemple la colère des dieux, la première étant la conséquence de la seconde.À partir de 1860, la pensée tendait vers l'idée que les homosexuel(le)s souffraient plutôt d'une maladie. Cette position de la communauté médicale et scientifique a perduré jusque vers les années soixante, où plusieurs voix se sont manifestées pour remettre en question cette vision de l'homosexualité. En 1974, l'Association américaine de psychiatrie a éliminé l'homosexualité de sa liste des maladies mentales, le Manuel diagnostique et statistique des troubles mentaux. Le 17 mai 1990, c'était au tour de l'Organisation mondiale de la santé de prendre la même position et de retirer l'homosexualité de sa Classification internationale des maladies dans sa dixième version (CIM-10).La maladie a inspiré de nombreuses créations artistiques.Le personnage du malade tient par exemple la place centrale dans Le Malade imaginaire, la dernière comédie écrite par Molière.Mais aussiHôpital général, de Slaughter : Tous les aspects de la médecine y sont représentés : l’organisation hospitalière…L’Hôpital, d’Alfonse BoudardLe Pavillon des cancéreux de Soljenitsyne : le cancerLa Mort du pantin, de Pierre MoustiersUn cri, de Michèle LoriotLa Peste, de Camus : les épidémiesLe Hussard sur le toit de Giono : Le choléraOpération épidémie, de SlaughterLa Montagne magique de Thomas Mann : La tuberculoseUn grand patron, de Pierre Véry : la formation médicaleLe Destin de Robert Shanon, de CroninLe Médecin de campagne de Balzac : l’exercice de la médecineLe Docteur Pascal de ZolaVoyage au bout de la nuit de L.-F. CélineLes Hommes en blanc, d’André SoubiranLe Livre de San Michele, d’Axel MuntheSept morts sur ordonnance, de Georges Conchon : les problèmes morauxKnock, de Jules Romain : l’arrivisme, les tentations et dérives de la médecineLes Grandes Familles, de Maurice Druon : la tentation des honneurs avec le professeur LartoyLes Thibault, de Roger Martin du GardOscar et la Dame rose, d’Eric-Emmanuel Schmitt.Philippe Adam et Claudine Herzlich, Sociologie de la maladie et de la médecine (1994), Paris, Armand Colin, 2014.Marc Augé et Claudine Herzlich (dir.), Le Sens du mal. Anthropologie, histoire, sociologie de la maladie, Bruxelles, Éditions des archives contemporaines, coll. « Ordres sociaux », 1984.Philippe Batifoulier, Capital-Santé. Quand le patient devient client, Paris, La Découverte, 2014.Frédéric Bauduer, Histoires des maladies et de la médecine, Paris, Ellipses, coll. « Sciences humaines en médecine », 2017.Henri Bergeron et Patrick Castel, Sociologie politique de la santé, Paris, PUF, coll. « Quadrige », 2014.Max Blecher, Aventures dans l’irréalité immédiate, suivi de Cœurs cicatrisés, trad. d’Elena Guritanu, Paris, L’Ogre, 2015.Max Blecher, La Tanière éclairée, trad. par Georgeta Horodinca et Hélène Fleury, Paris, Maurice Nadeau, 1989.Norbert Elias, La Solitude des mourants (1982), trad. Sybille Muller, suivi de Vieillir et mourir : quelques problèmes sociologiques, trad. Claire Nancy, Paris, Christian Bourgois éditeur, 1987.Dr. Christophe Fauré, Vivre ensemble la maladie d'un proche, Albin Michel, 2002.Jean-Claude Fondras, Santé des philosophes, philosophes de la santé, Nantes, éditions nouvelles Cécile Defaut, 2014.Elodie Giroux et Maël Lemoine (dir.), Philosophie de la médecine. Santé, maladie, pathologie, Paris, Vrin, 2012.Xavier Guchet, La Médecine personnalisée. Un essai philosophique, Paris, Les Belles Lettres, 2016.Hervé Guibert, À l'ami qui ne m'a pas sauvé la vie, Paris, Gallimard, coll. « Folio », 1990.Céline Lefève, Lazare Benaroyo et Frédéric Worms (dir.), Les Classiques du soin, Paris, PUF, 2015.Thomas Mann, La Montagne magique (1924), trad. Maurice Betz, Paris, Le Livre de poche, 1991.Claire Marin, Violences de la maladie, violence de la vie, Paris, Armand Colin, 2008.Ruwen Ogien, Mes Mille et Une Nuits : la maladie comme drame et comme comédie, Paris, Albin Michel, 2017.Roselyne Rey, Histoire de la douleur, Paris, La Découverte, coll. « Histoire des sciences », 1993 ; nouvelle édition avec des postfaces de Jean Cambier et Jean-Louis Fischer, Paris, La Découverte, 2011.Susan Sontag, La Maladie comme métaphore (1977, 1978), trad. Marie-France de Paloméra, suivi de Le Sida et ses métaphores, trad. Bruce Matthieussent, Paris, Christian Bourgois éditeur, 1993.Virginia Woolf, De la maladie (1930), trad. Élise Argaud, Paris, Payot & Rivages, 2007.Ressources relatives à la santé : Orphanet (en) Classification internationale des soins primaires (en) Diseases Ontology (en) Medical Subject Headings (en) NCI Thesaurus (no + nn + nb) Store medisinske leksikon (cs + sk) WikiSkripta (fr) Site officiel de l'Organisation mondiale de la santé(fr) Site officiel du Ministère de la Santé, de la Jeunesse et des Sports français.(fr) (en) Site officiel de Santé Canada.(fr) Site officiel du Ministère de la Santé et des Services sociaux du Québec(fr) Site officiel du Service public fédéral de la santé belge.(fr) Site officiel du Ministère de la Santé du Congo-Kinshasa(fr) Site officiel du Ministère de la Santé du Luxembourg(fr) (en) (de) (it) Site officiel de l'Office fédéral de la santé publique suisse.(fr) Classification étionosographique des pathologies sur psychobiologie.ouvaton.org Portail de la médecine"
médecine;"La médecine (du latin : medicina, qui signifie « art de guérir, remède, potion »), au sens de pratique (art), est la science témoignant de l'organisation du corps (anatomie), son fonctionnement normal (physiologie), et cherchant à préserver la santé (physique comme mentale) par la prévention (prophylaxie) et le traitement (thérapie) des maladies. La médecine humaine est complémentaire et en synergie avec la médecine vétérinaire.La médecine contemporaine utilise l'examen clinique, les soins de santé, la recherche et les technologies biomédicales pour diagnostiquer et traiter les blessures et les maladies, habituellement à travers la prescription de médicaments, la chirurgie ou d'autres formes de thérapies.Il n'existe pas suffisamment de données fiables pour déterminer le début de l'usage des plantes à des fins médicinales (phytothérapie). Les données médicales contenues dans le Papyrus Edwin Smith peuvent être datées du XXXe siècle av. J.-C.. Les premiers exemples connus d’interventions chirurgicales ont été réalisés en Égypte aux alentours du XXVIIIe siècle av. J.-C. (voir chirurgie). Imhotep sous la troisième dynastie est parfois considéré comme le fondateur de la médecine en Égypte antique et comme l'auteur originel du papyrus d’Edwin Smith qui énumère des médicaments, des maladies et des observations anatomiques. Le papyrus gynécologique Kahun traite des maladies des femmes et des problèmes de conception. Nous sont parvenues trente-quatre observations détaillées avec le diagnostic et le traitement, certains d'entre eux étant fragmentaires. Datant de 1800 av. J.-C., il s’agit du plus ancien texte médical, toutes catégories confondues. On sait que des établissements médicaux, désignés par l’expression Maisons de vie ont été fondés dans l’Égypte antique dès la première dynastie.Les plus anciens textes babyloniens sur la médecine remontent à l’époque de l’ancien empire babylonien dans la première moitié du IIe millénaire av. J.-C. Cependant, le texte babylonien le plus complet dans le domaine de la médecine est le Manuel de diagnostic écrit par Esagil-kin-apli le médecin de Borsippa, sous le règne du roi babylonien Adad-ALPA-iddina (1069-1046 av. J.-C.).Hippocrate, est considéré comme le père fondateur de la médecine moderne et rationnelle,, et ses disciples ont été les premiers à décrire de nombreuses maladies. On lui attribue la première description des doigts en baguette de tambour, un signe important pour le diagnostic de la bronchopathie chronique obstructive, du cancer du poumon et des cardiopathies cyanogènes congénitales. Pour cette raison, le symptôme des doigts en baguette de tambour est parfois appelé hippocratisme digital . Hippocrate a également été le premier médecin à décrire la face hippocratique. Shakespeare fait une allusion célèbre à cette description dans sa relation de la mort de Falstaff dans Henry V, acte II, scène III,. Le Corpus hippocratique popularise la théorie des humeurs. La médecine rationnelle grecque et latine coexiste cependant pendant toute l'Antiquité avec les cultes des Dieux guérisseurs.Agnodice (Hagnodice) ou Hagnodikè (en grec ancien : ????????) fut, selon une légende grecque rapportée par Hygin (Caius Julius Hyginus) dans la 274e de ses Fabulae, l'une des premières femmes médecin et gynécologue. Issue de la haute société athénienne, elle se déguisa en homme pour suivre les cours de médecine du célèbre médecin Hérophile. Vers 350 av. J.-C., elle passa l'examen et devient gynécologue, mais sans révéler qu'elle était une femme.La médecine pratiquée et enseignée en occident a ses racines dans les connaissances acquises et protocolées de l'Antiquité au Ier millénaire av. J.-C. de l'Orient à l'Empire romain.Elles proviennent de la Torah, étonnement rationnelle en la matière, car tenant compte des conditions climatiques. En effet, les cinq livres de Moïse qui la constituent, contiennent diverses « lois » ayant des conséquences directes sur la santé à travers différents rituels, tels que l'isolement des personnes infectées (Lévitique 13:45-46), le lavage des mains après avoir manipulé un cadavre (Livre des Nombres 19:11-19) et l’enfouissement des excréments à l’extérieur du campement (Deutéronome 23:12-13).La traduction dans les années 830-870 de 129 œuvres du médecin grec Galien (1er siècle av J.C.) en arabe par Hunayn ibn Ishaq et ses élèves sert de modèle à la médecine des civilisations islamiques et se propage rapidement à travers l’Empire arabe, reprenant en particulier, l'insistance de Galien sur une approche rationnelle et systématique de la médecine. Qusta ibn Luqa joua aussi un rôle important dans la traduction et la transmission des textes grecs. Les médecins musulmans ont mis en place certains des premiers hôpitaux, institution qui importée en Europe à la suite des croisades.En Europe occidentale, l'effondrement de l'autorité de l’empire romain a conduit à l’interruption de toute pratique médicale organisée. La médecine était exercée localement, alors que le rôle de la médecine traditionnelle augmentait, avec ce qui restait des connaissances médicales de l'antiquité. Les connaissances médicales ont été préservées et mises en pratique dans de nombreuses institutions monastiques qui s’étaient souvent adjoint un hôpital et disposaient de carrés d'herbes médicinales. Une médecine professionnelle organisée est réapparue, avec la fondation de l’école de médecine de Salerne en Italie au XIe siècle qui, en coopération avec le monastère du Mont Cassin, a traduit de nombreux ouvrages byzantins et arabes.À partir du XIe siècle, l'Église veut dissocier la vocation de moine de la profession de médecin. La volonté d'encadrer le savoir aboutit à la formation d'universités aux mains des ecclésiastiques. Les médecins de l'université de médecine de Montpellier, dépositaires des doctrines des médecins juifs et arabes, privilégient les plantes, ceux de l'Ancienne université de Paris privilégient la purge et la saignée.Au XIXe siècle, Karl August Wunderlich publie Das Verhalten der Eigenwärme in Krankheiten, qui établit que la fièvre est seulement un symptôme et met fin au credo d'une maladie infectieuse jusqu'alors nommée « fièvre intermittente ». En 1881 Theodor Billroth réalise la première gastrectomie, il révolutionne la chirurgie du pharynx et de l'estomac. En utilisant l'analyse statistique, le médecin Pierre-Charles Alexandre Louis (1787-1872) montre que l'utilisation des saignées chez les malades atteints de pneumonie n'est pas bénéfique mais néfaste. Ceci esquisse la notion d'étude randomisée en double aveugle.Madeleine Brès (1842-1921) est la première femme de nationalité française à accéder aux études de médecine en 1868, mais sans avoir le droit d'accéder aux concours. Elle obtient son doctorat en médecine, en 1875 et devient gynécologue et pédiatre. Elle démontre dans sa thèse que le lait du nourrisson se modifie au cours de l'allaitement et crée une des premières crèches parisiennes. Elizabeth Garrett Anderson, britannique la devance de cinq ans en France dans l'obtention de son doctorat.En 1854, Florence Nightingale est la première à utiliser les statistiques pour réorganiser les soins aux blessés de la guerre de Crimée et faire baisser la mortalité des soldats,,.Le 25 novembre 1901, Aloïs Alzheimer décrit le tableau clinique de la maladie qui porte son nom, dont il n'existe toujours aucun traitement connu à ce jour. Les traitements médicaux font des progrès spectaculaires avec l'invention de nouvelles classes de médicaments. Felix Hoffmann dépose le brevet de l'aspirine le 6 mars 1899. En 1909, le Nobel de médecine Paul Ehrlich invente la première chimiothérapie en créant un traitement à base d'arsenic contre la syphilis. En 1921 Frederick Banting de l'université de Toronto isole l'insuline et invente un traitement du diabète sucré. Le premier antibiotique date de 1928 avec la découverte de la pénicilline par Alexander Fleming.Selon la psychanalyste argentine Raquel Capurro, la médecine a été le premier domaine influencé par le positivisme d'Auguste Comte, à partir du milieu du XIXe siècle, à travers des personnalités telles que le docteur Robinet parmi d'autres.La délimitation de ce qui est médecine et de ce qui ne l'est pas est source de débat.La plus grande partie de cet article traite de la médecine telle qu'elle s'est développée à partir de l'époque moderne, et pratiquée à partir du XIXe siècle. Les innovations majeures apportées par la médecine occidentale à partir du XIXe siècle (anesthésie et asepsie puis vaccination et antibiotiques au XXe siècle), ses succès, ainsi que sa diffusion à travers le monde par le biais notamment de la colonisation par l'Occident vont inciter à poser, dès la fin du XIXe siècle, la médecine scientifique occidentale comme modèle de médecine faisant autorité, lequel s'est diffusé au niveau mondial à travers son industrialisation au XXe siècle.Certains chercheurs réhabilitent de même certains aspects de la médecine médiévale occidentale. Ainsi l'historien de la médecine Roger Dachez qui met en valeur l'aspect préventif et la vision globale qu'avait de la médecine le Moyen Âge.De même, toujours à la fin du XXe siècle, notamment sous l'effet de la mondialisation, les médecines traditionnelles ou non occidentales ont vu leur place reconnue au sein de la médecine mondiale : en 2002, l'organisation mondiale de la santé a ainsi mis en place sa première stratégie globale en matière de médecine traditionnelle.On identifie ainsi, à côté de la médecine occidentale, d'autres types de médecines, dites « alternatives » incluant : médecine chinoise, médecine tibétaine traditionnelle, médecine ayurvédique, médecine traditionnelle, et médecine non conventionnelle.En Occident, l'usage de médecines alternatives et complémentaires est constaté dans certaines conditions où les traitements de biomédecine semblent inefficaces, notamment dans le cas de maladies chroniques.Les étapes de l'acte médical sont formées de :l'étiologie qui désigne l'étude des causes de la maladie ;la pathogénie ou pathogenèse qui désigne l'étude du mécanisme causal ;la physiopathologie qui désigne l'étude des modifications des grandes fonctions au cours des maladies ;la sémiologie qui désigne l'étude de l'ensemble des signes apparents. Elle est apparentée à ce qui est nommée la clinique, opposée à la para-clinique qui sont les résultats des examens complémentaires. Face à la complexité croissante des techniques d'imagerie, il s'est développé une sémiologie des examens complémentaires ;le diagnostic qui désigne l'identification de la maladie ;le diagnostic différentiel qui désigne la description des maladies comportant des signes proches et qui peuvent être confondues ;la thérapeutique qui désigne le traitement de la maladie ;le pronostic qui désigne l'anticipation de l'évolution de celle-ci ;la psychologie qui désigne la partie de la philosophie qui traite de l’âme, de ses facultés et de ses opérations. La psychologie du patient est un élément important de la réussite du processus médical. Comme le dit dès 1963 l'historien de la médecine Jean Starobinski, « une médecine vraiment complète ne se borne pas à cet aspect technique ; s'il accomplit pleinement son métier, le médecin établit avec son patient une relation qui satisfera les besoins affectifs de ce dernier. L'acte médical comporte donc un double aspect : d'une part les problèmes du corps et de la maladie font l'objet d'une connaissance qui n'est pas différente de celle que nous prenons du reste de la nature - et l'organisme du patient est alors considéré comme une « chose » vivante capable de réagir conformément à des lois générales ; d'autre part, le rapport thérapeutique s'établit entre deux personnes, dans le contexte d'une histoire personnelle - et la médecine devient alors cette fois un art du dialogue, où le patient s'offre comme un interlocuteur et comme une conscience alarmée ». Georges Canguilhem écrivait lui que « l’acte médicochirurgical n’est pas qu’un acte scientifique, car l’homme malade n’est pas seulement un problème physiologique à résoudre, il est surtout une détresse à secourir ». Une décision médicale doit tenir compte à la fois des données de la science, mais également des préférences des patients et de l’expérience du praticienEn travaillant ensemble comme une équipe interdisciplinaire, de nombreux professionnels de la santé hautement qualifiés sont impliqués dans la prestation des soins de santé modernes. Voici quelques exemples : les infirmiers, les techniciens médicaux d'urgence et les ambulanciers, les scientifiques de laboratoire, pharmaciens, podologues, physiothérapeutes, inhalothérapeutes, psychologues, orthophonistes, ergothérapeutes, radiologues, des diététiciens, des bioingénieurs, des chirurgiens et des vétérinaires.Un patient admis à l'hôpital est habituellement sous les soins d'une équipe spécifique en fonction de leur problème de présentation principale, par exemple, l'équipe de cardiologie, qui peut ensuite interagir avec d'autres spécialités, par exemple, la chirurgie, la radiologie, pour aider à diagnostiquer ou traiter le problème principal ou des complications ultérieures. Les médecins ont de nombreuses spécialisations et sous-spécialisations dans certaines branches de la médecine, qui sont énumérés ci-dessous. Il existe des variations d'un pays à l'autre en ce qui concerne les spécialités et les sous-spécialités.Les principales branches de la médecine sont :les sciences fondamentales ;les spécialités médicales ;les domaines interdisciplinaires, comme les humanités médicales.L'anatomie : étude de la structure physique des organismes. Contrairement à l'anatomie macroscopique ou brute, la cytologie et l'histologie sont concernés par des structures microscopiques.La biochimie : étude de la chimie qui se déroule dans les organismes vivants, en particulier la structure et la fonction de leurs composants chimiques.La biologie moléculaire : étude des mécanismes moléculaires des processus de réplication, de transcription et de traduction du matériel génétique.La biomécanique : étude de la structure et des mouvements des systèmes biologiques au moyen de la mécanique.La biophysique : science interdisciplinaire qui utilise les méthodes de la physique et de la chimie physique pour étudier les systèmes biologiques.La biostatistique : application des statistiques à des champs biologiques dans le sens le plus large. Une connaissance de la biostatistique est essentiel dans la planification, l'évaluation et l'interprétation de la recherche médicale. Il est également fondamental de l'épidémiologie et de la médecine fondée sur des preuves (EBM).La cytologie : étude des cellules.L'embryologie : étude du développement précoce des organismes.L'épidémiologie : étude de la démographie des processus de la maladie, et inclut, mais sans s'y limiter, l'étude des épidémies.La génétique : étude des gènes, et leur rôle dans l'héritage biologique.L'histologie : étude des structures des tissus biologiques par microscopie optique, la microscopie électronique et l'immunohistochimie.L'immunologie : étude du système immunitaire, qui comprend le système immunitaire inné et adaptatif.La microbiologie : étude des micro-organismes, y compris les protozoaires, les bactéries, les champignons, les virus et les prions.La neuroscience : étude du système nerveux.La nutrition (mise au point théorique) et la diététique (orientation pratique) : étude de la relation entre la nourriture et des boissons à la santé et à la maladie, en particulier dans la détermination d'une alimentation optimale. thérapie nutritionnelle médicale se fait par des diététistes et est prescrit pour le diabète, les maladies cardiovasculaires, le poids et les troubles alimentaires, les allergies, la malnutrition et les maladies néoplasiques.La pathologie en tant que science : étude des maladies, de leurs causes, progressions et traitements.La pharmacologie : étude des médicaments et de leurs actions.La physiologie : étude du fonctionnement normal de l'organisme et les mécanismes de régulation sous-jacents. La physiologie peut être subdivisée (physiologie cardiaque, endocrinienne…).La physique médicale : étude des applications des principes de physique en médecine.La toxicologie : étude des effets nocifs des médicaments et des poisons. Par pratique l'anatomopathologie : étude microscopique des tissus malades ;l'anesthésie-réanimation : l'anesthésie qui est la médecine péri-opératoire, la réanimation qui est la prise en charge des malades présentant au moins deux défaillances d'organe ou une nécessitant une technique de suppléance ;la biologie médicale ;la chirurgie : thérapeutique médicale qui comporte une intervention mécanique au sein même des tissus ;l'éducation de la santé ;la médecine esthétique : type de soins visant à améliorer l'aspect plastique du patient ;la médecine générale (médecine de famille) ;la médecine du travail : médecine préventive consistant à éviter toute altération de la santé des travailleurs du fait de leur travail, notamment en surveillant les conditions d'hygiène du travail, les risques de contagion et l'état de santé des travailleurs ;la médecine d'urgence : médecine hospitalière (service des urgences) et extrahospitalière (Samu), traitement des urgences vitales ;la nutrition : prise en charge du métabolisme et de l'alimentation ;la pharmacie : dispensation des médicaments et prise en charge pharmaco-thérapeutique ;la radiologie, spécialité de l'imagerie médicale. Par type de patient L'andrologie : médecine de l'homme, prise en charge des maladies spécifiques du sexe masculin ;la gynécologie : spécialité médicochirurgicale, dont l'activité variée inclut notamment la médecine de la femme, le suivi gynéco-obstétrical et les cancers des organes génitaux féminins ainsi que des seins ;l'obstétrique : médecine de la femme enceinte. À noter la pratique médicale à part entière des sages-femmes, qui se consacrent à la surveillance de la grossesse normale ;la médecine fœtale : médecine du fœtus grâce à l'apparition de méthodes d'explorations de la vie intra-utérine (échographie, Doppler, amniocentèse) ;la médecine légale : recherche des causes de la mort sur un cadavre (nécropsie) et rédaction d'un rapport pour la Justice ;la pédiatrie : médecine des enfants, domaine très large et englobant généralement la génétique clinique ;la néonatologie : médecine et réanimation des nouveau-nés et des prématurés ;la gériatrie : médecine des personnes âgées ;la médecine des gens de mer : médecine des marins et travailleurs de la mer.la médecine vétérinaire : médecine des animaux. Par organe L'angiologie : médecine des vaisseaux ;la cardiologie : médecine des maladies du cœur et du système vasculaire ;la dermatologie : médecine des maladies de la peau ;l'endocrinologie : médecine des maladies des glandes, des anomalies hormonales, des troubles de la nutrition et des métabolismes ;l'hématologie : médecine des maladies du sang ;l'hépato-gastro-entérologie : aussi appelée gastroentérologie, médecine des maladies de l'appareil digestif dans son ensemble, incluant celles du tube digestif et celles du foie, du pancréas, ainsi que de la paroi abdominale. La gastroentérologie comprend également les activités d'endoscopie digestives, soit haute (endoscopie œsogastroduodénale), soit basse (iléocoloscopie) ;l'immunologie : médecine des maladies ou des troubles du système immunitaire ;la néphrologie : médecine des maladies des reins ;la neurologie : médecine des maladies du système nerveux ;l'odontologie : soins des dents ;l'ophtalmologie : médecine des maladies des yeux, de l'orbite et des paupières ;l'orthopédie : discipline chirurgicale traitant les affections de l'appareil locomoteur ;l'oto-rhino-laryngologie (ORL) : médecine des maladies des oreilles, du nez et de la gorge ;la pneumologie : médecine des maladies de la plèvre, des bronches et des poumons ;la proctologie : médecine des maladies du rectum et de l'anus ;la rhumatologie : discipline médicale traitant les affections de l'appareil locomoteur ;la stomatologie : médecine des maladies de la bouche ;l'urologie : médecine de l'appareil urinaire. Par affection L'addictologie : médecine des dépendances, regroupant l'alcoolisme, le tabagisme et la toxicomanie (branche de la psychiatrie selon certains) ;l'alcoologie : médecine des troubles liés à l'alcool ;l'allergologie : médecine des allergies ;la cancérologie ou oncologie : médecine des cancers (comprenant la chimiothérapie des tumeurs) associée avec la radiothérapie : traitement des tumeurs par radiations ionisantes ;la diabétologie : médecine des diabètes ;l'infectiologie : médecine des maladies infectieuses ;la psychiatrie : médecine des troubles comportementaux, psychiques et des maladies mentales ;la toxicologie : traitement des empoisonnements et intoxications ;la traumatologie : traitement des patients ayant subi de graves blessures, généralement accidentelles ;la vénérologie : médecine faisant l'étude des maladies transmises par l'acte sexuel. Types de chirurgie Chirurgie cardiaqueChirurgie digestiveChirurgie de la face et du cou (cervico-faciale)Chirurgie généraleChirurgie pédiatriqueChirurgie orthopédiqueChirurgie dentaireChirurgie plastique, reconstructrice et esthétiqueChirurgie thoraciqueChirurgie urologique (Urologie)Chirurgie vasculaireChirurgie viscéraleNeurochirurgieTechniques chirurgicales Divers Anatomie et cytologie pathologiques (voir anatomopathologie)Anesthésie-réanimationBiologie médicaleGénétiqueGynécologie obstétriqueInformatique Médicale et Technologies de l'InformationMédecine généraleMédecine interneMédecine hyperbareMédecine nucléaireMédecine nutritionnelle (voir nutrition)Pathologie (pays anglophones)PédopsychiatrieMédecine physique et de réadaptationSanté publiqueLes académies de médecineles Centers for Disease Control and Prevention, soit « centres de contrôle et de prévention des maladies »les hôpitauxles organismes de recherche médicaleles organismes publicsles Conseils de l'Ordre de médecinsl'Agence européenne des médicamentsUne profession de la santé est une profession dans laquelle une personne exerce ses compétences ou son jugement ou fournit un service lié au maintien ou l'amélioration de la santé des individus, ou au traitement ou soins des individus blessés, malades, souffrant d'un handicap ou d'une infirmité. Des exemples de profession peuvent notamment inclure : médecin, pharmacien, chirurgien-dentiste, sage-femme, masseur-kinésithérapeute, physiothérapeute, ergothérapeute, psychomotricien, infirmier, podologue, aide-soignant, ambulancier, et attaché de recherche clinique.Chaque profession possède son propre cursus de formation. En plus des études permettant d'exercer la profession de médecin dont l'organisation varie selon les pays, on trouve donc notamment les études en soins infirmiers, et les études de pharmacie.L'étudiant en médecine s'appelle carabin.Les apports de la médecine, particulièrement de la médecine occidentale depuis le XIXe siècle, se mesure notamment par l'allongement de la durée de la vie, l'espérance de vie en bonne santé, la réduction de la mortalité infantile, et l'éradication ou la capacité technique d'éradication de très anciennes épidémies (tuberculose, peste, lèpre, etc.). Ces progrès se poursuivent comme avec les succès de nouvelles thérapies (ou actes chirurgicaux) sur des pathologies considérées encore incurables il y a une quinzaine d'années (comme certains cancers et maladies auto-immunes).La médecine n'est pas une science exacte, et l'acte médical peut parfois affecter la personne humaine de manière négative, par exemple via :des « effets secondaires » ou indésirables de médicaments ou traitements, qui devront pour certains (Distilbène par exemple) être supportés par plusieurs générations. La recherche de ces effets se fait par pharmacovigilance ;l'antibiorésistance est due à la sélection de souches bactériennes résistantes à divers antibiotiques à cause d'un usage non raisonné de ces derniers ;les maladies nosocomiales peuvent apparaître en hôpital à cause de la concentration de malades. La forte pression exercée par les traitements ainsi que par les désinfectants et antiseptiques sur ce « pot pourri » de germes amène à long terme à l’émergence d'agents infectieux résistants qui pourront infecter facilement les malades déjà affaiblis ;les résultats de maladresses, d'erreurs médicales, de défauts d'organisation, de prises excessives de médicaments ou de traitements inadaptés. Un trouble ou une maladie est dite iatrogène lorsqu'elle est provoquée par un acte médical ou par les médicaments, même en l’absence d’erreur du médecin, du soignant, du pharmacien ou tout autre personne intervenant dans le soin. En France, 4 % des hospitalisations sont consécutives à des soins, et 40 % de ces cas seraient évitables. Ces problèmes comprennent une partie des maladies nosocomiales dont les plus fréquentes sont les infections nosocomiales.De nombreux progrès sont annoncés ou espérés dans les années à venir, en matière de santé-environnement, d'épidémiologie, d'allongement de la durée de vie, si ce n'est de la durée de vie en bonne santé. La médecine prédictive, le clonage, les cellules-souches posent des questions nouvelles en termes de bioéthique.Des défauts d'anticipation font que, par exemple en France, en 2025, alors que la population aura augmenté (et la population âgée plus encore), le nombre de médecins aura diminué de 10 % et la densité médicale de 15 %, à la suite du non-remplacement des médecins baby-boomers induit par les quotas d’accès aux études de médecine dans les années 1970 à 1990. La médecine libérale devrait perdre 17 % de ses effectifs, et le secteur salarié 8 %, sauf en milieu hospitalier où le ministère envisage une hausse de 4 % ; 13 % des généralistes auront disparu, contre 7 % pour les spécialistes (ophtalmologistes, oto-rhino-laryngologistes et psychiatres surtout). La faible « densité médicale » augmentera aussi le coût des soins, l’impact des déplacements en termes de pollution (et secondairement de santé) et pourrait diminuer l'efficience médicale (une moindre densité médicale augmente la mortalité), d'autant plus que les patients sont plus pauvres.Cet article est partiellement ou en totalité issu de l'article intitulé « Histoire de la médecine » (voir la liste des auteurs).(en) Charles Singer et E. Ashworth Underwood, A Short History of Medicine, New York et Oxford, Oxford University Press, 1962(en) Roberto Margotta, The Story of Medicine, New York, Golden Press, 1968.(en) Roberta Bivins, Alternative Medicine? : A History, Oxford University Press, 5 octobre 2007, 264 p. (ISBN 978-0-19-156881-7, lire en ligne)(en) Robert A. Schwartz, Gregory M Richards et Supriya Goyal, « Clubbing of the Nails », Medscape Reference,? 28 février 2012 (lire en ligne, consulté le 11 juin 2012).Stanis Perez, Histoire des médecins. Artisans et artistes de la santé de l'Antiquité à nos jours, Perrin, 2015, 470 pages.Ressource relative à la littérature : (en) The Encyclopedia of Science Fiction Ressource relative à la santé : (en) Medical Subject Headings Haute Autorité de Santé : recommandations, conférences de consensus, etc. (France)Code de la santé publique (France)Service Public Fédéral (SPF) Santé publique, Sécurité de la Chaîne alimentaire et Environnement (Belgique)CISMeF : annuaire de sites médicaux Internet francophoneBase de données de publications médicales(en) Medline, base de données de publications médicales Portail de la médecine"
médecine;"Un médicament est toute substance ou composition présentée comme possédant des propriétés curatives ou préventives à l'égard des maladies humaines ou animales. Par extension, un médicament comprend toute substance ou composition pouvant être utilisée chez l'être humain ou l'animal ou pouvant leur être administrée, en vue d'établir un diagnostic médical ou de restaurer, corriger ou modifier leurs fonctions physiologiques en exerçant une action pharmacologique, immunologique ou métabolique.L'ensemble de la chaîne des médicaments (recherche, production, contrôle qualité, distribution en gros, délivrance aux patients, pharmacovigilance) est sous la responsabilité de spécialistes diplômés des médicaments, les pharmaciens.La notion de médicament est précisément définie en France par l'article L5111-1 du Code de la santé publique.On peut distinguer différents types de médicaments selon leur utilisation, leurs composants, leur mode d'enregistrement réglementaire, etc. :médicament générique ;médicament biosimilaire ;médicament orphelin ;médicament biologique ;médicament à base de plantes ;médicament essentiel ;médicament stupéfiant.Posologie : c'est la dose usuelle du médicament utilisé. Elle dépend de la maladie, de l'âge du patient, de son poids et de certains facteurs propres : fonction rénale, fonction hépatique. Elle ne doit naturellement être en aucun cas modifiée sans un avis médical ou éventuellement du pharmacien.Pharmacocinétique : c'est la vitesse à laquelle la substance active du médicament va être absorbée, distribuée dans l'organisme, métabolisée (transformée), puis éliminée de l'organisme. Elle conditionne la méthode de prise : orale (par la bouche), intraveineuse ou autre, mais aussi le nombre quotidien de prises, leur horaire, la dose journalière. Schématiquement, la pharmacocinétique est l'étude de l'action de l'organisme sur le médicament.Pharmacodynamique : c'est le mode d'action de la substance active qui va entraîner les effets thérapeutiques. Schématiquement, la pharmacodynamie est l'étude de l'action du médicament sur l'organisme.Indication : c'est une maladie ou une situation pour laquelle un médicament est utilisé.Contre-indication : c'est la ou les situations où la prise du médicament peut se révéler dangereuse. Ce dernier ne doit, par conséquent, pas être donné. On distingue les contre-indications relatives où dans certains cas, le rapport bénéfice-risque de la prise de la molécule reste acceptable, et les contre-indications absolues où le médicament ne doit pas être pris, quel que soit le bénéfice escompté.Association déconseillée : à éviter, sauf après évaluation du rapport bénéfice/risque ; nécessité d'une surveillance étroite.Précaution d'emploi : c'est le cas le plus fréquent ; association possible en respectant les recommandations.A prendre en compte : signalement du risque ; au praticien d'évaluer l'opportunité de l'association ; pas de conduite spécifique à tenir.Synergie : cela correspond à l'interaction entre deux médicaments présentant une activité pharmaceutique identique. L'intensité de l'activité de l'association est supérieure à celle que l'on pourrait obtenir avec l'un des médicaments administré seul.Potentialisation : elle s'exerce entre deux médicaments dont l'activité pharmaceutique est différente.Antagonisme : il s'agit d'une interaction entre deux médicaments dont l'activité pharmaceutique est identique ou différente. L'administration simultanée de deux médicaments entraîne l'inhibition partielle ou complète de l'action de l'un d'entre eux.Un médicament peut avoir une ou plusieurs actions, décrites comme :Action substitutive : consiste à apporter à l'organisme l'élément nutritif ou physiologique déficient (par exemple : méthadone ou vitamine C).Action par reproduction directe ou indirecte des effets d'une substance naturelle : le médicament reproduit ou stimule une fonction cellulaire ou organique, ou encore la transmission d'un influx nerveux au niveau du SNC (système nerveux central) ou autonome (par exemple : sympathomimétique ou parasympathomimétique).Action par antagonisme direct ou indirect des effets d'une substance naturelle : le médicament exerce un blocage partiel ou complet d'une fonction cellulaire ou organique en fixant sur des récepteurs spécifiques (par exemple : sympatholytique).Action mécanique (par exemple : huile de paraffine favorisant le transit digestif).Action sur certains processus métaboliques : action sur la perméabilité cellulaire ou la réactivité de certaines cellules à leur excitant physiologique ou pathologique (par exemple : médicament anticalcique (modifiant la perméabilité des ions calcium)).Le médicament est composé de deux sortes de substances : d'une ou plusieurs substances actives (aussi désigné principe actif — c'est souvent la substance active qui est désignée dans le langage courant par médicament) et d'un ou plusieurs excipients.La ou les substances actives sont constituées d'une quantité de produit active (dose) ayant un effet pharmacologique démontré et un intérêt thérapeutique également démontré cliniquement. Il est à remarquer que toute substance pharmacologiquement active ne constitue pas nécessairement la base d'un médicament et encore moins d'une thérapie médicamenteuse.Les excipients sont des substances auxiliaires inertes servant à la formulation de la forme galénique ou destinée à créer une absorption par le corps. Ces excipients sont le plus souvent des substances inertes sur le plan pharmacologique. Les excipients permettent de formuler la ou les substances actives, c’est-à-dire de présenter la substance active sous une forme galénique déterminée. La formulation permet en plus de présenter le médicament sous la forme la plus adaptée pour la voie d'administration souhaitée et éventuellement, le cas échéant, de moduler la vitesse de libération de la substance active vers l'organisme. Comme exemple d'excipients on citera : l'eau et le saccharose sont les deux excipients constituant le sirop simple — ou encore, pour des formes sèches, le ou les amidons modifiés et la ou les celluloses modifiées sont des agents de délitement utilisés dans des formes sèches (comprimés, gélules, etc.) pour accélérer la désintégration (ou encore délitage) de celles-ci une fois arrivées dans l'estomac. Les excipients sont dans leur très grande majorité, des substances chimiquement inertes et pharmacologiquement inactives. Mais il s'avère qu'ils ne sont pas toujours exempts d'effets pharmacologiques sur certains patients. En effet, certains excipients sont connus pour être à l'origine d'effets secondaires (e.g. réactions allergiques ou d'intolérance) chez une minorité de patients particulièrement sensibles. On parle alors d'excipient à effet notoire. On citera en exemple le lactose chez des patients intolérants au lactose. Le prescripteur ou le pharmacien devra en tenir compte lors de la prescription et de la dispensation du médicament. Ceci est très important notamment lors de la substitution d'un produit princeps par une forme générique du produit original. Le produit générique n'étant pas nécessairement formulé avec les mêmes excipients que le produit princeps d'origine. Ceci est une des raisons pour lesquelles un patient peut ne pas tolérer les produits génériques de substitution.Il est à remarquer qu'une substance active peut être par exemple un produit de contraste (sulfate de baryum) qui n'est pas pharmacologiquement actif car il n'est pas destiné à traiter le patient mais à aider à poser le diagnostic (il est actif sur le rayonnement auquel sera exposé le patient).La galénique (de Galien, médecin de l'Antiquité) ou « art de formuler les médicaments », va permettre de présenter la substance active à des doses différentes et sous différentes formes galéniques (les formes d'administration de la substance active au patient). On parlera de comprimés, de gélules, de capsules molles, de suppositoires, d'ampoules, de gouttes (orales, oculaires ou nasales), de collutoires, de collyres, de pommades, de gels et crèmes, de solutions, d'ovules, d'emplâtre ou de dispositifs transdermiques, etc. On peut ainsi classer les formes galéniques selon la voie d'administration aux patients pour laquelle elles ont été conçues. On parlera alors d'injectables (ampoules de solution ou de suspension, implants…) destinées aux différentes voies parentérales (sous-cutanées, intraveineuse, intramusculaires, intra-articulaires…). Ces formes doivent être stériles, apyrogènes et, parfois, isotoniques. Les autres formes liquides non injectables sont destinées aux voies orales (à avaler per os ou sublinguales, à enrobage entérique ou à désintégration rapide), nasales, auriculaires et oculaires, dermiques mais aussi transdermiques (timbre ou patch). Il existe encore des formes pour la voie, rectale, oculaire, auriculaire, etc.Une spécialité pharmaceutique est un médicament qui a un nom commercial (qui fait l'objet d'une propriété commerciale, nom commercial dit aussi nom de fantaisie). Chaque spécialité fait l'objet d'un enregistrement auprès des autorités de santé, qui est préparé industriellement selon des normes très strictes (les bonnes pratiques de fabrication) et est vendu par un laboratoire pharmaceutique. Sous son même nom de marque, il existe différentes formes pharmaceutiques et différents conditionnements, chacun faisant l'objet d'un enregistrement spécifique. Une même spécialité pourra être commercialisée éventuellement sous un ou plusieurs noms de marque et restera protégée tant qu'elle fera l'objet d'une propriété intellectuelle et d'une protection des droits intellectuels et/ou commerciaux (brevet, exclusivité commerciale, licence). Une fois la propriété intellectuelle perdue (épuisement des droits du ou des brevets), le médicament peut être commercialisé sous des formes dites génériques (en plus des formes commerciales existantes). Les formes génériques devant être bioéquivalentes au premier produit de marque mis sur le marché appelé encore produit « princeps » ou spécialité originale.Ce n'est pas parce que l'on absorbe des doses équivalentes d'une même substance active sous des formes différentes (une solution au lieu d'un comprimé par exemple) que l'effet pharmacologique recherché sera équivalent. La prise à jeun ou après un repas change également l'effet pharmacologique de la substance active. On parle alors de disponibilité de la substance active ou encore de « biodisponibilité ». Deux formes offrant la même biodisponibilité seront dites bioéquivalentes.Le principe de bioéquivalence décrit deux médicaments contenant la même quantité de substance active. Les substances actives sont dites bioéquivalentes si, pour un même groupe d'individus, leurs effets thérapeutiques sont estimés biologiquement équivalents. Des différences au niveau des caractéristiques physiques des substances actives (structure cristalline ou polymorphisme, taille des cristaux) ou caractéristiques de formulation (présence de certains excipients, compression, délitement, enrobage,..) peuvent faire que deux formes galéniques qui contiennent la même quantité de substance active sont très différentes au niveau de leur mise à la disposition de cette substance active au niveau du système digestif. Il en est de même pour des formes injectables où l'on injecte des substances actives en suspension. Mais comme il est difficile et surtout très coûteux de tester une équivalence thérapeutique basée sur des tests cliniques et/ou biologiques, on teste en fait les variations de la concentration plasmatique de la substance active inchangée au cours du temps, variation consécutive à la prise du médicament au t=0. La courbe de biodisponibilité est représentée par la concentration plasmatique en inchangé Cp = f(t). C'est la mesure de l'aire sous la courbe qui donne la biodisponibilité de la substance active tel que présenté dans la forme galénique. Deux médicaments bioéquivalents donnent des moyennes d'aires sous la courbe (donc de concentrations plasmatiques en produit inchangé = f(t) qui sont équivalentes dans une population d'une vingtaine d'individus sains. Pour enregistrer un produit générique, il est nécessaire de démontrer par une étude de bioéquivalence que la forme générique est bioéquivalente à la forme princeps. Les problèmes de bioéquivalence existent cependant et se posent de façon importante pour des substances actives peu solubles (solubilité aqueuse inférieure à 1 mg/ml) lorsqu'elles sont administrées par voie orale ou pour des formes galéniques modifiées telles que les formes à libération prolongée, appelées encore formes retard. Pour des formes en solution vraie et présentées en injectable et injectées par voie IV en bolus, il n'existe par définition aucune différence de bioéquivalence entre formes puisque la biodisponibilité est totale (on dit alors que la biodisponibilité est absolue et égale à 1, quelle que soit la spécialité injectable utilisée. En revanche, pour des solutions orales, la biodisponibilité n'est plus absolue mais relative, car elle est relative à la vitesse de transit gastrique de chaque individu (à jeun, pendant ou après un repas, repas léger ou gras, etc.) et à une fenêtre d'absorption duodénale, si celle-ci existe. Par voie orale, on est donc toujours dans le relatif. Pour assurer une qualité de biodisponibilité des formes galéniques orales, dans la pratique industrielle, on teste la vitesse de dissolution des formes galéniques orales en laboratoire (test de dissolution) et ceci sur chaque lot avant de le libérer vers la distribution.Les injectables à libération prolongée (formes intra-musculaire, intra-articulaire, implants, etc.) peuvent, en revanche, montrer des biodisponibilités fort différentes entre elles et par rapport à la forme IV bolus. Ceci ne signifiant pas nécessairement des effets thérapeutiques essentiellement différents ou nécessairement supérieurs ou inférieurs. Là commence le domaine de la pharmacocinétique en liaison avec la toxicité (animale et humaine) et les études cliniques (animale et humaine).Au début du XXe siècle n'étaient considérés comme médicaments qu'une douzaine de produits de synthèse et une centaine de produits naturels. Au début du XXIe siècle, nous utilisons des centaines de substances synthétiques et il ne reste que très peu de remèdes courants d'origine exclusivement naturelle. Le XXe siècle a vu l'essor des médicaments de synthèse produits par des laboratoires pharmaceutiques. Depuis peu, les protéines, molécules du vivant, sont de plus en plus utilisées comme médicament.Actuellement, pour une utilisation en santé humaine et animale, de la découverte d'une nouvelle substance active à l'Autorisation de mise sur le marché (AMM) en passant par la mise au point de(s) (la) forme(s) galénique(s) (le médicament délivré en officine), généralement une période de 10 à 15 ans se sera écoulée et plusieurs centaines de millions d'euros auront été investies.Le processus de développement peut être décrit selon les étapes suivantes :recherche d'une substance originale candidate au statut de candidat médicament selon plusieurs méthodes : modélisation informatique, criblage (screening), observation de médecines traditionnelles (medicine man (en)), étude des caractéristiques des plantes ou substances naturelles (pharmacognosie), et parfois par les faveurs du hasard (sérendipité) lors d'observations cliniques ;les substances candidates sont alors le plus souvent brevetées ce qui confère, dans ce cas, à l'inventeur un droit de propriété intellectuelle permettant l'exploitation commerciale exclusive de la molécule pour une durée maximale de 20 ans. Compte tenu du fait que la protection court à compter du dépôt du brevet et non celle de la mise sur le marché, pour le médicament, en France, en Europe et aux États-Unis un certificat complémentaire de protection (CCP ou SPC, en anglais) peut être obtenu. Par ailleurs, pour encourager des développements complémentaires, les autorités de santé peuvent accorder une exclusivité commerciale supplémentaires de quelques années dans des conditions particulières, par exemple indications orphelines, médicaments pédiatriques, etc.depuis 2009, commencent à se développer aux États-Unis des coopératives de conception libre de médicaments, notamment génétique ;étude de l'effet de la substance in vitro sur des micro-organismes en culture, ex vivo sur des organes isolés ou sur des récepteurs biologiques purifiés, puis in vivo, c'est-à-dire sur l'animal de laboratoire vivant ;recherche d'une forme galénique la mieux adaptée. On cherche tant que possible à obtenir une forme orale biodisponible et stable. Celle-ci étant la plus simple à prendre par le futur patient (compliance).Les dernières phases de recherche enclenchées dans le développement d'un nouveau médicament sont les études cliniques : depuis près de vingt ans, les différentes études cliniques qui doivent être réalisées à l'appui d'une demande d'enregistrement (demande d'AMM) font l'objet d'une standardisation internationale (harmonisation ICH) reconnue par tous les pays de l'OCDE. Elles sont structurées en trois phases avant la mises sur le marché et une, la phase IV, après cette mise sur le marché. Pour chaque nouvelle indication thérapeutique et parfois aussi par catégorie de formes galéniques (injectable, orale, topique…), il sera nécessaire de reconsidérer le plan clinique existant et de voir si les études cliniques existantes peuvent être utilisées à l'appui de la nouvelle indication / forme pharmaceutique ou si de nouvelles études sont nécessaires et doivent être entreprises avant d'aller plus avant. Lors de la mise sur le marché de copies génériques les études de bioéquivalence seront entreprises. Une substance active va donc faire l'objet d'études cliniques quasiment de façon continue pendant toutes les années de sa présence sur le marché.Les différentes études cliniques se font en quatre phases. Phase I La phase I est dite d'innocuité (ou encore de tolérance) du produit. Elle est généralement menée sur des volontaires sains. Elle vise à établir la dose minimale active (si son activité peut être mise en évidence sur le volontaire sain) et surtout pour établir la dose maximale tolérable, en doses uniques croissantes et/ou répétées. Pour des produits comme des antibiotiques, des anticancéreux, des hormones, etc., l'utilisation de volontaires sains est exclue. On cherche à connaître la pharmacocinétique ADME de la molécule (c'est-à-dire la vitesse d'absorption (A = la vitesse de passage dans le sang à partir d'une solution orale), M = la vitesse de métabolisation (transformation biologique par le foie et d'autres organes), D = la vitesse de distribution et de répartition dans les différents tissus à partir du compartiment plasmatique et E = la vitesse d'élimination de la molécule par l'organisme aussi appelée clearance). Les données ADME préalablement collectées sur les modèles animaux (rat, souris, chien et singe) servent d'encadrement et de comparaison pour les données ADME humaines. Comme il n'est pas éthique d'exposer des volontaires sains à des produits très actifs (anti-cancéreux, antithyroïdiens, hormones, antibiotiques, etc.), cette phase I est dans ce cas réalisée en phase II sur des patients qui eux peuvent bénéficier de l'effet thérapeutique supposé du produit testé. Dans tous les cas, l'accord du patient, après une information éclairée, est indispensable. Aucune expérimentation ne peut se faire à l'insu du patient et sans son accord « éclairé » par les explications du responsable de l'étude. Phase II Elle consiste en des tests dits de biodisponibilité sur patients volontaires et d'efficacité sur patients volontaires. Elle vise à établir la relation entre dose et effet. On établit le domaine (range) des doses actives à partir des données obtenues sur animaux en toxicologie préclinique. On établit le « range » des doses actives tolérées sans chercher à atteindre une dose maximale qui serait toxique. Ce range deviendra progressivement la posologie du produit pour telle indication. C'est lors de ces tests que l'on détecte les premiers effets secondaires, qui une fois confirmés en phase II et IV seront souvent les effets secondaires principaux du produit. Si ces effets sont trop importants par rapport à l'intérêt de l'effet thérapeutique apporté, le développement du produit sera arrêté. Phase III Le médicament dont l'activité pharmacologique a été confirmée en phase II doit être testé pour évaluer son intérêt clinique réel. Cette phase vise à établir le rapport entre bénéfice et risques. Le candidat médicament est comparé à un médicament de référence et toujours à un placebo (lorsqu'il n'existe pas d'opposition éthique à ne pas administrer de substance active au patient volontaire) dans une plus large étude clinique. Une randomisation (tirage au sort) est effectuée pour déterminer quel bras de traitement sera le patient. L'expérimentation dite « double aveugle » est un standard actuellement (ni le patient, ni le médecin ne savent si c'est un médicament, le placebo ou la référence qui est administrée). Ces méthodes statistiques sont un gage de rigueur et de qualité des données générées dans l'étude.Les données de toxicologie animale et d'innocuité clinique (innocuité = phase 2), les données cliniques (efficacité) et les données pharmaceutiques (qualité) sont rassemblées en un dossier dit de demande d'enregistrement qui est déposé pour obtenir une autorisation de mise sur le marché (AMM) à l'Agence européenne (EMEA). Si l'autorité estime (évaluation sur dossier uniquement) que les informations déposées à l'appui de la demande d'enregistrement sont suffisantes, elle autorise la commercialisation du médicament mais uniquement dans les indications cliniques approuvées. Si l'autorité estime qu'un complément d'information est nécessaire, elle exigera des compléments d'information à déposer avant de commercialiser la spécialité ou à remettre dans un délai assez court un an deux ans, mais sans empêcher la mise sur le marché du médicament.Le plus souvent, lorsqu'il s'agit d'un médicament contenant une nouvelle molécule (NCE = New Compound Entity), celle-ci est couverte par des droits de propriété intellectuelle (brevet ou patent). Cette propriété s'obtient par le dépôt d'une demande de brevet. Cette propriété intellectuelle une fois accordée, court sur une période maximale de 18 à 20 ans depuis le dépôt de la demande de brevet. Au bout d'un certain nombre d'années, le brevet de la substance active tombe dans le domaine public, et ainsi ouvre la possibilité de copie par des laboratoires spécialisés dans la production de médicaments génériques. Ces « génériques » doivent aussi faire l'objet d'enregistrement auprès des autorités de santé. Ces produits étant (on ne dit plus équivalents mais) essentiellement similaires aux produits originaux qualifiés de princeps, seule la partie pharmaceutique du dossier d'AMM est déposée pour obtenir un enregistrement. Une période dite de protection des données de 5 ans peut être obtenue auprès des autorités de santé pour empêcher les copies génériques d'un produit original, innovateur qui a mis longtemps pour être développé, plus que sa période de protection du brevet.Ainsi, il ne faut faire pas de confusion pour le médicament entre la protection des droits de propriété industrielle (brevet, CCP) qui est accordée par les agences de propriété industrielle (INPI au niveau national, Office Européen des Brevets au niveau européen) et les protections dites réglementaires auxquelles s'engagent les agences de santé (ANSM au niveau national ou EMA Agence Européenne du Médicament au niveau européen). La protection offerte par les agences de santé porte ainsi sur les données cliniques de développement dans l'indication considérée du médicament princeps.En France, un médicament expérimental est produit selon des critères de qualité équivalent au produit mis sur le marché. La loi dit que (annexe I de la décision du 26 mai 2006 modifiant l'arrêté du 10 mai 1995 modifié relatif aux bonnes pratiques de fabrication (industrie pharmaceutique)) tout principe actif sous une forme pharmaceutique ou placebo expérimenté ou utilisé comme référence dans une recherche biomédicale, y compris les médicaments bénéficiant déjà d'une autorisation de mise sur le marché, mais utilisés ou présentés ou conditionnés différemment de la spécialité autorisée, ou utilisés pour une indication non autorisée ou en vue d'obtenir de plus amples informations sur la forme de la spécialité autorisée. Phase IV La phase IV (ou post-marketing) est le suivi à long terme d'un traitement alors que le traitement est autorisé sur le marché. Elle doit permettre de dépister des effets secondaires rares ou des complications tardives. Cette phase est à la charge des laboratoires.Parmi les médicaments, des familles thérapeutiques sont notamment retrouvées :qu'on pourrait regrouper en 6 catégories plus vastes :Les hypnotiques (somnifères) et les anxiolytiques sont quelquefois rassemblées sous le nom de « psycholeptiques », terme qui est en fait assez vaste. Cette classification selon Delay et Deniker (1957) a été modernisée plus tard par Pelicier et Thuillier (1991).Traditionnellement, les médicaments sont prescrits par les médecins à leurs patients qui vont se les procurer chez leur pharmacien.Certains médicaments peuvent être obtenus sans ordonnance (automédication ou médication officinale) ; en France, lorsqu'un médicament est acheté sans être prescrit, il n'est pas remboursé par l'assurance maladie, mais il peut l'être par certaines mutuelles. Dans la plupart des pays, un médicament doit avoir obtenu une autorisation de mise sur le marché (AMM) pour être vendu. L'AMM est connue sous l'appellation « NDA » (new drug application) aux États-Unis et sous « NDS » (new drug submission) au Canada.Les organismes de régulation de la santé dressent des listes de médicaments en fonction des risques que représentent leur prise. Par exemple, en France, il existe plusieurs listes de substances vénéneuses : les médicaments qui renferment ces substances ne peuvent être acquis que sur ordonnance (sauf cas limités d'exonération) :liste I : médicaments toxiques (dans les conditions normales d'emploi) ;liste II : médicaments dangereux, moins toxiques que ceux de la liste I (dangereux en conditions anormales d'emploi) ;stupéfiants : substances psychoactives fortes capables de provoquer une dépendance et des effets délétères sur la santé psychique et physique, tout en représentant un danger particulier pour la santé publique.psychotropes : complémentaires aux stupéfiants, mais représentant un risque pour la santé publique jugé moindre et dont les conditions de prescription sont plus souples. Regroupe globalement les benzodiazépines et les barbituriques.Les différences entre ces listes sont surtout théoriques et ne garantissent pas forcément une description précise du caractère dangereux du médicament. Par exemple, le sécobarbital est classé comme stupéfiant tandis que le phénobarbital, dont le profil addictogène et nocif est comparable, ne figure que sur la Liste II ainsi que la liste des psychotropes. De ce fait, ces classements ne sont pas des garanties et de nombreux produits figurent d'ailleurs sur deux registres complémentaires.Du fait de l'émergence régulière de nouvelles substances l'Arrêté du 22 février 1990 fixant la liste des substances classées comme stupéfiants ainsi que celui fixant la liste des psychotropes sont fréquemment consolidés, et ont uniquement cours jusqu'à ce qu'une version ultérieure ne les remplace.Selon leurs particularités et leurs conditions d'utilisation ou de manipulation, certains médicaments en France sont soumis à des « conditions de prescription » telles que : médicaments à prescription restreinte :ceux qui sont réservés à l'usage hospitalier,ceux qui ne peuvent être prescrits que par un médecin hospitalier,ceux nécessitant une surveillance spécifique et une prescription par un médecin spécialisé,médicaments d'exception : médicaments particulièrement onéreux, ils doivent faire l'objet d'un suivi spécifique et de justifications médicales pour la prise en charge ;médicaments restreints et d'exception.Le médicament peut s'administrer, selon sa forme galénique, par plusieurs voies d'administration :de manière globale (systémique) : la substance active passe dans le sang et est transportée partout dans l'organisme, afin d'atteindre sa cible :administration orale, dite per os : comprimé, sirop, gélule, solution buvable, granulé,suppositoire,administration par voie pulmonaire (inhalation ou instillation), avec absorption par les muqueuses des voies respiratoirespar timbre transdermique (à travers la peau) : par exemple pour pallier l'envie de fumer, ou comme anti-inflammatoire ou antidouleur (morphinique),L'administration par voie parentérale est faite au moyen d'une injection. Elle peut être :intraveineuse, en une fois on dira en bolus ou par une perfusion lente. La veine pouvant être superficielle, habituellement au bras (voie veineuse périphérique) ou profonde (voie veineuse centrale), le plus souvent au niveau du cou (veine jugulaire) ou sous la clavicule (veine sous clavière). La voie intraveineuse permet d'administrer un produit qui doit agir très rapidement (urgence) ou un produit mal toléré avec le risque d'irriter la veine (phlébite),sous-cutanée : sous la peau, fréquemment au niveau du ventre ou des cuisses (insuline),intradermique : dans le derme,intramusculaire : dans un muscle (cuisse) pour un produit qui doit agir lentement.de manière locale, directement sur le site d'action désiré:administration par voie oculaireadministration par voie vaginale / en intra-utérin, respectivement par le vagin et l'utérusde manière dermique (topique) : la substance active est amenée directement à l'endroit où il doit agir au niveau de la peau: pommade, crème dermique, gel dermique, etc. (action cutanée ou topique),L'efficacité et l'évaluation du médicament tiennent compte de la balance bénéfice/risque, des effets secondaires et paradoxaux, des interactions et contre-indications. Le profil de risque est surtout lié à la relation entre les effets secondaires et la maladie soignée.Le rapport bénéfice/risque est pris en compte - ainsi des effets secondaires sévères seront indéniablement mieux acceptés pour échapper à un cancer que pour éviter la douleur ou l'obésité. Du côté du médecin, celui-ci doit prendre en compte dans ce rapport au risque la durée du traitement (effet cumulatif), et ne pas négliger le risque sur le fœtus quand il s'agit d'une femme enceinte (exemple : thalidomide, mieux connu sous le nom Softenon). La posologie et les effets secondaires connus doivent être inscrits sur la notice accompagnant le médicament.De plus, certains médicaments sont strictement réglementés et ne peuvent être prescrits que sous certaines conditions (voir prescription, distribution). Les données récoltées, touchant un grand nombre de patients, sont transmises aux autorités de santé qui réévaluent la balance bénéfice/risque du médicament. Il peut en ressortir des effets graves qui n'étaient pas apparus lors des études cliniques et ainsi mener le laboratoire ou l'autorité à retirer le médicament.Les événements indésirables médicamenteux concernent des effets indésirables iatrogènes qui peuvent être graves (Évènement indésirable grave (EIG)), qu'ils soient le fait d'une erreur médicamenteuse ou non.Les médicaments peuvent provoquer une réaction anaphylactique ou anaphylactoïde. C'est le cas par exemple de l'acide acétylsalicylique, des inhibiteurs de l'enzyme de conversion de l'angiotensine et des sartans, des pénicillines, des céphalosporines, des produits de contraste, des anesthésiques locaux, des anti-inflammatoires non stéroïdiens. Des réactions croisées sont possibles, par exemple entre les pénicillines et les céphalosporines. Les bêtabloquants peuvent aggraver l'évolution d'une réaction anaphylactique et contrecarrer la réponse à l'adrénaline.La prescription d'un médicament n'est pas neutre - les effets induits ne sont pas toujours légers, ils peuvent être graves. Les effets secondaires peuvent à leur tour être mal interprétés, comme symptômes d'autre chose, ou d'une aggravation de l'état de la personne… ce qui complique singulièrement la situation et peut conduire à des prescriptions supplémentaires (inadaptées) à d'autres effets secondaires, et aussi à une dépendance.Les Français sont les plus gros consommateurs au monde de somnifères (3 fois plus que les Britanniques, ou que les Allemands) dont les effets secondaires peuvent être la dépression, avec ou sans tendances suicidaires, des états phobiques, l'agressivité et un comportement violent[réf. nécessaire].Dans le domaine des antibiotiques, une sur-prescript"
médecine;"Un organisme (du grec organon, « instrument »), ou organisme vivant, est, en biologie et en écologie, un système vivant complexe, organisé et est le produit de variations successives au cours de l'évolution. Il est constitué d'une ou plusieurs cellules vivantes (on parle alors, respectivement, d'organisme unicellulaire ou pluricellulaire). Les organismes vivants sont classifiés en espèces partageant des caractéristiques génétiques, biologiques et morphologiques communes.Les organismes complexes, multicellulaires, sont constitués d'un ensemble de cellules vivantes différenciées, assurant des fonctions spécialisées et opérant de manière concertée. Ces cellules dérivent en général d'une progénitrice unique et partagent le même patrimoine génétique. Elles interagissent de façon à fonctionner comme un ensemble stable dynamiquement.Un organisme vivant se trouve en effet dans un état thermodynamique de non-équilibre, mais conservant un environnement interne approximativement constant, grâce à l’apport continu d'énergie et, le cas échéant, de nutriments. Ce phénomène d'équilibre dynamique maintenu par l'organisme est appelée homéostasie.Quelques centaines d'espèces, dites « organismes modèles », sont utilisées comme modèles d'étude par les scientifiques et les laboratoires de recherche pour comprendre les mécanismes fondamentaux du vivant.Un organisme est un être organisé, qui peut être un organisme unicellulaire ou un organisme multicellulaire. Le terme d'organisme complexe s'applique à tout organisme vivant ayant plus d'une cellule.Il est difficile de définir avec précision ce qu'est un être vivant. On peut donner quelques caractéristiques du vivant :la capacité de se maintenir en vie en puisant dans l'environnement l'énergie et les composants nécessaires. Cette capacité s'appuie sur le métabolisme qui inclut diverses fonctions, telles que la nutrition, la respiration, la photosynthèse... ;la capacité de se développer selon une certaine organisation (croissance, morphologie, division cellulaire, développement) ;la capacité de se reproduire et de donner naissance à d'autres organismes vivants (reproduction végétative ou sexuée) ;la nécessité d'un environnement favorable pour survivre (température, pression, oxygène, eau...).La matière vivante est fondée sur la chimie organique avec comme base le carbone.Tout organisme vivant est mortel, par définition.Selon la source d'énergie utilisée, on distingue les organismes chimiotrophes, tirant leur énergie de molécules et les phototrophes, tirant leur énergie de la lumière.Tous les organismes vivants sont composés d'un nombre plus ou moins grand de cellules. Un organisme se développe en général à partir d'une cellule unique, par divisions cellulaires successives. Au cours de ce développement, les cellules subissent des étapes de différenciation, ce qui leur permet d'acquérir des spécialisations associées à des fonctions particulières. Un ensemble de cellules spécialisées de même type qui s'associent forment un tissu et l'organisation structurée de différents tissus constitue un organe. À l'intérieur d'un organisme vivant complexe, on trouve ainsi différents types cellulaires et différents tissus, variables suivant le type d'organisme considéré (animaux, plantes...).Ce type d'organisation hiérarchique : cellule, tissu, organe peut s'étendre aux grandes fonctions de l'organisme, on parle alors de système ou d'appareil, qui sont une collection d'organes participant à la même grande fonction : système nerveux, système respiratoire, appareil reproducteur, système racinaire (chez les plantes)...L'ensemble de l'organisme suit en général un plan d'organisation commun à tous les individus d'une même espèce. Ce plan d'organisation détermine la disposition relative des organes et des tissus, l'existence et le positionnement de membres ou d'appendices. Il est déterminé génétiquement et partagé en général par des espèces voisines sur le plan évolutif. Certains organismes vivants ont au cours de leur cycle de vie des stades d'existence très différenciés (stade larvaire, stade adulte...) avec des morphologies et des plans d'organisation qui peuvent varier, au travers d'étapes de métamorphose.Georges Chapouthier a proposé d'interpréter la complexité des organismes par l'application répétée de deux principes généraux, compatibles avec la sélection darwinienne : le principe de « juxtaposition » d'unités identiques, puis le principe d'« intégration » de ces unités dans des ensembles plus complexes, dont elles constituent alors des parties.EspèceOrganisme modèleOrganicismeRessource relative à la santé : (en) NCI Thesaurus  Portail de la biologie   Portail de l’écologie"
médecine;"En médecine, un patient est une personne physique recevant une attention médicale ou à qui est prodigué un soin.Le mot patient est dérivé du mot latin patiens, participe présent du verbe déponent pati, signifiant « celui qui endure » ou « celui qui souffre ».Il existe plusieurs dénominations communes au terme patient, dont personne soignée, bénéficiaire de soins , ""usager"" ou encore client employé notamment dans la culture anglophone[réf. souhaitée]. Dans la recherche médicale, le patient est parfois appelé sujet. On commence même à utiliser le terme d’actient (patient qui agit) du fait de l'évolution des patients à se renseigner par eux-mêmes et à poser de plus en plus de questions au praticien.[réf. souhaitée].En médecine, le patient bénéficie d'examens médicaux, de traitements prodigués par un médecin ou un professionnel de la santé pour faire face à une maladie ou à des blessures. Le patient peut également bénéficier d'actes de prévention.Knock ou le Triomphe de la médecine, pièce de théâtre de Jules Romains, ayant fait l'objet de plusieurs adaptations cinématographiques, qui illustre de manière humoristique les relations entre médecins et patients.Luc Perino, Patients zéro. Histoires inversées de la médecine, La Découverte, 2020Jean-Philippe Pierron, « Une nouvelle figure du patient ? Les transformations contemporaines de la relation de soins », Sciences sociales et santé, vol. 25, no 2,? 2007, p. 43-66 (DOI 10.3406/sosan.2007.1858)Relation médecin-patientCharte du patient hospitaliséÉducation thérapeutique du patientDossier médical du patientPatient zéro Portail de la médecine"
médecine;"La santé est « un état de complet bien-être physique, mental et social, et ne consiste pas seulement en une absence de maladie ou d'infirmité ». Dans cette définition par l'Organisation mondiale de la santé, OMS, depuis 1946, la santé représente « l’un des droits fondamentaux de tout être humain, quelles que soient sa race, sa religion, ses opinions politiques, sa condition économique ou sociale »,. Elle implique la satisfaction de tous les besoins fondamentaux de la personne, qu'ils soient affectifs, sanitaires, nutritionnels, sociaux ou culturels.. Mais cette définition confond les notions de santé et de bien-être.Par ailleurs, « la santé résulte d’une interaction constante entre l’individu et son milieu » et représente donc « cette « capacité physique, psychique et sociale des personnes d’agir dans leur milieu et d’accomplir les rôles qu’elles entendent assumer d’une manière acceptable pour elles-mêmes et pour les groupes dont elles font partie ». René Dubos présente en 1973 la santé comme « la situation dans laquelle l'organisme réagit par une adaptation tout en préservant son intégrité individuelle. C'est l'état physique et mental relativement exempt de gênes et de souffrances qui permet à l'individu de fonctionner aussi longtemps que possible dans le milieu où le hasard ou le choix l'ont placé. »,Pour René Leriche en 1936, « la santé c'est la vie dans le silence des organes. »,Dans les sociétés traditionnelles (« primitives »), la santé relève généralement autant de l'individu que du groupe. Elle est imbriquée avec les croyances animistes et religieuses, et le rôle des guérisseurs (chamans, sorciers, etc.) qui utilisent à la fois la pharmacopée locale, le toucher et des pratiques relevant de la magie, de la divination, ou de la psychologie.À partir du XVIIIe siècle, la maladie cesse progressivement d'être considérée comme une fatalité et le corps redevient un sujet de préoccupation. Ce mouvement concerne d'abord les élites, puis s'étend progressivement à l'ensemble de la société. La santé devient alors un droit que les États se doivent de garantir.L'état de santé se recherche à la fois pour chaque individu, avec la médecine clinique, ou pour une population, avec la santé publique.La santé d'une population est classiquement évaluée d'abord par les taux de mortalité et de morbidité, avec l’espérance de vie. La santé est une notion relative, « parfois non présentée comme corollaire de l'absence de maladie : des personnes porteuses d'affections diverses sont parfois jugées « en bonne santé » si leur maladie est contrôlée par un traitement. A contrario, certaines maladies peuvent être longtemps asymptomatiques, ce qui fait que des personnes qui se sentent en bonne santé peuvent ne pas l'être réellement. »« État de santé ressentie » : c'est l'un des indicateurs d'état de santé. Il est publié tous les deux ans depuis 2002, pour les pays de l'OCDE. Après une tendance à la hausse de 2002 à 2008, il a chuté de plusieurs points en 2010 « Quelles que soient les tranches d’âge, le pourcentage des femmes et des hommes s’estimant en bonne ou très bonne santé baisse en 2010. Et lorsque l’on considère l’ensemble des sexes, il en est de même pour le quintile de revenu le plus élevé ». En 2008, 74,9 % des hommes se jugeaient en bonne ou très bonne santé, contre 70,6 % en 2010. Pour les femmes ce taux est passé de 70,1 % à 66,5 %.La santé mentale peut être considérée comme un facteur très important de la santé physique pour les effets qu'elle produit sur les fonctions corporelles. Ce type de santé concerne le bien-être émotionnel et cognitif ou une absence de trouble mental. L'Organisation mondiale de la santé (OMS) définit la santé mentale en tant qu'« état de bien être dans lequel l'individu réalise ses propres capacités, peut faire face aux tensions ordinaires de la vie, et est capable de contribuer à sa communauté ». Il n'existe aucune définition officielle de la santé mentale. Il existe différents types de problèmes sur la santé mentale, dont certains sont communément partageables, comme la dépression et les troubles de l'anxiété, et d'autres non communs, comme la schizophrénie ou le trouble bipolaire.Pour l'Organisation mondiale de la santé (OMS), la santé reproductive est une composante du droit à la santé.Cette notion récente évoque la bonne transmission du patrimoine génétique d'une génération à l'autre. Elle passe par la qualité du génome, des spermatozoïdes et des ovules, mais aussi par une maternité sans risque, l'absence de violences sexuelles et sexistes, l'absence de maladies sexuellement transmissibles (MST), la planification familiale, l'éducation sexuelle, l'accès aux soins, la diminution de l'exposition aux perturbateurs endocriniens, etc.Un certain nombre de polluants (dioxines, pesticides, radiations, leurres hormonaux, etc.) sont suspectés d'être, éventuellement à faibles ou très faibles doses, responsables d'une délétion de la spermatogenèse ou d'altération des ovaires ou des processus de fécondation puis de développement de l'embryon. Certains sont également cancérigènes ou mutagènes (ils contribuent à l'augmentation du risque de malformation et d'avortement spontané).Les soins de santé reproductive recouvrent un ensemble de services, définis dans le Programme d’action de la Conférence internationale sur la population et le développement (CIPD) tenue au Caire (Égypte) en septembre 1994 : conseils, information, éducation, communication et services de planification familiale ; consultations pré et postnatales, accouchements en toute sécurité et soins prodigués à la mère et à l’enfant; prévention et traitement approprié de la stérilité ; prévention de l’avortement et prise en charge de ses conséquences ; traitement des infections génitales, maladies sexuellement transmissibles y compris le VIH/SIDA ; le cancer du sein et les cancers génitaux, ainsi que tout autre trouble de santé reproductive ; et dissuasion active de pratiques dangereuses telles que les mutilations sexuelles féminines.La santé au travail fait partie des principaux thèmes de santé identifiés par l'OMS.Un déterminant de santé est un facteur qui influence l’état de santé d'une population soit isolément, soit en association avec d’autres facteurs.L'hygiène est l'ensemble des comportements concourant à maintenir les individus en bonne santé. Ils demandent de pouvoir notamment faire la part entre les « bons microbes » et ceux qui sont pathogènes ou peuvent le devenir dans certaines circonstances. Ces circonstances l'hygiène cherche à les rendre moins probables, moins fréquentes ou supprimées. Après une phase hygiéniste, dont l'efficacité de court terme est indiscutable, sont apparus une augmentation des allergies, des maladies auto-immunes, des antibiorésistances et des maladies nosocomiales jugées préoccupantes. La recherche de juste équilibre entre exposition au risque et solution médicale usuelle est rendue difficile dans un contexte d'exposition accrue à des cocktails de polluants complexes (pesticides en particulier) et perturbateurs hormonaux, de modifications sociétales et climatiques planétaires (cf. maladies émergentes, risque pandémique, zoonoses, risque de bioterrorisme, etc.).La lutte contre les infections nosocomiales à l'hôpital, ou contre les toxi-infections alimentaires par exemple, est née après la découverte de l'asepsie sous l'influence par exemple de Ignace Semmelweis ou Louis Pasteur. Les comportements individuels et collectifs sont de toute première importance dans la lutte contre les épidémies ou les pandémies.Cette discipline de l'hygiène vise donc à maîtriser les facteurs environnementaux pouvant contribuer à une altération de la santé, comme la pollution par exemple, avec des problèmes paradoxaux à gérer : par exemple, l'amélioration des conditions d'hygiène semble avoir paradoxalement pu favoriser la réapparition de maladies comme la poliomyélite et diverses maladies auto-immunes et allergies.De nombreux facteurs de risque sont intrinsèquement liés au mode de vie. Les soins corporels, l'activité physique, l'alimentation, le travail, les problèmes de toxicomanie, notamment, ont un impact global sur la santé des individus.Nutrition : Aliments - Oligo-élément - AlicamentProduits d'hygiène : Crème solaire - Dentifrice - Préservatif - SavonToxicomanies & dépendances : Alcool - Cannabis - Cocaïne - Tabac - Jeu pathologiqueDe nombreux risques et dangers sont liés au domaine de la santé, l'évolution humaine et également les changements de son mode de vie ne sont pas sans conséquences. L'alimentation et les nouvelles technologies sont également des facteurs de risques en France et dans le reste du monde. Les rythmes, les cadences de travail ; les gestes inadaptés sont des facteurs très importants sur la santé. Ils entraînent des troubles psychosomatiques et parfois des handicaps pour la vie.Quatre facteurs permettraient d'allonger considérablement la durée de la vie : absence de tabac, consommation d'alcool égale ou inférieure à un demi verre par jour, consommation de 5 fruits et légumes par jour, exercice physique d'une demi-heure par jour. Le tout donnerait une majoration de l'espérance de vie de 14 ans par rapport au non-respect de ces facteurs.Du strict point de vue de l'alimentation, de nombreuses études concordantes concluent qu'une alimentation exclusivement végétarienne permet de limiter les risques de cancer et de maladies cardio-vasculaires, et donc d'avoir une espérance de vie en bonne santé plus longue,. Les études mettent à la fois en évidence les bénéfices d'une alimentation riche en légumes et fruits et les risques relatifs liés à la consommation de viande, poisson et produits laitiers,,,. Les compléments alimentaires synthétiques ne seraient absolument pas nécessaires,.D'autres pistes sont explorées pour allonger la durée de vie en bonne santé : le jeûne, le jeûne intermittent et la restriction calorique.Par ailleurs, l'« hygiénisme moral » trans-national débuté au XIXe siècle (à ne pas confondre avec la médecine alternative créée par Herbert Shelton) est une doctrine contre le « relâchement des mœurs », ce qui serait le meilleur moyen de garantir la santé. C'est ce courant qui a par exemple déclaré la lutte contre la syphilis ou l'alcoolisme comme priorité nationale. C'est également lui qui déclare que si les obèses sont gros, c'est qu'ils sont gourmands et paresseux, ou encore que les fumeurs n'ont pas de volonté; Il semble persister dans certaines politiques et campagnes d'information et d'éducation des citoyens à l'hygiène.C'est un domaine, parfois nommé « santé environnementale », qui se développe depuis la fin du XXe siècle, à la suite de la prise de conscience du fait que l'environnement, notamment lorsqu'il est pollué, est un déterminant majeur de la santé.La pollution aiguë ou chronique, qu'elle soit biologique, chimique, due aux radiations ionisantes, ou due aux sons ou la lumière (ces facteurs pouvant additionner ou multiplier leurs effets) est une source importante de maladies.Dans l'Union européenne, la Commission a adopté (11 juin 2003) une « stratégie communautaire en matière de santé et d'environnement », traduite le 9 juin 2004, en un « Plan d'action » (2004-2010), qui vise notamment les maladies dites « environnementales ». Cela concerne l'asthme et les allergies respiratoires, en cherchant plus généralement à « mieux prévenir les altérations de la santé dues aux risques environnementaux » (dont l'exposition aux pesticides et à leurs résidus). Des systèmes de veille sanitaire permanente doivent identifier les menaces émergentes (dont nanotechnologies, OGM, maladies émergentes, impacts des modifications climatiques, etc.) et en évaluer l'impact sanitaire selon des actions réalisées au niveau communautaire mais aussi national. Un « plan d'action environnement et santé » va être développé afin de mettre en œuvre cette stratégie ; de plus un processus de consultation a été lancé. Le plan d'action vise à faire le point sur les connaissances scientifiques existantes et à évaluer la cohérence et les progrès réalisés dans l'installation du cadre législatif communautaire en matière de santé et d'environnement. Un nouveau système d'information sur la santé est prévu « qui fonctionnera également dans le domaine de l'environnement » et veut devenir « la plus importante source de données fiables pour l'évaluation de l'impact des facteurs environnementaux sur la santé ». Ces aspects seront coordonnés avec les systèmes de réaction rapide et une approche intégrée « visant à juguler les déterminants environnementaux de la santé ».En ce qui concerne plus spécifiquement la France, un premier Plan national santé-environnement a été lancé en 2004 et un second en 2009, à la suite du Grenelle de l'environnement. Le bilan des actions menées devrait être fait en 2013.La santé publique désigne à la fois l'état sanitaire d'une population apprécié via des indicateurs de santé (quantitatifs et qualitatifs, dont l'accès aux soins) et l'ensemble des moyens collectifs susceptibles de soigner, promouvoir la santé et d'améliorer les conditions de vie.La notion de santé publique regroupe plusieurs champs :la santé au travail incluant la médecine du travail et parfois des démarches épidémiologiques ;la gestion des campagnes de prévention, qui doivent influencer les autres secteurs de la société pour y promouvoir la santé (économie, écoles, trafic, habitation, environnement, style de vie, etc.), la vaccination... ;l'organisation des réseaux de soins : premiers secours, hôpitaux, médecine libérale, médecine d'urgence... ;la formation initiale et continue des professions médicales et paramédicales ;la sécurité sociale et l'assurance maladie (Sécurité sociale en France) ;la recherche médicale et pharmacologique.Les règles en matière de santé font l'objet de textes internationaux édictés par l'OMS ou la FAO (Codex alimentarius pour l'alimentation).L'Union européenne a produit de nombreuses directives, règlements ou décisions pour protéger la santé des consommateurs ou d'animaux consommés.La promotion de la santé telle que définie par l'OMS est le « processus qui confère aux populations les moyens d'assurer un plus grand contrôle sur leur propre santé, et d'améliorer celle-ci ». Cette démarche relève d'un concept définissant la « santé » comme la mesure dans laquelle un groupe ou un individu peut d'une part réaliser ses ambitions et satisfaire ses besoins, et d'autre part évoluer avec le milieu ou s'adapter à celui-ci.La santé est prise en compte par le droit, y compris du point de vue des Conditions de travail.Les crises sanitaires sont des pandémies importantes, qui touchent entre une dizaine de personnes (cas des crises très médiatisées qui touchent les pays développés, comme certaines crises alimentaires) et des millions de personnes. Elles peuvent avoir des coûts économiques, sociaux et politiques considérables.L'OMS a d'ailleurs été créée pour qu'une pandémie telle que celle produite par la grippe espagnole ne se reproduise pas avec les mêmes effets (30 à 100 millions de morts selon les sources).Les sommes en jeu dans le domaine de la santé sont considérables, tant pour les coûts induits par les maladies, les pollutions et l'absentéisme, que par le marché des soins et des médicaments (en 2002, le marché mondial du médicament a été évalué à 430,3 milliards de dollars, contre 220 milliards en 1992). Le marché pharmaceutique a augmenté de 203 milliards d'euros. Et la consommation médicale progresse plus rapidement que le PIB dans les pays développés.Des crises sanitaires telles qu'une pandémie peuvent avoir des coûts économiques, sociaux et politiques considérables.La santé comme concept peut être un objet d’étude anthropologique. Tel que rapporté par Roy, elle est souvent conceptualisée comme une construction sociale par les anthropologues puisque le rapport que les sociétés ont avec elle est très variable d’une à l’autre, et selon les époques également. Le travail anthropologique cherchera donc à mieux comprendre l’expérience que font les groupes sociaux et culturels de la santé. Cet objet d’étude, pour faire preuve de rigueur méthodologique, doit être replacé dans son contexte global, notamment à travers les changements sociaux. On cherche alors à comprendre les phénomènes de relation santé/maladie, bien que de plus en plus le schéma santé/vie prend place. Pour dire autrement, selon Massé, l’anthropologie médicale s’intéresse à comment les acteurs sociaux définissent la bonne ou la mauvaise santé, et comment les maladies sont soignées dans ce contexte.Quelques approches théoriques sont nées en anthropologie médicale, rapportées par Roy. Parmi elles, celle de la théorie médico-écologique, celle de la phénoménologie et celle de la critique de la médecine et de la santé internationale.La théorie médico-écologique est formulée par Alexander Alland au début des années 1970, mais est reprise par d’autres quelques années après. Elle part du principe que les groupes humains adaptent leur culture à l’environnement. Cette théorie propose l'idée que l’adaptation culturelle est intimement liée à l’adaptation biologique en fonction de l'environnement et du milieu dans lequel le groupe se trouve. Ainsi, la santé est liée à ces transformations externes.L’approche phénoménologique se développe en parallèle à cette dernière. Des auteurs comme Kleinman et Good en sont un point d’origine, en cherchant à redonner une subjectivité à l’expérience humaine de la santé, s’éloignant de l’objectivité préconisée par la médecine. Pour ce faire, des perspectives expérientielles et sémantiques sont mobilisées.L’approche critique de la médecine et de la santé internationale se développe dans les années 1960. Elle a pour objet les conditions notamment politiques et économiques, donc globales, dans lesquelles sont vécues la santé et la maladie : les inégalités sociales façonnent l’accès à l’information, aux ressources de maintien de la santé et aux traitements. Un texte clé pour comprendre ce mouvement est notamment celui de Baer, Singer et Johnsen.De nombreux médias et émissions sont spécialisés dans les thèmes de la santé. En voici une sélection :Le Magazine de la santé, sur France 536.9, sur la Radio télévision suisseQuoi de neuf doc ?, sur TV5 MondeRadio Public SantéRadio France internationale, émission Priorité santéRadio Canada première chaîne, émission RDI SantéPlace à la santéSanté MagazineAlternative santéEnvironnement, Risques et SantéHealth On the Net Foundation est fondation qui indique aux internautes dans quels sites internet, ils peuvent obtenir des informations justes et sérieuses dans le domaine de la santé.PubMedSantepratiquePortail Santé-UEFasosante.netCarenity (réseau social santé sur internet destiné aux malades et à leurs proches).André Rauch, Histoire de la santé, PUF, Que-sais-je ?, 1995Georges Canguilhem, La santé, concept vulgaire et question philosophique, Sables, Pin-Balma, 1990Ressources relatives à la santé : (en) Medical Subject Headings (en) NCI Thesaurus Ressource relative à la recherche : Horizon 2020 Liste des thèmes de santé, site de l'OMS(en) Global Health, site Our World in Data Portail des soins infirmiers   Portail de la médecine   Portail de la société"
médecine;"En médecine, un symptôme (du grec ????????, « rencontrer ») ou signe fonctionnel est un signe qui représente une manifestation d'une maladie, tel qu'il est observé chez un patient. En général, pour une pathologie donnée, les symptômes sont multiples, et parfois il peut ne pas y avoir de symptôme (la maladie ou le malade est dit dans ce cas asymptomatique) ou peu de symptômes (maladie ou malade paucisymptomatique). Inversement, un même symptôme peut très souvent être attribué à différentes maladies : on ne peut donc en général pas conclure automatiquement qu'un symptôme (par exemple, le mal de gorge) est dû à une maladie donnée (par exemple, la grippe) ; ce serait commettre le sophisme de l'affirmation du conséquent.Le mot ????????, en grec, signifie « accident », « coïncidence » ; il est constitué du préfixe ???, « avec » et de ?????, « arriver », « survenir ». Le symptôme est donc, à l'origine, « ce qui survient ensemble », ce qui « concourt » ou « co-incide », au sens littéral du terme.Les symptômes sont les signes cliniques dont le malade se plaint (comme la douleur, la toux, le vertige, la tristesse). Les symptômes sont les éléments d'alerte d'un processus pathologique en cours, motivant ainsi le recours à une consultation médicale permettant d'objectiver la plainte en retrouvant des signes, qui, rassemblés en syndrome, puis en maladie en établissant un diagnostic, permettront de guider l'attitude thérapeutique.Les symptômes sont donc à différencier :des autres signes cliniques :les signes physiques, découverts en examinant le malade : contracture abdominale, souffle cardiaque,certains signes généraux : fièvre, hypotension artérielle ;des signes paracliniques obtenus à l'aide d'examens complémentaires :les signes radiologiques à la suite de radiographies,les signes biologiques à la suite de prélèvements.Par exemple, dans l'arthrose de hanche, le patient peut se plaindre de douleur à la marche (symptôme), et le praticien pourra objectiver à l'examen une limitation de mobilité de la hanche (signe physique), et sur une radiographie du bassin (signe radiologique).En créant la psychanalyse, Sigmund Freud va donner un sens au symptôme. À la suite des Études sur l'hystérie (1895), il n'a plus cesse de l'interroger dans les manuscrits à une époque « où la psychiatrie le réduisait à un phénomène hétérogène et opaque de la vie psychique ».Le symptôme peut être une manifestation somatique : une paralysie, des troubles du langage.Il peut être aussi une manifestation psychique : angoisse, hydrophobie.En étudiant le cas d'Anna O. (Bertha Pappenheim), une hystérique soignée par Josef Breuer grâce à la méthode cathartique, Freud a d'abord vu dans le symptôme un résidu mnésique d'expériences émotives (c'est-à-dire de traumatismes psychiques).Ensuite, en formulant sa nouvelle compréhension du système psychique, il a interprété différemment le symptôme.L'appareil psychique est composé de différentes instances en conflit : le moi, le ça et le surmoi.Quand une représentation (pulsionnelle) tombe sous le coup d'un interdit, elle est refoulée dans l'Inconscient par la censure opérée par le moi, mais jamais anéantie. Un processus alors de tentative de réapparition des éléments refoulés se met en place : c'est le retour du refoulé. Il y a plusieurs façons de déjouer la censure : le rêve, les lapsus, les oublis et les actes manqués ou bien les symptômes. Ces formations substitutives sont des formes de déguisement de la représentation, rendus acceptables pour la conscience pour pouvoir réinvestir son champ. Ainsi, ils permettent la satisfaction du désir sans éveiller la censure en formant un compromis entre les désirs et les interdits. Ce sont tous ces déguisements qui sont investigués, interprétés dans la cure psychanalytique.Remarque : il y a des liaisons associatives entre le symptôme et ce à quoi il se substitue.Le symptôme est le substitut de représentations tombées sous le coup d'un interdit et refoulées dans l'Inconscient. Il est le déguisement de ces représentations pour qu'elles puissent réinvestir le champ de la conscience, en étant acceptable. Et, il apporte une satisfaction de remplacement au désir inconscient, sans éveiller la censure et même en satisfaisant les exigences défensives. Cette double-satisfaction explique la capacité de résistance du symptôme car il est maintenu des deux côtés.Récapitulatif :il est formation de compromis en tant qu'il est le produit du conflit défensif ;il est formation substitutive dans la mesure où c'est le désir qui cherche à se satisfaire ;il est formation réactionnelle dans la mesure où c'est le processus défensif qui prévaut.Le symptôme est satisfaction, décharge pulsionnelle, il offre un bénéfice primaire. On ne saurait chercher à retirer au malade mental son symptôme, en ce qu'il en jouit, et que le psychologue doit reconnaitre comme jouissance.Ce bénéfice primaire correspond à la signification que porte le symptôme, signification qui seule permet l'expression d'un désir inconscient - le symptôme se rattache donc à la représentation, voire au discours. Pour Jacques Lacan, le symptôme est donc métaphore (Le symptôme est une métaphore que l'on veuille ou non se le dire).Le symptôme peut également engendrer un bénéfice secondaire, plaisir supplémentaire qui ne se relie donc pas directement au sens que veut énoncer ce signe de la maladie, mais qui provient plus d'un hasard relatif cette fois à la nature même du symptôme. Ainsi, le procédurier paranoïaque ralliant à lui un mouvement de soutien.Du point de vue psychosociologique, le symptôme est la façon particulière dont un individu trouve sa place dans le monde et règle son rapport à celui-ci, en fonction des contraintes et des stimulations psychosociales qui lui parviennent. Le symptôme est un prolongement de la personnalité, qui permet à cette dernière d'appréhender le monde mais aussi de s'en distancier, par un ensemble de protections constitutives dudit symptôme.Ainsi le symptôme est-il, du point de vue du sujet :stratégie d'individualisation ;matériau de la personnalité ;interprétation continue du monde ;modalité comportementale dynamique ;dispositif protecteur du Moi (ou ego) ;routine pathologique sitôt qu'il étouffe la créativité du sujet ou porte atteinte à l'intégrité d'autrui.Cet article est partiellement ou en totalité issu de l'article intitulé « Symptôme fonctionnel » (voir la liste des auteurs). Médecine et biologie  Psychanalyse Augustin Jeanneau, « symptôme », dans Alain de Mijolla (dir.), Dictionnaire international de la psychanalyse 2. M/Z, Paris, Hachette, 2005 (ISBN 2-0127-9145-X), p. 1769-1770.Augustin Jeanneau et Roger Perron, « symptôme (formation de -) », dans Alain de Mijolla (dir.), Dictionnaire international de la psychanalyse 2. M/Z, Paris, Hachette, 2005 (ISBN 2-0127-9145-X), p. 1770-1772.Yves Morhain, « Permanence du corps et variations du symptôme hystérique et/ou psychosomatique », Psychothérapies, 2011/2 (Vol. 31), p. 131-141. DOI : 10.3917/psys.112.0131. [[ lire en ligne]]Valentin Nusinovici, « symptôme,sinthome », dans Alain de Mijolla (dir.), Dictionnaire international de la psychanalyse 2. M/Z, Paris, Hachette, 2005 (ISBN 2-0127-9145-X), p. 1772-1774.Marcel Scheidhauer, « Le symptôme, le symbole et l'identification dans l'hystérie dans les premières théories de Freud », in: Enfance, tome 40, n°1-2, thématique : « Identités, Processus d'identification. Nominations », 1987, p. 151-162, sur le site de Persée, consulté le 30 mars 2021 [lire en ligne].Liste des symptômes en médecine humaineSémiologie médicaleTableau cliniqueSigne physiquePathomimieSymptôme (pathologie végétale)Inhibition, symptôme et angoisseSinthome Portail de la médecine   Portail de l’agriculture et l’agronomie   Portail de la psychologie"
médecine;"Le système immunitaire d'un organisme est un système biologique complexe constitué d'un ensemble coordonné d'éléments de reconnaissance et de défense qui discrimine le soi du non-soi. Ce qui est reconnu comme non-soi est détruit.Il protège l'organisme des agents pathogènes : virus, bactéries, parasites, certaines particules ou molécules « étrangères » (dont certains poisons), mais est responsable du phénomène de rejet de greffe.Il est hérité à la naissance, mais autonome, adaptatif et doué d'une grande plasticité, il évolue ensuite au gré des contacts qu'il a avec des microbes ou substances environnementales étrangères au corps.On dénombre plusieurs types de systèmes immunitaires parmi les espèces animales, et généralement plusieurs mécanismes immunitaires collaborent au sein d'un même organisme.Pour de nombreuses espèces, dont les mammifères, le système est constitué de 3 couches. Ses principaux effecteurs sont les cellules immunitaires appelées leucocytes (ou globules blancs) produites par des cellules souches, au sein de la moelle osseuse rouge. Cette séparation en trois couches n’empêche pas une interaction très importante des couches entre elles:La barrière épithéliale comme la protection de la peau et les muqueuses, l'acidité gastrique ;Les mécanismes de défense non spécifique ou innée ou naturelle dérivant des cellules immunitaire de la lignée myélocytaire .Les mécanismes de défense spécifique ou adaptative dérivant des cellules immunitaires de la lignée lymphocytaire. L' immunité humorale s'oppose aux agents pathogènes extracellulaires grâce à des anticorps spécifiques, protéines sécrétés grâce aux lymphocytes B. L'immunité cellulaire s'oppose aux pathogènes intracellulaires, pris en charge par les lymphocytes T . Cette dernière couche n’existe que chez les vertébrés.On appelle réponse immunitaire l'activation des mécanismes du système immunitaire face à la reconnaissance de non-soi, agressive ou pas, face à une agression ou à une dysfonction de l'organisme.L'ensemble de ces systèmes (y compris lors de la vaccination) permet la résilience immunitaire, notion qui recouvre la somme des mécanismes efficaces de défense d’un organisme vis-à-vis d’un agent pathogène (du grec pathos : souffrance) ; il se dégrade avec l'âge (Immunosénescence).Le système peut entraîner un dysfonctionnement autoimmune.Toutes les cellules du système immunitaire dérivent d'une cellule souche présente dans la moelle osseuse. Cette cellule souche donne deux lignées de cellule : la lignée lymphocytaire et la lignée myélocytaire.[citation nécessaire]Les cellules de l'immunité innée sont produites par la lignée myélocytaire. Les cellules de l'immunité adaptative sont produites par la lignée lymphocytaire.Un seul type de cellule est produite par les deux lignées : la cellule dendritique.[réf. nécessaire]La cellule souche multipotente donne le progéniteur lymphoïde (lymphoid progenitor) qui se divise en trois types de cellules :le progéniteur cellulaire B (B cell progenitor). Ces cellules se différencient en cellule à mémoire B (Memory B cell) et en plasmocyte qui secrète des anticorps. Ces cellules sont un composant majeur du système immunitaire adaptatif ;le progéniteur cellulaire T (T cell progenitor) donne naissance aux cellules à mémoire T, aux lymphocytes T cytotoxiques et aux lymphocytes T auxiliaires. Les cellules à mémoire T ont les mêmes fonctions que les cellules à mémoire B. Les cellules cytotoxiques reconnaissent les antigènes des agents pathogènes et les détruisent. Les lymphocytes T auxiliaires secrètent des cytokines qui stimulent les cellules de la lignée B ;les cellules lymphocytaires tueuses (ou lymphocytes NK, sigle de l'anglais Natural Killer, signifiant « tueur naturel ») sont les cellules tueuses du système immunitaire inné. Elles détectent les cellules infectées par les virus et les détruisent ;la lignée lymphocytaire produit aussi des cellules dendritiques.Responsable de la production des hématies et plaquettes, cette lignée donne naissance à des cellules impliquées dans le système immunitaire inné et dans le système immunitaire adaptatif en produisant des cellules portant les antigènes des agents pathogènes pour les présenter aux cellules du système immunitaire adaptatif :les neutrophiles sont des cellules du système immunitaire inné s’attaquant aux agents pathogènes et les détruisant. Cette destruction peut aboutir à la destruction de la cellule elle-même aboutissant à la formation de pus ;les éosinophiles libèrent des cytokines en présence de parasite ;les monocytes vont se différencier en macrophages et en cellules dendritiques. La cellule dendritique permet une communication entre les cellules responsables de l'immunité innée et les cellules responsables de l'immunité adaptative. Les macrophages sont des cellules « poubelles » ;un dernier type de cellules impliquées dans les réactions allergiques, les cellules mastocytaires et les basophiles.L'organisme se défend contre les dysfonctions de ses cellules et les agressions, c'est-à-dire des processus qui ont pour conséquence de détruire des êtres vivants. Ces agressions peuvent revêtir différentes formes :des agressions dites physico-chimiques :mécaniques (frottements, chocs, chutes, etc.),facteurs climatiques (altitude, changement brusque de température, rayonnements, sécheresse, poussières, etc.),agression par des agents chimiques (acides, bases, etc.) ou autres éléments (aluminium, notamment) ;des agressions par d'autres êtres vivants : un organisme constitue pour un autre organisme un endroit idéal pour le développement de ses propres cellules et aussi pour un certain nombre de microorganismes qui pourraient y proliférer, il s'agit alors d'une infection. Les agresseurs dans ce cas peuvent être : des virus, des bactéries, des champignons, des levures, des helminthes ou des prions.Le système inné est un mécanisme très rapide de défense aux infections: il permet souvent d'arrêter un agent pathogène ou, du moins, de permettre la mise en route du système adaptatif qui a des armes plus puissantes et plus spécifiques pour arrêter l'agent pathogène. Il a longtemps été considéré comme un système non spécifique, mais la découverte de récepteurs cellulaires spécifiques de plusieurs familles de pathogènes (comme les bactéries gram-négatives) dans les années 2000 a changé notre regard sur le système inné.La peau est le premier, le plus grand et le plus important élément du système de défense : il empêche l'entrée de la plupart des corps étrangers. Les épithéliums cutanés secrètent des peptides anti-microbiens dès la pénétration d'un agent pathogène. Ils secrètent aussi des cytokines et des chimiokines.Les personnes qui perdent trop de peau, par brûlure par exemple, risquent de succomber à des infections. Pour éviter cela, elles sont placées dans des chambres hospitalières maintenues aussi stériles que possible.Les muqueuses (ou tissus continus) qui recouvrent les voies oropharyngiennes et digestives, ainsi que les voies respiratoires et urogénitales, constituent également une barrière physique. Les cellules très étroitement juxtaposées sont imperméables à la plupart des agents infectieux. En surface de certaines muqueuses, un film de mucus animé par les battements de cils vibratiles permet de fixer, enrober puis évacuer vers l'extérieur la plupart des particules ou êtres vivants intrus.Les cellules possèdent divers systèmes « passifs » de défense chimique et biochimique (enzymes, acides gras, acide lactique, pH du corps, etc.).La peau et les muqueuses sont recouvertes d'une flore bactérienne dite « normale » qui protège, souvent par concurrence, des micro-organismes pathogènes.Le système immunitaire inné est déclenché par des récepteurs cellulaires reconnaissant des structures moléculaires uniques aux agents pathogènes ou par des molécules signifiant les dégâts. Cellules effectrices de l'immunité innée Elles dérivent toutes de la lignée myélocytaire de l'hématopoïèse. Elles sont parfois regroupées sous le terme de leucocytes phagocytaires ou phagocytes. Ce terme est très réducteur car il laisse à penser que la seule fonction de ces cellules est la phagocytose, alors qu'elles ont d'autres fonctions essentielles.Ce sont des cellules immunitaires qui reconnaissent les microorganismes grâce à de nombreux récepteurs cellulaires présents à leur surface. Ces récepteurs permettent aux phagocytes de reconnaître certaines structures présentes à la surface des microorganismes infectieux et d'internaliser ces derniers à l'aide d'une vacuole digestive. Par la suite, ils fusionnent la vacuole contenant les microbes avec un lysosome. Les lysosomes peuvent contenir des formes toxique d'oxygène comme du monoxyde d'azote (NO) ou du peroxyde d'hydrogène (H2O2), et ils peuvent aussi contenir du lysozyme et d'autres enzymes digestives qui dégradent des structures microbiennes. Cellules résidentes dans le tissu sous épithéliales Les cellules résidentes sont les premières activées en cas de franchissement de la barrière épithéliale (cutanée, respiratoire ou intestinales) par un microbe. Macrophage Les macrophages ont une plus grande capacité de phagocytose que les granulocytes neutrophiles, et, lorsqu'ils phagocytent un microorganisme, des voies cellulaires internes les stimulent, ce qui les rend plus efficaces dans leurs tâches. Mastocyte Ces cellules contiennent des granules contenant des substances vasodilatatrices comme l'histamine. Cette substance en vasodilatant le vaisseau entraîne une diminution de la vitesse de circulation du sang permettant au leucocyte neutrophile de traverser la paroi vasculaire. Cellule dendritique Les cellules dendritiques qui dérivent aussi des monocytes sont des cellules présentatrices d'antigènes. Leur rôle est de capturer un microbe au site d'infection, de migrer vers les tissus lymphoïdes et de présenter les antigènes du microbe aux lymphocytes T à l'aide d'une molécule de CMH. Ce type de molécule joue un rôle très important dans la réaction immunitaire primaire. Cellules présentes dans le sang  Leucocyte neutrophile Les granulocytes neutrophiles représentent 60 à 70 %[réf. nécessaire] des leucocytes. Ils pénètrent dans les tissus infectés pour phagocyter les microbes présents et les détruire. Généralement, les granulocytes neutrophiles s'autodétruisent en même temps qu'ils détruisent les microbes. Ils ont normalement une espérance de vie de seulement quelques jours. Ce sont des cellules présentes dans le sang et capables de migrer vers un site où se produit une infection. Leucocyte éosinophile Les granulocytes éosinophiles sont présents en très petite quantité dans l'organisme. Ils ont une faible capacité de phagocytose, mais ils sont essentiels dans le combat contre les parasites présents dans l'organisme. Ils se lient à la paroi du parasite et libèrent des enzymes qui vont causer des dommages importants à celui-ci. Leucocyte basophile Les leucocytes basophiles sont les plus rares des leucocytes. Leur taux est si faible que l'absence de leucocyte basophile au cours d'une numération formule sanguine ne doit pas être considérée comme anormale. Monocyte Les monocytes représentent 5 % des leucocytes. Ils circulent dans le sang et migrent vers un tissu où ils se transformeront par la suite en macrophages. Les macrophages et les cellules dendritiques sont des cellules résidentes dans les tissus sous-épithéliaux. Molécules de l'immunité innée Il existe quatre grands groupes de molécules intervenant dans l'immunité innée : les peptides anti-microbiens, le système du complément, l'interféron I alpha et I beta et les protéines de la phase aiguë dont la plus utilisée en pratique médicale est la protéine C reactive. Première étape ; le franchissement de la barrière épithéliale L'introduction d'un agent infectieux, comme une bactérie gram négatif, au cours d'une piqûre à travers la peau déclenche dans les minutes qui suivent la libération de peptides anti-microbiens et de cytokines par les cellules de l'épithélium cutané. Deuxième étape ; la stimulation des cellules résidentes Les cellules résidentes de l'immunité innée (macrophage, mastocyte, cellule dendritique) reconnaissent le pathogène par des récepteurs appelés pattern recognition receptor (PRR) ou en français récepteurs de reconnaissance de motifs moléculaires, dont il existe 4 types principaux. Pour les bactéries gram-négatives, il s'agit d'un récepteur de type Toll (TLR ou Toll Like Receptor). La bactérie contient sur la surface de sa paroi des lipopolysaccharides spécifiques aux bactéries gram-négatives qui sont reconnus par les TLRs. Les structures biochimiques reconnues par le TLR sont appelées motifs moléculaires associés aux pathogènes.La liaison TLR-PPR va déclencher des événements qui diffèrent selon le type cellulaire. Au niveau des mastocytes, elle entraînera la libération d'histamine, celle-ci déclenchant la dilatation des vaisseaux aboutissant à un ralentissement de la circulation sanguine. Au niveau des macrophages et des cellules dendritiques, elle entraînera la libération des cytokines et des chimiokines; les chimiokines vont attirer les leucocytes une fois que ceux-ci ont traversé l’endothélium de la paroi vasculaire. La liaison TLR-PPR active une voie de signalisation qui va déclencher une synthèse de protéine anti-microbienne. Troisième étape ; le recrutement des cellules immunitaires sanguines Le ralentissement du débit sanguin secondaire à la vasodilatation permet aux leukocytes de traverser la paroi. Outre les leucocytes, les facteurs du complément traversent la paroi participant aussi à la réaction du système inné. Au niveau cutané, la manifestation clinique de l'infection se traduit par quatre signes : rougeur, chaleur, douleur et œdème. Ces quatre signes caractérisent la réaction inflammatoire. Si l'infection n'est pas contenue localement Si le système inné n'arrive pas à contenir l'infection, la cellule dendritique va se diriger vers un ganglion lymphatique par les canaux lymphatique. Elle va se maturer au cours du voyage. Dans le ganglion, elle présentera à la cellule T CD4 + auxiliaire des petits morceaux de 30 à 40 acides aminés de la bactérie phagocytée. Cette présentation de l’antigène se fait par son complexe majeur histocompatibilité de classe II.L'immunité adaptative repose sur 3 acteurs : les organes lymphoïdes, les lymphocytes B et les lymphocytes T. Ces 3 acteurs vont permettre de reconnaître un agent pathogène, de le signaler et de déclencher soit l'immunité humorale soit l'immunité cellulaire. Que ce soit l'immunité humorale ou l’immunité cellulaire, l'immunité adaptative ne se déclenchera que si cet antigène porte aussi un récepteur cellulaire de pathogénicité montrant bien la complexité et l'interaction des deux immunités.Spécificité : Les lymphocytes T ou B sont porteurs de récepteurs spécifiques. Chaque lymphocyte ne va reconnaître qu'un seul agent pathogène. L'ensemble des lymphocytes est appelé le répertoire des lymphocytes. La multiplicité des lymphocytes pour chaque agent pathogène est possible grâce à la recombinaison V(D)J. Cette recombinaison somatique pourrait produire plus de 10 000 000 000 de type de récepteurs différents aboutissant à un répertoire gigantesque.Reconnaissance du soi et du non-soi :le soi d'un individu est défini par des récepteurs du complexe majeur d'histocompatibilité (CMH, nommé HLA chez l'humain) présents sur la membrane de ses cellules, associés aux fragments peptidiques qu'ils présentent. Il existe deux types de molécule de CMH : le CMH de type I qui est présent chez presque toutes les cellules de l'organisme et le CMH de type II qui est retrouvé principalement chez les cellules dendritiques, les macrophages et les lymphocytes B ;le non-soi d'un individu est défini par des récepteurs cellulaires ou toute autre molécule différente du soi qui sont ainsi reconnus comme étrangers par notre organisme. Le non-soi déclenche une réaction immunitaire. La reconnaissance du non-soi se calque sur celle du soi, y compris au sein des liquides circulants extracellulaires, lymphatiques, veineux, artériels mais aussi des différents mucus (voir ci-dessus). Les immunoglobulines portés par les membranes des globules blancs et les immunoglobulines dissoutes se fixent sur les molécules présentes dans ces différents liquides. En l'absence de molécules HLA, ou si cette molécule est inconnue, le système immunitaire la reconnaît comme du non-soi et déclenche la mort programmée ou apoptose. Une des limites d'efficacité et de sensibilité du système immunitaire repose donc sur la spécificité de la distinction entre le soi et le non-soi :par exemple, les lymphocytes et les plasmocytes ne pénètrent pas dans certains tissus comme ceux du cerveau ou de la thyroïde, ils ne les reconnaissent donc pas comme du soi. Qu'une inflammation s'y installe, ou que ces cellules entrent à leur contact, et ils les identifieront comme du non-soi, puis ils secréteront des anticorps qui s'y fixeront pour les détruire, ce qui initiera une réaction dite auto-immune qui ira en s'amplifiant,de même, si les molécules présentes à la surface des agents pathogènes ou des cellules cancéreuses proviennent du soi ou en sont suffisamment proches, le système immunitaire les considèrera comme du soi et ne déclenchera pas de réaction immunitaire,le problème est similaire pour les muqueuses où la frontière entre le soi et le non-soi est très ténue. Des molécules habituellement bien tolérées peuvent donc y devenir allergisantes quand elles pénètrent dans des espaces d'où elles devraient être absentes.Les organes lymphoïdes comprennent le thymus, la moelle osseuse, la rate, les amygdales, l'appendice et les ganglions lymphatiques.Le système immunitaire humoral agit contre les bactéries et les virus avant leur pénétration dans les cellules. Les cellules responsables de la destruction des pathogènes extra-cellulaires sont les lymphocyte B agissant en sécrétant des anticorps. Lymphocyte B La production et la maturation des lymphocytes B se fait dans la moelle osseuse. Les lymphocytes B sont le support de l'immunité humorale. Ils possèdent à leur surface des récepteurs, nommés BCR, B Cell Receptor ou récepteurs des cellules B. Chaque lymphocyte B possédée plusieurs BCRs mais pour un seul agent pathogène. Ce BCR est une immoglobuline membranaire formée de 2 chaînes légères et de 2 chaînes longues. Il existe autant de lymphocytes B que de pathogènes. L'ensemble des lymphocytes B est appelé le répertoire des lymphocytes B. Chaque BCR possède 2 sites de fixations à l'antigène.Le lymphocyte B avant d'être activé est appelé naïf. L'activation du lymphocyte B par l'intermédiaire des BCR déclenche une expansion clonale du lymphocyte activé, avec production de cellule mémoire, et déclenche à distance des cellules produisant des anticorps. Ces cellules produisant des anticorps sont appelés plasmocytes. L'activation du lymphocyte B par un antigène nécessite l'implication des cellules lymphocytaires T CD4.Les lymphocytes B sont nommés B car ces lymphocytes ont été découverts chez l'oiseau dans la bourse de Fabricius ; par la suite le « B » fut conservé car c'est l'initiale de bone marrow (l'anglais de moelle osseuse) qui correspond au lieu de maturation de ces cellules à la suite d'une exposition à une interleukine (molécule chimique permettant le clonage des lymphocytes B et leur différenciation) produite par les lymphocytes T4. Anticorps ou immunoglobulines  Structure Ses principaux moyens d'action sont les immunoglobulines, aussi appelées anticorps. Les anticorps sont des molécules ayant une forme de « Y » formées de quatre chaînes polypeptidiques : deux chaînes légères (environ 200 acides aminés chacune) et deux chaînes lourdes (environ 450 acides aminés chacune).Les chaines légères ont des régions constantes et des régions variables. Les régions variables dépendent de la régulation somatique. Les anticorps ont une forme en Y. La barre verticale du Y est constituée de deux chaînes lourdes constantes qui vont déterminer la fonctionnalité de l’immunoglobuline. Les deux barres inclinées du Y sont formées chacune d'une chaîne lourde et d'une chaîne légère, chacune ayant une partie constante et une partie variable qui est responsable de la spécificité de l'anticorps. Fonctions Il existe 5 classes d'anticorps : les IgM, les IgG, les IgA, les IgE et les IgD. Les IgM sont les premiers anticorps à être produits lorsque le corps reconnaît un nouvel antigène. Ceux-ci se retrouvent dans le corps sous forme de pentamère et ils sont très efficaces pour activer le complément. Les IgG sont la classe d'anticorps la plus retrouvée dans le sang, c'est aussi la seule classe d'anticorps qui peut traverser le placenta et donner au fœtus une immunité passive. Les IgA se retrouvent dans les sécrétions (salive, larme, mucus, etc.) sous la forme de dimères. De plus, la présence de ce type d'anticorps dans le lait de la femme permet aux nouveau-nés de recevoir une immunité passive durant la période d'allaitement. Les IgE sont les anticorps impliqués dans les réactions allergiques puisqu'ils provoquent la libération d'histamine et d'autres substances impliquées dans ce genre de réaction par les granulocytes basophiles. Finalement, les IgD sont retrouvés à la surface des lymphocytes B dits « naïfs » (qui n'ont pas encore été exposés à un antigène) et servent de récepteurs cellulaires à ceux-ci. Contrairement aux quatre autres classes d'anticorps, les IgD ont une région transmembranaire qui leur permet de se fixer à la membrane cellulaire des lymphocytes B.Les quatre fonctions principales des anticorps sont :la neutralisation et l'agglutination d'antigènes : la neutralisation est le fait d'empêcher les microbes de se lier au cellules de l'hôte ; l'agglutination est le fait de former des agrégats de microbes qui sont facilement phagocytés ;l'opsonisation (recouvrement de la cellule cible pour qu'elle soit phagocytée). L'anticorps, après s'être fixé sur un agent pathogène, le présente à un macrophage pour que celui-ci le détruise. Cette reconnaissance par les macrophages se fait par l’intermédiaire de récepteurs FCR qui se lient à la région constante de la chaîne lourde et activent la phagocytose ;l'activation du complément ;l'élimination d'agents pathogènes trop gros pour être phagocytés, comme un ver. Les anticorps se lient au niveau du ver et activent la libération de substances toxiques par les cellules éosinophiles. C'est la cytotoxicité dépendante des anticorps ou ADCC (Antibody Dependent Cytotoxycity).La fonction principale de l'immunité cellulaire est de détruire les agents infectieux intracellulaires. Les cellules responsables de la destruction des pathogènes intra-cellulaires sont les lymphocytes T qui agissent directement en injectant des substances toxiques dans les cellules infectées. Lymphocyte T La formation et la maturation des lymphocytes T se fait dans le thymus où le lymphocyte prend le nom de thymocyte. Le lymphocyte T est aussi porteur d'un récepteur pour reconnaître les antigènes pathogènes : les récepteurs des cellules T ou TCR. À la différence des récepteurs des cellules B, le récepteurs des cellules T ne reconnaissent qu'un seul type de molécules : les peptides.La reconnaissance de la présence d'un agent infectieux intracellulaire par les lymphocyte T se fait par l'intermédiaire du complexe majeur d'histocompatibilité, nommés aussi CMH ou MHC, présent sur les cellules.Ce complexe majeur d'histocompatibilité a été découvert lors des transplantations d'organes. Ces MHC recueillent en permanence les peptides formés continuellement par la cellule par la dégradation protéique intracellulaire ; elles sont spécifiques à un individu.Les peptides formés en permanence par la cellule par la dégradation protéique intracellulaire et portés par les MHC à l'extérieur de la cellule permettent aux lymphocytes T de vérifier la « santé » de la cellule. En cas d'infection virale, les MHC vont présenter à l'extérieur des peptides viraux qui vont être reconnus par les lymphocytes T. Il en est de même lors d'une greffe d'organe après laquelle les MHC produits seront reconnus comme n’appartenant pas à l'organisme (au soi) risquant de déclencher un rejet de greffe.Le système immunitaire cellulaire s'occupe des cellules infectées par des virus, bactéries, et les cellules cancéreuses. L'action s'effectue via les lymphocytes T. Les lymphocytes T sont capables d'interagir avec les cellules de l'organisme grâce à leurs récepteurs cellulaires T ou TCR (T Cell Receptor) formés de deux chaînes polypeptidiques: la chaîne ? (alpha) et la chaîne ? (bêta). Ces récepteurs sont tout aussi spécifiques aux antigènes que les anticorps ou que les récepteurs de lymphocytes B, mais, contrairement aux anticorps et aux récepteurs de lymphocytes B, les récepteurs de lymphocytes T ne reconnaissent que de petits antigènes qui doivent être présentés par une molécule de CMH à la surface d'une cellule infectée.Aux lymphocytes T s'ajoutent aussi les lymphocytes NK (natural killer). Ces cellules sont impliquées dans une réponse à mi-chemin entre spécifique et non spécifique, selon les situations. Ils jouent notamment un rôle en début de grossesse, le fœtus devant se protéger contre eux pour pouvoir survivre dans le ventre de sa mère. Les deux types de complexe majeur d'histocompatibilité Lorsqu'un agent pathogène pénètre dans une cellule, il reste dans le cytoplasme ou infecte les vacuoles. Les mécanismes pour détruire l'agent différent selon sa localisation et expliquent en partie l'existence de deux familles de MCH, les MCH I et MCH II. Le complexe majeur d'histocompatibilité de type I Les MHC I sont produites par les infections cytoplasmiques. Ils activent les lymphocytes T CD 8 qui possèdent le récepteur CD 8. Ces cellules jouent un rôle prédominant dans l'infection virale. Les lymphocytes T CD 8 sont nommés lymphocytes cytotoxiques ou CTL. En effet, la liaison de CD8 sur la molécule de CMH permet de garder le lymphocyte T et la cellule infectée liés plus longtemps, ce qui favorise l'activation du lymphocyte. Une fois activé, le lymphocyte T cytotoxique libère des protéines, comme la perforine ou des granzymes qui provoquent la formation de pores dans la paroi cellulaire de la cellule infectée, entraînant sa mort. Cela a pour effet de priver le pathogène d'un lieu de reproduction et de l'exposer aux anticorps et aux leucocytes phagocytaires qui circulent dans la région infectée. Les MHC I sont présents sur toutes les cellules nuclées de l'organisme. Les hématies ne possèdent donc pas de MHC I. Le complexe majeur d'histocompatibilité de type II Les MHC II sont présents sur un nombre très restreint de cellules : cellules dendritiques, macrophages et lymphocytes B.Les MHC II sont produits par les infections vacuolaires ou la phagocytose. Ils activent les lymphocytes T CD 4 qui possèdent le récepteur CD 4. Les lymphocytes T CD 4 sont nommés lymphocytes helpers ou auxiliaires. En activant les TCD4, ceux-ci libèrent des cytokines transformant les lymphocytes B en plasmocytes sécrétant des immunoglobulines.Vidéo de la réponse adaptativePour que la cellule lymphocytaire B produise des anticorps spécifiques et que le lymphocyte T CD8+ naïf se transforme en lymphocyte T CD8 tueur, il faut deux signaux :un signal reçu directement par le lymphocyte B naïf ou le lymphocyte T CD8+ lorsque le microbe se fixe directement sur les récepteurs du lymphocyte B ou T ;un signal donné par le lymphocyte T CD4+ helper après activation par la cellule dendritique. Première étape : la présentation de l’antigène par la cellule dendritique La cellule dendritique est une cellule immunitaire résidant dans le derme ou dans le tissu conjonctif sous-épithélial des bronches ou de l'intestin. Dès l’introduction d'un pathogène, elle va être activée par les molécules émises par les cellules de l'épithélium (les peptides anti-microbiens et les cytokines pro-inflammatoires : l' interleukine-1, l' interleukine-6 et l'interféron-1 alpha et beta).La cellule dendritique immature possède des récepteurs de reconnaissance de motifs moléculaires qui reconnaissent la famille du microbe porteur d'un motif moléculaire associé aux pathogènes. Elle va internaliser le microbe, le transporter vers un ganglion lymphatique par la lymphe. Au cours du transport, elle va devenir une cellule dendritique mature avec apparition de molécules qui vont lui permettre de se fixer à un lymphocyte T CD4+ auxiliaire naïf.Durant son transport et dans le ganglion, la cellule dendritique coupe le microbe en morceaux compris entre 30 et 50 acides aminés. Ces morceaux vont être présentés aux lymphocytes T CD4+ auxiliaires grâce au complexe majeur d'histocompatibilité de type II (MCH II). C'est la présentation de l'antigène. Deuxième étape : l'activation de la cellule T CD4+ auxiliaire par la synapse immunologique L'activation de la cellule T CD4+ se fait grâce à la synapse immunologique. Des molécules d'adhésion immobilisent la cellule dendritique au lymphocyte TCD4+. La cellule dendritique présente un peptide à la cellule T CD4+. La protéine CD4 se fixe sur un domaine du MCH II. Enfin, des co-récepteurs sont produits par la cellule dendritique après stimulation des récepteurs de reconnaissance de motifs moléculaires. L'ensemble représente la synapse immunologique : la cellule T CD4+ est avertie d'une espèce particulière de microbe par le MCH II de la cellule dendritique à travers les récepteurs de reconnaissance de motifs moléculaires de cette cellule dendritique.En fonction du signal de famille de pathogène donné par la cellule dendritique lors de la présentation de l'antigène, des cytokines de types différentes vont être émises par la cellule dendritique et activent de façon spécifique les lymphocytes T CD4+ notamment en Th1, Th2. Chaque groupe est spécialisé d'une famille de pathogènes (virus, ver, bactérie, etc.). Troisième étape : l’activation des lymphocyte B par la reconnaissance de l’antigène ayant activé la cellule T CD4+ auxiliaire Le même microbe qui a été reconnu par la cellule dendritique se fixe sur les récepteurs des cellules B. Cette fixation va entraîner une activation et un processus aboutissant à la présentation de peptides microbiens par les complexes majeurs d'histocompatibilité de type II (MCH II) au récepteur du T CD4+ : c'est le premier signal.Le lymphocyte T CD4+ va reconnaître que ce peptide est le même que celui présenté par la cellule dendritique : c'est le deuxième signal.En fonction du type de CD4+ (Th1, Th2), le TCD 4+ va synthétiser des cytokines, principalement des interleukines, qui à leur tour vont déterminer le type d’anticorps sécrétés. Le lymphocyte B naïf se transforme en lymphocyte B activé. Il va commencer à produire des anticorps A, G ou E. Ces anticorps vont rejoindre le site de l'infection par les canaux lymphatiques aboutissant au canal thoracique se jetant dans l'aorte et vont atteindre le site de l'infection. Un groupe de lymphocytes à mémoire est aussi créé. Quatrième étape : l’activation des lymphocyte T CD8+ tueurs par la cellule T CD4+ auxiliaire  Première étape. Libération des molécules anti-microbiennes du système inné L'introduction d'un agent infectieux, comme un virus, déclenche dans les minutes qui suivent la libération de peptides anti-microbiens et de cytokines par les cellules de l'épithélium cutané. Deuxième étape. Mobilisation des cellules présentes dans les vaisseaux sanguins Les molécules citées ci-dessus vont aller se fixer sur les récepteurs PRR des macrophages, des mastocytes et de la cellule dendritique. Les mastocytes vont relâcher des granules d'histamine qui ont des capacités vasodilatatrices, ce qui va dilater les parois des vaisseaux sanguins et donc ralentir la vitesse de la circulation sanguine pour permettre au granulocytes de traverser la paroi vasculaire. Les macrophages eux, vont relâcher des chimiokines qui vont se fixer sur les récepteurs PRR des granulocytes et vont les attirer dans le vaisseau sanguin. Les cellules dendritiques vont capturer un microbe et migrer jusqu'aux vaisseaux lymphatiques et aux ganglions, où ils vont présenter une molécule MHC II au lymphocyte T4. Troisième étape. Mise en action des granulocytes présents dans les tissus Grâce à la dilatation des vaisseaux produite par les mastocytes et grâce aux chimiokines, les granulocytes présents dans les tissus vont traverser la paroi vasculaire. Les granulocytes neutrophiles vont phagocyter, les ganulocytes basophiles vont relâcher de l'histamine, qui va déclencher la réaction inflammatoire, et les granulocytes éosinophiles vont se lier à la paroi du parasite et libérer des enzymes qui vont causer des dommages importants à celui-ci. Première étape. Présentation la molécule MHC II au lymphocyte T4 Les cellules dendritiques c"
médecine;La thyroxine ou T4 est une hormone thyroïdienne agissant comme une prohormone devant être désiodée en triiodothyronine, ou T3, par la thyroxine 5'-désiodase pour être pleinement active. Elle est biosynthétisée chez les mammifères dans la thyroïde par iodation de la thyroglobuline sous l'action de l'iode introduit dans les cellules par la pendrine et oxydé en iode atomique par la thyroperoxydase, une enzyme dont l'expression est accrue par la thyréostimuline (TSH). Elle est inactivée par la thyroxine 5-désiodase, qui la convertit en 3,3',5'-triiodothyronine ou « T3 inverse », isomère inactif de la T3.Les hormones thyroïdiennes jouent un rôle important dans le métabolisme énergétique et agissent en relation avec d'autres hormones, telles que l'insuline, le glucagon, l'adrénaline ou encore l'hormone de croissance.La L-thyroxine (énantiomère S-(–), ou lévothyroxine) est synthétisée en laboratoire comme médicament contre l'hypothyroïdie ou comme traitement à vie en cas de thyroïdectomie. Elle est prise sous forme de comprimés à raison de 12,5 ?g à 200 ?g par jour, habituellement une demi-heure avant le petit déjeuner afin d'en maximiser l'absorption dans la mesure où elle est mal absorbée par l'intestin. Elle peut également être administrée par intraveineuse dans les cas d'hypothyroïdie sévère.Liste d'hormones Portail de la chimie   Portail de la biochimie   Portail de la médecine
médecine;La thyroïdite de Hashimoto ou thyroïdite chronique lymphocytaire est une thyroïdite chronique auto-immune particulièrement fréquente caractérisée notamment par la présence d'anticorps anti-thyroperoxydase et par une infiltration lymphoïde de la glande thyroïde. Généralement évoqué à l'examen clinique devant un goitre et une hypothyroïdie, le diagnostic de la maladie nécessite la réalisation d'examens complémentaires biologiques et morphologiques. Le traitement de la maladie fait généralement appel à une hormonothérapie substitutive.C’est en examinant des pièces de thyroïdectomie obtenues chez quatre femmes d’âge moyen dans un contexte de goitre compressif que le médecin japonais Hakaru Hashimoto (1881?1934) découvre la maladie en 1912. Il publie sa découverte avec l’article K?j?sen rinpa-setsu shush?-teki henka ni kansuru kenky? h?koku ou Zur Kenntnis der lymphomatösen Veränderung der Schilddrüse (Struma lymphomatosa) dans « Archiv für klinische Chirurgie », y décrivant alors l’infiltration lymphocytaire de la glande thyroïde.En 1957, la thyroïdite de Hashimoto devient la première maladie auto-immune spécifique d’organe à être reconnue. Initialement sous-diagnostiquée et considérée comme une maladie rare,, la thyroïdite de Hashimoto est aujourd'hui reconnue comme une des pathologies thyroïdiennes les plus fréquentes.Les estimations de l’incidence et de la prévalence de la thyroïdite de Hashimoto sont variables. Une incidence de 1/1000 a été proposée ainsi qu’une prévalence de 8/1000. La maladie est ainsi la première cause d’hypothyroïdie dans les pays où les apports en iode sont satisfaisants,.Il existe une nette prédominance féminine de la maladie de Hashimoto avec un rapport estimé entre 8 et 20 femmes pour 1 homme. Cette thyroïdite survient généralement aux alentours de l’âge de 40 ans mais peut se voir à tout âge y compris en population pédiatrique. Elle serait également plus fréquente chez les populations caucasiennes et asiatiques.Une histoire familiale de maladies de la thyroïde est fréquente (20 % des cas), en faveur d'une prédisposition génétique. Celle-ci est le plus souvent associée à des haplotypes particuliers tels que HLA-DR3, HLA-DR4 et HLA-DR5. Par ailleurs, la thyroïdite de Hashimoto pourrait être liée, avec un niveau de risque plus faible, avec le polymorphisme de gènes impliqués dans la réponse immunitaire comme le gène CTLA-4 (Cytotoxic T-lymphocyte Associated-4) (en) qui entraîne une diminution du fonctionnement des produits du gène et une régulation négative de l'activité des lymphocytes T,. Ce mécanisme est également retrouvé dans le diabète de type 1.La carence iodée serait un facteur de protection contre le risque de thyroïdite de Hashimoto, et une correction excessive en iode ,, une carence en sélénium, des maladies infectieuses et quelques médicaments ont été impliqués comme facteurs de risque chez les personnes avec un risque génétique déjà prédéterminé.L'incidence est augmentée chez les patients avec des anomalies chromosomiques, comme dans le syndrome de Turner,, le syndrome de Down (ou trisomie 21) et le syndrome de Klinefelter.Des recherches récentes suggèrent un potentiel rôle du virus HHV-6 (possiblement variant A) dans le développement ou la stimulation de la thyroïdite de Hashimoto.Le rôle du tabac est discuté. Des études indiquent un risque plus élevé chez les fumeurs, d'autres tendent à montrer un effet protecteur du tabac avec une réduction des taux sériques en auto-anticorps ainsi qu'une évolution moins fréquente vers l'hypothyroïdie. Le mécanisme physiopathologique de cet effet protecteur n'est cependant pas compris.La thyroïdite de Hashimoto est associée à la survenue d'autres maladies auto-immunes : diabète de type 1,, maladie cœliaque,, vitiligo, maladie de Biermer, insuffisance surrénale (notamment dans le cadre d'un syndrome de Schmidt) et polyarthrite rhumatoïde.La maladie de Basedow peut être associée à la thyroïdite de Hashimoto et il existe des formes de passage entre ces deux maladies. On parle ainsi de « Hashitoxicose », entité décrite pour la première fois en 1971.Sur le plan physiopathologique, les anticorps dirigés contre la thyroperoxydase et/ou la thyroglobuline causent une destruction progressive des follicules thyroïdiens de la glande thyroïde.Macroscopiquement, le goitre est symétrique, non adhérent aux éléments péri-thyroïdiens et présente une surface capsulaire discrètement bosselée.En microscopie les lésions consistent en une association de fibrose interstitielle, d'infiltration lymphoïde et de destruction épithéliale,. Le degré de fibrose est très variable. L'infiltration lymphoïde présente une organisation en follicules avec des lymphocytes B au centre et des lymphocytes T dans le cortex. Les cellules épithéliales thyroïdiennes sont également modifiées, apparaissant élargies et acidophiles (cellules de Hürthle).Ces signes sont liés à la présence du goître induit par la thyroïdite. Celui-ci est diffus et sa surface est le plus souvent régulière. Sa consistance est très particulière : ferme, « suiffée » ou « caoutchoutée ». Parfois ces changements peuvent ne pas être palpables. Le goitre peut éventuellement être responsable de signes compressifs (dysphonie, dysphagie et dyspnée). La palpation cervicale ne retrouve généralement pas d'adénomégalie. Il n'est pas mis en évidence non plus de douleur ou de signes inflammatoires locaux.Ces derniers sont liés à la dysthyroïdie avec au premier plan l'hypothyroïdie. L'hyperthyroïdie peut également être présente, en particulier au début de l'évolution de la maladie. On soulignera que les signes de dysthyroïdie peuvent être absents initialement, la fonction thyroïdienne n'étant pas nécessairement perturbée. En revanche plus la maladie évolue et plus l'hypothyroïdie devient fréquente.Parmi les signes cliniques induits par l'hypothyroïdie on retrouve notamment : constipation, bradycardie, myxœdème, anémie, règles irrégulières, asthénie, troubles de la concentration et de la mémoire, dépression, peau sèche et épaissie, perte de cheveux.La positivité à un taux élevé des anticorps anti-TPO, retrouvée dans 95 % des cas, est le meilleur signe biologique pour diagnostiquer la thyroïdite de Hashimoto. Elle survient préférentiellement chez des sujets HLA B8-DR3 (en). Le titre en anticorps est de plus associé au degré d'infiltration lymphoïde de la glande. On notera que la positivité des anticorps anti-TPO est par ailleurs très rare chez les sujets sains.En cas de négativité des anticorps anti-TPO, on peut retrouver une augmentation des anticorps anti-thyroglobuline.Il n'y a pas nécessairement, au début, de trouble de la fonction hormonale, mais la maladie évoluera toujours vers une hypothyroïdie avec des taux de T4 anormalement bas et secondairement des taux de TSH élevés.Enfin, la thyroïdite de Hashimoto n'est pas associée à la présence de marqueurs sériques de l'inflammation.L'échographie de la thyroïde montre un goitre hypoéchogène,. Le parenchyme thyroïdien devient plus hétérogène au cours de l'évolution. On peut notamment mettre en évidence des pseudo-nodules et des nodules de régénérations hyperéchogènes (white knight). Des ganglions récurrentiels peuvent être visualisés. La vascularisation est hétérogène en Doppler couleur. L'étude en Doppler pulsé retrouve une élévation des vitesses systoliques, toutefois moindre que dans la maladie de Basedow.La scintigraphie est inutile dans ce contexte d'hypothyroïdie. Lorsqu'elle est réalisée elle montre des résultats très variables ne contribuant donc pas au diagnostic.La thyroïdite de Hashimoto peut être associée à toute autre maladie auto-immune : collagénose, insuffisance surrénalienne, à un cancer de la thyroïde ou entraîner des complications cardio-vasculaires. Elle peut aussi entraîner des symptômes laissant penser à tort à un virage maniaque (manie) caractéristique d'un trouble bipolaire.La thyroïdite de Hashimoto peut se compliquer d'une encéphalopathie (encéphalopathie de Hashimoto). Cette entité a été décrite pour la première fois en 1966 et seuls une centaine de cas ont été rapportés dans la littérature depuis.Le lymphome thyroïdien complique moins de 1 % des thyroïdites auto-immunes,. Toutefois celui-ci doit être évoqué devant toute augmentation de volume du goitre ou en cas de survenue d'adénopathie.Le traitement chirurgical n'a aujourd'hui que rarement sa place dans la thyroïdite de Hashimoto. On le réservera essentiellement aux goitres compressifs.La prise en charge est avant tout médicale, consistant en une hormonothérapie thyroïdienne substitutive. Les hormones thyroïdiennes permettraient une diminution du volume du goitre tout en corrigeant l'hypothyroïdie latente ou évidente.Endocrionologie, diabète et maladies métaboliques. Collège des enseignants d'endocrinologie, diabète et maladies métaboliques p361InfoThyroAFMT : site officiel de l'Association française des malades de la thyroïde Portail de la médecine
médecine;"Un vaccin est une préparation biologique administrée à un organisme vivant afin d'y stimuler son système immunitaire et d'y développer une immunité adaptative protectrice et durable contre l'agent infectieux d'une maladie particulière. Un vaccin est administré lors d'une vaccination à un individu sain.La substance active d’un vaccin est un agent antigénique soit à pathogénicité atténuée par une forme tuée ou affaiblie du micro-organisme pathogène, ou par une de ses toxines, ou par une de ses composantes caractéristiques, par exemple une protéine d'enveloppe, soit un acide nucléique. Plusieurs types de vaccins existent selon le procédé utilisé pour obtenir des anticorps neutralisants : virus entiers atténués ou inactivés, vecteurs viraux génétiquement modifiés réplicatifs ou non réplicatifs (adénovirus, vaccine), sous-unités vaccinales obtenues par recombinaison génétique, anatoxines et acides nucléiques (ADN, ARN messager).La réaction immunitaire primaire met en mémoire l'antigène menaçant présenté pour que, lors d'une contamination ultérieure, l'immunité ainsi acquise puisse s'activer plus rapidement et plus fortement.L'Organisation mondiale de la santé (OMS) signale que des vaccins homologués sont disponibles pour plus de vingt infections différentes ainsi évitables. Les vaccins les plus connus sont ceux contre la poliomyélite, antidiphtérique, antitétanique, contre la coqueluche, la tuberculose, la rougeole, la grippe saisonnière, les fièvres hémorragiques Ebola et la Covid-19.En dépit d'un solide consensus scientifique, il existe au niveau mondial une controverse sur l'intérêt des vaccins et de la vaccination, variable selon les pays et les contextes sanitaires et sociétaux.Le mot « vaccin » dérive du mot « vaccine » (lui-même issu du latin vaccinus qui signifie « de vache »), utilisé par Edward Jenner en 1798 pour désigner sa formulation médicale recueillie à partir des pustules de variole présentes sur le pis des vaches (variole de la vache, appelée en français vaccine, bénigne) puis inoculée aux humains pour les préserver de la variole humaine. Cette formulation constitue ainsi le premier de tous les vaccins.En 1881, pour honorer Jenner, Louis Pasteur proposa que les termes « vaccin » et « vaccination » soient étendus pour couvrir les nouvelles inoculations protectrices alors en cours de développement.Avant l'introduction de la vaccination avec des éléments provenant de cas de variole de la vache, la variole pouvait être prévenue par l'inoculation délibérée du virus de la variole, pratique appelée plus tard variolisation pour la distinguer de la vaccination antivariolique. La pratique de l'inoculation de la variole a ses premiers indices au Xe siècle en Chine et la plus ancienne utilisation documentée de 1549, également chinoise. Les Chinois ont alors mis en œuvre une méthode d'« insufflation nasale » administrée en soufflant du matériel de variole en poudre, généralement des croûtes, dans les narines. Diverses techniques d'insufflation ont été enregistrées au cours des XVIe et XVIIe siècles en Chine. Deux rapports sur la pratique chinoise de cette inoculation ont été reçus par la Royal Society de Londres en 1700.À la fin des années 1760, alors qu'il étudiait son futur métier de chirurgien / apothicaire, Edward Jenner connut l'histoire, courante dans les zones rurales, que les travailleurs laitiers n'avaient jamais la variole humaine, souvent fatale ou défigurante, parce qu'ils avaient déjà contracté la variole de la vache (la vaccine) qui était beaucoup moins violente chez l'homme. En 1796, Jenner a pris du pus de la main d'une laitière ayant la vaccine, l'a gratté dans le bras d'un garçon de 8 ans, James Phipps. Six semaines plus tard, il lui a inoculé la variole humaine : celui-ci ne l'a pas développé. Jenner a rapporté en 1798 que l'inoculation de son produit était sans risque, chez les enfants comme chez les adultes, cette vaccination étant beaucoup plus sûre que l'inoculation de la variole humaine. Cette dernière pratique pourtant alors usuelle, a ensuite été interdite en Angleterre en 1840.La seconde génération de vaccins a été introduite dans les années 1880 par Louis Pasteur qui a développé des vaccins contre le choléra et contre l'anthrax du poulet. À partir de la fin du XIXe siècle, les vaccins étaient considérés comme une question de prestige national et des lois sur la vaccination obligatoire ont été adoptées.Le XXe siècle a vu l'introduction de plusieurs vaccins efficaces, notamment ceux contre la diphtérie, la rougeole, les oreillons, le tétanos et la rubéole. Les principales réalisations comprennent le développement du vaccin contre la poliomyélite dans les années 1950. Grâce au vaccin l'éradication mondiale de la variole humaine a été obtenue dans les années 1970. Maurice Hilleman a été le plus prolifique des développeurs de vaccins au XXe siècle. Comme les vaccins sont devenus plus courants, de nombreuses personnes ont commencé à les tenir pour acquis. Cependant, les vaccins restent insaisissables pour de nombreuses maladies importantes, notamment l'herpès simplex, le paludisme, la gonorrhée et le VIH.Un vaccin est une préparation administrée pour provoquer l’immunité protectrice et durable de l'organisme contre une maladie en stimulant la production d’anticorps. Un ou plusieurs antigènes microbiens doivent être utilisés pour induire une nette amélioration de cette immunité.Le but principal des vaccins est d'obtenir, par l'organisme lui-même, la production d'anticorps et l'activation de cellules T (lymphocyte B ou lymphocyte T à mémoire) spécifiques à l'antigène. Une immunisation réussie doit donc procurer une protection contre une future infection d'éléments pathogènes identifiés. Un vaccin est donc spécifique à une maladie mais pas à une autre.Outre le vaccin actif lui-même, les excipients et les composés de fabrication résiduels suivants peuvent être présents ou ajoutés dans certaines préparations vaccinales :des adjuvants, pour stimuler la réponse immunitaire (plus précoce, plus puissante, plus persistante) au vaccin, de façon non spécifique, du système immunitaire inné ; ils permettent de réduire la dose de vaccin, comme :des sels ou gels d'aluminium,une association aluminium-lipides (AS04),des émulsions eau-squalène,des virus végétaux ; début mai 2008, Denis Leclerc a proposé d'utiliser un virus végétal (qui ne peut se reproduire chez l'homme) comme pseudovirion jouant le rôle d'adjuvant, pour rendre des vaccins plus longuement efficaces contre des virus qui mutent souvent (virus de la grippe ou de l'hépatite C, voire contre certains cancers). Le principe est d'associer à ce pseudovirion une protéine-cible interne aux virus, bactéries ou cellules cancéreuses à attaquer, et non comme on le fait jusqu'ici une des protéines externes qui sont celles qui mutent le plus. Ce nouveau type de vaccin, qui doit encore faire les preuves de son innocuité et de son efficacité, déclencherait une réaction immunitaire à l'intérieur des cellules, au moment de la réplication virale,des produits antimicrobiens :des antibiotiques, pour empêcher la croissance de bactéries pendant la production et le stockage du vaccin,le formaldéhyde, utilisé pour inactiver les produits bactériens des vaccins anatoxines ; Le formaldéhyde est également utilisé pour inactiver les virus indésirables et tuer les bactéries qui pourraient contaminer le vaccin pendant la production,le thiomersal, antimicrobien controversé contenant du mercure qui est ajouté aux flacons de vaccin qui contiennent plus d'une dose pour empêcher la contamination et la croissance de bactéries potentiellement nocives,le glutamate monosodique (MSG) et le 2-phénoxyéthanol sont utilisés comme stabilisants dans quelques vaccins pour aider le vaccin à rester inchangé lorsque le vaccin est exposé à la chaleur, à la lumière, à l'acidité ou à l'humidité ;la protéine d’œuf, présente dans les vaccins contre la grippe et contre la fièvre jaune car ils sont préparés à partir d’œufs de poule ; d'autres protéines peuvent parfois être présentes.L'immunogénicité (ou efficacité sérologique) est la capacité d'un vaccin à induire des anticorps spécifiques. Les anticorps sont produits par des lymphocytes B se transformant en plasmocytes. Le temps nécessaire à l'induction d'anticorps est de 2 à 3 semaines après la vaccination. Cette production d'anticorps diminue progressivement après plusieurs mois ou années. Elle est mesurable et cette mesure peut être utilisée dans certains cas pour savoir si le sujet est vacciné efficacement (vaccin anti-hépatite B et anti-tétanos en particulier).Le nombre de lymphocytes B mémoire, non sécrétant, mais qui réagissent spécifiquement à la présentation d'un antigène, semble, lui, ne pas varier au cours du temps. Ce qui permet d'induire une protection de longue durée, jusqu'à des décennies (ou tant que le sujet reste immunocompétent), car la réactivation de l'immunité mémoire lors d'une nouvelle infection s'effectue alors en quelques jours.Cependant, certains vaccins ne provoquent pas la formation d'anticorps mais mettent en jeu une réaction de protection d'immunité cellulaire, c'est le cas du BCG (« vaccin Bilié de Calmette et Guérin », vaccin anti-tuberculeux).Selon certaines études, notamment sur la grippe A (H1N1), la vaccination d'un individu ne le rend pas non contagieux pour autantL'efficacité clinique d'un vaccin se mesure par la réduction de la fréquence de la maladie chez les sujets vaccinés (taux de protection effectif de la population vaccinée). Elle est parfois estimée par des marqueurs de substitution (taux d'anticorps connus protecteurs), mais l'efficacité sérologique (mesurée en laboratoire) ne concorde pas toujours avec l'efficacité clinique (mesurée en épidémiologie de terrain).Le suivi et la surveillance d'une politique vaccinale s'effectuent par l'épidémiologie des maladies vaccinables (surveillance par des réseaux de laboratoire hospitaliers, centres de référence, réseaux sentinelles, notification systématique ou obligatoire…), la surveillance des effets indésirables (pharmacovigilance, registres de suivi…) et par des études séro-épidémiologiques (séroprévalence). Ces études permettent d'évaluer l'immunité collective des populations, dont la situation et la localisation des sujets non-vaccinés, réceptifs ou vulnérables.Selon le type de vaccin, et l'état de santé du sujet, les vaccins peuvent être contre-indiqués ou fortement recommandés.Lors d'une vaccination, les effets indésirables pouvant être reliés au vaccin administré dépendent d'abord de l'agent infectieux combattu, du type de vaccin (agent atténué, inactivé, sous-unités d'agent, etc.), de ses excipients (nature du solvant, adjuvants, conservateurs chimiques antibactériens, etc.) utilisés.Suivant les vaccins, certains effets indésirables, en général bénins, se retrouvent de manière plus ou moins fréquente. L'une des manifestations les plus courantes est la fièvre et une inflammation locale qui traduisent le déclenchement de la réponse immunitaire recherchée par la vaccination. Dans de très rares cas, la vaccination peut entraîner des effets indésirables sérieux et, exceptionnellement, fatals. Un choc anaphylactique, extrêmement rare, peut par exemple s'observer chez des personnes susceptibles avec certains vaccins (incidence de 0,65 par million, voire 10 par million pour le vaccin rougeole-rubéole-oreillons (RRO)). En France, la loi prévoit le remboursement des dommages et intérêts par l'Office national d'indemnisation des accidents médicaux lorsqu'il s'agit de vaccins obligatoires. Liés à l'hydroxyde d'aluminium La myofasciite à macrophages a été associée à la persistance pathologique de l'hydroxyde d'aluminium utilisé dans certains vaccins. Cependant lors de sa réunion de décembre 2003, le Comité consultatif mondial sur la sécurité des vaccins, après avoir examiné les données d’une étude cas témoins réalisée en France, a conclu, en accord avec ses précédentes déclarations, que la persistance de macrophages contenant de l’aluminium au site d’injection d’une vaccination antérieure n’est associée ni à des symptômes cliniques ni à une maladie spécifique. C'est aussi la conclusion à laquelle est parvenue l'agence française de sécurité du médicament, qui ne voit dans la myofasciite à macrophages qu'un phénomène histologique auquel aucun syndrome clinique spécifique ne peut être associé.Un sérum ne doit pas être confondu avec un vaccin. Mais ils peuvent parfois être associés lors de l'injection : c'est la sérovaccination.Par abus de langage, le terme de vaccination s'applique parfois à diverses inoculations et injections. Ainsi l'immunocastration des porcs est souvent présentée comme un vaccin (contre l'odeur de verrat). En 1837, Gabriel Victor Lafargue parla de « vaccination morphinique » pour ce qui n'était qu'une injection sous-épidermique. Dans cette catégorie se place également le vaccin de Coley (qui génère une hyperthermie destinée à détruire des tumeurs).Les vaccins sont souvent classés en deux grandes catégories selon qu'ils sont issus ou non d'agents infectieux. La première catégorie se répartit entre vaccins vivants atténués et vaccins inactivés. La seconde catégorie se répartit principalement entre vaccins conjugués, à anatoxine, à sous unité protéique, à ARN / à ADN.On les trouve aussi classés selon le type d'agent infectieux traité : vaccins bactériens / vaccins viraux.Les abréviations des noms de vaccins se sont relativement harmonisées au niveau mondial mais il n'y a pas encore au début de l'année 2020 de normalisation mondialement partagée. L'OMS, en collaboration avec l'institut norvégien de la santé publique, en propose une nomenclature. Les États-Unis utilisent une autre liste,.Plus de vingt vaccins font partie de la liste des médicaments essentiels de l'OMS,, dans la classe ATC J07, pour les adultes comme pour les enfants. Vivant atténué Les agents infectieux sont multipliés en laboratoire jusqu’à ce qu’ils perdent naturellement ou artificiellement, par mutation, leur caractère pathogène. Les souches obtenues ont perdu leur virulence (rendues incapables de développer la maladie), mais elles restent vivantes avec une capacité transitoire à se répliquer chez l'hôte. Ils créent donc une infection a minima.Ce genre de vaccin stimule l'immunité spécifique de façon généralement plus efficace et plus durable que celui composé d’agents infectieux inactivés. Ils peuvent parfois induire après vaccination des réactions locales ou générales qui sont des symptômes mineurs de la maladie qu'ils préviennent. Du fait de ce risque infectieux potentiel, ils sont contre-indiqués en principe chez la femme enceinte et les personnes immunodéprimées.Les vaccins vivants ne contiennent pas d'adjuvants : ils n'en ont pas besoin.Les principaux vaccins vivants disponibles sont le vaccin BCG (tuberculose)  le ROR (Rougeole, Oreillons, Rubéole), le vaccin contre la  varicelle, contre le zona, contre la fièvre jaune, le vaccin oral contre la poliomyélite, contre les gastro-entérites à rotavirus. Inactivé Les agents infectieux, une fois identifiés et isolés, sont multipliés en très grand nombre puis altérés, chimiquement ou par la chaleur. Ils conservent néanmoins une certaine capacité immunogène (aptitude à provoquer une protection immunitaire) moins ciblée. C'est pourquoi ils nécessitent l'ajout d'adjuvant et font souvent l'objet de plus de rappels de vaccination.Un vaccin inactivé peut être :entier ou complet, lorsqu'il est composé du micro-organisme complet mais tué ou inactivé ; par exemple, le vaccin « cellulaire » contre la coqueluche. Ces vaccins sont très efficaces mais plus « réactogènes » avec un risque plus élevé de réactions indésirables ;sous-unitaire ou « à sous-unités » lorsqu'il est composé d'une fraction du micro-organisme inactivé. Il peut être obtenu de façon classique ou à partir de biotechnologie ou de biologie de synthèse ; par exemple, le vaccin « acellulaire » contre la coqueluche. Cette fraction peut-être un peptide de surface du germe, un polysaccharide de paroi bactérienne, une anatoxine, une particule pseudo-virale ou tout autre composant immunogène du micro-organisme. Ces vaccins sont moins immunogènes, mais avec moins d'effets secondaires.Les vaccins polyosidiques ou à polysaccharides activent les seuls lymphocytes B. Ils sont inefficaces avant l'âge de deux ans. Par exemple, le vaccin polysaccharidique contre le pneumocoque. Ils ont une faible réponse mémoire et nécessitent plus de rappels.voir aussi Vaccin à ARN, Vaccin à ADN Conjugués Les vaccins conjugués se basent sur la liaison d'un polysaccharide (antigène capsulaire) avec une protéine porteuse. Cette conjugaison permet d'induire une bonne réponse mémoire et d'activer les lymphocytes T, ce qui les rend utilisables chez l'enfant de moins de deux ans. Le premier de ce type a été le vaccin contre Haemophilus influenza b ou Hib, agent de méningite purulente du nourrisson. D'autres vaccins conjugés de ce type sont le vaccin contre le méningocoque, le vaccin contre le pneumocoque. Anatoxine Un vaccin anatoxine est produit par inactivation physique ou chimique de la molécule initialement toxique qui cause la maladie et qui est produite par l'agent infectieux. La molécule ainsi inactivée perd ses propriétés toxiques mais conserve sa structure et des propriétés immunisantes. Les vaccins anatoxines sont connus pour leur efficacité.Les vaccins antitétaniques et antidiphtériques sont des vaccins à base d'anatoxines. Sous-unité protéique Une sous-unité protéique, un fragment protéique (ou un assemblage de plusieurs fragments) de la surface du micro-organisme pathogène, peut créer une réponse immunitaire.Le vaccin contre l'hépatite B, celui contre la grippe saisonnière, ceux contre le papillomavirus humain (HPV), sont des vaccins à sous-unité protéique. Issus du génie génétique (biologie de synthèse) Certaines de ces molécules peuvent être obtenues par génie génétique, et ainsi être produites en grande quantité. La stratégies de développement la plus connue consiste à insérer des gènes microbiens dans des Escherichia Coli, des levures ou des cellules animales en culture, de façon à leur faire produire des protéines microbiennes spécifiques, par exemple l'antigène de surface de l'hépatite B, qui est ensuite utilisée dans le vaccin contre l'hépatite B.D'autres stratégies sont la production de pseudoparticules virales, dépourvues d'ADN viral et incapables de se répliquer (vaccin contre les papillomavirus) ; la recombinaison génétique permet des virus atténués « réassortants » (vaccin contre la grippe, vaccin contre les rotavirus).Il existe également une recherche sur des vaccins oraux basés sur des plantes (production d'antigènes par des algues). Hétérologue Un vaccin hétérologue (ou hétérotypique) est fabriqué à partir d’un micro-organisme différent de celui de la maladie à combattre mais ayant des similitudes immunologiques suffisantes pour induire une protection croisée de qualité acceptable.L'exemple classique est l'utilisation par Jenner du virus de la vaccine (variole de la vache) pour protéger l'humain contre la variole. Un exemple actuel est l'utilisation du vaccin BCG préparé à partir d'une souche atténuée de bacille tuberculeux bovin (Mycobacterium bovis) pour protéger contre la tuberculose humaine.L'autovaccin (autogenous vaccine) est un vaccin élaboré à partir d'une souche spécifique de micro-organismes prélevée sur le malade lui-même. Ils connaissent un développement important en médecine vétérinaire dans les élevages de porc notamment,. « On entend par autovaccin à usage vétérinaire, tout médicament vétérinaire immunologique fabriqué en vue de provoquer une immunité active à partir d’organismes pathogènes provenant d’un animal ou d’animaux d’un même élevage, inactivés et utilisés pour le traitement de cet animal ou des animaux de cet élevage » (article L 5141-2 du Code de la santé publique).Les vaccins multivalents ou combinés, associent des combinaisons d'antigènes, permettant de cibler plusieurs maladies différentes en un seul vaccin (par exemple Rougeole-Oreillons-Rubéole ou Diphtérie-Tétanos-Poliomyélite-Coqueluche-Hib-Hépatite B). Ces vaccins permettent de diminuer le nombre d'injections et d'augmenter la couverture vaccinale.Plusieurs vaccins sont en cours de développement :Les vaccins combinant des cellules dendritiques avec des antigènes afin de présenter ceux-ci aux globules blancs du corps, stimulant ainsi une réaction immunitaire. Ces vaccins ont montré des résultats préliminaires positifs pour le traitement des tumeurs cérébrales et sont également testés dans le mélanome malin [réf. nécessaire].Vaccin à ADN : Le mécanisme proposé est l'insertion (et l'expression, renforcée par l'utilisation de l'électroporation, déclenchant la reconnaissance du système immunitaire) d'ADN infectieux, viral ou bactérien, dans des cellules humaines ou animales. Certaines cellules du système immunitaire qui reconnaissent les protéines exprimées monteront alors une attaque contre ces protéines et les cellules qui les expriment. Parce que ces cellules vivent très longtemps, si le pathogène qui exprime normalement ces protéines est rencontré plus tard, elles seront attaquées instantanément par le système immunitaire[réf. nécessaire]. Ces vaccins sont très faciles à produire et à stocker[réf. nécessaire]. Plusieurs vaccins à ADN sont disponibles en 2019 pour un usage vétérinaire mais aucun vaccin à ADN n'a encore été approuvé pour un usage chez l'homme. Comme le vaccin à ARN, le vaccin à ADN est parfois dit « vaccin génétique », car il introduit dans l'organisme un élément génétique du virus.Vecteur recombinant : en combinant la physiologie d'un micro-organisme et l'ADN d'un autre, l'immunité peut être créée contre les maladies qui ont des processus d'infection complexes[réf. nécessaire]. Le vaccin contre le virus Ebola en est un exemple.Le vaccin à ARN est composé d'un ARN messager conditionné dans un vecteur tel que des nanoparticules lipidiques. Ce type de vaccin a montré son efficacité pour lutter contre la maladie à coronavirus 2019(SARS-Cov-2) , voir Vaccin contre la Covid-19.Des vaccins peptidiques des récepteurs des lymphocytes T qui modulent la production de cytokines et améliorent l'immunité à médiation cellulaire[réf. nécessaire]Des vaccins utilisant le ciblage de protéines bactériennes impliquées dans l'inhibition du complément et qui neutraliseraient le mécanisme clé de virulence bactérienne[réf. nécessaire]. Vétérinaires La vaccination des animaux est utilisée à la fois pour prévenir certaines de leurs maladies infectieuses et pour prévenir la transmission de maladies aux humains. Les animaux de compagnie et les animaux élevés comme bétail sont quasi-systématiquement vaccinés.En cas de propagation de la rage, la vaccination antirabique des chiens peut être exigée par la loi. Des populations sauvages (renard, raton laveur) peuvent également être alors vaccinées.En plus de celui de la rage, les principaux vaccins canins sont celui contre la maladie de Carré, le parvovirus canin, l'hépatite canine infectieuse, l'adénovirus-CAV2, la leptospirose, la bordatella, la toux de chenil et la maladie de Lyme.Les vaccins DIVA (pour Differentiation of Infected from Vaccinated Animals), également connus sous le nom de SIVA (pour Segregation of Infected from Vaccinated Animals), permettent de différencier les animaux infectés des animaux vaccinés. Pour les végétaux Les vaccins actuels sont essentiellement faits pour les humains et d'autres animaux (vaccins vétérinaires) mais on sait maintenant que les plantes ont aussi un système immunitaire, et qu'il est possible de les vacciner. Un premier vaccin commercialisé pour les plantes a été créé en 2001 par la société Goëmar.Grâce à des tests moléculaires permettant d'identifier les siRNA efficaces contre le virus de la tomate (Tomato bushy stunt virus ou TBSV, de la famille des Tombusviridae), un vaccin a pu être produit et, en laboratoire, a donné les résultats espérés ; il peut en outre être pulvérisé sur les feuilles (pas besoin d'injection). Un projet est de faire un vaccin contre le virus de la mosaïque du concombre (capable de détruire des champs entiers de concombres, citrouilles ou  melons). La méthode est aussi plus simple et plus rapide que de concevoir une plante OGM résistant au virus. Selon une autre étude publiée sur un magazine spécialisé, des chercheurs ont testé leurs molécules sur des plantes, par spray, et 90% d’entre elles n’ont pas été infectées par le virus.Le développement complet est un processus le plus souvent très long, qui se compte habituellement en années, avec plusieurs étapes successives : une phase préclinique (hors expérimentation humaine), trois phases cliniques (avec expérimentation humaine), une phase d'autorisation administrative, une phase de production industrielle, une phase de vaccination et une phase finale de pharmacovigilance.Autrefois, ce processus débutait par des expérimentations animales qui se sont révélées décevantes pour prédire l'efficacité d'un vaccin. Actuellement[Quand ?], on débute beaucoup plus tôt l'expérimentation humaine : c'est ce qu'on appelle la médecine expérimentale ou translationnelle. Ce développement doit respecter les différentes phases d'un essai clinique de vaccin.Les chercheurs doivent d'abord choisir une voie d'administration : voie nasale, orale ou par injection. Ce choix peut dépendre du vecteur choisi, de l’antigène, de l'adjuvant ou d'autres paramètres. Si la voie par injection est choisie, il faut aussi choisir quelle injection : intradermique, sous-cutanée ou intramusculaire.Les phases 1 et 2 permettent d'établir l'innocuité du projet de vaccin. La phase 3, plus étendue, permet de tester son efficacité. Celle-ci se mesure uniquement vis-à-vis de la prévention de la maladie ou de l'infection, que le vaccin projeté est censé empêcher. Plusieurs moyens permettent d'évaluer cette efficacité :Surtout avec un essai d'efficacité contrôlé et aléatoire comprenant des critères cliniques adaptés. Son but doit démontrer la diminution de l'infection ou de la maladie après immunisation par rapport au groupe de référence non immunisé.On peut faire aussi une évaluation de l'efficacité par observation, toujours avec des critères cliniques, pour évaluer l'effet protecteur d'un vaccin dans des conditions réelles au sein d'une population ouverte.Dans certains cas, on peut se contenter de critères immunologiques comme par exemple un titrage des anticorps. Phase I Dans cette phase, on vérifie d'abord l'innocuité du produit avant de s'intéresser à son efficacité. Généralement on teste alors  le produit candidat-vaccin à dose croissante sur des petits groupes (rarement plus de 100 volontaires). Le nombre de doses peut varier en fonction du type de vaccin. Les effets secondaires sont soigneusement répertoriés. Mais à ce stade, certains effets secondaires graves comme la réaction anaphylactique sont rarement détectés en raison du très petit nombre de participants.Le protocole d'étude doit établir les effets secondaires spécifiquement au vaccin et les quantifier (injection peu douloureuse ou très douloureuse).Les chercheurs s’intéressent bien entendu à la réponse immunologique (par exemple, le dosage des anticorps). Mais ce dosage n'est pas forcément synonyme d'efficacité du vaccin. On parle alors d'immunogénicité du vaccin. On propose enfin un ""meilleur"" dosage du vaccin. Phase II Si la phase I est concluante (pas d'effets secondaires graves plus réponse immunitaire satisfaisante), on peut entamer la phase II, où l'on commence d'abord par augmenter la taille du groupe étudié : même protocole que la phase I mais plus de participants (phase II a) sur plusieurs centaines à quelques milliers de volontaires.On teste alors l'efficacité de la réponse immunitaire ainsi que la tolérance du projet de vaccin. On identifie largement les effets secondaires constatés. On cherche aussi à déterminer la posologie adaptée (quantité de produit, nombre de doses) et un premier calendrier vaccinal (durée entre vaccinations).Beaucoup de candidat-vaccins ne passent pas cette phase : ils ont une réponse immunitaire satisfaisante mais celle-ci n'est pas efficace ou suffisante pour empêcher la maladie ou leurs effets secondaires sont jugés trop graves. Phase III Si la phase II est satisfaisante le projet de vaccin peut passer en phase III. Les tests de sécurité et d'efficacité continuent avec une population beaucoup plus grande (de l'ordre de plusieurs dizaines de milliers de volontaires) et hétérogène (sexe, classes d'âge, diversité génétique, etc). Il s'y ajoute les études d'homogénéité d'un lot de vaccin à un autre qui consistent à vérifier l'homogénéité de la fabrication de plusieurs lots cliniques d'un point de vue clinique.Enfin, des études d'administrations simultanées vérifient l'absence d'interférence significative lorsqu'il est administré concomitamment à un vaccin déjà homologué et inclus dans les programmes courants de vaccination.Malgré la plus grande taille des groupes étudiés, les effets secondaires très rares ne seront pas forcément tous connus au cours de la phase III : les essais cliniques de sécurité en phase III sont normalement conçus pour observer les effets indésirables jusqu'à un taux de 1 pour 10 000.Cette phase est la plus longue et la plus coûteuse : entre 2 et 13 ans et environ 750 millions d'euros.Cette phase va définir le ratio risques/bénéfices qui est obligatoire pour l'enregistrement et l'autorisation de chaque vaccin.Alors que le vaccin est commercialisé et que la vaccination est en cours, cette dernière phase, souvent dénommée phase IV, est une étude de pharmacovigilance consistant notamment en une surveillance de la sécurité et des effets secondaires du vaccin sur une population beaucoup plus large. Ceci est effectué en détectant des possibles manifestations post-vaccinales indésirables (MAPI), en les analysant médicalement, en évaluant la causalité des effets observés vis à vis du vaccin et en restituant les résultats obtenus aux autorités.Cette phase peut remettre en cause la phase d'autorisation administrative.Le marché mondial des vaccins s'apparente à un oligopole. Quatre producteurs principaux se partageant l'essentiel du marché, même s'il existe un grand nombre d'autres demandeurs. Suivant les sources, la répartition en part de marché s’établit ainsi :2016 :1 Merck : 19 %, 2 Sanofi : 18 %, 3 GlaxoSmithKline : 16 % et 4 Pfizer : 13 % ;2016 : 1 Merck & Co : 24,5 %, 2 GlaxoSmithKline : 22,6 %, 3 Pfizer : 22 %, 4 Sanofi : 20,2 %.De nombreuses barrières à l'entrée sont présentes, cela signifie que les entreprises candidates à l’entrée sur ce marché ont un coût de production supérieur à celui des entreprises en place. Effectivement, des gros investissements sont nécessaires quant à la construction d'un laboratoire et à la recherche de nouveaux vaccins.Les efforts d'investissement dans ce milieu sont principalement concentrés en Europe et en Amérique du Nord. Plus de 50 % des investissements en recherche et développement ont été menés en Europe entre 2002 et 2010.Le dépôt de brevets sur les processus de développement de vaccins peut parfois être considéré comme un obstacle au développement de nouveaux vaccins. En raison de la faible protection offerte par un brevet sur le produit final, la protection de l'innovation concernant les vaccins se fait souvent par le biais du brevet de certains procédés alors utilisés ainsi que la protection du secret pour d'autres procédés[réf. nécessaire].Selon l'Organisation mondiale de la santé, le plus grand obstacle à la production locale de vaccins dans les pays moins développés n'a pas été les brevets, mais les exigences substantielles en matière financière, d'infrastructure et de main-d'œuvre nécessaires à l'entrée sur le marché. Les vaccins sont des mélanges complexes de composés biologiques et, contrairement au cas des médicaments, il n'existe pas de vrais vaccins génériques. Le vaccin produit par une nouvelle installation doit subir des tests cliniques complets de sécurité et d'efficacité similaires à ceux subis par celui produit par le fabricant d'origine. Pour la plupart des vaccins, des procédés spécifiques ont été brevetés. Ceux-ci peuvent être contournés par des méthodes de fabrication alternatives, mais cela nécessitait une infrastructure de recherche et développement et une main-d'œuvre qualifiée. Dans le cas de quelques vaccins relativement nouveaux comme le vaccin contre le papillomavirus humain, les brevets peuvent imposer une barrière supplémentaire[réf. nécessaire].Certaines plantes transgéniques ont été identifiées comme des systèmes d'expression prometteurs pour la production de vaccins. Des plantes complexes telles que le tabac, la pomme de terre, la tomate et la b"
médecine;"En médecine, l’étiologie (ou étiopathogénie) est l'étude des causes et des facteurs d'une maladie. Ce terme est aussi utilisé dans le domaine de la psychiatrie et de la psychologie pour l'étude des causes des maladies mentales. L'étiologie définit l'origine d'une maladie en fonction de signes ou symptômes, c'est-à-dire en jargon de ses manifestations sémiologiques.En littérature, on parle de récit ou conte étiologique lorsqu'une histoire, orale ou écrite, a pour but de donner une explication imagée à un phénomène ou une situation dont on ne maîtrise pas l'origine. En philosophie, l’étiologie est l'étude de l'ensemble des causes d'un phénomène.L’étiologie est l’étude de la causalité ou de l’origine. Le mot est dérivé du grec ?????????? (aitiología) « donnant une raison pour » (?????, aitía, « cause »; et -??????, -logía). Ainsi, l’étiologie est l’étude des causes, des origines, des raisons pour lesquelles les choses sont telles qu'elles sont ou encore la façon dont elles fonctionnent; l'étiologie peut également se référer aux causes elles-mêmes. Le mot est couramment utilisé en médecine[réf. à confirmer] (concernant les causes et les facteurs de la maladie) et en philosophie, mais aussi en physique, psychologie (pour l'étude des causes des maladies mentales), géographie, analyse spatiale, théologie et biologie, en référence aux causes ou aux origines de divers phénomènes.L'étiologie (du grec ancien : ??????????, « recherche, exposition des causes ») concerne une école philosophique de l'Antiquité s'intéressant à l'étude des causes.Le sens étiologique est l'un des trois types de sens littéral identifiés par Thomas d'Aquin, lorsqu’un énoncé a été dit en fonction d’une condition particulière d’énonciation .En médecine, l’étiologie d’une maladie ou d’une condition se réfère aux études fréquentes pour déterminer un ou plusieurs facteurs qui se réunissent pour causer la maladie[source insuffisante]. De même, lorsque la maladie est répandue, les études épidémiologiques étudient quels facteurs associés, tels que l’emplacement, le sexe, l’exposition aux produits chimiques, et bien d’autres, rendent une population plus ou moins susceptible d’avoir une maladie, une condition ou une maladie, aidant ainsi à déterminer son étiologie.Parfois, la détermination de l’étiologie est un processus imprécis. Dans le passé, l’étiologie de la maladie marine dorénavant connue, le scorbut[réf. à confirmer], était longtemps inconnue. Lorsque de grands navires furent construits, les marins commencèrent à prendre la mer pendant de longues périodes et manquaient souvent de fruits et légumes frais. Sans connaître la cause précise, le capitaine James Cook soupçonnait le scorbut d’être causé par le manque de légumes dans l’alimentation.Intrigué et basé sur ses soupçons, il a forcé son équipage à manger de la choucroute tous les jours. Il y a eu des résultats positifs et il en a donc déduit qu'en mangeant cela, il empêchait le scorbut, même s’il ne savait pas exactement pourquoi. Il a fallu encore environ deux cents ans pour découvrir l’étiologie précise: le manque de vitamine C dans l’alimentation d’un marin.Conditions héréditaires : l'hémophilie, un trouble qui entraîne des saignements excessifsTroubles métaboliques et endocriniens ou hormonaux : ce sont des anomalies dans la signalisation chimique et l’interaction dans le corps. Par exemple, le diabète sucré ou mellitus est une maladie endocrinienne qui cause une glycémie élevée.Troubles néoplastiques ou cancer : les cellules du corps deviennent incontrôlées.Problèmes d’immunité : tels que les allergies, qui sont une réaction excessive du système immunitaire.Un mythe étiologique, ou mythe d’origine, est un mythe destiné à expliquer les origines des pratiques culturelles, des phénomènes naturels, des noms propres etc.Dans le passé, lorsque de nombreux phénomènes physiques n’étaient pas bien compris ou bien lorsque les histoires n’étaient pas notés, des mythes survenaient souvent pour fournir des étiologies.Ainsi, un mythe étiologique[réf. à confirmer], ou mythe d’origine, est un mythe qui a surgi, a été raconté au fil du temps ou a été écrit pour expliquer les origines de divers phénomènes sociaux ou naturels. Par exemple, l’Énéide de Virgile est un mythe national écrit pour expliquer et glorifier les origines de l’Empire romain.En théologie, de nombreuses religions ont des mythes de création expliquant les origines du monde ou sa relation avec les croyants.On parle de conte étiologique lorsqu'une histoire a pour but de donner une explication imagée à un phénomène ou une situation dont on ne maîtrise pas l'origine scientifiquement. Exemple : Pourquoi les chiens n'aiment-ils pas les chats ? Ce type de littérature est très ancien et attesté dans la tradition orale. Les premiers récits écrits sont souvent basés sur ces traditions. Les Métamorphoses, d'Ovide (43 av. J.-C.) en sont un exemple déjà tardif et érudit, inspiré des Heteroeumena de Nicandre de Colophon (IIe siècle av. J.-C.). Dans Histoires comme ça (Just so Stories), Rudyard Kipling renoue de façon fantaisiste avec la tradition étiologique, qui sert de ressort à chacun de ses contes : pourquoi les éléphants ont-ils une trompe ? (L'Enfant d'éléphant) ; comment est née l'écriture ? (Comment naquit la première lettre ?).Sémiologie médicaleTableau cliniqueRessources relatives à la santé : (en + es) MedlinePlus (cs + sk) WikiSkripta Cours d'étiologie de l'Université catholique de Louvain Portail de la médecine"
médecine;"La fièvre est l'état d'un animal, à sang chaud (endotherme) ou à sang froid (ectotherme), dont la température interne est nettement supérieure (hyperthermie) à sa température ordinaire, de façon contrôlée.Chez les endothermes (essentiellement les mammifères et les oiseaux), ce phénomène physiologique semble être principalement une réponse hypothalamique stimulée par des substances pyrogènes principalement libérées par les macrophages et/ou lors des phénomènes inflammatoires.Chez l'humain, la fièvre accroît les défenses par plusieurs voies complémentaires : elle stimule l'immunité spécifique et non spécifique et la microbiostase (inhibition de la croissance) en diminuant le fer disponible pour les micro-organismes pathogènes afin de diminuer leur virulence. Le phénomène se déroule suivant trois phases :montée thermique ;plateau d’hyperthermie ;défervescence.La température corporelle normale moyenne des humains est de 37 °C (entre 36,5 et 37,5 °C selon les individus et le rythme nycthéméral). La fièvre est définie par une température rectale au repos supérieure ou égale à 38,0 °C. S'il n'existe pas de consensus concernant un seuil à partir duquel la fièvre elle-même serait dangereuse, certains auteurs estiment en se basant sur des données animales que le système nerveux central pourraît présenter des signes de souffrance à partir de 41,5 °C. Lorsque la fièvre est modérée, entre 37,7 et 37,9 °C, on l'appelle fébricule.Chez certains ectothermes, la fièvre s'obtient en se déplaçant dans des zones plus chaudes ; cette fièvre est qualifiée de comportementale.Il n'existe pas de définition précise universellement admise de la fièvre notamment du fait de difficultés concrètes de mesure en situation clinique (la température mesurée dépend du moment de la journée, de la proximité d'un repas, de caractéristiques environnementales). Cependant, le Brighton Collaboration Fever Working Group s'accorde à la définir en 2004 comme relevant d'une température corporelle supérieure ou égale à 38 °C, et ce quelles que soient les modalités de mesure, l'âge ou les conditions environnementales. L'OMS de son côté, considère comme fiévreuse une température axillaire supérieure ou égale à 37,5 °C,,.La température corporelle se mesure à l'aide d'un thermomètre médical. Suivant le placement de celui-ci, on parle de :température buccale : thermomètre placé dans la bouche (la méthode la plus courante dans les pays anglo-saxons, sauf pour les jeunes enfants) ;température rectale : bout du thermomètre placé dans le rectum via l'anus (la méthode la plus précise, traditionnellement conseillée pour les jeunes enfants) ;température axillaire : sous le bras. Cette mesure est moins précise que la mesure rectale. Selon les sources, la température axillaire est entre 0,5 °C, et 0,9 °C de moins que température rectale. Le CHU de Rouen ajoute systématiquement 0,9 °C à la mesure par thermomètre axillaire électronique ;température tympanique : mesure infrarouge de la température du tympan.La température buccale et la température axillaire étant moins élevées que la température rectale, prise comme référence, des corrections doivent être appliquées (+0,5 °C pour la buccale, +0,7 °C pour l'axillaire[réf. nécessaire]).L'état d'homéothermie est maintenu par une commande centrale exercée par la partie antérieure de l'hypothalamus (« thermostat hypothalamique »). L'hypothalamus reçoit des informations provenant des neurones associés aux thermorécepteurs périphériques, et aussi du sang circulant. En retour l'hypothalamus envoie des informations vers les neurones périphériques qui contrôlent les pertes de chaleur (vasodilatation périphérique et sudation) ou la production de chaleur (frissons musculaires).La fièvre est une hyperthermie qui dépend du contrôle hypothalamique, et qui se traduit par un trouble de régulation des mécanismes de perte ou de production de chaleur. L'augmentation de température par le thermostat résulte de l'effet de substances sanguines dites pyrogènes, exogènes ou endogènes.Les pyrogènes exogènes proviennent de micro-organismes infectants, comme l'endotoxine des bactéries gram-négatives, ou les toxines des bactéries gram-positives, soit par action directe sur le thermostat, soit par action indirecte en activant la production de pyrogènes endogènes par les cellules de l'hôte (leucocytes).Les leucocytes peuvent produire des pyrogènes endogènes capables d'induire un état fébrile. Il s'agit de protéines solubles de la famille des cytokines, parmi les plus importantes : l'interleukine 1, le TNF, les interférons... La plupart des cellules de l'organisme, soumises à des stress cellulaires, peuvent produire des pyrogènes. Ceci explique que tout état fébrile n'indique pas forcément une maladie infectieuse.Les cytokines agissent sur des récepteurs spécifiques présents sur toutes les cellules de l'organisme, comme les récepteurs Toll qui activent les mécanismes inflammatoires. Ces derniers se traduisent notamment par une extravasation des leucocytes et leur migration vers les tissus pour neutraliser l'agent agresseur. Lorsque l'agression est maitrisée par les réponses inflammatoires et immunitaires, le thermostat hypothalamique induit un retour de la température corporelle à la normale.La fièvre est donc considérée comme une réaction de défense de l'organisme contre une agression microbienne, physique ou chimique, qui s'est conservée tout au long de l'évolution des vertébrés. Son avantage se voit le plus clairement chez les poissons et les poikilothermes (animaux « à sang froid ») qui résistent mieux aux infections en augmentant la température de leur corps. Chez les mammifères, les avantages sont plus faibles (l'augmentation étant relativement plus limitée). De plus une fièvre très élevée (supérieure à 41 °C) peut léser le système nerveux central, d'où un probable système de régulation empêchant que la fièvre ne dépasse un certain plafond.La fièvre est un signe médical fréquent. Il appartient au médecin d'essayer de la rattacher à une étiologie (diagnostic) et d'évaluer sa gravité.Pour établir un diagnostic devant ce signe le médecin recherchera ses caractéristiques sémiologiques : homme / femme - âge - antécédents - ethnie - facteurs de risques - caractère aiguë / prolongée - lieu de séjour - fièvre isolée ou regroupement syndromique - incidence et prévalence locales et saisonnières, etc.En médecine générale, le plus souvent (voir carré de White) la fièvre conduira vers un diagnostic de pathologie bénigne.La cause de la fièvre peut être infectieuse (bactérie, virus ou parasite) ou non infectieuse (par exemple Vascularite, thrombose veineuse profonde ou effet secondaire). La fièvre due à une infection bactérienne est généralement plus élevée que celle due à un virus.Malgré leurs faibles prévalences dans les pays occidentaux, il est indispensable que le médecin sache écarter des atteintes particulièrement graves :au retour d'un voyage : paludisme ;si fièvre persistante : tuberculose ;associée à des douleurs articulaires : arthrite septique ;isolée, chez l'homme : prostatite ;en cas de souffle cardiaque : endocardite infectieuse ;si présence d'un purpura associé : purpura fulminans.Voici ce que constate l'ANSM sur la fièvre de l'enfant, par la plume d'Aude Chaboissier : « Les bénéfices attendus du traitement sont désormais plus centrés sur l'amélioration du confort de l'enfant que sur un abaissement systématique de la température, la fièvre ne représentant pas, par elle-même et sauf cas très particuliers, un danger. »Au cas par cas (par exemple : antécédents convulsifs, allergie, comorbidité, fiabilité de l'entourage, traitements associés, etc.) le médecin doit peser le rapport entre les bénéfices attendus et les risques encourus (hépatotoxicité, cardiotoxicité, syndrome de lyell, choc anaphylactique, cellulite faciale, etc.) avant de prescrire ou non des antipyrétiques.Attitudes pratiques pour la prise en charge d'une fièvre persistante supérieure à 38,5 °C :recourir à des mesures physiques simples : avant toute prise de médicament, il faut éviter de couvrir l'enfant, ne pas le maintenir dans une pièce surchauffée et lui donner à boire autant et aussi souvent que possible ; ceci, sans qu'il soit indispensable de lui donner un bain tiède comme cela était classiquement conseillé ;si prescrit on peut donner une seule classe de médicaments antipyrétiques en tenant compte des contre-indications et des précautions d'emploi qui lui sont propres ;brumiser de l'eau sur le visage pour rafraîchir si besoin.Ces recommandations concernent le confort de l'enfant et ne font ni baisser, ni n’élèvent la température car le thermostat hypothalamique mettra en marche les mécanismes de thermogenèse (si l'enfant est trop refroidi) et de thermolyse (si l'enfant est dans un environnement trop chauffé) afin de laisser le corps à la température prévue tant que les mécanismes immunitaires seront activés.Devant une fièvre de l'enfant, dans les pays occidentaux, il est fréquent de demander un avis diagnostic auprès d'un médecin généraliste pour affirmer le caractère bénin de l'épisode afin que l'enfant puisse être pris en charge par des adultes rassurés.Toutefois, un diagnostic médical est primordial si la fièvre de l'enfant présente des caractéristiques inhabituelles : nourrissons, fièvre plus élevée qu’habituellement, durée et évolution inhabituelle, comportements inhabituels (pleurs continus, fatigue, agitation, etc.), teint inhabituel, éruptions cutanées, signes d'accompagnements (vomissements, etc.), épidémie locale de pathologies potentiellement graves (méningite, etc.). La poussée dentaire n'est pas une cause de fièvre.Au-dessus de 40 °C, la température peut être un signe de maladie grave et peut être mal tolérée par l'organisme : Exemple : chez les personnes ayant une prédisposition, le risque de convulsion augmente.Chez le jeune enfant, cette fièvre peut entraîner des convulsions qui, si elles sont impressionnantes, sont en général bénignes ; il faut toutefois impérativement éviter que cette situation ne se prolonge, il faut donc abaisser lentement la température de l'ensemble du corps.On préconisait auparavant de donner systématiquement des bains d'eau dont la température est de 2 °C en dessous de la température du bébé, et la prescription médicale consistait souvent en une bithérapie aspirine-paracétamol ; la chute de la température était une priorité avec trois objectifs : empêcher le développement de l'hyperthermie maligne, éviter les convulsions fébriles et améliorer le confort de l'enfant.Cependant, aucune étude récente n'a mis en évidence l'effet des antipyrétiques pour la prévention des convulsions, et par ailleurs, seuls certains enfants (2 à 5 %) sont sujets aux convulsions.Une fièvre réelle (supérieure à 38 °C) chez un enfant doit toujours donner lieu à une consultation médicale, mais rarement aux urgences de l'hôpital sauf pour les nourrissons de moins de 3 mois.Il convient de prendre contact rapidement avec un médecin (le Samu en France) afin d'avoir des conseils et éventuellement une intervention médicalisée en présence de signes de gravité tels que : température supérieure à 40 °C ;perte de poids ;convulsions qui se répètent ou durent malgré le refroidissement ;taches sur la peau ;troubles de la conscience ;pleurs incessants.Avant d'aller consulter un médecin, il est nécessaire d'attendre 24 h pour un enfant entre 4 mois et moins de 2 ans puis 48 h pour un enfant de 2 ans et plus, sauf si les symptômes s'aggravent.La fièvre ayant un rôle dans la lutte contre l'infection, pour un enfant n'étant pas sujet aux convulsions et hors urgence (voir ci-dessus), l'administration d'antipyrétique n'est plus systématique, et n'est envisagée qu'à partir de 38,5 °C. On conseille alors plutôt le paracétamol en monothérapie,,.L'utilisation de l'ibuprofène chez l'enfant est controversée,. Il peut y avoir des effets secondaires rares mais graves chez l'enfant varicelleux.Par le passé, la fièvre a pu être délibérément provoquée dans un but de guérison. C'est ce que l'on a appelé « la pyrothérapie ». C'est le Dr Konteschweller Titus qui forgea le mot « pyrétothérapie » en 1918 rappelant notamment à cette fin l'usage du vaccin contre la typhoïde. Cette approche obtint une certaine reconnaissance avec la mise au point par Julius Wagner-Jauregg de la malariathérapie pour la guérison de la syphilis (cela lui valut le Nobel en 1927). D'autres procédés ont été utilisés,,,, que l'avènement des antibiotiques notamment ont relégué dans un oubli presque total. Dans les dernières années cependant – notamment dans le domaine de la lutte anticancéreuse – la réévaluation de la littérature à l'aune des connaissances contemporaines suscite un regain d'intérêt pour la - fever therapy, pyrétothérapie ainsi que pour la thermothérapie (élévation de la température par voie externe),.Tout comme chez les organismes endothermes, chez les ectothermes, la température de la fièvre augmente la capacité défensive de l'hôte en diminuant le taux de réplication des pathogènes et en augmentant l'efficacité du système immunitaire. En effet, la fièvre est une défense immunitaire ancienne avec des mécanismes physiologiques apparemment bien conservés au sein d'une large diversité de taxons d'invertébrés et de vertébrés. Pour ce faire, certains ectothermes modifient leurs comportements habituels assurant leur thermorégulation : ils se placent dans des endroits chauds afin d'élever leur température. Ce mécanisme, nommé fièvre comportementale a été identifié dans les années 1970 chez les iguanes du désert, les crapets arlequins et les têtards. Il concerne aussi les poissons et les insectes. Il permet aux insectes fébriles d'acquérir une survie et une fécondité supérieures aux non fébriles, mais l'atteinte et le maintien de la température élevée exigent des efforts coûteux pour l'organisme, parfois mortels.Dans le cas d'une infection fongique par Beauveria bassiana de la Mouche domestique, les hautes températures ont un effet négatif sur la croissance du champignon. Au petit matin, lorsque le champignon s'est développé à sa température optimale tout au long du cycle de la nuit, les immunosuppresseurs sont à des niveaux élevés et la réponse fébrile est la plus intense, pendant au maximum deux heures. À mesure que les facteurs immunitaires exogènes sont réduits ou éliminés de l'hémolymphe, la mouche se déplace progressivement vers des zones plus fraîches. Pendant la nuit, le champignon se rétablit, car la mouche ne peut pas exprimer de fièvre pour supprimer la croissance fongique. Et le cycle recommence le lendemain. La Mouche domestique provoque également des intensités de fièvre différentes, sélectionnant des températures plus élevées lorsqu'elle est infectée par une dose fongique plus élevée, montrant ainsi une capacité à gérer le bénéfice-risque de la fièvre.Fièvre chez l'enfant, Esculape.com« Fièvre chez l'enfant », sur ameli.frRecherches sur les fièvres, selon qu'elles dépendent des variations des saisons, et telles qu'on les a observées à Londres pendant les vingt années consécutives  (avec des observations de pratique sur la meilleure manière de les guérir. Suivies de L'histoire des constitutions épidémiques de Saint-Domingue, et de la description de la fièvre jaune). Tome premier. William Grant, traduit par Jean-Baptiste-René Pouppé-Desportes, édition de 1821Du typhus d'Amérique ou fièvre jaune, François-Victor Bally (1775-1866). Édition : Paris : Imprimerie de Smith. 1814.De la dengue : fièvre éruptive des pays chauds, et de sa distribution géographique: thèse pour le doctorat en médecine présentée et soutenue le 25 juin 1875, Albert  Morice (1848-1877). Portail de la médecine"
musique;"L'accordéon est un instrument de musique à vent de la famille des bois. Le nom d'accordéon regroupe une famille d'instruments à clavier, polyphonique, utilisant des anches libres excitées par un vent variable fourni par le soufflet actionné par le musicien. Ces différents instruments peuvent être de factures très différentes.Une personne qui joue de l'accordéon est un accordéoniste.Le Sheng, instrument de musique polyphonique religieux utilisé dans les orchestres de cour et de théâtre en Chine ancienne, est le plus ancien instrument à anche libre connu : il est constitué d'une chambre à vent sur laquelle sont fixés des tuyaux de bambou où vibre l'anche. Cet orgue à bouche est présent dès 2700 à 2500 av. J.C.. On le retrouve dans le reste de l'Asie sous d'autres noms : Sompoton sur l'île de Bornéo, Khène au Laos, Sho au Japon. Marin Mersenne cite entre 1636 et 1644, un Khên du Laos.En 1674, un Khène fait partie de l'inventaire de la collection du royaume du Danemark. Johann Wilde (en) aurait ramené un Sheng à la cour de Saint-Pétersbourg en 1740. Le jésuite et missionnaire Joseph-Marie Amiot fait parvenir en 1777 deux paires de sheng à Monseigneur Bertin à Paris.C'est durant la seconde moitié du XVIIIe siècle et le début du XIXe siècle que le procédé sonore de l'anche libre est l'objet de toutes les attentions des inventeurs. S'il est souvent avancé que le Sheng fut à l'origine de l'accordéon, le lien entre l'instrument asiatique et les instruments occidentaux n'est cependant pas évident, d'autant que la guimbarde, autre instrument à anche libre, existe en Europe depuis au moins l'époque gallo-romaine.En 1769 est organisé un concours à Saint-Pétersbourg, dont l'objet est l'invention d'un instrument qui imiterait la voix humaine. Le physicien danois Christian Gottlieb Kratzenstein (de) remporte le concours avec l'invention de sa « machine parlante ». Néanmoins, à la lecture de son travail publié à Bordeaux en français, on peut constater qu'il ne fait aucune allusion aux instruments asiatiques. Et que la construction de sa machine est exclusivement née de l'étude anatomique du larynx.C'est dans ce bain obscur entouré de mystères et contradictions, que les brevets d'invention autour des instruments à anche libre vont naître, s'interpénétrer, s'influencer, se doter parfois de manière douteuse de paternité, mais permettant peu à peu l'émergence d'une nouvelle espèce d'instruments.En 1810, on assiste à la naissance de l'« orgue expressif » de Gabriel-Joseph Grenié qui introduit le soufflet à pédalier, dont le système prendra plus tard le nom d'harmonium. Il réinvente l'anche libre, comme on peut le lire dans son mémoire de brevet.En 1818, l'Autrichien Anton Haeckl invente le Physharmonica, premier instrument à anches libres clavier et à soufflet manuel. Un brevet lui a été accordé le 8 avril 1821. Dans le journal Allgemeine musikalische Zeitung du 14 avril 1821, la publicité du physharmonica dit entre autres : « Le maître fait aussi des versions très petites qui reposent confortablement sur le bras gauche, et dont on joue de la main droite. » Cet élément est primordial pour l'avenir.En 1821, inspiré par la guimbarde, l'Allemand Christian Friedrich Ludwig Buschmann invente un instrument à anches métalliques : l'« aura ». Cet instrument, qui deviendra l'harmonica, inspirera des fabricants se copiant, améliorant, inventant tout une multitude d'instruments dérivés.Anton Reinlein obtient en 1824 à Vienne un brevet pour son harmonica « à la manière chinoise », Christian Messner ouvrira l'une des premières usines à Trossingen en 1827 puis en 1832 lance la fabrication de ses « mundharmonika ».En 1822, Buschmann monte un soufflet sur son « aura » qui devient « handaeoline », l'éoline à main.En 1827, Marie Candide Buffet (1797-1859) fabrique des « harmonicas métalliques à bouche ».En 1829, Cyrill Demian, facteur de piano et orgues à Vienne (Autriche), fabrique un instrument dans la veine de Buschmann et Haeckl, dont il veut déposer le brevet sous le nom d'« Aeolina »[réf. nécessaire]. Ce nom étant déjà pris par un modèle Buschmann et ce nouvel instrument étant, contrairement à ses prédécesseurs, voué à l'accompagnement et, en ce sens, n'émettant que des accords, Demian et ses fils dépose leur brevet le 6 mai 1829 sous le nom d'« Accordion » ; cet instrument est muni d'un soufflet manié par la main gauche, la main droite se réserve à un clavier dont chacune des 5 touches émet un accord, différent en tirant ou en poussant.Le 23 juin 1829, la même année que le brevet de Demian, Charles Wheatstone invente le « symphonium », rebaptisé « concertina », dont le brevet sera déposé le 8 février 1844[réf. nécessaire]. Ce modèle est unisonore.En France, en 1830, Marie Candide Buffet positionne un clavier mélodique en main droite à la place des accords[réf. nécessaire]. Demian invente, vers 1834, la combinaison d’un deuxième clavier pour les accords, et d’un premier pour la mélodie[réf. nécessaire].En 1834, Carl Friedrich Uhlig crée le « konzertina » allemand, bisonore, après avoir rencontré Demian et ayant désiré créer un instrument mélodique[réf. nécessaire]. C’est ce modèle qui inspirera Heinrich Band (de) la même année, en faisant évoluer la forme des claviers[réf. nécessaire].En 1841, Louis Léon Douce dépose un brevet pour son « accordéon harmonieux », instrument unisonore[réf. nécessaire].À partir de 1847 Carl Friedrich Zimmermann (de) développe le même type de concertinas que Band,. Les termes de « bandonion » puis « bandonéon » arriveront en 1854 en hommage au fabricant à Henrich Band.En 1852, Philippe-Joseph Bouton conçoit l’instrument avec un clavier piano à la main droite. En Autriche, le « Schrammelharmonika » sera le premier instrument avec le clavier main droite moderne qui va inspirer les Italiens. En Italie, en 1863, Paolo Soprani fonde la première industrie du « fisarmonica » (nom italien de l'accordéon) à Castelfidardo, ville considérée comme l'un des berceaux de l'accordéon moderne. Autre berceau, Stradella, dans la province de Pavie où Mariano Dallapé invente un nouvel instrument encore plus proche de l'accordéon moderne en 1871. Le terme « fisarmonica » est très important, car Soprani ne va pas fabriquer des accordéons, mais des « physharmonika ». Cette distinction n'est pas anodine car, en 1861, le Maître de chapelle de Loreto (à proximité de Castelfidardo) expose un instrument décrit comme « accordéon par la forme, mais véritable fisarmonica ». À l'époque, fisarmonica et accordéon sont deux instruments différents en Italie. C'est l'origine de l'industrie italienne.La première génération d'instruments encore usités apparaît à la fin du XIXe siècle. Jusqu'à aujourd'hui, les modèles n'ont cessé de se perfectionner, d'évoluer, de se spécialiser selon les styles, selon les coutumes, selon les traditions culturelles ayant accueilli l'une ou l'autre forme de l'instrument à anche libre et à soufflet manuel.Dans l'accordéon, deux anches sont montées sur une même plaquette, une de chaque côté de la plaquette. Une anche ne fonctionne que dans un seul sens, lorsque l'air la pousse vers la plaquette, donc une seule des deux anches fonctionnera pour un sens donné du soufflet. Une « peau musique » (en cuir, en vinyle ou en matériau composite souple) empêche la perte d'air par l'interstice entre l'anche qui ne parle pas et la plaquette (on dit de l'anche qui produit du son qu'elle « parle »).La vibration est due à un phénomène dit « de relaxation » : elle n'est donc pas sinusoïdale et comporte de nombreux harmoniques responsables d'une famille typique de timbres. Les harmoniques sont utilisés pour faciliter l'accord des basses fréquences (< 100 Hz).La fréquence de vibration est pratiquement indépendante de la puissance du souffle d'air, l'anche vibrante jouant d'ailleurs, à pleine puissance, le rôle de limiteur de débit. Cependant, lorsque des anches de fréquences extrêmement proches (différence inférieure à 1 Hz, tout au plus) sont alimentées en air par un système commun, il arrive que l'anche la moins stable en fréquence s'accorde à la fréquence de l'autre par effet de « couplage » ou de « pilotage », masquant leur « désaccord », voire interdisant un vibrato différentiel intentionnel de fréquence inférieure à 1 Hz.Dans l'accordéon, les anches donnant les sons les plus graves (< 50 Hz environ) ont une longueur de 5 à 10 centimètres et sont chargées, près de leur extrémité vibrante, par une masse de laiton (généralement — ou d'étain sur les anches anciennes ou modifiées par un accordeur). Les anches produisant les sons les plus aigus (plus de 6 kHz dans l'aigu du piccolo) ont une longueur inférieure à 6 millimètres.En raison de la très courte longueur d'onde des sons les plus aigus produits (de l'ordre de quelques cm), on constate souvent des phénomènes d'ondes stationnaires dus aux « obstacles » à leur propagation (cases exiguës du sommier qui supporte les plaquettes, soupapes…) qui peuvent affaiblir, voire neutraliser totalement, le son produit. Des solutions empiriques de facture permettent d'éliminer ce phénomène.L'accord se fait en jouant sur les paramètres raideur et masse : on augmente la fréquence en diminuant la masse par enlèvement de matière (limage d'épaisseur) à l'extrémité libre de l'anche (ou de sa charge rapportée). On diminue la fréquence en diminuant l'épaisseur (raideur) de l'anche (enlèvement par grattage : (grattoir) près de sa partie fixe, flexible (le « ressort »).Une anche vibrante de grandes dimensions et de fréquence infrasonique, destinée à produire un vibrato en amplitude, a été utilisée dans l'accordéon de concert Cavagnolo : cette anche est placée dans une paroi séparatrice (équivalente à une « plaque ») disposée entre le soufflet et la « caisse du chant ». Ce système générateur de vibrato semble être resté sans suite en raison, sans doute, de sa fréquence invariable, de son effet trop systématique (un accord, grave ou aigu, vibre « en bloc ») et de sa limitation du débit d'air (contradictoire avec l'expressivité naturelle de l'instrument), en dépit de la présence d'un moyen de neutralisation : une très large soupape.Le musicien ouvre et referme le soufflet central, positionné entre les deux parties droite et gauche de l'instrument, munie chacune d'un clavier : une partie droite, qui reste statique, et une partie gauche, qui s'écarte et se rapproche de la partie droite à chaque va-et-vient du soufflet (on parle de « tiré » ou de « poussé » du soufflet). En même temps, l'instrumentiste appuie sur les touches des claviers de l'instrument pour décider des notes à produire. L'air du soufflet passe ainsi dans le mécanisme, et actionne une ou plusieurs anches accordées à la lime et au grattoir. L'anche au repos possède une courbure qui la porte « au vent » : le réglage de cette courbure a pour but de permettre et faciliter l'attaque, à toutes les puissances.Véritable homme-orchestre, l'accordéoniste peut exécuter le rythme aussi bien que la mélodie et l'harmonie, ce qui lui a valu une place importante dans les bals populaires français.Cet instrument aux accords tout faits et à la sonorité « désaccordée » ne suscita pas l'adhésion de tous d'où, dès les années trente, l'invention des basses chromatiques (clavier mélodique de main gauche similaire à celui de la main droite, remplaçant grâce à un convertisseur le clavier traditionnel basse-accord) et la présence possible de registres permettant de changer la sonorité de l'instrument en appuyant sur un bouton.Il existe plusieurs sortes d'accordéons qui se différencient d'une part, par l'organisation des notes sur les claviers et d'autre part, par la manière de produire des notes en actionnant le soufflet.L'accordéon chromatique possède les 12 demi-tons de la gamme chromatique. Une touche enfoncée produira la même note que l'on tire ou que l'on pousse le soufflet. Certains ont des boutons, d'autres des touches de piano. Suivant les modèles, la tessiture peut dépasser 4 ou 5 octaves.Les accordéons diatoniques peuvent jouer des gammes diatoniques. Une touche enfoncée ne produira pas la même note selon que le musicien tire ou pousse le soufflet. On dit qu'il est bi-sonore.Ces deux descriptions correspondent aux deux familles d'accordéons les plus répandues. De nombreuses variantes ont été réalisées (chromatique bi-sonore, diatonique uni-sonore, systèmes mixtes).L'accordéon est utilisé en musique populaire, musique traditionnelle, musique folklorique, dans les musiques actuelles, ainsi qu'en musique classique et contemporaine. « On peut tout jouer avec l'accordéon » déclare Yvette Horner.La plus ancienne pièce de concert est Thème varié très brillant pour accordéon, écrit en 1836 par Mlle Louise Reisner de Paris. Le compositeur russe Piotr Ilitch Tchaïkovski inclut (de façon optionnelle) quatre accordéons diatoniques dans sa Suite pour orchestre no 2 en Do Majeur, op. 53 (1883), simplement pour ajouter une petite couleur au troisième mouvement (Scherzo burlesque). Le compositeur italien Umberto Giordano inclut l'accordéon diatonique dans son opéra Fedora (1898). L'accordéoniste apparait sur la scène, avec également un joueur de piccolo et un joueur de triangle, trois fois dans le troisième acte (qui se déroule en Suisse), pour accompagner une courte et simple chanson qui est chantée par un petit savoyard.En 1915, le compositeur américain Charles Ives inclut un chœur d'accordéons diatoniques (ou de concertinas) avec également, entre autres, deux pianos, un célesta, une harpe, un orgue, un zither et un thérémine optionnel dans son Orchestral Set no 2. La partie d'accordéon, écrite pour la main droite seulement, consiste en dix-huit mesures à la fin de l'œuvre. Le premier compositeur à avoir écrit spécifiquement pour l'accordéon chromatique est Paul Hindemith. En 1921, il inclut l'harmonium dans sa Kammermusik No. 1, une œuvre de musique de chambre en quatre mouvements pour douze musiciens, mais plus tard il récrit la partie d'harmonium pour l'accordéon. D'autres compositeurs allemands ont aussi écrits pour l'accordéon.En 1922, Alban Berg inclut un accordéon dans son opéra Wozzeck. L'instrument, marqué Ziehharmonika bzw. Akkordeon dans la partition, apparaît seulement durant la scène de la taverne, avec un ensemble sur scène (Bühnenmusik) consistant en deux violons, une clarinette, une guitare et un bombardon en fa (ou tuba basse).D'autres compositeurs du XXe siècle ont écrit pour l'accordéon comme Kurt Weill dans L'Opéra de Quat'sous (1928), Sergueï Prokofiev et sa Cantate pour le 20e anniversaire de la révolution d'octobre, op. 74 (1936), Dmitri Chostakovitch l'utilise dans la Jazz Suite No. 2 (1938), ainsi que Jean Françaix dans Apocalypse According to St. John (1939) ou Darius Milhaud dans Prélude et Postlude pour Lidoire (1946), ainsi que John Serry Sr. dans American Rhapsody (1955) et Concerto pour accordéon (1964) ,,. L'accordéon est présent aujourd'hui dans le répertoire de musique contemporaine. Principalement en musique de chambre, des compositeurs comme Henk Badings (Sonate pour accordéon seul, 1981), Luciano Berio (Sequenza XIII pour accordéon seul, 1995) ou Jean Françaix, Concerto pour accordéon 1997) ont écrit pour l'instrument.L'accordéon et ses variantes sont présents dans de très nombreuses musiques traditionnelles ou musique folkloriques. L'Écosse, l'Irlande ou la Grande-Bretagne furent ouverts à intégrer l'accordéon à leur folklore adaptant et composant dans leurs styles respectifs, soit des reels, des jigues ainsi que des valses. En Amérique, on retrouve traditionnellement l'accordéon dans le folklore québécois composé principalement de reels et de riggodons ainsi que dans la musique cadienne, principalement des ballades. L'Autriche, la Suisse ou la Bavière sont parfois représentées par des valses, des marches) ou des polkas. Les ensembles de musiques tsiganes et klezmers ont aussi des formes d'accordéons spécifiques comme le Bayan que l'on retrouve dans la musique traditionnelle en Russie.En Amérique latine, de nombreux genres musicaux utilisent différentes sortes d'accordéons comme le norteña au nord-est du Mexique, le chamamé en Argentine, la cumbia et le vallenato en Colombie ou instrument musique brésilienne, le baião au nord-est.L'accordéon est présent dans la musique de jazz. Cela a commencé avec la collaboration de Django Reinhardt et Jo Privat à l'époque du jazz swing. L'accordéoniste Marcel Azzola a aussi réalisé des arrangements pour accordéon des plus grands standards de jazz comme All the Things You Are. Plus récemment, des accordéonistes se sont éloignés du musette traditionnel pour s'intéresser au jazz, comme les artistes Richard Galliano, ou Vincent Peirani.Avant la Seconde Guerre mondiale, des musiciens comme Gus Viseur ou Tony Murena font déjà le lien entre jazz et musette. Après la guerre, l'accordéon est utilisé par des auteurs-compositeurs-interprètes comme Léo Ferré ou Jacques Brel, et des virtuoses comme Aimable, qui promènera son instrument en tournées mondiales[réf. nécessaire]. Mais l'instrumentarium du jazz moderne (be-bop, free jazz), puis du rock dans les années 1960, tend à le ringardiser[réf. nécessaire].Bien que créé en Europe, cet instrument se répand au Moyen-Orient et en Afrique du Nord, et est adopté par la musique populaire, puis s'introduit dans de nouvelles tendances musicales. Le chanteur de raï Cheb Khaled explique ainsi : « Mon instrument, c'est l'accordéon. Je l'ai appris à l'école de la rue. De naissance […] Dans le temps, quand les gens fêtaient les mariages, il y avait le violon, l'accordéon, la darbouka, mais pas de trucs électroniques. Et l'accordéon donnait un son typique, oriental. C'est original, c'est un beau son. »L’accordéon chromatique, avec des touches piano, s’est ancré à partir des années 1960 dans divers types de chanson populaire en Algérie : la chanson oranaise, le chaâbi, et le raï. Il participe à une transition entre les instruments traditionnels et les claviers électroniques. On le trouve également dans la musique populaire égyptienne, par exemple dans le style Baladi (en), avec des techniques et des modes d’interprétation spécifiques, notamment dans le jeu du bourdon ou le jeu en contretemps rythmique, qui se rapprochent des arrangements instrumentaux pratiqués par les joueurs d’instruments plus traditionnels tels que le mizm?r ou zurna,. Et en Égypte, une artiste des années 2010 comme Youssra el Hawary s'en empare,. De façon générale, l'accordéon n'est pas réservé à une musique populaire et festive, mais il sait prendre place dans une musique dite savante. L'oudiste Anouar Brahem s'est ainsi associé pour des spectacles et albums, à l'accordéoniste de jazz franco-italien Richard Galliano dans les années 1990.Le « piano du pauvre », ou « piano à bretelles », est entré dans la littérature française dès 1833, grâce au vicomte François-René de Chateaubriand dans Mémoires d'outre-tombe. L'accordéon en France est lié à l'histoire du bal musette. Il reste cependant pointé du doigt par certains : Octave Mirbeau le destine « aux polkas pour les bals ».L'histoire de l'accordéon est liée également à celle du swing manouche, avec dès les débuts de ce mouvement des collaborations répétées entre Jo Privat et Django Reinhardt, des compositions très en avance sur leur temps de Gus Viseur et aujourd'hui de nombreux artistes swing tels que Ludovic Beier … Dans les années 1950, l'accordéon devient l'instrument des bals populaires, Yvette Horner et André Verchuren étant alors les deux figures emblématiques de cet instrument.Dans les années 1970, l'accordéon redevient populaire grâce à l'attrait des musiques traditionnelles et folkloriques qui l'utilisent (musique bretonne, slave, musique cadienne…) ; par l'utilisation par des chanteurs français comme Renaud qui le remettent au goût du jour ; par l'apparition d'accordéonistes majeurs, se détournant du musette, comme Marc Perrone ou Richard Galliano ; et par son utilisation par des groupes de la scène alternative comme la Mano Negra ou Les Négresses vertes.L'accordéon a maintenant acquis ses lettres de noblesse en musique classique (même si cela reste méconnu du grand public). Il est enseigné dans les conservatoires de musique depuis les années 1970. L'accordéon est également présent dans la création contemporaine d'avant-garde. On peut citer Pascal Contet, qui contribue activement à développer le répertoire contemporain avec des compositeurs comme Bernard Cavanna, Vinko Globokar, Jacques Rebotier, Jean-Pierre Drouet, Bruno Giner, Marc Monnet, Sofia Goubaïdoulina,Jean Françaix, Poul Rovsing Olsen, Gérard Pesson… Citons également quelques membres de la jeune génération : Fanny Vicens, Vincent Lhermet, Jean-Etienne Sotty. Côté traditionnel, l'accordéon fait partie des instruments de la musique bretonne qui revient à partir des années 1970.Aujourd'hui, l'accordéon est largement utilisé aussi bien par des artistes de variétés (Patrick Bruel, Yann Tiersen…) que par des groupes « alternatifs » (Les Ogres de Barback, Les Têtes Raides, Red Cardell, Les Hurlements d'Léo, La Rue Ketanou, N&SK, Sagapool), les groupes de rap (Java, le Ministère des affaires populaires), le duo féminin Délinquante qui utilise cet instrument de façon notable, des groupes régionaux qui arrangent ces morceaux et/ou en composent de nouveaux tel qu'Accordé à vent, groupe du Pas-de-Calais, le duo Kof a Kof avec Roland Becker au saxophone et Régis Huiban à l'accordéon chromatique, avec des musiciens de jazz tels que Richard Galliano, Marcel Azzola, Marc Berthoumieux, Jacques Bolognesi ou Marcel Loeffler, Lionel Suarez, René Sopa.En 2005, Serge Lama a effectué une tournée avec un seul musicien, l'accordéoniste Sergio Tomassi jouant sur un accordéon numérique. Claude Parle développe l'accordéon dans le domaine des musiques improvisées et en relation avec la musique contemporaine, la danse, notamment la danse Buto (Masaki Iwana, Toru Iwashita, Atsushi Takenouchi (en)) ou le jazz contemporain (depuis les années 1970).Depuis avril 1990, San Francisco a pour instrument officiel l'accordéon.Lyon avec la firme Cavagnolo, et Tulle avec la fabrique Maugein sont des villes importantes pour l'accordéon chromatique français. Historiquement, la ville de Brive, avec l'usine Dedenis, fut très longtemps le siège de la première industrie de l'accordéon en France. Outre ces petites fabriques, plusieurs artisans fabriquent en France des instruments sur mesure ou commandés à l'unité, principalement des accordéons diatoniques, mais aussi des accordéons chromatiques.La ville de Castelfidardo est « la capitale mondiale de l’accordéon ». Une vingtaine d'entreprises familiales y sont regroupées dont Soprani, Pigini et Bugari ; 90 % des pièces détachées européennes y sont fabriquées.La manufacture d'accordéons la plus ancienne du monde, serait la société Harmona Akkordeon GmbH à Klingenthal en Saxe.Hohner, fabricant allemand d'harmonicas et d'accordéons situé à Trossingen, est présent en Europe et aux États-Unis.Les premiers accordéons des États-Unis ont été fabriqués à San Francisco.Il existe en France de nombreux festivals intégrant l'accordéon, ainsi qu'un certain nombre de festivals dédiés à l'instrument (qui peuvent être généralistes ou centrés sur un style de musique précis). Par exemple :L'Accordéon, moi j'aime !, en Belgique à TournaiQuelques exemples de surnoms de cet instrument (en France) : piano à bretelles, piano du pauvre, boîte à frisson, orgue portatif, branle-poumons, boîte à chagrin, soufflet à punaises, dépliant, calculette prétentieuse, boîte à soufflets et boîte du diable (boest an diaoul, en Basse-Bretagne et boueze en Haute-Bretagne). Ouvrages Pierre Monichon, L'Accordéon, éditions Van de Velde et Payot, 1985Pierre Monichon et Alexandre Juan, L’Accordéon, Éditions Cyrill Demian, 2012, 170 p.Joseph Amiot, Mémoire sur la musique des chinois, tant anciens que modernes, Paris, Nyon l'aîné, 1779 (lire en ligne) Articles Émile Leipp, Pierre Monichon, Alain Abbott et Étienne Lorin, « L'Accordéon — De l'accordion à l'accordéon de concert : éléments d'anatomie, de physiologie et d'acoustique », Bulletin du Groupe d'Acoustique Musicale (GAM), no 59,? 1972.Yves Defrance, « Traditions populaires et industrialisation : Le cas de l'accordéon », Ethnologie française, Presses universitaires de France, vol. 14, no 3,? 1984, p. 223-236 (résumé/1re page) Autres Thierry Benetoux, Comprendre et réparer votre accordéon, 2001, 243 p. (ISBN 978-2951718401)Interviews d'accordéonistes : Le souffle de l'accordéonCompositions pour accordéonHistorique des évolutions technologiques de l'accordéonTypes d'accordéonsAccordéon diatoniqueAccordéon chromatique à clavier « boutons » ou « piano »Accordéon à basses chromatiquesHarmonéon (aussi connu sous le nom d'accordéon de concert)Accordéon à touches pianoInstruments à anches libres, proches cousins de l'accordéon :BandonéonConcertinaAccordinaHarmonium indienRessources relatives à la musique : MusicBrainz (en) Musical Instruments Museums Online  Portail des musiques du monde   Portail de la musique   Portail de la musique classique"
musique;"Un chanteur ou une chanteuse est une personne qui utilise sa voix pour produire une succession de sons formant une mélodie. Ces sons peuvent être de simples phonèmes ou des textes littéraires. Le chant est présent dans tous les genres musicaux, qu'ils soient classiques ou modernes.La musique vocale couvre tous les genres musicaux, tant dans le domaine religieux (motet, oratorio, passion, etc. et plus généralement les différents rites et cérémonies) que profane (opéra, mélodie, chanson, etc.).Le chanteur peut utiliser comme support :un seul phonème sous forme de vocalises se rapprochant de l'art instrumental, comme dans les airs de la Reine de la nuit de La Flûte enchantée de Mozart ou la Bachianas brasileiras n° 5 de Villa-Lobos ;une série d'onomatopées, particulièrement dans le scat, popularisé par Louis Armstrong ou Ella Fitzgerald, mais aussi dans le rap du mouvement hip-hop;une comptine, un poème, un texte formant une chanson, un cantique, un hymne, une ode, un lied, une mélodie, etc. ;un livret spécialement créé ou inspiré d'une œuvre littéraire ou religieuse comme dans le motet, la cantate, l'oratorio, l'opéra, l'opérette, la comédie musicale, etc.Les thèmes et styles abordé peuvent être très divers, de la berceuse à l'épopée, en passant par la romance, couvrant tout l'éventail des sentiments humains. Ils peuvent avoir également une dimension politique ou sociale. On emploie parfois alors le terme de « chanteurs engagés » ou, dans le cas de l'humour et de la satire, de « chansonniers ».Si le timbre de la voix est relativement inné car en partie lié à la morphologie, un chanteur peut améliorer sa technique vocale en prenant des cours de chant.Dans le chant lyrique et le monde opératique, les chanteurs sont classés en plusieurs catégories en fonction du type et de la tessiture de leur voix :pour les femmes (du plus aigu au plus grave) :sopranomezzo-sopranocontralto (ou alto)pour les hommes (du plus aigu au plus grave) :sopranistecontreténorhaute-contreténorbarytonbasseCes classifications disposent de sous-classifications en fonction de la « couleur » du timbre, de l'« emploi » théâtral ou de l'agilité.Les castrats, hommes ayant subi à leur adolescence l'ablation des testicules afin d'empêcher la mue de leur voix selon une pratique en cours jusqu'à la fin du XIXe siècle, couvraient la tessiture actuelle des contre-ténors sopranistes et altistes. Mais l'absence de mue des castrats, par le moindre développement de sonorités graves et par la relative facilité à accéder au registre de tête qu'elle impliquait, apparentait leur voix à celles des femmes : les castrats conservaient ainsi tout au long de leur vie une voix « adolescente ».Le terme générique pour les chanteurs (hommes et femmes) spécialisés dans le genre lyrique (opéra, opérette) est « artiste lyrique ». Cette dernière notion fait appel également à des talents d'acteur. Les chanteuses solistes classiques sont souvent appelées « cantatrice », terme d'origine italienne . Certaines cantatrices célèbres  comme Maria Callas sont également appelées « divas » (litt. « déesses »).Note : Ces classifications n'ont en revanche pas de sens dans les genres musicaux qui n'utilisent pas la technique classique.Une formation à la musique spécialité chant se fait dès l’âge de 6 ans.Grâce au solfège et à la formation musicale, le chanteur peut accéder plus rapidement à des répertoires de style varié, transcrits sous forme de partitions.Il peut se produire en soliste ou en groupe (duo, trio, quatuor, etc.) jusqu'au chœur ou chorale pouvant réunir plusieurs centaines de personnes. On emploie alors le terme de « choriste ».Les chanteurs peuvent s'exprimer a cappella (sans accompagnement instrumental) ou être soutenu par l'accompagnement d'un instrument (piano, guitare, etc.), de plusieurs (quatuor à cordes, etc.), voire de tout un orchestre.Dans la tradition de ménestrels du Moyen Âge, le chanteur des rues se produit de façon itinérante a cappella ou s'accompagnant d'un instrument, le plus populaire au XIXe siècle étant l'orgue de Barbarie.Les droits des interprètes relèvent des droits voisins du droit d'auteur en France.Bernard Epin et Max Rongier, Profession chanteur, Paris, Éditions Farandole, 1977  (ISBN 2-7047-0031-1)ChansonChantChanteur de chanson (au Japon)Chœur (musique)Corde vocaleCrooner (variété américaine)Musique vocaleTechnique vocaleVoix (instrument)Voix (musique classique) Portail du travail et des métiers   Portail de la musique"
musique;"Dans la musique occidentale, et plus précisément, la musique classique, un contre-ténor (ou contreténor) est le type de voix masculine utilisant principalement sa voix de fausset (ou voix de tête), et dont la tessiture peut correspondre à celle d'un soprano (on parle alors de sopraniste), à celle d'un alto (altiste) ou à celle d'un contralto (contraltiste).Le contre-ténor, appelé aussi falsettiste, est à différencier de la haute-contre qui est un ténor utilisant sa voix de tête pour les aigus ou sur-aigus.Le contre-ténor a connu ses heures de gloire au cours de la Renaissance et pendant la période baroque, notamment en Espagne, en Angleterre ou en Allemagne, où ils étaient utilisés dans la musique sacrée. En Italie on n'a que très peu fait usage des contre-ténors dans l'opéra et, dans la musique sacrée également, ils furent complètement remplacés par les castrats au moins depuis la fin du XVIIe siècle. En France, c'est le règne de la haute-contre. À partir de la période classique, la technique vocale falsettiste des contre-ténors n'a pratiquement plus été utilisée à l'exception des chœurs des cathédrales anglicanes et du genre musical profane anglais du glee. On doit à Jean-Jacques Rousseau la disparition de ces voix en France si l'on se réfère à son dictionnaire de la musique.[réf. nécessaire]Ce n'est qu'au milieu du XXe siècle que les contre-ténors ont été remis à l'honneur, à l'occasion de la redécouverte du répertoire de la « musique ancienne » (c'est-à-dire la musique antérieure à la période classique). On leur fait alors chanter les rôles d'altos masculins dans les cantates de Bach, puis par extension, les rôles de castrats de l'opéra séria.Il existe, par ailleurs, un répertoire plus contemporain pour contre-ténors, notamment dans Le Songe d'une nuit d'été (1960) et Death in Venice (1973) de Benjamin Britten, Le Grand Macabre (1978) de György Ligeti, Akhnaten (1983) de Philip Glass ou The Tempest (2004) de Peter Tahourdin ; Péter Eötvös dans son opéra Trois Sœurs (1998, d'après Les Trois Sœurs de Tchekhov) confie les rôles des trois sœurs ainsi que de leur belle-sœur Natacha à quatre contreténors (deux contraltistes, deux sopranistes).Dans la polyphonie médiévale (et notamment dans le motet), on appelait contreteneur (lat. contratenor) la ou les voix disposées contre la teneur (tenor).Lorsque l'ambitus de ces voix rajoutées cessa de se confondre avec celui du ténor, on les distingua par les termes de :« contratenor bassus » (« contre la teneur, en bas »), vite abrégé en bassus (mais donnant aussi  basse-contre) ;« contratenor altus » (« contre la teneur, en haut »), abrégé ou traduit en contratenor, contra, altus (it. alto), contralto et haute-contre.La plupart de ces termes ont pris depuis des sens spécifiques.L'acception du terme de contreténor pour désigner une voix très aiguë provient plutôt de l'anglais countertenor. À la Renaissance, en France, la partie de contreténor désignait une ligne de chant qui sonnait contre celle de la ligne de ténor. Elle avait alors une tessiture assez comparable à la ligne de ténor. Peu à peu la ligne de contreténor s'est scindée en deux lignes de tessitures distinctes : la ligne de contreténor haute et la ligne de contreténor basse, qui ont donné les lignes de contralto et de basse.En français, la voix de contreténor peut être aussi appelée alto masculin ou falsettiste alto. Mais l’usage critique et musicologique actuel tend à faire de contreténor et de falsettiste des termes synonymes, si bien que la tessiture du contreténor peut s’étendre du registre de contralto à celui de soprano voire de soprano léger (comme chez Francesco Divito, Michael Maniaci ou André Vásáry).O Solitude de Henry PurcellMusic for a While de Henry PurcellA Chlorys de Reynaldo HahnOmbra mai fu de Georg Friedrich HaendelLascia ch'io pianga de Georg Friedrich HaendelChiamo il mio ben così de Christoph Willibald GluckChe farò senza Euridice de Christoph Willibald GluckNon so più cosa son, cosa faccio de Wolfgang Amadeus MozartVoi che sapete de Wolfgang Amadeus MozartVedrò con mio diletto de Antonio VivaldiArtaserse, un opéra par Leonardo Vinci, qui comporte un grand nombre de rôles pour contreténorsPour une liste plus exhaustive, voir la catégorie:contreténor, artistes disposant d'un article sur Wikipédia.MusiqueMusique classiqueVoix (instrument)Voix (musique classique)ChantSopranisteLe site des contreténors Portail de la musique classique   Portail de l’opéra"
musique;"Terme générique, une flûte (ou flute) est un instrument de musique à vent dont le son est créé par l'oscillation d'un jet d’air autour d'un biseau droit, en encoche ou en anneau.Ce souffle peut être dirigé librement par l'instrumentiste dans le cas des flûtes traversières, des instruments de type quena ou encore des flûtes de Pan, ou canalisé par un conduit en étant émis par le musicien lui-même dans le cas des différents types flûtes à bec ou en étant créé par une soufflerie mécanique dans le cas du jeu d'orgue.Les flûtes sont le plus souvent de forme tubulaire mais parfois globulaire, en graminée, en bois, en os ou en corne, mais aussi en pierre, en terre cuite, en plastique, en métal (or, argent…), en ivoire et même en cristal, la flûte peut être formée d'un ou de plusieurs tuyaux, avec ou sans trous, ou posséder une coulisse.Dès la Préhistoire, elle se retrouve partout dans le monde sous toutes sortes de formes. En septembre 2008, plusieurs morceaux d'une flûte datant du Paléolithique supérieur (environ 35 000 ans) ont été découverts dans la grotte d'Hohle Fels au sud-ouest de l'Allemagne, dans le Jura souabe. Cette flûte avait été fabriquée dans un radius de vautour fauve et témoigne du fait que les tout premiers Homo sapiens jouaient déjà de la musique.La flûte de pan était utilisée en Grèce dès le VIIe siècle av. J.-C. Le tin whistle est apparu au XIIe siècle, la flûte à bec au XIVe siècle. Certaines, à l'époque baroque, se virent ajouter un système de clés permettant d'obstruer les trous. Cette invention, dont il est impossible de tracer l'origine, fut notamment développée par Theobald Boehm au XIXe siècle.      Instruments de type  « flûte à bec » Le flageoletla flûte à bec et ses variantes historiques : le pipeau, le flageolet, le chalumeau, etc.le tin whistle, flûte droite en métal d'origine irlandaisela flûte harmonique (même si toutes ne sont pas à conduit)le flaviol, le galoubet, le txistule pinquillo andin (Bolivie et Pérou)la tarka ou l'anata andines (Bolivie, Pérou, Chili)le mohoceño andinle suling (flûte indonésienne)la sodina (flûte malgache)la fujara slovaquele salamouri géorgien Flûtes globulaires L'ocarinale sifflet Flûtes nasales La flûte nasale droitele sifflet à nez Flûtes à coulisse La jazzoflûte ou flûte à coulissela flûte Scoatariu Flûtes à embout coulissant Le glissando headjoint de Robert Dick (en) (pour flûte traversière).Le vibrabek de Jean-Pierre Poulin (pour tin whistle). Flûtes traversières L'Irish flute, flûte traversière en boisle fifrela flûte traversière classique, et ses variantes :flûte en sol (ou flûte alto) ;flûte basse ;flûte octobasse.le koudi chinoisle piccolola flûte traversière baroque (parfois appelée traverso de l'italien flauto traverso)le bansurî, flûte indiennele palahuito, flûte traversière andinele dízi (flûte traversière chinoise), incluant le b?ngdí (piccolo) et le qudi (flûte),le daegeum, flûte traversière coréenne proche par son organologie du dizi chinois, utilisée dans la musique a'ak, équivalent coréen du gagaku japonaisles flûtes traversières japonaises (nom générique : fue ou yokobue) : ry?teki (flûte du gagaku), nohkan (flûte du théâtre nô), (flûte du gagaku), kagurabue (flûte du gagaku), dengakubue (utilisée dans les cérémonies liées au riz : dengaku), shinobue, misatobue…la flûte peule (appelée aussi Tambin) Flûtes à encoche La quena, flûte de roseau andine (principalement dans les pays andins), et ses variantes :le quenacho, modèle tenor, plus grand ;la quenilla, modèle soprano, plus petite.Le xiao chinoisle shakuhachi, flûte de bambou japonaisele danso coréen Flûtes obliques le ney (ou nay), flûte orientale, gasba (flûte algérienne et tunisienne), qawala égyptienle kaval turcle kaval bulgarele blul arménien Flûtes globulaires le xun chinois, Flûtes de Pan Le siku des Indiens Aymaras des Andes (Bolivie, Pérou et Argentine), également appelé antara en langue quechua et zampoña en espagnol, avec ses modèles de différentes tailles : toyo, chili, mala et sanka, de la plus grande à la plus petitele rondador de l'Équateurle naï roumainle paixiao chinoisLa flûte, outil de tissage dans la manufacture de basse lice de BeauvaisLa flûte à altérateurs : des cylindres amovibles bémolisent les notes en diminuant le diamètre des trous de jeu.l'aulos, des Grecs anciens : cet instrument à anche n'est pas une flûte, mais est cependant presque toujours appelé « flûte double » dans la littérature.                                                             Peu prisée pendant les quarante premières années de l'histoire du jazz en raison d'un volume sonore modeste vite étouffé par les sections de cuivres et d'autre part en concurrence directe avec la clarinette, ce n'est qu'à partir des années 1950 qu'elle éveille l'intérêt des jazzmen.Des musiciens comme James Moody, Gigi Gryce, Frank Wess, Eric Dolphy, Herbie Mann, des chefs d'orchestre comme Count Basie, Quincy Jones et Gil Evans ont su l'imposer comme un instrument de jazz à part entière. Roland Kirk élargira les possibilités expressives de l'instrument et nombre de musiciens l'adoptent dès lors comme instrument principal alors qu'au début elle n'était que le bonus des saxophonistes;John Coltrane ne s'y sera essayé qu'une seule fois dans To be. Longtemps utilisée par la musique classique pour son caractère pastoral et poétique la flûte jazz revendique sa place à part entière dans l'espace musical de la modernité.Ian Anderson, du groupe de rock progressif Jethro Tull, a utilisé la flûte dans ses compositions et sur scène, influencé par la technique de Roland Kirk, en pratiquant l'over-blowing (technique consistant à forcer le souffle pour obtenir une note plus haute sans la former par le doigté), mais aussi en chantant en superposition du son de la flûte.D'autres musiciens de rock progressif comme Peter Gabriel, Andrew Latimer ou Ray Thomas ont utilisé la flûte dans les compositions de leurs groupes respectifs (Genesis, Camel, The Moody Blues). Par ailleurs, le groupe de folk metal Ithilien allie des instruments traditionnels, tels que la flûte, avec une touche de metal moderne.D. Buisson, « Les Flûtes Paléolithiques d’Isturitz (Pyrénées-Atlantiques) », Bulletin de La Société Préhistorique Française,? 1990, p. 420–433. (lire en ligne).T. Clodoré et A-S. Leclerc (dir), Préhistoire de la musique, catalogue de l'exposition de Préhistoire de Nemours, éditions du musée de Préhistoire de Nemours, 2002.T. Clodoré-Tissot et P. Kersalé, Instruments et musiques de la Préhistoire, éditions Lugdivine, 2010, no 9.Flûte : jeu d'orgueFlûte longitudinale Portail des musiques du monde   Portail de la musique"
musique;"D'après l'encyclopédie Larousse, le genre musical est un « ensemble de formes de même caractère, réunies par leur destination (par exemple la musique de chambre) ou par leur fonction (par exemple la musique sacrée) ». Le genre musical est un concept sans limites précises, il est compliqué d'établir une liste exhaustive des genres. La dénomination d’un genre peut venir d’une expression qui a marqué une scène musicale (Krautrock), de techniques ou sources sonores utilisées par le genre musical populaire (techno, synthpop), de son origine géographique (Miami bass, UK garage), ou de l’intention que porte le style (rock psychédélique). Un genre musical peut être vocal ou instrumental.Selon le genre envisagé, diverses caractéristiques matérielles ou humaines peuvent être prises en considération.La source sonore est souvent déterminante pour appréhender un genre : celle-ci définit les instruments, les voix, les formations ou les effectifs de telle ou telle musique.La voix étant un instrument tout à fait particulier (elle réside dans le corps de l'exécutant et permet d'ajouter du texte à la musique, mais aussi des sons vocaux tels que des cris, des râles, des souffles...), la musique vocale et la musique instrumentale généreront des genres musicaux différents.Du point de vue du nombre des interprètes, on peut, par exemple, distinguer : le genre quatuor à cordes (pour quatre solistes), le genre sérénade pour orchestre de chambre (petite formation orchestrale), le genre concerto symphonique (pour orchestre symphonique et soliste).Le lieu de destination peut parfois déterminer un genre ou un autre. Il peut s'agir de musique destinée à être jouée en extérieur ou en intérieur et dans des types de lieux variés (type de lieux, dimension, volume, acoustique…). Par exemple, le genre marche militaire est normalement destiné à être joué en plein air, tandis qu'un trio de musique de chambre est plutôt destiné à une salle, de dimension réduite de préférence.La durée moyenne d'une œuvre musicale varie notablement d'un genre à l'autre et peut servir de caractéristique. Par exemple, les morceaux du genre opéra sont généralement plus longs que ceux du genre chanson.Lorsqu'on veut déterminer un genre musical, le critère sociologique est probablement le plus pertinent. Il permet de répondre aux questions suivantes : « à quoi sert cette musique ? » (sa fonction), « à qui s'adresse-t-elle ? » (quel groupe social) et « dans quelles circonstances est-il joué ? ». Ainsi la musique religieuse (ou musique sacrée) regroupera certains genres musicaux, tandis que son contraire (la musique profane) en regroupera d'autres. Quelques exemples : musique pour la scène, musique de film, musique de danse, musique militaire, musique funéraire, musique d'ascenseur, jingle, etc.Le genre doit ensuite être distingué du système musical, c'est-à-dire, de l'ensemble des usages propres à telle ou telle pratique musicale : échelles, modes rythmiques, règles d'écriture et aspects techniques divers.En effet, un système de composition peut créer différents genres musicaux. Le système tonal, depuis la fin de la Renaissance jusqu'à nos jours, a imprégné divers genres appartenant aussi bien à la musique instrumentale, à la musique vocale, à la musique sacrée ou à la musique profane.Réciproquement chaque genre peut appartenir à différents systèmes musicaux. Par exemple la musique vocale sacrée est susceptible d'exister dans un très grand nombre de systèmes : homophonie, hétérophonie, musique modale, musique tonale, musique atonale, musique sérielle, musique acousmatique, etc.Le genre musical doit également être distingué de la forme musicale.En effet, des œuvres musicales appartenant à un même genre peuvent revêtir différentes formes. Par exemple, une mélodie peut suivre la forme binaire (ABABA...), la forme rondo (ABACAD...), ou encore, une forme plus complexe et inhabituelle, la forme libre (ABCDE...).À l'inverse, des œuvres musicales appartenant à des genres différents peuvent revêtir la même forme. Par exemple, la forme fugue peut se retrouver dans une messe, dans une pièce pour orgue, dans une ouverture, dans un opéra, etc.Pour désigner le concept qui fait l'objet du présent article, le mot « forme » est souvent employé en lieu et place du mot « genre », ce qui ne manque pas de susciter de regrettables équivoques. Cette confusion provient du fait que dans un contexte donné — une époque, une esthétique, etc. — un genre revêt souvent une forme privilégiée, à tel point que le premier donne son nom au second — ou inversement. Quoi qu'il en soit, les deux concepts ne doivent pas être confondus. De même, le genre sonate est à distinguer de la forme sonate. Si l'emploi du mot « genre » a été retenu ici, c'est, d'une part parce que dans le domaine de la musique, ce mot revêt un sens analogue à celui qu'il reçoit dans d'autres arts — cinéma, littérature, peinture, etc. — d'autre part et surtout, parce que dans ce même domaine, le mot « forme » a déjà d'autres significations, ainsi qu'on vient de le voir.Allan Moore a répertorié quatre façons d'appréhender la relation entre le genre et le style musical :dans la première le style décrit la manière d'articuler les gestes musicaux (en) et le genre est lié à l'identité et au contexte de ces gestes ;dans la seconde le genre met l'accent sur le contexte des gestes et se rapporte à l'esthétique alors que le style qui met l'accent sur leur mode d'articulation se réfère à la poïétique (le processus de création) ;dans la troisième, le genre est normalement socialement contraint alors que le style est peu déterminé par le social et a un certain degré d'autonomie ;dans le quatrième, le style a plusieurs niveaux hiérarchiques, du plus général qui peut être socialement constitué au plus local ; le système des genres est lui aussi hiérarchique mais les « sous-genres » constituent des genres à part entière d'une façon différente des « sous-styles ».En dépit des distinctions ci-dessus, il n'est pas toujours facile de s'entendre sur la définition exacte de tel ou tel genre : certains ont des frontières floues, d'autres sont inventés par les critiques, tel le post-rock, ou plus récemment encore, le nu metal.Parfois, un nom de genre est susceptible d'évoluer en fonction de l'époque ou du lieu. Par exemple, dans la musique classique, le genre sonate, au XVIe siècle, désigne approximativement toute pièce musicale exclusivement instrumentale (par opposition à la cantate, pièce essentiellement vocale) ; tandis qu'au XIXe siècle, le même mot renvoie plus précisément à un « genre instrumental propre à la musique de chambre, généralement constitué de plusieurs mouvements ».D'autres fois au contraire, plusieurs mots désignent plus ou moins un même genre. Par exemple, dans la musique baroque, les termes suite, ordre, cassation et partita renvoient au même genre (avec éventuellement quelques nuances variant selon le compositeur).Le progrès de la distribution musicale électronique a créé la possibilité d’avoir accès à de très vastes catalogues musicaux, et, en dépit des limites évoquées ci-dessus, a augmenté la nécessité d’une classification cohérente des genres musicaux.En raison de l'incohérence des taxonomies musicales existantes, un projet de metadatabase (en) global de titres musicaux a été proposé par François Pachet et Daniel Cazaly. Cette proposition a comme objectif celui de décrire et classifier (dans le cadre de la musique occidentale) les titres musicaux (et non pas des albums ou artistes), en suivant les principes d’objectivité, indépendance, similarité et cohérence, en s’appuyant sur une série de descripteurs musicaux. Les créateurs de la plate-forme musicale Gracenote, dont le chercheur Oscar Celma, ont quant à eux répertorié plus de 2 000 genres musicaux, qui ont été créés par les labels musicaux pour des raisons de marketing ou bien par les fans (Celma et d'autres parlent alors de folksonomy, soit de taxonomie musicale créée par de simples auditeurs et non par des musiciens ou des professionnels de l'industrie musicale).Entre les différents genres, le nombre de caractéristiques communes est susceptible de varier. Certains genres sont très éloignés de nature (par exemple, la comédie musicale a peu de points communs avec le psaume) ; d'autres au contraire, peuvent être considérés comme très proches, sinon apparentés (par exemple, la sonate correspond à une évolution de la suite).Une classification arborescente peut être opérée par exemple en adoptant la classification suivante :musique vocalemusique vocale sacréemusique vocale profanemusique instrumentalemusique instrumentale sacréemusique instrumentale profaneCette double classification selon la source sonore et fonction sociale n'est pas exempte de reproches (par exemple, un même genre peut exister dans différents types de musique), mais elle évite d'employer trop de termes n'appartenant pas nécessairement au secteur musical qui intéresse l'utilisateur. Par ailleurs, elle est relativement facile à utiliser.[réf. nécessaire]Liste alphabétique des genres musicauxListe des genres musicaux par zone géographiqueListe des genres de la musique occidentalePrincipes de classement des documents musicaux (PCDM)Le format ID3 propose aussi une liste de genre musicaux.Le mot genre revêt un sens tout à fait spécifique dans le domaine de la musique de la Grèce antique. Dans ce contexte, le terme désigne alors une division particulière de chaque tétracorde constitutif d'une échelle musicale.Un tétracorde est la succession de quatre degrés conjoints — par exemple : mi, ré, do, si. Parmi ces degrés — qui correspondent aux quatre cordes de la lyre — les degrés extrêmes — mi et si dans notre exemple — sont fixes, tandis que les degrés intermédiaires — ré et do dans notre exemple — sont mobiles. Une gamme est donc composée de deux tétracordes consécutifs — « mi, ré, do, si » et « la, sol, fa, mi », par exemple — séparés par un ton, appelé « disjonction » — « si, la » dans notre exemple.Le genre diatonique divise chaque tétracorde de la manière suivante, de l'aigu au grave : deux tons (9/8) et un demi-ton diatonique ou limma (256/243). Il y a par exemple : « mi, ré, do, si ». C'est le seul genre dont la musique occidentale ait hérité, et qui est à la base de l'échelle diatonique pythagoricienne.Le genre chromatique divise chaque tétracorde de la manière suivante, de l'aigu au grave : tierce mineure (32/27), demi-ton chromatique ou apotome (2187/2048) et demi-ton diatonique ou limma (256/243). Il y a par exemple : « mi, do                    ?              {\displaystyle \sharp }  , do                    ?              {\displaystyle \natural }  , si ».Le genre enharmonique divise chaque tétracorde de la manière suivante, de l'aigu au grave : tierce majeure (5/4 ou 81/64, selon les théoriciens) et deux diésis, équivalent approximativement au quart de ton (selon les théoriciens : 36/35, 28/27, 39/38, 40/39, 31/30, 32/31...). Il y a par exemple : « mi, do, xx, si » — la note xx est à mi-chemin entre do et si.Gérard Denizeau, Les genres musicaux, vers une nouvelle histoire de la musique, Paris, Larousse, 1998. Portail de la musique"
musique;"En acoustique, un partiel harmonique est une composante d’un son périodique, dont la fréquence est un multiple entier d'une fréquence fondamentale.Si on appelle « ƒ » la fréquence fondamentale, les partiels harmoniques ont des fréquences égales à : ƒ, 2ƒ, 3ƒ, 4ƒ, 5ƒ, etc.Les partiels harmoniques sont des composants importants d’un son musical – du moins dans une conception traditionnelle du « son musical » : dans les musiques d'aujourd'hui, les bruits peuvent être aussi des sons musicaux. La fondamentale détermine la hauteur perçue, la puissance relative des harmoniques de rang supérieur influe, avec des caractères dynamiques, sur le timbre.En musique, par assimilation, on appelle « harmoniques » les sons qu'on obtient sur les instruments à cordes en forçant la vibration d'une corde à un mode supérieur à son mode fondamental. Par exemple, en effleurant la corde au tiers de sa longueur, on empêche son déplacement latéral à cet endroit, tout en la laissant osciller autour de ce point fixe, créant un nœud qui l'oblige à vibrer à une fréquence triple de celle qu'elle aurait, libre. Le son ainsi produit se trouve à un intervalle de douzième avec celui de la corde libre (une octave plus une quinte).Le mot « harmonique » est utilisé aussi de manière moins technique pour désigner des éléments de l'harmonie, par exemple dans l'expression « intervalle harmonique », qui désigne simplement un intervalle appartenant à l'harmonie.À hauteur (donc fréquence) identique, les sons émis par deux instruments différents (par exemple un violon et une flûte) ne sonnent pas de la même manière. Chacun se caractérise par ce qu’on appelle son timbre, qui permet de l’identifier. Cela traduit le fait qu’aucun son réel n’est absolument simple : il résulte de la combinaison de sons partiels. La fréquence d'ensemble de ceux-ci détermine la note de musique que l'on perçoit (voir  Fondamentale absente) et leur évolution au cours de l'émission de la note déterminent son timbre.Les sons produits par les instruments à vibration entretenue (ceux dont la vibration est soutenue par un apport constant d'énergie, par exemple par le frottement de l'archet pour les instruments à cordes, ou par un souffle constant pour l'orgue ou les autres instruments à vent) contiennent de nombreux harmoniques, alors que d'autres instruments comme les percussions émettent des fréquences inharmoniques (2,576ƒ, 5,404ƒ... par exemple pour un triangle). De plus, chaque harmonique possède une intensité relative par rapport aux autres. Le spectre harmonique révèle ainsi l’ensemble des fréquences qui déterminent le timbre de chaque instrument.La fréquence fondamentale est celle du premier partiel harmonique du son considéré, que l’on désigne comme harmonique 1 ou harmonique fondamental. La note que l’on perçoit correspond à cet harmonique, même s'il est absent du spectre sonore. Certains sons peuvent cependant tromper l’oreille, un harmonique aigu pouvant s’entendre plus que la fondamentale et la cacher[réf. souhaitée].Les harmoniques d’une note, forcément plus aigus que cette note, sont souvent appelés harmoniques supérieurs par opposition à la théorie erronée des harmoniques inférieurs avancée par certains théoriciens de la musique.Le tableau des fréquences de notes ci-dessous indique une correspondance entre les fréquences harmoniques d’une note et les notes qui s’accordent en consonance avec la fondamentale. Ainsi par exemple pour la note do, les notes constituant des intervalles consonants avec elle sont mi (la tierce), sol (la quinte), si  (la septième), do (l’octave), ré (la neuvième), etc. La raison en est qu'un nombre élevé des harmoniques supérieurs de ces notes consonances sont aussi des harmoniques de la fondamentale : la consonance se définit alors par la concordance entre les harmoniques. Les harmoniques d’une note sont donnés par les fréquences multiples de la fondamentale. Ainsi pour do à 32,7 Hz noté do?1, les harmoniques sont :N.B. : les 7e et 11e harmoniques n'ont été utilisées dans l'histoire récente de la musique occidentale que dans les musiques  microtonales ou les  musiques spectrales.L’image ci-dessous indique les harmoniques du do1 sur une portée et précise par les flèches et les chiffres (en cents) l’écart de hauteur entre chacun des 16 premiers harmoniques et la note la plus proche dans la gamme tempérée. Considérant que le demi-ton (du tempérament égal) fait 100 cents, la déviation de 31 cents de l'harmonique 7 est un tiers de ton plus bas que la moitié de la distance entre les harmoniques 6 et 8 et la déviation de 49 cents de l'harmonique 11 est quasiment à mi-chemin entre deux notes existantes, c’est-à-dire un quart de ton : c'est la raison pour laquelle elles ont eu peu d'usage dans la musique classique.Les écarts des harmoniques avec les notes de la gamme tempérée se retrouvent quelle que soit la note fondamentale et sont propres au rang de l'harmonique.Sur un piano, enfoncer doucement la pédale et faire résonner une note, permet de les écouter lorsque l’intensité sonore diminue : les cordes correspondant aux harmoniques vibrent par sympathie ; la série indiquée ci-dessus est alors relativement audible.C’est le bon ajustement des harmoniques de deux notes entendues simultanément qui validera la consonance de l’intervalle ou de l’accord entendu. On retrouve bien les raisons pour lesquelles un accord est parfait (do-mi-sol = 4-5-6) : les harmoniques des notes de l’accord sont en concordance avec les harmoniques de la fondamentale.Sur les instruments à cordes, on peut faire entendre un son harmonique en effleurant d’un doigt une division entière de la corde. Ci-contre, les divisions correspondant à la fondamentale F (corde à vide) et aux trois premiers harmoniques, et la façon de noter une note ainsi jouée.Harmonique est à l'origine un adjectif qualifiant ceux des partiels d'un son qui sont multiples d'une même fréquence, dite fondamentale. Les autres partiels sont dits inharmoniques. Seuls les partiels harmoniques contribuent à l'identification de la note de musique.Employé comme substantif, il remplace l'expression « partiel harmonique ». Il est par conséquent, comme partiel, du genre masculin.Si, dans un discours, on utilisait harmonique comme substantif pour remplacer « note harmonique » ou « division harmonique », il serait du genre féminin, comme les substantifs de l'expression à laquelle il se substituerait[réf. souhaitée].Par contamination du substantif féminin harmonie, il s'emploie aussi au féminin.Patrice Bailhache, Antonia Soulez et Céline Vautrin, Helmholtz du son à la musique, Paris, Librairie philosophique J. Vrin, 2011, 253 p. (ISBN 978-2-7116-2337-2, lire en ligne)Philippe Gouttenoire et Jean-Philippe Guye, Vocabulaire pratique d'analyse musicale, Delatour france, 2006, 128 p. (ISBN 978-2-7521-0020-7)André Calvet, Le Clavier Bien Obtempéré. Essai de Tempéramentologie, Montpellier, Piano e forte éditions, 2020, 470 p.PartielSpectre harmoniqueSérie de FourierJeux de mutations à l’orgueHarmonique artificielFondamentale manquanteFréquence fondamentale(fr) Site pédagogique pour voir et entendre les harmoniques sur un piano Portail de la musique   Portail de la physique"
musique;"La harpe est un instrument de musique à cordes pincées de forme le plus souvent triangulaire, muni de cordes tendues de longueurs variables dont les plus courtes donnent les notes les plus aiguës. C'est un instrument asymétrique, contrairement à la lyre dont les cordes sont tendues entre deux montants parallèles. L'instrumentiste qui joue de la harpe est appelé harpiste.Au début, il existait deux sortes de harpes : la harpe arquée et la harpe angulaire. Elle est, avec la flûte et certains instruments à percussion, l'un des plus anciens instruments de musique. Elle est peut-être née de l’arc musical dont la corde, tendue et relâchée, vibre et émet un son.L’origine de la harpe remonte à la Mésopotamie. Les premières harpes et lyres ont été trouvées à Sumer vers 3500 av. J.-C.. Plusieurs harpes ont été trouvées dans des sépultures et des tombes royales à Ur. Elle est connue des musiciens de l'Égypte antique, comme de Sumer (actuel Irak) et de Babylone. La harpe s'est répandue à travers les diverses civilisations et tous les continents sous des formes différentes.La harpe était un instrument universel : on la célèbre sur tous les continents et toutes les catégories sociales s'expriment à travers son art.En Europe, elle est signalée au sud-est de l'Écosse sur les pierres « pictes » aux alentours du IXe siècle apr. J.-C., et en Irlande pendant le haut Moyen Âge. Elle a alors pris sa forme moderne : triangulaire, apparemment posée sur la pointe, et dotée de la colonne qui relie la console (où s'accrochent les cordes) au bas de la caisse de résonance. Son usage se répand ensuite dans tout le continent.Le nombre de cordes et la forme variaient en fonction de l’évolution des civilisations, des besoins de la musique, de la technique de fabrication et de l'exigence d'inépuisables raffinements musicaux.La harpe médiévale reste immuablement diatonique, alors que le chromatisme envahit peu à peu la musique. À la Renaissance on utilise encore des harpes diatoniques (Gargantua de Rabelais apprend à jouer de la harpe). Mais le manque de chromatisme entraîne une désaffection de l'instrument au profit du luth et des instruments à clavier en train de naître. Pour pallier ce handicap, les luthiers italiens construisent la arpa doppia, la harpe double contenant deux rangées de cordes parallèles. C'est alors que, en 1697, un luthier bavarois, Hochbrücker, imagina un mécanisme qui, à l'aide de pédales permit d'effectuer certaines modulations. Cette harpe fut introduite en France en 1749. C'est une harpe à simple mouvement.C'est vers 1800 que le célèbre facteur de pianos, Sébastien Érard, invente le fameux mouvement à fourchettes qui va permettre à la harpe de rivaliser à nouveau avec les autres instruments chromatiques. Pour des raisons pratiques, en privilégiant de passer une pédale au lieu d'une autre, les harpistes ont souvent recours aux homophones ou notes enharmoniques. Pour répondre à ces critiques, en 1894, Gustave Lyon, directeur de la maison Pleyel, essaya de reprendre le principe des harpes chromatiques à double rangées de cordes croisées. Debussy composa pour cet instrument ses Danses sacrées et profanes. Le succès de cette harpe fut cependant de courte durée et à la mort de Gustave Lyon en 1936, elle disparut presque complètement de la vie musicale. Les danses sont maintenant jouées sur la harpe à pédales (double mouvement), car le système de fourchettes s'est considérablement amélioré. La harpe à pédales, ou harpe classique, est celle que l'on utilise dans les orchestres symphoniques et dans les formations de musique de chambre. C'est la harpe la plus sophistiquée.Elle possède de 40 à 46 cordes (pour les harpes d'étude) et 47 cordes (pour les harpes de concert), ce qui lui donne une tessiture de six octaves. Ces cordes sont principalement en boyau, à l'exception des cordes les plus graves (les deux dernières octaves) qui sont en métal, elles sont appelées cordes filées (filetage cuivre sur âme acier), les cordes les plus aiguës sont en nylon. Certaines harpes n'ont pas de cordes en boyau mais des cordes en nylon les remplacent, ce qui donne une autre sonorité à l'instrument ; les concertistes (et les instrumentistes) préfèrent souvent les cordes en boyau, qui donnent une sonorité plus « ronde » et franche, ce qui donne aussi une harmonie de matière à l'orchestre. Certaines cordes sont colorées pour permettre de repérer les notes principales : les do sont rouges et les fa sont noirs ou bleus. Les autres cordes sont incolores.La harpe à pédales peut être à simple mouvement ou à double mouvement. Dans les deux cas, on fait allusion au mécanisme reliant les pédales aux cordes pour en modifier la longueur et permettre de jouer les altérations musicales, c'est-à-dire les dièses et les bémols. Ces mécanismes ne font que réduire la longueur vibrante de la corde et n'en changent pas (idéalement) la tension.Sur une harpe double mouvement, inventée par Sébastien Érard en 1810, chaque corde peut jouer trois hauteurs : bémol si la pédale est relâchée (= en haut), bécarre si elle est bloquée sur le cran du milieu, et dièse si elle est tout à fait enfoncée.Il y a 7 pédales qui modifient les 7 notes de la gamme sur toutes les octaves. De gauche à droite, elles correspondent aux notes ré, do, si, mi, fa, sol, la pour la grande harpe. Les 3 premières pédales sont réservées au pied gauche, les 4 dernières au pied droit. Sur certains modèles, notamment sur les harpes Érard, une huitième pédale servait à actionner les volets de fermeture de la caisse de résonance. La harpe Erard de la photo ci-contre en possédait originellement (ouïes rectangulaires). Cette huitième pédale est appelé « pédale de renforcement ». L'ouverture ou la fermeture des volets changeaient la puissance du son propagé. Jean-Baptiste Krumpholtz a composé une sonate (Sonate dans le style pathétique Op. 14 N° 2) spécialement pour harpe à pédale de renforcement.La harpe à simple mouvement, tout comme la harpe celtique, ne permet que deux hauteurs par corde. L'invention de la harpe à simple mouvement est attribuée au facteur allemand Hochbrücker (1662/73 - 1763). On accorde la harpe à simple mouvement généralement en mi bémol majeur - toutes pédales relâchées - ce qui permet par la suite de jouer jusqu'à 3 bémols ou jusqu'à 4 dièses. Le nombre des tonalités est donc limité, mais le mécanisme, plus simple, permet la fabrication d'instruments moins coûteux.Inventée en 1894 par Gustave Lyon, directeur de la firme Pleyel, pour concurrencer la harpe diatonique à pédales, elle comporte deux plans de cordes croisés : un plan de cordes pour les bécarres, un plan pour les bémols et dièses. Elle permet l'exécution de tous les traits chromatiques avec une grande vitesse, mais contrairement à la harpe diatonique, elle ne permet pas les glissandi dans tous les modes et tonalités.Pour montrer les possibilités de l'instrument, la firme Pleyel commanda en 1904 une œuvre à Claude Debussy qui composa les Danses sacrée et profane pour harpe chromatique et orchestre à cordes. Mais cette œuvre est aussi jouable sur harpe diatonique, avec toutefois de très difficiles passages de pédales. Notons qu'André Caplet composa une première version de son Conte fantastique pour harpe chromatique et orchestre en 1908, intitulée Légende. Il adaptera ensuite l'œuvre pour harpe diatonique et quatuor à cordes en 1924.En riposte et afin de promouvoir les possibilités de la harpe diatonique, la firme Érard passa commande en 1905 d'une œuvre à Maurice Ravel qui composa l'Introduction et Allegro pour harpe avec accompagnement d'un quatuor à cordes, d'une flûte et d'une clarinette.Il avait été prévu une évolution de la harpe chromatique en y ajoutant des pédales, permettant ainsi à la fois les chromatismes rapides et les glissandi de la harpe diatonique. Cette harpe devait voir le jour en 1914, mais la Première Guerre mondiale mit fin au projet et la harpe chromatique tomba dans l'oubli progressivement dans les années d'après-guerre.Une classe de harpe chromatique a existé au Conservatoire national supérieur de musique de Paris de 1903 à 1933.Une classe de harpe chromatique a perduré au Conservatoire royal de Bruxelles jusqu'en 2005. Elle avait été ouverte en 1900, fermée en 1953, puis rouverte en 1978. Le départ à la retraite de Francette Bartholomée, titulaire de la classe de harpe au Conservatoire royal de Bruxelles (qui enseignait à la fois la harpe diatonique et la harpe chromatique) et son remplacement par un professeur qui ne pratique que la harpe diatonique a signifié la fermeture du Cours de harpe chromatique en 2005.Il est à noter qu'une association (Harpa Nova) a été nouvellement créée en Belgique à l'initiative de Vanessa Gerkens, une élève de Francette Bartholomée, pour soutenir l'enseignement de la harpe chromatique, promouvoir sa facture par de nouveaux luthiers (Pleyel ne construit plus de harpes chromatiques depuis 1930) et la sauvegarde des harpes Pleyel encore en existence. Un nouveau modèle de harpe chromatique de cinq octaves appelé « la Phoenix » a été récemment produit pour Vanessa Gerkens par le luthier français Marc Brûlé.Quelques citations irlandaises du XIIIe siècle :« tout gentilhomme doit avoir un coussin sur sa chaise, une femme vertueuse et une harpe bien accordée »« trois objets ne sont pas saisissables par voie de justice : le livre, la harpe et l'épée »La harpe celtique est un instrument central du monde celte ; plus que « traditionnelle », elle est une expression d'une culture classique celtique et, maintenant, d'une musique celtique contemporaine ; elle possède généralement 32 à 38 cordes. Elle est reconnaissable à son arc, toujours cintré. Les harpes celtiques cordées en métal que l'on trouvait au Moyen Âge, en Irlande et en Écosse notamment, sont toujours fabriquées et jouées de nos jours. Cet instrument médiéval qui se joue avec les ongles, a cependant fortement évolué, pour aboutir à ce qu'on peut appeler aujourd'hui les néo harpes celtiques, cordées en boyau ou nylon et qui se jouent avec la pulpe du doigt, ce qui implique une technique de jeu complètement différente qui se rapproche de la technique de jeu classique. Ce dernier type de harpe celtique sert parfois dans l'apprentissage de la harpe à pédales, sa taille la rendant plus accessible aux enfants, et son prix, plus accessible aux parents.De nos jours, les cordes sont le plus souvent en nylon dont la sonorité est un peu moins puissante et peut avoir une sonorité un peu « chinoise ». Les cordes en nylon ont en revanche l'avantage d'être moins sensibles aux changements de température et de casser moins souvent. Mais on trouve aussi des instruments montés en boyau (de mouton) ou encore en métal. Certaines cordes sont généralement colorées, comme pour la harpe à pédales, ce qui permet de repérer les notes de la gamme. Ainsi, les do sont rouges et les fa sont noirs ou bleus.Des taquets, crochets, clapets (ou palettes), fixés près de la partie supérieure de chaque corde, permettent de modifier la hauteur d'un demi-ton pour jouer les altérations (dièses/bémols). Ces clapets représentent en quelque sorte les touches noires d'un piano. On accorde généralement la harpe celtique en mi bémol majeur avec les taquets en position basse, ce qui permet ensuite de jouer dans les tonalités ayant jusqu'à quatre dièses ou jusqu'à trois bémols.La harpe celtique correspond à tout un répertoire, traditionnel ou savant, irlandais, écossais et, depuis les années 1950, breton. Mais elle s'adapte aussi à des répertoires classiques et contemporains (jazz, folk-rock, « world », électro-rock, pop, new age et métal). Elle accompagne idéalement le chant soliste. Sa petite taille en fait un instrument de choix pour débuter l'apprentissage de la harpe à pédales, bien qu'elle possède une technique de jeu propre, différente du jeu sur harpe classique. La plupart des instruments sont acoustiques mais il existe des harpes électro-acoustiques et purement électriques (cf Alan Stivell).Il existe également de petites harpes, pouvant être sanglées, dont on peut jouer debout et en se déplaçant. Traditionnellement, cette harpe dite bardique possède des cordes métalliques. Sa période de référence est le Moyen Âge, du Ve siècle au XVe siècle. Son répertoire s'oriente autour de la musique ancienne et traditionnelle celtique.On rencontre aussi un petit instrument moderne au son dynamique et brillant possédant le plus souvent 22 cordes en nylon, dans le registre aigu. Dite « harpe troubadour », elle fait référence aux musiciens qui utilisaient ce type d’instrument pour accompagner chants, danses et récits. Pour jouer des altérations sur cette harpe, il est nécessaire de l'accorder pendant le morceau, car elle ne possède pas de crochets.Cette harpe médiévale comporte deux rangs de cordes parallèles correspondant l'un aux notes naturelles et l'autre aux altérations (comme les touches blanches et les touches noires d'un clavier). Elle n'a rien à voir avec la grande harpe chromatique.On joue de cette harpe au Pays de Galles où c'est un instrument traditionnel. Son enseignement, hors académie, s'est transmis confidentiellement. Dans ce pays, elle bénéficie aujourd'hui d'un regain d'intérêt, avec des joueurs comme Llio Rhydderch ou Robin Huw Bowen qui interprètent un répertoire traditionnel tout en s'ouvrant à d'autres cultures musicales.Cette harpe est appelée harpe andine. Elle a été importée par les conquistadors au XVIe siècle. Elle possède 7 octaves et mesure 1,50 m de longueur et 76 cm de largeur.Cette harpe (arpa andina) est un instrument important du patrimoine musical péruvien et sur toute la Cordillère des Andes. Importée par les Conquistadores, elle a été adaptée pour répondre aux besoins d'expression musicale propres à la région. Elle est donc un produit du syncrétisme européo-andin, comme le charango par exemple. Notamment, sa caisse de résonance a été agrandie et la tension sur les cordes est plus faible que sur la harpe occidentale. Instrument diatonique, elle n'a pas de pédale. Essentiellement en bois, elle varie de taille et de forme selon les régions. Elle comporte de 32 à 38 cordes, et l'on trouve sur la même harpe aussi bien des cordages de nylon, de métal ou de boyaux (ces derniers, pour les notes basses seulement, sont en voie de disparition). Elle comporte un chevillier pour l'accordage, comme la guitare.La harpe andine est conçue pour être facile à transporter, et certains musiciens en jouent en marchant, pour accompagner les processions et les danses de carnaval. Elle est utilisée en instrument soliste, en accompagnement de petits ensembles ou avec de grands orchestres, essentiellement dans un registre de musique folklorique.Contrairement à la harpe occidentale qui utilise le système tonal, les musiciens andins jouent de la harpe en utilisant un système pentatonique. Elle est largement utilisée dans la musique vernaculaire du Pérou, surtout pour les genres de huayno des cordillères centrale et sud.Harpe des Andes                   Proche de la précédente, et donc à mi-chemin en taille entre la harpe des Andes et la harpe européenne, on trouve la harpe paraguayenne. Celle-ci comporte de 32 à 46 cordes (généralement 36), en nylon, qui sont également réparties autour de la ligne médiane de la tête de harpe : ainsi les forces de tension sont équilibrées, et la fabrication de ce type de harpe est moins lourde que celle des autres types. Elle aussi comporte un chevillier pour l'accordage.Harpe paraguayenne                Certainement l'une des plus anciennes formes de harpe connue, répandue en Égypte ancienne et en Asie. La caisse de résonance forme avec la console un angle plus ou moins droit qui n'est pas fermé par un joug.La harpe chinoise ou konghou : cette harpe a disparu depuis des siècles alors qu'elle avait pourtant une position importante en Chine.La harpe iranienne ou chang : cette harpe fut aussi prééminente en Iran mais a disparu depuis trois siècles.La harpe turque ou çeng : disparue elle aussi depuis deux siècles.la harpe géorgienne ou changi.Appelées aussi « harpes coudées », elles sont aussi fort anciennes et leurs conceptions assez rudimentaires.La harpe birmane ou saung : datant du VIIIe siècle, c'est l'une des plus anciennes harpes encore jouée de nos jours. C'est un instrument rare et précieux, très ornementé et réservé autrefois à la Cour.La harpe arquée des Khantys (Ostiaks).Les harpes africaines : C'est le type de harpe le plus courant rencontré sur le continent africain. Les harpes arquées sont fort nombreuses et réparties sur plusieurs ethnies et pays. Elles se déclinent en divers types elles-mêmes : naviforme, anthropomorphe, semi-ovoïde, ovale, rectangulaire, etc. Elles accompagnent le chant.Ce sont des hybrides de harpe arquée et de harpe angulaire avec deux ou plusieurs manches. On en rencontre en Nouvelle-Guinée et en Afrique. Il ne faut pas les confondre avec les lyres dont le sens des cordes est différent.On en rencontre plusieurs variétés en Afrique noire. Comme leurs noms l'indiquent, il s'agit de harpes hybrides ayant la forme et les caractéristiques d'un luth, mais utilisées avec une technique de jeu de harpe.Cet instrument hybride d'Afrique subsaharienne se présente sous la forme d'une variété d'arc musical ou de cithare mais avec des éléments propres à la technique de jeu de la harpe, notamment grâce à un haut chevalet similaire à celui des harpes-luths. Précisons que le terme de cithare ne convient pas à la harpe angulaire d'Afrique centrale qui se joue droite et verticale et non à plat sur les genoux comme se joue une harpe chromatique ou cithare.Il existe depuis deux siècles une multitude de variétés de guitares ornées d'un second manche non fretté et dont on joue en partie avec une technique de harpe.Harpe classiqueVoir la Catégorie:Harpiste classique.Harpe celtiqueVoir dans l'article Harpe celtique.Clotilde Cerdà i Bosch (1861-1926), jeune prodige de la harpe, connue par son nom de scène d'Esmeralda Cervantes avec le soutien de Victor Hugo.Charnassé Hélène et Vernillat France, Les instruments à cordes pincées - PUF Paris, 1970Élisabeth et Rémi Chauvet et alii (Myrdhin, Alan Stivell, Dominig Bouchaud, Tristan Le Govic…), Anthologie de la harpe : La harpe des Celtes, éditions de la Tannerie, avec un CD audio et un historique de la harpe.Alan Stivell et Jean-Noël Verdier,  Telenn, la Harpe Bretonne, éditions Le Télégramme.Michel Faul, Nicolas-Charles Bochsa : harpiste, compositeur, escroc, éditions Delatour 2003.  (ISBN 2-7521-0000-0).Michel Faul, Les tribulations mexicaines de Nicolas-Charles Bochsa, harpiste, éditions Delatour 2006.  (ISBN 2-7521-0033-7).Christine Y Delyn, dessins de Denis Brevet, Clairseach, la harpe irlandaise : aux origines de la harpe celtique, éd. Hent Telenn Breizh, 1998. Ouvrage de référence, abondamment illustré, sur l'histoire de la harpe irlandaise ancienne, et son rôle dans la civilisation gaélique, 175 p.H. Avelot, « L'art et la mode chez les Pahouins », in Arts d'Afrique noire, Villiers le Bel, 2001.Roslyn Rensch, The Harp, Its History, Technique and Repertoire,Harpes d'AfriqueLes harpes du Musée de la musique (Paris) Portail de la musique classique   Portail des musiques du monde"
musique;"En musique, la hauteur est l'une des caractéristiques essentielles d'un son ou note, les autres étant la durée, l'intensité, le timbre et l'expression.En musique occidentale, on désigne la hauteur par le nom d'une note sur une échelle ou gamme. La hauteur relative d'une note dans la gamme s'appelle degré. Le solfège écrit vers le haut de la portée les notes aiguës, et inversement, vers le bas, les graves. On écrit les signes correspondant aux instruments qui n'ont pas de hauteur précise, bien qu'ils puissent sonner aigu ou grave, comme les tambours, sur une ligne horizontale quelconque, l'important étant leur placement dans le temps, noté de gauche à droite.La perception des hauteurs est un des champs d'investigation de la psychoacoustique. La tradition musicale et les études de psychologie expérimentale convergent pour analyser cette perception en deux composantes distinctes, provenant de deux systèmes physiologiques différents. La perception de la hauteur spectrale place le son entendu assez approximativement sur une échelle entre graves et aiguës. La perception de la hauteur tonale rend capable de différencier deux sons harmoniques proches, indépendamment de leur richesse en graves et en aiguë.Dépendant de capacités d'interprétation et sujette à l'apprentissage, la perception des hauteurs apparaît comme une fonction cognitive.La sensation de hauteur est très bien partagée. Presque tout le monde peut reconnaître et chanter un air dont le contour mélodique n'est ni trop subtil, ni trop accidenté. Les médecins traitent l'incapacité à le faire comme un handicap, qu'ils appellent amusie. Cependant, le discernement des hauteurs et l'identification des notes est une capacité cognitive. Les musiciens la cultivent en s'entraînant à identifier une note à partir du son, dans un exercice appelé dictée musicale.Ils développent aussi leur capacité de discriminer des sons justes, c'est-à-dire dont la hauteur est précisément celle qu'exige la gamme dans laquelle la musique se joue. Cette capacité est particulièrement importante pour les musiciens qui pratiquent un instrument comme le violon, capable d'émettre un son dans un domaine continu de hauteurs. La notion de justesse est cependant dépendante des usages et de la culture, et ne peut guère être valide que lorsque le contexte est bien déterminé.On appelle intervalle l'écart de hauteur entre deux notes. Si elles sont émises simultanément, on parle d'intervalle harmonique ; si elles sont émises successivement, on parle d'intervalle mélodique.L'intervalle nul s'appelle l'unisson ; on parle d'unisson quand deux instruments ou deux voix émettent simultanément une note de même hauteur.L'octave est un intervalle de statut particulier dans toutes les cultures musicales. Dans la notation musicale occidentale, le nom des notes se répète à chaque octave, et on doit préciser, en cas d'ambiguïté, à quelle octave on se réfère. Le principe de l'identité des octaves indique que deux notes séparées par une octave sont équivalentes, et correspondent en pratique à une unisson.L'identification précise des intervalles entre notes, qu'on appelle oreille relative, est plus répandue que l'identification des fréquences en elles-mêmes, qu'on appelle oreille absolue. Cette dernière capacité exige probablement une disposition physiologique innée, et à coup sûr un entraînement commencé à un âge précoce.L'échelle des hauteurs des notes de musique se base sur l'octave. L'octave correspond à une division par deux de la longueur de l'élément vibrant qui produit le son, qu'il s'agisse d'une corde ou d'un tuyau. L'acoustique enseigne que cette division par deux correspond à une multiplication de la fréquence fondamentale par deux.Par exemple, pour un tuyau d'orgue :le tuyau d'orgue à bouche ouverte le plus long, do1, mesure 2,599 m ;un tuyau moitié moins long, de 1,30 m, donne un do à l'octave supérieure, dit do2 ;diviser encore la longueur par deux donne 0,65 m, et toujours do, dit do3 ;diviser encore la longueur par deux donne 0,325 m, et toujours do, dit do4.L'échelle des hauteurs est en progression arithmétique, quand la fréquence et les paramètres qui la gouvernent sont en progression géométrique, ce qui fait d'elle une échelle logarithmique.L'intervalle d'une octave est divisé en six tons ou douze demi-tons. Dans les systèmes modernes, tous les demi-tons tempérés sont égaux, et correspondent à une multiplication de la fréquence de la note par la même valeur. Pour des analyses plus fines, les notations du solfège peuvent être remplacés par les savarts ou les cents.La musique occidentale se base sur une théorie de la tonalité, qui privilégie certains rapports de hauteur. Les noms des notes se répètent à chaque octave. Dans une octave, sept niveaux de l'échelle des douze demi-tons correspondent à une note qui a un nom. Les autres en dérivent par une altération.Si les musiciens pensent les intervalles de façon linéaire, suivant la notation du solfège, les théoriciens de la musique rapportent les notes à des fréquences de vibration. Pour passer de l'échelle musicale à la fréquence, il suffit de connaître la fréquence d'une note. Les autres s'en déduisent par calcul.La Conférence internationale de Londres en 1953 donne comme référence, généralement admise, le la3 à 440 hertz.Dans la gamme tempérée, on peut calculer la fréquence des notes avec la formule suivante :                    f        =        55        ×                  2                      o            c            t            a            v            e            +                                                            d                  e                  m                  i                  t                  o                  n                  ?                  9                                12                                                          {\displaystyle f=55\times 2^{octave+{\frac {demiton-9}{12}}}}  où octave et demiton sont des nombres entiers, correspondant à la note, de do (demiton=0) à si (demiton=11).Le tableau ci-contre donne les fréquences des notes dans l'octave du la de référence (octave 3). Il faut multiplier les fréquences par 2 pour une octave au-dessus, et les diviser par 2 pour une octave en dessous. La colonne de droite indique l'écart relatif par demi-tons, correspondant à l'intervalle musical.Cependant, ces calculs ne concernent que les notes de la musique occidentale, dans sa version moderne. Ils ne s'appliquent qu'à la gamme à tempérament égal et ne différencient pas les demi-tons diatoniques et chromatiques (voir aussi « Comma »).La hauteur relative d'une note de musique est l'intervalle qui la sépare d'une note considérée comme la base. Le changement de la note de base conserve les intervalles et les mélodies. Quand des musiciens réunis n'ont pas de diapason qui leur donne le la, ils peuvent jouer en se basant sur un la estimé.Dans la musique tonale, on indique le degré qu'occupe la note sur les sept que comporte l'échelle musicale. Le premier degré est la tonique et désigne la tonalité. Les degrés sont définis en termes de hauteur nominale, indépendamment de l'octave (ou du registre) où on peut les trouver.Quand les musiciens effectuent une transposition musicale, ils changent délibérément la note de base (la tonique), tout en conservant tous les intervalles (ce qu'on appelle le contour mélodique).Dans la musique tonale, la hauteur de référence permettant d'apprécier une hauteur relative, peut être :dans le cas d'un intervalle mélodique, soit la note précédente, soit la tonique ;dans le cas d'un intervalle harmonique, soit la basse, soit la fondamentale de l'accord en cours.La hauteur correspond à une sensation, c’est-à-dire à un phénomène psychoacoustique lié à une cause physique. Dans le contexte des études psychoacoustiques, qui ne concernent pas que la perception des sons musicaux, cette perception peut être appelée tonie.Le son est une vibration de l'air. Lorsque cette vibration est un périodique, c'est-à-dire qu'elle se répète identique à elle-même pendant un cycle d'une durée toujours égale appelée période, cette période, ou la grandeur inverse, la fréquence, en est un caractère d'autant plus important que le mathématicien Joseph Fourier a montré au début du XIXe siècle que toute fonction périodique, représentant numériquement un phénomène, peut s'analyser en une somme de sinusoïdes, dont les fréquences sont des multiples de la fréquence du phénomène périodique. On appelle son pur un son comportant une seule fréquence, et donc décrit par une fonction sinus. On savait déjà que la vibration des cordes et des colonnes d'air qui sont à la base des instruments de musique comportent des vibrations harmoniques ; Fourier montre que tout son peut se décomposer en une somme de sons purs.Toutefois, ce calcul vaut pour des sons qui s'étendent à l'infini, tant dans le passé que dans l'avenir. Mais une note de musique a une durée. La poursuite de l'analyse montre que plus la durée considérée est courte, moins la détermination de la fréquence est précise. Le produit de la durée par l'incertitude sur la fréquence est une constante, dans un système donné. Quand la durée diminue, l'incertitude augmente.Il faut garder à l'esprit cette incertitude quand on raisonne sur la musique. La musique est faite de sons changeants. Même l'instrument au son le plus régulier, comme l'orgue, a un temps d'attaque et une vibration interne, ne serait-ce que parce qu'il est joué et entendu dans un espace réverbérant. Par conséquent, en toute rigueur aussi bien qu'en pratique, la détermination d'une hauteur est une opération intellectuelle d'abstraction, et non une mesure.Les musiciens de la culture occidentale ont, depuis des siècles, accordé une importance privilégiée à la hauteur du son. Cette importance se reflète dans l'enseignement de la musique et dans la facture instrumentale. Ils ont aussi, souvent, voulu se rattacher philosophiquement à la science et aux mathématiques. Il faut donc, lorsqu'on étudie la question de la perception de la hauteur, examiner avec un soin critique le legs de la tradition musicale, qui reprend, dans ses principes, des opinions que l'on fait remonter à l'antiquité et à Pythagore en particulier.Notion intuitive pour les musiciens qui s'entraînent à en affiner leur perception dès le début de leur apprentissage, et apprennent au passage, en quelques lignes, une doctrine qui la rattache aux fréquences, la hauteur tonale n'est pas facile à définir rigoureusement.La hauteur d'un son pur correspond à sa fréquence de vibration, que l'on mesure en hertz (nombre de vibrations périodiques par seconde). Plus la vibration est rapide, plus le son est dit aigu ou haut ; plus la vibration est lente, plus le son est dit grave ou bas,.Si un son pur est une vibration sinusoïdale, avec une seule fréquence, les sons des notes musicales sont des sons complexes, qu'on peut décomposer en vibrations à plusieurs fréquences. Lorsque ces fréquences sont réparties selon une règle de distribution harmonique, que la fréquence fondamentale est dans le domaine audible et que leur enveloppe coïncide, un seul son est entendu avec une seule hauteur, correspondant à la fréquence fondamentale, même si elle n'est pas la plus forte, et que le son ne présente que ses multiples.Un son composé de multiples d'une même fréquence se perçoit comme à la hauteur de cette fréquence, même si celle-ci n'est pas présente dans ce mélange. On appelle ce paradoxe « perception de la fondamentale absente ».Exemple :un son pur de fréquence 110 Hz s'évalue à la hauteur d'un la1 ;le mélange des fréquences 3 × 110 = 330 Hz ; 4 × 110 = 440 Hz ; n × 110, s'évalue à la hauteur d'un la1, bien que toutes les fréquences soient largement supérieures.Parmi les instruments de musique, le basson a la propriété d'émettre des sons graves alors que la puissance sonore se trouve presque entièrement dans les harmoniques. Les facteurs d'orgue utilisent cette perception pour créer des notes basses avec des couples de tuyaux plus courts.L'échelle de Mel, proposée en 1937, montre que si l'on demande à des auditeurs de situer les sons purs successifs, en recherchant, pour un son de base, celui qui sonne à un intervalle d'une octave, l'écart de fréquence trouvé augmente significativement avec la fréquence.Cette augmentation se combine avec des facteurs mécaniques d'inharmonicité dans l'accordage inharmonique du piano.L'écart de fréquence entre les sons purs qu'on puisse à peine distinguer varie aussi selon la fréquence et l'intensité. Il est minimal dans la région des 1 500 Hz (octave 6). En tout, l'être humain peut différencier environ 1 800 fréquences de sons purs. Cette performance correspond à la capacité de distinguer deux sons de fréquence proche. Elle ne signifie pas que les sujets soient capables de situer la fréquence sur une échelle.La reconnaissance de la hauteur est la plus précise pour des sons d'une durée au-delà de une demi-seconde pour l'homme. Dans ces conditions, elle est de l'ordre de 1 savart (ou 5 à 10 cents) pour les fréquences les mieux discriminées, vers 1 500 Hz. Pour des sons plus brefs, le seuil de différenciation augmente. Un son très bref n’a pas de hauteur définie ; on parle de « claquement ».La hauteur perçue dépend faiblement de l'intensité. Stanley Stevens a montré qu’un son paraît baisser quand on augmente son volume pour les fréquences inférieures à 2 000 Hz. Pour des fréquences supérieures à 3 000 Hz, un accroissement d’intensité élève la hauteur perçue. Heureusement, ce phénomène ne concerne que les sons purs, et il n’affecte donc pas les instruments de musique[réf. souhaitée].Pour les sons qui n'ont pas de fréquence fondamentale audible, et donc ne peuvent s'associer à une note de musique, comme ceux des cymbales, on distingue des registres musicaux, dont les plus élémentaires sont le registre grave et le registre aigu ; entre les deux, on parle de registre médium.La perception du registre est indépendante de l'existence d'une note fondamentale. On ne peut attribuer une note à un tambour, mais on distingue un tambour grave d'un tambour aigu.D'autre part, les auditeurs identifient comme différents deux sons harmoniques stables, de même fondamentale mais de spectre sonore différents. On dit de celui dont les harmoniques aiguës sont plus puissantes qu'il a un timbre plus brillant, alors que les deux sons correspondent à la même note.Les études psychoacoustiques semblent confirmer qu'il s'agit de deux perceptions auditives distinctes, appelées la tonie ou hauteur brute, pour la hauteur spectrale, et le chroma, ou hauteur fondamentale ou nominale pour la capacité à distinguer des notes (avec parfois une erreur d'octave).La séparation de la variation de la hauteur tonale (par déplacement de la fondamentale) et de celle de la hauteur spectrale (par modification de l’enveloppe spectrale) dans un procédé de synthèse sonore permet de créer des variations de hauteurs paradoxales engendrant des effets inattendus, comme celui de la Gamme de Shepard.La résolution de la hauteur spectrale serait d'environ une tierce majeure, soit un rapport de l'ordre de 1 à 1,25. Le chroma, qui n'existe que pour les sons nettement périodiques, permet une résolution qui atteint 1/300 d'octave et donc l'identification des notes, mais il perd progressivement sa précision à partir de 2 000 Hz (do 6) et ne donne aucune indication au-delà de 4 500 Hz. La note la plus aiguë du piano est le do 7, aux alentours de 4 200 Hz, compte tenu de l'inharmonicité.Il serait bien agréable à l'esprit que ces sensations fussent indépendantes. Cela n'est pas tout à fait le cas. Diana Deutsch a fait écouter à des sujets des sons de Shepard, mélange de toutes les octaves audibles d'une fréquence musicale, écartés d'exactement trois tons, soit une demi-octave. Les sujets ont nettement désigné certaines notes comme plus aigües. Si les sensations de hauteur spectrale et de hauteur fondamentale étaient indépendantes, les auditeurs n'auraient pu choisir entre trois tons montants et trois tons descendants. Il s'est avéré, de plus, que le son de Shepard désigné comme le plus aigu varie d'une personne à l'autre, et se trouve corrélé avec le dialecte parlé par les sujets participants. Un tutti d'orchestre, avec de nombreux instruments jouant la note à des octaves différentes, présente des similitudes spectrales avec un son de Shepard. L'expérience montre que le principe de l'équivalence perceptive d'une transposition n'est pas universel, et que les sujets ont en général une certaine forme d'oreille absolue.Dès lors qu'on considère que la perception des hauteurs est une capacité cognitive, les conditions de la perception, qui fournissent au sujet des informations sur ce qu'il est censé percevoir, entrent en jeu. L'écoute est dirigée par une intention. C'est une critique récurrente faite aux expériences perceptives menées en laboratoire : elles conditionnent la sensation,.La fréquence des sons musicaux que nous entendons, à la différence de ceux du laboratoire, fluctue. La tenue de l'archet du violon, le souffle dans les instruments à vent, influencent irrégulièrement la fréquence. Pourtant, nous attribuons sans difficulté une hauteur, juste ou non, à ces sons. Des variations de fréquences dont la physique nous dit qu'elles sont de même proportion, sur un enregistrement de piano, nous semblent insupportables. La reconnaissance d'une hauteur dépend donc partiellement de l'identification préalable de la source.Les sons musicaux sont joués en séquence. L'expérience montre que les musiciens n'évaluent pas la hauteur juste à la même fréquence selon l'intervalle qui précède. S'il s'agissait d'un processus purement physiologique, celle-ci devrait correspondre toujours à la même vibration physique ; mais elle diffère notamment selon que la mélodie suit un mouvement ascendant ou descendantEnfin, l'échelle musicale exerce un effet d'attraction. Dans un contexte musical, l'évaluation des hauteurs est plus précise aux alentours de ses degrés.Une alternative aux expériences de psychologie expérimentale avec des sons fabriqués exprès consiste à mesurer, grâce aux ordinateurs, les hauteurs statistiquement présentes dans une exécution musicale considérée comme représentative ou exemplaire, impliquant des instruments capables d'une variation de l'accord, comme les violons et la voix humaine. Cette mesure met en évidence une variation du diapason corrélée avec l'interprétation.D'autre part, la qualité de timbre d'un instrument de musique, et celle de la voix humaine, inclut le vibrato, qui est une variation périodique perceptible de la hauteur. En physique, cette vibration équivaut à la combinaison de deux fréquences, de part et d'autre de celle de la note produite, dont la différence est égale à la fréquence de vibration. Lorsque cette vibration n'est que transitoire dans l'exécution musicale, elle n'affecte pas la perception de la justesse, qui est une perception de la hauteur, mais celle du timbre.La perception de la hauteur, orientée par ce que l'auditeur sait de la situation, résumée dans l'expression « scène auditive », ne se résume pas aux résultats obtenus par des expériences de laboratoireSi l'on excepte les musiques décoratives dont l'objet est de masquer des sons désagréables, en restant aussi peu remarquables que possible, l'écoute de la musique, comme celle de la parole, implique une attention active et dirigée vers ce que l'auditeur identifie comme éléments pertinents. Si la parole utilise, économiquement, une faible partie des capacités de perception et de discrimination des sons, la musique, joue avec l'ensemble de ces capacités. La reconnaissance d'une modulation des hauteurs est, dans toutes les cultures, un signe distinctif de l'écoute musicale. Dans la culture occidentale, les auditeurs, même non musiciens, sont imprégnés d'un système d'organisation des hauteurs divisées en notes, dont la succession paraît, ou non, bien formée, de la même façon que, pour un locuteur non grammairien, un énoncé paraît, ou non, bien formé.Claude Abromont et Eugène de Montalembert, Guide de la théorie de la musique, Librairie Arthème Fayard et Éditions Henry Lemoine, coll. « Les indispensables de la musique », 2001, 608 p. [détail des éditions] (ISBN 978-2-213-60977-5)Michèle Castellengo, « 6. Perception de la hauteur », dans Présentation des recherches - 1963-2002, Paris, Laboratoire d'acoustique musicale, 2002 (lire en ligne)Claude-Henri Chouard, L'oreille musicienne : Les chemins de la musique de l'oreille au cerveau, Paris, Gallimard, 2001, 348 p. (ISBN 2-07-076212-2).Charles Delbé, Musique, psychoacoustique et apprentissage implicite : vers un modèle intégré de la cognition musicale, Université de Bourgogne, 2009 (lire en ligne)Laurent Demany, « Perception de la hauteur tonale », dans Botte & alii, Psychoacoustique et perception auditive, Paris, Tec & Doc, 1999 (1re éd. 1989).(en) Hugo Fastl et Eberhard Zwicker, Psychoacoustics : Facts and Models, Springer, 2006 (ISBN 978-3-540-23159-2)Alain Goyé, La Perception Auditive : cours P.A.M.U., Paris, École Nationale Supérieure des Télécommunications, 2002, 73 p. (lire en ligne)Stephen McAdams (dir.) et Emmanuel Bigand (dir), Penser les sons : Psychologie cognitive de l'audition, Paris, PUF, coll. « Psychologie et sciences de la pensée », 1994, 1re éd., 402 p. (ISBN 2-13-046086-0)Stephen McAdams, « Introduction à la cognition auditive », dans McAdams & alii, Penser les sons, Paris, PUF, 1994Albert S. Bregman, « L'analyse des scènes auditives : l'audition dans des environnements complexes », dans McAdams & alii, Penser les sons, Paris, PUF, 1994Emmanuel Bigand, « Contributions de la musique aux recherches sur la cognition auditive humaine », dans McAdams & alii, Penser les sons, Paris, PUF, 1994Gérard Pelé, Études sur la perception auditive, Paris, L'Harmattan, 2012.Pierre Schaeffer, Traité des objets musicaux : Essai interdisciplines, Paris, Seuil, 1977, 2e éd. (1re éd. 1966), 713 p.Arlette Zenatti et al., Psychologie de la musique, Paris, Presses universitaires de France, coll. « Psychologie d'aujourd'hui », 1994Michèle Castellengo, « La perception auditive des sons musicaux », dans Arlette Zenatti, Psychologie de la musique, Paris, Presses universitaires de France, coll. « Psychologie d'aujourd'hui », 1994.Diana Deutsch, « La perception des structures musicales », dans Arlette Zenatti, Psychologie de la musique, Paris, Presses universitaires de France, coll. « Psychologie d'aujourd'hui », 1994.W. Jay Dowling, « La structuration mélodique : perception et chant », dans Arlette Zenatti, Psychologie de la musique, Paris, Presses universitaires de France, coll. « Psychologie d'aujourd'hui », 1994.Acoustique musicalePsychoacoustiqueOreille relative et oreille absolueTessitureAmbitusRegistre (musique)Système tonalCircularité de hauteur sonore Portail de la musique classique   Portail de la musique"
musique;"Un instrument de musique est un objet pouvant produire un son contrôlé par un musicien — que cet objet soit conçu dans cet objectif, ou bien qu'il soit modifié ou écarté de son usage premier. La voix ou les mains, même si elles ne sont pas des objets à proprement parler, sont considérées comme des instruments de musique dès lors qu'elles participent à une œuvre musicale.Hector Berlioz commence son Traité d'instrumentation et d'orchestration (1843) en déclarant que « tout corps sonore mis en œuvre par le compositeur est un instrument de musique ».L'ensemble des instruments utilisés pour une œuvre mais aussi et surtout dans une société donnée ou une époque est appelé « instrumentarium ». L'étude académique des instruments de musique est appelée organologie et prend le plus souvent ses sources dans l'ethnomusicologie.Les plus vieux instruments de musique connus, sont des flûtes à encoche de type quena à 5 trous datant d'au moins 35 000 ans. Elles ont été retrouvées dans des grottes du Jura Souabe, région située au sud-ouest de l'Allemagne. Dans la mesure où ces flûtes sont déjà techniquement évoluées et si on se base sur la prise en main complexe des quenas modernes, elles impliquent très certainement un savoir-faire musical bien antérieur.Tous les instruments de musique dont on a retrouvé la trace archéologique jusqu'à aujourd'hui sont le fait d'Homo sapiens.Il existe plusieurs lieux dans le monde dans lesquels des instruments de musique ont été trouvés ; par exemple, un triton nodifer a été trouvé à la Font Aux Pigeons (Châteauneuf-les-Martigues). On sait qu'il servait de trompe dans les régions de Grèce aux bergers pour appeler les troupeaux. Des flûtes percées paléolithiques ont aussi été trouvées au Pays basque dans la grotte d'Isturitz. Un autre instrument, le rhombe (instrument à vent) pouvait être en os, en bois de cervidé, en ivoire ou en bois. Cet instrument, à la forme foliacée, avait des extrémités percées ; ces perforations permettaient d'attacher un objet et de le faire tournoyer afin d'obtenir un son plus fort.Par son mode d'attaque, par la forme et la matière de sa caisse de résonance, par le soutien ou non de sa vibration, l'instrument de musique détermine le timbre — l'une des quatre caractéristiques du son avec la hauteur, la durée et l'intensité. Les progrès de l'acoustique musicale permettent de mieux comprendre les composantes du spectre harmonique spécifique à chaque source sonore.Un instrument de musique comporte souvent deux parties distinctes[réf. nécessaire] :celle qui crée la vibration ;celle qui transforme cette vibration en un timbre qui caractérise cet instrument.Peu importe leur matière, les instruments sont classés par leur méthode de production du son : l'organologie est l'étude détaillée de ces outils faiseurs de musique et de leurs différentes catégorisations. Le timbre de ces instruments peut être parfois transformé par un accessoire comme les sourdines pour les cordes et les cuivres, ou un kazoo pour la voix.Pour un son donné, la vibration peut provenir d'une corde, d'une colonne d'air ou d'une percussion ; des instruments peuvent combiner plusieurs systèmes, les plus récents vont de l'électromécanique jusqu'au virtuel.Les instruments à cordes sont également appelés « cordophones ».De matière, de grandeur et de grosseur variées, les cordes peuvent être frottées, pincées ou frappées. La classification traditionnelle distingue par conséquent :les instruments à cordes frottées, comme le violon, la trompette marine ou la vielle à roue ;les instruments à cordes pincées, comme la guitare, le clavecin ou la harpe ;les instruments à cordes frappées, comme le piano ou le cymbalum.Les instruments à vent, également appelés « aérophones », mettent en jeu une colonne d'air. Celle-ci peut être produite par le souffle du musicien, par une soufflerie mécanique ou par une poche d'air. On distingue :la voix, qui exploite toutes les possibilités des membranes muqueuses du larynx (cordes vocales) ;les bois, qui comportent un biseau ou une anche :les instruments à biseau, comme toutes les flûtes ou les jeux à bouche d'orgue,les instruments à anche libre, comme les harmonicas ou les accordéons,les instruments à anche simple, comme les clarinettes ou les saxophones,les instruments à anche double, comme les hautbois ou les bombardes,les cuivres, qui utilisent la vibration des lèvres dans une embouchure, comme les trompettes, les cornets à bouquin ou le didgeridoo, ainsi que les cors, les trombones et les tubas.N.B. Contrairement à ce que cette classification pourrait laisser penser, ce n'est pas la matière utilisée dans la facture instrumentale qui est déterminante, mais bien la manière de produire le son. Ainsi, s'il existe des flûtes et des clarinettes en métal et en bois, toutes font partie des « bois ». Le saxophone construit en cuivre est un « bois » car il est muni d'un bec à anche battante. Il existe également des « cuivres » fabriqués en bois, comme les cornets à bouquin et le serpent, et à l'origine, le cor est un olifant en corne ou fabriqué dans une défense d'éléphant.Les instruments de percussion — à hauteur déterminée ou non — englobent tout instrument par lequel un corps en frappe un autre. Cette catégorie d'instruments a été subdivisée par les théoriciens en membranophones et idiophones. Le développement de cette famille nombreuse au XXe siècle (plus de 500), particulièrement des instruments à claviers ou à lamelles, a imposé une nouvelle catégorisation autant pour les percussionnistes que pour les enseignants. À l'orchestre ou dans les classes de percussion, la distinction est faite entre :les claviers, constitués d'une série de lames accordées en bois ou en métal, frappées par des baguettes (comme le xylophone ou le steel drum) ou par l'intermédiaire d'un clavier (célesta) ;les peaux, naturelles ou synthétiques, elles sont constitués d'une membrane frappée par les mains ou par des baguettes, accordée ou non, comme le djembé ou la timbale ;les accessoires, c'est-à-dire toutes les autres percussions ne produisant généralement qu'un son, du triangle aux maracas en passant par les claves ou la crécelle.Les instruments de combinaison associent plusieurs modes de mise en vibration. On distingue :les instruments mécaniques, comme la serinette ou l'orgue de Barbarie ;le claviorganum, combinant orgue et clavecin actionnés par le(s) même(s) clavier(s).la Marble Machine, créée par le groupe Wintergatan, combinant guitare basse, vibraphone, cymbale ainsi que des percussions émulées à l'aide de microphones de contact, actionnée par des billes ou directement à la main. L'énergie est fournie par le musicien via une manivelle, et stockée dans un volant d'inertie. Un programmateur mécanique et des embrayages permettent au musicien d'activer des boucles « pré-enregistrées » sur chacun des instruments.Les instruments électromécaniques, comme l'orgue Hammond ou le Yamaha CP80.Les instruments de musique électronique, comme le Thérémine et les ondes Martenot.Les instruments électroanalogiques, comme le Moog Micromoog (synthétiseur analogique) ou les Yamaha DX7 et Roland AX-Synth (synthétiseurs numériques portables).Les instruments virtuels de l'Atari au Macintosh.Le Musikinstrumenten-Museum, musée des instruments de musique à Berlin ; il rassemble environ 3500 instruments.Le Musée des Instruments de musique (MIM), créé à Bruxelles en 1877, réunit dans les locaux d'un superbe immeuble Art nouveau une collection de plus de 8 000 instruments : instruments occidentaux mécaniques, électriques et électroniques, instruments traditionnels européens, instruments du monde.En 1999, fut ouvert à Ouagadougou (Burkina Faso), le Musée de la musique qui réunit une collection d'instruments de musique traditionnels des terroirs ethnoculturels du pays.Le musée des Beaux-Arts de Chartres abrite un instrumentarium, dont les représentations dans la cathédrale Notre-Dame sont au nombre de 320 pour 26 instruments différents. La pratique de ces instruments a donné lieu à l'enregistrement de deux albums,, ;Le musée des Instruments à vent, à La Couture-Boussey, centre de facture d'instruments à vent depuis le XVIIe siècle ;Le Musée de la Musique, à Paris, fait partie de la Cité de la musique.Le Musée national des instruments de musique de Rome ouvert à Rome en 1964, rassemble une collection exceptionnelle de 3 000 instruments, de l'Antiquité jusqu'à nos jours, couvrant tous les genres musicaux.Le Musée de la musique de Bâle (Musikmuseum en allemand) situé dans l’ancienne prison Lohnhof depuis l’an 2000.Le Musée national de la musique (?eské muzeum hudby) à Prague est installé dans l’ancienne église Sainte-Marie-Madeleine de style baroque. Il est situé à Malá Strana. Plus de 400 instruments de musique d'époque y sont exposés.* Liste des instruments de musiqueListe de fabricants d'instruments de musiqueHistoire de la catégorisation des instruments de musiqueInstrument de musique en bambouRessource relative aux beaux-arts : (en) Grove Art Online Musée virtuel avec extraits sonores et photosListe exhaustive et illustrée des instruments de musiqueInstruments du monde avec extraits sonores, explications et photosUstensiles & Instruments, Le ventre et l'oreille n°3, 2019, (ISSN 2650-3050). Portail de la musique   Portail des arts                    "
musique;"Un instrument à cordes est un instrument de musique dans lequel le son est produit par la vibration d'une ou plusieurs cordes. L'organologie les classe dans la catégorie des cordophones. L'histoire des instruments à cordes est vieille de plusieurs milliers d'années. Les premiers n'avaient probablement qu'une seule corde, comme l'arc musical. Dès l'Égypte ancienne, on connaissait les joueurs de harpe. Au Moyen Âge, les ménestriers s'accompagnaient au luth, etc.La vibration de la corde seule est peu audible. Une plaque couplée aux cordes, la table d'harmonie, prélève une partie de l'énergie vibratoire de la corde pour la transmettre à l'air et obtenir un son. La table d'harmonie peut être une peau de tambour, comme dans le banjo ou la kora.La fréquence fondamentale de la vibration dépend des caractéristiques de longueur, de masse et de tension de la corde. Les cordes ont subi de nombreuses évolutions, du fait notamment des techniques disponibles et de critères esthétiques et symboliques. On a utilisé des fibres végétales, des produits animaux comme le crin de cheval, le boyau ou la soie, des fils métalliques, et plus récemment les fibres synthétiques comme le nylon.Pour augmenter la masse de la corde sans en détériorer l’élasticité, les luthiers fabriquent des cordes où une âme souple est entourée d'un fil métallique (souvent un alliage de cuivre) qui en augmente la masse. Cette augmentation permet des sons plus puissants et plus graves.Il existe trois modes de jeu principaux sur les instruments à cordes, correspondant aux possibilités d'excitation de la vibration de la corde :par pincement des cordes, c'est-à-dire par un déplacement initial de la corde qui est ensuite relâchée, avec les doigts éventuellement armés d'onglets ou un plectre (ou médiator) ;par frappe avec des baguettes ou de petits marteaux ;par frottement avec un archet ou tout autre dispositif légèrement adhésif qui permet une  excitation continue par relaxation, alors que dans les deux premiers cas, la vibration décroît après l'excitation initiale.Dans chacun de ces trois cas, il existe des instruments où un mécanisme excite la corde.Certains instruments comportent de plus des cordes excitées indirectement, sans que le musicien ne les actionne, qu'on appelle cordes sympathiques (viole d’amour, Sitar).On peut aussi classer les instruments à corde par disposition. On distingue alorsceux dont les cordes sontperpendiculaires à la table d'harmonie comme les harpes,parallèles à la table d'harmonie comme les luths ;et sonnentsur toute leur longueur, comme le piano,sur une partie de leur longueur, sur une action du musicien, comme le violon. La guitare, la basse, le banjo, la mandoline, le luth... On joue de ces instruments en pinçant les cordes avec les doigts ou avec un plectre.La harpe et le clavecin utilisent des cordes chromatiques tendues sur une table d'harmonie en bois de résonance (souvent épicéa). Le clavecin possède de par son coffre un résonateur avec une rosace comme le luth.Dans la Bible, les civilisations orientales, la Grèce ainsi qu'au Moyen Âge, on retrouve les traces du psaltérion. Le psaltérion est joué avec une plume, c'est l'ancêtre de la famille d'instruments du genre clavecin.  L'ajout d'un clavier au psaltérion donne naissance à l'épinette, terme qui désignait autant le clavecin que l'épinette contrairement à l'acception moderne.En y adaptant un clavier et un mécanisme nommé sautereau muni d'un plectre pour pincer les cordes, le clavecin et l'épinette sont apparus au XIVe siècle. Les claviers de trois octaves se sont agrandis jusqu'à cinq octaves (63 notes) au cours des siècles ce qui a donné de grands instruments ; qui parfois ont voulu rivaliser avec l'orgue en multipliant les registres et en ajoutant, un deuxième clavier (Flandre, France) voire un troisième clavier ou un pédalier (Allemagne). Par exemple le piano, le cymbalum, le clavicorde. Les cordes sont frappées avec un marteau lorsqu'on appuie sur la touche.Le piano utilise des cordes tendues sur une caisse de résonance en bois. Pour faire sonner les cordes, le piano les frappe avec des petits marteaux. Le tympanon, le cymbalum apparaissent au Moyen Âge. Le tympanon, joué à l'aide de mailloches, donnera par la suite au XVe siècle le clavicorde muni d'un clavier. Au bout de la touche du clavier de clavicorde est fichée une lame métallique qui vient directement percuter la corde. Cette pièce est appelée tangente car elle divise la corde en deux parties, dont l'une est étouffée pour ne pas vibrer. Le clavicorde est le premier instrument à clavier et à cordes frappées. Au XVIIe siècle Bartolomeo Cristofori (1655-1731) invente le piano, mais ce piano-forte équipé d'une transmission clavier?marteau est très éloigné du piano actuel : à la place de la tangente est disposée une fourche dans laquelle s'articule un levier dont la grande extrémité est munie d'un marteau garni de peau, la petite extrémité est retenue par une barre fixe, il faut relâcher la touche pour répéter la note. Ce système sera perfectionné plus tard avec l'invention de l'échappement simple qui porte le nom de mécanique viennoise ; la barre fixe est remplacée par un élément muni d'un ressort qui se retranche dès que le marteau a frappé et permet ainsi de rejouer la note aussitôt. Ce système d'échappement va s'améliorer dans le courant du XIXe avec le double échappement. Ceci est la première génération de piano, qui en compte trois, jusqu'au piano actuel. Ce sont les violons et les instruments similaires de l'orchestre symphonique européen, et de nombreux instruments de musique populaire ou érudite de par le monde, comme le erhu chinois.Dans la plupart des cas, on frotte les cordes avec un archet. La surface légèrement adhésive déplace la corde, jusqu'à ce que la force de rappel à sa position de repos dépasse la limite d'adhérence. La corde revient alors vers sa position de repos, la dépasse et revient dans l'autre sens ; à un certain moment, la vitesse de la corde par rapport à l'archet est nulle, et elle adhère de nouveau. Ce processus produit une vibration, qui a la particularité d'avoir une fréquence fondamentale légèrement différente de celle de la corde vibrant sans excitation. Cette différence dépend des autres paramètres, comme la nature de l'enduit sur l'archet, en général, une résine appelée colophane, la force d'appui, la vitesse du mouvement, contrôlés par l'instrumentiste, comme le paramètre principal, la longueur de la corde, que le musicien règle, dans le cas du violon, en appuyant une ou plusieurs cordes sur le manche. Les vibrations se transmettent par le chevalet à la caisse de résonance.La corde à vide, dont la longueur et la tension sont fixes, varie un peu de fréquence fondamentale avec la force d'appui et la vitesse de l'archet, en même temps que change le volume sonore. Les instruments à cordes frottées de la musique orchestrale occidentale, de la famille des violons, n'ont pas de frettes. C'est ce qui permet à l'instrumentiste de jouer  juste les différentes notes, tout en faisant varier les trois causes qui affectent la puissance et la fréquence fondamentale des vibrations qui se transmettent par le chevalet à la caisse de résonance.La vielle à roue frotte les cordes avec la tranche d'un disque mu par une manivelle. La construction et le réglage de l'instrument déterminent la force d'appui sur la corde.Liste des instruments de musiqueListe des instruments à cordes (musique classique)Liste des instruments à cordesListe des cordophones dans le système Hornbostel-SachsHistoire de la catégorisation des instruments de musiqueAccordages d'instruments à cordes (en) avec illustrations Portail de la musique"
musique;"Un instrument à vent (ou aérophone) est un instrument de musique dont le son est produit grâce aux vibrations d'une colonne d'air provoquées par le souffle d'un instrumentiste (flûte, trompette… ), d'une soufflerie mécanique (orgue, accordéon) ou d'une poche d'air (cornemuse, veuze… ). Ils sont regroupés en deux grandes familles : les bois pour lesquels le son est produit par vibration d'une anche ou à travers un biseau ;les cuivres pour lesquels le son est produit par les lèvres du musicien.Ces catégories dépendent du mode de production du son d'un instrument et non du matériau utilisé pour sa conception. Ainsi les instruments à vent peuvent être fabriqués avec toutes sortes de matières (du bois, du métal, du plastique, du Plexiglas, du cristal, de l'ivoire ou de l'os), et certains utilisent des technologies mécaniques, électroniques ou informatiques.« Les plus vieux instruments à vent connus sont des flûtes fabriquées dans des os de vautour et datées de 35 000 ans pour celle d’Isturitz au Pays Basque, et de 40 000 ans pour celle de Hohle Fels, en Allemagne ».Les premiers instruments à embouchure en bassin (trompes irlandaises et danoises) datent de l'âge du bronze puis ont été utilisés depuis l'antiquité ; trompes et cors avaient essentiellement un usage militaire.Avec l'invention de l'anche (languette taillée directement dans la paroi de l'instrument ou indépendante en paille ou en roseau), la famille des bois s'est élargie avec les instruments à anches qui apparaissent au proche-orient ; des double clarinettes (Arghul) primitives sont présentes sur les représentations en ancienne Égypte. Ce type d'instrument s'est alors répandu en Afrique du nord et en Europe (aulos: instrument à deux chalumeaux en roseau...).Il existe de nombreux vestiges d'instruments à vents autour du bassin méditerranéen : « Les plus anciennes flûtes de Pan découvertes en Europe sont originaires des régions orientales du continent : d’une nécropole néolithique (2000 av. J.-C.) d’Ukraine méridionale et d’un site de la région de Saratov. Chacune se compose de sept à huit tuyaux en os creux d’oiseau… »...Au IIIe siècle av. J.-C., Ctésibios d’Alexandrie invente un orgue appelé hydraulos, réunion de plusieurs monaules (flûte grecque à une seule tige) à un clavier et alimentés avec de l’air comprimé créé par une colonne d’eau.Le Moyen Âge a été une période foisonnante pour la création de nouveaux instruments à vent.Dès lors, les instruments ont été constamment améliorés depuis la Renaissance.Les instruments sont classés par leur méthode de production du son et non par les matériaux qui les composent :la Voix :les femmes (soprano, alto… ) ;les hommes (ténor, baryton, basse… ).les Bois, une colonne d'air mise en vibration sur un biseau ou par une anche :Instrument à biseau,Instrument à conduit, comme la flûte à bec (en bois) ou le positif (tuyau en métal) ;Instrument à embouchure libre, comme les flûtes traversières, la quena (droite), le siku (polycalame) ou les flûtes obliques (comme le ney).Instrument à ancheInstrument à anche libre, comme l'harmonica, l'accordéon ou le bandonéon ;Instrument à anche simple, comme la clarinette (en ébène) ou le saxophone (en métal) ;Instrument à anche double, comme le hautbois, la bombarde ou le basson.les Cuivres, une colonne d'air mise en vibration par les lèvres du musicien, comme la trompette (en métal), le cornet à bouquin et le didgeridoo (en bois) ou l'olifant (en ivoire).Instruments mécaniques, combinant plusieurs systèmes comme le limonaire ou l'orgue de Barbarie.Instruments à vent électroniques, utilisant le souffle et les doigtés d'instruments à vent.Les instruments à vent, Georges Gourdet, Que sais-je ? n°267, Presses Universitaires de France, 1967Instrument de musiqueBois (musique)Cuivre (musique)Onde stationnaire dans un tuyauOrchestre d'harmonieBrass bandJean-Luc Matte, « Typologie des instruments à vent », sur jeanluc.matte.free.fr, 23 février 2011 (consulté le 23 février 2012). Portail des musiques du monde   Portail de la musique"
musique;"En musique, l'intervalle entre deux notes est l'écart entre leurs hauteurs respectives. Cet intervalle est dit harmonique si les deux notes sont simultanées, mélodique si les deux notes sont émises successivement.En acoustique, l'intervalle entre deux sons harmoniques est le rapport de leurs fréquences.Chaque intervalle est caractéristique[Comment ?] d'une échelle musicale, elle-même distinctive d'un type de musique (indienne, occidentale, musique orientale, etc.). La perception des intervalles diffère selon les cultures. Il n'existe pas de système musical universel contenant tous les intervalles de toutes les échelles musicales. Seul l'intervalle entre un son et sa répétition, l'unisson, peut être considéré comme n'appartenant pas en propre à un genre musical déterminé[réf. nécessaire].Lorsqu'un système musical[Quoi ?] ne possède pas de théorisation écrite[Quoi ?], les musicologues[Lesquels ?] utilisent[réf. nécessaire] la terminologie du solfège[Quoi ?] pour rendre compte des intervalles et des échelles propres à ce système.Historiquement, l'étude des intervalles a commencé par l'étude des rapports entre fréquences. L’école pythagoricienne a, grâce au monocorde, réussi à construire des micro-intervalles, quasi imperceptibles, par simple soustraction d’intervalles. Autrement dit par la division des fractions qui les représentent. La physique permet de comprendre les relations fractionnaires dues à la nature des ondes sonores, l'acoustique musicale et la psychoacoustique permettent de comprendre comment les intervalles et sons musicaux sont perçus.Ce type de représentation atteint vite ses limites : la perception d'un intervalle musical dépasse la notion de rapport de fréquences surtout avec l'usage d'instruments inharmoniques tels que le piano.Un intervalle est pur (ou « naturel ») lorsque le rapport des fréquences de ses deux notes est égal à une fraction de nombres entiers simples. En acoustique, la pureté se manifeste par l'absence de battement.L'addition de deux intervalles s'obtient en multipliant leurs rapports de fréquences. Une quinte pure (3/2) plus une quarte pure (4/3) donne une octave pure (2/1) :                                          3            2                          ×                              4            3                          =        2              {\displaystyle {\frac {3}{2}}\times {\frac {4}{3}}=2}  La soustraction de deux intervalles s'obtient en divisant leurs rapports. Une octave pure moins une quinte pure donne une quarte pure (complément à l'octave de la quinte) :                                          2                          3              2                                      =                              4            3                                {\displaystyle {\frac {2}{\frac {3}{2}}}={\frac {4}{3}}}  L'essence d'une mélodie (ou d'une harmonie) est déterminée par la nature des intervalles séparant les notes qui la constituent, et non pas par les notes elles-mêmes.Un intervalle mélodique est dit :ascendant si le deuxième son est plus aigu que le premier (par exemple, en musique occidentale : do puis sol dans la même octave),descendant si le deuxième son est plus grave que le premier (sol puis do dans la même octave),conjoint si ses notes sont deux degrés consécutifs de l'échelle considérée (do-ré ou sol-fa sont conjoints dans la même octave en gamme de do majeur),disjoint s'il n'est pas conjoint (do-mi, ou do-do si les deux do sont séparés par une ou plusieurs octaves ; do et do# sont deux notes différentes).Si l'intervalle est constitué du même son répété deux fois, c'est un unisson.En musique tonale, en musique modale, ou en musique atonale, la notion d'intervalle renvoie plus précisément à la distance entre deux degrés d'une gamme musicale.Dans la musique classique et donc dans le système tonal, les intervalles sont nommés et théorisés par le solfège et la fonction des différents degrés dépend de l'intervalle qui sépare chacun d'eux de la tonique. Aux différents intervalles sont associés les notions de consonance et dissonance.Les degrés de l'échelle diatonique sont séparés par des espaces conjoints (ou intervalles) inégaux, les tons et les demi-tons diatoniques.Les intervalles séparant deux degrés de l'échelle diatonique sont toujours nommés en utilisant un nom suivi d'un qualificatif (adjectif) :le nom est lié au nombre de degrés englobés ; ce nombre dépend de la gamme musicale utilisée ;le qualificatif dépend de l'étendue réelle de l'intervalle, compte tenu des tons et demi-tons : ainsi, une tierce est dite majeure lorsqu'elle englobe deux tons, mineure si elle n'englobe qu'un ton et un demi-ton diatonique. Nom Le nom de l'intervalle dépend de son étendue en degrés, c'est-à-dire du nombre de notes qui séparent la première note de la deuxième. Il dépend donc aussi de la tonalité choisie. L'intervalle s'appelle :une prime, lorsque l'on a un unissonune seconde, entre deux notes de noms successifs. exemple: Do-Ré, Do-Ré#une tierce. entre trois notes de noms successifs, exemple: Do-Mi, Do-Mi? ou encore Do?-Mi# car Do, Ré et Mi se suiventune quarte, entre quatre notes de noms successifs, exemple: Do-Fa.Le raisonnement est le même pour la quinte, la sixte, la septième, l'octave, etc.Au-delà de l'octave, le noms deviennent neuvième, dixième, onzième, etc.Ainsi, do-sol constitue une quinte car l'intervalle constitué de do, ré, mi, fa, sol est long de cinq degrés.Jadis, le terme servant à désigner la longueur d'un intervalle servait également à désigner un degré par rapport à la tonique ou par rapport à une autre note de référence. Il est donc préférable d'indiquer la fonction des degrés, par exemple : « sol est la dominante de la gamme de do » ou « sol est le cinquième degré de la gamme de do » plutôt que « sol est la quinte de la gamme de do ». En revanche, il est correct de définir que « do-sol forme une quinte ».Cependant, en harmonie tonale, l'habitude est conservée de désigner les notes réelles d'un accord au moyen de l'intervalle qui sépare celles-ci de la basse, ou de la fondamentale, en fonction du contexte. Par exemple : « au premier renversement, la tierce (sous entendu : de la fondamentale) va à la basse ». Et inversement, « sur ce premier renversement, la sixte (sous entendu : de la basse, cette sixte est donc la fondamentale) est au soprano ».Les notes extrêmes d'un intervalle à chiffre pair — seconde, quarte, etc. — ont des positions différentes sur la portée : une sur la ligne, l'autre dans l'interligne ; au contraire, les notes extrêmes d'un intervalle à chiffre impair — unisson, tierce, etc. — ont des positions identiques sur la portée : soit sur deux lignes, soit dans deux interlignes. Qualificatif Le nom d'un intervalle ne donne pas son étendue exacte. Par exemple, les deux tierces do-mi et do-mi?, bien qu'englobant le même nombre de noms de notes (Do-Ré-Mi : 3 notes), n'ont pas la même étendue tonale. La première est dite majeure (elle s'étend sur deux tons) ; l'autre est dite mineure (elle s'étend sur un ton et demi). Il existe cinq qualificatifs principaux :majeur,mineur,juste,augmenté,diminué.Plus rarement, on rencontre les qualificatifs « sur-augmenté » et « sous-diminué ».Au sein de l'échelle diatonique naturelle, les intervalles se partagent en deux familles :ceux qui, ni augmentés ni diminués, n'ont qu'une étendue « moyenne », qualifiée de juste,ceux qui ont deux étendues « moyennes » possibles : ils peuvent être soit majeurs soit mineurs (l'étendue d'un intervalle majeur est plus grande d'un demi-ton chromatique que celle de l'intervalle mineur). Intervalles justes La quarte, la quinte et l'octave peuvent être qualifiées de justes :la quarte juste fait exactement 2 tons et 1 demi-ton,la quinte juste fait exactement 3 tons et 1 demi-ton,l'octave juste fait exactement 5 tons et 2 demi-tons. Intervalles mineurs et majeurs La seconde, la tierce, la sixte et la septième peuvent être qualifiées de mineure ou de majeure :Par exemple, les intervalles do-ré et mi-fa sont tous deux des secondes, mais la première est majeure, car do et ré sont éloignés d'un ton, tandis que la deuxième est mineure, car mi et fa sont éloignés d'un demi-ton.Si on classe les intervalles par ordre croissant d'étendue, l'intervalle mineur précédera l'intervalle majeur correspondant. Selon ce classement, il est possible de reconstituer tous les intervalles de l'échelle diatonique naturelle en partant de do en utilisant uniquement des intervalles majeurs ou justes (les autres sont équivalents à l'un d'eux par enharmonie). Intervalles augmentés et diminués Quelle que soit la nature de l'intervalle, il est toujours possible de le rallonger ou le raccourcir d'un ou plusieurs demi-tons par l'ajout ou le retrait d'une altération. On parle alors d'intervalle augmenté et diminué si un demi-ton chromatique a été ajouté ou soustrait, et d'intervalle sur-augmenté ou sous-diminué si sa longueur a été modifiée de deux demi-tons chromatiques.Par exemple, do-sol#, est une quinte augmentée car la distance de do à sol est égale à cinq degrés, et qu'un demi-ton a été ajouté à l'intervalle. Cet exemple permet de voir que la quinte augmentée a un nom différent mais la même sonorité (dans un système à tempérament égal) que la sixte mineure (ici do-la bémol) ; cependant, ces intervalles sont différents en musique tonale ou modale, car bien que leurs sons soient identiques, leurs fonctions ne le sont pas. Cela influe sur le sens donné au discours, et peut également influencer l'interprétation musicale.Un intervalle simple (d'étendue inférieure à l'octave) peut être renversé par inversion de ses notes. Le renversement d'un intervalle est aussi appelé intervalle complémentaire, ou intervalle différentiel. Un intervalle ajouté à son renversement donne une octave juste. Par exemple, la quinte juste do-sol a pour renversement la quarte juste sol-do ; l'étendue de ces deux intervalles donne l'octave do-do ou sol-sol.Un intervalle mineur renversé donne un intervalle majeur, et inversement. De même pour les altérations : un intervalle augmenté a pour renversement un intervalle diminué. Par exemple, la tierce majeure fa-la a pour renversement la sixte mineure la-fa :Quant à un intervalle juste, son renversement est également un intervalle juste. Formule de renversement La formule suivante permet de trouver le renversement d'un intervalle donné :9 – étendue initiale = étendue du renversement.Par exemple, une septième renversée donne une seconde (9 – 7 = 2).La théorie du redoublement de l'intervalle repose sur le principe de l'identité des octaves. On définit :l'intervalle simple : de longueur inférieure ou égale à l'octave ;l'intervalle redoublé : de longueur supérieure ou égale à l'octave, c'est un intervalle formé d'une ou plusieurs octaves justes, plus un intervalle simple.L'octave juste est le seul intervalle pouvant être analysé à la fois comme un intervalle simple et comme un intervalle redoublé (le redoublement de l'unisson juste). L'octave diminuée est un intervalle simple, tandis que l'octave augmentée est un intervalle redoublé. Deux octaves forment une quinzième.En harmonie classique, les intervalles ont la signification de leur réduction à l'intervalle simple (par exemple seconde pour la neuvième). En jazz, les intervalles conservent leur sens propre dans la constitution des accords jusqu'à la treizième. Formule de redoublement Étendue de l'intervalle réduit + (7 × nombre d'octaves) = intervalle à l'octave.Ainsi, une seconde devient une neuvième à l'octave : 2 + (7 × 1) = 9.Cette même seconde devient une seizième à deux octaves : 2 + (7 × 2) = 16.Ce qui correspond bien à l'octave d'une neuvième : 9 + (7 × 1) = 16.Le qualificatif d'un intervalle redoublé est le même que celui de l'intervalle simple correspondant : par exemple, la dixième do-mi est majeure parce qu'elle est le redoublement de la tierce do-mi, qui est également majeure. Il suffit donc d'étudier les qualificatifs des seuls intervalles simples pour comprendre les qualificatifs de tous les intervalles.La transposition d'un intervalle est le déplacement de celui-ci en hauteur — au moyen des altérations sans modification de son étendue exacte.Si un demi-ton chromatique est ajouté, ou bien retranché, aux deux notes extrêmes d'un intervalle donné, le nom et le qualificatif de cet intervalle ne changent pas, en d'autres termes, les intervalles sont équivalents.Par exemple, do-mi est une tierce majeure, mais do                    ?              {\displaystyle \sharp }  -mi                    ?              {\displaystyle \sharp }  , ou encore, do                    ?              {\displaystyle \flat }  -mi                    ?              {\displaystyle \flat }  , sont aussi des tierces majeures ; fa-si                    ?              {\displaystyle \flat }   est une quarte juste, mais fa                    ?              {\displaystyle \flat }  -si                    ?        ?              {\displaystyle \flat \flat }  , ou encore, fa                    ?              {\displaystyle \sharp }  -si, sont aussi des quartes justes ; etc.Les intervalles peuvent être identifiés à l'audition, par la perception acoustique de leur rapport de fréquences. Certaines ambiguïtés peuvent alors exister car une même différence de hauteurs peut être exprimée sous forme d'intervalles de noms différents suivant le contexte dans lequel elle se situe (voir : enharmonie).Chaque intervalle peut en revanche être identifié de façon non ambigüe à la lecture de sa notation sur une partition. Le musicien peut alors associer sonorités, fonction, et formalisme.Dans le solfège, trois intervalles simples peuvent servir de référence pour apprécier l'étendue de tous les autres :la seconde majeure, qui englobe un ton (exemple : do-ré),la tierce majeure, qui englobe deux tons (exemple : do-mi),la quarte juste, qui englobe deux tons et un demi-ton diatonique (exemple : do-fa).Grâce aux altérations qui augmentent ou diminuent un intervalle d'un demi-ton chromatique, et aux règles de renversement et de redoublement, il suffit, pour trouver le qualificatif d'un intervalle donné, de retenir les points suivants :Toutes les secondes sans altérations sont majeures — un ton —, sauf mi-fa et si-do, qui sont mineures — un demi-ton diatonique.Toutes les tierces sans altérations sont mineures — un ton et un demi-ton diatonique — sauf do-mi, fa-la et sol-si, qui sont majeures — deux tons.Toutes les quartes sans altérations sont justes — deux tons et un demi-ton diatonique — sauf fa-si qui est augmentée — trois tons (le triton).Les musiques traditionnelles (arabe, chinoise, indienne, turque, etc.) y compris en Europe (notamment dans les Balkans) utilisent parfois des micro-intervalles, comme des quarts de ton, ou des intervalles composites (par exemple, trois-quarts de tons), voire des divisions en deçà du quart de ton.Les musiciens occidentaux ont inventé des unités de mesure (tels que le cent et le savart) qui permettent de décrire ces micro-intervalles qui n'appartiennent pas traditionnellement à la musique classique occidentale (mais qui étaient reconnus au travers des commas).La musique arabe étant basée sur une gamme naturelle et non tempérée, elle repose sur un système d'intervalles différents des intervalles décrits au-dessus. Un intervalle de trois quarts de ton — dit « seconde neutre » — différencie de manière caractéristique les genres dits « zalzaliens » des genres diatoniques (à intervalles de secondes mineures et majeures) et des genres à seconde augmentée (type hijaz). On le trouve dans de nombreux Maqâms de la musique arabe (râst en particulier), dans la musique turque (musique classique ottomane - Türk sanat müzi?i), ouzbèke, ouïghoure, certaines formes de Musique kazakhe et kirghize, certaines musiques afghanes ou iranienne, etc., ou encore les musiques des Balkans (grecque, yougoslave et bulgare…).Les ?ruti utilisent un système musical qui divise l’octave en vingt-deux parties : ce système est difficile à percevoir pour une oreille habituée aux échelles occidentales. Inversement, la quinte, qui dans le solfège occidental est l'intervalle exprimant très précisément la distance entre cinq degrés — en référence à la gamme diatonique —, ne peut avoir la même valeur et jouer un rôle comparable dans une échelle musicale divisant l'octave en 22 degrésAccordConsonanceÉchelleMicro-intervalle(fr) Résumé des intervalles(fr) Liste des intervalles(fr) Expression des Intervalles d'après le théoricien Kirnberger, vers 1776(fr) Générateur de schémas pour la mémorisation des intervalles mélodiquesAdolphe Danhauser, Théorie de la musique : Édition revue et corrigée par Henri Rabaud, Paris, Henry Lemoine, 1929, 128 p. (ISMN 979-0-2309-2226-5)Claude Abromont et Eugène de Montalembert, Guide de la théorie de la musique, Librairie Arthème Fayard et Éditions Henry Lemoine, coll. « Les indispensables de la musique », 2001, 608 p. [détail des éditions] (ISBN 978-2-213-60977-5)Pierre-Yves Asselin, Musique et tempérament, Éditions JOBERT, 2000, 236 p. (ISBN 2-905335-00-9) Portail de la musique classique   Portail de la musique"
musique;"Un microphone (souvent appelé micro par apocope) est un transducteur électroacoustique, c'est-à-dire un appareil capable de convertir un signal acoustique en signal électrique.L'usage de microphones est aujourd'hui largement répandu et concourt à de nombreuses applications pratiques :télécommunications (téléphone, radiotéléphonie, Interphone, systèmes d'intercommunication) ;sonorisation ;radiodiffusion et télévision ;enregistrement sonore notamment musical ;mesure acoustique.On appelle également micro, par métonymie, les transducteurs électromagnétiques de guitare électrique (micro de guitare) et les transducteurs piézoélectriques (capteur piézo) utilisés pour des instruments dont le son est destiné à être amplifié.Le composant électronique qui produit ou module la tension ou le courant électriques selon la pression acoustique, est appelé capsule. On utilise aussi le terme microphone par synecdoque. Un tissu ou une grille protège généralement cette partie fragile.Le premier usage du terme microphone désignait une sorte de cornet acoustique. David Edward Hughes l'a le premier utilisé pour désigner un transducteur acoustique-électrique. Améliorant le dispositif de Graham Bell, Hugues fait valoir la capacité du dispositif qu'il a co-inventé à transmettre des sons beaucoup plus faibles.Une membrane vibre sous l'effet de la pression acoustique et un dispositif qui dépend de la technologie du microphone convertit ces oscillations en signaux électriques. La conception d'un microphone comporte une partie acoustique et une partie électrique, qui vont définir ses caractéristiques et le type d'utilisation. Capteurs de pression (omnidirectionnels) Si la membrane est au contact de l'onde sonore d'un seul côté, tandis que l'autre est dans un boîtier avec une pression atmosphérique constante, elle vibre selon les variations de pression. On parle d'un capteur de pression acoustique. Ce type de capteur réagit à peu près de la même manière aux ondes sonores quelle que soit la direction d'origine. Il est insensible au vent. Il est à la base des microphones omnidirectionnels.Les microphones à effet de surface sont des capteurs de pression fixés sur une surface de quelque étendue formant baffle, qui double la pression acoustique dans l'hémisphère limité par la surface d'appui (Voir PZM (microphone) (en)). Capteurs de gradient de pression (bidirectionnels ou directivité en 8) Si la membrane est au contact de l'onde sonore des deux côtés, elle ne vibre pas lorsqu'une onde arrive en travers, puisque les surpressions sont égales des deux côtés. On appelle ce type de membrane un capteur de gradient de pression acoustique. C'est la base des microphones bidirectionnels ou à directivité en 8. Types mixtes ou variables En associant ces deux types, soit par des moyens acoustiques, en contrôlant de façon plus subtile l'accès des ondes sonores à la face arrière de la membrane, soit par des moyens électriques, en combinant le signal issu de deux membranes, on obtient des directivités utiles, notamment cardioïde (dite aussi unidirectionnelle) :On construit des microphones de directivité cardioïde large, supercardioïde et hypercardioïde en changeant les proportions entre la composante omnidirectionnelle et la composante bidirectionnelle. Des microphones peuvent offrir un réglage ou une commutation de la directivité.Ces constructions permettent de donner plus d'importance à une source vers laquelle on dirige le micro et d'atténuer le champ sonore réverbéré, qui vient de toutes les directions. On définit un indice de directivité comme l'expression, en décibels du rapport entre un son venant dans l'axe du microphone et un son de même pression acoustique efficace venant d'une source idéalement diffuse (venant de partout autour du microphone).                          Tubes à interférences Les microphones à tube à interférences donnent des directivités accentuées, mais fortement dépendantes des fréquences. À cause de leur forme allongée, on les appelle micro canon. Taille de la membrane La taille de la membrane influe sur la conversion en vibrations, puis en signal électrique.Au contact d'une paroi perpendiculaire à la direction de propagation, une onde sonore développe une puissance proportionnelle à l'aire et au carré de la pression acoustique :                    P        =        S        .                                            p                              ?                                  2                                                                                    ?                                  0                                                           c                                            {\displaystyle P=S.{\frac {p'^{2}}{\rho _{0}\ c}}}  S est la surface de la paroi ;p est la pression acoustique ;                              ?                      0                                {\displaystyle \rho _{0}}   est la masse volumique de l'air (1,2 kg/m3 aux conditions normales de température et de pression) ;c est la vitesse du son, 343 m/s dans les mêmes conditions.On ne peut récupérer qu'une partie de cette puissance sous forme de signal électrique décrivant l'onde sonore. Plus la membrane est grande, moins il est nécessaire d'amplifier le signal, et par conséquent, moins on le soumet à un traitement amenant inévitablement une certaine quantité de bruit et de distorsion.La taille de la membrane détermine par conséquent la sensibilité maximale du microphone. Mais dès que la plus grande dimension de la membrane devient significative par rapport à la longueur d'onde d'un son, elle constitue, pour les ondes sonore qui n'arrivent pas perpendiculairement, un filtre en peigne. Bien entendu, d'autres phénomènes comme la diffraction sur les bords interviennent, rendant la réponse réelle plus complexe.La présence d'un entourage rigide autour de la membrane crée un effet de surface qui augmente la pression acoustique pour les fréquences dont la longueur d'onde est inférieure à la taille de l'ensemble membrane-entourage. Cet obstacle peut-être plat ou sphérique, il constitue autour d'une capsule capteur de pression un filtre acoustique, comme la grille de protection, qui délimite une cavité dont les caractéristiques influent sur la réponse du microphone, particulièrement aux plus hautes fréquences.Les applications (téléphone mobile, micro cravate) qui exigent des micros de petite taille limitent par là même la taille de la membrane. Microphone à charbon Les premiers microphones, employés d'abord dans les téléphones, utilisaient la variation de résistance d'une poudre granuleuse de carbone, quand elle est soumise à une pression. Quand on comprime la poudre, la résistance diminue. Si on fait passer du courant à travers cette poudre, il va être modulé suivant la pression acoustique sur la membrane qui appuie sur la poudre. On ne peut évidemment construire de cette manière que des capteurs de pression. Ces microphones sont peu sensibles, fonctionnent sur une plage de fréquence limitée, et leur réponse n'est que très approximativement linéaire, ce qui cause de la distorsion. Ils ont l'avantage de pouvoir produire une puissance assez élevée sans amplificateur. Ils ont été utilisés dans les combinés téléphoniques, où leur robustesse était appréciée, et à la radio avant l'introduction de procédés donnant de meilleurs résultats. Microphone dynamique à bobine mobile Dans les microphones électromagnétiques à bobine mobile, une bobine est collée à la membrane, qui la fait vibrer dans le fort champ magnétique fixe d'un aimant permanent. Le mouvement crée une force électromotrice créant le signal électrique. Comme la conversion de l'énergie sonore dégagée par l'action de la pression acoustique sur la membrane donne directement un courant utilisable, ces microphones sont dits dynamiques, car contrairement aux micros à charbon et aux micros électrostatiques, ils n'ont pas besoin d'alimentation.L'apparition dans les années 1980 d'aimants au néodyme a permis des champs magnétiques plus intenses, avec une amélioration de la qualité des microphones électromagnétiques. Microphone à ruban Dans les microphones électromagnétiques à ruban, la membrane est un ruban gaufré souple installé dans le champ magnétique d'un aimant permanent. Il fonctionne comme le microphone électromagnétique à bobine mobile, avec l'avantage de la légèreté de la partie mobile. Il ne requiert pas d'alimentation. L'impédance de sortie est bien plus faible que celle des autres types, et il est assez fragile. Microphone électrostatique Dans les microphones électrostatiques, la membrane, couverte d'une mince couche conductrice, est l'une des armatures d'un condensateur, chargé par une tension continue, l'autre armature étant fixe. La vibration rapproche et éloigne les armatures, faisant varier la capacité. La charge étant constante et égale au produit de la tension et de la capacité, la variation de la capacité produit une variation inverse de tension. L'impédance de sortie est très élevée. Les micros électrostatiques ont besoin d'une alimentation, d'une part pour la polarisation du condensateur, d'autre part pour l'amplificateur adaptateur d'impédance qui doit être proche de la membrane.L'alimentation peut être fournie par un conducteur spécial relié à un boîtier d'interface qui assure aussi l'adaptation d'impédance. Cependant, ce n'est le cas que pour quelques microphones très haut de gamme. La plupart des modèles utilisent une alimentation fantôme, ainsi nommée parce qu'elle ne nécessite aucun conducteur supplémentaire.La sensibilité des microphones électrostatiques est supérieure à celle des microphones dynamiques. Il y a besoin de moins de puissance sonore pour faire vibrer la membrane seule que l'appareil membrane-bobinage, et l'amplificateur adaptateur d'impédance prélève une puissance infime. Cet amplificateur est conçu pour le capteur et contrôle aussi la bande passante ; la réponse du condensateur seul est un filtre passe-bas (Rayburn 2012, p. 33). Ces amplificateurs furent d'abord composés d'un tube électronique et d'un transformateur. Plus récemment, leur niveau de bruit et de distorsion ainsi que leur sensibilité aux interférences ont été abaissés par l'emploi de transistors ou de transistors à effet de champ, sans transformateurs. Microphone électrostatique haute fréquence Le condensateur formé par la membrane et une armature fixe n'est pas polarisé par une tension continue, mais constitue, avec une résistance, un filtre dont la fréquence de coupure varie comme la capacité. Le niveau de modulation haute-fréquence suit donc la vibration de la membrane. L'étage suivant comporte une démodulation sur une diode qui conduit les transistors de sortie. Microphone électrostatique à électret Les microphones électrostatiques à électret tirent parti d'une propriété de certains matériaux de conserver une charge électrostatique permanente. Un matériau de cette sorte constitue une armature de condensateur, la membrane l'autre. Les microphones à électret n'ont pas besoin de tension de polarisation, mais ils ont néanmoins un amplificateur adaptateur d'impédance, qui requiert une alimentation. Si la tension de crête de sortie n'est pas trop élevée, cette alimentation peut être fournie par une pile.La charge de polarisation diminue dans le temps, ce qui se traduit par une perte de sensibilité du micro au fil des années.asymétrique sur courte distance (comme dans les téléphones mobiles ou les dictaphones). Le signal est la tension entre le conducteur unique et la masse.symétrique quand les câbles sont plus longs. Le signal est la différence entre le conducteur dit « chaud » ou « + » et le conducteur dit « froid » ou « - ». Les interférences, qui s'appliquent à peu près également aux deux conducteurs, sont réduites. Les applications professionnelles utilisent une transmission symétrique avec des connecteurs XLR. L'adaptation est en tension, les microphones ayant des impédances de sortie inférieures à 600 ohms et les entrées pour microphone des impédances de plusieurs kilohms. La ligne peut comprendre une alimentation fantôme.sans fil pour libérer les porteurs de micros. La transmission peut être analogique ou numérique. L'électronique se trouvant à proximité immédiate de la membrane est peu sensible aux interférences. L'usage d'antennes de réception doubles avec des récepteurs choisissant le signal le plus fort (diversity) assure la sécurité de la transmission. Le plan de fréquences limite le nombre de micros sans fil.La conception ou le choix d'un modèle existant doit tenir compte de l'usage auquel le microphone est destiné :directivité ;sensibilité ;sensibilité aux interférences (vent, interférences électromagnétiques) ;pression acoustique maximale ;bruit propre ;bande passante ;robustesse (résistance aux intempéries, à l'humidité, aux variations rapides de pression atmosphérique, aux surcharges acoustiques et électriques, aux mauvais traitements) ;système de fixation (micros tenus à la main, micro cravate, micros de studio, micros d'instrument, intégration dans un appareil) ;transmission, par câble standard, par câble spécial, sans fil (micro HF) ;poids ;encombrement ;prix.La qualité de la transcription du son dépend des caractéristiques et de la qualité du microphone mais aussi, et principalement, de l'emplacement du microphone par rapport à la source, ainsi que de l’environnement de la prise de son (bruits, vent…).La directivité est une caractéristique essentielle du microphone. Elle indique sa sensibilité selon la provenance du son par rapport à son axe.Le diagramme polaire représente la sensibilité du microphone selon la direction d'origine de l'onde sonore. La longueur du point central à la courbe indique la sensibilité relative en décibels. Dans la plupart des cas, la sensibilité ne dépend que de la direction par rapport à l'axe principal du microphone ; dans le cas contraire, deux diagrammes sont nécessaires. La directivité dépend aussi de la fréquence ; les diagrammes complets comprennent plusieurs courbes de valeurs relatives. En général, le diagramme est symétrique, et on peut mettre, pour une meilleure lisibilité, des demi-courbes de part et d'autre de l'axe.Le plus souvent, la réponse en fréquence est la plus égale quand le microphone est face à la source. Si d'autres sons ne viennent pas se mêler à celui de la source principale, on peut utiliser les différences de réponse hors de l'axe pour égaliser la sonorité.Omnidirectionnel : Le micro capte le son de façon uniforme, dans toutes les directions. Il s'utilise surtout pour enregistrer le son d'une source étendue, comme un orchestre acoustique, ou une ambiance. Il sert dans plusieurs systèmes de captation stéréophonique. Il capte la réverbération ; il est donc souhaitable que l'acoustique de la salle se prête à l'enregistrement. On met aussi à profit son insensibilité aux bruits de manipulation et au vent, par exemple pour enregistrer des déclarations ou du chant. On l'évite en sonorisation en raison de sa sensibilité à l'effet Larsen dès que la source est un peu éloignée. Les microphones omnidirectionnels le sont en réalité d'autant moins pour les hautes fréquences que leur membrane est grande ; c'est pourquoi il est préférable de les désigner comme des capteurs de pression, selon leur principe acoustique.Cardioïde : privilégie les sources sonores placées devant le micro. Utilisé pour la sonorisation, pour le chant, pour la prise de son d'instruments, le microphone unidirectionnel est le plus répandu. L'apparence de son diagramme directionnel le fait appeler cardioïde (en forme de cœur). Il rejette bien les sons provenant de l'arrière, et atténue ceux provenant des côtés. En contrepartie, il est plus sensible au vent, aux bruits de manipulation, aux ""plops"", et est plus affecté par l'effet de proximité, qui renforce les basses pour les sources proches. De nombreux modèles commerciaux sont traités pour limiter ces inconvénients.Super-cardioïde: le super cardioïde capte en priorité les sons venant de face, et sur un plan d'environ 140° de façon à éviter les bruits environnants, il est aussi appelé super unidirectionnel.Hypercardioïde : similaire au cardioïde, avec une zone avant un peu plus étroite et un petit lobe arrière. Il présente, accentués, les mêmes avantages et inconvénients que le cardioïde. Il est souvent utilisé en conférence, quand les orateurs s'approchent peu des micros.Canon : forte directivité vers l'avant, directivité ultra cardioïde permettant de resserrer le faisceau sonore capté. Utilisé pour enregistrer des dialogues à la télévision ou au cinéma, et pour capter des sons particuliers dans un environnement naturel. L'accroissement de directivité ne concerne pas les basses fréquences.Bi-directionnel ou directivité en 8 : deux sphères identiques. Le microphone bidirectionnel est utilisé le plus souvent en combinaison avec un microphone de directivité cardioïde ou omnidirectionnelle afin de créer un couple MS (voir Systèmes d'enregistrement stéréophonique). Les angles de réjection des microphones bidirectionnels permettent d'optimiser les problèmes de diaphonie lors de l'enregistrement d'instruments complexes comme la batterie par exemple.Les professionnels du son ont tendance à préférer les microphones électrostatiques aux dynamiques en studio. Ils offrent en général un rapport signal sur bruit largement supérieur et une réponse en fréquence plus large et plus étale.Pour les sources très puissantes, comme un instrument de percussion, les cuivres ou un amplificateur pour guitare électrique, un microphone dynamique a l'avantage d'encaisser de fortes pressions acoustiques. Leur robustesse les fait souvent préférer pour la scène.Avantages : robustesse, pas d'alimentation externe ni d'électronique, capacité à gérer de fortes pressions acoustiques, prix en général nettement inférieur à un microphone électrostatique de gamme équivalente.Inconvénients : manque de finesse dans les aigus le rendant inapte à prendre le son de timbres complexes : cordes, guitare acoustique, cymbales, etc.Quelques modèles de références : Les micros broadcast Shure SM7b, Electrovoice RE20 et RE27N/D très utilisés aux États-Unis et dans certaines radios nationales et locales françaises ; Shure SM-57, un standard pour la reprise d'instrument (notamment la caisse claire et la guitare électrique) et Shure SM-58 pour la voix (Micro utilisé entre autres par Mick Jagger sur Voodoo Lounge, Kurt Cobain sur Bleach etc.). Il est intéressant de savoir que ces deux micros sont identiques au niveau de la construction et que ce n'est qu'une courbe différente d'égalisation (due au filtre anti-pop qui n'existe pas sur le SM57) qui les différencie[réf. nécessaire]. Leurs versions hypercardioïdes, le BETA57 et BETA58, jouissent d'une notoriété moindre, malgré une qualité de fabrication nettement supérieure. Citons encore le Sennheiser MD-421 très réputé pour les reprises de certains instruments acoustiques (dont les cuivres) et d'amplis de guitare ou de basse.Le microphone électrostatique présente l'avantage d'excellentes réponses transitoire et bande passante, entre autres grâce à la légèreté de la partie mobile (uniquement une membrane conductrice, à comparer avec la masse de la bobine d'un microphone dynamique). Ils ont en général besoin d'une alimentation, en général une alimentation fantôme. Ils comportent souvent des options de traitement du signal telles un modulateur de directivité, un atténuateur de basses fréquences, ou encore un limiteur de volume (Pad).Les microphones électrostatiques sont plébiscités par les professionnels en raison de leur fidélité de reproduction.Les sonomètres professionnels utilisent tous des microphones à capteur de pression (omnidirectionnels) électrostatiques. Cet usage exige que le microphone soit étalonné ; le pistonphone est un appareil couramment utilisé à cette fin.Avantages : sensibilité, définition.Inconvénients : fragilité, nécessité d'une alimentation externe, contraintes d'emploi. Sauf les capteurs de pression, il est généralement fixé sur une monture à suspension faite de fils élastiques, généralement en zigzag, destinée à absorber les chocs et les vibrations. Il est rare qu'il soit utilisé comme microphone à main, sauf certains modèles qui incorporent une suspension interne.Ces caractéristiques font qu'ils sont en général plus utilisés en studio que sur scène.Quelques modèles de référence : Neumann U87ai, U89i et KM 184 (souvent en paire pour une prise stéréo), Shure KSM44, AKG C3000 et C414, Schoeps série Colette.Facilement miniaturisable, le micro à électret est très utilisé dans le domaine audiovisuel (micro cravate, micro casque, etc.) où on l'apprécie pour son rapport taille/sensibilité. Les meilleurs modèles parviennent même à rivaliser avec certains micros électrostatiques en termes de sensibilité.Les électrets actuels bénéficient d'une construction palliant cette fâcheuse espérance de vie limitée que l'électret connaît depuis les années 1970.Avantages : possibilité de miniaturisation extrême, sensibilité.Inconvénients : amoindrissement de la sensibilité au fil du temps.Quelques modèles de références : AKG C1000, Shure SM81 KSM32, Rode Videomic, Sony ECM, DPA 4006 4011.                                                Une capsule de microphone donne un signal correspondant à un point de l'espace sonore. Des agencements de capsules donnent plusieurs signaux qui permettent de représenter la direction de la source, ou d'obtenir des directivités particulières.Microphones stéréophoniques.Ensemble de 4 capsules en forme de tétraèdre donnant un goniomètre audio, et permettant de décider de la direction de l'axe et de la directivité à distance et après coup (Soundfield SPS200).Réseau de capsules alignées pour obtenir une directivité différente dans l'axe parallèle et l'axe perpendiculaire à l'alignement des capsules (Microtech Gefell KEM 970).L'hydrophone : il existe aussi des micros pour écouter les sons dans l'eau. Ces micros servent principalement à des usages militaires (écoute des bruits d'hélice pour la détection de sous-marins), à moins que l'on ne compte dans la catégorie les capteurs de Sonar.Le microphone de contact, qui capte les vibrations d'un solide comme le microphone piezzoélectrique.Un mouchard est un microphone de petite taille dissimulé afin de faire de l'espionnage.Les accessoires de microphone sontles filtres acoustiques (voir Taille de la membrane)les pieds de micro sur lesquels on peut les fixer ;les perches pour la prise de son pour l'image ;les suspensions élastiques pour éviter que le micro ne capte les vibrations de son support ;les écrans anti-pop pour éviter que le courant d'air produit par la bouche à l'émission de consonnes occlusives ou plosives « p », « b », « t » et « d » atteigne la membrane ;les bonnettes qui peuvent être en mousse de matière plastique ou des enveloppes en tissu, éventuellement double et avec poils synthétiques, pour éviter les bruits du vent et de la pluie ;les câbles de raccordement, qui doivent être de préférence souples pour éviter de transmettre des bruits ;les unités d'alimentation ;les réflecteurs paraboliques de prise de son ;les préamplificateurs de micros.Pierre Ley, « Les microphones », dans Denis Mercier (direction), Le Livre des Techniques du Son, tome 2 - La technologie, Paris, Eyrolles, 1988, 1re éd.Mario Rossi, Audio, Lausanne, Presses Polytechniques et Universitaires Romandes, 2007, 1re éd., p. 479-531 Chapitre 8, Microphones(en) Glen Ballou, Joe Ciaudelli et Volker Schmitt, « Microphones », dans Glen Ballou (direction), Handbook for Sound Engineers, New York, Focal Press, 2008, 4e éd.(de) Gehrart Boré et Stephan Peus, Mikrophone - Arbeitsweise und Ausführungsbeispiele, Berlin, Georg Neumann GmbH, 1999, 4e éd. (lire en ligne)(en) Gehrart Boré et Stephan Peus, Microphones - Methods of Operation and Type Examples, Berlin, Georg Neumann GmbH, 1999, 4e éd. (lire en ligne)(en) Ray A. Rayburn, Earle's Microphone Book : From Mono to Stereo to Surround — a Guide to Microphone Design and Application, Focal Press, 2012, 3e éd., 466 p.(en) Base de données de microphones Portail de la musique   Portail de l’électricité et de l’électronique   Portail des technologies   Portail de la radio   Portail de la télévision   Portail de l’informatique"
musique;"Un musicien ou une musicienne est une personne qui joue ou compose de la musique.En matière de musique vivante, on peut partir de l'opposition entre le musicien exécutant — chanteur ou instrumentiste — et l'auditeur de musique, dont le rôle est évidemment plus effacé. Cependant, lorsque chacun est à la fois exécutant et auditeur — par exemple, les membres d'une communauté religieuse en train de chanter des cantiques — cette distinction devient inopérante. Une deuxième opposition vient s'ajouter à la précédente et concerne les seuls musiciens : il s'agit de la distinction entre, d'une part le compositeur, qui « pense » et « écrit » la musique — pour les types de musique possédant un système de notation —, d'autre part l'interprète, qui exécute celle-ci — avec selon les cas, une plus ou moins grande part d'invention personnelle et d'improvisation. Certains musiciens célèbres, interprètes — individus ou groupes — ou compositeurs, ont laissé dans l'histoire des traces justifiant une étude biographique distincte du courant musical auquel ils appartiennent.Un chanteur utilise sa voix ;Un instrumentiste joue d'un instrument de musique ;Un compositeur compose de la musique ;Un parolier (ou auteur) écrit des textes ;Un chef, chef d'orchestre ou chef de chœur, dirige un ensemble musical ;Un auteur-compositeur-interprète écrit le texte et la musique de chansons qu'il chante lui-même.Les CFPM préparent au titre de musicien indépendant des musiques actuelles reconnu par l'État RNCP niveau IV.L'ONISEP distingue parmi les divers métiers de la musique les professionnels auxquels on peut donner la qualification de musiciens et ceux à qui on ne peut pas la donner. Chanteur Un chanteur est quelqu’un qui vocalise des sons musicaux avec un ton et une hauteur, et utilise sa propre voix pour produire de la musique. Les chanteurs peuvent chanter en solo ou en groupe et sont souvent accompagnés de musique instrumentale. La forme de chant la plus populaire aujourd’hui est le rap. En 2020, Forbes révèle que le chanteur le mieux payé au monde est un rappeur et c’est Kaynye West avec pas moins de 170 millions de dollars de revenus. Instrumentiste Techniquement parlant, un instrumentiste est toute personne qui ne joue que d’un instrument de musique, de manière professionnelle. Généralement, ils font partie d’un orchestre, groupe ou ensemble musical. En France, l’intervention d’un instrumentiste solo pour une performance coûte environ 540 euros. DJ Un disc-jockey, plus communément abrégé en DJ, est un animateur qui joue de la musique préenregistrée. En y apportant certaines modifications à l’aide d’outils électroniques, celui-ci devient un artiste qui propose ses créations ou mix à un public. En France, une prestation d’un animateur DJ pour un public privé coûte en moyenne 622 euros. Mais le DJ le mieux payé en France reste David Guetta avec un revenu annuel qui varie entre 10 et 30 millions de dollars par an. Chef d’orchestre Un chef d'orchestre dirige par ses gestes et son contact visuel une performance musicale de plusieurs instrumentistes ou chanteurs. Interprète Les interprètes sont des musiciens, chanteurs ou instrumentistes, professionnels, qui maîtrisent des répertoires d’autres artistes. Ils reproduisent des titres sur scène moyennant une rémunération sous forme de cachets ou de factures pour le musicien micro-entrepreneur ou en association. En France, une prestation d’interprète coûte entre 620 et 720 euros. Compositeur Un compositeur est un musicien qui crée des compositions musicales. Dans le langage courant ce titre est principalement réservé à ceux qui composent de la musique classique comme Mozart ou de la musique de film comme John Williams. Ceux qui écrivent la musique des chansons populaires peuvent être appelés auteurs-compositeurs. Auteur-compositeur-interprète Un auteur-compositeur-interprète écrit le texte et la musique de chansons qu'il présente lui-même à son public. Parolier Un parolier est un auteur-compositeur qui écrit des paroles pour des chansons, par opposition à un compositeur, qui compose des musiques pour des chansons. Le plus grand parolier de tous les temps est très certainement Bob Dylan, prix Nobel de littérature en 2016 «?pour avoir créé dans le cadre de la grande tradition de la musique américaine de nouveaux modes d’expression poétique?».Liste de musiciensMusique Portail du travail et des métiers   Portail de la musique"
musique;"La musique classique désigne, pour le grand public, l'ensemble de la musique occidentale appelée « savante » par distinction avec la musique populaire, à partir de la musique médiévale et jusqu'à nos jours. Si une ambiguïté persiste dans la définition de ce que représente la musique classique, elle est due à l'usage systématique (et sans doute impropre) de l'adjectif « classique », qui sert également à définir une période spécifique de la musique savante occidentale, dont les œuvres datent d'entre 1750 (date de la mort de Bach) et les années 1820 (voir Classicisme et Musique de la période dite classique). On inclut ainsi dans le terme de « musique classique », dans le langage commun, l'ensemble des traditions écrites de la musique occidentale, à partir des chants grégoriens ou du système écrit de Guido d'Arezzo jusqu'aux différentes avant-gardes expérimentées depuis 1945, allant de Pierre Boulez à Thomas Adès.Le terme de « musique classique » est marqué d'une compréhension à deux niveaux. Il semble englober en son sein :Les musiques considérées comme nobles, solennelles ou vouées à des franges dominantes de la société (en témoignent l'acceptation en son sein des chants liturgiques et, plus généralement, des harmonies jugées  complexes en opposition avec une monodie plus « populaire »), qui semblent réservées à certaines salles de concert prestigieuses et certains instruments particuliers, et sont issues d'un héritage généralement voué aux classes supérieures.Les musiques qui présupposeraient une érudition et un apprentissage particulier, souvent dispensé dans les écoles de musique et les conservatoires, qu'il s'agisse d'un apprentissage pratique des instruments-phares (piano, violon, violoncelle) ou d'un apprentissage des matières théoriques (formation musicale, écriture, harmonie ou analyse).Une autre manière de distinguer la musique « classique » est d'évoquer l'importance de la tradition écrite dans son déroulement à travers les siècles : cette particularité du mode d'expression musical est moins présente dans l'expression des musiques populaires ou traditionnelles, notamment de la musique dite « légère ».L'acceptation du terme de musique ""classique"" correspond de nos jours à une synecdoque généralisée : on entend par ""musique classique"" l'ensemble des musiques savantes composées depuis le Moyen Âge, mais le terme se confond avec un adjectif qui, du point de vue musicologique, fait conventionnellement référence au « classicisme », période souvent considérée comme la plus représentative du genre par le grand public. Si l'utilisation du terme de ""musique savante"" permettrait d'évoquer la complexité de cette musique en comparaison aux musiques populaires tout en rappelant que ce genre continue d'évoluer encore aujourd'hui, le terme de ""musique classique"" reste le plus populaire, et est automatiquement assimilé à des œuvres comme Les Quatre Saisons de Vivaldi, ou aux compositions de Bach, Mozart ou Beethoven. Dans les faits, et tel que le genre est défini dans les écoles de musiques et conservatoires, la musique ""classique"" rassemble toutes les musiques savantes occidentales transmises par des partitions (à la différence des musiques populaires et traditionnelles qui se transmettent le plus souvent par l'oral), à partir des chants monodiques, avec le système tonal et, par la suite, avec les systèmes atonal, modal, polytonal ou dodécaphonique. L'utilisation du terme de musique classique est plutôt récente, et on suggère que l’expansion de l'industrie musicale de la seconde moitié de XXe siècle est à l’origine du changement de terminologie : l'adjectif « classique »  remplace l'adjectif « savante » utilisé auparavant, afin de rompre avec la perception des pratiques liées à ce genre musical.Pour définir la musique classique, qui est d'ordinaire considérée comme ""savante"", ""sérieuse"" ou ""grande"", il faudrait pouvoir la confronter à une musique qui n'entrerait dans aucun de ces trois adjectifs ; or, si on a souvent opposé musique savante et musique populaire, la frontière qui délimite leurs deux champs se trouve souvent être mince. La musique classique sait s'inspirer de thèmes populaires (on peut penser, au XVIIIe siècle, au concerto comique de Michel Corrette La servante au bon tabac qui s'inspire du thème populaire J'ai du bon tabac (1733), ou de façon bien plus renommée, à l'appropriation du thème Ah ! vous dirai-je, maman dans les variations de Mozart (1781/1782)), et l'influence des musiques populaires sur le genre est perceptible dès la Renaissance, en voyant par exemple les motifs musicaux de la villanelle être repris dans les livres de madrigaux. Dans le sens inverse, la musique savante peut s'ancrer dans le monde séculier, et elle le fait souvent grâce aux mélodies les plus connues de son répertoire : par exemple, le chœur Va, pensiero de Nabucco a été utilisé par les révolutionnaires italiens lors du Risorgimento, et  les supporters du FC Nantes ou des Girondins de Bordeaux utilisent la « Marche triomphale » d'Aïda du même Giuseppe Verdi dans les chants qu'ils entonnent pendant les matchs : cette conjonction immédiate de deux mondes que l'imaginaire commun a tendance à nettement séparer, renforce l'idée que musique classique et musique populaire ne sont pas deux termes satisfaisants pour rendre compte des phénomènes en jeu.La musique classique trouve son origine tant dans le chant grégorien que dans la musique profane des troubadours et trouvères médiévaux. Les musiciens sont d'abord des nobles, puis des roturiers éclairés, cultivés, pratiquant un art de la composition que l'on ne peut pas qualifier de populaire, car rarement adressé au peuple lui-même. L'art des troubadours et des trouvères ne se confond pas alors avec celui des « ménestrels », musiciens ambulants populaires, formés dans les nombreuses écoles de « ménestrandie », ancêtres des académies et conservatoires actuels. Dès les débuts, la distinction entre « populaire » et « savant » semble moins s'ancrer sur une différence liée à la musique elle-même, ou à sa qualité, mais sur une différence de statut social qui précéderait la création de la musique : les ménestrels pratiquant un art tout autant appris que celui des troubadours/trouvères, la seule vraie différence se situe dans le milieu social vers lequel ils s'adressent.En latin, le sens premier de ""classis"" est l'appel  des citoyens pour la défense du pays, puis par extension les différentes classes de citoyens susceptibles d'être appelés. Le pluriel de classis,  ""classici"", désigna les citoyens appartenant à la première des classes créées par Servius Tullus ; de là le sens de ""scriptores classici"" qui désignait les  ""écrivains de première classe"", d'où les ""classiques"". L'adjectif « classique » pourrait ainsi désigner, tel que le Dictionnaire de l'Académie française l'appliquait à la littérature, « un auteur ancien fort approuvé et qui fait autorité dans la matière qu’il traite ». L'autorité acquise par ces auteurs classiques (souvent les antiques grecs et romains) sur les créateurs des XVIIe et XVIIIe, pourrait peut-être se retrouver dans la relation entre l'opinion commune actuelle et les œuvres de Mozart ou Beethoven. Ainsi, l'opinion commune continue à identifier la musique ""classique"" de façon assez claire et stéréotypée, sans tenir compte des nombreuses évolutions dont a pu témoigner l'histoire de la musique savante occidentale, du Moyen Âge à nos jours : le terme désigne aujourd'hui, des œuvres souvent datées d'au moins un siècle, transmises à l'écrit par des partitions, et qui exigeraient une écoute « attentive ». Le degré de raffinement et d'éducation nécessaire pour écouter de la musique classique serait décisif : le décorum des concerts de musique classique en serait la preuve, puisqu'il se distingue par des règles très précises (l'absence de communication entre les interprètes et la musique en comparaison avec les musiques populaires, l'interdiction d'applaudir entre les mouvements d'une pièce, et l'interdiction communément admise de danser, parler ou chanter pendant que la pièce est jouée).Outre l'emploi conscient de techniques musicales et d'une organisation formelle hautement développées, c'est probablement l'existence d'un répertoire qui différencie le plus sûrement la musique classique de la musique populaire, et ce, depuis le début de la Renaissance. La musique d'essence populaire était alors peu ou pas écrite,  transmise à l'oral, ce qui a limité la constitution d'un répertoire fixé dans un temps long. De même que pour les contes collectés par les frères Grimm, le travail de Bartok et Kodaly au début du XXe siècle a consisté à ""récupérer"" les thèmes populaires des campagnes slaves pour les empêcher de tomber dans l'oubli : là où les frères Grimm ont écrit avec des mots, Bartok et Kodaly ont pu éditer des retranscriptions et permettre à ce patrimoine de subsister.De nos jours, le problème peut s'envisager autrement : la musique populaire gravée sur disque ne laisse place qu'à une seule interprétation admise par tous (les versions produites en concert, à l'exception des enregistrements de concerts eux-mêmes sortis en disques, ne sont considérées que comme des reproductions d'une autre musique), quand la tradition musicale savante différencie l'interprète du compositeur, et laisse les interprètes produire des versions à chaque fois différentes d'une même pièce, en attribuant à chaque interprétation le même crédit et la même légitimité. La musique savante occidentale ne semble donc pas se définir par une écoute ou une archive sonore comme la musique populaire actuelle : elle trouve sa source dans un répertoire écrit sans cesse renouvelé, autant par les compositeurs qui élargissent le catalogue disponible, que par les interprètes qui proposent sans cesse des nouvelles versions de pièces potentiellement déjà jouées des milliers de fois.La musique classique disposerait donc de ce que Nicholas Cook a appelé un « capital esthétique », c’est-à-dire un répertoire, de par la distinction entre interprète et compositeur, tandis que la musique populaire serait écrite pour ou par un musicien ou un groupe de musiciens pour lui-même,.Toujours d'après Nicholas Cook, la conception de la musique dont notre époque a hérité date du XIXe siècle, et tient principalement au personnage de Ludwig van Beethoven. La notion de répertoire, de « musée musical » dont Liszt réclamera la fondation en 1835 en tant qu'institution, n'aurait pas existé avant l'ère romantique. Ainsi, des compositeurs tels que Jean-Philippe Rameau, Jean-Sébastien Bach ou Joseph Haydn écrivaient leurs œuvres pour une occasion précise (la messe du dimanche ou le dîner du prince Esterházy par exemple), et le rituel du ""concert"" comme exacerbation des émotions du public, ne s'est développé que récemment. La Passion selon saint Matthieu, dont l'exécution en 1829 par Felix Mendelssohn était la première depuis la création de l'œuvre cent ans plus tôt, a été présentée au public moins comme un objet de culte et une représentation de la Passion du Christ, que comme un objet musical digne d'intérêt esthétique ""pur"". Ce que souligne ainsi Nicholas Cook, c'est que le terme de musique classique a été créé pour désigner les œuvres de ce musée musical imaginaire, musée qui n'existait pas avant que le XIXe siècle ne métamorphose la perception qu'avait le public de la majorité de la musique savante. La notion de musique classique aurait donc été formée a posteriori de la moitié de la musique qu'elle est censée désigner, et serait donc plus que sujette à caution.S'il est difficile de vraiment considérer la musique du Moyen Âge comme faisant partie de la musique « classique », il faut reconnaître que ses principes sont l'origine même des langages musicaux employés depuis lors. La musique savante occidentale commence (au-delà des rares traces de musiques antiques, comme l'Épitaphe de Seikilos) avec la monodie médiévale, et c'est à partir d'elle que l'ensemble des innovations suivantes s'ancrent.D'elle vient également la relation difficile à entremêler entre la musique savante et la musique cérémonielle, vouée aux cérémonies religieuses : la difficulté pour nous de différencier musique « classique » et musique « populaire » vient sans doute du manque de précision dont fait preuve un terme comme « musique médiévale », qui confond tous les styles, tous les ensembles d'instruments, et toutes les occasions différentes auxquelles la musique était dédiée.Jean-François Paillard, La Musique française classique - collection Que sais-je ?, no 878 - PUFNicholas Cook (trad. de l'anglais par Nathalie Getnili), Musique, une très brève introduction [« Music, a very short introduction »], Paris, Allia, 2006, 155 p. (ISBN 2-84485-206-8, OCLC 227201878, BNF 40094540)John Burrows, Charles Wiffen, La Musique classique, Gründ, 2006, Collection Le Spécialiste, 512 p.  (ISBN 978-2-7000-1347-4)Patrick Hauer, Dictionnaire des grands compositeurs et leurs œuvres, du XVIIe siècle au XXe siècle, éditions Dictionnaires d'aujourd'hui, 2007, 660 p.Régis Chesneau, Pour en finir avec le « classique », Paris, Éditions L'Harmattan, coll. « Logiques sociales : Série Musiques et champ social », 2019, 200 p. (ISBN 9782140123535, OCLC 1237118810, lire en ligne)Hyacinthe Ravet, « La petite musique du genre, ou comment combattre le sexisme dans la musique classique », sur The Conversation, 16 octobre 2019.Chronologie de la musique classique occidentale Portail de la musique classique   Portail des arts"
musique;"Une œuvre de musique de chambre est une composition musicale dédiée à un petit ensemble de cordes, bois, cuivres ou percussions, qui traditionnellement et avant l'affirmation des concerts publics, pouvait tenir dans la « grande chambre » d'un palais. Chaque partie est écrite pour un seul instrumentiste sans qu'il n'y ait à l'origine ni chef d'orchestre ni soliste attitré. Si certaines voix sont doublées ou triplées, particulièrement dans les cordes, on parle d'orchestre de chambre ; au-delà, on parle d'orchestre, avec le qualificatif de sa composition instrumentale (symphonique, à cordes, d'harmonie, de fanfare).La conception de ces ensembles -  duos, trios, quatuors, quintettes, sextuors, septuors, octuors, nonettes et dixtuors - demande aux compositeurs une connaissance de l'harmonie, de la polyphonie, du contrepoint, de l'organologie. En musique classique, la composition de quatuors à cordes est le plus souvent considérée comme l'un des summums de l'écriture musicale[réf. nécessaire].Ces ensembles de quelques solistes, menant leur voix indépendamment des autres, ont le plus souvent comme « meneur » l'instrument le plus aigu (le premier violon dans le quatuor à cordes, le flûtiste dans le quintette à vent), mais certaines œuvres plus complexes ou d'un effectif plus important, nécessitent la présence (toujours facultative, mais souvent essentielle) d'un chef d'orchestre (Gran Partita pour 13 instruments à vent en si bémol majeur de Mozart, Sérénade en ré mineur, op. 44 pour vents, violoncelle et contrebasse de Dvo?ák).En principe, le terme exclut les pièces comportant une partie chantée, malgré le caractère intimiste que revêtent souvent la mélodie et le lied. Certains compositeurs des XXe et XXIe siècles cependant, introduisent la voix dans des ensembles de musique de chambre, comme Schönberg dans son deuxième quatuor à cordes, op. 10 (1908), Ottorino Respighi dans Il tramonto (1914), Olivier Greif dans son Quatuor à cordes no 3 avec voix (1998) et Elena Ruehr dans son deuxième quatuor « Song of the Silkie » (2000). Dans certaines œuvres récentes, le compositeur choisit d'adjoindre une partie de sons fixés (bande magnétique ou CD par exemple) aux instruments acoustiques, voire de les amplifier à l'aide de microphones, ou même de transformer leur son grâce à l'ordinateur[réf. nécessaire].L'expression « musique de chambre » apparaît à la période baroque, les compositions qu'elle recouvre étant destinées à être jouées dans l'intimité des intérieurs de nobles ou d'amateurs fortunés. Dès le Moyen Âge et pendant la Renaissance de nombreux ensembles de cordes, de bois, de cuivres, mixtes ou en consort se forment et jouent indépendamment de la polyphonie vocale, mais suivant le plus souvent ses règles d'écriture à quatre voix (soprano, alto, ténor et basse). Ces compositions de camera s'opposent alors aux œuvres de la chiesa, de la chapelle et ainsi apparaissent comme l'expression d'une musique profane. Les sonates de Giovanni Legrenzi donnent leur titre définitif à ces compositions. A la fin du XVIIIe siècle, le quatuor à cordes devient l'expression la plus aboutie de la composition pour amateurs. Mozart et Haydn introduisent des formes qui séparent nettement la musique de chambre des symphonies pour orchestre.Au tournant du  XXe siècle, avec Gustav Mahler et surtout Arnold Schönberg, elle quitte les espaces privés pour rejoindre l'orchestre et de nouveaux territoires.Il existe de nombreux types de duos qui peuvent porter des titres les plus divers :du même instrument : souvent écrits dans un but pédagogique, certains ont dépassé le cadre d'une classe instrumentale, par exemple ceux pour 2 cors de Mozart, 2 violoncelles d'Offenbach, ceux pour 2 clarinettes ou 2 violons de Bartók ou pour 2 saxophones de Hindemith. Le piano est bien sûr un cas particulier avec des œuvres écrites soit pour piano quatre mains soit pour deux pianos ; il existe là un répertoire de concert spécifique et très varié, d'œuvres originales comme les Marches militaires de Schubert, les Danses hongroises de Brahms, les Danses slaves de Dvo?ák pour citer les plus connues) ou d'arrangements parfois faits par les auteurs eux-mêmes comme Ravel et son Boléro ;de deux instruments différents :pour deux instruments mélodiques, ce sont souvent des œuvres de commande pour deux amis (…),les plus fréquents associent un instrument mélodique et un instrument polyphonique comme l'immense répertoire des sonates pour instrument et clavier (piano, clavecin mais aussi orgue, harpe, guitare…). Les premiers exemples sont pour « dessus et basse continue » : le « dessus » pouvant être un violon, une flûte à bec ou traversière, un hautbois, une musette… ; la « basse continue » peut comprendre plusieurs instrumentistes (théorbe, viole de gambe, épinette, violoncelle, clavecin, basson ou guitare…) mais elle est toujours considérée comme une seule voix puisque tous doublent la partie de basse. Les sonates existent pour tous types de combinaisons et certains compositeurs comme Paul Hindemith se sont donné pour but d'en écrire pour chacun des instruments de l'orchestre.En musique baroque, la sonate en trio (sonata a tre, triosonate) désigne une écriture à trois parties, mais pouvant être jouée par :un musicien, par exemple les Six sonates en trio pour orgue de Jean-Sébastien Bach, (BWV 525 à BWV 530), une main par clavier plus la basse au pédalier ;deux musiciens, sonates pour violon et clavecin concertant (BWV 1014-1019), une voix pour le violon, une pour la main droite et une pour la main gauche du clavecin ; cette dernière, sortant du rôle de basse continue, peut être occasionnellement doublée par un violoncelle ou un basson ;trois musiciens, toutes solutions possibles sans basse continue ;quatre (ou plus) musiciens, la « sonate en trio » est alors écrite pour deux dessus et une basse continue, cette dernière étant jouée par une basse mélodique (viole de gambe, violoncelle, basson…) et un ou plusieurs instruments polyphoniques (théorbe, guitare, épinette, clavecin, positif, orgue…) ;cinq musiciens, la « sonate en trio » est alors écrite pour trois instruments mélodiques et une basse continue, cette dernière étant jouée par une basse mélodique (viole de gambe, violoncelle, basson…) et un ou plusieurs instruments polyphoniques (théorbe, guitare, épinette, clavecin, positif, orgue…). Exemples : les sonates en trio pour 2 hautbois, basson et basse continue de Fasch, Heinichen, Zelenka, avec une basse harmonisée au clavecin et / ou au théorbe, doublée par une contrebasse ou un violoncelle…Par la suite, ce sont surtout les trios avec piano qui dominent, mais on trouve aussi des ensembles d'instruments de même famille comme le trio à cordes (violon, alto et violoncelle), le trio d'anches (hautbois, clarinette, basson), le trio de cuivres (trompette, cor, trombone). Certains compositeurs ont usé de formules plus audacieuses comme Mozart avec ses « trios pour cors de basset », Haydn et ses 126 Barytontrios pour baryton à cordes, alto et basse, Beethoven et son trio pour deux hautbois, cor anglais… Au cours du XXe siècle, des trios beaucoup plus audacieux (…).Idéal de la musique occidentale classique, cette formule met en valeur au maximum le potentiel de la musique polyphonique et harmonique. Elle fut d'abord vocale, puis instrumentale et basée sur l'écriture à quatre voix superposées soprano, alto, ténor et basse. La formation reine de la musique de chambre est évidemment le quatuor à cordes, dont l'immense répertoire est à la fois un modèle, un but à atteindre et une source d'inquiétude pour chaque compositeur qui ose s'y risquer…Il existe de nombreux autres quatuors d'instruments de même famille : on trouve ainsi des quatuors de saxophones, de trombones, de clarinettes, de violoncelles, de flûtes, de percussions...Il existe des quatuors hétérogènes, comme le quatuor avec piano (violon, alto, violoncelle et piano), le quatuor d'anches (hautbois, clarinette, saxophone et basson), voire des ensembles combinant bois, cordes et instruments polyphoniques comme le Quartette op. 22 d'Anton Webern (violon, clarinette, saxophone et piano).Les quintettes sont souvent une extension des quatuors à un autre instrument, comme les quintettes pour quatuor à cordes et contrebasse (une sorte de « petit orchestre »), piano, ou clarinette par exemple.Il existe cependant deux formules entièrement originales : le quintette à vent (flûte, hautbois, clarinette, cor et basson) et le quintette de cuivres (deux trompettes, cor, trombone et tuba).Les formules les plus connues sont pour cordes, mais aussi pour percussions ou clarinettes. Ils sont souvent l'occasion pour un compositeur d'entremêler des sonorités très variées, comme dans le Sextetto mistico de Heitor Villa-Lobos (flûte, hautbois, saxophone, célesta, harpe et guitare) ou le Septuor pour cordes, piano et trompette de Camille Saint-Saëns. Au-delà de sept instruments, on rejoint souvent les « petits ensembles », qui nécessitent en général la présence d'un chef d'orchestre, quoique Mozart (entre autres) ait écrit des octuors d'instruments à vent.Florent Albrecht, Festivals de musique de chambre en France : dynamiques et enjeux contemporains, L'Harmattan, Paris, Budapest, Torino, 2003, 197 p.  (ISBN 2-7475-5191-1)Walter Willson Cobbett, Dictionnaire encyclopédique de la musique de chambre, complété sous la direction de Colin Mason, traduit de l'anglais par Marie-Stella Pâris, édition française revue et augmentée par Alain Pâris, Robert Laffont, coll. « Bouquins », Paris, 1999, 2 vol., 1627 p.  (ISBN 2-221-07847-0) (vol. 1 : A-J) ;  (ISBN 2-221-07848-9) (vol. 2 : K-Z)André Cœuroy et Claude Rostand, Les chefs-d'œuvre de la musique de chambre, Éd. Le Bon Plaisir, Plon, Paris, 1952, 282 p.Joël-Marie Fauquet, Les Sociétés de musique de chambre à Paris de la Restauration à 1870, Aux amateurs de livres, Paris, 1986, 448 p.  (ISBN 2-905053-25-9) (texte remanié d'une thèse de 3e cycle de Musicologie, Paris 4, 1981)Gérard Pernon, Dictionnaire de la musique, Paris, Jean-Paul Gisserot, Coll. Histoire de la musique, 2007, p. 50-51.Serge Gut et Danièle Pistone, La musique de chambre en France : de 1870 à 1918, H. Champion, Paris, 1985, 239 p.  (ISBN 2-85203-048-9)François-René Tranchefort (dir.), Guide de la musique de chambre, Fayard, Paris, 1989 (plusieurs rééd.), 995 p.  (ISBN 2-213-02403-0)Marc Vignal (dir.), Larousse de la musique, 1982 -  (ISBN 2-03-511303-2)Mozart et la musique de chambre, films documentaires réalisés par Alex Szalat, ADAV, Paris, 1991, 2 parties : 1. L'Enfant de l'Europe ; L'Art de la fugue (1 cassette vidéo VHS, 2 × 52 min) ; 2. À mon cher ami Haydn ; Histoire d'un texte ; Mozart et les quintettes (1 cassette vidéo VHS, 3 × 52 min) Portail de la musique classique"
musique;"La musique est un art et une activité culturelle consistant à combiner sons et silences au cours du temps. Les paramètres principaux sont le rythme (façon de combiner les sons dans le temps), la hauteur (combinaison dans les fréquences), les nuances et le timbre. Elle est aujourd'hui considérée comme une forme de poésie moderne.La musique donne lieu à des créations (des œuvres d'art créées par des compositeurs), des représentations. Elle utilise généralement certaines règles ou systèmes de composition, des plus simples aux plus complexes (souvent les notes de musique, les gammes et autres). Elle peut utiliser des objets divers, le corps, la voix, mais aussi des instruments de musique spécialement conçus, et de plus en plus tous les sons (concrets, de synthèses, abstraits, etc.).La musique a existé dans toutes les sociétés humaines, depuis la Préhistoire. Elle est à la fois forme d'expression individuelle (notamment l'expression des sentiments), source de rassemblement collectif et de plaisir (fête, chant, danse) et symbole d'une communauté culturelle, nationale (hymne national, musique traditionnelle, musique folklorique, musique militaire) ou spirituelle (musique religieuse).Bien que la musique soit une caractéristique universelle de la culture humaine, peu de choses sur ses origines et ses fonctions sont établies. La musique pourrait avoir une origine commune avec la parole. Ce concept est nommé le musilangage et propose que le chant et la parole ont servi de première forme de communication aux ancêtres des humains. Charles Darwin et Herbert Spencer ont les premiers proposé une origine évolutive à la musique. Darwin a suggéré notamment que la musique a permis la communication d'émotions et a servi dans les mécanismes de séduction d'une manière similaire au chant des oiseaux. La musique aurait pu donc évoluer comme un signal de qualité génétique pour les compagnons potentiels. Cependant, si cette hypothèse est communément admise chez les oiseaux, il reste en 2015, difficile de trouver des preuves concrètes pour confirmer cette hypothèse chez l'homme.L’origine de la musique est notamment le sujet d'étude de la musicologie évolutive (en).L'histoire de la musique est une matière particulièrement riche et complexe, principalement du fait de ses caractéristiques : la difficulté tient d'abord à l'ancienneté de la musique, phénomène universel remontant à la Préhistoire, qui a donné lieu à la formation de traditions qui se sont développées séparément à travers le monde sur des millénaires. Les premiers instruments représentés sur des peintures rupestres incluent l'arc musical, et attestent de leur utilisation, il y a plus de 13 000 ans. Dans la grotte de Hohle Fels, une flûte en os datée de 35 000 ans avant notre ère a été retrouvée. Un autre type d'instrument primitif est le Rhombe. Il y a donc une multitude de très longues histoires de la musique selon les cultures et civilisations. La musique occidentale (musique classique ou pop-rock au sens très large) ne prennent qu'au XVIe siècle l'allure de référence internationale, et encore très partiellement.La difficulté vient également de la diversité des formes de musique au sein d'une même civilisation : musique savante, musique de l'élite, musique officielle, musique religieuse, musique populaire. Cela va de formes très élaborées à des formes populaires comme les berceuses. Un patrimoine culturel d'une diversité particulièrement large, contrairement à d'autres arts pratiqués de manière plus restreinte ou élitiste (littérature, théâtre…). Enfin, avec la musique, art de l'instant, se pose la question particulière des sources : l'absence de système de notation d'une partie de la musique mondiale, empêche de réellement connaître l'étendue de la musique du temps passé, la tradition n'en ayant probablement sauvé qu'un nombre limité.La réalisation d'une synthèse universelle apparaît très difficile car beaucoup d'histoires de la musique traitent essentiellement de l'histoire de la musique occidentale. Il n'est en général possible que de se référer aux ouvrages et articles spécialisés par civilisation ou par genre de musique.Il existe alors deux « méthodes » pour définir la musique : l’approche intrinsèque (immanente) et l’approche extrinsèque (fonctionnelle). Dans l'approche intrinsèque, la musique existe chez le compositeur avant même d’être entendue ; elle peut même avoir une existence par elle-même, dans la nature et par nature (la musique de la rivière, des oiseaux…, qui n'a aucun besoin d'intervention humaine). Dans l'approche extrinsèque, la musique est une fonction projetée, une perception, sociologique par nature. Elle a tous les sens et au-delà, mais n'est perçue que dans un seul : la musique des oiseaux n'est musique que par la qualification que l'on veut bien lui donner.L'idée que l'être est musique est ancienne et semble dater des pythagoriciens selon Aristote. Dans la Métaphysique il dit : « Tout ce qu'ils pouvaient montrer dans les nombres et dans la musique qui s'accordât avec les phénomènes du ciel, ses parties et toute son ordonnance, ils le recueillirent, et ils en composèrent un système ; et si quelque chose manquait, ils y suppléaient pour que le système fût bien d'accord et complet ».Il est à noter que la définition de la science des sons par les pythagoriciens est « une combinaison harmonique des contraires, l'unification des multiples et l'accord des opposés ». La science des sons est une des quatre sciences de la mesure, supérieure aux mathématiques car elle s'appuie sur la justesse, si vous essayez de terminer l'opération de diviser 10 par 3 en mathématiques vous ne pouvez terminer cette opération alors que le temps musical le permet.Les deux sciences sensibles de la mesure que sont la musique et l'astronomie ont été laissées de côté à l'époque de Platon pour ne conserver que les deux sciences techniques de la mesure que sont l'arithmétique et la géométrie. Il est bon de se rappeler qu'au départ la science des sons était éthique et médicale et servait à calmer les passions humaines et à remettre les facultés de l'âme à leur juste place, dixit Pythagore, et lorsque cette expérience était réalisée vous étiez capable d'être vous-même et de là d'acquérir les savoir-faire comme dans une sorte d'accordage de l'être humain qui vise à laisser s'exprimer la résonance universelle de la sagesse.Cette définition intègre l'homme à chaque bout de la chaîne. La musique est conçue et reçue par une personne ou un groupe (anthropologique). La définition de la musique, comme de tout art, passe alors par la définition d'une certaine forme de communication entre les individus. La musique est souvent jugée proche du langage (bien qu'elle ne réponde pas à la définition ontologique du langage), communication universelle susceptible d'être entendue par tous et chacun, mais réellement comprise uniquement par quelques-uns. Boris de Schlœzer, dans Problèmes de la musique moderne (1959), dit ainsi : « La musique est langage au même titre que la parole qui désigne, que la poésie, la peinture, la danse, le cinéma. Ceci revient à dire que tout comme l’œuvre poétique ou plastique, l’œuvre musicale a un sens (qui n’apparaît que grâce à l’activité de la conscience) ; avec cette différence pourtant qu’il lui est totalement immanent, entendant par là que close sur elle-même, l’œuvre musicale ne comporte aucune référence à quoi que ce soit et ne nous renvoie pas à autre chose ».La musique est généralement considérée comme un pur artefact culturel. Certains prodiges semblent néanmoins disposer d'un don inné. Les neuropsychologues cherchent donc à caractériser les spécificités des capacités musicales. Le caractère plus ou moins inné des talents artistiques est scientifiquement discuté.Pour beaucoup, la musique serait propre à l'humain et ne relèverait que peu de la biologie, si ce n'est par le fait qu'elle mobilise fortement l'ouïe. Un débat existe pourtant sur le caractère inné ou acquis d'une partie de la compétence musicale chez l'Homme, et sur le caractère adaptatif ou non de cette « compétence ». Compétence « culturelle » ? Plusieurs arguments évoquent une origine et des fonctions culturelles ou essentiellement socio-culturelles. De nombreux animaux chantent instinctivement, mais avec peu de créativité, et ils semblent peu réceptifs à la musique produite par les humains. Une rythmique du « langage » et du chant existe respectivement chez les primates et chez les oiseaux, mais avec peu de créativité.Chez l'humain, le chant et le langage semblent relever de compétences cérébrales en partie différentes. L'alphabet morse est une sorte de code « musical » qui a un sens (caché pour celui qui ignore le code). C'est clairement un artefact culturel (personne ne naît en comprenant le morse, car ni sa production ni son interprétation ne sont inscrites dans nos gènes). Chez l'homme, la voix, le langage et la capacité à interpréter un chant évolue beaucoup avec l'âge, ce qui évoque un lien avec l'apprentissage.Enfin, la musique n'est pratiquée « à haut niveau » que par quelques individus, et souvent après un long apprentissage ; ceci évoque une origine culturelle, ce que les ethnomusicologues et les compositeurs contemporains ont longtemps renforcé. Mais il existe des exceptions, et l'exploration du fonctionnement du cerveau questionne ce point de vue. Compétence « biologique » ? Les neuropsychologues ont dès le début du XXe siècle mis en évidence une composante génétique à certains troubles de l'élocution. Ils ont aussi démontré que certaines structures du cerveau (aires cérébrales frontales inférieures pour l'apprentissage de la tonalité,, et l'hémisphère droit notamment) dont l'intégrité est indispensable à la perception musicale, révèlent l'existence d'un substrat biologique. Ce substrat neuronal peut d'ailleurs être surdéveloppé chez les aveugles (de naissance ou ayant précocement perdu la vue) ou être sous-développé chez les sourds. Certains auteurs estiment que tout humain a une compétence musicale. Ceci ne permet cependant pas d'affirmer que la compétence musicale est biologiquement acquise.La musique, ou plus exactement la « capacité musicale », la « dysmélodie » et l'amusie congénitale, (incapacité à distinguer les fausses notes, associée à une difficulté à faire de la musique, ou à « recevoir » la musique), qui toucherait 4 % des humains selon Kalmus et Fry (1980), ou les émotions suscitées par la musique évoquent une composante biologique importante, notamment étudiée par le Laboratoire international de Recherche sur le Cerveau, la Musique et le Son (BRAMS) de l'Université de Montréal,. Des études pluridisciplinaires associant la musicologie à la génétique et aux recherches comportementales et comparatives permettraient de préciser les liens entre musique et fonctions cérébrales en neurosciences.La pratique de la musique semble être un « fait culturel » très ancien, mais 96 % des humains présentent des capacités musicales jugées « spontanées » par les neuropsychologues. Au-delà des aspects neurologiques de l'émission et de l'audition de la voix et du chant, le cerveau animal (des mammifères et oiseaux notamment) montre des compétences innées en termes de rythme, notamment utilisées pour le langage. La musique et la danse ont des aspects fortement transculturels ; elles semblent universellement appréciées au sein de l'humanité, depuis 30 000 ans au moins d'après les instruments découverts par l'archéologie, et la musique d'une culture peut être appréciée d'une autre culture dont le langage est très différent.L'imagerie cérébrale montre que la musique active certaines zones de plaisir du cerveau et presque tous les humains peuvent presque spontanément chanter et danser sur de la musique, ce qui peut évoquer des bases biologiques et encourager une biomusicologie.La mémorisation ou la production d'une mélodie semblent mobiliser des réseaux neuronaux particuliers. Compétence adaptative ? La musique aurait-elle une fonction biologique particulière ? ... même si elle ne semble pas avoir une utilité claire en tant que réponse adaptative (tout comme la danse qui lui est souvent associée).Quelques auteurs comme Wallin estiment que la danse et la musique pourrait avoir une valeur adaptative en cimentant socialement les groupes humains, via la « contagion émotionnelle » que permet la musique.Les résultats de l'étude de la compétence musicale du bébé et du jeune enfant (ex : chantonnement spontané), et de l'émotion musicale et du « cerveau musical » dans le cerveau, apportent des données nouvelles. Hauser et McDermott en 2003 évoquent une « origine animale » à la musique, mais Peretz et Lidji en 2006 proposent un point de vue intermédiaire : il existe une composante biologique, mais « la musique est une fonction autonome, contrainte de manière innée et faite de modules multiples qui ont un recouvrement minimal avec d’autres fonctions (comme le langage) ».Si la musique produit des effets sur les groupes, c'est parce que dès qu’on entend une mélodie, on peut s’y associer. Les muscles s’activeraient pour que l’on puisse se mettre à chanter ou à danser comme les autres. Ainsi, le rythme d’une mélodie servirait de ciment social en tissant un lien physique.D’ailleurs, la musique stimule des régions du cerveau dédiées à la perception du lien social. Il s’agit notamment du sillon temporal supérieur, une région du cortex cérébral localisée près des tempes, et qui s’active par exemple quand on observe les mouvements des yeux d’une personne, ou que l’on est sensible au ton de sa voix (et non à la signification des mots qu’elle prononce).En 2008, Nikolaus Steinbeis, de l’Institut Max Planck pour la cognition humaine et les sciences du cerveau, et Stefan Koelsch, de l’Université de Sussex en Grande-Bretagne, ont montré que cette zone « sociale » s’active chez des personnes écoutant des accords musicaux. Tout se passe comme si, en entendant de la musique, notre cerveau se tournait vers l’autre. La musique contribuerait à tisser des liens sociaux ; les hymnes le font à l’échelle des nations, les groupes de rock à celle des communautés d’adolescents, les comptines entre parents et enfants.La musique pourrait aussi avoir une base biologique forte, mais en quelque sorte résulter des hasards de l'évolution et n'avoir aucune fonction adaptative ; c'est une possibilité retenue en 1979 par Gould et Lewontin.Selon Claude Debussy, « la musique commence là où la parole est impuissante à exprimer ». Mais pour Saint-Saëns, « Pour moi, l'art c'est la forme. L'expression, voilà qui séduit avant tout l'amateur ». Pour Stravinsky, « L'expression n'a jamais été la propriété immanente de la musique ».Selon cette définition, la musique est l’« art des sons » et englobe toute construction artistique destinée à être perçue par l’ouïe.Parmi les œuvres musicales on distingue la composition musicale, produite avant son interprétation, et l'improvisation musicale, conçue au moment où le musicien la joue. Ces deux techniques suivent les règles de l'écriture musicale. La transcription musicale consiste à adapter une œuvre à un autre medium.La musique, comme art allographique, passe par l'œuvre musicale. Chacune est un objet intentionnel dont l'unité et l'identité est réalisée par ses temps, espace, mouvement et forme musicaux, comme l'écrit Roman Ingarden. Objet de perception esthétique, l'œuvre est certes d'essence idéale, mais son existence hétéronome se concrétise par son exécution devant un public, ou par son enregistrement, y compris sa numérisation. Comme toute œuvre, l'œuvre musicale existe avant d'être reçue, et elle continue d'exister après. On peut donc s'interroger sur ce qui fait sa pérennité : combien d'œuvres survivent réellement à leurs compositeurs ? Et sont-elles vraiment toutes le reflet de son style, de son art ? On entend surtout par œuvre musicale le projet particulier d'une réalisation musicale. Mais cette réalisation peut être décidée par l'écoute qu'en fait chaque auditeur avec sa culture, sa mémoire, ses sentiments particuliers à cet instant précis autant que par la partition, transcription qui ne comporte pas toute la musique. À partir de la Renaissance et jusqu'au début du XXe siècle, l'unique support de la musique a été la partition de musique. Cette intrusion de l'écrit a été l'élément-clé de la construction de la polyphonie et de l'harmonie dans la musique savante. La partition reste unie au nom du ou des musiciens qui l'ont composée ou enregistrée. Certaines œuvres peuvent être collectives, d'autres restent anonymes. Depuis la généralisation des moyens techniques d'enregistrement du son, l'œuvre peut également s'identifier à son support : l'album de musique, la bande magnétique ou à une simple calligraphie de la représentation du geste musical propre à transcrire l'œuvre du compositeur.L'informatique musicale a fait évoluer encore cette notion d'œuvre, puisqu'à présent un logiciel est susceptible d'engendrer « automatiquement » une œuvre musicale, ou de produire des sons auxquels l'interprète pourra réagir. Formalisme Dans son essai sur les « célibataires de l'art », Jean-Marie Schaeffer estime que, dans l’art moderne (et a fortiori dans l’art technologique du XXe siècle), la question-clé : « Qu’est-ce que l’art ? » ou « Quand y a-t-il art ? » s’est progressivement transformée en : « Comment l’art fonctionne-t-il ? ». En musique, ce déplacement d’objet a posé le problème des éléments que l’on peut distinguer a priori dans l’écoute structurelle d’une œuvre. En 1945 apparurent les premières formes d'informatique, et en 1957 on a assisté, avec l’arrivée de l'électronique musicale, à un point de bifurcation. D'abord une nouvelle représentation du sonore qui, bien que difficile à maîtriser, a en fait ouvert des perspectives nouvelles. Ensuite, ces techniques ont remis en cause certaines réflexions théoriques sur la formalisation de la pensée créatrice, renvoyant le compositeur à la confrontation, essentielle dans sa démarche, entre un formalisme abstrait et l’élaboration d’un matériau fonctionnel. La transition vers l’atonalité a détruit les hiérarchies fonctionnelles et transformé le rôle tenu par les fonctions tonales, élaboré depuis Monteverdi.De fait, la logique des formes musicales est donc devenue surtout une logique fonctionnelle, dans la mesure où elle permet de maintenir la cohésion de l'œuvre, même si les éléments de composition sont multiples (éléments rythmiques, contrapuntiques, harmoniques, etc.). La notion de processus compositionnel, a permis de passer de la vision statique de l’objet musical (celui que l’on peut répertorier, et qui cesserait de vivre en entrant dans le patrimoine) à une vision dynamique. Cette vision est évolutive, ce que ne prenaient pas en considération les théories fondées sur la GestaltPsychologie qui figent la pensée dans des images accumulées dans la mémoire. Le processus musical est plus que la structure : il est en effet une forme dynamique, un devenir. Ce devenir est marqué par l’empreinte du sonore, c’est-à-dire par un matériau musical, et pas uniquement par l’outil ou par la théorie.À partir de la théorie de la communication de Shannon et Weaver, d'autres définitions insistent plus sur les moyens de réception que sur la chaîne de production de la musique. Fonctionnalités L'utilisation de musique dans d'autres œuvres (qui sont donc des œuvres de collaboration tel qu'un film, un dessin animé, un conte musical ou un documentaire) pose la question des fonctionnalités de la musique, en particulier dans les contenus audiovisuels. La musique remplit des fonctions lorsqu’elle est utilisée (ou incorporée, synchronisée). La musicologue polonaise Zofia Lissa présente douze fonctions principales, la plupart n’étant pas mutuellement exclusives. Elle cherche à en comprendre la façon dont la musique est utilisée dans les films et l'effet qu'elle produit : par exemple la fonction de Leitmotiv qui contribue à tracer la structure formelle d'un film : description des personnages, des atmosphères, des environnements, ou encore la fonction d'anticipation d'une action subséquente. Plus largement, se pose la question des fonctionnalités de la musique dans un ensemble audiovisuel (qui peut être un flux radiophonique ou un flux télévisuel composé de contenus qui se succèdent sans interruption). Dans un tel contexte, la musique (sous la forme d'un indicatif d'émission, d'un jingle, etc.) remplit pour les diffuseurs diverses fonctions. Elle peut agir comme un élément d’accroche pertinente et capter une attention par sa capacité à séduire ou à émouvoir ou encore à annoncer. Mario d'Angelo, en s'appuyant doublement sur une compréhension des finalités recherchées du côté de l'offre (par les concepteurs des contenus audiovisuels et du flux télévisuel) et des finalités perçues du côté de la réception (par les téléspectateurs), retient six fonctions : mnémonique, identitaire, émotionnelle, esthétique, didactique et narrative ; elles ne sont pas mutuellement exclusives.Le temps gouverne la musique comme il gouverne la perception du son : depuis le micro-temps, qui est l'échelle de la vibration sonore car le son est une mise en vibration de l'air, jusqu'à la forme musicale, construction dans un temps de l'écoute. Comme la forme musicale ne nous est révélée qu’au fur et à mesure, chaque instant est en puissance un moment d’avenir, une projection dans l’inconnu. C’est le sens du titre d’une œuvre d’Henri Dutilleux qui propose de nous plonger dans le « mystère de l’instant ». Le théologien suisse Hans Urs von Balthasar livre cette métaphore judicieusement musicale de la condition humaine : « Faites donc confiance au temps. Le temps c’est de la musique ; et le domaine d’où elle émane, c’est l’avenir. Mesure après mesure, la symphonie s’engendre elle-même, naissant miraculeusement d’une réserve de durée inépuisable ». Composantes Dans cette composante temporelle, la musique peut se déployer selon trois dimensions fondamentales :le rythme, qui relève de la durée des sons et de leur niveau d'intensité (la dynamique) ;la mélodie, qui est l'impression produite par la succession de sons de hauteurs différentes ;la polyphonie ou harmonie (ces deux termes, pris dans leur sens le plus large), considère la superposition voulue de sons simultanés (cf aussi fusion) ;une autre catégorie du son est apparue dans la musique savante à partir du XVIe siècle, celle du timbre. Elle permet une polyphonie mêlant plusieurs instruments (le terme harmonie est d'ailleurs encore utilisé dans ce sens lorsque l'on parle de « l'harmonie municipale »), ou une monodie spécialement dédiée à l'un d'eux.Selon les genres musicaux, l'une ou l'autre de ces trois dimensions pourra prédominer :le rythme, par exemple, a généralement la primauté dans certaines musiques africaines traditionnelles (Afrique noire notamment) ;la mélodie prime dans la plupart des musiques de culture orientale ;l'harmonie est le socle de la musique savante occidentale, ou d'inspiration occidentale. Dimensions sonores et son Grâce au développement des recherches de l'acoustique musicale et de la psychoacoustique, le son musical se définit à partir de ses composantes timbrales et des paramètres psychoacoustiques qui entrent en jeu dans sa perception. D'objet sonore, matériau brut que le musicien doit travailler, ce matériau devient objet musical ; la musique permet de passer à une dimension artistique qui métamorphose le « donné à entendre ». Le silence n'est plus « absence de son ». Même le fameux 4?33? de John Cage, est un « donné à entendre ». Mais ce « donné à entendre » englobe désormais un matériau de plus en plus large. Depuis le début du XXe siècle, cet élargissement s’opère vers l’intégration des qualités intrinsèques de notre environnement sonore (concerts bruitistes, introduction des sirènes chez Varèse, catalogues d’oiseaux de Messiaen, etc.). Comment distinguer alors bruit et signal, comment distinguer ordre et désordre, création musicale et nuisance sonore ? Le bruit, c’est uniquement ce qu’on ne veut pas transmettre et qui s’insinue malgré nous dans le message ; en lui-même il n’a aucune différence de structure avec un signal utile. On ne peut plus distinguer comme auparavant le son purement musical et le bruit. Avec l’élaboration d’une formalisation par nature des fonctions du bruit, les sons inharmoniques (apériodiques) qui liés à la vie courante participent désormais, dans l’intégration du sonore, à la construction musicale. Tous les éléments de notre environnement sonore contiennent une certaine part de bruit, qui a vocation de devenir fonction structurante par destination.L’ensemble de ces bouleversements conceptuels accompagne les découvertes scientifiques et techniques qui permirent de développer des factures instrumentales nouvelles (notamment avec l'électronique). L’instrument de musique primitif se voulait représentation des sons naturels (le vent dans les arbres se retrouvant dans le son de la flûte, le chant des oiseaux dans celui de l’homme…). À cette condition, il était le seul capable de traduire le musical (d’opérer une distinction entre sons harmoniques et bruits). L’extension des techniques aidant, la notion même d’instrument s’est trouvée redéfinie… . La machine et l’instrument se sont rejoints. Ce que les hommes acceptent de reconnaître comme musical correspond désormais à une appropriation d’un matériau sonore étendu, à une intégration de phénomènes jusqu’alors considérés comme bruits. Intrusion de l’aléatoire Avec la composition assistée par ordinateur, première expérimentation musicale à utiliser l’ordinateur, les théories musicales se sont tour à tour préoccupées d’infléchir ou de laisser l’initiative à la machine, et, parallèlement, de libérer totalement l’homme de certaines tâches de régulation ou de lui laisser une part importante de création. La problématique oscille ainsi, de façon quasi paradoxale mais finalement foncièrement dialectique, entre déterminisme et aléatoire, entre aléa et logique, entre hasard et nécessité. Le formalisme aléatoire (mathématisé) « calcule » sans qu’il n’empiète sur les atouts sensibles du compositeur. Les objets mathématiques qui se sont développés créent véritablement un intermédiaire vers des paradigmes esthétiques que l’expérimentation musicale essaie petit à petit de mettre à jour, intermédiaire qui se situerait entre un ordre régulier, périodique, et un chaos incontrôlé, aléatoire et singulier. Hiller, le père de la composition assistée par ordinateur, sans juger qui pourrait effectuer les compromis, considérait déjà que « la musique est un compromis, voire une médiation, entre la monotonie et le chaos ».Artistiquement, à la théorie de l'information de Shannon répond la théorie de l’indétermination de John Cage (l’information est maximale donc nulle). En 1951, Cage et Feldman s’en remirent à l’aléatoire codifié du I Ching pour bâtir leur œuvre Music of Changes. Cette œuvre, qui brise les carcans de la notion traditionnelle d’œuvre musicale, sert de manifeste artistique au concept de l’indétermination.Cage introduit subrepticement le hasard dans la composition dans un sens plus combinatoire. Music of Changes laisse place à l’aléa contre la logique en réhabilitant le pouvoir créateur de l’expérience divinatoire, le pouvoir de la création par le hasard. John Cage, Morton Feldman et Earle Brown utilisaient aussi un hasard codifié, l’aléatoire du I Ching, livre de divination chinois qui laisse entrevoir un certain nombre de combinaisons par pentagrammes. Le hasard est sublimé par le destin dans une prédication divinatoire (Concerto pour piano (1957)). Puis, chez Cage les théories devenant paroxystiques, il prône la raréfaction de la musique jusqu’au total aléa (4’33’’) : l’écoute est focalisée vers des objets sonores qui n’ont pas été directement prévus pour cela. Peu de critiques ont pu abonder dans son sens, déplorant que ces théories ne servent qu’à la justification d’un « coup » médiatique. Musique algorithmique Pour tenter de réduire la proportion de hasard fatalement confiée à la technique, la machine fut utilisée par la suite pour ses fonctions de contrôle de l’automation qui assure un enchaînement continu d’opérations mathématiques et logiques. Pierre Barbaud débuta dans cet esprit ses travaux sur la composition « automatique » et mit au point avec Roger Blanchard en 1959 le programme ALGOM I-5 pour l’ordinateur Gamma 60 du Centre de calcul électronique de la compagnie Bull à Paris. Musique stochastique Cette mathématisation accrue des possibles continua à être prise en compte mais en essayant de reprendre à la machine la part de responsabilité qu’elle avait conquise. Dès 1954, Iannis Xenakis avait créé son opus un, Metastasis pour 65 instruments ; c’est la première musique entièrement déduite de règles et de procédures mathématiques. Pour son créateur, il s’agit de mettre en pratique une relation directe entre musique et architecture, combinaison certes peu commune mais qui pour l’assistant de Le Corbusier va de soi. Il la mettra à profit en utilisant les mêmes règles de construction dans l’élaboration des plans du pavillon Philips pour l’exposition universelle et internationale de Bruxelles en 1958 (pavillon où seront jouées dans un même concert les créations des œuvres de Varèse (Poèmes électroniques) et de Xenakis (Concret PH)).En 1956 est publiée une théorisation de la musique stochastique qui s’appuie entre autres sur la théorie des jeux de von Neumann. Le hasard n’y est déjà plus une simple chance ; contrairement à la troisième sonate de Pierre Boulez ou aux autres œuvres « ouvertes », contrairement à Cage, et à sa démission de compositeur, la probabilité est entièrement calculée et les règles sont explicitées (Achorripsis ou ST/10-1 en 1961). Le processus global est prévisible, même si les évènements qui le composent sont aléatoires. Par cette philosophie de la création, Xenakis essaie de se rapprocher des phénomènes biologiques et des événements du monde vivant.Un « système musical » est un ensemble de règles et d'usages attachés à un genre musical donné. On parle parfois de « théorie musicale ». La conception de la musique comme système peut aller très loin, et les anciens Grecs comptaient la musique comme une des composantes des mathématiques, à l'égal de l'arithmétique, de la géométrie et de l'astronomie. Voir l'article « Harmonie des sphères ». Plus près de nous, Rameau dans son Dictionnaire de la Musique arrive à considérer la musique comme étant à la base des mathématiques.Certaines musiques possèdent en outre un système de notation. La musique occidentale, avec son solfège, en est un exemple notoire. Dans ce cas, il est difficile de séparer le système musical du système de notation qui lui est associé. Certaines musiques traditionnelles sont uniquement de transmission orale, et développent des systèmes musicaux différents. Signes musicaux En occident, la musique s'écrit avec des signes : les notes de musique, les clés, les silences, les altérations, etc. Les notes de musique s'écrivent sur une portée, composée de 5 lignes parallèles. La portée comporte aussi des barres verticales. L'espace entre deux barres de mesure est une mesure. Il existe aussi des doubles barres. Les sept notes de musique sont : do (ou ut), ré, mi, fa, sol, la et si. Les notes s'écrivent sur la portée ou sur des lignes supplémentaires placées au-dessus ou en dessous de la portée. La portée va du plus grave (en bas) au plus aigu (en haut). Une même note peut être jouée de façon plus ou moins grave ou aiguë.Une octave est la distance qui sépare deux notes identiques, plus ou moins graves. Il existe de très nombreux signes musicaux pour indiquer la durée d'une note. En particulier :la figure de la note : ronde (la plus longue), puis blanche (avec une queue simple), puis noire, puis croche (queue avec une croche), puis double croche, puis triple croche, puis quadruple croche (la plus courte) ;le point, placé à côté de la note (ou du silence), la fait durer plus longtemps (il ajoute à la note la moitié de son temps initial). Le point peut être doublé (mais le deuxième point dure la moitié du premier) ;la liaison entre deux notes du même nom fait durer cette note tout le temps de la liaison (même si d'autres notes s'intercalent entre les deux) ;le triolet (identifié par un chiffre 3 sous ou sur les trois notes reliées) allonge également la durée de la note. Deux triolets forment un sextolet (identifié par un chiffre 6).Les silences sont les moments du morceau de musique sans son. Il existe sept figures de silence : la pause, la demi-pause, le soupir, le demi-soupir, le quart de soupir, le huitième de soupir, le seizième de soupir. Les clés indiquent la base de départ de la lecture de la partition. Il existe trois clés : la clé de so"
musique;La mélodie est une succession de sons ordonnés selon des rapports de rythme et de modulation par opposition à l'harmonie consistant dans l'accord de plusieurs sons exécutés simultanément. Le terme « mélodie » vient du latin melodia issu du grec ancien ??????? / melôidía, « chant », composé de ????? / mélos, « arrangement musical », et ??? / ôid?, « chant ».Dans la musique occidentale, chaque note d'une mélodie est déterminée par l'intervalle mélodique qui la sépare de la note fondamentale — ou note de référence — appelée « tonique » dans la musique tonale. Le mot mélodie s'oppose ainsi à la polyphonie — ou harmonie, ces deux termes pris dans leur sens le plus large de « procédé musical utilisant les simultanéités délibérées » —, cette technique d'écriture constituant l'une des singularités de la musique occidentale depuis le milieu du Moyen Âge.Du XIIIe au XVIe siècle, la musique occidentale est plus précisément dite polyphonique et modale, et son procédé de composition est appelé contrepoint. Une pièce musicale de cette époque, peut être considérée comme une « superposition de mélodies », chacune d'elles étant exécutée par les différentes parties de l'ensemble.Après la Renaissance, la musique est harmonique et son procédé de composition est le système tonal. Avec cette nouvelle technique, une partie se détache des autres pour exécuter la ligne mélodique principale (ou simplement mélodie) — généralement située à la partie supérieure de l'édifice. Dans ce nouveau sens, la mélodie s'oppose donc à la basse continue ainsi qu'aux différentes parties intermédiaires qui constituent l'accompagnement de la ligne principale — accompagnement pouvant être synthétisé en une série d'accords. On parle alors de « mélodie accompagnée ».ModesRythmeHarmoniePolyphonieMélodie (genre)Claude Abromont et Eugène de Montalembert, Guide de la théorie de la musique, Librairie Arthème Fayard et Éditions Henry Lemoine, coll. « Les indispensables de la musique », 2001, 608 p. [détail des éditions] (ISBN 978-2-213-60977-5) Portail de la musique classique   Portail de la musique   Portail des arts
musique;"En musique, une note est un symbole ou une lettre permettant de représenter un fragment de musique par une convention d'écriture de la hauteur et de la durée d'un son.Le symbole visuel d'une note de musique sur une partition est constitué : d'une « tête » qui indique la hauteur du son (sa fréquence en hertz),d'une « hampe » qui soutient les crochets et les barres de durée,d'une « durée », matérialisée par un ou des crochets ou par une ou plusieurs barres horizontales ou inclinées, qui indique sa longueur ou durée temporelle.La « tête » est généralement de forme ronde ovalisée et peut être évidée (ronde, blanche…) ou pleine (noire, croche…). Pour certains instruments, elle peut prendre diverses formes comme pour la percussion. Par exemple, le symbole  : croix est utilisé pour les cymbales,triangle pour les triangles,rectangle pour le tambourin,…La « hampe » est un trait vertical fin attaché à toutes les têtes de notes, sauf à la ronde. En règle générale, elle est placée au-dessus et à droite de la tête quand la tête de note est en dessous de la troisième ligne de la portée, et en dessous et à gauche quand la tête de note est sur ou au-dessus de la troisième ligne.La « durée » des notes est matérialisée par un crochet simple ou des crochets multiples superposés, ou par une barre ou plusieurs barres épaisses parallèles, horizontales ou inclinées. L'assemblage des « durées » de notes constitue le rythme : à partir de la croche, le nombre de crochets (pour une note) ou de barres (pour un ensemble de notes) détermine la durée rythmique de la note : un pour la croche, deux pour la double-croche, trois pour la triple-croche, etc. Pour la ronde, la blanche et la noire, pointées ou non, qui n'ont pas de symbole de « durée » (crochet ou barres horizontales), la longueur de la note est donnée par le fait que la tête de note soit remplie ou pas, et/ou par le fait qu'elle ait une hampe ou pas.L'assemblage des « têtes » de notes disposées à différentes hauteurs — qui donne la mélodie — et des « durées » de notes — qui donnent le rythme — forment la partition et le solfège, qui sont destinés à être lus et déchiffrés par le musicien interprète.En plus de la hauteur et de la durée, les notes font l'objet d'autres effets acoustiques créés par la manière de jouer du musicien, comme d'une part, l'intensité ou nuance, indiquée sur la partition par des lettres placées à proximité des notes, comme  pour piano,  pour fortissimo, etc., et du phrasé musical d'autre part (attaques douces, attaques brutales, notes piquées, accents, etc.) indiqué par des symboles musicaux spécifiques (points, traits, chevrons, etc.). Enfin, une même note jouée par divers instruments, certes de même hauteur, se différencie également par son timbre, spécifique à chaque instrument (timbre doux pour la flûte, percutant pour le piano quand il est martelé, féerique pour le glockenspiel, puissant et cuivré pour la trompette, plus sourd pour le cor, etc.).Des « têtes » de notes placées les unes après les autres, de gauche à droite, sont jouées successivement, ce qui forme une mélodie : chaque instrument d'un orchestre joue sa mélodie. Mais lorsque plusieurs « têtes » de notes sont superposées, elle doivent être jouées simultanément, ce qui crée un accord. C'est le cas pour un pianiste qui joue plusieurs notes en même temps avec une main, ou pour un ensemble instrumental où chaque musicien joue une des notes de l'accord. Tout son musical (ou note) possède une fréquence fondamentale (nombre de vibrations par seconde calculé en hertz) correspondant à sa hauteur.Tous les instruments de musique fonctionnent selon l'équation des cordes vibrantes, qui donne la fréquence du son joué en fonction des paramètres géométriques de construction de l'instrument (longueur de corde, diamètre de tuyau, etc.). Cette équation admet un ensemble de solutions de fréquences multiples de la fréquence dite fondamentale, qui correspond à la vibration du système vibrant (corde, tuyau, etc.).Lorsqu'un son donné de fréquence                               f                      0                                {\displaystyle f_{0}}   est joué, l'ensemble des fréquences                               f                      0                                {\displaystyle f_{0}}  ,                     2                  f                      0                                {\displaystyle 2f_{0}}  ,                     3                  f                      0                                {\displaystyle 3f_{0}}  ,                     4                  f                      0                                {\displaystyle 4f_{0}}  , etc., sont également émises par l'instrument. La note correspondant à la fréquence                               f                      0                                {\displaystyle f_{0}}   est appelée fondamentale, et les autres sont ses harmoniques. L'ensemble de ces fréquences, associées à leur intensité (c'est-à-dire si on les entend fort ou non) est appelé spectre, ou plus communément timbre.Dans un son de fondamentale                               f                      0                                {\displaystyle f_{0}}  , les harmoniques les plus présentes sont l'octave (                    2                  f                      0                                {\displaystyle 2f_{0}}  ), la quinte (                    3                  f                      0                                {\displaystyle 3f_{0}}  ), l'octave (                    4                  f                      0                                {\displaystyle 4f_{0}}  ), la tierce majeure (                    5                  f                      0                                {\displaystyle 5f_{0}}  ), etc..Deux notes dont les fréquences fondamentales ont un rapport qui est une puissance de deux (c'est-à-dire la moitié, le double, le quadruple…) donnent deux sons très similaires et portent le même nom. Cette observation permet de regrouper toutes les notes qui ont cette propriété dans la même catégorie de hauteur.Dans la musique occidentale, douze fréquences fondamentales différentes portent un nom. Sept d'entre elles sont considérées comme les principales et ont pour noms : do, ré, mi, fa, sol, la et si. Elles correspondent aux harmoniques naturels lorsque la note do est jouée.Les cinq notes restantes sont dites « altérations » et sont des notes intermédiaires. Par exemple, la  est une note de fréquence intermédiaire entre le la et le si.Pour distinguer deux notes de même nom dans deux octaves différentes, on numérote les octaves et donne ce numéro aux notes correspondantes : par exemple, le la3 a une fréquence de 440 hertz définie dans la norme internationale ISO 16 (bien qu'en pratique, cela puisse parfois varier). Cette fréquence de référence est donnée par un diapason.Depuis le XVIIe siècle, on considère que les notes sont également réparties sur une octave, c'est-à-dire que le rapport de fréquences entre une note et la suivante est de                                           2                          12                                            {\displaystyle {\sqrt[{12}]{2}}}   (exemple : la3 = 440 Hz ; la 3 = 466,16 Hz).La définition de l'écart entre les notes est ce que l'on appelle le tempérament, et lorsque l'écart entre une note et la suivante (en termes de fréquences) est toujours identique, ce tempérament est dit « égal ».Dans la gamme tempérée, la formule permettant de mesurer la fréquence d'une note par rapport à une note de départ est :                               f                      n                          =                  f                      0                          ×                  2                      n                          /                        12                                {\displaystyle f_{n}=f_{0}\times 2^{n/12}}  . Avec                     n              {\displaystyle n}   le nombre de demi-tons au-dessus de la note de départ                               f                      0                                {\displaystyle f_{0}}  . On s'aperçoit que la fréquence croît de manière géométrique par rapport à la note.De ce fait, chaque demi-ton correspond à une augmentation / diminution de la fréquence suivant un rapport de 1,0594630943 : 1 de la note voisine.Néanmoins, le tempérament égal a ses limites : lorsqu'une note est jouée, les harmoniques 3 et 5 (la quinte et la tierce) sont audibles, et leur fréquence est de                     3                  f                      0                                {\displaystyle 3f_{0}}   et de                     5                  f                      0                                {\displaystyle 5f_{0}}  .Par exemple, pour un la3 (440 Hz), on entend également la quinte (1 320 Hz) et la tierce majeure (2 200 Hz). Cette quinte et cette tierce se retrouvent également à la moitié ou le quart de leur fréquence (une octave en dessous) : 660 Hz (mi4) et 550 Hz (do 4).Or, dans la gamme tempérée (à tempérament égal), le mi étant la quinte du la, elle est séparée du la d'un facteur                               2                      7                          /                        12                                {\displaystyle 2^{7/12}}  , c'est-à-dire que le mi4 a sa fondamentale à 659,25 Hz, et le do 4 a sa fondamentale à 554,37 Hz.On remarque alors que si on joue une tierce harmonique (la4 et do 4 simultanément par exemple) avec un tempérament égal, l'harmonique de tierce de la fondamentale et la fréquence fondamentale de la tierce ne sont pas à la même fréquence.Dans la théorie de la musique, on parle de « degré ». Celui-ci représente une hauteur relative appartenant à une échelle musicale donnée. En effet, il existe de nombreuses possibilités pour choisir les fréquences des notes dans une octave. Le choix des échelles dépend des époques, des instruments et des types de musique.Selon l'échelle, on obtiendra des gammes musicales différentes. La musique classique en utilise deux : l'échelle diatonique et l'échelle chromatique.Pour nommer les notes de musique, la musique occidentale utilise deux systèmes différents, selon le pays :le premier système, inspiré de l'Antiquité, utilise les premières lettres de l'alphabet. Il est en vigueur, dans deux variantes simplifiées (ne différant que par la désignation du si), dans les pays anglophones et germanophones ;le second système utilise les syllabes d'un chant latin. Il a été élaboré pendant la deuxième moitié du Moyen Âge et il est en usage en France, en Italie, etc.Depuis Guido d'Arezzo, les notes de musique peuvent être désignées par ut, ré, mi, fa, sol, la. Cette pratique a été standardisée par les recommandations du pape Jean XIX. Auparavant, en Occident, divers systèmes de notation existaient. La nouvelle méthode permettait d'apprendre en un jour ce qu'il fallait un an pour apprendre avec la méthode grecque utilisant des lettres pour noter tant les tons que les échelles.La série constituée des syllabes ut, ré, mi, fa, sol, la (le si a été ajouté plus tard), promue par le moine bénédictin italien Guido d'Arezzo au XIe siècle, a été mise en place pour la notation musicale dans les pays de rite catholique dit « latin ». Cette série est constituée des premières syllabes de chaque demi-vers de l’Hymne à saint Jean-Baptiste, un chant religieux latin attribué au moine et érudit Paul Diacre : L’ut, renommé do, est surtout utilisé dans le langage théorique : clés, tonalités…La septième note si fut ainsi dénommée à partir des initiales de Sancte Ioannes dans le dernier vers. C'est plus tard, en Italie, que le nom ut, seule note de la gamme sans consonne en son début pour marquer l'attaque de celle-ci, a été remplacé par la syllabe do, à la diction plus aisée. Son origine exacte reste inconnue, mais do pourrait être la première syllabe de Domine : Seigneur, donc Dieu, en latin. En outre, selon certains, cette syllabe ouverte contrairement au ut fermé, serait plus facile à émettre, pour les besoins du déchiffrage chanté de musiques sans paroles, dans l'apprentissage du solfège. Depuis lors, on parle de la note do (ou do bémol, do dièse, do bécarre, etc.), mais on évoque toujours la clé d'ut. Quant aux tonalités, les deux noms coexistent : ut majeur ou do majeur, ut mineur ou do mineur, etc.En général, la musique est souvent représentée par une ou des notes, que ce soit dans la bande dessinée, pour symboliser un chant, ou dans l'informatique, pour spécifier que le fichier est un fichier musical.La représentation des symboles musicaux en informatique existe dans différents jeux de caractère (Unicode, LaTeX, LilyPond…). Par exemple, l'encodage en Unicode, pour ? (une noire), ? (une croche), ? (deux croches) et ? (deux doubles-croches) :Il existe également des notes dans la table des emojis.Cependant, cet encodage ne permet pas le positionnement sur la portée, ni aucune autre manière de distinguer les hauteurs de son. Pour cela, on emploie d'autres normes. Par exemple, un des aspects de la norme MIDI est la numérotation des notes. On dit alors que le la 440 Hz est le numéro 69 et que toute différence de numéro par rapport à cette note est comptée en demi-tons. Par exemple, tous les multiples de 12 sont des do. À l'origine, seuls les numéros de 0 à 127 étaient permis, mais selon la variante de cette échelle, des notes plus graves ou plus aiguës peuvent aussi être permises. À partir du moment où on suppose un tempérament égal (gamme tempérée), on peut aussi représenter des notes intermédiaires en utilisant des fractions (voir la formule de fréquence mentionnée ci-haut).D'autres notations informatisées utilisent les lettres de notes anglaises de A à G et le symbole # tenant lieu de dièse, mais pas de symbole bémol car non-nécessaire (tout bémol a un équivalent dièse). Les notes non-dièse peuvent être suivies d'un trait d'union ou d'une espace. On termine ensuite avec le numéro d'octave, qui augmente de 1 à chaque do comme dans la table ci-haut, mais qui peuvent être décalés (le la 440 pourrait être écrit A-2, A-3 ou A-4 selon l'échelle choisie). C'est ce qui est utilisé visuellement dans les éditeurs de type tracker (à la Amiga), quoique à l'interne, leurs formats utilisent des périodes (inverses de fréquences) comme le format MOD, ou une combinaison note de la gamme et octave (par exemple, les deux parties de la division avec reste d'une note MIDI par 12) comme le format S3M.Adolphe Danhauser, Théorie de la musique : Édition revue et corrigée par Henri Rabaud, Paris, Éditions Henry Lemoine, 1929, 128 p. (ISMN 979-0-2309-2226-5).Claude Abromont et Eugène de Montalembert, Guide de la théorie de la musique, Librairie Arthème Fayard et Éditions Henry Lemoine, coll. « Les indispensables de la musique », 2001, 608 p. [détail des éditions] (ISBN 978-2-213-60977-5).Acoustique musicaleDésignation des notes de musique suivant la langueHistoire de la notation musicaleLes notes, les altérations, les tons, le B.-A. BA Portail de la musique classique   Portail de la musique   Portail de l’écriture"
musique;"Un orchestre symphonique, ou orchestre philharmonique, est un ensemble musical formé des trois familles d'instruments : cordes, instruments à vent (bois et cuivres) et percussions. La composition des orchestres a beaucoup varié jusqu'à la fin de la période baroque. Ils associaient les instruments les plus divers ainsi qu'en témoignent les multiples combinaisons des six Concertos brandebourgeois (1721) de Bach. Ce n'est pas avant le milieu du XVIIIe siècle que la forme de l'orchestre symphonique commence à se figer et à s'étoffer progressivement de hautbois, de bassons, parfois de cors, de trompettes, et de timbales.La période classique avec Gossec, Haydn ou Mozart voit souvent les vents associés par deux (2 flûtes, 2 hautbois, 2 clarinettes, 2 bassons, 2 cors, 2 trompettes). Les pupitres de la période romantique s'ordonnancent plutôt par trois avec l'ajout plus ou moins systématique d'instruments comme le piccolo, le cor anglais, la clarinette basse, les saxophones, le contrebasson, les trombones ou le tuba. C'est aussi la période qui connaît la grande évolution des percussions. Au début du XXe siècle, l'orchestre symphonique peut être de grande taille, généralement, plus de quatre-vingts musiciens, l'effectif dépassant parfois la centaine d'instrumentistes.Depuis la fin du XVIIe siècle, sa principale fonction est dédiée à l'exécution, dans les salles de concert, d'œuvres symphoniques ou concertantes, profanes ou sacrées. Cette formation est également utilisée pour l'accompagnement en fosse, dans les salles d'opéra, des représentations d'art lyrique ou chorégraphique. Les compositeurs de musiques de film ou encore de musiques de jeu vidéo aujourd'hui, héritières des musiques de scène, utilisent eux aussi toutes les ressources musicales et expressives de l'orchestre symphonique.L'orchestre symphonique est constitué de trois familles d'instruments : les cordes, les vents (comprenant les bois et les cuivres) et les percussions. La composition précise de l'orchestre dépend de l'œuvre exécutée.Chaque pupitre comprend un premier soliste (pouvant être secondé par un second ou un troisième soliste) dont le rôle, comme son nom l'indique, est de jouer les parties solo d'une partition orchestrale, mais aussi de diriger des répétitions partielles de son pupitre. Les autres musiciens sont appelés des tuttistes.Le « premier violon solo » a un rôle hiérarchique et représente souvent l'orchestre devant son chef (qui le salue lors des concerts) et devant le public (il commande les levers des musiciens et accueille le chef d'orchestre). Il est de tradition que ce soit lui qui demande le « la » au hautbois pour vérifier l'accord des instruments.La disposition des différents pupitres peut varier notamment si l'orchestre est caché comme dans le cas d'une fosse d'opéra par exemple.En règle générale, les cordes sont réparties en demi-cercle de gauche à droite du chef d'orchestre, et de l'aigu vers le grave (premiers violons, deuxièmes violons, altos, violoncelles et, derrière ces derniers, contrebasses).Les vents peuvent être répartis en ligne (de l'aigu vers le grave) :cors / trompettes / trombones / tubasflûtes / hautbois / clarinettes / bassonscordesou en carrés (favorisant l'écoute entre les divers pupitres) :clarinettes / bassons       //      trombones / tubasflûtes / hautbois           //      cors / trompettescordesLes cordes sont la partie la plus constante de l'orchestre symphonique. Elles sont divisées en cinq pupitres, habituellement répartis de la manière suivante, de gauche à droite (en regardant l'orchestre) : les premiers violons, au nombre de 16 environ ;les seconds violons, au nombre de 14 environ ;les altos, au nombre de 12 environ ;les violoncelles, au nombre de 10 environ ;les contrebasses, au nombre de 8 environ.Ces effectifs sont à diviser par deux pour un orchestre symphonique de type « Mozart ».Disposée le plus souvent en arrière des cordes, la famille des bois peut être d'un effectif très variable suivant le répertoire abordé. Les flûtes, hautbois, clarinettes et bassons (la « petite harmonie ») peuvent former une section oscillant de deux à plus de vingt musiciens, incluant notamment les saxophones, certains compositeurs, comme Igor Stravinsky dans Le Sacre du printemps, utilisant cinq instruments et leurs dérivés par pupitre, exploitant ainsi toutes leurs richesses sonores, individuelles ou collectives. Chaque type d'instrument forme un pupitre comportant jusqu'à cinq musiciens dont les partitions sont toutes distinctes : contrairement aux cordes, aucune partie n'est doublée.La puissance sonore de la famille des bois étant supérieure à celle des cordes, dans un rapport de 2 à 7 suivant les instruments et les registres, à l'exception des notes les plus graves de la flûte d'une intensité à peu près équivalente à celle du violon, ceci explique le nombre réduit des pupitres de bois par rapport à ceux de cordes. Ils jouent fréquemment un rôle de soliste, particulièrement les flûtes et les hautbois, un peu moins les bassons qui renforçaient souvent les violoncelles, au moins dans les compositions du XVIIIe siècle.Dès l'origine et tout au long de la deuxième moitié du XVIIIe siècle, les deux bassons sont les bois les plus permanents de l'orchestre, d'abord accompagnés de deux hautbois, parfois de deux flûtes, puis de deux clarinettes. Des parties de piccolo et de contrebasson apparaissent chez Ludwig van Beethoven dans le finale de la symphonie no 5 en 1808, ou de la symphonie no 9 en 1824. Le cor anglais se dévoile dans l'ouverture de l'opéra Guillaume Tell de Rossini en 1829 ou, l'année suivante, dans la Symphonie fantastique d'Hector Berlioz. Dans L'Arlésienne de Georges Bizet en 1872, c'est le saxophone alto qui prend place, mais son emploi ne sera, et n'est encore de nos jours, qu'épisodique dans l'histoire de l'orchestre symphonique.Composition des bois d'un grand orchestre symphonique moderne Les flûtes1 piccolo2 à 4 flûtes traversières, l'une d'entre elles pouvant jouer le deuxième piccoloN.B. : certaines œuvres utilisent une flûte en sol jouée par l'un des flûtistesLes hautbois2 à 4 hautbois le quatrième pouvant jouer le deuxième cor anglais1 cor anglaisN.B. : certaines œuvres, peu fréquentes, utilisent un hautbois d'amour joué par l'un des hautboïstesLes clarinettes1 clarinette en mi? (dite « petite clarinette »)2 à 4 clarinettes en si? ou en la (très rarement en ut), la quatrième pouvant jouer la deuxième clarinette basse1 clarinette basseLes bassons2 à 4 bassons, le quatrième pouvant jouer le deuxième contrebasson1 contrebassonLe saxophoneNé au sein de l'orchestre du XIXème, il a sa place dans certains chefs-d'œuvre et dans la musique symphonique à partir de 1845.Disposés en arrière des bois, les cuivres participent à l'orchestre selon les besoins liés au répertoire abordé. Ils se composent des instruments suivants :2 à 4 trompettes, l'une d'elles pouvant jouer la trompette piccolo2 à 8 cors d'harmonie, quatre d'entre eux pouvant jouer les tubas wagnériens2 à 4 trombones1 trombone basse1 ou 2 tubasLeur très grande puissance sonore, de l'ordre du double de celle des bois et 6 ou 7 fois de celle des cordes, moins écrasante dans le grave mais atteignant jusqu'à plus de 15 pour la trompette par rapport au violon dans le registre aigu, impose leur place à l'arrière et leur nombre relativement limité.Dans un orchestre symphonique, les instruments de percussion, généralement placées derrière les cuivres, sont le plus souvent classés en trois groupes :Les claviersbois : xylophone, marimbamétal : vibraphone, glockenspiel, célesta, jeu de cloches ou carillon tubulaireLes peauxaccordables : timbales (le plus souvent par jeu de cinq)grandes : grosse caissemoyennes : tambour d'orchestre, caisse claire, timbale(è)spetites : tambourin et tambour de basque, bongos, congas, tumbasLes accessoires :bois : wood-block, temple block, castagnettes, fouet, güiro, maracas...métal : cymbales (frappées, suspendues, charlestons…), triangle, grelots, gong, tam-tam...divers : flexatone, sifflet, klaxon, sirène...Bernard Lehmann a relevé une origine sociale plus élevée des musiciens jouant d'instruments à cordes par rapport à ceux des instruments à vent, et des bois par rapport aux cuivres au sein des orchestres parisiens. Les cuivres sont également issus plus fréquemment d'harmonies municipales (cf. étude sociologique sur les orchestres d'harmonie en Alsace). Cette gradation ne se retrouve toutefois pas à un niveau des rémunérations. Ainsi, les bois ont une proportion de solistes nettement plus importante que les cordes, essentiellement composées de tuttistes. Les rémunérations diffèrent en conséquence.« Un orchestre symphonique est la plus belle métaphore de la société que je connaisse. Chacun est indispensable, mais doit savoir s'effacer pour faire vivre une réalité supérieure » — Riccardo Muti.En France la plupart des grands orchestres se sont constitués au XIXe siècle et au XXe siècle soit à partir des conservatoires de musique (les professeurs formant le noyau de l'orchestre comme à Paris ou à Strasbourg) soit par la Radio publique, à partir des années 1930 (en 1964, l'ORTF comptait 8 orchestres radio-symphoniques dont deux à Paris (où résidaient aussi l'Orchestre lyrique de l'ORTF et l'Orchestre de chambre ainsi qu'un orchestre de musique légère).Le cadre de la gestion des orchestres est très variable : certains sont en régie directe, d'autres sont des établissements publics ou en relèvent (comme celui de l'Opéra de Paris) ; certains sont des syndicats mixtes. Avec la création d'orchestres régionaux à partir de 1969 sous l'égide de Marcel Landowski au ministère de la Culture, on assiste à la création de grands orchestres gérés sous forme associative qui vont pour la première fois bénéficier d'une subvention importante de l'État. C'est le cas de la Société des concerts du Conservatoire de Paris, qui était une association symphonique (comme les Orchestres Pasdeloup ou Colonne) et qui deviendra l'Orchestre de Paris avec un cadre de gestion rénové et une structure de financement stabilisée. Mais certains orchestres resteront dans le giron municipal comme ceux de Strasbourg ou Bordeaux. Les orchestres régionaux avaient pour mission de diffuser la musique sur le territoire de toute une région. Ils devinrent aussi les représentants de la vitalité de la musique en France (commandes passées aux compositeurs, répertoire de la musique française, tournées à l'étranger, etc.).Les orchestres symphoniques sont par conséquent de véritables institutions culturelles en ce sens qu'ils ont une place reconnue par les autorités publiques dans la vie culturelle, soit localement, soit régionalement, soit nationalement. Ils ont acquis une légitimité à travers des missions de service public et d'intérêt général qui leur sont confiées et qui leur donnent stabilité et permanence comme l'explique Mario d'Angelo. Le rôle d'institution musicale confère aussi aux orchestres symphoniques un rôle dans la transmission d'un savoir artistique et de la construction des valeurs artistiques. Se pose pour tout orchestre, au-delà de son modèle économique, la question de la gouvernance, mais aussi celle de la gestion de sa ressource humaine et artistique. En outre l'environnement concurrentiel plus aigu (forte médiatisation), requiert de la part des dirigeants de l'orchestre qu'ils trouvent des moyens de communication de plus en plus importants pour la notoriété de l'orchestre, régionalement, nationalement ou internationalement. Le choix du directeur artistique est une autre décision cruciale. De même pour la résidence de l'orchestre : dispose-t-il d'un lieu dédié et acoustiquement bon, pour ses répétitions et ses concerts, avec idéalement la possibilité de répéter et jouer dans le même lieu et de pouvoir y développer des activités éducatives ? On note sur ces deux derniers aspects des différences sensibles entre les orchestres, que ce soit en France ou en Europe. Partout en Europe, la situation des orchestres symphoniques présente les mêmes enjeux de gestion qu'en France pour assurer leur maintien et leur développement : assurer la convergence de soutiens publics et privés en rénovant leur mode de gouvernance. Les orchestres doivent aussi développer leur public globalement vieillissant. À l'instar de l'Orchestre national de Lille, ils mettent en œuvre la démocratisation de la musique classique en jouant dans les lieux et pour les publics les plus divers (usines, prisons, hôpitaux...). L'Allemagne compte 122 orchestres stables aux statuts divers allant de la régie à la fondation, en passant par la société coopérative (comme l'Orchestre Philharmonique de Berlin) ou l'établissement régional de radio (au total 10 orchestres radio-symphoniques). Au Royaume-Uni, tous les orchestres sont en gestion privée à but non lucratif ; les quatre grandes formations londoniennes sont des coopératives (self governing orchestra) comme l'Orchestre symphonique de Londres (LSO). Le LSO dans une indépendance affichée à l'égard des pouvoirs publics n'en demeure pas moins une des grandes institutions musicales ayant su diversifier ses activités dès les années 1930 (enregistrements au disque et musiques de films). Depuis les années 2000, le LSO a mené un projet qui lui a permis d'avoir un lieu spécifique (LSO Luke's, une ancienne église restaurée et aménagées) pour y développer de nouvelles activités : studios d'enregistrements et activités éducatives « LSO Discovery ». La stratégie de diversification du LSO est également territoriale ; il est en résidence à New York (où il a mis en place une fondation pour financer cette résidence) et à la Salle Pleyel à Paris. L'orchestre-entreprise qu'est devenu le LSO n'est ainsi plus seulement limité à une activité principale de concert à Londres (à l'auditorium du Barbican Centre) et de tournées internationales.Liste d'orchestres symphoniquesPhilharmonie Musique symphoniqueMusique de chambreMusique instrumentaleOrganologieHector Berlioz et Joël-Marie Fauquet, De l'instrumentation, Paris, Le Castor astral, coll. « Les inattendus », 1994, 169 p. (ISBN 2-85920-227-7)Hector Berlioz, Traité d'instrumentation et d'orchestration, Paris, Henry Lemoine, 1843, réed.1993, 312 p. (ISMN 979-0-2309-4518-9). François-Auguste Gevaert, Nouveau traité d'instrumentation, Paris-Bruxelles, Lemoine & Fils, 1885, 340 p.  (Texte disponible sur www.imslp.org)Charles Koechlin, Traité de l'orchestration, vol. 1, Paris, Éditions Max Eschig, 1954, 322 p. (en) Paul Mathews, Orchestration : an anthology of writings, New York, Routledge, 2006, 230 p. (ISBN 0-415-97683-9)(en) (ru) Nikolaï Rimski-Korsakov et Maximilian Steinberg (trad. Edward Agate), Principles of Orchestration (?????? ???????????), vol. 1, Berlin, Editions Russes de Musique,? 1913, trad.1922, 152 p. Charles-Marie Widor, Technique de l'orchestre moderne, Paris, Henry Lemoine, 1925, 200 p. Hyacinthe Ravet, L’orchestre au travail. Interactions, négociations, coopérations, Paris, Vrin, 2015, 379 pages  (ISBN 978-2-7116-2615-1)Christian Merlin, Au cœur de l'orchestre, Paris, Fayard, 2012, 520 pages  (ISBN 978-2-21366315-9)Pauline Adenot, Les musiciens d'orchestre symphonique, de la vocation au désenchantement, Paris, L'Harmattan, 2008, 381 p. (ISBN 978-2-296-05747-0, lire en ligne)Bernard Lehmann, L'orchestre dans tous ses éclats. Ethnographie des formations symphoniques, Paris, La Découverte, 2005, 262 p. (ISBN 978-2-7071-4610-6, LCCN 2002424262)Vincent Dubois, Jean-Matthieu Méon, & Emmanuel Pierru, Les mondes de l'harmonie. Enquête sur une pratique musicale amateur, Paris, La Dispute, 2005, 312 p. (ISBN 978-2-84303-149-6, LCCN 2009515324)Brigitte Ouvry-Vial, Georges Zeisel, L'Orchestre : Des rites et des dieux, Paris, Editions Autrement, 2000, 240 p. (ISBN 978-2-86260-218-9) Portail de la musique classique   Portail de la musique"
musique;"En musique, la polyphonie est la combinaison de plusieurs mélodies, ou de parties musicales, chantées ou jouées en même temps.Dans la musique occidentale, la polyphonie désigne le système de composition musicale, créé à l'église à partir du IXe siècle environ et qui connut un brillant développement, depuis un premier apogée aux XIIe et XIIIe siècles, jusqu'à la fin de la Renaissance (fin du XVIe siècle) et au-delà. À partir de la deuxième moitié du XVIe siècle, la pensée et le sentiment harmonique naissants prirent une place de plus en plus importante. Un désir de simplification joint au développement de l'individualité et du chant soliste feront qu'on passera progressivement du contrepoint linéaire à l'enchaînement vertical des accords (un texte musical contrapuntique se déroule horizontalement, chaque voix ayant sa propre vie à l'intérieur de l'ensemble, alors qu'un texte harmonique enchaîne des accords). Le nouveau système, toutefois, n'a pas remplacé le précédent : ces deux types d'écriture ont pu subsister parallèlement et aussi se mêler, pendant les siècles qui suivirent jusqu'à aujourd'hui.Par extension, c'est la capacité de jouer plusieurs notes à la fois ; on parle alors d'instruments polyphoniques.En Occident, la monodie, en usage au Moyen Âge et au-delà, recouvre des genres très différents comme le chant grégorien, la poésie aristocratique chantée des troubadours et des trouvères et la chanson de tradition orale appelée aussi chanson folklorique. L'accompagnement (s'il en existe un) n'est pas donné et n'est pas de nature mélodique. Dans l'opéra, l'expression un peu paradoxale de « monodie accompagnée » a une signification bien différente. Dans ce style vocal né avec l'opéra baroque au début du XVIIe siècle, le chant soliste est accompagné par une basse continue, aussi bien que par un orchestre. Ce type de monodie relève donc d'une écriture harmonique.Pour la notion de polyphonie en théorie de la littérature ou en linguistique voir les travaux de Mikhaïl Bakhtine et Oswald Ducrot. Dans ce cas, le sens du mot « polyphonie » n'a pas de lien avec celui qui a été développé plus haut.Durant l'Antiquité, l'art musical (et donc celui de l'Église primitive) n'avait apparemment connu que la monodie. La grande invention du Moyen Âge fut celle de la polyphonie. C'est au IXe siècle que cet art commence à apparaître et à se développer, de manière encore discrète mais manifeste, à l'église tout d'abord. Le philosophe Scot Érigène fait déjà allusion, dans son ouvrage De Divisione Naturæ, à la pratique d'une musique à plusieurs parties.L'avènement de la polyphonie occidentale constitue un des plus grands bouleversements de l'histoire de la musique. Voici ce qui paraît se dégager de plus net, sur ses débuts, tout d'abord (et tout naturellement) assez obscurs. À l'origine, à l'époque de l'Empire carolingien ou de son partage, elle était improvisée au lutrin, au cours d'offices religieux importants, à partir du chant grégorien, selon la technique du « chant sur le livre » (technique qui se perpétua jusqu'à la fin du XVIIIe siècle, en France). Ce qu'on avait à chanter était en grande partie à la charge des chantres professionnels qui travaillaient quotidiennement dans les nombreuses églises cathédrales et collégiales. Nous verrons que le chant polyphonique s'était primitivement répandu dans les abbayes, spécialement bénédictines, dont la vocation était de chanter la gloire de Dieu. Mais pendant quelques siècles, cette pratique cultuelle et culturelle est restée non-écrite, et donc invisible à nos yeux.Il s'agissait initialement d'amplifier la monodie grégorienne, en lui adjoignant une seconde voix, entendue en même temps qu'elle (c'est ce que Hucbald appelle, en 895, la « diaphonie », au départ en mouvements parallèles). Cela donna naissance à différents procédés polyphoniques, dont le déchant (discantus), et, rapidement, à des formes musicales élaborées dont la plus connue est appelée organum. Les œuvres entrant dans ce cadre formel font intervenir de 2  à   4 voix, entendues ensemble.La voix de déchant se définit par le mouvement contraire qu'elle adopte par rapport à la voix principale (appelée teneur). Quand la teneur monte, le discantus descend, et inversement. Ce principe remplaça rapidement le développement par mouvements parallèles, aux possibilités bien plus réduites. Le mouvement contraire reste aujourd'hui à la base de toute progression harmonique (certains parallélismes, comme les quintes ou quartes parallèles sont toujours considérés comme strictement interdits par les traités d'harmonie).La notion de déchant prit un sens plus large par la suite. C'est ainsi qu'elle désigna plus généralement l'écriture polyphonique. Un exemple de l'utilisation de ce mot apparaît dans le titre complet des Vêpres de 1610, de Claudio Monteverdi (Vespro della Beata Vergine). On trouve en effet le verbe decantare, dans le sous-titre latin de l'œuvre : Vesperæ pluribus [vocibus] decantandæ (« Vêpres chantées à plusieurs voix », c'est-à-dire jusqu'à dix voix, traitées en polyphonie).Il avait tout d'abord semblé indispensable, à l'oreille des premiers polyphonistes, d'éviter les superpositions de tierces et de sixtes, perçues comme dissonances, et de n'admettre comme consonances que les octaves, quartes et quintes (qui sont du reste les premières harmoniques nées de la vibration d'un son fondamental). Depuis la fin du XIe siècle environ, ce qui était considéré comme dissonances était néanmoins présent, mais, devant aboutir à une résolution, menait à une consonance de quarte ou de quinte (de même, dans le système tonal actuel, la note perçue par l'oreille comme « note sensible » doit son nom au fait qu'elle tend vers la note tonique, cf. par ex. le mouvement si ? do lorsqu'on est dans le ton de do majeur).Cette notion de consonance imparfaite est présente dans une formule cadentielle (formule de fin de phrase) typique du XIVe siècle : la cadence à double sensible (appelée également cadence de Machaut). Celle-ci apparaît couramment dans les polyphonies à 3 voix. À la fin d'une phrase musicale, si la pièce contient, par ex., les trois notes superposées sol-si-mi, le mouvement de cadence qui peut leur succéder mène alors à la résolution fa-do-fa : l'intervalle de tierce sol-si aboutit à la quinte fa-do, amenant ainsi un sentiment auditif de stabilité.Une forme assez différente de polyphonie s'est développée dans un pays qui, par son isolement insulaire et par son éloignement des régions méditerranéennes, échappait plus que d'autres à l'influence des traditions gréco-latines : l'Angleterre. Ce nouveau procédé se répandit ensuite sur le continent. Ainsi, l'Angleterre a évolué différemment du monde latin, si bien que la polyphonie savante des Anglais se distingua, un temps, de celle des autres peuples par l'emploi de deux procédés qui lui sont particuliers : le gymel (du latin cantus gemellus : « chant jumeau ») et le faux-bourdon, qui se caractérisent essentiellement par l'usage de successions de tierces et de sixtes, considérées, à partir du XIIe siècle environ, comme consonances imparfaites, et non plus comme dissonances.Le gymel, comme son nom l'indique, est un chant à deux voix dont la seconde accompagne à la tierce inférieure ou supérieure le thème donné par la première (appelée « teneur », car c'est elle qui « tient » le chant). Les deux voix doivent conclure en se rejoignant à l'unisson par mouvement contraire.Le faux-bourdon est un chant à trois voix qui consiste à faire entendre en même temps que la mélodie principale deux autres mélodies parallèles, à la tierce et à la quinte inférieure, - à la quinte inférieure en apparence, en écriture, pour les yeux seulement, car cette troisième partie, qui a l'air d'une basse, ou, comme on disait autrefois, d'un « bourdon », n'est en réalité qu'une fausse basse, qu'un faux-bourdon (mot employé par analogie avec des instruments comme la vielle à roue ou les différentes sortes de cornemuses). En effet, cette troisième voix doit se chanter une octave plus haut qu'elle n'est écrite, c'est-à-dire qu'elle sonne à la quarte supérieure et non à la quinte inférieure du thème donné. La polyphonie d'un faux-bourdon est donc théoriquement constituée d'un enchaînement de quartes, de sixtes et de tierces parallèles superposées. L'évolution de cette forme musicale, au cours du temps, l'a amené à s'écarter de ce schéma de base. Vers 1771-1772, le bénédictin de Saint-Maur Dom Robert-Florimond Racine évoquera avec plaisir « cette forme ravissante que l'on n’entend qu’avec étonnement ». Le XIXe siècle continuera à en faire usage.Une polyphonie populaire naquit de la polyphonie savante. Elle est donc née à l'église. Actuellement, la polyphonie populaire subsiste dans les polyphonies corses, par exemple.Aux XIe et XIIe siècles, l'abbaye bénédictine Saint-Martial de Limoges et d'autres lieux (abbayes et églises) jouent un rôle important dans le développement de la polyphonie, en l'absence de ville-phare.Mais précisément, la polyphonie connaîtra un premier rayonnement national et international à Paris (alors qualifiée de « nouvelle Athènes »), aux XIIe et XIIIe siècles, grâce aux chantres-compositeurs de l'École de Notre-Dame de Paris, cathédrale nouvellement construite et centre culturel de premier plan (tout comme le sera l'université, créée à Paris au milieu du XIIIe siècle, par le théologien Robert de Sorbon). À Notre-Dame, les chantres les plus connus ont été Léonin et Pérotin. Ils représentent les premiers grands créateurs de ce que les musiciens du siècle suivant appelleront l’Ars antiqua, par opposition à l’Ars nova. Ce dernier, né peu avant 1300, se voudra résolument nouveau (Jacques Chailley le qualifie d’avant-gardiste). L'art musical du XIVe siècle s'éloignera donc très clairement de ce que nous appelons le classicisme du grand siècle médiéval, le XIIIe siècle.Dès cette époque apparaît la technique du hoquet, qui sera plus particulièrement développée au XIVe siècle, pendant la période de l’Ars nova. Cette technique consiste en une alternance entre les voix, qui se succèdent et se répondent rapidement, chacune d'elles pouvant ne faire entendre qu'une seule syllabe à la fois, la ligne mélodique se déroulant ainsi en alternance très rapprochée. Cette manière de faire sera reprise dans la deuxième moitié du XXe siècle. Ce mode de déroulement spécifique, à l'origine proche du tuilage primitif, n'est pas exclusivement pratiqué en Europe.Les créateurs de l’Ars nova se lancèrent ainsi dans de nouvelles recherches, parfois très abstraites (comme chez Philippe de Vitry), pour aboutir à l’Ars subtilior, à la fin du XIVe siècle. L'amiénois Pierre de la Croix fut le réformateur de la notation franconienne, la notation mesurée, née au XIIIe siècle. Le grand musicien né avec le XIVe siècle est le Rémois Guillaume de Machaut (v. 1300-1377), auteur, en particulier, de la première messe polyphonique entière (la Messe Nostre Dame). Comme les autres musiciens, il écrivit aussi nombre de pièces profanes, tant l'art de la polyphonie s'était répandu dans bien des domaines musicaux.L'art profane d'Adam de la Halle, à la fin du XIIIe siècle, était à la charnière de la monodie et de la polyphonie, si bien qu'on le considère souvent comme le dernier trouvère, à la fois poète et musicien. Machaut, purement polyphoniste, a eu lui aussi une activité aussi bien musicale que poétique, suivant une très longue tradition remontant à l'Antiquité. Jusqu'à la fin du XVe siècle, avec Eloy d'Amerval par exemple, elle continuera à se perpétuer.À partir des années 1420 se développe entre le Nord de la France et les Flandres une nouvelle école musicale, grâce à plusieurs générations de musiciens et de compositeurs formés dans les maîtrises du nord. Ils seront désignés sous le nom d’École franco-flamande. Ils répandront par la suite leur art dans les grands centres européens, surtout en Italie, alors en pleine Renaissance (le Quattrocento).En raison des troubles qui règnent en France pendant la guerre de Cent Ans, la culture musicale se déplace dans les régions du nord de la France, en Flandre, et en territoire bourguignon. La cour de Bourgogne est le centre de ce renouveau artistique, qui rassemble musiciens français, flamands, bourguignons et anglais, contribuant aux échanges et à la diffusion de musiques nouvelles. Rome (ville où la papauté s'est réinstallée, après l'épisode avignonnais du XIVe siècle) le devient également.Au début de cette période (vers la fin du XIVe siècle et au début du suivant), « la fameuse contenance angloise [...] déferle sur le continent ». Cette expression désignait alors le « style musical anglais, à la fois brillant et simple, qui influence tous les compositeurs du continent au début du XVe siècle, lesquels rompent ainsi avec le style complexe et cérébral de l’Ars Subtilior de la fin du XIVe siècle ». Le principal auteur anglais représentant ce style est John Dunstable.La polyphonie se développe de deux façons :dans la musique religieuse enseignée et pratiquée dans les maîtrises et les chœurs (les « psallettes ») des cathédrales (et des collégiales) du nord. Par exemple : Cambrai, Tournai, Bruges, Anvers, etc. Leurs usages sont imités dans les psallettes des autres régions de France et d'Europe ;dans les chansons polyphoniques de divertissement, essentiellement à la cour du duc de Bourgogne à Dijon.Les principaux représentants de ce mouvement sont les grands compositeurs de l'époque :Guillaume Dufaÿ (1397-1474), Gilles Binchois (v. 1400-1460) - qui furent influencés par l'anglais John Dunstable (1390-1453) -, Johannes Ockeghem (v. 1420-1497), qui exerça à Paris et à Tours, Antoine Busnois (1433-1492), bourguignon, Josquin des Prés (v. 1440-1521), Alexandre Agricola (1446-1506), Jean Mouton (1459-1522), Jacob Obrecht, (1457-1505), Pierre de La Rue (1460-1518), Antoine Brumel (1460-1520), Clément Janequin (v. 1485-1558), connu pour ses chansons polyphoniques, Clemens non Papa (v. 1510-1555), Jacobus de Kerle (1531-1591), Roland de Lassus (1532-1594), musicien européen, Claude Le Jeune (v. 1530-1600), partisan de la Réforme calviniste, Eustache Du Caurroy (1559-1609), etc.En Italie : Adrien Willaert (1490-1562), Cyprien de Rore (1515/16-1565), l'un et l'autre d'origine franco-flamande mais installés en Italie, Giovanni Pierluigi da Palestrina (v. 1525-1594) musicien de la Contre-Réforme, Costanzo Porta (1528/29-1601) maître « virtuose » de la polyphonie, longtemps apprécié de son vivant, Marc'Antonio Ingegneri (1535-1592) qui forma Claudio Monteverdi, Carlo Gesualdo (1566-1613) musicien d'une grande originalité et audace harmonique (qui nous surprennent encore aujourd'hui), etc.En Espagne : Cristobal de Morales, (v. 1500-1553), Diego Ortiz (1510-570), Francisco Guerrero (1528-1599), Tomás Luis de Victoria (v. 1548-1611), etc.En Angleterre : John Taverner (1490-1545), Christopher Tye (1500-1572/3), Thomas Tallis (1505-1585), William Byrd (1543-1623), John Bull (1562/3-1628), Orlando Gibbons (1583-1625), etc.En Allemagne : Heinrich Isaac (1450-1517), Ludwig Senfl (1486-1543), Leonhard Lechner (1553-1606), Hans Leo Hassler (1564-1612), Gregor Aichinger (1564-1628), Michael Praetorius (1571-1621), etc.À la Réforme religieuse luthérienne, qui se manifeste de manière assez forte dans les années 1530 en Allemagne, on voit apparaître un art liturgique d'une toute autre conception, avec la naissance du choral luthérien (le Choralgesang). Ces chorals, nouvelle forme musicale, vont être composés (texte et musique) par Luther lui-même ou par d'autres auteurs, dont Martin Agricola (1486-1556). D'un côté, Luther encourage le chant communautaire. Les fidèles chantent ensemble à l'unisson des mélodies faciles et rapidement connues de tous. Il rassemble une série de mélodies de diverses origines, les structure sous la forme de chorals et les propose comme mélodies qui auront une valeur liturgique (cf. par ex. Ein Kinderlied für Weihnachten, parmi bien d'autres choses). De l'autre côté, il est un admirateur de la musique polyphonique, donc il ne l'exclut pas. Il laisse ouvert la possibilité que les chants communautaires soient soumis à diverses formes d'élaboration polyphonique. Les mélodies pour les chants communautaires simples mais de grande qualité restent au cœur du répertoire (un peu comme le chant grégorien dans le culte catholique), mais c'est un matériau sur lequel les compositeurs peuvent élaborer des versions plus complexes et développées. C'est la base pour un développement plus tardif, qui s'épanouira tout au long des XVIe et XVIIe siècles, puis notamment dans la musique de J.S. Bach (XVIIIe siècle).Michael Praetorius (1571-1621) a été compositeur pour l'Église luthérienne. Il a également repris beaucoup de mélodies, comme matériau de base pour ses œuvres polyphoniques. C'est le cas notamment pour Vom Himmel hoch, da komm ich her. Ces timbres sont des mélodies préexistantes, sacrées ou profanes. Certaines sont même des mélodies grégoriennes. Dans les décennies suivantes principalement, on trouve aussi un bon nombre de parodies (procédé, bien entendu dénué de toute idée de caricature, qui consiste à prendre une mélodie profane et à la chanter sur un texte de la liturgie). Ainsi, l'échange à double sens entre les univers profane et sacré nourrit l'inspiration musicale (depuis le XVe siècle au moins, d'assez nombreuses chansons de tradition populaire reprennent des motifs grégoriens, en les adaptant).Ces notions (d'échange à double sens entre musique religieuse et musique profane) sont valables aussi bien chez les luthériens que chez les catholiques, les musiciens luthériens ayant repris les traditions de l'Église catholique et du monde profane.En France, Jean Calvin, personnage central de la Réforme religieuse protestante, estime que la musique doit être extrêmement simple. Apparaissent alors les psaumes monodiques. Les 150 psaumes de la bible sont harmonisés par Claude Goudimel (v. 1505-1572). On peut citer aussi ceux que Richard Crassot harmonisa et publia à Lyon en 1565. Pour Calvin, il ne doit pas y avoir d'instrument. S'il y a un orgue on le détruit. Malgré cela, il n'était pas fermé à l'idée de musique polyphonique, mais elle devait se faire toujours sans instrument. La polyphonie savante de Claude Le Jeune (cf. ses Octonaires de la vanité et inconstance du monde) en est un des principaux témoins, même si elle est strictement profane. En effet, il est important de préciser que ces Octonaires ont été composés sur une poésie d'inspiration calviniste, mais sans du tout être liturgique : écrits pour la vie civile et la réflexion, ils n'étaient pas destinés à être chantés au cours d'un office et développent simplement leur philosophie de vie.En Angleterre, la musique polyphonique demeure au cœur de la liturgie. Thomas Tallis, resté catholique, va développer, pour la liturgie anglicane, le style de l'anthem, motet spécifiquement anglais (et chanté dans cette langue). Gibbons créera le Full Anthem (en chant polyphonique) et le Verse Anthem (dont les versets font alterner chantre soliste et chœur à plusieurs voix : Gibbons adapte de cette manière la très ancienne pratique des « versets alternés », déjà présente dans la tradition grégorienne monodique).Plus généralement, depuis le début du XVIe siècle environ, chez Tallis et chez bien d'autres auteurs, l'écriture polyphonique se développe dans des directions nouvelles : la musique devient assez fréquemment polychorale. Dans ce cas, ce n'est plus seulement un chœur, mais plusieurs qui vont faire vivre des polyphonies réparties dans l'espace. Le Spem in alium de Tallis (à 40 voix réelles) est bien connu (8 chœurs à cinq voix). Mais c'est là une exception. On était beaucoup plus fréquemment répartis en deux chœurs à 4 voix, ou selon d'autres formations (qui pouvaient être impaires) : tout cela se développa en particulier à la basilique Saint-Marc de Venise, où les Gabrieli (Andrea et son neveu Giovanni) écriront à 8, ainsi qu'à 16 voix réelles, par exemple. Le fameux motet Osculetur me, à 8 voix (1582), du franco-flamand européen Roland de Lassus, est écrit dans ce style, pour deux chœurs à 4 voix, qui se répondent et/ou se mêlent. Au début de la période, Josquin avait déjà écrit un Qui habitat (Ps. 90), à 24 voix. On doit citer aussi le canon à 36 voix (Deo gratias), de Johannes Ockeghem (une des principales figures de la seconde moitié du XVe siècle). Alessandro Striggio (v. 1535-1592), qui vivait à la même époque qu'Andrea Gabrieli, est l'auteur d'un motet Ecce beatam lucem (« Voici la lumière bienheureuse »), à 40 voix. L’Agnus Dei de sa Messe sur « Ecco sì beato giorno », à 40 voix, fait même intervenir 60 voix...Vers la fin du XVIe siècle, le madrigal italien contribuera grandement au développement d'une nouvelle technique, qui, petit à petit, deviendra radicalement différente de la polyphonie traditionnelle et finira par créer une nouvelle conception de l'art musical : la monodie accompagnée.La polyphonie ne mourut pas pour autant. Témoins, par exemple, l'œuvre de Heinrich Schütz (au XVIIe siècle), presque toute l'œuvre de J. S. Bach ou encore, parmi bien d'autres choses, le Stabat Mater à 10 voix réelles, d'un de ses contemporains, le claveciniste Domenico Scarlatti. Ordinairement connu pour ses sonates de clavecin, il montre ici sa grande habileté dans l'art du contrepoint. Le compositeur napolitain écrivit ce motet pendant sa période romaine (1715-1719), pour la Cappella Giulia (la chapelle papale), si bien que (contrairement à ce qu'on pourrait imaginer) la partition ne doit rien au style polychoral qui s'était spécialement développé à Venise au début du XVIIe siècle : les choristes ne sont pas répartis en chœurs différents (même si les 10 voix ne sont pas constamment employées en une masse indivise).En France, Marc-Antoine Charpentier est un des représentants les plus importants pour l'usage et le développement de ce style d'écriture musicale. Parmi les principaux auteurs du XVIIe siècle, relevons également les noms d'Henry Du Mont et de beaucoup d'autres (comme Charles d'Helfer par ex.), au cours de ce siècle et à l'époque suivante (André Campra, Henry Desmarest sous Louis XIV et Louis XV, Henry Madin dans la première moitié du XVIIIe siècle, etc.).En Angleterre, l'écriture linéaire est présente dans certains épisodes de l'opéra de chambre d'Henry Purcell, Didon et Énée (1689). On la rencontre aussi dans sa Music for the Funeral of Queen Mary (1695), là encore, essentiellement dans les chœurs, mais pas uniquement : la fameuse « Mort de Didon » est bâtie sur un ground (un ostinato) présentant un chromatisme descendant, à la basse continue, sur lequel se développe la partie de solo, dont la dynamique et le chromatisme différent de la phrase d'introduction instrumentale (c'est la basse de cette phrase introductive qui sera reproduite obstinément jusqu'à la fin de l'air).L'écriture polyphonique ne se limite donc pas aux œuvres chantées : les 15 Sonates du Rosaire (Rosenkranzsonaten, pour violon et basse continue), composées vers 1678 par l'autrichien Heinrich Biber, en sont un exemple célèbre, parmi beaucoup d'autres.En Angleterre, l'écriture en contrepoint dans les consorts of viols (ensembles de violes) de la fin du XVIe et du XVIIe siècle est caractéristique de ce type d'ensembles, où l'unité, due à la texture très dense du tissu contrapuntique, ne laisse aucune place à l'individualité : difficile de suivre l'évolution d'une voix ou d'une autre à l'intérieur de la polyphonie.À l'époque, et jusqu'au XIXe siècle, l'habitude s'est prise de traiter certains chœurs de manière fuguée. C'est particulièrement vrai et développé pour les chœurs de fin (mais bien sûr, pas systématiquement et pas uniquement). On trouve cela, parmi de nombreux autres exemples, dans plusieurs œuvres célèbres de : Vivaldi (en particulier dans le chœur final de son Magnificat, vers 1725), Haendel (Le Messie, 1741, Solomon, 1749, etc.), W. A. Mozart (Requiem, 1791), ou encore dans le Requiem allemand de Johannes Brahms (1868), etc. Le Requiem de Mozart comporte plusieurs fugues. Celui de Brahms en présente deux (dans les numéros 3 et 6). L'écriture polyphonique et contrapuntique (qui ne rompt pas avec l'héritage d'Heinrich Schütz ou de Bach par exemple) est, d'une manière générale, bien présente dans l'œuvre de Brahms, romantique mais aussi digne continuateur des classiques ou de l'école germanique d'inspiration luthérienne.La musique profane n'est pas en reste. On peut citer, entre autres exemples :L'ouverture de La Flûte enchantée, opéra féerique et initiatique de Mozart (1791), qui présente un fugato orchestral développé. Ce Singspiel présente d'autres épisodes d'écriture linéaire ;Dans son célèbre quintette à cordes en sol mineur (1787, Koechel 516) sa maîtrise du contrepoint apparaît clairement, en particulier dans les développements ; il en est de même dans son non moins célèbre quatuor à cordes connu sous le nom de « quatuor des dissonances » (Koechel 465).Le Minuetto et le Finale de sa Symphonie « Jupiter » (1788) mettent en œuvre un contrepoint très complexe mais limpide, clairement hérité de J. S. Bach. Ceci est préfiguré sans sa symphonie n° 38 : Prague (1786).La Grande Fugue en si bémol majeur pour quatuor à cordes, opus 133, de Ludwig van Beethoven, a d'abord été composée comme dernier mouvement de son Quatuor op. 130 (1825). Il l'en détacha en 1827 pour la publier séparément ;Ses deux sonates pour violoncelle et piano (n° 4 et 5 : op. 102, n° 1 et 2) présentent chacune un mouvement en style de contrepoint élaboré (la seconde se termine par une fugue). Mais l'écriture polyphonique apparaît aussi bien dans d'autres circonstances, comme le dernier mouvement de sa Cinquième symphonie, par exemple : cela constitue alors un élément de langage, parmi d'autres. Beethoven avait été formé au contrepoint, essentiellement par le maître de chapelle et compositeur Johann Georg Albrechtsberger, à Vienne ;En 1846, dans sa Damnation de Faust, Berlioz ironisera sur la notion de fugue, forme musicale née de la polyphonie et du contrepoint vocal (il la reprendra à sa manière dans la scène de la taverne d'Auerbach à Leipzig). Selon Antoine Livio, dans son ouvrage intitulé Maurice Béjart, « la bestialité [des convives] atteint son paroxysme lorsque Brander réclame : ""pour l’Amen, une fugue ! une fugue, un choral ! Improvisons un morceau magistral"" ». Berlioz, auteur d'un célèbre Requiem (1837) et d'un Te Deum non moins célèbre (1849) ne comprenait plus guère ce langage musical que sous l'angle d'une théâtralité très romantique. Par ailleurs, la politique anticléricale du roi Louis-Philippe (qui régna de 1830 à 1848) est bien connue. Cela ne l'empêcha pas d'utiliser le mode d'écriture polyphonique dans ces œuvres, la fugue en particulier, dans le Sanctus de la messe de Requiem par exemple.Chez Richard Wagner, « le style des Maîtres chanteurs de Nuremberg [1868] se caractérise par un contrepoint qui suit les règles de Bach. Wagner qualifia un jour de « Bach appliqué » le prologue, dont les trois thèmes principaux sont réunis, à la fin, par des liens contrapuntiques. C'est la maîtrise de ces éléments de style qui permit à Wagner de reprendre, après douze ans d'interruption, la composition de l’Anneau du Nibelung (le 3e acte de Siegfried et le Crépuscule des dieux [1876]) avec une force d'expression musicale accrue. »Gustav Mahler utilise également ce mode d'écriture, par exemple dans sa symphonie nº 2, sous-titrée Résurrection (en allemand : Auferstehung).Au début du XXe siècle, Arnold Schoenberg, âgé de 33 ans (1907), utilise une écriture polyphonique savante, en imitations, dans Friede auf Erden (« Paix sur la terre », pour chœur à 8 voix et petit ensemble instrumental facultatif). Il en est de même dans ses Gurre-Lieder, lieder en forme de cantate, d'esprit post-romantique, pour 5 soli, récitant, grand chœur et grand orchestre (1900-1911). L'école sérielle, issue du Schoenberg dodécaphonique, reprendra elle aussi les principes de l'écriture polyphonique et contrapuntique, à partir des années 1920 jusqu'à la remise en cause globale du sérialisme dans les années 1980 environ.En 1942 (orchestration en 1946), le musicien d'origine alsacienne Charles Koechlin composa L'Offrande musicale sur le nom de Bach. Cette œuvre monumentale (dont l'effectif va du piano seul au très grand orchestre) attend toujours d'être donnée en France (elle n'avait été créée qu'en 1973, par l'orchestre de la Radio de Francfort). C'est cependant une des pièces maîtresses de son auteur.Dans West Side Story, drame lyrique (comédie musicale) de Leonard Bernstein (1957, 1961), le n° 13 de l'acte 1, Cool, est une fugue.Le musicien hongrois György Ligeti (1923-2006), une des principales figures de la seconde moitié du XXe siècle, actualise à sa manière l'écriture polyphonique évoluant sur un mode linéaire. En 2017 le Centre international de création musicale écrit : « En 1961, la pièce pour grand orchestre Atmosphères poursuit la voie inaugurée dans Glissandi en introduisant la technique de « micro-tonalité », où [la micropolyphonie résultant d']un contrepoint extrêmement serré avec de petits intervalles et un grand nombre de voix n’est plus perçu[e] en tant que tel[le], dans son détail, mais en tant que masse sonore mouvante. Lontano (1967) pour orchestre et Lux Æterna (1966) pour chœur explorent des voies similaires. Ligeti, par cette esthétique de l’ambivalence harmonie-timbre, influera beaucoup sur la génération des compositeurs de l’école spectrale. / Ligeti affina cette technique - où la répétition d’un même son dans plusieurs voix à des vitesses presque identiques crée des déphasages évoluant lentement dans le temps - dans diverses œuvres, notamment dans les scherzos du Deuxième quatuor à cordes (1968) et du Concerto de chambre (1970), ainsi que dans les Trois pièces pour deux pianos (1976). En plus de cette technique purement rythmique, Ramifications (1969) pour double orchestre à cordes brouille les lignes en accordant un des deux orchestres à un diapason légèrement différent de celui de l’autre. ».Des pièces comme Atmosphères, Lontano et Lux Æterna ont été reprises au cinéma par le réalisateur Stanley Kubrick (2001, l'Odyssée de l'espace) et The Shining (Lontano seulement).Dans l'Église catholique, l'usage de la polyphonie était considéré au deuxième rang après le chant grégorien, par le motu proprio Inter pastoralis officii sollicitudines du pape Pie X en 1903. Puis, en 1928, l'exécution de la polyphonie, surtout celle de Giovanni Pierluigi da Palestrina, était fortement recommandée dans la constitution apostolique Divini cultus sanctitatem, et par le pape Pie XI. Même si le concile Vatican II garde cette recommandation, la pratique subit un gros déclin à la suite de sa réforme liturgique, avec les chants sacrés vraiment simples.Le plus connu des compositeurs anglais du XXe siècle, Benjamin Britten, utilisa l'écriture polyphonique dans ses œuvres. Certaines devinrent très célèbres, comme la Simple Symphony (terminée en 1934), A Ceremony of Carols (1942) et sa Young Person's Guide to the Orchestra (« Présentation de l'orchestre à une jeune personne »). Cette dernière œuvre prend la forme d'une série de variations pour orchestre sous-titrées Variations et Fugue sur un thème de Purcell (1946). Les Variations sur un thème de Frank Bridge (1937) comportent également une fugue, enchaînée avec le Finale.Au cours du premier tiers du XXe siècle, Charles Tournemire par exemple, porté par le mouvement de renouveau du chant grégorien à l'intérieur de l'Église catholique, faisait entendre des mélodies grégoriennes utilisées en cantus firmus, dans certaines de ses pièces d'orgue (cf. dans L'Orgue mystique de 1927-1932 : le Cycle de Noël, op. 55 — le Cycle de Pâques, op. 56 — le Cycle Après La Pentecôte, op. 57). Jusqu'à nos jours, bien d'autres auteurs ont repris les principes de l'écriture contrapuntique.À la fin du XXe siècle, Jean-Louis Florentz pratiquera le même type d'écriture, en utilisant cette fois des mélodies d'origine non-européenne, comme sa « Harpe de Marie », n° 3 de ses Laudes pour orgue (1983-1985). On y entend des thèmes liturgiques en usage dans le christianisme éthiopien, ou encore (dans le n° 4 de ces Laudes : « Chant des fleurs »), d'autres thèmes venus cette fois du Burundi. Dans certaines de ces pièces, il arrive à Florentz d'associer cela à sa perception personnelle des sons et des harmoniques entendus dans le ronflement d'un moteur d'avion de ligne. En cela il est influencé par l'ingénieur et compositeur Pierre Schaeffer.Comme bien d'autres, le compositeur espagnol Hèctor Parra utilise l'écriture en style de contrepoint dans ses trois pièces pour orgue d'inspiration profane intitulées Tres Miradas (2016. Commande de Radio France, création l"
musique;"Un sopraniste est un chanteur adulte de sexe masculin dont la tessiture est proche de la soprano féminine. Sopraniste signifie ""qui joue la partie de dessus"" et pourrait être logiquement étendu à tous les musiciens jouant une partie dans la tessiture soprano. L'usage actuel veut qu'il ne serve qu'à désigner les contre-ténors (encore appelés falsettistes) soprani, et les castrats soprani (aujourd'hui disparus). Il est difficile d'en arrêter une définition fournie tant le nombre de sopranistes est aujourd'hui minuscule.Du XVIIe  au  XIXe siècle, lorsque les castrations de garçons prépubères étaient autorisées en Europe (à l'exception de la France), il existait des castrats contraltistes et sopranistes, mais ils utilisaient uniquement la voix de tête que l'absence de mue avait préservée. On peut donc qualifier certains castrats de sopranistes, bien que leur technique vocale ne soit pas assimilable à celle des sopranistes actuels. L’usage de l'Église du XVIIe?–?XVIIIe siècle a favorisé l’emploi du terme de « soprano naturel » pour qualifier les castrats sopranistes, au mépris des femmes soprani et des falsettistes sopranistes, deux tessitures ou pratiques vocales ayant parfaitement cours à l'époque. Farinelli, Bernacchi, Caffarelli, Carestini ou Gizziello sont les exemples les plus célèbres de castrats sopranistes.Un contre-ténor sopraniste est un chanteur dont la tessiture se situe au-dessus de celle du contre-ténor altiste. Ce dernier est souvent nommé contraltiste, et plus souvent encore alto par référence à la voix qu'il chante dans un chœur, mais également pour le différencier du joueur de violon alto.La voix d'un sopraniste peut être plus ou moins étendue, généralement assez proche de celle de la mezzo-soprano, environ du la2 au la4 (au-dessous et au-dessus d'une portée en clef de sol). Pourtant, elle est souvent plus pure et moins profonde, d'où sa qualification d'angélique.Tout comme le contre-ténor altiste, le sopraniste utilise sa voix de fausset pour chanter, ainsi que ponctuellement sa voix de poitrine pour des rôles particuliers, pour amuser, ou pour certaines notes graves où la voix mixte ne suffit plus.Il existe très peu de sopranistes à ce jour, mais citons Nicolas Hay, Hugo Mangon, Lionel Stoffel, Fabrice di Falco, Valer Barna-Sabadus, Jörg Waschinski, Jacek Laszczkowski, Mathieu Salama, David Hansen, Franco Fagioli, Arno Raunig, Adriano D'Alchimio, Aris Christofellis, Oleg Riabets, Gary Boyce, Edson Cordeiro, Yves Le Pech, Radu Marian, Angelo Manzotti, Paul Laumont, Michael Maniaci, Bagdasar Khachikyan, Chris Colfer, Francesco Divito, Oswald Musielski, Barabasi Zsolt, Marcel Lucien Arpots, Ludovic Baas, Nicolas Achten, Javier Fuentes, Paulo Abel do Nascimento, Étienne Cousineau, Alain Vu, Patrick Husson, Grégoire Solle, Philippe-Emmanuel Toussaint ou David Albert-Brunet... À noter que Max Emanuel Cen?i? ainsi que Philippe Jaroussky, aujourd'hui tous deux mezzo-soprano (voire alto), commencèrent leur carrière comme sopranistes (Cen?i? chanta par exemple Frühlingsstimmen de Johann Strauss II, air pour soprano léger ou soprano colorature). À noter que, hors de la sphère du chant classique, on trouve aussi des sopranistes tels que Klaus Nomi, Ugo Farell ou encore Thomas Otten (à la voix plus sombre de mezzo-soprano), notamment.Il existe, également, quelques très rares chanteurs masculins capables d'atteindre la colorature (le soprano colorature), à la tonalité encore plus aiguë (par exemple le hongrois André Vasary, mais aussi F. Divito ou M. Maniaci).MusiqueMusique classiqueVoix (instrument)Voix (musique classique)Chant(fr) Le site des contre-ténors et sopranistes Portail de la musique classique   Portail de l’opéra"
musique;"En musique, le système tonal est un ensemble de relations entre des notes et des accords structurées autour d'une tonique.Le langage tonal se construit sur les échelles diatoniques majeures ou mineures et en appliquant les lois de l'harmonie tonale.Ce système musical occidental s'est progressivement mis en place à la Renaissance ; il est utilisé dans la musique savante — de manière presque exclusive — depuis le XVIIe siècle jusqu'à la fin du XIXe siècle. À partir du  XVIIIe siècle, il s'est inséré puis circonscrit dans la gamme tempérée (partage d'une octave en douze demi-tons égaux) qui a permis son essor et son extension. Il est aussi la base de musiques contemporaines comme la chanson, le rock, la pop et le jazz, dont certaines développeront leur règles spécifiques.Le système tonal est le résultat d'une lente évolution qui s'est opérée sur quelque six siècles — de l'époque carolingienne à la fin du Moyen Âge. Apparu à partir de la Renaissance, le système tonal a succédé au système modal dont il est à la fois un appauvrissement et un enrichissement. Enrichissement harmonique, il correspond aussi à un net appauvrissement mélodique, puisqu'on passe de huit modes (et plus) à deux seulement, qui ont tendance à uniformiser les lignes mélodiques. Au cours du XVIe siècle, les deux systèmes ont plus ou moins coexisté, à l'intérieur d'une échelle non encore tempérée : simplement, la ""note sensible"" tendait à envahir les modes et préludait à l'introduction de la gamme ""tonale"" . Par ailleurs, quoique abandonné par l'école strictement classique de la seconde moitié du XVIIIe siècle, le système modal est encore largement utilisé jusqu'au XXIe siècle : depuis la fin du XIXe siècle (essor des écoles nationales en Europe), ce sont les systèmes modaux (issus des musiques traditionnelles) qui ont permis de ré-enrichir la tonalité.Le système tonal repose sur les sept degrés hiérarchisés de l'échelle diatonique, organisés autour du degré fondamental qu'est la tonique — « pôle d'attraction » des autres notes — ainsi que sur une note dominante, cinquième degré de la gamme : la tonique et la dominante permettent d'établir un système dynamique, la dominante créant une tension, que résout la tonique, les autres degrés se rattachant à l'une ou à l'autre.Parmi les sept degrés d'une gamme, trois sont considérés comme les « très bons degrés », le premier, le quatrième et le cinquième, parce qu'ils remplissent les trois « fonctions tonales » : fonction de tonique (Ier degré), fonction de dominante (Ve degré) et fonction de sous-dominante (IVe degré). La sous-dominante élargit la tonique (cadence plagale) et amène une cadence (cadence parfaite). Le deuxième degré est employé comme la sous-dominante (il a l'avantage d'être plus dynamique et d'évoluer plus facilement vers une dissonance) et le sixième degré permet une ambigüité conclusive, en même temps qu'une ouverture d'un mode majeur vers ton relatif (dans le même souci d'ambigüité). Seul le septième degré, ""note sensible"", reste fondamental mélodiquement mais inusité harmoniquement. Ce degré est plutôt employé comme un cinquième degré, la dominante, auquel est ajouté une septième mineure et soustrait de sa fondamentale.Le mot mode définit la structuration d'une échelle musicale (gamme de fréquences), qui, dans les systèmes modal et tonal, est hiérarchisée. Dans un sens plus restreint, il se réfère au schéma d'intervalles constitutif de chacun des deux « modes », majeur et mineur, de la musique tonale. On n'emploie pas le mot, pour les structures non hiérarchisées (musique atonale en général). Il désigne plus largement toute organisation définie à partir de n'importe quel paramètre sonore, dans la musique moderne (modes rythmiques, modes d'intensités, de timbres…).Au cours du XVIe siècle, les modes anciens médiévaux disparaissent progressivement : seuls subsisteront les modes majeur et mineur qui sont les héritiers des précédents. Cette suprématie ne sera remise en cause qu'à la fin du XIXe siècle, avec la réutilisation des modes anciens et l'introduction de nouvelles échelles issues des musiques traditionnelles européennes.Le point commun entre ces deux modes, majeur et mineur, est l'apparition d'une ""note sensible"" (septième degré à distance d'un demi-ton de la tonique), qui permettra l'élaboration de la dynamique tonale (Cette dynamique repose, en effet, sur l'alternance ""tonique-dominante-tonique"", qui permet la résolution de la tension engendrée par la note sensible sur le degré de la tonique, mélodiquement; harmoniquement, cette résolution est construite sur  l'enchaînement ""dominante-tonique"").La différence entre le mode majeur et le mode mineur repose précisément sur la position des tons et des demi-tons de l'échelle diatonique par rapport à la tonique. Plusieurs schémas d'intervalles s'imposent : Le mode majeur est formé de deux tétracordes contenant chacun un ton, un ton, un demi-ton ;Dans le mode mineur, un ton, un demi-ton, un ton forment le premier tétracorde ; le second tétracorde varie en fonction des gammes :un demi-ton, un ton, un ton pour la gamme mineure naturelle,un demi-ton, un ton et demi, un demi-ton pour la gamme mineure harmonique,un ton, un ton, un demi-ton (tétracorde majeur) pour la gamme mineure mélodique ascendante.Dans tous les cas, les deux tétracordes sont séparés par un ton.On transpose ces schémas pour obtenir les différentes tonalités, à partir des modèles initiaux de do majeur (notes naturelles) et la mineur (seule la sensible est altérée). On distinguera tout de même les modes mélodiques (deux possibilités pour le mode mineur) et les stricts modes harmoniques. À l'écoute, le mode majeur apparaît franc et lumineux, le mode mineur restant beaucoup plus en demi-teinte, avec une pointe de nostalgie. Cependant, depuis l'époque romantique, certains compositeurs en font une utilisation inverse.En musicologie, le mot mode désigne uniquement la composition d'une échelle, quelle qu'elle soit. Du fait de la structure hiérarchique des échelles modales et tonales, le mot mode tend à se référer à une échelle hiérarchisée. En ce sens, les échelles atonales ne correspondent pas à des modes.Dans un sens plus restreint, le mot mode se réfère à la constitution des deux échelles tonales possibles, majeure et mineure. Ce cadre utilise le terme de tonalité qui se caractérise par deux paramètres :Le ton, c’est-à-dire la note sur laquelle s'établit la tonique de l'échelle considérée ;Le mode, c’est-à-dire le système de séquence d’intervalles, majeur ou mineur, appliqué à ce ton.Les deux échelles de référence du système majeur/mineur sont le Do majeur et La mineur, elles ne demandent d’appliquer aucune altération sur les notes usuelles pour obtenir la séquence d’intervalles souhaitée.Enfin certains compositeurs, comme Olivier Messiaen, ont élargi la notion de mode aux rythmes (éléments constitutifs des échelles rythmiques), aux intensités (gammes d'intensités), aux timbres, et à tous les paramètres du son musical.La quinte juste est à la racine des musiques traditionnelles du monde entier, même de transmission orale ; en Europe aussi, la quinte juste accompagne les chants populaires (par exemple, en ""bourdon"" tenu). C'est donc cet élément ancestral et populaire que l'on retrouve, à la racine du système savant.Parmi les mouvements mélodiques les plus utilisés dans la musique tonale, l'intervalle de quinte juste occupe une place privilégiée — cf. cycle des quintes. C'est un intervalle à la fois structurel (ordre des gammes), harmonique (présent par exemple dans le mouvement de basse de la tonique à la dominante, ou dans l'accord parfait) et mélodique (de nombreux chants ou phrasés musicaux vocaux tournent autour de la quinte ou s'élaborent en ambitus de quinte).Harmoniquement, ce mouvement régit fréquemment la basse. L'enchaînement des trois bons degrés (V - I - IV) produit deux fois cet intervalle. Par ailleurs, l'enchaînement de l'accord de septième de dominante et de l'accord parfait de tonique — archétype du système tonal, appelé cadence parfaite — est également construit sur un mouvement harmonique de quinte juste à la basse (et un mouvement mélodique de seconde mineure ascendante, la ""note sensible"" se résolvant sur la tonique) à la partie supérieure (mélodique)  .Presque toute la musique tonale est construite sur une alternance de moments de tension (autour de la région de la dominante) et de moments de détente (retour à la tonique).D'autre part, la dissonance (intervalles de seconde ou de septième, par exemple), réalisée selon certaines règles, est ressentie comme une tension nécessitant une détente — cette dernière consistera en une résolution sur la consonance de base la plus proche, à la suite d'un mouvement mélodique d'attraction. Pour plus d'informations sur les enchaînements d'intervalles harmoniques, sur les dissonances, les consonances, etc., consulter l'article Mouvement harmonique.La sensible, VIIe degré des modes majeur et mineur, par son attraction vers la tonique située au demi-ton diatonique supérieur, participe également de ce mécanisme, et constitue donc l'un des deux éléments de la relation d'attraction fondamentale du système tonal .Enfin, concernant l'intervalle de quinte employé à la basse harmonique, à partir de la tonique, on peut dire que la quinte juste ascendante — ou son renversement, la quarte juste descendante — est du côté de la tension, tandis que la quinte juste descendante — ou son renversement, la quarte juste ascendante — est du côté de la détente (ces intervalles représentent le trajet de la dominante à la tonique, et son inverse).Jusqu'à la Renaissance, le procédé d'écriture — appelé contrepoint — consiste en une superposition de mélodies. À une mélodie (""teneur"" en valeurs longues, placée à la basse, à l'époque), on adjoint des ""contrechants"" (mélodies complémentaires) qui se rejoignent en fin de strophe ou de phrase; ensuite, l'évolution majeure sera le passage de la mélodie au soprano, les contrechants étant alors écrits au-dessous. Cela permettra l'émergence progressive du procédé d'écriture tonale — appelé faussement ""monodie"", c'est-à-dire ""mélodie accompagnée"", dès l'opéra baroque, ou mélodie accompagnée d'accords à  partir de la période classique  — qui remplace par des enchaînements harmoniques la polymélodie de la Renaissance. Dans une ""harmonisation"" tonale, le compositeur  privilégie une mélodie particulière, généralement confiée à la partie supérieure, soutenue par la partie basse — chargée de jouer les fondamentales et de remplir  les fonctions tonales — tandis que les parties intermédiaires remplissent la mission, plus modeste, de complément harmonique (ce qui n'empêche pas les compositeurs de toutes les périodes, ""Classique"" incluse - Mozart, Haydn, d'écrire de sublimes contrechants dans les parties intermédiaires, restant aussi sensibles (et formés) au contrepoint qu'à l'harmonie.Dans la dimension polyphonique d'une œuvre, on distinguera la polymélodie, qui relève du contrepoint, et les accords, que régissent les règles d'enchaînement de l'harmonie. De fait, tous les compositeurs ont une double conscience, à la fois horizontale (jeu des contrechants) et plus strictement verticale (rapports de simultanéité).Le mot polymélodie — que l'on abrège malencontreusement en polyphonie, est habituellement associé au système modal. On parle alors, plus précisément, de « contrechants » issus d'un son procédé de composition, le contrepoint — du XIIe siècle au XVIe siècle.C'est au contraire le mot harmonie qui est généralement associé au système tonal. On parle alors, plus précisément, d'« harmonie tonale » ou  encore, d'« harmonie classique », technique d'écriture qui complète le contrepoint au cours du XVIe siècle, et qui se perpétue jusqu'au début du  XXe siècle. La principale caractéristique de l'harmonie classique par rapport à la polyphonie, est de faire de l'accord une entité autonome avec une fonctionnalité spécifique.Au XVIIIe siècle, influencés par les découvertes dans le domaine de la physique, les musiciens cherchent à unifier les échelles musicales. Cette unification, rendue possible par le tempérament des instruments, et produisant une échelle unique — la gamme tempérée — est indissociable du système tonal. En 1722, Rameau, dans son Traité de l’harmonie réduite à ses principes naturels, et Bach, dans Le clavier bien tempéré, donnent, l’un la méthode, l’autre la mise en pratique, du système tempéré qui installe la tonalité, nouvelle logique dans la construction musicale.Parvenus aux confins de l’exploration harmonique et stylistique de la musique romantique, les compositeurs du début du XXe siècle essayent de se délier du système tonal, et de purifier l’écoute de la musique de ses éternels couplages entre tensions et détentes, que la tonalité lui a inculquées. L’« extra-tonalité » s’emploie à explorer, parfois en les combinant, tantôt la modalité — Debussy, Moussorgsky, etc. —, tantôt l’espace harmonique dans son entier — dodécaphonisme —, tantôt l’espace rythmique — Stravinsky… La musique dodécaphonique — œuvres d'Arnold Schönberg, Alban Berg, ou Anton Webern —, utilise des échelles de douze sons, dont aucun degré n'a plus d'importance que les autres. Elle ouvre la voie à l'atonalité.Depuis la fin du XIXe siècle, le compositeur cherche, crée, bouleverse, tonalités, modes, harmonie, formes, instruments… Il essaie tout, utilise tout, même les éléments d'un lointain passé venus jusqu'à lui. Le XXe siècle est l'époque des combinaisons les plus inattendues, le règne de l'audace, de la nouveauté, du paradoxe.Dans un tel contexte, on assiste à une contestation générale du système et des règles d'école : l'échelle diatonique et le système tonal, le principe de la résolution de la dissonance, la mesure et la régularité métrique, etc.Le compositeur dispose de l'apport classique — le système tonal avec ses modes majeur et mineur —, mais il exhume le mode mineur naturel, ainsi que les modes anciens, utilise certaines échelles appartenant à d'autres civilisations — gamme tzigane, gamme arabe, etc. —, ou encore, les modes défectifs, c'est-à-dire, possédant moins de sept degrés — telle que la gamme pentatonique, parfois appelée gamme chinoise.Ces diverses échelles — ou gammes — peuvent être transposées dans n'importe quelle échelle diatonique grâce à l'armure, ou armature, terme moins usité.La gamme habituelle, la gamme classique, est à la fois heptatonique et diatonique, ce qui signifie que, d'une part, cette gamme est composée de sept degrés et que, d'autre part, elle est toujours constituée, quel que soit le mode, de tons et de demi-tons diatoniques.Au début du XXe siècle, les compositeurs ont imaginé deux nouvelles gammes dérivées de l'échelle diatonique classique, mais exclusivement formées, l'une, de demi-tons — la gamme chromatique, que nous avons déjà rencontrée —, l'autre, de tons — la « gamme par tons ». Ces deux gammes sont donc utilisées de façon atonale, c'est-à-dire sans référence au système tonal. Gamme chromatique La gamme chromatique était connue des compositeurs classiques — Monteverdi, Bach, Mozart, etc. —, mais seulement dans le contexte du système tonal.Or, dès le début du XXe siècle, certains compositeurs modernes utilisent les 12 sons de l'échelle chromatique hors du système tonal : ces 12 sons, traités de façon équivalente, peuvent aussi être employés dans un ordre préétabli — appelé série —, sans tonique, sans dominante, sans aucune fonction tonale, etc. Ce système créé de toutes pièces par des compositeurs tels que Schönberg, Berg et Webern, s'appelle le dodécaphonisme et, dans le second cas, musique sérielle. Dans le strict système dodécaphonique, les douze demi-tons ne sont pas hiérarchisés ; dans une œuvre sérielle, c'est la série qui recrée une fonctionnalité dans l'œuvre.Le dodécaphonisme impliquant le tempérament égal, est par là tributaire de l'écriture traditionnelle — appellation des sept notes, appellation des intervalles, pour ne rien dire de la mesure et des instruments… C'est, d'ailleurs, malgré quelques recherches en quarts de tons, ce qui le limite. Gamme par tons La gamme par tons — parfois improprement appelée gamme chinoise —, est une gamme hexatonique dont les six degrés sont séparés par des tons : elle a été très utilisée par Debussy.Tout au long du XXe siècle, de nombreux compositeurs démontent méthodiquement et progressivement les différentes pièces de la musique traditionnelle et du système tonal, et s'acheminent vers de nouveaux systèmes.Voici les principales étapes de ce mouvement : polytonalité : procédé associant harmoniquement et mélodiquement des éléments appartenant à des tonalités différentes : tout d'abord deux — bi-tonalité —, puis davantage, jusqu'à aboutir à la musique atonale, dont la musique sérielle est un exemple notoire ;utilisation d'échelles formées d'intervalles inférieurs au ton : nouvelles échelles, désormais totalement indépendantes de l'échelle diatonique traditionnelle : échelles au tiers de ton, au quart de ton (ou autres micro-intervalles), etc. ;musique aléatoire : système privilégiant l'imprévu au détriment du prévu — musique à faire, depuis une partition —, en prenant en compte le paramètre du hasard ;musique concrète : système basé sur le collage et le traitement de sons enregistrés sur bande ;abandon des instruments classiques — jugés trop dépendants des conceptions traditionnelles — et utilisation directe des sons artificiels produits par des instruments électroniques ou électro-acoustiques.Le système de notation traditionnel — le solfège — qui depuis le XIIe siècle environ, était le seul et unique moyen de conserver une trace, même imparfaite, du geste musical, va sérieusement être concurrencé par l'invention — dès la fin du XIXe siècle — et surtout la banalisation, au cours de la deuxième moitié du XXe siècle, des divers procédés d'enregistrement et de diffusion sonore : disques, magnétophones, ordinateurs et synthétiseurs.Par ailleurs, le système de notation traditionnel est inadapté à la plupart des musiques nouvelles. Chaque musique nouvelle suppose son propre système de notation, ses propres règles de codification, « son propre solfège ».Cependant, et paradoxalement, un siècle après les premières œuvres sérielles, la très grande majorité de la musique consommée dans le monde occidental reste tributaire du système tonal — musique populaire, musique industrielle, et même, tout un pan de la musique savante. « De gré ou de force, nous baignons tous dans la tonalité. »Analyse harmoniqueGammes et tempéraments dans la musique occidentaleHarmonie tonaleTonalitéHeiner Ruland : ""Évolution de la musique et de la conscience - Approche pratique des systèmes musicaux"", ÉAR, Genève, 2006  (ISBN 2-88189-173-X).Philippe Gouttenoire et Jean-Philippe Guye, Vocabulaire pratique d'analyse musicale, DELATOUR FRANCE, 2006, 128 p. (ISBN 978-2-7521-0020-7) Portail de la musique classique   Portail de la musique"
musique;"En musique, le timbre est l'ensemble de caractéristiques du son qui permet de reconnaître un instrument ou une voix — par opposition à la note ou aux mots. En musique comme dans les autres domaines, le « timbre » d'une voix humaine est l'ensemble de ses caractères distinctifs.La notion de timbre apparaît en Europe au XVIIIe siècle dans la musique polyphonique. Au XXe siècle, la synthèse électronique des sons a permis le développement de timbres entièrement nouveaux. La musique pop  se caractérise par son invention de timbres, notamment de guitare électrique ; la musique électronique les a diversifiés et enrichis. L'étude de la perception des timbres, commencée au XIXe siècle avec les expériences psychoacoustiques de Helmoltz sur les sons continus, s'est poursuivie avec ces nouveaux instruments en incluant la dynamique sonore, puis en s'inscrivant dans les recherches sur la cognition, considérant que la reconnaissance des timbres est une instance de celle des formes.Par ailleurs, un timbre est une cloche, une clochette ou, anciennement, un petit tambour, fixes, que l'on frappe avec un marteau ; c'est aussi un boyau, un ressort ou un balai, effleurant une partie vibrante d'un instrument de musique pour en colorer le timbre comme celui d'un mirliton — par exemple sur une caisse claire, un balafon, une kora ; dans la chanson, un timbre était au XIXe siècle un air connu de tous sur lequel on mettait des paroles au goût du jour.Hector Berlioz présente « l'étude fort négligée jusqu'à présent, de la nature du timbre, du caractère particulier et des facultés expressives de chacun d'eux » comme un des thèmes principaux de son Traité d'instrumentation et d'orchestration , dont l'objet est justement l'organisation des timbres des instruments de l'orchestre.L'identification des timbres est considérée, en Occident, comme une compétence musicale de base. Elle permet de séparer, à l'écoute d'un ensemble, les lignes mélodiques des parties que jouent des instruments différents. Prokofiev a composé Pierre et le Loup pour en faciliter l'acquisition aux enfants. Il comprend des parties de violons, de flûte, de hautbois, de clarinette, de cors, de basson, de trompette. Cependant, ce qu'on appelle théorie de la musique ne le mentionne pas. Les partitions indiquent les noms des instruments : il faut que cela vaille comme description du timbre — avec des indications comme pédale, sourdine, col legno , etc., alors que la hauteur, la durée et la puissance sonore figurent explicitement. Contrairement à ces caractéristiques, le timbre ne correspond pas à une échelle de valeurs quantifiables, c'est une qualité, et non un degré ; on parle parfois de « couleur de son », comme en allemand où Klangfarbe équivaut au français timbre.La notion de timbre « n'est pas facile à cerner ». Comme la puissance mécanique de la vibration gouverne le volume sonore, et la fréquence dominante ou la fondamentale — dans le cas de sons musicaux — détermine la hauteur, Helmholtz a cherché à relier le timbre à la répartition des harmoniques. Cette analyse est insuffisante, au moins pour la voix humaine : les voyelles modifient cette répartition, mais n'empêchent pas de distinguer les locuteurs ou chanteurs. La définition courante du timbre est négative et psychologique : « caractère de la sensation auditive qui différencie deux sons de même hauteur et de même intensité… »Contrairement aux sons de la langue ou aux notes de musique, on ne peut pas décrire ou produire vocalement des timbres ; on peut seulement donner des analogies — dire qu'un timbre est rauque ou perçant, par exemple —. La mémoire des timbres est, de ce fait, capitale pour l'étude de la cognition auditive. Les recherches ont montré que la mémoire auditive n'a pas de rapport avec la mémoire motrice, et que la mémoire du timbre est particulièrement saillante chez les non-musiciens, tandis que les musiciens retiennent plus la mélodie et l'harmonie.La création et le perfectionnement des timbres est longtemps resté du domaine de la pratique des facteurs d'instruments. L'organologie décrit les instruments de musique, mais pas leur son. L'esprit scientifique s'est d'abord attaché à des notions reliées à des grandeurs physiques mesurables isolément : la hauteur, dont on sait depuis l'Antiquité qu'elle est en rapport avec la longueur de l'élément vibrant, et l'intensité, en rapport avec la puissance. Le timbre serait ce qui permet à l'être humain de distinguer finement des sources quand ces deux grandeurs sont égales. Il n'a pas de définition physique. On a cherché, depuis le XIXe siècle, à relier cette perception à des mesures physiques, sans succès. Certains théoriciens comme Schoenberg en ont conclu qu'il faut revenir sur la distinction entre hauteur. « c’est par son timbre — dont une dimension est la hauteur — que le son se signale ». Comme la hauteur se décompose en hauteur spectrale et en hauteur tonale, le timbre s'analyse en plusieurs composantes.Les premiers travaux de Joseph Fourier sur la décomposition d'une fonction périodique en une somme de fonctions sinusoïdales simples avaient laissé penser que la solution du problème devait se trouver dans l'analyse harmonique du son.Les appareils d'analyse de son de plus en plus perfectionnés sont venus infirmer ces hypothèses, qui ne sont correctes que pour un son périodique. Le sonagraphe, disponible à partir des années 1950, a permis d'explorer plus avant la décomposition du son en partiels et en harmoniques. La seule courbe d’enveloppe, qui exprime l’amplitude globale en fonction du temps, est apparue comme inadéquate à la description des caractéristiques du timbre. C'est la combinaison de la variation de l'amplitude de chaque composante harmonique (potentiellement une infinité) qui est nécessaire.Seul le sonagraphe, appareil de représentation graphique de la totalité des dimensions du phénomène (temps - fréquence - amplitude) a permis de suivre un spectre évolutif, dont chaque composante a une intensité relative qui évolue avec le temps.Les performances auditives humaines sont limitées en fréquence : au-delà de 5 kHz, on ne distingue plus les hauteurs que très approximativement, et on ne peut percevoir l'harmonicité des partiels. Au-delà de 15 kHz, les sons sont inaudibles, quoique des conjonctions de sons inaudibles puissent engendrer, par intermodulation, des perceptions sonores. Cela explique que pour les notes très aiguës, dont les harmoniques sont vite repoussées au-delà de cette limite, il devient difficile de distinguer la nature de l'instrument, sinon par ses caractéristiques dynamiques. Le sonagraphe limite en général son exploration des fréquences à 8 kHz, donnant les principaux formants discernables.D’autres éléments, physiquement simples à décrire, ne sont qu’intuitivement perçus comme influençant notre perception du timbre, et l’importance de leur rôle au sein du champ de cette reconnaissance est difficile à appréhender. La brillance, les formants, par exemple, mais aussi le vibrato, la texture sonore.Helmholtz dans sa théorie physiologique de la musique, a présenté une théorie s’appuyant sur la mise en évidence des harmoniques d’un son périodique et le calcul de leur intensité au moyen de résonateurs.Il découvrait parallèlement les fréquences de partiels inharmoniques et observait leur importance dans la nature du son. À sa suite, Carl Stumpf, philosophe et psychophysiologiste allemand, notait dans les années 1930, l'importance des transitoires (portion infime de l’attaque du son), du vibrato, des composantes spectrales (régions formantiques), de la chute dans la dimension du timbre. La portion d’attaque est essentielle à l’identification de l’instrument, ce que l'on sait également grâce aux travaux de Pierre Schaeffer. Ces transitoires d’attaque sont des phénomènes qui peuvent durer de 20 ms jusqu’à 200, voire 300 ms, selon les instruments, et qui affectent toute modification de la perception du timbre. Ces travaux révélèrent que les caractères proprement musicaux des sons sont inscrits dans la partie stationnaire.Mais on doit surtout aux travaux du Laboratoire d'acoustique musicale de l’université Paris VI (LAM), dirigé par Émile Leipp dans les années 70, d'avoir montré que bien des composantes du son, discrètes et continues; ne sont que des composantes psychologiques, psychoacoustiques, qui ne prennent place qu’au niveau cérébral, neuronal de la reconnaissance du timbre. L’étude des modes de jeu de certains interprètes révèle par exemple que la phase stationnaire est continuellement différenciée et varie perpétuellement au cours de l’exécution d’une œuvre.Beaucoup de composantes timbrales sont donc des éléments vivants, dynamiques : même si notre oreille ne peut les reconnaître intuitivement, elle sait le faire inconsciemment.À partir de ces analyses, la synthèse sonore a procédé par décomposition-recomposition pour élaborer ses modèles, et a ainsi permis de franchir un pas supplémentaire dans la compréhension des mécanismes de la reconnaissance du timbre.Commencée pour l’analyse-synthèse des sons cuivrés par Jean-Claude Risset (entre 1964 et 1969), l'étude des composantes spectrales du timbre des instruments fut reprise par James Andrew Moorer et John Michael Grey qui mirent en exergue un spectre à trois dimensions (fréquence, intensité, temps), ainsi que par Dexter Morrill dans une remarquable étude de la trompette : ces analyses ont permis de mettre en valeur l’évolution temporelle du spectre, et révélé l’importance de l’attaque et de l’enveloppe dynamique.Elles démontrent l’émergence progressive de certains harmoniques (de rang élevé) plus forts dans la partie stationnaire que dans l’attaque et la décroissance. Le concept d’espace de timbres introduit (en 1975) par J. M. Grey a ouvert la voie à la notion controversée de matériau musical, en le situant dans une représentation multidimensionnelle. Ainsi, le passage à des représentations à n (>2) dimensions est particulièrement significatif de la prise de possession des paradigmes mathématiques pour la représentation du timbre.Nous avons donc tout à fait besoin d’une vision neuve pour évaluer ce champ conceptuel de timbre. Juger un timbre revient à comparer deux matrices d’information. Du coup, les recherches qui visent à faire du timbre un élément de construction révèlent une difficulté essentielle et provoquent une réflexion sur la fonction formelle d’un paramètre mal connu.Petit à petit, l’intégration de certaines données scientifiques permet à de nouveaux paramètres de la vibration sonore de prendre place dans notre connaissance de la formation du timbre et dans celle, plus créatrice, du domaine d’influence de la fonction timbrale. Il en est ainsi, comme nous l’avons dit, des transitoires, de l’attaque, du vibrato, des composantes spectrales (régions formantiques), de la chute, etc. Mais si un classement par formes (formes d’ondes, d’attaques, d’enveloppe, de spectre) suffirait à l’établissement d’une typologie du sonore, il n’est pas sûr qu’un tel classement puisse s’effectuer au niveau musical, tant ce niveau se situe dans une autre hiérarchie. En fait, bien des composantes du son (discrètes et continues) ne sont que des composantes psychologiques, qui ne prennent place qu’au niveau cérébral, neuronal, et les données auxquelles elles se rattachent dans la musique sont fondées sur une base plus esthétique que rationnelle.On définit le timbre de la voix humaine comme l'ensemble des caractéristiques qui permettent de l'identifier.Enfant puis adulte, le timbre de la voix change lorsque l'individu grandit. Il peut devenir plus grave ou plus aigu, selon les personnes. En travaillant souvent sa voix, le timbre peut se modifier.Un timbre est à l'origine une cloche fixe ou un petit tambour, frappés avec une baguette ou un maillet.C'est aussi un boyau, un ressort ou un balai, effleurant une partie vibrante d'un instrument de musique pour en colorer le timbre comme celui d'un mirliton — par exemple sur une caisse claire, un balafon, une kora.Autre sens du mot timbre en musique : un timbre ou pont-neuf désignait au XIXe siècle un motif ou un air connu sur lequel on mettait des paroles au goût du jour pour en faire des chansons, comme c'était l'usage dans les sociétés chantantes et goguettes.Michèle Castellengo (préf. Jean-Sylvain Liénard et Georges Bloch), Écoute musicale et acoustique, Paris, Eyrolles, 2015 (ISBN 9782212138726, présentation en ligne), p. 287-385 Ch. 7 « La question du timbre ».Pierre Schaeffer, Traité des objets musicaux : Essai interdisciplines, Paris, Seuil, 1966, 2e éd., 713 p., notamment Livre III, « Corrélations entre le signal physique et l'objet musical », pp.159-260Robert G. Crowder, « La mémoire auditive », dans McAdams & alii, Penser les sons, Paris, PUF, 1994, p. 123-156, section 6 « La mémoire et les images liées au timbre musical », p. 149-152Helmholtz (Hermann Ludwig von), Die Lehre der Tonempfindungen als physiologische Grundlage für die Theorie der Musik (1863), Théorie physiologique de la musique, Paris, Masson, 1868, reprint Sceaux, J. Gabay, 1990, 544 p.Jean-Claude Risset, Hauteur et timbre des sons : Rapport IRCAM, Paris, IRCAM Centre Georges Pompidou, novembre 1978, 19 p..Jean-Claude Risset, « Quelques aspects du timbre dans la musique contemporaine », dans Arlette Zenatti, Psychologie de la musique, Paris, Presses universitaires de France, coll. « Psychologie d'aujourd'hui », 1994, p. 87-114.Barrière (Jean-Baptiste) (coord.), Le timbre, métaphore pour la composition, IRCAM, Christian Bourgois éditeur, Paris, 1991, 594 p.Son musicalAcoustique musicaleMélodie de spectre constantRecueil de timbresLAM (laboratoire d'acoustique musicale de l’université Paris VI) Portail de la musique   Portail de la musique classique   Portail de l’opéra"
musique;Une chanson, ou un chant, est une œuvre musicale composée d'un texte et d'une mélodie destinée à être interprétée par la voix humaine. Cette interprétation peut se faire sans accompagnement instrumental, c'est-à-dire a cappella, ou au contraire être accompagnée d'un ou plusieurs instruments (guitare, piano, groupe, voire un big band ou un grand orchestre symphonique). Elle peut être à une voix (monodie) ou à plusieurs (polyphonie) comme dans une chorale.Simple comptine enfantine de quelques mots ou longue chanson de geste (voir les 4 002 vers de La Chanson de Roland du XIIe siècle), cette expression littéraire et musicale peut revêtir des formes et des structures diverses (couplet/refrain, strophe ou laisse, canon, mélodie accompagnée ou lied allemand…) et couvrir des genres bien différents comme la musique traditionnelle ou folklorique, la musique classique ou ethnique, le rock 'n' roll ou le  jazz, le rap ou le slam.La création d'une chanson nécessite généralement la participation de deux artistes : l'auteur des paroles (parolier) et le compositeur de la musique. Leur travail se fait à leur gré, la mélodie naissant parfois du texte ou le texte de la mélodie, ou même les deux simultanément comme pour certains auteurs-compositeurs.L'interprète (le chanteur ou la chanteuse) donne vie à la création. Certains artistes comme Georges Brassens, Jacques Brel, Charles Aznavour ou Jean-Jacques Goldman, (parmi bien d'autres), réunissent les trois fonctions et sont alors nommés auteurs-compositeurs-interprètes ou chansonniers dans la tradition jusqu'au début du XXe siècle.Parfois, un quatrième musicien intervient : l'arrangeur musical, celui qui harmonise et donne la couleur particulière à la chanson par son orchestration (organisation des instruments d'accompagnement, notamment lors d'un enregistrement).Une chanson est composée le plus souvent d’une introduction, d’un couplet, d’un refrain, d'un pont et d’une fin. La longueur de ses éléments varie en fonction des choix opérés par les auteurs-compositeurs et aussi en lien avec les médias qui diffusent les chansons (notamment la radio, qui impose des critères plus ou moins stricts).  Le couplet est l'une des deux structures mélodiques constitutives se déroulant en alternance avec le refrain et dont la principale caractéristique est de présenter des paroles différentes à chaque nouvelle exposition, ce qui permet de faire évoluer le contenu du récit. La musique, c’est-à-dire les accords du couplet souvent ne changent pas au cours de la chanson.Le refrain est la répétition régulière de paroles d’une même chanson. C’est la partie de la chanson dont les gens se souviennent le plus souvent. Généralement, les différents refrains d'une même chanson possèdent non seulement les mêmes paroles, mais aussi la même mélodie.Il peut y avoir un pré-refrain qui est une courte partie qui se trouve directement avant un refrain mais qui se distingue du couplet par les paroles et sa musique. Le pont désigne une partie dont les accords se différencient des accords principaux. C’est une partie distincte de la chanson qui n’est pas du tout pareil au couplet ou au refrain. Entre chaque section il est aussi possible d’avoir un ou plusieurs interludes qui ne contiennent pas de paroles.Boris Vian, En avant la zizique… et par ici les gros sous, Le Livre de Poche, 1997, 192 pages  (ISBN 9782253140887). Portail de la musique   Portail des arts
musique;La trompette est un instrument de musique à vent de la famille des cuivres clairs. Elle est fabriquée dans un tube de 1,50 m de long comme le cornet. Le métal utilisé pour fabriquer la trompette est surtout le laiton (en moyenne 70 % de la trompette est fabriquée avec du laiton). Pour en jouer, on utilise souvent 3 pistons (parfois 4 dans la piccolo) ainsi que de l'air (colonne d'air).Deux trompettes ont été retrouvées dans le tombeau de Toutânkhamon (une en or et une en argent), ce qui semble indiquer l'origine très ancienne et peut-être égyptienne de cet instrument. En Grèce, la trompette alors appelée salpinx était considérée comme un instrument de guerre. On y trouvait trois épreuves : le son le plus fort (avec le plus de décibels), le son portant le plus loin, et le son le plus aigu. À Rome, on utilisait le cornu, le buccin (buccina) et le lituus. Les Celtes utilisaient le carnyx. Les Hébreux avaient également trois types de trompettes ou cors, le hazozerah, le chofar et le keren, ou du moins trois substantifs pour désigner cet instrument,.La trompette à la Renaissance ne comporte pas de piston. À cette époque, les européens ont empruntés aux Arabes les mots de la « buisine » (« buysine », « buzine », « busine »...) ainsi que le nom de « trumpa ».La crise de la trompette a duré soixante-cinq ans (1750-1815). D’une part, l’art du clarino avait atteint un sommet difficile à dépasser et, d’autre part, l’apparition de l’idéal bourgeois faisait incarner à la trompette un aspect héroïque démodé.L’époque classique montre un brusque changement dans la fonction des trompettes. Après avoir rempli une fonction héroïque qui donne le ton sous forme mélodique, la trompette se fond maintenant dans les tutti. Pour continuer à jouer son rôle héroïque, elle ne fait que couronner brièvement les crescendo. Elle doit s’adapter à la variété des tonalités, on voit donc apparaître des trompettes en fa, sol, si                    ?              {\displaystyle \flat }   ou la. Dans la musique classique, le registre du clarino ne monte plus aussi haut que dans le baroque : on monte rarement au-dessus du sol (« juste au-dessus de la portée »), parfois on rencontre un la ou un do mais très rarement.Dès la fin du baroque, on a essayé de rendre la trompette chromatique car la plupart des notes à jouer se trouvent maintenant dans la troisième octave des partiels, les notes sont plus écartées donc les possibilités sont plus restreintes. Différentes techniques vont essayer de trouver une solution à ce problème.Une des plus anciennes de ces techniques est le bouchage qui fut inventé en 1775, puis inutilisé à partir de 1840. L’idée vient du corniste Anton Joseph Hampel (de), qui en 1750 avait remarqué qu’en introduisant la main dans le pavillon, on pouvait faire baisser la note émise d’un demi, voire un ton complet. La technique n’a pas été mise en œuvre tout de suite sur les trompettes car leur forme ne permettait pas à l’instrumentiste de mettre sa main au niveau du pavillon. C’est en 1777 qu’un facteur « enroula » plus la trompette pour lui donner une forme de demi-lune. On bouchait le pavillon avec trois doigts de la main droite. Le bouchage influence cependant le timbre de la trompette. En France, David Buhl fut le plus éminent des trompettistes jouant avec ce procédé. Dans sa méthode, il distinguait la trompette d’ordonnance (instrument de cavalerie en mi                    ?              {\displaystyle \flat }  ) et la trompette d’harmonie (instrument d’orchestre en sol). On pouvait mettre cette dernière dans des tons plus graves à l’aide de coulisses de rechange et on obtenait les demi-tons au-dessous d’une note donnée en bouchant le pavillon. Le gros défaut de cette technique est l’inégalité sonore entre les notes ouvertes et bouchées.La deuxième technique est la trompette à clefs. Elle a les mêmes dates d’apparition et de disparition que le bouchage. L’idée commença, encore une fois, à être expérimentée sur le cor. La première trompette à clefs a été construite en 1777 mais ne connut aucun succès car le timbre caractéristique de la trompette disparaissait presque entièrement et était à mi-chemin entre la trompette et le hautbois. Indépendamment les uns des autres, plusieurs inventeurs firent différents essais dans le même sens. C’est en 1793 qu’un amateur nommé Nessman a mis au point une trompette à clefs qui gardait le timbre de la trompette et avec laquelle il pouvait monter une gamme chromatique. L’expérimentateur le plus heureux et en même temps le plus grand virtuose de la trompette à clefs fut A. Weidinger. D’ailleurs pour lui et sa trompette à clefs, Haydn, un de ses amis, composa son fameux concerto en mi bémol majeur, qui fait usage du registre du clarino et peut être joué avec seulement trois clefs, alors que celui de Hummel a un plus grand choix de notes graves et en nécessite une quatrième. Le gros défaut de cet instrument est le même que pour la trompette à boucher : l’inégalité entre les notes où certaines clefs sont ouvertes et ces mêmes notes lorsqu’elles sont toutes fermées.La troisième technique a été utilisée surtout en Angleterre entre 1790 et 1885 : c’est la trompette à coulisse. Comme son nom l’indique, le moyen utilisé ici pour rendre la trompette chromatique est la coulisse. Cette coulisse, qui est en forme de U comme sur un trombone mais moins longue que sur celui-ci, est plus proche de l’instrument et comporte un mécanisme permettant de revenir à la position initiale. Elle était appréciée grâce à sa sonorité noble et naturelle mais c’était plus un instrument d’orchestre que de solo à cause notamment de sa raideur mécanique. La trompette à coulisse se construisait en fa mais comportait des coulisses pour l‘accorder dans des tons inférieurs. Elle a subsisté plus longtemps que la trompette à boucher et celle à clefs grâce à la forte personnalité des personnes qui la défendaient. Piston La grande invention du XIXe siècle pour la trompette est le piston. C’est un des deux grands évènements de l’histoire de la trompette avec l’admission de la trompette dans la musique de concert vers 1600. Le piston a été inventé vers 1815 (mais des ébauches existaient dès 1788), il fut une réponse au vœu de faire devenir la trompette chromatique, dans le registre grave vers 1750. Le système de pistons avait tous les avantages des systèmes antérieurs de « chromatisation » sans aucun des inconvénients.Le piston est un élément de l'instrument très fragile et une simple chute peut le dérégler. Les avantages sur les autres systèmes sont : l’instrument est entièrement chromatique et toutes les notes présentent le même timbre (peut-être pas au début, mais des perfectionnements le permirent très vite). Alors que la trompette à clefs raccourcissait le tube en provoquant des pertes de charge — des « fuites » —, la trompette à pistons comme la trompette à coulisse l’allonge.Cependant on ne fait plus intervenir la physique en tirant sur une coulisse mais on agit mécaniquement sur un, deux, voire trois pistons, ce qui permet d'améliorer la dextérité. À la technique traditionnelle, vient maintenant s’ajouter un élément nouveau : l’habileté digitale. Alors que les trompettistes du baroque n’avaient que trois éléments à coordonner (lèvres, souffle et langue), ceux qui utilisèrent une trompette à pistons en avaient quatre : souffle, lèvres, langue et doigts.La trompette à pistons s’imposa rapidement dans la musique militaire mais se heurta à des oppositions (surtout par conservatisme) dans le milieu symphonique. L'inconvénient majeur du piston, hormis sa fragilité résolue par l'usage de monel, un alliage résistant, est qu'une trompette à perce parfaitement cylindrique et à pistons ne peut pas être juste, pour des raisons physiques. La modulation de la perce de la branche d'embouchure et l'adaptation de la colonne d'air et — un peu — des lèvres permettent de corriger la justesse. Chronologie sur les pistons La trompette à pistons (pistons de type Périnet) en si                    ?              {\displaystyle \flat }   — plus communément appelée trompette en si bémol — est celle qui est la plus utilisée aujourd’hui dans la plupart des pays. Mais la trompette à valves rotatives (appelée aussi « trompette à palettes ») est largement présente en Allemagne et en Europe de l'Est.La trompette en ut est aussi beaucoup utilisée, en particulier en France, dans les orchestres symphoniques et pour certains concerti pour trompette. Elle existe aussi en version à pistons ou à valves rotatives.À cause d’une attaque trop aléatoire avec une trompette normale en si                    ?              {\displaystyle \flat }  , certains instrumentistes utilisent la trompette piccolo pour jouer surtout des œuvres baroques dans lesquelles le registre aigu est souvent très utilisé (anciennement appelé clarino). La trompette piccolo ne monte pas plus haut que la trompette normale en si                    ?              {\displaystyle \flat }  , elle n'est pas plus facile à jouer dans le registre aigu, cependant les traits aigus sont plus stables. Elle existe en version à pistons ou à valves rotatives. La plupart du temps, elle est aussi en si                    ?              {\displaystyle \flat }   (qui peut être mise en la avec une coulisse additionnelle), parfois en ré.La trompette de poche est surtout utilisée par les jeunes trompettistes débutants. Elle est choisie car sa petite taille est adaptée à celle des enfants et son poids est mieux réparti donc elle n’est pas déséquilibrée vers l’avant. Mais certains trompettistes professionnels l’utilisent.Contrairement aux idées reçues, la trompette de poche a la même longueur de tube que la trompette normale, car celui-ci est simplement plus enroulé.Il existe d’autres types de trompettes qui sont dans des accords différents, mais qui sont beaucoup moins utilisées que celles citées ci-dessus (trompette en sol, ré, mi                    ?              {\displaystyle \flat }  , fa ou trompette basse).La note et le volume de la trompette peuvent aussi être modifiés à l'aide d'une sourdine. Il en existe de nombreux types.Les plus connues sont les sourdines sèche, bol, wah-wah, plunger ou harmon.La trompette naturelle est fabriquée dans un tuyau de 1,50 m de long et constituée par l'embouchure, le tube (ou perce) et le pavillon. La perce est cylindrique, ce qui lui donne un son brillant, par comparaison au son plus doux de la famille des saxhorns. Cet instrument est encore employé dans la musique baroque sur des instruments anciens, et dans la musique militaire.Dans la trompette à pistons, un mécanisme est ajouté qui permet d'accroître la longueur du tube, ce qui permet de jouer des notes plus graves et de combler ainsi les notes faisant défaut dans la série harmonique.Le doigté est celui des instruments à pistons.Le registre courant s'étend sur deux octaves et demie, du fa# grave au do au-dessus de la portée (contre-ut). Certaines pièces du répertoire classique dépassent cette tessiture (par exemple, le 2e concerto brandebourgeois de Jean-Sébastien Bach). Pour ces pièces les instrumentistes utilisent généralement la trompette piccolo. Il est à noter qu'en jazz, il n'est pas rare d'entendre des musiciens monter jusqu'au bi-contre-ut voire plus haut.Les trompettes — sauf la trompette en ut — sont en général des instruments transpositeurs qui jouent des sons réels différents des notes écrites. Ainsi par exemple une trompette en si                    ?              {\displaystyle \flat }   joue un son réel qui est un ton plus bas que la note écrite.La trompette actuelle la plus courante est un instrument soprano, en si                    ?              {\displaystyle \flat }  .Il existe aussi des trompettes en ut (encore très utilisées par les musiciens classiques, car le son est souvent un peu plus fin, et pour l'enseignement dans les conservatoires, à partir d'un certain niveau), en ré et en mi                    ?              {\displaystyle \flat }   et la trompette piccolo en si                    ?              {\displaystyle \flat }   (souvent à quatre pistons) pour un registre plus élevé, largement utilisée dans la musique baroque. Le 4e piston de certaines trompettes piccolos sert à atteindre les notes graves de la tessiture de la trompette en descendant généralement d'une quarte.Il existe une multitude de trompettes moins usitées : celles en sol et en fa qui sont assez proches de la trompette piccolo en si                    ?              {\displaystyle \flat }  .La trompette basse est rarement utilisée en France. Son registre est sensiblement le même que celui du trombone à pistons ou de l'euphonium.Certains orchestres spécialisés utilisent encore des trompettes baroques pour jouer des pièces de cette époque (par exemple le Messie ou la Musique pour les feux d'artifice royaux, de Haendel). Ces très longues trompettes permettant un certain chromatisme dans le registre de jeu (la première fréquence de résonance de l'instrument étant très basse, les harmoniques correspondant au registre de jeu sont assez rapprochées pour donner un quasi-chromatisme). Cependant, leur usage est assez anecdotique, ne serait-ce qu'à cause de la difficulté de jeu de ces instruments, parfois facilitée quelque peu par la perce sur des répliques d'instruments anciens d'un trou harmonique bouché ou libéré par l'instrumentiste.Enfin, il existe quelques trompettes atypiques : trompettes à quatre pistons permettant de jouer des quarts de tons (cf. Don Ellis, Ibrahim Maalouf…), ou trompettes combinant pistons et coulisse (la Firebird de la marque Holton, etc.)… Il existe également des trompettes qui, bien qu'acoustiquement très proches d'une trompette standard, ne sont pas enroulées : la trompette héraldique appelée aussi « trompette thébaine », utilisées pour des effets de mise en scène par exemple dans la marche triomphale d'Aida, opéra de Giuseppe Verdi.Les trompettes disposent d'une ou plusieurs clés d'eau pour évacuer le condensat d'eau produit par le souffle humide du musicien au contact du métal. Certaines trompettes sont munies d'un ou plusieurs barillets.Un barillet fonctionne à l'instar d'une valve, et permet de dévier la colonne d'air d'une coulisse vers une autre, le plus souvent à l'aide d'un bouton circulaire à cran que l'on tourne sur une position ou l'autre. L'instrumentiste peut ainsi changer l'accord de sa trompette. Sur la photo ci-contre, le barillet permet de changer l'accord de l'instrument d'ut en si bémol.S'il permet théoriquement de disposer d'une trompette à la fois en Ut et en Si bémol dans le même instrument, ce système a néanmoins une limite de justesse. En effet, la longueur des coulisses des pistons est calculée comme une fraction de la longueur totale du tube de l'instrument. Or, une trompette en Si bémol est plus longue qu'une trompette Ut, les coulisses des pistons de ces instruments doivent être différentes pour garantir des notes justes sans effort excessif du musicien sur ses lèvres. Certaines marques fabriquent, ou ont fabriqué, des trompettes transpositrices livrées avec deux jeux de coulisses de pistons pour pallier ce problème.Gabriele Cassone, The Trumpet Book, Zecchini Editore, 2009  (ISBN 88-87203-80-6).Charles Koechlin, Les instruments à vent, Paris, PUF, coll. « Que sais-je ? » (no 267), 1948, 128 p. (OCLC 843516730).Marc Honegger, Dictionnaire de la musique : technique, formes, instruments, Éditions Bordas, coll. « Science de la Musique », 1976, 1109 p. [détail des éditions] (ISBN 2-04-005140-6).Paul Archibald et Myriam De Visscher, La trompette et les cuivres.Edward Tarr, La trompette : son histoire de l'Antiquité à nos jours, Payot, 1977.Ressources relatives à la musique : MusicBrainz (en) Musical Instruments Museums Online Tout sur la trompette : une approche scientifique et pratique de l'instrument et de l'émission du son.The International Trumpet Guild. Portail de la musique   Portail de la musique classique   Portail du reggae   Portail du jazz
musique;"La musique traditionnelle, parfois abrégé en « musique trad », désigne l'ensemble des musiques associées à une culture régionale ou à une zone géographique. Musiques orales et populaires, elles se transmettent à l'oreille, bien que certains groupes et musiciens actuels préfèrent les transcrire sur partition afin de les interpréter ou de les répertorier.Elle se différencie de la musique dite folklorique car elle ne vise pas à montrer le passé d'une musique (avec costumes, etc.), mais à faire vivre les musiques appartenant à un patrimoine de culture populaire dans l'actualité : chaque groupe ou musicien peut s'approprier la musique à sa manière, en cela influencé par son environnement culturel et social, et la faire vivre.Les trois concepts essentiels dans la définition de la musique traditionnelle sont donc l'ancrage socio-culturel géographique, la transmission et la re-création.Dans une vision folklorique, le Conseil International des Organisations de Festivals de Folklore et d'Arts Traditionnels (CIOFF) qui organise chaque année plus de 300 festivals de folklore, un programme est considéré comme de culture traditionnelle si son contenu correspond à la définition de la Convention sur la sauvegarde du patrimoine culturel immatérielde l'UNESCO, c'est-à-dire qu'il doit :être transmis de génération en génération ;être recréé en permanence par les communautés et les groupes en fonction de leur milieu et de leur interaction avec la nature et de leur histoire ;procurer aux communautés et aux groupes un sentiment d'identité et de prospérité ;contribuer ainsi à promouvoir le respect de la diversité culturelle et la créativité humaine.Le CIOFF considère par ailleurs un programme comme d'« expression authentique » si :le contenu est régional ;le costume est authentique ou fidèlement reconstruit ;la musique et la danse sont présentées sans aucun arrangement.Les musiques actuelles sont souvent opposées à la musique traditionnelle, sur le champ de l'historicité et d'une connotation socio-culturelle empreinte de modernisme alors que le Ministère de la Culture classe les musiques traditionnelles au sein des musiques actuelles. Cette confusion est largement due à l'assimilation entre folklore et tradition, dans l'esprit du grand public mais aussi dans le réinvestissement du mouvement trad, et au fait que la musique folklorique est elle plutôt sujette à une fixation picturale.Les musiques traditionnelles ont largement subi et bénéficié des innovations successives. Ainsi, le mouvement folk revivaliste des années 1970 a souvent apporté l'électrification et le rajeunissement des groupes de musiciens (comme Alan Stivell ou Malicorne). La scène bretonne a joué un rôle important dans la popularité du répertoire traditionnel (Tri Yann, Matmatah, Gwerz …). Plusieurs festivals, comme le festival de Ris-orangis, le festival de Cornouaille à Quimper ou les Rencontres musicales de Nedde, ou structures associatives et groupes participent de nos jours à la réactivation et au renouvellement de la musique traditionnelle en France.Il se trouve que dans l'autre sens, la musique traditionnelle est parfois source d'inspiration dans les musiques populaires dites actuelles (la présence de la vielle à roue chez Olivia Ruiz, le répertoire breton revisité par Nolwenn Leroy ou auparavant la chanson La belle sardane de Charles Trenet en sont des exemples). Preuve que musiques dites actuelles et musiques traditionnelles sont toutes deux d'assise populaire, que la distinction est avant tout commerciale et parfois porteuse de préjugés, et que les musiques traditionnelles, par définition mouvantes et perpétuellement réinventées, pour peu qu'on n'érige pas les métissages et la re-création en dogmes, sont tout autant actuelles.Les musiques traditionnelles actuelles, outre leur caractère régional, sont la continuité du mouvement folk qui a eu lieu sur tout le territoire français au cours des années 1960-80. Une vaste entreprise de collectage a débuté durant cette période et a grandement contribué au corpus des musiques (et de danses) dites traditionnelles en France. La professionnalisation et leur intégration aux conservatoires des enseignants en musique traditionnelle à partir de la fin des années 1980 a contribué à l'essor de ces musiques.On distingue plusieurs grands ensembles de musiques traditionnelles, proches par leur bases culturelles. Ces musiques sont plus ou moins convergentes avec la langue (langue d'oc, langue d'oïl, langue celtique) parlée :la musique traditionnelle la plus connue du grand public est la musique bretonne qui est l'expression musicale de Bretagne ;la musique auvergnate, du Morvan, du pays limousin avec des compositeurs comme Didier Mario qui fait honneur du bon vivre en Limousin avec La valse limousine, du Berry, de la Corrèze, du Bourbonnais et même du Poitou. Proches par leur répertoire et le type de danses (à l'exception peut-être de la musique auvergnate relativement particulière et ayant essaimé partout en pays de langue d'oïl) ;on la retrouve aussi en Vendée, Haute Bretagne (Bretagne Gallicante, ou pays Gallo), Flandre Gallicante et Wallonie, Picardie, musique flamande ;la musique de langue d'oc, musique occitane, musique aquitaine, musique landaise, musique provençale, etc.la musique catalane ;la musique basque ;la musique alsacienne ;la musique corse ;la musique flamande ;la musique savoyarde.Cette musique vivante a largement été influencée par des migrations populaires aux XIXe et XXe siècles ; on retrouve des répertoires communs dans diverses régions. Restent quelques bassins culturels avec une musique caractéristique : Flandre française, ou dans les Alpes.Chacune de ces cultures inclut dans les instruments utilisés une cornemuse, et/ou un accordéon (arrivé d'Italie à fin du XIXe siècle). On peut y associer suivant les régions une flûte, violon, vielle à roue, bombarde, percussion et autres instruments spécifiques à une aire géographique.On peut entendre aujourd'hui des musiques en France dans les festivals, notamment au Festival interceltique de Lorient, aux Rencontres internationales de luthiers et maîtres sonneurs de Saint-Chartier, Le Son continu à Ars, aux Nuits Basaltiques du Puy-en-Velay, au Festival international de folklore de Romans-sur-Isère et dans les bals folk, bals trad ou fest-noz, le plus souvent dans des salles des fêtes, aussi en extérieur.Les musiques celtiques du nord de l'Europe (Irlande, Écosse, Pays de Galles, Île de Man)La musique folk britanniqueLes musiques scandinaves (Norvège, Danemark, Suède, Îles Féroé)Les musiques de la péninsule ibérique (Espagne, Catalogne, Asturies, Galice, Portugal, la musique flamenco)Les musiques d'Europe centrale ( Autriche, Allemagne, Pologne, Tchéquie, Slovaquie, Hongrie…)Les musiques des pays baltes (Estonie, Lettonie, Lituanie)Les musiques des Balkans (Bulgarie, Roumanie, Macédoine, Grèce…)La musique klezmerLes musiques de l'Italie (Tarantelle et autres...)Les musiques tsiganesCes musiques traditionnelles ont donné un essor décisif d'une part au genre du poème symphonique, et d'autre part à l'ethnomusicologie.[réf. nécessaire] Ainsi, Constantin Br?iloiu, l'ami du hongrois Béla Bartók et du roumain Georges Enesco - eux-mêmes compositeurs d'une musique savante fortement inspirée par le folklore de leurs pays, au même titre que d'autres musiciens de la seconde moitié du XIXe siècle (Liszt, Dvo?ák, Grieg, Sibelius…) et de la première moitié du XXe siècle (Janá?ek, Szymanowski, Harsányi, De Falla, Ropartz…) - a parcouru, durant l'entre-deux guerres, les villages les plus reculés de l'Europe centrale et orientale pour recueillir, magnétophone en main, des dizaines de milliers de chansons, danses, thèmes mélodiques, rythmes, etc. Tandis que la musique savante se voyait ressourcée au contact direct de la musique folklorique, ces recherches, menées aussi par d'autres ailleurs en Europe, ont abouti à une connaissance renouvelée du phénomène musical.[réf. souhaitée]On rencontre des influences irlandaises et françaises dans la musique traditionnelle québécoise. La jigue irlandaise mélangée à la chanson à répondre française en est globalement le résultat. Cette culture propre au Québec s'est diffusée grâce à la tradition orale. Autrefois, les veillées amenaient les musiciens des différentes paroisses à se rencontrer et à échanger leurs versions de chanson à répondre.Aujourd'hui, la musique traditionnelle québécoise mélange cette tradition orale, des textes retrouvés dans les archives ou encore des mélodies qui se transmettent dans les familles. Plusieurs groupes professionnels tels que Le Vent du Nord, De Temps Antan, Les Charbonniers de l'enfer, Galant, tu perds ton temps, Les Tireux d'Roches, La Volée d'Castors, Bon Débarras, Genticorum et la populaire La Bottine souriante proposent différentes façons de faire de la musique traditionnelle contemporaine. Que ce soit par les différentes sonorités ou des sujets actuels adaptés à la façon traditionnelle, il en existe aujourd'hui pour tous les goûts.Les instruments utilisés sont le violon, l'accordéon diatonique, la podorythmie (tenir un rythme avec ses pieds), les cuillères, la guimbarde, la flûte traversière, la mandoline, la guitare, la vielle à roue et l'harmonica (aussi appelé ruine-babine).Des organisations et des festivals mettent de l'avant ce type de musique et le folklore québécois comme EspaceTrad, à Montréal, le festival La Virée Trad, à Carleton-sur-Mer,  le Festival de musique Ripon Trad, en Outaouais, ou le Festival de musique trad Val-d'Or. Plusieurs musiques traditionnelles sont pratiquées :assiko ;hongo.Chanson traditionnelleMusique folkRépartition géographique des genres et styles musicauxHistoire de la musiquePatrimoine immatériel, Tradition orale, Livres et disques., Centre de Musiques Traditionnelles Rhône-Alpes, 2007 (lire en ligne [PDF])« Catalogue collectif d'archives sonores et audiovisuelles sur les traditions orales », sur Portail du patrimoine oral, 2011 (consulté le 1er mars 2021).Ressource relative à la musique : (en) MusicBrainz Tradzone, le principal forum en France sur les danses et musiques traditionnellesTout sur les musiques et danses traditionnelles du mondeMusictrad, le serveur des musiques traditionnelles en FranceTradmag: le magazine des musiques traditionnellesPartitions de musiques traditionnelles de France et du mondeLe site de référence de la musique traditionnelle du sud de l'ItalieTrad en Poche, musiques traditionnelles à l'harmonica Portail des musiques du monde   Portail des arts"
musique;"La musique populaire désigne les genres de musique tirant leur origine et trouvant leur public dans les milieux populaires. Elle se développe dans un milieu urbain et industrialisé et est souvent associée à l'histoire de la révolution industrielle et technologique ayant amené la technique phonographique, ainsi qu'à l'histoire de la mondialisation.Le terme est souvent utilisé comme un comparatif par certains défenseurs de la musique savante, qui perçoivent la musique populaire comme un produit commercial et pointent ses faiblesses esthétiques, qu'ils jugent en comparaison de la musique classique européenne. Si la musique populaire est souvent associée à la musique commerciale ou de masse, elle s'en distingue néanmoins par des critères qualitatifs et par sa capacité à former des communautés de mélomanes en se nourrissant de formes musicales inscrites dans diverses traditions historiques.Il ne faut pas confondre la musique populaire avec la musique pop, qui est un genre spécifique de musique populaire.Le terme de musique populaire est l'objet de débats. Le sociologue Simon Frith estime que le terme de culture populaire « n'a de sens qu'en tant que comparatif » et que ses plus fréquents objets de comparaison sont la haute culture, la culture folklorique et la culture de masse. La musique populaire est en effet souvent comparée à la musique savante, la musique traditionnelle et la musique commerciale.Depuis les années 1980, le milieu universitaire retient la « définition anglo-saxonne » du terme, comme le rappelle le musicologue français Olivier Julien : « sont populaires non pas les musiques qui ne sont pas savantes, mais les musiques qui ne sont ni savantes, ni folkloriques ». Citant Philip Tagg, pionnier des études sur la musique populaire, le même auteur précise que « les musiques populaires […] partagent avec les musiques folkloriques l'absence de cadre institutionnel, mais ont en commun avec la musique savante d'être jouées et composées par des musiciens professionnels ». La musique populaire se distingue aussi de la musique savante (transmise par la partition) et de la musique traditionnelle (transmise par la tradition orale) par son rapport avec la technique phonographique, qui lui fait traverser l'histoire. Pour Simon Frith, elle se distingue aussi de la musique de masse, car si la musique populaire est « consommée d'une manière particulière clairement différenciée de celle des élites culturelles », l'adjectif « populaire » ne se confond pas avec celui de « masse » : « de nombreuses musiques populaires […] ont de plus faibles ventes […] que des enregistrements de musique classique à succès ».La musique populaire européenne a hérité de certains des usages de la musique modale, du système tonal et des instruments de la musique classique, mais la musique populaire de manière générale peut aussi se référer à d'autres genres musicaux et à d'autres traditions musicales. Par exemple la musique populaire japonaise se nourrit à la fois du jazz et de sa musique traditionnelle, et certains morceaux des Beatles empruntent aussi bien à la musique classique européenne qu'à la pop américaine ou à la musique traditionnelle indienne. La variété des genres et l'éclatement des frontières musicales qui caractérise la musique populaire la lient intimement à l'histoire de la mondialisation et de la révolution industrielle.Pour ses détracteurs, la musique populaire est assimilée à la culture de masse ou à la musique « commerciale ». C'est le cas de Theodor W. Adorno, qui a rendu célèbre le concept d'industrie culturelle et voyait dans les genres de musique populaire comme le jazz des simples modes ou produits commerciaux.Il a des racines très anciennes dans le chant traditionnel dit folklorique ou de folklore vivant, en France pour partie chanté en Breton, basque, provençal, corse, flamand, alsacien, etc. puis en français surtout à partir du XIXe siècle. Les thèmes des saisons, des amours, des âges de la vie, du mariage, des guerres et de la mort sont récurrents. Il accompagnait la vie de tous les jours, les travaux des champs et la garde des troupeaux par les enfants, les danses, les fêtes, etc.C'est une personne (homme ou femme), souvent anonyme, qui chante sur la voie publique, parfois associée au camelot. Ce chanteur vit de l'argent que ses auditeurs lui donnent. Des styles et modes particuliers existent selon les époques et les pays (ex : les prosopopées dites lamenti italiens composés et imprimés durant la Renaissance, de 1453 aux années 1630-1650 ; lamenti storici, parodiques, satiriques et musicaux). Parfois sans instruments, parfois muni d'un porte-voix, il cherche à attirer et captiver un maximum d'auditoire en un temps très court et s'appuie pour cela sur une musique mélodieuse, un air déjà connu et/ou un texte accrocheur, parfois politique et satirique, devant alors parfois se jouer de la police.Très populaires avant l'invention des médias modernes (radio, télévision, enregistrement sonore), ils ont largement contribué à la diffusion d'idées ou d'informations au même titre que les journaux. En effet, en dehors de quelques grands standards de la musique populaire, leur répertoire s'inspirait souvent de faits majeurs ou de faits divers remarquables, assurant une publicité à ces événements. Au XIXe siècle avec la révolution industrielle, l'apparition d'une classe ouvrière urbaine et son exode rural, elle contribue à porter et diffuser la chanson ouvrière et « sociale ».Habitués à se mettre en public dans des conditions difficiles, les chanteurs de rue avaient souvent une personnalité originale et extravertie. Au nombre de ceux-ci le célèbre Aubert (né vers 1769, attesté en vie en 1848), doyen des chanteurs des rues de Paris fut nommé par ses confrères « Syndic des chanteurs des rues » de Paris. En 1848, il parle au nom de la délégation de 800 chanteurs, musiciens et mendiants des rues de Paris venus rendre hommage à l'Élysée au chansonnier Béranger membre de la commission des secours.Le chanteur des rues a toujours fait partie des « Cris (et bruits) de la rue », mais le développement de la voiture et l'augmentation du volume sonore lié à la vie moderne, la difficulté d'occuper la voie publique, l'accusation de mendicité et surtout la banalisation des enregistrements sonores ont réduit la présence des chanteurs de rue. Il en reste malgré tout, y compris officiellement,.Le marchand de musique ou de chanson est une profession aujourd'hui disparue en Europe mais qui était encore active dans l'entre deux-guerres, avant la large diffusion de la radio puis de la télévision. C'est un métier connexe à la chanson populaire depuis plusieurs siècles (chanson autrefois spécifiquement éditée et diffusée sur feuilles volantes). Les marchands de musique étaient itinérants et vendaient des partitions de chant en entonnant eux-mêmes la musique. Ils parcouraient les villes et se déplaçaient de foire en foire. Ils proposaient leurs chansons sous forme de feuille volante, souvent grossièrement imprimée, à des personnes qui ne savaient globalement pas lire la musique, mais qui étaient intéressées par la mélodie ou par le texte de la chanson. Ces feuilles volantes étaient parfois également illustrées par des gravures, œuvres d'illustrateurs connus, intéressantes du point de vue artistique et iconographique. Certains marchands de musique ne déchiffraient pas les partitions, mais avaient une bonne mémoire des airs. Leurs feuilles volantes restent une mine d'information sur les idées, les coutumes et les centres d'intérêt des Européens au XIXe et au début du XXe siècle.Dès le début du XIXe siècle, les orphéons fédèrent les masses. Il s'agit d'abord de chorales d'enfants puis d'ouvriers. Quelques noms : Wilhem, pédagogue et fondateur du premier orphéon en 1833. Delaporte qui contribuera dans la seconde partie du XIXe siècle à donner une ampleur nationale au mouvement. À partir des années 1850, le terme « Orphéon » désigne les chorales, les fanfares et les orchestres d'harmonie qui ont connu un essor dû au développement de l'industrie des instruments de musique. Les héritiers actuels du mouvement sont le mouvement À Cœur Joie (chorales) l'UFF (fanfares) la CFBF (batterie-fanfare), la CMF (orchestres d'harmonie, orchestres à plectre).Au XIXe siècle, des centaines de goguettes rassemblent à Paris, dans sa banlieue et aux alentours des dizaines de milliers d'ouvriers ou journaliers, hommes ou femmes. Il en existe encore au siècle suivant. La goguette de la Muse rouge disparaît seulement en 1939.La musique populaire s'appuie sur quelques standards musicaux et commerciaux. Elle est aussi à l'origine d'un certain vocabulaire.Il s'agit essentiellement de chansons (des paroles soutenues par une musique instrumentale ou un petit chœur). Une chanson dure la plupart du temps entre 3 et 5 minutes (durée initiale de la face d'un disque 78 tours ou d'un vinyle 45 tours). Les textes utilisent le vocabulaire courant, voire familier. La musique est essentiellement tonale, écrite dans le mode majeur ou en mode mineur. Sa structure repose souvent sur une alternance entre un refrain et quelques couplets (en général, moins de cinq).L'ensemble, musique et paroles, est facile à mémoriser par écoute répétée. Elle s'efforce ainsi d'être facilement compréhensible et donc diffusable internationalement. À cet effet, on note une nette prédominance de l'anglais dans les paroles, au moins en ce qui concerne celle qui s'exporte massivement. La musique s'efforce de pouvoir être diffusée le plus largement possible : utilisation d'instruments courants (guitares, claviers, cuivres, cordes, percussions), arrangements musicaux standards, quasi-monopole de la langue anglaise pour les paroles de la version dite « internationale » sans pour autant éliminer toute forme de production nationale.Avant l'invention des médias audios modernes (radio, télévision, disques), la diffusion était assurée par des chanteurs de rue qui vendaient les partitions sur les marchés en entonnant eux-mêmes les chansons. La généralisation de la radio a favorisé l'émergence d'une diffusion sur les ondes par des chanteurs qui initialement interprétaient en direct puis se sont enregistrés. Actuellement, la diffusion est massive et se fait par ondes radio, par CD (on parle alors d'EPK), et par diffusion de clips vidéo au cours d'émissions de télévision, mais surtout sur YouTube, ou par des applications de musique numérique, comme Spotify, Deezer ou Apple Music.Un tube, ou « hit », est une chanson qui a particulièrement « bien marché », c'est-à-dire qu'elle a atteint des sommets de vente.Un disque d'or ou de platine récompense l'auteur d'une musique qui s'est bien vendue.Un hit-parade (en anglais : chart) est une compétition permanente de musique populaire organisée par des chaînes de radio ou de télévision. L'objectif est d'être no 1 (être « au top »), ce qui est théoriquement déterminé par le nombre de disques vendus ou par le vote des auditeurs. Plus longtemps une chanson est en tête du hit-parade, plus elle s'assure une large diffusion, favorisant les retombées commerciales.Il est notable que l'aspect commercial et promotionnel soit une caractéristique dominante de la musique populaire depuis la deuxième moitié du XXe siècle : première en termes de part de marché dans le monde de la musique, la musique populaire est l'objet d'enjeux commerciaux énormes pour les producteurs de musique, ce qui justifie l'emploi de méthodes commerciales poussées, identiques à celles utilisées pour les produits de consommation courante : méthodes dites des « grands lessiviers » : Procter & Gamble, Henkel, etc. C'est ainsi qu'une musique fait l'objet d'une « politique de lancement » pour toucher une « cible privilégiée », qu'on « fait la promotion » d'un nouveau chanteur en espérant que ses ventes « décollent », ou qu'on résilie le contrat d'un chanteur qui ne « se vend plus assez » ou dont le genre « arrive en fin de vie », quitte à le rappeler s'il « rebondit ». Les droits d'exploitation des musiques les plus populaires représentent une source importante de revenus que l'on ne cède pas facilement.La principale production de musique populaire est donc le résultat d'une politique visant à générer des profits. Ces enjeux commerciaux sont surtout le fait des grandes majors du disque (Universal, EMI, Sony, BMG). Les maisons de disques indépendantes (comme Tôt ou Tard, Naïve Records) à la diffusion plus limitée semblent être moins à la recherche de profits. Certains musiciens ne trouvant pas de maisons de disques « s'autoproduisent », mais ils bénéficient alors d'une distribution « classique » (vente de CD en magasins) et d'une visibilité réduite, bien que le développement d'Internet ait changé la donne au cours des dernières années ; on voit notamment des sites permettant de participer à la production d'artistes inconnus du grand public et des outils de diffusion comme MySpace ou autres,.Si l'enregistrement de musique en studio fait toujours appel à des professionnels, la musique populaire est la musique la plus jouée par des amateurs. De nombreux « groupes de garage » se créent dans le but de reprendre leurs musiques préférées à partir des enregistrements de leurs vedettes. Les plus talentueux et les plus constants pourront même arriver à jouer en public (soirées privées, clubs d'étudiants, bals…). Ce type de réinterprétation à partir des disques a remplacé le modèle de la musique traditionnelle fondé en grande partie sur la transmission par le jeu. De nombreux groupes de rock, de pop ou de jazz ont commencé par faire de la musique sous cette forme. Parmi les plus célèbres on peut citer les Beatles et les Rolling Stones.Le karaoké est également une forme de réinterprétation devenue courante : à partir d'un enregistrement de l'arrangement musical « réputé exact », un soliste au micro chante la mélodie. Très utilisé dans les soirées conviviales et exclusivement fondé sur des chansons à succès, le karaoké laisse une part d'interprétation au soliste. Actuellement, les musiciens amateurs peuvent profiter de la vulgarisation des outils d'enregistrement et de reproduction (stations de mixage, samplers, logiciels de mixage, graveurs de CD, etc.) pour autoproduire leur musique et n'hésitent plus à la diffuser, par Internet notamment.(en) Frans A. J. Birrer, « Definitions and research orientation: do we need a definition of popular music? » (Définitions et axe de recherche : avons-nous besoin d'une définition de la musique populaire ?), 1985, in D. Horn, ed., Popular Music Perspectives, 2 (Gothenburge, Exeter, Ottawa et Reggio Emilia), p. 99-106.Hugh Dauncey & Philippe Le Guern, Stéréo, Sociologie comparée des musiques populaires France-Angleterre, Bordeaux, IRMA / Éditions Mélanie Seteun, 2008.Marcello Sorce Keller, Contextes socioéconomiques et pratiques musicales dans les cultures traditionnelles, in Jean-Jacques Nattiez (general ed.), Musiques, Une encyclopédie du XXIe siècle, Volume 3 : Éd. Actes Sud / Cité de la musique, p. 559–592.(en) Marcello Sorce Keller, The Problem of Classification in Folksong Research: A Short History, Folklore, XCV(1984), no 1, 100- 104.Vassal, Jacques. Folksong [soi-disant]: une histoire de la musique populaire [en majeure partie] aux États-Unis. Nouv. éd. Paris : Éditions Albin-Michel, 1972, cop. 1971. 354 p.Volume! La revue des musiques populaires. La seule revue universitaire française exclusivement consacrée à l'analyse pluridisciplinaire des musiques populaires.Philippe Darriulat, La Muse du peuple : chansons politiques et sociales en France 1815-1871, Presses universitaires de Rennes, 2010R. Thérien, I d'Amours (1992), Dictionnaire de la musique populaire au Québec, 1955-1992, Lavoisier.B. Bartók (1948), Pourquoi et comment recueille-t-on la musique populaire ?, Impr. A. KundigC. D. Pessin (2004), Chanson sociale et chanson réaliste - Cités, avec cairn.infoMarie-Dominique Amaouche-Antoine, « Le cahier de chansons du conscrit » ; Revue d'histoire moderne et contemporaine (1954-) T. 34e, No 4 (oct. - déc., 1987), p. 679–685, Éd. : Société d'Histoire Moderne et Contemporaine ([URL : https://www.jstor.org/stable/20529337 1re page])Top 50Billboard magazineCatégorie:Classement musical Portail de la musique"
musique;"La musique pop (ou simplement la pop) est un genre musical apparu dans les années 1960 au Royaume-Uni et aux États-Unis. Les chansons pop parlent en général de l'amour ou des relations amoureuses. Ce genre se concentre souvent sur une idée d'accessibilité, en mettant l'accent sur des mélodies accrocheuses, entraînantes, et en prenant souvent la forme de morceaux courts avec des rythmes associés à la danse. La musique pop fut beaucoup influencée par les technologies, comme l'enregistrement à pistes multiples (vers la fin des années 1960), et le synthétiseur (durant les années 1970 et 1980).L'expansion de la musique pop a été interprétée de diverses manières, notamment comme un processus d'américanisation et d'impérialisme culturel américain ou, plus globalement, de mondialisation. Par ailleurs, de nombreux débats chez les théoriciens de la pop posent la question du statut de cette musique : art majeur ou simple objet de consommation ?, La musique pop saura pourtant au fil des décennies profondément se diversifier et évoluer, autant dans des scènes grand public qu'indépendantes ou artistiques.Le terme « chanson pop » (pop song en anglais) est apparu pour la première fois en Angleterre en 1926 pour indiquer qu'une pièce de musique avait un certain aspect attirant. Le terme « musique pop » (« pop music ») est développé en Angleterre vers 1955 pour décrire le rock 'n' roll et les nouveaux styles musicaux des jeunes qui ont été influencés par celui-ci.Par la suite, le terme « pop » (parfois « pop rock ») désigne un sous-genre apparu dans les années 1950-1960. Le rock 'n' roll évolue alors pour se subdiviser en deux branches principales, le rock plus fidèle aux racines blues dont il est issu et la pop qui met plus l'accent sur les mélodies et les harmonies vocales. On peut de ce point de vue considérer que la pop connaît sa maturité avec l'avènement des Beatles. Les représentants les plus emblématiques de la branche rhythm and blues étaient les Rolling Stones (qui sur le tard reprirent cependant l'étiquette rock 'n roll).La pop, expression issue de l'anglais « popular music » (« musique populaire »), s'est donc petit à petit distinguée comme un sous-genre du rock, dans les années 1960. Si on considère que les Beatles ont créé ou au moins amené la musique pop, alors il s'agit d'une transformation adoucie et plus pétillante du rock 'n' roll. Le premier album sera Rubber Soul, toutefois précédé de quelques chansons de l'album Help!, où figure notamment Yesterday.En France, Serge Gainsbourg était considéré à ses débuts comme un chanteur de variétés et non de musique pop à l'époque de La Javanaise ou du Poinçonneur des Lilas. Ses chansons commencèrent à être cataloguées dans la pop à partir de créations comme Qui est « in » qui est « out » en 1966, année qui voit émerger les artistes de la pop française des années 1960.Après 1967, le terme « musique pop » est de plus en plus utilisé pour désigner un rock « soft », par opposition au rock traditionnellement plus énergique. La musique rock était perçue comme plus grave et avec des buts plus élevés, alors que la musique pop était perçue comme plus éphémère, axée sur les sentiments et plus accessible. Selon le sociologue de musique anglaise, Simon Frith, la musique pop est souvent produite à des fins commerciales, non à des fins artistiques ; elle est aussi créée pour être accessible à tout le monde. La musique pop est créée par les grandes compagnies, par les musiciens eux-mêmes. Ce point de vue est néanmoins discutable en regard des innombrables groupes pop qui n'ont jamais percé, et des nombreux autres, signés sur de petits labels indépendants (Mojave 3, Big Star, Yo La Tengo, etc.) restés confidentiels, loin des « majors » (grandes maisons de disques) et des artistes formatés et dynamisés par des campagnes commerciales.Le terme « musique pop » en français, ne correspond pas à l'anglais « pop music », ce dernier recouvrant tout ce qui est commercial et populaire. Les interprètes pop les plus connus sont Michael Jackson appelé « King of Pop » par les anglophones ou encore Madonna, appelée « Queen of Pop » et Britney Spears surnommée « Princess of Pop ». Pourtant, leur style musical est très différent de la musique pop des Beatles, The Beach Boys, Blur, Elton John, Oasis, Love, Coldplay, Radiohead, Red Hot Chili Peppers, Breeders ou Arctic Monkeys. Ces derniers rejoignent le genre pop rock, une version plus douce et mélancolique du rock.La musique pop s'inclut dans un mouvement de masse, dans lequel, et dès les années 1950, les industriels prennent en compte le pouvoir d'achat des adolescents, en leur fournissant, dans l'acception mercantile du concept d'identité, clubs où danser, tee-shirts et blousons de cuir pour se vêtir, motos pour se déplacer, plus tard tableaux à accrocher au mur ou sculptures à contempler, films à visionner, et musique pour se retrouver. À l'origine (i.e. dans les années 1950), la pop désigne en Amérique l'ensemble des musiques populaires (l'étiquette est alors en France de musique dite « de variété »). Mais la particularité essentielle de ce style musical reste en fait l'absorption de tous les autres genres : la pop music se nourrit de diverses racines, telles le rhythm and blues, le jazz, le folk, le blues, la soul, le funk, la musique légère (comme l’opérette, ou le vaudeville), la musique ancienne et classique, la techno, ou les musiques extra européennes (et en particulier la musique latine).La pop a également intégré une instrumentation extraordinairement variée : à partir de la configuration basique du rock 'n' roll (guitare, contrebasse ou basse, batterie, voire, piano et saxophone), elle y adjoint en premier lieu à peu près tous les pupitres de l’orchestre symphonique (vents, cuivres, percussions et cordes). Puis, après avoir enrichi les sonorités de la guitare de diverses pédales d’effet (sustain, wah wah…), et les progrès technologiques aidant, elle se pare de claviers aux sonorités très diverses (mellotron, synthétiseur, piano électrique, vocoder, sequencer, etc.). Enfin, la pop, à l’instar de la musique contemporaine, a approché les studios d’enregistrement comme des instruments de musique supplémentaires à part entière, agrémentant sa palette sonore par l’usage de filtres, effets de distorsion ou de saturation, écho, réverbération, etc.La musique pop a par la suite apporté une rythmique dansante et légère qui se base beaucoup sur les instruments électroniques (les synthétiseurs et autres boites à rythmes) et numériques (des drum machines Roland) et les effets électroniques surtout marqués à partir de la fin des années 1970 et au début dans les années 1980 avec l'influence anglo-américaine de groupes et artistes issus du courant new wave.Mais l’originalité majeure de la pop est d'être la première expression musicale définie en amont par un média : la nature de la pop est en effet d’être accessible au public le plus large possible, donc diffusée en radio, ce qui affecte à ses débuts son tempo (souvent proche du battement du cœur humain), sa durée (en moyenne trois minutes, avec quelques notables exceptions, tel le Hey Jude des Beatles, de plus de sept minutes), son débit et sa richesse harmonique (lent, et réduite), sa texture (soyeuse et policée), l’immédiateté de ses refrains (une chanson pop est généralement construite sur le principe couplet-refrain-couplet-refrain-montée à la tierce (modulation médiantique)-refrain ad lib), et le caractère apparemment inoffensif de ses textes,,.Au milieu des années 1990, les groupes anglais Oasis, Pulp et Blur ont réussi à ré-insérer la guitare électrique dans la musique pop mainstream, formant le style britpop. À la même période, une nouvelle tendance se développe dans le milieu de la pop : les boys bands. Véritable phénomène de mode, ses représentants les plus célèbres sont Take That au Royaume-Uni et les Backstreet Boys aux États-Unis. Leur succès colossal permet à des groupes féminins d'émerger comme les Spice Girls et All Saints. Un des forts courants pop des années 1990 a été les chanteuses « à voix ». Des artistes comme Céline Dion, Mariah Carey, Whitney Houston, Toni Braxton sont aux sommets des palmarès et vendent des dizaines de millions d'albums. Ensuite, quelques-unes d'entre elles ont allié la pop plus dance à leur voix. Christina Aguilera, Kelly Clarkson, Alicia Keys et Leona Lewis sont maintenant les représentantes de cette musique pop.À l'aube du XXIe siècle, ces groupes, chanteurs et chanteuses à voix sont peu à peu supplantés par des artistes solo comme Michael Jackson, Beyoncé, Lady Gaga, Madonna, Britney Spears, puis dans les années 2010, Bruno Mars, Ariana Grande, Rihanna, Katy Perry, Shakira, Taylor Swift, Kesha, Adele, P!nk, Miley Cyrus, Selena Gomez, Dua Lipa, Lorde, Charli XCX, Ava Max, Bebe Rexha.Philippe Daufouy et Jean-Pierre Sarton, Pop music/rock, éditions Champ Libre, 1972.Agès Guéraud, Dialectique de la pop, Paris, La Découverte, 2018, 528 p. (ISBN 978-2-7071-9958-4)Volume ! La revue des musiques populaires, revue de recherche consacrée à l'analyse des musiques populaires.Ressources relatives à la musique : Last.fm SoundCloud  Portail de la musique   Portail de la culture   Portail des arts"
musique;"La musique électronique est un type de musique conçu dans les années 1950 avec des générateurs de signaux et de sons synthétiques. Avant de pouvoir être utilisée en temps réel, elle a été enregistrée sur bande magnétique, ce qui permettait aux compositeurs de manier aisément les sons, par exemple dans l'utilisation de boucles répétitives superposées. Ses précurseurs ont pu bénéficier de studios spécialement équipés ou faisaient partie d'institutions musicales préexistantes. La musique pour bande de Pierre Schaeffer, également appelée musique concrète, se distingue de ce type de musique dans la mesure où sa conception n'est pas basée sur l'utilisation d'un matériau précis mais l'inversion de la démarche de composition qui part des sons et non de la structure. La particularité de la musique électronique de l'époque est de n'utiliser que des sons générés par des appareils électroniques.Le terme de « musique électronique », trop large pour désigner un genre de musique spécifique, renvoie dans le contexte francophone à la branche populaire de la musique faisant usage d'instruments électroniques, par opposition à la musique électroacoustique, généralement créée par des compositeurs ayant reçu une formation académique en conservatoires, universités ou centre de recherches. De plus, depuis quelques années, le terme « électro » est souvent utilisé de façon abusive pour désigner la musique électronique populaire ; l'electro étant un genre de musique électronique, et non la « musique électronique » dans son ensemble. Journalistes spécialisés et amateurs de musique électronique sont généralement prudents sur l'utilisation du terme « électro », souhaitant ainsi éviter l'amalgame avec les autres genres de musique électronique. Possédant une diversité sonore sans égal, la multiplication des genres et sous-genres dans la musique électronique était inévitable.Le désir des compositeurs de construire des instruments électriques, puis électroniques, date de la fin du XIXe siècle. Les premiers instruments ont été le fruit de recherches souvent longues. Ces recherches visaient au départ à élargir l’instrumentarium orchestral et à permettre de nouvelles recherches de timbre. Citons pour mémoire : l'electromusical piano et la harpe électrique d’Elisha Gray et Alexander Graham Bell (1876), le singing arc de William Duddell (1899), le telharmonium (ou dynamophone) de Thaddeus Cahill (1900), l’ætherophone ou thérémine de Lev Theremin (1920) et l’électrophon ou sphärophon de Jörg Mager (de) (1921). Ses instruments tiraient tous parti des tubes électroniques et dont la diversité des sonorités était, malheureusement pour leur développement commercial, proportionnelle à leur encombrement.La compositrice germano-américaine Johanna Magdalena Beyer est une pionnière de la musique électronique et synthétique, avec Music of the Spheres, une des premières compositions pour instruments acoustiques et électroniques en 1938.En 1939, Hammond crée le Novachord, considéré comme le premier synthétiseur polyphonique au monde.Les premières recherches musicales expérimentales se sont servies du matériel des divers laboratoires de musique et des techniques d’enregistrement radiophoniques qu'ils ont détournés de leur fonction première. C'est à cette époque que se sont constituées dans les studios d'enregistrement et dans les institutions musicales (notamment les radios) des entités spécialisées dirigées par des musiciens et consacrées à la musique électronique. En 1951, Herbert Eimert prend ainsi en charge le studio de musique électronique de la WDR (Westdeutscher Rundfunk) à Cologne ; Pierre Schaeffer transporte son Club d’essai (devenu GRMC, Groupe de Recherche de Musique Concrète) et s’installera en 1958 à la R.T.F. (Radiodiffusion-télévision française) à Paris ; Luciano Berio et Bruno Maderna fondent ce qui, en 1955, deviendra le studio de phonologie de la RAI (Radiotelevisione Italiana) à Milan. Dans les radios européennes, à Stockholm, à Helsinki, à Copenhague et à la B.B.C (British Broadcasting Corporation) à Londres, se mettent aussi sur pied des studios consacrés à la musique électronique.Le BBC Radiophonic Workshop est l’atelier de création sonore de la B.B.C (British Broadcasting Corporation), où travaillent les compositrices Delia Derbyshire (également directrice adjointe de la B.B.C) et Daphne Oram. Daphne Oram quittera l’atelier pour créer son propre studio en 1959, Oramics Studios for Electronic Composition, où elle développe une machine, Oramics, permettant de générer des sons électroniques, retravaillés par la suite sur une bande magnétique.Aux États-Unis, Vladimir Ussachevsky et Otto Luening débutent également en 1951 les travaux de leur centre rattaché en 1955 à l’Université de Colombia, puis inauguré en 1959 sous le nom de Columbia Princeton Electronic Music Center (C.P.E.M.C.). Les subsides de l’université leur permettront d’acquérir des synthétiseurs RCA. En 1956, après avoir ouvert un studio à New York d'enregistrement sur bande magnétique très couru par les musiciens d'avant-garde, Louis et Bebe Barron produisent la première bande originale de film entièrement composée électroniquement : il s'agit de Planète interdite produit par la MGM. Des recherches sont également entreprises au studio de sonologie d’Utrecht à partir de 1961, et dans les années 1970 le studio de Stockholm (E.M.S.) réalise des recherches d’interfaces pour musicien (appelées « synthèse hybride »).Le matériau musical récupéré par ces chercheurs est de plus en plus diversifié et sa maniabilité permettra aux compositeurs de se libérer progressivement de son inertie propre. En conséquence, leurs exigences se sont faites de plus en plus drastiques. Dès les premiers balbutiements de cette expression musicale, les compositeurs se prennent au jeu d’une écriture en conformité avec cette nouvelle technique, qui marierait le plus agréablement possible les critères physiques et les critères esthétiques du matériau sonore devenu musical. Libérée de la production instrumentale, la représentation peut s’attacher à l’effet plus que la source, pour composer en fonction de la phénoménologie du son. C’est pourquoi les compositeurs recherchent la possibilité d’extraire de la technologie une nouvelle liberté d’écriture, une nouvelle liberté de choix dans les éléments constitutifs de l’expression et une prise en compte des problèmes de composition et de leur résolution formelle.La génération des années 1960 a tenté de se dégager des tendances de l’écriture musicale d’après-guerre et de recréer une nouvelle forme attachée à ces nouveaux instruments pour permettre l’émergence d’un nouveau type de musique. Puis la synthèse sonore sort des laboratoires et entre dans un nombre de plus en plus grand d’institutions publiques et privées consacrées à l’expérimentation musicale. Les compositeurs de la génération des années 1970 seront aidés par le temps réel et la miniaturisation des composantes des instruments électroniques.En 1952, Karlheinz Stockhausen compose Konkrete Étude au Club d’Essai de Pierre Schaefer. Mais c’est en 1953 et 1954 qu’il réalise les premières œuvres de musique de synthèse avec Elektronische Studie I et II. À la fin des années 1950, la musique électronique évolue vers un traitement conjoint de sons concrets (musique concrète) et de sons électroniques (musique électronique) pour donner ce qui se nomme dès lors la musique électroacoustique. C'est dans ce contexte que sera créée l'œuvre Gesang der Jünglinge, par Karlheinz Stockhausen, à Cologne le 30 mai 1956. Cette œuvre mêle des voix d’enfants démultipliées et des sons électroniques dispersés dans l’espace. Elle est conçue pour cinq groupes de haut-parleurs répartis géographiquement et permettant de construire une polyphonie spatialisée. Karlheinz Stockhausen peut-être considéré comme le premier grand compositeur de musique électronique. Il a exercé une grande influence sur les compositeurs des générations suivantes, et beaucoup de musiciens de pop music se réclament de lui.Pierre Henry est un compositeur français de musique électroacoustique né le 9 décembre 1927 à Paris. Il est connu du grand public pour le morceau Psyché Rock de la suite de danses Messe pour le temps présent. Ce morceau, plus accessible au grand public de par sa partie instrumentale rock, n'est toutefois pas forcément représentatif de son œuvre musicale, ou de son approche musicale en général.Des recherches amènent trois équipes indépendantes à développer le premier synthétiseur électronique facile à jouer. Le premier de ces synthétiseurs à apparaître est le Buchla. Apparu en 1963, il était le produit des efforts conduits par le compositeur de musique électronique Morton Subotnick. En 1962, grâce à une bourse obtenue à la Fondation Rockefeller, Subotnick et son associé Ramon Sender emploient l'ingénieur électrique Don Buchla afin de construire une « boîte noire » à composition.Subotnick décrit son idée de la façon suivante : « Notre idée était de construire une boîte noire qui serait la palette du compositeur à la maison. Cela serait leur studio. L'idée était de la concevoir de telle sorte que ce soit comme un ordinateur analogique. Ce n'était pas un instrument de musique mais cela permettrait des modulations… Ce serait une batterie de modules de générateur d'enveloppes à tension asservie et cela aurait des séquenceurs directement inclus… Ce serait une batterie de modules que tu pourrais assembler. Il n'y avait pas de machines qui lui étaient comparables jusqu'à ce que CBS l'achète… Notre but était que ça soit moins de 400 $ pour le tout et nous sommes arrivé assez près de cela. C'est pourquoi l'instrument d'origine pour lequel j'ai récolté des fonds valait moins de 500 $[réf. nécessaire]. »Un autre synthétiseur facile à jouer, le premier à utiliser un clavier comme celui du piano, fut le fruit du travail de Robert Moog. En 1964, celui-ci invite le compositeur Herbert Deutsch (en) à passer le voir à son studio de Trumansburg. Moog avait rencontré Deutsch l'année précédente, avait écouté sa musique et décidé de suivre la suggestion du compositeur de concevoir des modules de musique électronique. Lorsque Deutsch lui rend visite en 1964, Moog vient de créer les prototypes de deux oscillateurs à tension asservie. Deutsch joue avec les appareils pendant quelques jours et Moog trouve ses expérimentations tellement intéressantes musicalement qu'il construit un filtre à tension asservie. Plus tard, en septembre, alors que Moog est invité à la convention AES (Audio Engineering Society, société d'ingénierie sonore) où il présente une conférence sur « Les modules de la musique électronique », il vend ses premiers modules de synthétiseur au chorégraphe Alwin Nikolais. Avant la fin de cette convention, Moog était entré de plain-pied dans le marché du synthétiseur.Aussi en 1964, Paul Ketoff, un ingénieur du son pour la RCA Italiana de Rome contacte William O. Smith, directeur du studio de musique électronique de l'Académie américaine de la ville, en lui proposant de concevoir pour le studio de l'Académie un petit synthétiseur qui serait facile à jouer. Après consultation avec Otto Luening, John Eaton et d'autres compositeurs résidant à l'Académie à l'époque, Smith accepte la proposition et Ketoff a pu livrer son synthétiseur Synket (pour Synthesizer Ketoff) au début de 1965.L'une des contributions les plus importantes du développement de la musique électronique sera celle de Max Mathews qui réalise, en 1957 le premier son numérique. C'est le début d'une grande aventure qui scellera les noces de la musique et de l'informatique. Dans les laboratoires de l'université de Stanford, les travaux de Mathews seront vite suivis par ceux John Chowning (qui invente le procédé de synthèse par modulation de fréquence) et du compositeur français Jean-Claude Risset. Un autre pas décisif est ensuite franchi par le physicien italien Giuseppe di Giugno (en), qui réalise au début des années 1980 à l'Ircam la première machine numérique en temps réel : la 4X. L'idée en est qu'une machine peut être différente suivant les types de programmes qu'elle exécute. La musique électronique en temps réel va dès lors s'imposer et même irriguer le monde des musiques populaires et commerciales. À la fin des années 1980, Miller Puckette écrit le logiciel Max (en hommage à Max Mathews) devenu depuis Max-Msp qui va vite s'imposer comme le standard pour les compositeurs de musique électronique en temps réel. Il va collaborer avec le compositeur français Philippe Manoury sur un cycle d’œuvres (Jupiter, Pluton, Neptune, La Partition du Ciel et de l’Enfer, En écho, etc.) qui développent un suivi automatique de partitions ainsi que différents modèles d’interactions entre instruments acoustique et systèmes électroniques en temps réel.Même si la musique électronique a vu le jour dans le monde de la musique « savante », elle est entrée par la suite dans la culture populaire avec des degrés d'enthousiasme différents. Une des premières signatures électroniques est à la télévision britannique pour le thème de l'émission Doctor Who en 1963. Elle est créée par Ron Grainer et Delia Derbyshire, à la BBC Radiophonic Workshop (en) (les ateliers radiophoniques de la BBC).À la fin des années 1960, Wendy Carlos popularise la musique réalisée avec les premiers synthétiseurs avec deux albums : Switched-On Bach et The Well-Tempered Synthesizer, qui reproduisaient des pièces de musique baroque à l'aide d'un synthétiseur Moog. En 1969, George Harrison l'a introduit dans la musique rock en l'utilisant d'abord dans son album Electronic Sound réalisé en 1969, puis avec Abbey Road des Beatles. Puis dès 1972, le musicien japonais Isao Tomita produit son premier album Switched-On Rock - (Electric Samurai), inspiré par le travail de Walter Carlos. Mais c'est son album suivant, Snowflakes are Dancing, sur lequel il reproduit des pièces de Claude Debussy au synthétiseur Modulaire Moog, qui le fera connaitre. Puis le groupe allemand Tangerine Dream produit, depuis 1967, avec le Moog, une musique particulièrement inspirante, il publie une vaste discographie qui inclut des bandes sonores. Le Moog génère à l'époque une seule note à la fois, de telle sorte que, pour produire des pièces multi-couches comme ceux de Carlos, plusieurs heures de studio étaient requises. Les premières machines étaient connues pour être instables et se désaccordaient facilement. Certains musiciens tels que Keith Emerson d'Emerson, Lake & Palmer et Tangerine Dream les utilisaient néanmoins en tournée. Le thérémine, un instrument difficile à jouer, était utilisé dans certaines musiques populaires. Un thérémine est utilisé dans la musique du générique du feuilleton britannique Inspecteur Barnaby. Le tannerin ou electro-theremin, un instrument proche du thérémine inventé par le tromboniste Paul Tanner, est utilisé dans la chanson Good Vibrations des Beach Boys. Le Mellotron est utilisé dans la chanson Strawberry Fields Forever des Beatles, et une pédale à volume tonal a été utilisée comme instrument d'arrière-plan dans Yes It Is. Mais la pièce musicale exécutée avec le thérémine sans aucun doute la plus célèbre reste celle de Jimmy Page, le guitariste et fondateur de Led Zeppelin dans leur chanson phare, Whole Lotta Love.En 1970, le compositeur et jazzman Gil Mellé, pour le film Le Mystère Andromède de Robert Wise, fabrique une bande originale à partir d'appareils électroniques qu'il a lui-même modifiés et arrangés. Au fur et à mesure que la technologie se développait et que les synthétiseurs devenaient moins chers, plus robustes et plus portables, ils étaient adoptés par de plus en plus de groupes rock. Des exemples de ces utilisateurs précurseurs dans la musique rock sont des groupes tels que United States of America, Silver Apples et Pink Floyd. Si toute leur musique n'était pas électronique (à l'exception de Silver Apples), une grande partie des sons dépendait du synthétiseur, remplaçant fréquemment le rôle joué par l'orgue. Dans les années 1970, le style électronique est révolutionné par le groupe de Dusseldorf Kraftwerk, qui utilise l'électronique et la robotique pour symboliser et célébrer l'aliénation du monde moderne à la technologie. En Allemagne, des sons électroniques ont été incorporés à la musique populaire par des groupes comme Cluster, Neu!, Tangerine Dream, Can, Popol Vuh, Deutsch-Amerikanische Freundschaft (D.A.F.) et d'autres. Le courant dit de « musique planante », popularisé puis délaissé par Tangerine Dream, est repris par Klaus Schulze, ainsi que Vangelis et Jean Michel Jarre (fils du compositeur Maurice Jarre) qui popularise la musique électronique en France par ce biais avec ses albums Oxygène (1976) et Équinoxe (1978).Plusieurs pianistes de jazz importants, notamment Herbie Hancock, Chick Corea, Joe Zawinul de (Weather Report) et Jan Hammer (avec Mahavishnu Orchestra), commencent à utiliser les synthétiseurs dans leurs enregistrements de jazz fusion dans les années 1972-1974. Les pièces I Sing the Body Electric de Weather Report et Crossings d'Herbie Hancock utilisent des synthétiseurs, même si c'est davantage à des fins d'effets sonores qu'en tant qu'instruments mélodiques. Mais à partir de 1973, les synthétiseurs utilisés en tant qu'instrument solo commencent à faire partie intégrante du son jazz fusion, tel qu'entendu dans l'album Sweetnighter de Weather Report et l'album bien connu Head Hunters de Herbie Hancock. Chick Corea et Jan Hammer ont rapidement suivi le pas en développant chacun une façon unique de jouer du synthétiseur, utilisant les effets slide, vibrato, ring modulators, distorsion et wah-wah. Plus tard dans les années 1980, Herbie Hancock sortira l'album Future Shock, en collaboration avec le producteur Bill Laswell, album qui connaîtra un grand succès avec la pièce Rockit en 1983.À cette époque, il y a eu beaucoup d'innovation dans le développement des instruments de musique électronique. Les synthétiseurs analogiques ont fait place aux synthétiseurs numériques et aux sampleurs. Les premiers échantillonneurs, comme les premiers synthétiseurs, étaient du matériel cher et encombrant. Des sociétés privées telles que Fairlight et New England Digital vendaient des instruments pour plus de 75 000€ (100 000 dollars). Dans le milieu des années 1980 cependant, l'introduction de sampleurs numériques à prix modique rend la technologie accessible à plus de musiciens.À partir de la fin des années 1970, beaucoup de musiques populaires sont développés sur ces machines numériques. Des groupes et des musiciens tels que Laurie Anderson, Kraftwerk, Ultravox, Gary Numan, The Human League, Landscape, Visage, Daniel Miller, Heaven 17, Eurythmics, John Foxx, Thomas Dolby, Orchestral Manoeuvres in the Dark, Yazoo, Erasure, Klaus Nomi, Alphaville, Art of Noise, Yello, Pet Shop Boys, Depeche Mode et New Order ont développé de nouvelles manières de faire de la musique par des moyens électroniques. Fad Gadget (Frank Tovey) est cité comme le père de la new wave, bien qu'Ultravox, The Normal (Daniel Miller), The Human League et Cabaret Voltaire ont tous produit des singles de ce genre avant Fad Gadget.Les nouveaux bruits électroniques que permettaient les synthétiseurs contribuent à la formation du genre de la musique industrielle, dont les pionniers sont Throbbing Gristle en 1975, Wavestar, Esplendor Geométrico et Cabaret Voltaire. Des musiciens comme Nine Inch Nails en 1989, KMFDM et Severed Heads prennent pour modèle les innovations de la musique concrète et de l'art acousmatique, et les appliquent à la musique dance et rock. D'autres groupes, tels que Test Department et Einstürzende Neubauten, ont pris ces nouveaux sons pour en créer des compositions électroniques bruitistes. D'autres groupes encore, tels que Robert Rich, Zoviet France et Rapoon créent des environnements sonores en utilisant les bruits synthétisés. Enfin, d'autres encore, tels que Front 242 et Skinny Puppy combinent cette aridité sonore à la musique pop et dance, créant ainsi l'electronic body music (EBM). Pendant ce temps, des musiciens de dub, tels que le groupe de funk industriel Tackhead, le chanteur Mark Stewart et d'autres musiciens du label On-U d'Adrian Sherwood intègrent l'esthétique industrielle et de la musique bruitiste à la musique sur bande et les samples. Cela ouvre la voie pour une large part de l'intérêt qui a été porté à la musique dub dans les années 1990, dans un premier temps avec des groupes tels que Meat Beat Manifesto et plus tard les producteurs de downtempo et de trip-hop Kruder & Dorfmeister.Le développement de la musique house à Chicago, des sons techno et electro à Détroit dans les années 1980 et, plus tard, le mouvement acid house de Chicago et de la scène anglaise de la fin des années 1980 et du début des années 1990 ont tous contribué au développement et à la diffusion de la musique électronique. Parmi les artistes House qui ont influencé le genre, il convient de citer Frankie Knuckles, Marshall Jefferson, Jesse Saunders, Larry Heard, Kerri Chandler ou encore les Masters At Work.Pour l'electro et la techno, Aphex Twin, Juan Atkins, Derrick May, Kevin Saunderson, Carl Craig, Richie Hawtin, les Daft Punk ou encore le collectif Underground Resistance à l'origine formé de Mad Mike, Jeff Mills et Robert Hood. Sorti en 2000, l'album Kid A de Radiohead marque les esprits par sa nature très électronique pour un groupe ayant jusque-là bâti son succès sur une musique rock[réf. nécessaire].Au début des années 1990, la techno hardcore, un genre musical inspiré de la techno, du breakbeat, de l'EBM et de la new beat, émerge aux Pays-Bas et en Allemagne. Le genre recense plusieurs autres genres et sous-genres comme le gabber, la makina, le happy hardcore et le speedcore, notamment.Dans les années 2000, un fond sonore souvent électronique tourne en boucle pendant le morceau puis viennent s'ajouter toutes sortes d'instruments et de samples électroniques, avec les progrès techniques et les prix abordables des échantillonneurs ainsi que l'apparition du numérique et la démocratisation des home-studios.La musique électronique, en particulier au cours des années 1990, donne naissance à tellement de genres et de styles et de sous-styles qu'ils sont trop nombreux pour être cités ici. Même si on ne peut parler de frontières rigides ou clairement définies, on peut sommairement identifier de manière non-extensive :des genres ou styles contemporains : électroacoustique ou acousmatique, musique pour bande, concrète et improvisée ;des genres ou styles expérimentaux : krautrock, musique planante, nu jazz, rock progressif, new wave, cold wave, post-rock, industriel, electro, electronica, intelligent dance music et turntablism, witch house ;des genres ou styles consacrés à la danse : nu-disco, house, deep house, Chicago house, acid house, techno, techno de Détroit, techno minimale, house progressive, acid techno, EBM, breakbeat, drum and bass, jungle, makina, hardstyle, hardcore, Frenchcore, Acidcore, hardtechno, tribe, terrorcore, trance, trance psychédélique, garage house, ghetto house, guetto techno, freestyle, new beat, dance, dream house, fidget house, dutch house, jumpstyle, future bass ;des genres ou styles dit de chill-out : ambient, downtempo, dub, illbient, trip hop, chillstep.En 1954-1955 eut lieu une des premières étapes importantes vers un nouveau procédé de production musicale électronique : la commande directe en temps réel d’un équipement de synthèse sonore. Sont alors apparus les premiers synthétiseurs (les electronic music synthesizer) : le Mark 1 suivi en 1958-1959 par le Mark II. Ces appareils sont imaginés et construits par Harry F. Olson et Herbert Belar pour la RCA. Ils équiperont par la suite le studio de l'C.P.E.M.C. où Milton Babbitt, qui avait obtenu des crédits élevés par l'Université, les adoptera pour créer et développer sa technique d’écriture complexe et mathématisée (Composition for Synthesizer et Vision and Prayer pour soprano et sons de synthèse composés en 1961 et Songs of Philomel en 1964).Depuis, des projets entre chorégraphe et compositeur sont créés tels qu'en 1942 le ballet de Merce Cunningham sur la musique de John Cage. En 1960 à Stony Point, John Cage compose Cartridge Music, une des premières œuvres de musique électronique jouée en direct. Des têtes de lecture de phonographes étaient utilisées comme transducteurs pendant la production de l’œuvre et non plus par l’intermédiaire de l’enregistrement sur bande.Museum of Modern Electronic MusicDavid Blot et Mathias Cousin, Le Chant de la Machine, 2 vol., Paris, Éditions Allia 2000-2002, rééd. en 2016, préfacé par Daft Punk.Nicolas Dambre, Mix, Paris, Éditions Alternatives, 2001.Ulf Poschardt, DJ Culture, traduit de l’allemand par Jean-Philippe Henquel et Emmanuel Smouts, Éditions Kargo, Paris, 2002 (1re éd. : Hambourg, 1995).Ariel Kyrou, Techno Rebelle : un siècle de musiques électroniques, Paris, Denoël, 2002.Laurent Garnier, David Brun-Lambert, Electrochoc, Paris, Flammarion, coll. Documents, 2003.Peter Shapiro, Rob Young, Simon Reynolds, Kodwo Eshun, Modulations : Une histoire de la musique électronique, traduit de l’anglais par Pauline Bruchet et Benjamin Fau, Éditions Allia, Paris, 2004 (1re éd. : Londres, 2000).David Toop, Ocean of Sound, ambient music, mondes imaginaires et voix de l'éther, traduit par Arnaud Réveillon, Paris, L'Éclat, coll. « Kargo », 2004.Jean-Marc Mandosio, D'or et de sable (cf. le chapitre VI : Je veux être une machine : genèse de la musique industrielle), Paris, éditions de l'Encyclopédie des Nuisances, 2008.Techno, anatomie des cultures électroniques, hors-série Art press, no 19, septembre 1998.Emmanuel Grynszpan, Bruyante Techno. Réflexion sur le son de la free party, Bordeaux, Éd. Mélanie Seteun, 1999.Sophie Gosselin et Julien Ottavi, L’électronique dans la musique, retour sur une histoire, in Volume !, no 1-2, Éd. Mélanie Seteun, Bordeaux, 2003.Volume !, Musiques électroniques : enjeux culturels et technologiques, no 3-1, automne 2006.Ouvrage Collectif, Modulations, traduit de l'anglais par Pauline Bruchet et Benjamin Fau, Paris, Éditions Allia, 2021, 352p. Portail de la musique électronique   Portail des arts"
sport;"L'activité physique regroupe à la fois l'exercice physique de la vie quotidienne (à la maison, lors du jardinage, des courses et autres ravitaillements, lors du travail, de la marche, de l'usage des escaliers, des déplacements et des modes de transport), l'activité physique de loisirs, et la pratique sportive. Selon l'OMS, le sport est un « sous-ensemble de l'activité physique, spécialisé et organisé ». Outre la régularité et la fréquence de l'exercice, trois paramètres semblent importants lors de l'exercice : la quantité d'énergie dépensée en mode aérobie, le pic d'intensité de l'effort et la durée de l'effort. 30 minutes d'exercices par jour durant 5 jours apportent autant de bénéfice que 3 séances de 10 minutes espacées de 4 heures dans chaque journée, 5 jours par semaine.Des preuves paléontologiques montrent que l'homme préhistorique en dépit d'une activité physique intense connaissait la maladie et vivait bien moins longtemps que nous.Depuis plusieurs millénaires, avec la sédentarisation et les progrès de la santé, l'espérance de vie a régulièrement progressé, mais l'espérance de vie en bonne santé semble donner des signes de recul (notamment attribués à une dégradation de l’environnement, de l'alimentation et à une sédentarité excessive).L'activité physique est pratiquée par les armées pour entraîner leurs soldats et par les sportifs des jeux olympiques depuis l'antiquité grecque au moins.Hippocrate (460 av. J.-C. - 377 av. J.-C.) affirmait déjà à cette époque que le manque d'activité physique était préjudiciable pour la santé.Au XVIIIe siècle, le médecin italien Bernardino Ramazzini note que les messagers, se déplaçant alors en courant, ne présentent pas les mêmes problèmes de santé que les travailleurs sédentaires tels que les tailleurs ou les cordonniers.Les premières études scientifiques quantitatives portant sur l'intérêt de l'exercice physique pour la santé ne datent cependant que du début des années 1950, avec le travail de Morris & al (1953) basé sur une population d'employés du transport londonien (travail conduit durant les pires périodes de smogs londoniens) et mettant en évidence un excès de décès par maladie coronarienne chez les travailleurs sédentaires comparativement aux travailleurs physiquement plus actifs.Elles ont ensuite notamment porté sur les effets du sport ou de l'activité sur les muscles, le système circulatoire, pulmonaire, cardiovasculaire et endocrinien ou encore sur le sommeil et l'humeur ou le risque de dépression à différents âges de la vie.Depuis les années 1990 la sédentarité physique est considérée par la médecine comportementale comme un facteur de risque cardiovasculaire.Les premières recommandations médicales (à visées thérapeutiques) ne datent aux États-Unis que de 1995 notamment produites par l'American College of Sports Medicine (ACSM).S.N Blair (médecin du sport) estime en 2009 que le manque d'activité physique pourrait devenir le premier problème de santé publique au XXIe siècle.En 2008, moins de 50 % des adultes américains avaient un niveau d'activité physique tel que recommandé.L'acronyme HEPA (pour health enhancing physical activity) désigne l'activité physique bénéfique pour la santé (APBS).L'Organisation mondiale de la santé recommande un minimum de 2 h 30 d'activité physique aérobie d'intensité modérée par semaine chez l'adulte en bonne santé.Cependant, une activité plus courte (15 min par jour) diminue déjà significativement le risque de survenue de maladies cardiovasculaires quels que soient l'âge et le sexe, par rapport à l'absence totale d'activité.La pratique d'activité physique est désormais importante, voire essentielle pour être en bonne santé physique et psychique. que ce soit pour les femmes ou pour les hommes. chez les femmes, les changements corporels et hormonaux demandent des adaptations particulières. En revanche ce n'est plus une affaire d'hommes, l'activité sportive féminines est de plus en plus présente dans les différents milieux sportifs.L’activité physique chez les enfants favorise une croissance et un développement sains.En étant actif et en bougeant, l’enfant maîtrise des d’habiletés pour faire travailler ses muscles.Il développe ainsi sa force, sa puissance et son endurance.Aussi, plusieurs avantages à court et à long terme seraient identifiés grâce à celle-ci.Parmi ceux-ci se retrouve le développement des os.Effectivement, l’activité physique lors de l’enfance et de l’adolescence permet la maximisation de l’accumulation de minéraux dans les os. Cette accumulation déterminera par la suite le pic de masse osseuse que celui-ci atteindra à l'âge adulte. Par la suite, l’activité physique va favoriser le maintien d’un poids santé.En effet, la pratique de celle-ci régulièrement permet de lutter contre l’obésité.Par ailleurs, cette maladie présente une grande hausse de cas chez les enfants et les adolescents.Pratiquer une activité physique maximiserait les chances de contrer l’obésité grâce à la dépense calorique[source insuffisante].De plus, un niveau de vie actif chez les adolescents permettrait de réduire les chances de développer certaines maladies chroniques telles que le diabète de type 2.Cela s’expliquerait grâce à une grande sensibilité à l’insuline grâce à la pratique d’activité physique régulière.Ensuite, la pratique d’activité physique chez les adolescents apporterait plusieurs bienfaits et avantages sur leur santé mentale et émotionnelle.Effectivement, la pratique de celle-ci sur une base régulière permettrait une certaine réduction de l’anxiété et de symptômes de la dépression.Aussi, elle permettrait d’augmenter l’estime de soi des jeunes et offrirais une meilleure capacité à lutter contre le stress au quotidien.Par la suite, pratiquer l’activité physique, notamment les sports d’équipes, lors de l’adolescence, pourrait avoir une grande influence sur leurs résultats scolaires.Ceci leur donnerait un plus grand sentiment d’appartenance, ce qui aurait tendance à amener plusieurs bienfaits académique[source insuffisante].Les précisions manquent.Les consensus sont en faveur de 60 minutes par jour, intensité modérée à élevée lors de sports, jeux, ou d'activités de la vie quotidienne.L'activité physique est considérée à ces âges comme bénéfique, mais doit être prudente et adaptée à la condition physique de la personne.Des recommandations ont été publiées en 2007 par l'American College of Sports Medicine Association et l'American Heart Association . Outre un suivi médical, il est souvent recommandé de diversifier les activités et de les préparer par un renforcement musculaire et des assouplissements ; des exercices spécifiques d'équilibre sont utiles.Elle dépend du type de l'activité, de son intensité et de sa durée.Elle peut être quantifiée en énergie dépensée (sous forme de calories), en MET (« metabolic equivalent »). un MET à 1 correspond à une consommation énergétique au repos. Un MET à 2 correspond par conséquent à une consommation énergétique doublée par rapport à celle au repos. Cette quantification peut être indexé sur le poids de la personne.Selon une étude récente du département Santé de l'Université d'York publiée par la revue PLoS ONE et financée par une Fondation contre les maladies cardiaques (Heart and Stroke Foundation), la plupart des Canadiens (quels que soient leur âge, leur origine ethnique et leur classification IMC) sous-estiment l'effort à faire pour que l'exercice physique ait un bénéfice pour la santé, et ils sous-estiment l'effort nécessaire pour atteindre ce que l'OMS considère être un exercice modéré, et plus encore pour un exercice intense ; ceci même après qu'ils ont reçu des descripteurs d'intensité d'exercice couramment utilisés (rythme cardiaque par exemple).Les lignes directrices données pour les recommandations d'activité physique dans le monde utilisent des termes généraux pour décrire l'intensité de l'exercice (déterminé par un pourcentage donné de la fréquence cardiaque maximale d'un individu). Il semble que ces termes ne soient pas compris par le public.La pratique d'une activité physique est l'un des facteurs souvent cités d'une bonne santé et de l'allongement de l'espérance de vie.Elle a notamment des effets positifs sur le système cardiovasculaire, le maintien des muscles, la consommation en excès des lipides et glucides dans l'alimentation des pays développés, la lutte contre le surpoids et l'obésité, etc. Le sigle APBS (ou HEPA en anglais) désigne l'activité physique bénéfique pour la santé,.La pratique d'une activité physique modérée, par son influence sur le système hormonal, permet également de réduire le risque de certaines maladies, tel le cancer du sein.L'exercice physique est un des facteurs d'allongement de la durée de la vie (ou son absence est un facteur de diminution de cette durée). Une enquête sur 20 individus a été menée de 1993-2007, dont deux sont décédés en cours d'enquête, afin de déterminer l'impact du mode de vie sur l'espérance de vie. L'étude conclut que le « mode de vie idéal » - absence de tabac, consommation d'alcool égale ou inférieure à un demi verre par jour, consommation de cinq fruits et légumes par jour, exercice physique d'une demi-heure par jour - majore l'espérance de vie de 14 ans par rapport au cumul de quatre facteurs de risque. Le cumul des quatre facteurs de risque (tabac, alcool, manque de fruits et légumes et d'exercice physique) multiplie le risque de décès par 4,4, trois facteurs, de 2,5, deux facteurs de près de 2 et 1 facteur de 1,4. C'est la première fois que l'on analyse l'effet cumulé des facteurs de risque sur la mortalité,.L'exercice physique est particulièrement important pour les enfants et adolescents afin de réduire les risques d’obésité et de surpoids (qui touchent notamment la France, avec en 2013 près de 20 % des enfants de 3 à 17 ans obèses ou en surpoids.Outre la prévention des maladies cardiovasculaires, l'exercice physique semble diminuer le nombre de certains types de cancers (essentiellement cancer du sein et cancer colorectal). Santé cardiovasculaire En 1968, une étude publiée par Saltin et ses collègues fait date : c'est la première à établir un lien direct entre l'activité physique et le fonctionnement cardiaque. Les auteurs de cette étude mettent en évidence que vingt jours d'inactivité complète, avec des sujets alités, diminuent à la fois la consommation maximale d’oxygène (VO2max) et le débit cardiaque maximal d'environ 26 %. Ils montrent, inversement, que huit semaines d'activité aérobie augmentent ces mêmes paramètres de 18 et 14 % respectivement.L'un des effets les plus importants de l'activité physique sur la physiologie cardiaque est une augmentation de la VO2max. La VO2max correspond au produit du débit cardiaque maximal et de la différence artérioveineuse en oxygène (en) ; le débit cardiaque maximal étant le paramètre clé de la VO2max. L'effet d'une activité physique le plus universellement observé est une augmentation du débit cardiaque maximal.La morphologie cardiaque est également profondément modifiée par la pratique d'une activité physique sur le long terme.Il a ainsi été mis en évidence que le cœur des athlètes pratiquant des sports d'endurance était significativement plus gros.Ces modifications concernent notamment le ventricule gauche dont les parois ainsi que le volume augmentent. En conséquence, le volume télédiastolique (volume expulsé dans la circulation systémique vers les organes) augmente alors que le volume télésystolique (volume résiduel après la contraction) diminue conduisant à une augmentation du volume d'éjection systolique, le cœur est plus efficace. Le débit cardiaque correspondant au produit de la fréquence cardiaque par le volume d'éjection systolique, l'augmentation de VO2max observée à la suite d'une activité physique est majoritairement due à l'augmentation du VES.Les propriétés électrophysiologiques cardiaques sont également modifiées par l'activité physique, parmi celles-ci une bradycardie au repos est la plus connue. Ce phénomène résulte notamment d'une augmentation du tonus parasympathique et d'une diminution de la réponse à la stimulation adrénergique, neurotransmetteur du système nerveux sympathique.L'activité physique augmente également la variabilité de la fréquence cardiaque, c'est-à-dire la durée séparant deux contractions cardiaques consécutives.Enfin, la durée du potentiel d'action cardiaque est diminuée.Une forte capacité du système cardiorespiratoire — marqueur objectif du niveau d'activité physique — est associée à une diminution de 60 à 70 % du risque de maladie cardiovasculaire.Un mode de vie associé à une activité physique aérobie régulière augmente l'espérance de vie de trois à sept ans.L'exercice physique a presque toujours été présenté comme utile au maintien et à l'amélioration de la santé mentale.De nombreuses études ont confirmé des améliorations de l'humeur et de l'estime de soi cependant, dans les années 1990, le lien de causalité n'était pas clairement établi.Des chercheurs ont montré que la pratique de 20 à 40 minutes d'activité aérobique assez intense par jour peut diminuer l'anxiété et améliorer l'humeur pour plusieurs heures, mais ces changements sont transitoires, surviennent surtout chez les individus ayant un niveau normal ou élevé d'anxiété, et sont limités aux formes aérobies d'exercice,,.Les études ayant porté sur des programmes d'exercices à long terme, n'ont pas constaté d'améliorations de la santé mentale ou n'ont constaté que des effets modestes chez les individus « normaux ». Par contre des améliorations sont démontrées pour les personnes plus anxieuses et/ou dépressives et une pratiques a priori opposée (relaxation) peut avoir des bénéfices au moins comparables pour diminuer l'anxiété. Selon les données cliniques disponibles les avantages psychologiques apportés par l'exercice sont comparables aux gains obtenus avec les formes standard de psychothérapie.Inversement l'exercice physique uniquement pratiqué pour lui-même et de manière très intense ou compulsive, peut aussi nuire à la santé mentale, par exemple quand il entraine une dépendance excessive à l'activité physique ou sportive, alors souvent associée à des troubles de l'humeur et de la socialisation, une alimentation déséquilibrée, une anorexie ou boulimie, des comportements de dopage, voire à une détérioration de la santé physique. Chez l'athlète, le culturiste ou le joggeur un entraînement trop intense (surentraînement) peut induire une perturbation importante de l'humeur voire le syndrome de staleness (détérioration des performances physiques et intellectuelles et à des troubles du comportement, du sommeil, pouvant conduire à la dépression clinique), peut être en lien avec un dysfonctionnement du système hypothalamique. Une pratique de plus de dix heures de sport par semaine pourrait conduire à un surmenage de l'activité cérébrale.L'exercice pratiqué sans excès et dans un contexte positif a des effets bénéfiques, ou sinon peut nuire à la santé mentale. Pour un individu en bonne santé, le bénéfice principal pourrait être la prévention, alors que pour les personnes souffrant de troubles émotionnels légers à modérés l'exercice physique a une certaine valeur thérapeutique. Quand la pratique est collective, il peut contribuer à une resocialisation ou maintenir du lien social.Sa programmation (mode, durée, fréquence, intensité, variété, heures de pratiques, cadre…) influe sur la qualité des changements attendus en matière de santé mentale, autant d'aspects qui n'ont pas ou peu été étudiés, de même que les mécanismes biologiques et psychologiques reliant l'exercice à l'humeur et à la santé mentale.Le manque d'activité physique serait responsable d'un décès sur dix à travers le monde, soit presque autant que le tabac et l'obésité,.Une étude de l'ANSES en 2020 révèle que « 95% de la population française adulte est exposée à un risque de détérioration de la santé par manque d’activité physique ou un temps trop long passé assis ». Toujours selon cette enquête, 5% des adultes en France ont une activité physique suffisante pour protéger leur santé : les femmes sont plus exposées que les hommes à un manque d’activité physique. Plus d’un tiers des adultes français cumule un haut niveau de sédentarité et une activité physique insuffisante : en conséquence, ils sont plus exposés au risque d’hypertension ou d’obésité et ont un taux de mortalité et de morbidité plus élevés causés par des maladies cardiovasculaires et certains cancers.Certains handicaps et contextes psychologiques et socioculturels sont un frein à l'exercice physique.Les nuisances sonores le sont également : une étude récente a conclu qu'une exposition chronique au bruit des transports (bruit d'avions et de véhicules au sol) peut défavoriser la pratique de l’exercice régulier et ainsi nuire à leur santé,. La baisse d’activité physique est en Suisse de 3,2 % pour chaque point sur l'échelle de gêne due au bruit. Même de faibles niveaux de nuisances sonores ont été associés à une baisse d’activité physique.Chez la femme, une activité physique excessive peut avoir des conséquences sur le système hormonal (aménorrhée, ostéoporose et troubles de l'alimentation).Les myokines sont des cytokines, des substances solubles de signalisation cellulaire synthétisées par les myocytes, les cellules constitutives des muscles. Elles sont produites lors de l'activité physique.Inserm, Activité physique : Prévention et traitement des maladies chroniques, Montrouge, EDP Sciences, coll. « Expertise collective », 2019, 824 p. (ISBN 978-2-7598-2328-4, lire en ligne)Arnaud, P. et Terret, T., Histoire du sport féminin, tome 1 et 2, Paris, L’Harmattan, 1996.PNAPS prévention par activité physiqueRéseau suisse Santé et activité physique HEPA (it) (fr) (de)Manger Bouger PNNS INPESfemmes et sport, l'histoire d'un long combat Portail du sport   Portail de la médecine   Portail de la physiologie   Portail de l’éducation                  Cet article est partiellement ou en totalité issu de l'article intitulé « Exercice physique » (voir la liste des auteurs)."
sport;L'attaquant (ou avant) est un joueur de football dont la tâche principale est de concrétiser le jeu offensif de son équipe. Ce rôle décisif en fait l'un des postes les plus médiatisés dans ce sport.L'attaquant est placé à proximité des buts adverses et a de fait plus de possibilités pour marquer des buts que ses coéquipiers plus défensifs. Pour autant le rôle des attaquants peut varier en fonction de leurs caractéristiques, de l'organisation tactique et du jeu de leur équipe. Il s'agit en effet d'une catégorie qui regroupe plusieurs postes et, en second lieu, plusieurs profils.L'avant-centre (parfois appelé attaquant de pointe ou numéro 9), se positionne plutôt dans l'axe et a pour objectif premier de marquer des buts. Pour cette raison, l'avant-centre est parfois appelé « buteur » par abus de langage, alors que tous les joueurs sont susceptibles de marquer. Cette fonction est historiquement représentée par les joueurs arborant le numéro 9. Depuis les années 2000, l'avant-centre remplaçant en club préfère parfois porter le numéro 18, car la somme de 1 et 8 fait 9, plutôt qu'un numéro de remplaçant entre 12 et 15.Il existe plusieurs profils d'avant-centres, souvent en relation avec leur physique, leurs capacités techniques et leur style de jeu ainsi que leur jeu de tête.Certains sont des joueurs qui partent de loin, qui misent sur leur vitesse et leur conduite de balle pour transpercer la défense et inscrire un but. Ce profil de joueur profite souvent des balles qui lui sont données en profondeur et se révèle particulièrement redoutable face aux défenses placées très haut sur le terrain. En général, ils ont plus de difficultés à s'exprimer face aux défenses basses et regroupées. On peut citer comme exemples Ronaldo, Ruud Van Nistelrooy, Andreï Shevchenko ou encore Sergio Agüero.D'autres avant-centres passent la majorité de leur temps à rôder dans la surface de réparation, pour offrir des solutions à leurs milieux ou récupérer les ballons qui traînent. Ils font partie de cette catégorie d'attaquants nommés « renards des surfaces ». Ce type de joueur participe peu au jeu collectif de l'équipe et ne touche en général que très peu de ballons, se contentant surtout de son rôle de finisseur. Pour cela, il doit faire preuve d'un grand opportunisme et d'une grande efficacité devant les buts, les occasions étant rares. Les « renards des surfaces » misent surtout sur leur vivacité, leur rapidité gestuelle et leur capacité à se débarrasser du marquage adverse. Ce sont souvent des joueurs costauds capables de rivaliser physiquement avec les défenseurs, aussi bien sur les balles hautes que sur les balles au sol. Très dangereux dans les petits espaces et les défenses basses, ils ont plus de difficultés dans les défenses hautes car ce ne sont pas toujours des joueurs rapides ou ayant une bonne conduite de balle. Certains renards des surfaces, notamment en fin de carrière, peuvent parfois marquer moins de buts que les avant-centres rapides et techniques à l'échelle d'une saison mais ils sont les spécialistes pour marquer des buts décisifs dans les matches qui comptent le plus. Parmi les renards des surfaces les plus célèbres, les noms de Falcão, Trezeguet ou encore Pippo Inzaghi reviennent généralement. La saison 2006-2007 d'Inzaghi avec le Milan AC incarne à merveille le principe de renard des surfaces : remplaçant au profit d'Alberto Gilardino la majeure partie de la saison, il inscrit deux buts en finale de la Ligue des Champions, ce qui permet au Milan AC de remporter le trophée.Il existe un autre type d’attaquant de pointe : le « pivot ». Ce rôle est souvent dévolu aux joueurs particulièrement physiques, ne bénéficiant pas d'atouts techniques importants, et de préférence de grande taille. Ils ont pour fonction de servir de point d'appui en attaque pour les milieux, soit grâce à leur jeu de tête, soit par leur capacité à garder le ballon entre les pieds. À la réception des centres et des passes, ils servent à écarter les ballons vers des joueurs mieux placés qu'eux et donc plus susceptibles de marquer. Jouant dos au but, ils ont souvent moins d'opportunités pour marquer, mais doivent faire preuve de certaines qualités physiques et techniques pour contrôler les ballons et servir leurs coéquipiers. Ils sont souvent très surveillés par les défenseurs notamment à cause de leur jeu de tête, et permettent ainsi d'offrir des espaces à leurs coéquipiers. Olivier Giroud est souvent considéré comme l'incarnation de ce profil de joueur, notamment avec l'Équipe de France à la Coupe du Monde 2018 où il permet aux ailiers Antoine Griezmann et Kylian Mbappé (4 buts chacun) de s'illustrer tandis que lui ne marque aucun but.Idéalement, l'avant-centre est vif, opportuniste et bon en un-contre-un. S'il parvient à être altruiste quand c'est nécessaire, il s'agit d'un joueur très complet, pouvant devenir une arme dangereuse dans de nombreuses situations de jeu. Une certaine proportion d'échecs lui est pardonnée à la condition qu'il concrétise un nombre correct d'occasions.Il existe cependant d'autres attaquants qui ne jouent pas spécifiquement en pointe. Ces derniers sont utilisés dans les formations comportant deux attaquants et plus. En effet, l'avant-centre étant généralement un joueur à tendance individualiste, associer deux joueurs évoluant à ce poste peut nuire au jeu collectif.Au numéro 9 peut être associé un attaquant « de soutien » ou second attaquant, également appelé « neuf et demi », ou encore selon l'expression italienne consacrée « Trequartista ». Son rôle est d'aspirer les défenseurs pour démarquer l'avant-centre. C'est souvent un joueur très mobile, n'hésitant pas à s'excentrer, ou à revenir en arrière pour chercher les ballons. Il est en général bon dribbleur et plus passeur que buteur mais, du fait de son placement, doit tout de même savoir concrétiser les occasions. Il s'agit parfois d'un milieu offensif placé plus haut. Ce type de joueur est devenu très courant dans le football moderne car il permet de constituer une véritable alternative entre l'attaque et le milieu de terrain. Aujourd'hui, on peut citer comme exemple Joao Felix.Certains attaquants peuvent avoir une position désaxée sur le terrain, c'est par exemple le cas des ailiers dans les attaques à trois ou quatre joueurs. Leur rôle est alors de contourner par les côtés la défense adverse, afin d'adresser des centres devant le but aux attaquants axiaux. Ces derniers se chargent ensuite de concrétiser les passes et de les transformer en buts. Les meilleurs joueurs de l'Histoire ont souvent été placés dans cette position.Un autre profil d'ailier est celui de l'« attaquant intérieur », à l'instar de Cristiano Ronaldo ou encore Lionel Messi. Ces joueurs évoluent sur les côtés, mais leur objectif est généralement de repiquer dans l'axe afin de frapper dans le but. En effet, le couloir étant l'endroit sur le terrain où se trouvent le moins de joueurs, il est plus simple pour un attaquant d'éliminer un adversaire et de repiquer dans l'axe pour frapper au but. On ne peut pas pour autant parler d'un poste à part entière pour les attaquants intérieurs d'aujourd'hui, mais plutôt d'un type de joueur particulier, bien que jusque dans les années 1950, il existait un poste d’attaquant intérieur. Généralement appelé inter en français, et inside forward en anglais, il évoluait entre l'ailier et l'avant-centre dans les systèmes W-M et 2-3-5 pyramide.Dans cette perspective, une des appellations les plus ambiguës est celle de « faux numéro 9 ». Très souvent confondue avec un poste réel, cette dénomination doit en fait être employée pour qualifier un attaquant (souvent un avant-centre, mais pas obligatoirement) qui descend au milieu de terrain pour y récupérer le ballon et créer. Il s'agit généralement d'un joueur très bon techniquement (passes, dribbles, contrôle de balle) dont le but n'est plus uniquement de marquer, mais de créer des occasions pour lui et ses coéquipiers. Pour cela, donc, il ne reste plus cantonné à son poste d'attaquant et sa zone, mais peut jouer entre les lignes, sur les côtés, participer au jeu au milieu (et ainsi créer le surnombre)... L'intérêt principal réside dans le fait que les défenseurs adverses sont en face d'un dilemme : soit ils laissent le joueur quitter leur zone, s'exposant ainsi à un dribble ou une frappe lointaine s'il revient, soit ils le suivent, quitte à laisser derrière lui de la place exploitable par les coéquipiers de l'attaquant. Les attaquants Dries Mertens, Karim Benzema, Wayne Rooney et Lionel Messi illustrent parfaitement la réussite de ce style de joueur dans le football moderne. Néanmoins, par extension, cette appellation est également employée dans le cadre d'un dispositif tactique particulier : celui d'une équipe évoluant sans attaquant de formation. Le terme de « faux 9 » sert alors à désigner le ou les milieux de terrain qui deviennent attaquants lors des phases offensives. Ce système de jeu a notamment été mis en place par le sélectionneur espagnol Vicente del Bosque lors de l'Euro 2012, faisant évoluer le milieu de terrain Cesc Fàbregas dans cette configuration particulière du faux 9ou encore Pep Guardiola avec Manchester City lors de la saison 2020-2021.Jean-Michel Larqué et Henri Cettour, Football: Ses règles, son langage, son organisation, 1988.  (ISBN 978-2851823588).(it) Fulvio Damele, Calcio da manuale, 1998.  (ISBN 978-8844006709).Claude Doucet, Football : perfectionnement tactique, 2005.  (ISBN 978-2851806765).Benoît Meyer, Football : le ballon rond dans tous les sens, 2012.  (ISBN 978-2745323965).Dispositifs tactiques en footballPostes du football moderne Portail du football
sport;Une balle de cricket est de forme sphérique, et est faite de liège dur recouvert de cuir. Les règles qui précisent ses caractéristiques et son utilisation sont définies par la quatrième loi du cricket.Les dimensions et la masse de la balle de cricket dépendent de la catégorie des joueurs. Pour un match impliquant des équipes sénior masculines, la balle doit peser, neuve, entre 155,9 et 163 grammes (entre 51?2 et 53?4 onces), et sa circonférence doit être comprise entre 22,4 et 22,9 centimètres (entre 813?16 et 9 pouces), soit un diamètre compris entre 7,13 et 7,29 centimètres. Pour une rencontre entre équipes féminines, elle doit peser de 140 à 151 grammes (de 415?16 à 55?16 d'onces) et avoir une circonférence de 21 à 22,5 centimètres (de 81?4 à 87?8 de pouces). Pour un match entre des juniors (joueurs de moins de 13 ans), elle doit peser de 133 à 144 grammes (de 411?16 à 51?16 d'onces) et avoir une circonférence comprise entre 20,5 et 22,0 centimètres (de 81?16 à 811?16 de pouces).La balle de cricket est faite de liège dur recouvert de cuir. Le cuir est séparé en deux hémisphères liés entre eux par une couture. La balle est traditionnellement de couleur rouge, avec une couture blanche. Des balles de couleur blanche sont utilisées pour les matchs de limited overs cricket, qui se déroulent, pour certains, en partie en soirée. C'est un héritage de la World Series Cricket, une compétition rebelle organisée entre 1977 et 1979, qui a introduit ces matchs joués en partie en soirée et une couleur de balle bien visible à la lumière des projecteurs. Une brève expérimentation avait été menée au XIXe siècle pour introduire des balles bleues pour les matchs de cricket féminin, le rouge ayant été supposé trop choquant pour les femmes.Une nouvelle balle peut être réclamée par chacun des capitaines au début de chaque manche. Pour un match d'une durée de plus d'un jour, le capitaine de l'équipe au fielding peut réclamer une nouvelle balle lorsqu'un minimum d'overs a été joué avec la balle précédente. Ce nombre d'overs minimum, fixé par l'instance dirigeante du pays où se déroule le match, ne doit pas être inférieur à 80, soit 480 lancers.Depuis le 2011, en format One-day International, deux balles neuves sont utilisées durant chaque manche, chacune attachée à une extrémité du pitch. Chaque balle est donc utilisée durant 25 overs.La 41e loi du cricket précise des actions licites ou illicites qu'un joueur pourrait effectuer sur la balle en cours de match.N'importe quel fielder a le droit de polir la balle à condition qu'aucune substance artificielle ne soit utilisée. Il peut aussi enlever la boue présente sur la balle, et la sécher avec une serviette. Tout autre action visant à modifier les propriétés de la balle, par exemple en la frottant sur le sol, est interdite.Batte de cricketLexique du cricketLanceur Portail du cricket
sport;Un but, au hockey sur glace, donne un point à l'équipe qui le marque. Le but est marqué lorsque la rondelle (ou palet) franchit complètement la ligne de but entre les poteaux.La finalité d'un match de hockey est de marquer plus de buts que l'équipe adverse. Le gardien de but et les défenseurs ont pour principal rôle d'empêcher l'équipe adverse de marquer alors que les attaquants essaient prioritairement de marquer un but à cette même équipe. Bien évidemment, les attaquants ont aussi un rôle défensif à jouer et les défenseurs, un rôle offensif.Pour qu'un but soit marqué, la rondelle doit franchir entièrement - et d'une seule pièce - la ligne située entre les poteaux et sous la barre transversale.Un but ne compte pas si le tir est fait avec la crosse (ou bâton) levée au-dessus de la barre transversale, ou si le but est marqué sur une action volontaire du pied ou de la main.Un but dévié accidentellement par un joueur est validé, en revanche s'il est dévié par un arbitre, il est refusé.Aucun but ne peut être accordé si la cage n'est plus dans son socle (ou en place).Si le gardien est empêché de défendre son filet par un attaquant adverse, le but est refusé, et une pénalité d'obstruction est appelée. Il peut aussi être refusé s'il est marqué par une équipe qui a trop de joueurs sur la glace ou s'il est marqué avec un bâton cassé.Si un joueur touche la rondelle avant qu'elle entre dans son propre filet, ce qui au football association (soccer) est appelé but contre son camp, le but est accordé au dernier joueur ayant touché la rondelle de l'équipe qui a marqué, ou à défaut au joueur le plus proche du but.Dans de nombreuses ligues ainsi que dans le règlement international (IIHF), un but n'est pas validé si un joueur a un patin ou son bâton dans le territoire de but avant que la rondelle ne rentre.La Ligue nationale de hockey a supprimé cette règle après le but en triple prolongation au cours de la finale de la Coupe Stanley de 1999. Brett Hull des Stars de Dallas marqua le but qui mit fin à la série face aux Sabres de Buffalo. À la vidéo, il est clair que le patin du joueur est entré dans l'enclave avant la rondelle. La LNH justifia le but en disant que Hull avait marqué sur son propre rebond, gardant la possession et le contrôle de la rondelle en tout temps, lui donnant le droit de pénétrer dans l'enclave.Le juge de but est un officiel qui est placé derrière chaque filet et dont la tâche spécifique est d'indiquer quand la rondelle a franchi la ligne de but. Dans les patinoires qui en sont équipées, le juge allume une lumière rouge lorsqu'il voit la rondelle franchir la ligne. Dans tous les cas, l'arbitre principal reste le décideur final et peut annuler la décision du juge de but.Le nombre de buts marqués est l'une des statistiques les plus regardées.Chaque année, le trophée Maurice-Richard est donné au joueur de la LNH ayant marqué le plus de buts. Ce trophée est nommé ainsi en l'honneur de Maurice Richard, le premier joueur de l'histoire à avoir inscrit 50 buts en une saison au temps où la saison régulière de la LNH ne comptait que 50 matchs (comparativement à 82 aujourd'hui).Le joueur ayant marqué le plus de buts en une saison de LNH est Wayne Gretzky. Gretzky est aussi le joueur ayant marqué 50 buts le plus rapidement, ce qu'il fit au cours de la saison 1981-1982 qu'il termina avec 92 buts, il marqua son 50e but au cours du 39e match de la saison des Oilers d'Edmonton.Le nombre total de buts marqué dans la saison est aussi attentivement suivi. Au cours des dernières saisons, ce nombre a baissé. Beaucoup y ont vu une baisse d'intérêt et ont mis en cause l’accroissement de la taille de l'équipement des gardiens et l'apparition de systèmes de jeu défensifs tels que la « neutral zone trap ». Les fans du hockey défensifs, quant à eux, trouvaient que le nombre de buts marqués lors des années 1980 était anormal et que ce n'était qu'un retour à la normale.Lors de la saison 2004-05 de la Ligue américaine de hockey, quatre règles majeures furent changées afin d'augmenter le nombre de buts dans les matchs et de rendre ce sport plus attirant aux yeux des amateurs occasionnels:Augmentation des zones offensives en réduisant la zone neutre de 2 pieds (30,96 cm) de chaque côté et en reculant la ligne de but de 2 pieds vers l'arrière.Restrictions de jeu avec la rondelle pour les gardiens.Permission pour les joueurs hors jeu d'annuler la pénalité en revenant dans la zone neutre.Permission de faire franchir à la rondelle une ligne bleue et la ligne centrale en une foisCes règles furent ensuite adoptées dans l'ECHL et la LNH lors de la saison 2005-2006.Les buts sont considérés différemment pour les statistiques mais comptent de la même manière au cours des matchs:But à égalité numérique : lorsque les deux équipes ont le même nombre de joueurs sur la glace.But en supériorité numérique : lorsqu'une équipe joue avec plus de joueurs à cause d'une ou plusieurs pénalités que l'équipe adverse a subie(s).But en désavantage numérique : lorsqu'une équipe ayant moins de joueurs sur la glace à cause d'une ou plusieurs pénalités réussit à inscrire un but.But dans un filet désert : lorsqu'une équipe qui a fait entrer un attaquant supplémentaire à la place de son gardien prend un but.But de pénalité : lorsqu'un but est inscrit lors d'un tir de pénalité. C'est une confrontation en un contre un entre le gardien et le joueur en raison d'une pénalité.But automatique : cas particulier du tir de pénalité. Si un joueur est victime d'une faute en situation d'échappée face aux buts rendus vides par la sortie du gardien, l'arbitre accorde directement le but à l'équipe attaquante.But en prolongation : lorsque le but est inscrit en temps supplémentaire et met fin au match.Go-ahead goal : but qui donne l'avantage à une équipe alors que le score était à égalité.But égalisateur : but qui permet à une équipe d'égaliser le score.But gagnant : but qui donne la victoire à une équipe.Autres termes employés:« garbage goal » ou « but rebut »: but marqué plus par chance ou opportunisme que par talentBut en échappée : but marqué par un joueur qui s'est glissé derrière les défenseurs pour affronter le gardien seul.Un « quinella » survient lorsqu'un joueur, au cours de la même partie marque un but en égalité numérique, un en supériorité numérique, un en infériorité numérique, un but de pénalité et un but dans un filet désert.Mario Lemieux est le seul joueur de l'histoire de la LNH à avoir réussi cet exploit contre les Devils du New Jersey. On y fait souvent référence comme « 5 buts, 5 différentes façons ».Une rondelle lancée qui atteint le gardien est compté comme un tir. Chaque tir stoppé par le gardien est compté comme un arrêt.Les deux coéquipiers (un seul en Extraliga) qui ont touché la rondelle avant le buteur sont crédités d'une aide.Quand un joueur marque 3 buts dans le même match on parle de tour du chapeau ou coup du chapeau. S'ils sont marqués consécutivement, il s'agit d'un tour du chapeau naturel.Le hockey sur glace est un des rares sports dans lequel une sirène retentit lors d'un but. C'est particulièrement vrai en LNH où la sirène retentit après chaque but de l'équipe hôte. Les sirènes sont différentes suivant les équipes, certaines ont même des effets sonores telles une alarme de police ou une corne de brume de bateau, ou même les deux combinés, comme les Capitals de Washington.NHL Rulebook, Rule #57 - Goals and AssistsJoueurs de la LNH avec 500 buts50 buts en 50 matchs Portail du hockey sur glace
sport;La Charte olympique est un ensemble de règles et de lignes directrices qui définit les principes fondamentaux pour l'organisation des Jeux olympiques et pour diriger le mouvement olympique.La Charte est publiée dès 1908 sous le nom Comité International Olympique - Annuaire, et reprend des règles écrites par Pierre de Coubertin en 1899. La Charte olympique est connue sous son nom actuel depuis 1978. La version actuelle date du 26 juin 2019.La Charte olympique est composée de six chapitres et de soixante et un articles.Le chapitre 1 définit le Mouvement olympique, mission et rôle du CIO y compris la non-discrimination, égalité entre les hommes et les femmes, le développement durable. Ses trois principales parties constitutives sont le CIO, les fédérations internationales et les Comités nationaux olympiques (art. 2).Le chapitre 2 décrit le Comité international olympique comme une « organisation internationale non gouvernementale à but non lucratif dotée de la personnalité juridique » dont le siège est à Lausanne, en Suisse et ayant pour but de « remplir la mission, le rôle et les responsabilités que lui assigne la Charte olympique » (art. 15).Le chapitre 3 décrit la mission et le rôle des fédérations internationales. Ce sont des « organisations internationales non gouvernementales qui administrent un ou plusieurs sports à l'échelle mondiale et qui comprennent des organisations administrant ces sports au niveau national » (art. 25).Le chapitre 4 définit les Comités nationaux olympiques. Leur mission est de « développer, protéger et promouvoir le Mouvement olympique dans leurs pays respectifs, conformément à la Charte olympique » (art. 27).Le chapitre 5 décrit les règles des Jeux olympiques. Cela inclut la célébration des Jeux, l'élection de la ville hôte, le comité d'organisation, le code d'admission aux Jeux olympiques, le programme des Jeux et le protocole olympique (symboles et cérémonies).Le dernier chapitre décrit les mesures et sanctions en cas de violation de la Charte olympique.Comité international olympiqueJeux olympiquesCharte olympique - Comité international olympique, 26 juin 2019 [PDF].Site officiel du mouvement olympique Portail des Jeux olympiques
sport;"Le Comité international olympique ou CIO (en anglais, International Olympic Committee ou IOC) est une organisation créée par Pierre de Coubertin en 1894, pour réinstaurer les Jeux olympiques antiques puis organiser cet événement sportif tous les quatre ans, puis en alternant tous les deux ans à partir de 1994, Jeux olympiques d'été et Jeux olympiques d'hiver.Depuis 1981, c'est une organisation internationale non gouvernementale à but non lucratif dont le siège est à Lausanne selon la Charte olympique. Cette association est dotée de la personnalité juridique à durée illimitée et son statut est reconnu par la Confédération suisse par arrêté du Conseil fédéral du 17 septembre 1981. Depuis le 10 septembre 2013, le neuvième président du CIO est Thomas Bach. Il est réélu le 10 mars 2021 pour un second et dernier mandat de quatre ans. 127 ans après la fondation de l'institution et l'adoption de la devise proposée par Henri Didon, celle-ci est modifiée lors de la 138e session du CIO : elle devient « Plus vite, Plus haut, Plus fort - Ensemble ».À Paris, le 23 juin 1894, en clôture du premier congrès olympique, le baron Pierre de Coubertin fonde le Comité international olympique afin de faire revivre les anciens Jeux olympiques après une absence de plus de 1500 ans. Il veut ainsi contribuer à bâtir un monde pacifique au moyen du sport en promouvant la communication, le fair-play et l'entente entre les peuples. Le CIO est une organisation dont le but est de localiser l'administration et l'autorité pour les jeux, ainsi que de fournir une seule entité légale qui détient tous les droits et les marques. Par exemple le logo olympique, le drapeau, la devise et l'hymne olympique sont tous administrés et possédés par le CIO. Le président du Comité olympique représente le CIO dans son ensemble, et les membres du CIO le représentent dans leurs pays respectifs.Ayant pour principe immuable une totale neutralité politique, le CIO prend une série de décisions historiques en février 2022 à la suite de l'invasion de l'Ukraine par la Russie. Il s'élève en effet contre la violation de la trêve olympique et demande aux fédérations sportives internationales d'exclure les athlètes et officiels russes et biélorusses de la totalité de leurs compétitions. Alors que jusqu'alors et compte tenu de la suspension de la Russie pour cause de dopage institutionnel, les sportifs de ce pays pouvaient concourir aux Jeux d'été et d'hiver sous bannière neutre, l'institution lausannoise prend position pour l'Ukraine et sa « communauté olympique », une démarche qu'elle n'avait encore jamais empruntée dans son histoire.Le manuscrit original du manifeste, texte fondateur et ayant servi au discours de Pierre de Coubertin du 25 novembre 1892, avait été vendu aux enchères chez Sotheby's à New York, le 18 décembre 2019. Il avait été acquis par le milliardaire russe Alicher Ousmanov pour un montant record de 8,8 millions de dollars. Le 10 février 2020, le Comité international olympique a annoncé que l'acquéreur lui a offert le manuscrit, tout en souhaitant que celui-ci soit exposé au musée olympique de Lausanne. Le CIO est composé de 115 membres qui se réunissent au moins une fois par an, et élisent un président pour une durée de 8 ans. Les membres sont tous des personnes physiques. Le CIO comprend notamment parmi ses membres des athlètes actifs, d'anciens athlètes ainsi que des présidents ou dirigeants au plus haut niveau de fédérations internationales de sport, d'organisations internationales reconnues par le CIO. Le CIO recrute et élit ses membres parmi les personnalités qu'il juge qualifiées. Les moyens financiers proviennent d'une part des droits de retransmission télévisée et d'autre part des partenariats avec des sociétés multinationales. La présidence de Juan Antonio Samaranch (1980 à 2001) a vu une explosion des droits télévisés et de parrainage des Jeux. En décembre 1998, éclate le scandale de corruption qui a entouré la désignation de Salt Lake City comme ville organisatrice des Jeux olympiques d'hiver de 2002.Le CIO, autorité suprême du mouvement olympique, désigne lors de sa session annuelle les villes hôtes pour les Jeux olympiques d'été comme d'hiver. L'élection de la ville hôte requiert la majorité absolue des suffrages exprimés. Si, à un tour donné, aucune ville n'obtient la majorité absolue des suffrages exprimés, un nouveau tour est effectué en éliminant la ville ayant reçu le moins de voix. Lors du dernier tour s'il y a lieu, les deux finalistes sont convoqués pour assister au résultat final du vote. Le vote est secret et n'ont pas le droit de vote les membres du CIO ayant la même nationalité qu'une ville encore en lice. Le CIO se réunit également en congrès exceptionnels.Chargé d'organiser les Jeux olympiques, il en délègue l'organisation matérielle à un comité local d'organisation des jeux olympiques (COJO) et l'organisation technique des épreuves retenues au programme de chaque olympiade aux fédérations internationales compétentes. Il gère les Jeux olympiques d'été depuis 1896, ceux d'hiver depuis 1924 et depuis 2010 les Jeux olympiques de la jeunesse organisés pour la première fois à Singapour. Ses membres sont également impliqués dans l'organisation des Jeux régionaux ou continentaux, reconnus par le CIO et gérés par des comités olympiques ad hoc : jeux asiatiques, jeux africains, jeux panaméricains, jeux méditerranéens, jeux du Pacifique.Depuis 2001, le CIO dispose du Service olympique de radiotélévision (Olympic Broadcasting Service), une agence capable de retransmettre les épreuves sans forcément passer par un diffuseur partenaire. Son siège est situé à Madrid. Pour les Jeux olympiques de 2020, le CIO souhaite prolonger le mouvement en créant une chaîne olympique (qui sera plus un catalogue de contenus à la demande).Depuis sa création en 1894 jusqu'au 10 avril 1915, le siège du CIO est à Paris. À cette date, Coubertin décide de le mettre à l'abri des hostilités en le localisant en Suisse à Lausanne. Le CIO s'installe d'abord au casino de Montbenon de 1915 à 1922 puis à la villa Mon-Repos de 1922 à 1968 et depuis 1968, son siège principal est le château de Vidy sur les rives du Léman. En 1986, le CIO inaugure la Maison olympique. Pour la première fois de son histoire, celui-ci possède un bâtiment qui centralise l'essentiel de ses activités dans le monde. Le musée olympique, fondé sous l'impulsion de Juan Antonio Samaranch, est inauguré le 23 juin 1993. Il est situé au bord du Léman sur le quai d'Ouchy. C'est le deuxième musée le plus visité de Suisse et il reçoit le prix du musée européen de l'année en 1995.Coubertin veut faire de l'Olympisme une véritable religion laïque. Aussi celui-ci n'échappe-t-il pas à une véritable liturgie marquée par des symboles forts qui se sont établis au fil des Jeux et qui sont actuellement des marques protégées contre tout usage illicite. Sont successivement apparus : la devise olympique, le credo olympique, les anneaux olympiques, le serment olympique, la flamme olympique, le relais olympique et l'hymne olympique.Citius-Altius-Fortius : expression latine signifiant plus vite, plus haut, plus fort. Coubertin l'emprunte au père Henri Didon qui utilise la formule Citius-Fortius-Altius — exprimée pour la première fois le 7 mars 1891 — pour décrire le parcours éducatif du collège Albert-le-Grand, à Arcueil, dont il est recteur : plus vite (athlétiquement), plus fort (intellectuellement et mentalement), plus haut (spirituellement). Cette expression latine est gravée dans la pierre au-dessus de l'entrée principale de l'établissement et reproduite sous cette forme au frontispice des premières Revue olympique avant de prendre sa forme actuelle.« Le plus important aux Jeux olympiques n'est pas de gagner mais de participer, car l'important dans la vie ce n'est point le triomphe mais le combat ; l'essentiel, ce n'est pas d'avoir vaincu mais de s'être bien battu ». Il s'agit de la forme actuelle du credo tel qu'il apparaît sur le panneau d'affichage à la cérémonie d'ouverture des Jeux olympiques. Pierre de Coubertin a repris puis adopté ce credo après avoir entendu le sermon de l'évêque de Pennsylvanie, Ethelbert Talbot (en), prononcé à la cathédrale Saint-Paul le 19 juillet 1908, au cours des Jeux de la IVe Olympiade à Londres. Les paroles exactes de Talbot sont : « L'important dans ces Olympiades n'est pas tant d'y gagner que d'y prendre part ».Conçu par Pierre de Coubertin lui-même en 1913, le drapeau olympique est présenté officiellement en juin 1914 au congrès de Paris. Mais du fait de la Grande Guerre, il ne flotte pour la première fois qu'aux Jeux d’Anvers en 1920. Les 5 anneaux entrelacés représentent les cinq continents réunis par l’olympisme et les six couleurs (en comptant le blanc du fond) rappellent les drapeaux de toutes les nations car au moins une d'elles se retrouve dans celui de celles présentes à la création des Jeux en 1896. Ce symbole est donc celui de l’universalité de l’esprit olympique. Depuis, une opinion courante mais démentie par le CIO, associe un continent à chaque couleur des anneaux (le bleu représentant l'Europe, le noir l'Afrique, le jaune l'Asie, le vert l'Océanie et le rouge l'Amérique).« Au nom de tous les concurrents, je promets que nous prendrons part à ces Jeux olympiques en respectant et suivant les règles qui les régissent, dans un esprit de sportivité, pour la gloire du sport et l'honneur de nos équipes ». Écrit par Coubertin, ce serment est prononcé par un athlète du pays hôte tenant le pan du drapeau olympique de sa main gauche. C'est en 1920 à Anvers que l'escrimeur belge Victor Boin prononce le serment olympique pour la première fois. Depuis, un juge et un entraîneur du pays hôte prononcent également chacun un serment dont l'énoncé est légèrement différent.La flamme olympique est un symbole qui nous vient des Jeux olympiques de l'antiquité au cours desquels une flamme sacrée brûle en permanence sur l'autel de Zeus. La flamme est allumée pour la première fois aux Jeux de la IXe Olympiade en 1928 à Amsterdam puis à nouveau pendant les Jeux de la Xe Olympiade en 1932 à Los Angeles. La flamme est allumée — par des femmes vêtues de tuniques similaires à celles portées par les Grecs de l'antiquité — au cours d'une cérémonie dans l'antique stade olympique d'Olympie dans la région grecque du Péloponnèse. La flamme est allumée naturellement par les rayons du soleil d'Olympie, réfléchis à l'aide d'un miroir parabolique. La grande prêtresse remet ensuite le flambeau au premier relayeur.En 1936, Carl Diem, président du comité d'organisation des Jeux de la XIe Olympiade à Berlin, propose d'allumer la flamme en ancienne Olympie et de la transporter jusqu'à Berlin via un relais du flambeau. Son idée est adoptée et la tradition se perpétue depuis lors.Cantate de Kostís Palamás mise en musique par Spýros Samáras en 1896, l'hymne olympique est joué pour la première fois à l'occasion de la première olympiade. Cependant, il n'est adopté comme hymne olympique officiel par le CIO qu'en 1957.Le président représente le CIO et préside toutes ses activités. Il est élu par la session au scrutin secret. Autrefois illimitée, la durée du mandat présidentiel est fixée à huit ans depuis le 12 décembre 1999, renouvelable une fois pour quatre ans. L’article 20 de la Charte olympique définit le rôle du président, notamment sa fonction de représentation.En septembre 2013, l'Allemand Thomas Bach devient le neuvième président du CIO. Après un premier mandat de huit ans, il est réélu pour un deuxième mandat de quatre ans le 10 mars 2021,,. Liste des présidents 1894-1896 :  Dimítrios Vikélas ;1896-1925 :  Baron Pierre de Coubertin ;1925-1942 :  Comte Henri de Baillet-Latour ;1946-1952 :  Sigfrid Edström ;1952-1972 :  Avery Brundage ;1972-1980 :  Michael Morris, lord Killanin ;1980-2001 :  Juan Antonio Samaranch ;2001-2013 :  Comte Jacques Rogge ;Depuis 2013 :  Thomas Bach.Ils sont au nombre de quatre, élus pour un mandat de quatre ans. Ils ne peuvent exercer que deux mandats consécutifs et doivent attendre ensuite deux ans pour être à nouveau éligibles. Yu Zaiqing Juan Antonio Samaranch i Salisachs U?ur Erdener Anita DeFrantzLe CIO étant une organisation non gouvernementale ses membres, choisis par cooptation par le reste du comité, ne représentent aucun des pays dont ils sont ressortissants. Ce fonctionnement atypique, souvent mal compris, est assimilé à un manque de transparence dans le fonctionnement. Sont le plus souvent visés :le choix des membres, ressenti comme le fait du prince ou de réseaux plus ou moins occultes. En outre le passé de quelques-uns d'entre eux n'est pas toujours des plus clairs. Certains — dont l'ancien président Avery Brundage — restent suspects de sympathies avec le régime nazi avant la guerre alors que l'ancien président Juan Antonio Samaranch est assurément secrétaire des sports du régime de Franco en 1967 ;la désignation des villes olympiques par vote à bulletins secrets. Pour exemple à propos du choix du lieu des Jeux olympiques d'hiver de 2014, le chancelier autrichien, Alfred Gusenbauer, a déclaré : « Si c'est une question de pouvoir politique et de gros sous, alors Salzbourg n'avait aucune chance. Je suis persuadé que le concept que nous présentions était absolument le meilleur. » ;la gestion des fonds. Des critiques sont émises tant sur d'éventuelles compromissions avec les sponsors qui semblent parfois dicter le programme même des Jeux que sur l'usage qui est fait de l'argent récolté. Pour la période 2001-2004 le mouvement olympique a généré un revenu de plus de quatre milliards de dollars.Depuis Albertville en 1992, la participation des athlètes des pays tropicaux aux Jeux d'hiver semble impactée par l'instauration de minima drastiques. L'argument du CIO « les Jeux olympiques d'hiver sont une manifestation quelque peu particulière, étant donné que, pour des raisons tout simplement climatiques et géographiques, ils ne conviennent pas à certains pays du monde, que ce soit au niveau de l'organisation ou de la participation » ne semble pas satisfaire tous les détracteurs.Le choix de Pékin pour les Jeux olympiques de 2008 qui a entraîné des expulsions massives de populations (1,5 million selon l'ONG COHRE) a été vigoureusement critiqué par les associations de défense des droits de l'homme, et entraîné dans certains pays des manifestations spectaculaires lors de leur traversée par le relais de la flamme.En 2004, un reportage de la chaîne britannique BBC montre que certains membres du CIO, dont Ivan Slavkov, président du comité olympique bulgare, sont prêts à monnayer leur soutien lors de divers votes. D'autres soupçons de corruption ont été établis, notamment lors des Jeux de Salt Lake City et de Sotchi. Le CIO a pris des mesures disciplinaires dans les cas avérés.En juillet 2019, l’ex-gouverneur de Rio, Sérgio Cabral Filho, reconnaît avoir payé des pots-de-vin à des délégués du Comité international olympique pour décrocher l’organisation de la compétition en 2016.Début 2021, le CIO annonce vouloir diminuer de 30 % ses émissions carbone d'ici à 2024 et de 45 % d'ici à l'horizon 2030. Il indique ainsi que chaque futur comité d'organisation des JO devra intégrer la dimension écologique à ses programmes et donc minimiser l'impact écologique. Thomas Bach justifie cet engagement : « Cet objectif ambitieux permet au CIO de se conformer à l’accord de Paris ».Jean Durry, Le Vrai Pierre de Coubertin, Paris, Comité français Pierre de Coubertin, 1997.Luis Fernandez, La Faillite du sport français : face aux 7 faillites du sport français, le bon sens !, Communauté européenne, Rue du sport, 2011, 123 p. (ISBN 978-2-84653-045-3 et 2-84653-045-9).Stefan Huebner, Pan-Asian Sports and the Emergence of Modern Asia, 1913-1974. Singapour: NUS Press, 2016 (le CIO et le sport dans l'Asie).Résultat des élections des villes hôtes des Jeux olympiques d'étéRésultat des élections des villes hôtes des Jeux olympiques d'hiverListe des membres du Comité international olympiqueComité national olympiqueListe des codes pays du CIOProcédure de sélection de la ville hôte des Jeux olympiquesAcadémie internationale des sciences et techniques du sportMusée olympiqueAndrew Jennings (journaliste d'investigation - enquête sur la corruption au sein du CIO)Ressource relative aux organisations : Registre de transparence de l'UE Ressource relative à la recherche : CrossRef Site du Comité international olympique(en) Archives complètes des rapports du CIO de 1896 à 2002 Portail des Jeux olympiques   Portail de Paris   Portail de Lausanne"
sport;Le cricket est un sport collectif de balle et de batte opposant deux équipes composées normalement de onze joueurs chacune. Il se joue généralement sur un terrain de forme ovale, en herbe, au centre duquel se trouve une zone d'une vingtaine de mètres de longueur, à chaque extrémité de laquelle on trouve une structure de bois, le guichet. Une rencontre est divisée en plusieurs manches. Au cours de chacune d'entre elles, l'une des équipes essaye de marquer des points (courses), et possède simultanément deux batteurs sur le terrain, chacun devant l'un des guichets. Un point est notamment marqué à chaque échange de position de ces deux joueurs lorsque la balle est en jeu. Leurs onze adversaires sont également présents sur l'aire de jeu. La balle est lancée par l'un de ceux-ci en direction du guichet d'un des deux batteurs. L'objectif de la seconde équipe est d'empêcher la première de marquer, principalement en éliminant les batteurs adverses, par exemple en détruisant le guichet avec la balle sur le lancer.Plus de cent pays sont affiliés à l'International Cricket Council, qui organise notamment la Coupe du monde de cricket. De par son histoire, c'est dans les nations de l'ancien Empire britannique que le cricket est le plus populaire.Les origines du cricket sont obscures. Le prince Édouard, futur Édouard II d'Angleterre, pratique en 1300 le « creag et d'autres jeux », mais rien ne prouve que ce creag soit l'ancêtre du cricket. L'une des principales théories sur ses origines indique qu'il a évolué à partir d'un passe-temps d'enfants, dans le sud-est de l'Angleterre. Un poème attribué à John Skelton et écrit de manière présumée en 1533 suggère une origine flamande et une pratique originelle par des bergers.C'est en France que l'on trouve la plus ancienne trace mondiale liée au cricket (1478) dans une lettre de doléance adressée au roi Louis XI qui mentionne une dispute liée à ce jeu dans le village de Liettres, près de Saint-Omer. Un tournoi célèbre cet héritage tous les ans, le Liettres Challenge 1478. L'auteur Jean-Jules Jusserand mentionne dans Les Sports et jeux d'exercice dans l'ancienne France (1901) la référence de Liettres en 1478 et stipule que le cricket n'est autre chose qu'une variété du jeu de crosse ou de soule à la crosse. La première référence avérée au cricket en Angleterre date de 1597 : le médecin légiste John Derrick témoigne au cours d'un procès que ses amis et lui ont joué au « creckett » alors qu'il étudiait à la Royal Grammar School de Guildford, dans le Surrey, aux environs de 1550.Il existe plusieurs théories quant à l'origine du mot « cricket ». Étant donné qu'il existe au Moyen Âge de nombreux échanges entre le sud-est de l'Angleterre et le comté de Flandres, il pourrait venir du moyen néerlandais krick (bâton). Une autre possibilité est l'Anglo-Saxon cricc ou cryce (béquille, bâton). Samuel Johnson fait dériver cricket de cryce dans son Dictionary of the English Language (1755). En ancien français, criquet désignait un bâton de but au jeu de boule, massue. Le moyen néerlandais krickstoel désigne un tabouret utilisé dans les églises pour s'agenouiller, et dont la forme rappelle celle des premiers guichets. Le linguiste allemand Heiner Gillmeister soutient que l'origine du mot cricket est une expression de moyen néerlandais désignant le hockey, met de krik ketsen.La première tournée internationale jamais organisée a été annulée pour des raisons politiques. En 1789, l'ancien ambassadeur du Royaume-Uni en France, John Sackville, 3e duc de Dorset, prévoit une tournée en France. Les joueurs se rassemblent à Douvres, où ils croisent le duc, qui fuit la Révolution française, et n'iront pas plus loin. Le premier match international de l'histoire oppose, le 24 et le 26 septembre 1844, des joueurs américains et des joueurs canadiens à Bloomingdale Park à Manhattan. Il est annoncé comme un affrontement entre États-Unis et Canada alors que les joueurs proviennent majoritairement de deux clubs. Le Canada remporte cette opposition par 23 runs. Cinq mille personnes assistent au premier jour de jeu, qui fut l'occasion d'importants paris. Les équipes se retrouvent deux fois l'année suivante, à Montréal en juillet et à New York le mois suivant, pour deux victoires canadiennes, puis une fois en 1846, à Harlem, où le match s'achève quand les Canadiens abandonnent la partie, un joueur américain ayant jeté la balle sur le batteur canadien qui l'avait chargé pour l'empêcher d'attraper la balle au vol. À la suite de cet incident, il s'écoulera sept ans avant que les deux équipes ne se retrouvent.En septembre 1859, une sélection de douze joueurs professionnels anglais embarque pour les États-Unis. Plusieurs matchs sont organisés et opposent systématiquement onze Anglais à vingt-deux Américains. Si la Guerre de Sécession marque le déclin de la popularité du cricket aux États-Unis au profit du baseball, des équipes compétitives se développeront dans certaines villes, notamment à Philadelphie. Les équipes d'Angleterre et d'Australie effectueront occasionnellement des tournées en Amérique du Nord jusque dans les années 1920 et l'équipe de Philadelphie se déplacera plusieurs fois en Angleterre.Avec la guerre civile aux États-Unis, les organisateurs de tournées anglaises portent leur attention sur l'Australie. En 1861-62, Heathfield Stephenson mène la première équipe anglaise en tournée en Australie, et d'autres feront le même chemin dans les années qui suivent. Ces tournées privées, qui impliquent des joueurs professionnels, ont pour vocation de faire du profit. La tournée menée par Stephenson rapporte ainsi 10 000 £ de l'époque. La première équipeaustralienne à effectuer une tournée en Angleterre est composée exclusivement de joueurs aborigènes, en 1868. Elle dispute quarante-sept matchs sur le sol britannique, et effectue des démonstrations de lancer de boomerang et de lance. L'équipe a pour entraîneur Tom Wills, joueur de cricket qui a par ailleurs instauré en 1858 les premières règles du football australien.En 1876-77, James Lillywhite emmène une équipe entièrement professionnelle en Nouvelle-Zélande puis sur le sol australien. Le 15 mars 1877 débute un match organisé sous l'appellation All England v A Combined New South Wales and Victoria XI et oppose l'équipe de Lillywhite à une sélection de joueurs de Nouvelle-Galles du Sud et de Victoria. Remportée par les Australiens le 19 mars, cette rencontre sera rétrospectivement considérée comme étant une rencontre entre l'équipe d'Angleterre et l'équipe d'Australie, et comme étant le premier match de Test cricket de l'histoire. Les tournées anglaises en Australie et australiennes en Angleterre deviennent alors régulières. En 1882, la victoire surprise des Australiens à The Oval contre une équipe anglaise qui rassemble amateurs et professionnels donne lieu à un faire-part de décès satirique dans le Sporting Times, qui annonce la mort du cricket anglais, dont les cendres « seront transportées en Australie ». C'est la naissance des Ashes (littéralement, « les Cendres »), une compétition qui se tient tous les deux ans en moyenne et oppose depuis les deux sélections.Le peintre et paysagiste anglais William Andrews Nesfield commémore le premier match de cricket des Australiens sur le sol anglais dans une toile intitulée Première équipe australienne à visiter l'Angleterre pour jouer un match de cricket contre les Willsher's Gentlemen à Chilham Castle, Kent, août 1878 conservée à la National Library of Australia.Le cricket oppose deux équipes de onze joueurs sur un terrain généralement de forme ovale. Un match peut durer, selon la forme jouée, de quelques heures à plusieurs jours. Une rencontre est divisée en plusieurs manches. Au cours de chacune d'entre elles, l'une des équipes essaye de marquer des points (runs ou courses), l'autre essaye de l'en empêcher. Le but du jeu est de marquer plus de runs que l'équipe adverse.Au cours d'une manche, l'équipe qui doit marquer des points dispose de deux joueurs à la fois sur le terrain : les batteurs (batsmen). Ils sont équipés chacun d'un accessoire de bois appelé batte. Les onze joueurs de l'équipe adverse sont également dans l'aire de jeu. Les deux batteurs se trouvent au centre du terrain, aux extrémités d'une portion rectangulaire de l'aire de jeu appelée pitch. Chacun de ces batteurs se trouve devant un ensemble de trois piquets de bois verticaux surmontés de deux témoins : le guichet (wicket). Les deux guichets sont séparés d'une vingtaine de mètres.Parmi les onze joueurs de l'équipe au lancer, un gardien de guichet est désigné. Il est placé juste derrière le wicket d'un des batteurs adverses. Il porte des gants. Les dix autres joueurs sont susceptibles d'être lanceurs (bowlers), c'est-à-dire d'avoir à lancer la balle. Lorsqu'un de ces dix joueurs est désigné pour lancer la balle, il prend une course d'élan et, arrivé au niveau d'un des guichets, lâche la balle en direction de l'autre guichet. Le lancer se fait bras tendu au-dessus de l'épaule et, la plupart du temps, la balle effectue un rebond sur le terrain avant d'atteindre le batteur situé au niveau du guichet cible.Le batteur qui fait face au lancer doit essayer de taper la balle avec sa batte. Une fois le lancer effectué et, éventuellement, la balle tapée par le batteur, son coéquipier et lui peuvent échanger leur position. Chaque échange de position rapporte un run, et plusieurs échanges successifs sont possibles. Les batteurs gardent ensuite la position qu'ils occupent après les échanges : si le nombre d'échanges effectué est impair, c'est le batteur qui n'a pas fait face au lancer précédent qui fait face au suivant. Si la balle tapée par le batteur sort du terrain sans toucher le sol, son équipe marque six runs. Si elle sort après avoir touché le sol, elle marque quatre runs. Dans ces deux cas, les batteurs reprennent ensuite la position qu'ils occupaient avant le lancer.Si la balle lancée par le bowler touche le guichet, le batteur qui fait face au lancer est éliminé. Il est éliminé également si la balle qu'il tape avec sa batte est attrapée au vol par l'un des onze joueurs adverses. De même, si les batteurs sont en train de courir pour essayer de marquer un run et que l'un des guichets est détruit avec la balle par l'un des onze adversaires, celui qui se dirige vers le guichet détruit est éliminé. Il y a en tout neuf modes d'élimination possibles. Lorsqu'un batteur est éliminé, c'est l'un de ses coéquipiers qui n'a pas encore joué qui le remplace sur le terrain.L'équipe qui effectue les lancers change de lanceur toutes les six balles. Le nouveau lanceur désigné effectue son lancer depuis le côté opposé à celui du lanceur précédent et ainsi de suite jusqu'à la fin du match.Lorsque dix des onze batteurs d'une équipe ont été éliminés, les deux équipes échangent leur rôle.Les règles du cricket, appelées Laws of Cricket, sont constituées actuellement de quarante-deux lois et quatre annexes. Elles sont codifiées par le Marylebone Cricket Club (MCC), un club privé basé à Londres, et qui est l'ancienne instance dirigeante du cricket au niveau mondial.Le premier code connu est établi par des « Noblemen and Gentlemen » en 1744. Il est révisé en 1755 par un comité de « Noblemen and Gentlemen » du Kent, du Hampshire, du Surrey, du Sussex, du Middlesex, et de Londres. Cette version compte six lois. Un comité similaire révise une nouvelle fois les lois en 1786. Le premier code rédigé par le MCC est adopté le 30 mai 1788. Il est révisé régulièrement depuis.Un préambule est ajouté lors de l'édition 2000 du code : le Spirit of Cricket, c'est-à-dire l'« esprit du jeu » : il définit un contexte supplémentaire, un cadre moral dans lequel doit être joué un match de cricket :« Le cricket est un jeu qui doit beaucoup de son attrait unique au fait qu'il doit être joué non seulement en respectant les règles, mais aussi en respectant l'esprit du jeu. »— Preambule to the Laws, publié par le Marylebone Cricket ClubCe préambule rend notamment les capitaines responsables du fair-play de leur équipe et du respect de l'esprit du jeu.L'instance dirigeante du cricket au niveau mondial, l'International Cricket Council, émet des règlements qui complètent les règles du cricket. Ces règlements concernent notamment la tenue des matchs internationaux, l'éligibilité des joueurs pour une sélection, la méthode Duckworth-Lewis et la conduite à tenir pour les joueurs et officiels.Un terrain de cricket est généralement de forme légèrement ovale. Aucune dimension ou forme n'est spécifiée par les lois du cricket, mais il s'agit habituellement d'une ellipse de faible excentricité, et dont les axes mesurent généralement entre 90 et 150 mètres. Pour les matchs internationaux, des dimensions minimum sont fixées : depuis le 1er octobre 2007, celles-ci ont été portées à un minimum de 137,16 mètres pour l'axe le plus court, et 148,13 mètres pour l'axe le plus long. Les limites du terrain, appelées boundaries, doivent être marquées par une ligne blanche, une corde ou un objet solide ayant une arête ou marqué d'une ligne.Au centre du terrain, orienté selon le grand axe de celui-ci, se trouve une surface rectangulaire dont l'herbe est coupée plus court, le pitch. La longueur de celui-ci est de 20,12 mètres et sa largeur est de 3,05 mètres. Il est fermé à chacun de ses bouts par des lignes blanches appelées bowling creases, qui mesurent 2,64 mètres chacune. D'autres lignes blanches figurent sur le pitch : on trouve une ligne appelée popping crease devant chacune des bowling creases, à 1,22 mètre de celles-ci.À chaque extrémité du pitch se trouve une structure de bois appelée wicket (« guichet »). Les deux wickets sont parallèles l'un à l'autre et distants de 20,12 mètres. Un wicket est composé de trois stumps et deux bails.Les stumps sont des piquets cylindriques verticaux, répartis sur une largeur de 22,86 centimètres et dont le diamètre est compris entre 3,49 et 3,81 centimètres. Ils culminent à 71,1 centimètres au-dessus de la surface du pitch. Les bails sont des petits témoins amovibles d'une longueur de 10,95 centimètres qui surmontent les stumps.                  Pour un match de cricket impliquant des équipes sénior masculines, la balle de cricket doit peser, neuve, entre 155,9 et 163 grammes. Sa circonférence doit être comprise entre 22,4 et 22,9 centimètres (soit un diamètre compris entre 7,13 et 7,29 centimètres). Les dimensions et masses sont différentes pour les rencontres entre équipes féminines ou junior : de 140 à 151 grammes et de 21 à 22,5 centimètres de circonférence dans le cas des rencontres féminines, de 133 à 144 grammes et de 20,5 à 22,0 centimètres de circonférence dans le cas des matchs en junior.La balle de cricket est faite de liège dur recouvert de cuir. Le cuir est séparé en deux hémisphères liés entre eux par une couture. La balle est traditionnellement de couleur rouge, avec une couture blanche. Des balles de couleur blanche sont utilisés pour les matchs de lancers (overs) en nombre limités, qui se déroulent, pour certains, en partie en soirée. C'est un héritage de la World Series Cricket, une compétition rebelle organisée entre 1977 et 1979, qui a introduit ces matchs joués en partie en soirée et une couleur de balle bien visible à la lumière des projecteurs. Une brève expérimentation avait été menée au XIXe siècle pour introduire des balles bleues pour les matchs de cricket féminin, le rouge ayant été supposé trop choquant pour les femmes.Une batte de cricket est composée d'un manche en rotin et d'un corps en saule. Cette partie de la batte, appelée blade (« lame »), est plate d'un côté et arrondie de l'autre, pour garantir sa solidité. Le batteur utilise le côté plat pour frapper la balle.La longueur de la batte ne doit pas excéder 96,5 cm et sa « lame » ne doit pas excéder 10,8 cm à l'endroit où elle est la plus large. Le saule qui la compose peut être recouvert d'une matière protectrice, à condition que celle-ci ne dépasse pas 1,56 mm d'épaisseur, et que cette protection ne risque pas d'endommager la balle. Les règles du cricket stipulent que la ou les mains qui tiennent la batte et d'éventuels gants de protection sont considérés comme faisant partie de la batte : si la balle touche une main ou un gant qui tient la batte, on considère qu'elle est touchée par la batte.Lors de la World Series Cricket, une compétition rebelle organisée en Australie entre 1977 et 1979, le batteur anglais Dennis Amiss eut l'idée de se protéger la tête avec un casque de moto et d'autres joueurs lui emboitèrent le pas. En mars 1978, l'australien Graham Yallop fut le premier joueur à porter un casque lors d'un match international officiel, disputé au format Test cricket contre l'équipe des Indes occidentales.À l'origine lourd à porter, le casque s'est imposé peu à peu et il est désormais rare de voir un batteur jouer sans.Les batteurs ne sont pas les seuls joueurs sur le terrain à porter un casque : certains fielders proches du batteur adverse qui reçoit la balle en portent aussi.Outre le casque, d'autres équipements de protection sont autorisés pour le batteur : des pads, qui protègent les jambes, des gants, et des protections aux avant-bras. Le gardien de guichet adverse a lui aussi droit à des gants et des pads. Les autres joueurs de champ n'ont droit ni à l'un, ni à l'autre.À haut niveau, la tenue du joueur de cricket dépend du format de jeu. Dans les matchs dont la durée est limitée en temps, la tenue est traditionnellement de couleur blanche ou crème, tandis que des tenues colorées sont utilisées pour les matchs dont la durée est limitée en nombre de lancers. L'introduction des tenues colorées dans ce type de match est aussi un des multiples héritages de la World Series Cricket. La qualification de « pyjama cricket » est parfois employée de manière péjorative pour qualifier ce type de matchs et vient du fait qu'ils se jouent en tenue colorée.La tenue du joueur se compose d'un pantalon, d'une chemise à manches courtes ou longues et parfois d'un pull-over avec ou sans manches.Une rencontre est divisée en manches, en anglais innings. Au cours d'une manche, l'une des équipes, dite in, est à la batte, l'objectif du batteur étant double : éviter de se faire éliminer et marquer le plus de point possibles (runs). L'autre équipe, dite out, a également deux objectifs : éliminer les batteurs et empêcher l'équipe in de marquer des points.Un match de cricket peut être soit limité en temps (first-class cricket), soit limité en nombre de lancers (limited overs cricket).Le first-class cricket se joue en quatre manches, deux par équipe, chacune passant alternativement à la batte et au lancer, sauf en cas de forfait ou de l'application d'une règle appelée « enchaînement » (follow-on).Le limited overs cricket se joue en deux manches, une par équipe, chaque manche étant elle-même divisée en un nombre fixe de séries (overs).Un over compte généralement six lancers réguliers et est effectué par un même lanceur. Un joueur ne peut effectuer deux overs consécutifs. Les overs sont lancés alternativement de chaque côté du pitch.En dehors des restrictions de temps ou de lancers, une manche peut prendre fin de diverses manières :Dix des onze batteurs de l'équipe in ont été éliminés. L'équipe est dite all out.L'équipe in n'a plus qu'un batteur disponible parce que tous ses coéquipiers non éliminés sont blessés.Le capitaine de l'équipe in déclare forfait pour la manche.Le capitaine de l'équipe in décide d'arrêter la manche, tactique qui lui permet de disposer de plus de temps pour éliminer l'équipe adverse par la suite (déclaration).L'équipe in a marqué assez de points pour remporter la rencontre.Selon le format ou la variante de jeu, différents résultats sont possibles. Ils dépendent du nombre de manches dont dispose une équipe et de la manière dont est limité le match, en temps ou en nombre de lancers.D'une manière générale, si l'équipe qui batte en dernier dépasse le total de points de son adversaire en ayant encore n batteurs disponibles, on dit qu'elle remporte le match par n guichets. Une victoire de l'autre équipe est exprimée sous la forme de n courses d'écart. Dans les rencontres où chaque équipe dispose de deux manches et le vainqueur n'en a besoin que d'une, on dit qu'elle gagne par une manche d'écart (et n courses). Une équipe peut également remporter le match si son adversaire abandonne le match lors du déroulement de celle-ci.Marquer plus de points que l'équipe adverse ne suffit pas pour remporter une rencontre limitée en temps. Un draw (match nul) se produit lorsque, pour une raison ou pour une autre, la deuxième manche de l'opposant n'est pas achevée, dans la plupart des cas si dix de ses batteurs n'ont pas été éliminés. Un tie correspond quant à lui à l'égalité de points à la fin du match. Dans les rencontres limitées en temps, il faut également que la dernière manche du match soit achevée, sinon on a encore un draw.L'équipe qui batte peut marquer des runs (courses) de plusieurs manières. Un point est marqué à chaque échange de position entre les deux batteurs sur le terrain après que celui qui a fait face au lancer a touché la balle avec sa batte ou la main qui la tient. Plusieurs échanges sont possibles à l'issue d'un même lancer. Si la balle sort des limites du terrain sans avoir touché le sol, six points sont marqués (six). Si elle en sort après avoir touché le sol, quatre points sont marqués (four). Dans ces trois cas, les runs marqués sont crédités au batteur actif. Le batteur n'a obligation ni de toucher la balle avec sa batte, ni de courir. Un batteur qui marque plus de cent runs en une seule manche réalise un century, qui peut être multiple (double-century, etc.).Il est possible de marquer sans que le batteur ait touché la balle avec sa batte. Un bye est marqué par échange de position des deux batteurs sans que la balle ait été touchée. Un leg bye est marqué dans la même situation, mais lorsque la balle a été touchée par le corps du batteur. Ces runs ne sont crédités à aucun des deux batteurs, mais catégorisés comme « extras ».Un lancer invalide est qualifié de no ball. Plusieurs raisons sont possibles : no ball est par exemple signalé si le lanceur mord au moment de délivrer la balle (overstepped), si le lancer n'est pas effectué bras tendu (throwing) ou si la balle ne rebondit pas et passe au niveau du batteur au-dessus de ses hanches (beamer). Lorsqu'il y a no ball, le lancer est à rejouer et l'équipe à la batte marque un run, classé comme extra. Si la balle passe trop loin du batteur pour qu'elle puisse être jouée, on parle de wide. Un run est alors également ajouté aux extras, et le lancer est là encore à refaire.En outre, l'arbitre peut accorder jusqu'à cinq points de pénalités si l'équipe au lancer a un comportement répréhensible, comme le ralentissement volontaire du jeu ou la dégradation du terrain.Un batteur est éliminé s'il est dans l'une des situations suivantes :Les cinq premières méthodes citées sont les plus courantes, les quatre autres étant plutôt rares, voire quasiment jamais invoquées pour les deux dernières. En Test cricket, la forme la plus ancienne de cricket au plus haut niveau international, il n'y a eu qu'une instance de joueur éliminé obstructing the field, sept instances de joueurs éliminés handled the ball et aucun joueur n'a été timed out, alors que près de 1900 matchs ont été joués depuis 1877 dans ce format. Par ailleurs, lors de la réécriture des lois du cricket en 2017, la loi handled the ball (sanctionnant un batteur s'il prend en main une balle toujours en jeu) a été supprimée et son contenu reversé dans la loi 37 obstructing the field.Un batteur blessé ou malade peut être temporairement retired not out : il se retire du jeu et peut revenir dès qu'un de ses coéquipiers est éliminé ou se retire. Par contre, un batteur qui s'est retiré du jeu pour tout autre raison ne peut revenir qu'avec l'accord du capitaine adverse. Dans le cas contraire, il est signalé retired out.Un batteur peut être bowled, caught, leg before wicket, stumped et hit wicket uniquement si le lancer a été jugé valable par l'arbitre. Par contre, il peut être run out, hit the ball twice, obstructing the field et timed out quelle que soit la validité du lancer. Un guichet est mis au crédit du lanceur qui a lancé la balle si le batteur est bowled, caught, stumped, hit wicket ou leg before wicket, mais pas dans les autres cas.En théorie, un batteur ne peut être éliminé qu'après un appeal adressé à l'arbitre d'au moins un joueur de l'équipe adverse. Cet appel consiste généralement en l'exclamation « How's that? » ou « Howzat? ».Le Test cricket est une forme de cricket international dans laquelle chaque équipe dispose de deux manches pour marquer. Un test-match est limité en temps. Cette limite est actuellement de cinq jours, mais elle a varié selon les époques.Une rencontre disputée en 1877 entre une sélection de joueurs australiens et une équipe anglaise en tournée est rétrospectivement considérée comme le premier test-match de l'histoire.Huit autres sélections ont rejoint l'Angleterre et l'Australie à ce niveau depuis : l'Afrique du Sud (premier test-match en 1889), les Indes occidentales (1928), la Nouvelle-Zélande (1929), l'Inde (1932), le Pakistan (1952), le Sri Lanka (1982), le Zimbabwe (1992), le Bangladesh (2000) et l'Afghanistan (2017). Le droit de disputer des matchs considérés comme « tests » est accordé par l'International Cricket Council (ICC). Les fédérations correspondantes à ces équipes sont les « membres de plein droit » de l'ICC.Les test-matchs sont généralement disputés dans le cadre de séries bilatérales, qui comptent entre deux et six rencontres. L'Angleterre et l'Australie se disputent par exemple les Ashes (Cendres) depuis 1882, tandis que le Trophée Frank Worrell est remis depuis 1961 au vainqueur des séries de test-matchs entre l'Australie et les Indes occidentales. Rares sont les test-matchs disputés dans le cadre de tournois.First-class cricket est une classification qui regroupe des matchs de haut-niveau disputés sur plusieurs jours, généralement au moins trois. Chaque équipe dispose de deux manches pour marquer. Le Test cricket est ainsi une forme de first-class cricket.Cette classification englobe notamment des compétitions disputées au niveau national dans les pays dont les fédérations sont membres de plein droit de l'International Cricket Council. Le County Championship, historiquement la première d'entre elles, est disputé en Angleterre depuis 1890. Il oppose des équipes représentant des comtés traditionnels d'Angleterre et une équipe représentant un comté gallois. Les statisticiens du cricket ont cependant regroupé dans cette dénomination des rencontres plus anciennes, parfois en contradiction les uns avec les autres.Sont également désignés comme first-class des rencontres entre une équipe habilitée à jouer des test-matchs et une équipe locale dans un pays où elle est en tournée. L'ICC organise la Coupe intercontinentale de cricket entre les meilleures équipes parmi celles qui n'ont pas le droit de jouer des test-matchs, et la compétition entre également dans cette dénomination.L'expression « limited overs cricket » désigne l'ensemble des rencontres dans lesquels chaque équipe dispose d'un nombre maximum de lancers pour marquer, par exemple cinquante séries de six lancers. Ces matchs se déroulent en majorité sur une seule journée.La première compétition officielle de ce genre, la Gillette Cup, est organisée en 1963 en Angleterre pour faire face à la chute du nombre de spectateurs et donc des revenus des équipes du County Championship. Les autres principales nations du cricket suivent bientôt. En 1971 est disputé le premier One-day International (ODI) entre l'Australie et l'Angleterre. C'est dans ce format qu'est organisé la Coupe du monde de cricket, dont la première édition a lieu en 1975.Diverses innovations ont par la suite vu le jour dans ces formes de jeu : introduction des tenues colorées, des matchs disputés en partie en soirée (avec une première manche disputée l'après midi, et la seconde en soirée, procédé qui permet d'augmenter l'audience) nécessitant l'usage d'une balle blanche, par exemple.En 2003, l'England and Wales Cricket Board introduit une nouvelle variante de limited overs cricket, encore plus courte : le Twenty20 (ou T20), où chaque équipe dispose de vingt séries de six lancers pour marquer, ce qui permet à un match de finir en trois heures environ. Disputé au niveau international depuis 2005, le Twenty20 dispose d'un championnat du monde depuis 2007.Le cricket club est un cricket amateur, mais doté de règles formelles. Dans la grande majorité des cas, le nombre de manches est limité, usuellement à 30 ou 35 par période. Le cricket club est pratiqué de manière intensive dans les nations du cricket, mais aussi ailleurs par leurs émigrants. Il est fréquent de rencontrer des terrains de cricket club dont le pitch est en herbe synthétique.Le cricket de plage (beach cricket) est un terme appliqué à toutes les formes informelles de cricket, même s'il n'est pas joué sur une plage. Les règles sont habituellement issues d'un consensus entre les deux équipes, avant la partie. Bien souvent, les règles les plus complexes et les plus subtiles du cricket, comme le LBW, sont ignorées ou adaptées.Le kirikiti est une forme du cricket originaire des Samoa. Il est le sport national des Samoa, et est joué dans plusieurs autres pays du Pacifique, notamment parmi la diaspora des îles pacifiques en Nouvelle-Zélande. La balle est faite d'un caoutchouc très dur entouré de pandanus. Les joueurs n'ont pas de protection et ne portent souvent qu'un lavalava (en). La batte, faite de bois enveloppé de sennit, est façonnée sur le modèle du « lapalap », une arme de guerre samoane à trois faces, qui prend elle-même sa forme de la tige de la feuille de cocotier. En raison de sa forme anguleuse, la trajectoire des balles est très difficile à prévoir. Les règles du kirikiti sont variables, et la taille des équipes n'est pas définie précisément. Le kirikiti est autant un évènement sportif que social et festif : Une partie de kirikiti peut durer plusieurs jours et est accompagnée de chants, de danses et de festivitésAu niveau international, le cricket est organisé par l'International Cricket Council (ICC), fondé en 1909 par des représentants de l'Angleterre, de l'Australie et de l'Afrique du Sud sous le nom d'Imperial Cricket Conference. Le Marylebone Cricket Club (MCC), un club privé anglais crée à la fin du XVIIIe siècle, est quant à lui propriétaire et rédacteur des règles du cricket.Le MCC a eu une influence importante sur l'organisation du jeu pendant près de deux siècles, organisant notamment le jeu en Angleterre. Durant une bonne partie du XXe siècle, le MCC gère les tournées de l'équipe d'Angleterre.L'ICC regroupe les fédérations de plus de cent pays, nations ou groupes de pays. Elle compte dix membres de plein droit, dont les équipes masculines sont autorisées à jouer des test-matchs. Jusqu'en 1963, seuls les nations de l'Empire britannique puis du Commonwealth peuvent joindre l'ICC. Parmi les fédérations qui représentent plusieurs pays ou nations, on compte l'England and Wales Cricket Board (ECB, Angleterre et Pays de Galles) et le West Indies Cricket Board (WICB, plusieurs états et dépendances des Caraïbes). Les compétitions organisées par le WICB opposent des équipes représentant des États ou des groupements d'États, mais dont les meilleurs joueurs sont sélectionnés au niveau international par une équipe fédérale, celle des Indes occidentales.Pour les principales équipes nationales, la plupart des rencontres se déroulent dans le cadre de tournées. Au cours de celles-ci, l'équipe visiteuse affronte son hôte au cours d'une série de test-matchs et d'une série de matchs au format One-day International. Depuis l'introduction du Twenty20 international en 2005, un ou deux matchs de cette variante sont parfois disputés. Une tournée compte également plusieurs rencontres entre visiteurs et équipes locales.La première Coupe du monde qui se tient est celle de cricket féminin, en 1973. Il faut attendre 1975 pour que les hommes aient eux aussi leur compétition équivalente. La Coupe du monde est disputée au format ODI. Les équipes autorisées à pratiquer le Test cricket disputent la disputent automatiquement et sont rejointes par des équipes issues d'un tournoi de qualification. Entre un tournoi de qualification et le suivant, ces équipes et celles qui pratiquent le Test cricket peuvent disputer des rencontres reconnues officiellement comme ODI. L'International Cricket Council organise également le Trophée des Champions, qui rassemble moins de participants que la Coupe du monde et, depuis 2007, les championnats du monde de Twent
sport;La crosse, le bâton de hockey ou la canne de hockey désigne le bâton recourbé utilisé pour diriger le palet — également appelé puck, rondelle ou disque — ou la balle que ce soit au hockey sur glace, au roller hockey ou encore au hockey sur gazon. La crosse désigne aussi le jeu d'origine amérindienne.D'une longueur variable de 163 cm maximum, elle est prolongée par une palette incurvée d'une longueur maximum de 32 cm. La hauteur de la palette est comprise entre 5 et 7,5 cm. La crosse est composée de bois ou de différents matériaux composites comme le kevlar ou la fibre de verre.La crosse du gardien est plus coudée et la palette plus large (dessin de droite).Chaque région francophone possède un vocabulaire propre : la « crosse » est le terme utilisé en France, en Suisse romande on lui préfère le terme « canne de hockey ». Au Canada francophone, on parle soit de « bâton de hockey », soit tout simplement de « bâton » ou « hockey », parce que le terme crosse désigne l'équipement utilisé dans un autre sport collectif, la crosse, surtout populaire en Amérique du Nord.Il n’est pas rare de nos jours de voir des bâtons en graphite, d’autres en fibre de carbone, en plastique ou même en titane. Mais, à l'origine, la crosse de hockey était constituée d’un seul morceau de bois. Au XVIIe siècle, les Amérindiens jouaient avec des bâtons courbés, taillés dans un arbre, et avec une balle à un jeu semblable au curling sur gazon. En 1855, le hockey nait à Kingston, Ontario, importé par les soldats britanniques. À cette période, les joueurs confectionnaient eux-mêmes leur crosse en hêtre blanc. L'industrialisation de la fabrication des crosses date seulement du début du XXe siècle. Jacob Hespeler, entrepreneur Ontarien, a créé plusieurs moulins à scie afin de produire, en série, des crosses de hockey. À cette époque, les crosses étaient vendues 45¢ la douzaine, aujourd’hui, une crosse coûte entre 20 $ et 350 $ l’unité.En 2015, un musée canadien a acquis la plus vieille crosse du monde, datée des années 1830, pour la somme de 300 000 dollars.Aujourd’hui, avec la production en série, la crosse de hockey est fabriquée en plusieurs étapes. Le manche est fait d’un morceau de tremble sur lequel deux lames très minces de bouleau sont collées sous pression. Ensuite, le manche est scié en trois parties identiques, chaque partie permettant de fabriquer une crosse.Les trois manches sont décapés à la sableuse une première fois, puis renforcés avec de la fibre de verre et de la fibre de carbone.Les manches sont ensuite déposés dans un moule afin d’être cuits à 80 °C sous pression.La septième étape consiste à détailler les manches un par un à l’aide d’une moulurière qui arrondi les coins pour ensuite repasser dans une sableuse afin d’ôter les aspérités du bois.Un bloc est collé au bout du manche afin de recevoir la palette. La colle utilisée doit être résistante à l'eau.L’angle de la palette est très important. Les différents angles possibles sont définis par une norme. Les angles possibles sont numérotés de 1 à 6. Plus le chiffre est grand, plus l’angle entre la palette et le bout de la crosse est grand.Les étapes suivantes consistent à affiner la palette. Tout d’abord, l’ensemble de la crosse est à nouveau sablé afin d'éliminer les dernières aspérités. La crosse passe ensuite sur une CNC qui façonnera la palette en fonction du modèle choisi.Il existe plus de 6 000 modèles de palette. La courbure est obtenue après passage dans une machine à vapeur qui la rend malléable.Ensuite, la palette est renforcée avec une toile de fibre de verre et de la résine d’époxy. Après 24 heures de séchage à une température de 32 °C et une finition à la sableuse circulaire, la crosse est à nouveau plongée dans un bain de résine d’époxy.L'étape finale consiste en l'application de la marque du fabricant sur le manche par sérigraphie.En moyenne 40 000 crosses de hockey sont fabriquées chaque semaine. Portail du hockey sur glace
sport;"Le cyclisme recouvre plusieurs notions concernant la bicyclette : il est d'abord une activité quotidienne pour beaucoup, un loisir pour d'autres (cyclotourisme), enfin un sport proposant des courses selon plusieurs disciplines : l'école de cyclisme, le cyclisme sur route, le cyclisme sur piste, le cyclo-cross, le vélo tout terrain (abrégé couramment VTT), le BMX, le cyclisme en salle et le polo-vélo. Le sport cycliste est réglementé au niveau mondial par l'Union cycliste internationale (UCI).Le baron allemand Karl Von Drais invente en 1816 la draisienne, considérée comme l'ancêtre de la bicyclette. C'est un véhicule à deux roues alignées que le cycliste fait avancer en poussant sur le sol avec ses pieds. Il présente son invention à Paris, le 5 avril 1818. En 1861, le carrossier Pierre Michaux et son fils Ernest commencent la fabrication des premiers vélocipèdes à pédale. Pierre Michaux appelle cette pédale « pédivelle » et en généralise la fabrication en créant son entreprise en 1865, la « Maison Michaux » qui devient « La Compagnie parisienne » en 1869. Il est question, à partir de 1867, de succès populaire. Des engins similaires au vélocipède Michaux ont beaucoup de succès aux États-Unis à partir de 1866, lorsque Pierre Lallement, ancien associé de Pierre Michaux, obtient un brevet américain pour une machine qu'il appelle « bicycle ». Vers la fin des années 1870 apparaît le grand-bi. Il a une roue avant d'un très grand diamètre et une roue arrière plus petite. L'intérêt de la grande roue avant est d'augmenter la distance parcourue pour un tour de pédale. La roue avant étant plus haute et plus grande que la roue arrière, la conduite de l'engin est dangereuse et difficile.Deux ans plus tard, les premiers clubs cyclistes sont fondés : les Véloces Clubs de Paris, Toulouse, Rouen. En 1880, John K. Starley de la société The Coventry Sewing Machine Company (« société des machines à coudre de Coventry »), qui deviendra Rover, invente la « bicyclette de sécurité » avec des roues de taille raisonnable et une transmission par chaîne. Le cycliste y est installé à l'arrière, ce qui rend presque impossible la chute de type « soleil » où le cycliste est catapulté par-dessus la roue avant. Le 6 février 1881, l'Union vélocipédique de France (UVF) est créée à Paris. L'Écossais John Boyd Dunlop invente le pneumatique en 1888, ce qui contribue à améliorer encore le confort du cycliste. Trois ans plus tard, Michelin invente le pneu démontable. En 1896, le cyclisme devient sport olympique. En 1900, l'Union cycliste internationale est créée.En 1910, la roue libre fait son apparition sur le Tour de France (compétition créée en 1903), puis le dérailleur en 1937. En 1941, sous le régime de Vichy, Jean Borotra interdit aux femmes de participer à des compétitions cyclistes, jugeant que ce sport est nocif pour elles. En 1945, l'UVF est remplacée par la Fédération française de cyclisme (FFC). La Fédération Internationale de cyclisme amateur, fondée en 1900 par les membres des fédérations américaine, belge, française, italienne et suisse, fusionne en 1993 avec la Fédération Internationale de cyclisme professionnel pour devenir l'Union cycliste internationale (UCI). Plus de 170 fédérations nationales sont affiliées à l'UCI.L'équipement diffère sensiblement selon le type de pratiquant.En randonnée amateur ou en simple circulation, le port du casque par le cycliste n'est pas obligatoire, comme le phare, lorsque son utilisation n'est pas nécessaire (hors d'un tunnel, par beau temps, par exemple), bien que tous deux conseillés. On peut également installer des pare-boue (surtout en temps pluvieux ou dans une zone forestière). Mais il est obligatoire de porter et d'utiliser un phare diffusant un rayon blanc ou jaune lorsque le temps ou le lieu l’exige (dans un tunnel, lorsque le temps est pluvieux), de même qu'une bicyclette doit porter tout le temps un catadioptre blanc à l’avant et rouge à l'arrière du véhicule, et que le conducteur doit porter sur lui un gilet (recommandé hors agglomération, obligatoire de nuit ou quand la visibilité est nulle ou mauvaise (brouillard)).En course, lors de compétitions professionnelles, le port du casque est obligatoire dans toutes les disciplines afin d'éviter les accidents, tout comme pour le triathlon. De plus, depuis le 1er janvier 2000 le poids minimum des bicyclettes est fixé à 6,8 kg.Le cyclisme urbain constitue la branche du cyclisme dévolue au transport urbain. Il s'agit de tout ce qui est relatif aux déplacements à vélo sur de petites et moyennes distances (quelques kilomètres) en milieu quasi exclusivement urbain (dans la ville et sa proche banlieue), c'est-à-dire en partageant la voirie avec les autres modes de déplacement motorisés ou non. Sur une distance entre un et sept kilomètres le vélo est le mode de transport le plus rapide. Au-delà de cette praticité, le cyclisme urbain vise à limiter la pollution et à diminuer l'engorgement croissant des villes par l'usage massif de l'automobile. De plus, ses bienfaits multiples pour la santé individuelle sont reconnus. Il fait partie de l'écomobilité.Des aménagements spécifiques peuvent être construits pour tenter d'améliorer les conditions de circulation des cyclistes urbains. Ces aménagements peuvent être des bandes ou pistes cyclables, zone de rencontre, Double-sens cyclable et parkings dédiés aux vélos (ou vélos et cyclomoteurs). Des feux cyclistes spécialement dédiés sont également mis au point.Le cyclotourisme est la pratique de la randonnée à bicyclette, réalisée sans esprit de compétition. Le cyclotourisme se présente sous de multiples formes, de la promenade et de la pratique familiale à allure libre jusqu'aux rallyes et brevets sur tous les parcours et toutes les distances : rallyes libres, brevets-randonneurs, flèches, diagonales, cyclo-découvertes, cyclo-montagnardes, Audax. Créé en 1890, le Touring Club de France est la seule association représentative du cyclotourisme en France de la fin du XIXe siècle au début du XXe siècle. Par la suite, le club se tourne vers le tourisme automobile, ce qui amène la création le 8 décembre 1923 de la Fédération Française des Sociétés de Cyclotourisme, rebaptisée en 1945 Fédération française de cyclotourisme (FFCT). À partir des années 1970, le développement de la Fédération est devenu régulier et le nombre de clubs ne cesse d'augmenter en France.Le principe du cyclisme dans sa version sportive est de parcourir une distance donnée à bicyclette le plus rapidement possible. Les pratiquants sont répartis dans des catégories en fonction de leur âge, de leur sexe et de leur niveau.Le cyclisme sur route comprend plusieurs types d'épreuves incluant la « course à étapes » (Tour de France, Tour d'Espagne, Tour d'Italie, Tour de Suisse, Tour de Belgique, Tour de Pologne) ; la « course classique », course en ligne d'un jour (Paris-Roubaix, Tour de Lombardie, Liège-Bastogne-Liège, par exemple) ; la course « contre-la-montre », (Grand Prix des Nations, par exemple) et la course « contre-la-montre par équipe » (Eindhoven Team Time Trial, par exemple). Il est dit du cyclisme sur route que c'est un sport d'équipe, mais à classement individuel[réf. nécessaire].Les épreuves de cyclisme sur piste se déroulent sur des vélodromes et prennent des formes plus diversifiées : il existe des épreuves individuelles et des épreuves par équipes, des épreuves de sprint, de demi-fond et de fond. Les plus connues sont la vitesse, la poursuite et le kilomètre. Les principales compétitions sont les Jeux olympiques et les Championnats du monde de cyclisme sur piste. Le cyclocross est une spécialité hivernale, qui se court en circuit. La course dure une heure environ, sur un revêtement souvent mixte (asphalte et terre). Le BMX est essentiellement caractérisé par sa très petite taille et ses roues de 20"", ce qui lui confère une maniabilité sans égale et a permis la création de plusieurs disciplines : race, dirt, flat, rampe, street.Le vélo tout terrain, également appelée VTT, est la plus récente du cyclisme sportif. Le vélo tout terrain (ou Mountain Bike) a été inventé aux États-Unis en 1970. Il a été importé en France quelques années plus tard. Ce terme désigne également le vélo avec lequel cette discipline est pratiquée : compte tenu de sa polyvalence, ce type de vélo est, avec le VTC, le plus vendu en France actuellement.Les épreuves cyclosportives (ou cyclosport) sont à mi-chemin entre le cyclotourisme des randonneurs et la compétition cycliste, mais il s'agit d'épreuves de masse proposées à tous les cyclistes : les organisateurs proposent des dossards, un classement et un chronométrage comme dans les marathons de course à pied. En France, plus de 150 sont dénombrées, la plus célèbre et la plus ancienne étant « La Marmotte », créée en 1982 dans les Alpes, qui attire plus de 8 000 participants.Le cyclisme handisport est un sport dérivé du cyclisme et des compétitions existent depuis les années 1960. Championnats  Jeux olympiques Le sport cycliste est inscrit de longue date au calendrier des Jeux olympiques d'été. Les épreuves sont ouvertes tant aux hommes qu'aux femmes. Les épreuves sur route incluent la course en ligne et l'épreuve du contre-la-montre. Le cyclisme sur piste est également présent avec différentes disciplines et, depuis quelques années, le mountain-bike et le BMX ont été inclus au programme.Le cyclisme handisport ou paracyclisme est devenu un sport officiel aux Jeux paralympiques depuis les Jeux paralympiques d'été de 1988 à Séoul, avec une version sur route aux Jeux paralympiques d'été de 1996 à Atlanta. Nationaux et continentaux Chaque pays organise ses propres championnats pour déterminer les meilleurs athlètes par discipline. Les Championnats de France de cyclisme sont organisés tous les ans par la Fédération française de cyclisme. Ils rassemblent des athlètes par équipes et par régions et se déclinent par catégories d'âge (cadets, juniors, espoirs et élite). En Europe, il existe des championnats d'Europe dans chaque discipline. Personnalités La rédaction de L'Équipe magazine a établi un classement des 100 sportifs du siècle. Eddy Merckx, classé neuvième, est le premier d'entre eux. Quatre autres cyclistes ont été retenus : Fausto Coppi, Jacques Anquetil, Alfredo Binda et Bernard Hinault. Tous sont issus de la route et aucune femme ne figure dans ce classement. Par ailleurs, Eddy Merckx a été élu « Coureur du siècle » par l'UCI. Chaque année, plusieurs titres de meilleurs coureurs sont décernés. Le plus prestigieux de ces honneurs reste le Vélo d'or. Citons également le Mendrisio d'or et le coureur UCI de l'année. Performances et records La piste est également utilisée pour établir des records de temps sur une distance déterminée, ou de distance parcourue pendant une durée déterminée. Le plus célèbre est le record de l'heure, qui consiste à parcourir seul la plus grande distance possible pendant une heure. L'histoire du record de l'heure a été jalonnée par les exploits des plus grands coureurs spécialistes du contre-la-montre sur route ou de la poursuite sur piste (parmi lesquels Fausto Coppi, Jacques Anquetil, Eddy Merckx, Francesco Moser, Miguel Indurain peuvent être cités).La plus grande vitesse jamais atteinte sur du plat, l'a été par le Néerlandais Sebastiaan Bowier en septembre 2013, portant le record du monde à 133,78 km/h, sur son vélo couché hautement aérodynamique, grâce à un carénage aérodynamique. C'est le record toutes catégories pour les véhicules à propulsion humaine. Ultracyclisme L'ultracyclisme est une discipline regroupant les compétitions disputées sur de très longues distances. L'épreuve la plus connue de l'ultracyclisme avec assistance véhiculée est la Race Across AMerica (RAAM)[réf. nécessaire]. Cyclisme féminin Resté anecdotique pendant de nombreuses années, le cyclisme féminin est reconnu par l'Union cycliste internationale mais aussi par la Fédération française de cyclisme. Ce ne fut pas toujours le cas : en France, en 1941, sous le régime de Vichy, Jean Borotra interdit aux femmes de participer à des compétitions cyclistes, jugeant que le ce sport est nocif pour elles. Cette pratique prend un essor important avec l'organisation, en 1984, du Tour de France féminin. En effet, la Grande Boucle féminine internationale (anciennement Tour de France féminin) est une course cycliste sur route par étapes, qui s'est disputée chaque année entre 1984 et 2009 en France, exclusivement par des femmes. La course était considérée comme l'un des trois grands tours féminins avec le Tour d'Italie féminin et le tour de l'Aude. De 1984 à 1989, un Tour de France féminin est couru en lever de rideau de l'épreuve masculine. Cette épreuve est organisée par la Société du Tour de France organisatrice du Tour de France masculin. En 1990, cette épreuve change de nom et de format : elle devient le Tour de la CEE féminin qui s'arrête en 1993.Les jeux olympiques accueillent une épreuve féminine sur route depuis 1984, des compétitions féminines sur piste depuis 1988. En 2012, pour rétablir la parité aux jeux olympiques de Londres, le programme du cyclisme féminin est similaire au programme du cyclisme masculin : il comprend une course sur route (médaille d'or pour Marianne Vos, néerlandaise), une course contre la montre (médaille d'or pour Kristin Armstrong, américaine), une épreuve de vitesse (médaille d'or pour Anna Meares, australienne), une compétition de keirin féminin (médaille d'or pour Victoria Pendleton, britannique), l'omnium (médaille d'or pour Laura Kenny, britannique), la vitesse par équipes, la poursuite par équipes, ainsi qu'un cross-country en V.T.T. (médaille d'or pour Julie Bresset, française) et une compétition de B.M.X. (médaille d'or pour Mariana Pajón, colombienne).La Coupe du monde de cyclisme sur route féminine a été créée en 1998.Alors que les championnats du monde ont été créés en 1895 pour les hommes, le championnat du monde de cyclisme sur route féminin n'est apparu qu'en en 1958. Certaines épreuves, comme le 500 m femmes, sont toutes récentes. Réservées aux femmes, elles se disputent, seule contre la montre sur 500 mètres, départ arrêté avec un classement au temps. La cycliste accélère le plus rapidement possible jusqu'à sa vitesse maximale, puis tente de la maintenir jusqu'à la ligne d'arrivée. Les meilleurs temps pour le 500 mètres départ arrêté tournent autour de 34 secondes. Les tactiques ne sont pas prédominantes dans cette compétition. Il s'agit purement d'un test de puissance et de techniques précises.La pratique du cyclisme peut avoir des conséquences à long terme sur la santé de ses pratiquants.Les exercices physiques du cyclisme sont liés à l'amélioration de la santé et au bien-être. Elles assurent, selon la plupart des études de santé,, la perte de poids, améliorent l’endurance, diminuent les temps de récupération, permettent d'éviter les maladies cardiovasculaires liées à l'âge et amplifient les voies respiratoires. Tous ces effets sont bons à long terme. Néanmoins, les risques sont prépondérants en cas d'efforts trop fréquents[source insuffisante]. Bien qu'exposé à la pollution automobile, les bénéfices semblent[réf. nécessaire] l'emporter sur les effets de cette pollution, par rapport aux autres modes de déplacements.La pratique du cyclisme ne se fait pas sans risque de blessure. De telles contre-indications seraient d'abord liées au manque de prévention : vitesse trop élevée pour la route selon son état ou son inclinaison, non-respect du Code de la route (les grillages des feux urbains, le passage d'un Stop ou d'un Cédez-le-passage sans considération, la conduite à gauche ou en zigzag), intolérance des automobilistes. Ensuite, l'état de la chaussée et de la piste cyclable peut également jouer un rôle dans le déséquilibre du cycliste et la chute de celui-ci, si elles sont déplorables. Enfin, des conditions météorologiques, telles que la neige, la grêle ou même la foudre peuvent contribuer à blesser, voire tuer des cyclistes.Les traumatismes crâniens représentent, selon une étude de 2012, environ une personne sur vingt des tués et blessés graves en agglomération ; la moitié des accidents sont d'ailleurs liés[pas clair] à ce type de traumatisme. Les deux tiers des cyclistes accidentés dont le pronostic vital a été engagé présentaient effectivement un traumatisme crânien. Le cycliste peut également endurer des crampes, des fractures osseuses en cas de chute, un malaise, etc. Des douleurs à plus long terme peuvent également être la cause des blessures ressenties à vélo comme les hernie discale ou une tendinite… La vulve de la cycliste, présente chez une cycliste professionnelle sur cinq environ, est une déformation des grandes lèvres en raison du poids et du frottement des parties génitales contre la selle.Les auteurs de la précédente étude indiquent qu'en 2010, en France, 59 cyclistes sont décédés et 963 blessés.Depuis les origines du cyclisme, une presse spécialisée a accompagné son essor, tant celui du cyclisme de compétition que celui du cyclisme de loisir et du cyclotourisme. Pour la France, des ouvrages ont fait le recensement de cette littérature. De 1946 jusqu'aux débuts des années 1970, la presse sportive généraliste quotidienne (L'Équipe), hebdomadaire (Miroir Sprint, Miroir des sports), mensuelle (Sport digest, Sport collection, Sport mondial, Sport & vie) a dominé le marché en ouvrant largement ses colonnes au vélo. À partir de 1960, la presse magazine cycliste à périodicité mensuelle, prend le relais avec principalement :La France cycliste, organe de la Fédération française de cyclisme, paraît de 1946 à 2014 ;Miroir du cyclisme, bimestriel la première année puis mensuel, de janvier 1960 à avril/mai 1994 ;l'Équipe cyclisme magazine, en parution mensuelle à partir de décembre 1968, suivi de Cyclisme magazine en 1975, transformé en Vélo en 1978, puis en Vélo magazine en 1984. Un temps appelées Vélo-sprint 2000, ces parutions relevaient toutes du groupe L'Équipe ;Sprint international, de mars 1981 à 1986, puis Sprint 2000, d'août 1986 à décembre 1988 ;Cyclisme international, de avril 1986 à février 2004 ;Vélo Un , de 1993 à 1999 ;Le Cycle, de création plus ancienne, ouvert aux constructeurs de cycles et aux techniques du vélo, modernisé au début des années 1980.Actuellement, la presse cycliste magazine est représentée par de nombreux titres, couvrant l'ensemble des activités cyclistes. Pour la France seule, par ordre d'ancienneté de parution :Cyclotourisme est édité par la Fédération française de cyclotourisme depuis 1953.Vélo Magazine, édité par le groupe industriel auquel appartient le quotidien L'Équipe, poursuit sa parution avec constance depuis les années 1970.Le Cycle, « magazine des pratiquants du cyclisme ».VTT Magazine parait depuis 1988. Avec le titre suivant il accompagne la montée en France de la pratique du Vélo tout terrain.Vélo vert, créé en 1989, s'adresse à la même clientèle.Cyclo passion, « officiel du pratiquant », créé en 1994 par l'équipe éditoriale de Vélo Un.Top vélo paraît depuis 1997.Cyclo sport.Vélo tout terrain.Planète cyclisme, créé en 2005, de parution bimestrielle régulière, prend la suite des magazines cyclistes indépendants des organisateurs de courses, avec modernisme.Bike magazine, créé en 2007.Only bike, magazine créé en 2008 se concentre sur des annuaires très complets, créés dès 2003.Mountain bike action magazine.Le monde du vélo.Pédale !, parution irrégulière de numéros « hors-série » au moment du Tour de France.Le Sport Vélo, créé au début de l'année 2011 (après 1 premier essai en 2004) cessait sa parution en août 2013 après 27 numéros.Les bienfaits de ce sport pour la santé sont mis en avant par la Fédération française de cyclisme et celle-ci demandera lors de la pandémie du Covid-19 d’autoriser à nouveau la pratique de ce sport, dès le 11 mai 2020,.Les représentants de la Fédération française de cyclisme fustigent en parlant d'une ""véritable discrimination vis-à-vis de (leur) sport"".Ouvrages à vocation encyclopédiqueSport vélocipédique : les champions français, par E. Gendry (G. de Moncontour), 1891, éd. G. Meynieu (Angers) (en ligne sur Gallica - BNF).Le Cyclisme théorique et pratique, par L. Baudry de Saunier, éd. La Librairie Illustré, 1893 (en ligne sur Gallica - BNF).Le cyclisme, Marcel Viollette, Lucien Petit-Breton, Thornwald Ellegaard, Louis Darragon, Gaston Rivierre, Paul Meyan, Ernest Mousset, préface d'Henri Desgrange, 1912, éd. Pierre Laffitte et Cie - Paris (en ligne sur Gallica - BNF).Pierre Chany : La fabuleuse histoire du cyclisme, tome 1, Paris, éditions ODIL 1975. 1074 pages. Préface d'Antoine Blondin.Pierre Chany : La fabuleuse histoire des grandes classiques et des championnats du monde, tome 2, Paris, éditions ODIL, 1979. 984 pages.Gérard De Smaele, Le cyclisme dans les livres et les revues, L'Harmattan, 2015, 292 p. Préface de Keyzo Kobayashi et Isabelle Lesens. Postface de Jean Durry.Jean Durry « et ses amis » : l'en CYCLE opédie, Lausanne, éditions Edita, 1982. 424 pages. Préface de Pierre Chany, contributions de Jacques Seray, Daniel Rebour, Ami Guichard, Philippe Marte, Pierre Roques, Serge Laget, etc.Pascal Sergent, Encyclopédie illustrée des coureurs français depuis 1869, Eeklo, Editions de Eecloonaar, 1998, 768 p. (ISBN 90-74128-15-7).Guy Crasset, Hervé Dauchy, Pascal Sergent, Encyclopédie mondiale cyclisme, éditions De Eecloonaar (Belgique), 2134 pages en 3 volumes, parue en 2000 avec le patronage de l'Union cycliste internationale.Claude Sudres : Dictionnaire international du cyclisme, plusieurs éditions entre 1983 et 2004.Ouvrages généralistesJacques Borgé & Nicolas Viasnoff : Archives du vélo, Monaco, éditions Michèle Trinckvel, 1998. 204 pages.Françoise & Serge Laget : l'univers du vélo, Paris, éditions Solar, 2001. 144 pages.Françoise & Serge Laget : Le cyclisme (La Belle époque du sport), Courlay, éditions Jadault, 1978. 98 pages. Préface de Raymond Poulidor.Les cahiers de médiologie : La bicyclette, numéro 5-premier semestre 1998, Paris, Gallimard.Louise Roussel, À vos cycles ! Le guide du vélo au féminin, Tana, 2021, 208 p. (ISBN 979-1030103892)Ouvrages annuelsVelo, annuaire paraissant en Belgique depuis 1956, publie les résultats des compétitions cyclistes de l'année antérieure à son millésime, avec des éditions récapitulatives (la plus récente en 2009 sous l'appellation Velo plus 1869-2009, signée Joel Godaert) donnant les résultats chronologiques des courses. Ses premiers concepteurs étaient René Jacobs et Hervé Mahau, d'où son appellation courante : annuaire Jacobs.Le guide international du cyclisme. Il paraît depuis 2003 sous la signature de Benoît Gauthier et est édité par les éditions Onlybike. On y trouve les résultats des courses cyclistes de l'année antérieure au millésime de parution et les grandes lignes des palmarès de la plupart des coureurs du peloton professionnel, amateur et junior.L'année du cyclisme, créé en 1974 par le journaliste Pierre Chany aux éditions Calmann-Levy en était en 2013 à son quarantième millésime de parution. Sa vocation est plus restreinte que les deux annuaires cités précédemment mais les ouvrages sont agrémentés de nombreuses photographies.Ouvrages relatifs au cyclisme fémininLa Femme et la bicyclette, par le Dr J. Alvin, 1895Rémy Pigois, Cyclisme féminin, Editions Amphora, 1996 (ISBN 978-2-85180-307-8)(en) Selene Yeager, The Bicycling Big Book of Cycling for Women : Everything You Need to Know for Whatever, Whenever, and Wherever You Ride, Rodale Press Inc., 2015, 320 p. (ISBN 978-1-62336-486-1, lire en ligne) Ouvrages illustrés Collectif (trad. de l'anglais), Le cyclisme en infographie, Paris, L'imprévu, 2016, 128 p. (ISBN 979-10-295-0405-1)Championnat du monde de cyclismeCyclisme aux jeux olympiques d'étéCyclisme (handisport)Cyclisme artistiqueCyclisme urbainUltracyclismeGlossaire du cyclismeVélo couchéVélomobileRessource relative à la santé : (en) Medical Subject Headings Site de l'Union cycliste internationaleSite de la Fédération française de cyclisme« SPORT (Disciplines) Le cyclisme : Le cyclisme féminin », sur le site de l'encyclopédie Universalis.fr (consulté le 24 juillet 2016). Portail du sport   Portail du cyclisme   Portail de la bicyclette"
sport;"Les défenseurs (ou arrières) sont des joueurs de football dont la tâche principale consiste à perturber, ou idéalement empêcher, le jeu d'attaque de l'équipe adverse.Le ballon peut être récupéré à la suite d'un duel gagné par un défenseur (action individuelle) ou par une déstabilisation des adversaires par une stratégie collective (interception, provocation d'un hors-jeu, etc.). Outre sa solidité et sa rigueur physique, les qualités requises pour un bon défenseur sont donc le sang-froid, la concentration et l'intelligence de jeu, notamment dans le placement. Pendant longtemps, on a pu estimer que les défenseurs étant des « destructeurs » de jeu, ils n'avaient pas à montrer de capacités techniques particulières. Ce n'est plus le cas actuellement car ils sont amenés à participer à des tâches offensives.Une ligne de défense est habituellement constituée de quatre joueurs, plus rarement trois ou cinq. La défense à quatre « typique » comprend deux arrières latéraux, qui évoluent chacun sur un côté, et deux arrières centraux, qualifiés selon leur rôle de stoppeur ou de libéro (poste de plus en plus rare dans le football moderne).Lorsque le football est introduit en France à la fin du XIXe siècle, les postes des joueurs sont directement traduits des noms anglais. Ainsi, les joueurs chargés de défendre, c'est-à-dire d'empêcher les joueurs de l'autre équipe de marquer, prennent le nom d'arrières, de l'anglais back. Dans le schéma classique en 2-3-5, adopté des années 1880 aux années 1920, les équipes jouent avec deux arrières : un arrière droit, placé à droite du terrain, et un arrière gauche, placé à gauche du terrain. À la fin des années 1920, à la suite d'une modification de la règle du hors-jeu en 1925, de nombreuses équipes adoptent une formation en 3-2-5, dite WM, avec une ligne de trois arrières contenant un arrière droit, un arrière centre et un arrière gauche.À partir des années 1950, un nouveau dispositif est majoritairement adopté par les équipes, le 4-2-4. Un joueur supplémentaire est ajouté dans la ligne d'arrières, qui se compose alors d'une ligne de quatre joueurs : deux arrières latéraux (droit et gauche) et deux arrières centre. Leur rôle est toujours de défendre, et dans la terminologie, le nom du rôle des arrières centre va prendre le pas sur le nom de leur poste. Ainsi, dans le langage courant, l'arrière centre devient progressivement un défenseur central tandis que les arrières droit et gauche, eux, gardent leur nom, bien que leur poste n'ait plus rien à voir avec les arrières droit et gauche initiaux. Le terme défenseur remplace alors le terme d'arrière.Dans les années 1960, un système plus défensif appelé catenaccio, de l'italien cadenas, est proposé par plusieurs équipes, avec un cinquième défenseur ajouté derrière la ligne des quatre défenseurs. Il prend le nom de libéro, de l'italien libre. Ce système défensif tombe en désuétude pendant les années 1990, mais le terme libéro est resté et peut désigner ensuite le rôle d'un défenseur central qui dirige la défense et effectue des montées sur le terrain, au contraire du stoppeur, rôle de défenseur central davantage axé sur le marquage des attaquants adverses.Depuis les années 1970, les équipes jouent la plupart du temps avec quatre défenseurs (deux arrières latéraux et deux défenseurs centraux), mais peuvent aussi adopter une tactique plus défensive avec cinq défenseurs (deux arrières latéraux et trois défenseurs centraux), voire plus rarement une tactique à trois défenseurs (trois défenseurs centraux). Position Les défenseurs centraux occupent l'axe de la défense. Ils sont au nombre de deux ou de trois suivant l'organisation de l'équipe, soit alignés ou positionnés de manière que l'un d'entre eux occupe une position plus basse sur le terrain, le libéro. Dans ce dernier cas, on distingue deux types de défenseurs centraux : le « stoppeur » et le « libéro ».Les défenseurs modernes ne sont désormais plus caractérisés par les appellations de ""libéro"" ou ""stoppeur"", ou alors par erreur. Les quatre défenseurs jouent alignés, quand leur équipe n'a pas le ballon.L'expression ""charnière centrale"" désigne l'association des joueurs positionnés sur le terrain comme défenseurs centraux. Rôles Le « stoppeur » est un joueur caractérisé par ses capacités à empêcher un avant de pointe adverse à approcher des buts de son gardien. Il use du tacle pour prendre le ballon dans les pieds de son adversaire, de son jeu de tête pour empêcher les centres et les passes longues des milieux vers les attaquants et plus généralement de son physique pour stopper son adversaire. Le stoppeur est le joueur spécifiquement chargé de neutraliser l'avant-centre.Pendant une grande partie du XXe siècle, la défense par marquage individuel était prépondérante : chaque défenseur, à l'exception du libéro, se voit attribuer un attaquant qu'il suivra partout afin de gêner son jeu. Bien qu'efficace et simple à appliquer, cette option tactique montre cependant des faiblesses. Si un défenseur est battu, ce qui a de très fortes chances d'arriver pendant un match, l'attaquant qu'il marquait se retrouve seul et bénéficie donc d'une grande liberté d'action. De plus, les défenseurs « au marquage » ne peuvent pas participer au reste du jeu, au risque de s'éloigner trop de leur adversaire attitré.Dorénavant, la grande majorité des équipes pratique une défense dite « en zone », beaucoup plus flexible que le marquage individuel. Chaque défenseur couvre une partie du terrain, et il défendra contre l'adversaire qui s'y trouve. La défense en zone des premiers temps était rigide, les joueurs traçant des lignes mentales pour délimiter des endroits du terrain où ils n'allaient pas. Peu à peu, les entraîneurs ont enseigné une défense en zone intégrant les avantages de la défense individuelle. Les défenseurs sont alors amenés à se déplacer en bloc en fonction de la position du ballon et des adversaires. L'arrière droit se retrouve au marquage du joueur le plus à droite sur une action particulière, le défenseur central droit sur le deuxième le plus à droite, et ainsi de suite. Bien que très efficace, la défense en zone est particulièrement difficile à pratiquer, car elle nécessite plus de coordination, de concentration, de lucidité et d'intuition du jeu que le marquage individuel. L'entraînement collectif d'une défense consiste donc à cultiver les automatismes. C'est pourquoi beaucoup d'entraîneurs, une fois qu'ils ont trouvé leurs quatre (ou trois ou cinq) défenseurs, rechignent à en changer.Dans le jeu offensif, les défenseurs ont en général un rôle limité. Même s'ils peuvent se poster très haut sur le terrain, ils ne montent que rarement à l'attaque. Le risque d'un contre de l'adversaire est toujours possible, et un défenseur ne peut se permettre de monter pour ne pas risquer de perturber et d'affaiblir l'organisation défensive de son équipe. Néanmoins, tactiquement, il arrive que l'on laisse le droit à certains défenseurs centraux de monter. C'était très souvent le cas avec les libéros qui étaient des milieux reconvertis et qui donc possédaient des qualités pour jouer haut sur le terrain. Physionomie Physiquement, les défenseurs centraux sont en général plus grands et costauds que les autres joueurs, puisque ce sont souvent leurs capacités physiques qui leur permettent de prendre le dessus sur leurs adversaires. Dans le football moderne, ils mesurent souvent plus de 1,85 m. La taille est souvent avantageuse pour intercepter les ballons dans le jeu aérien ou couper les trajectoires des centres venus des ailes ou celles des passes longues en profondeur. Néanmoins, ce sont souvent leurs capacités à se placer et à travailler avec leurs coéquipiers de défense qui priment. Derniers remparts avant le gardien, leurs erreurs sont souvent lourdes de conséquences. Position Les deux arrières sont typiquement positionnés respectivement à droite et à gauche de la défense centrale (qu'elle soit composée d'un défenseur, comme cela était le cas auparavant, ou de deux comme le veut la norme au XXIe siècle). Ce positionnement nécessite une grande polyvalence en raison des tâches défensives et offensives que doit remplir un latéral, pouvant l'amener, selon les phases de jeu, à jouer successivement dans des positions de défenseur, de milieu ou d'ailier. En phase défensive, le latéral est souvent positionné le long de la ligne de touche, au contact de l'attaquant adverse. Toujours en phase défensive, lorsque le jeu est à l'opposé, il peut également se positionner en libéro de la défense car il dispose, à ce moment-là et en principe, de la meilleure vision d'ensemble. En phase d'attaque, il peut enfin se muer en contre-attaquant. Cette grande variété des positions qui peuvent être occupées par un latéral au cours d'un match suppose une grande versatilité des joueurs, souvent capables d'occuper d'autres postes. Rôles Les arrières latéraux ont pour rôle de protéger les côtés du terrain. Ce sont en général des joueurs rapides dont le rôle principal est d'empêcher l'adversaire (bien souvent l'ailier adverse) de déborder, de centrer ou de rentrer vers l'intérieur du terrain. Leur rôle n'est pas tant de récupérer le ballon dans les pieds de l'adversaire que de le gêner et de perturber ses transmissions de balle. À l'inverse des défenseurs centraux, les latéraux doivent souvent défendre en mouvement, sur des appels en profondeur par exemple, et se retrouvent rarement en position de dernier défenseur.Les arrières latéraux peuvent aussi avoir un rôle offensif même si cela n'est pas systématique. Pendant de longues années, les arrières latéraux ne se contentaient que de défendre et de fermer leurs couloirs. Depuis l'arrivée du système 4-2-4 et de ses successeurs au début des années 1960, le rôle des latéraux s'est accru d'une dimension offensive, à un degré variable selon la philosophie de jeu de l'entraîneur : ils doivent soutenir leur ailier afin de déborder ou d'apporter le surnombre. Dans ce rôle, ils évoluent souvent comme un second ailier et doivent faire preuve de qualités de dribbles et de centres. Giacinto Facchetti, le latéral gauche de l'Inter de Milan double champion d'Europe en 1964 et 1965, a été le premier grand exemple de ce nouveau type. On peut aussi citer l'Allemand (de l'Ouest) Manfred Kaltz dans les années 1980 et surtout le Brésilien Roberto Carlos dans les années 1990 comme prototypes du latéral moderne, dont l'entente avec son ailier est devenue une des clés de l'animation offensive d'une équipe. Le profil typique de l'arrière latéral moderne est un joueur rapide, bon centreur, capable de déborder ou de se replier très rapidement. Leur contribution aux phases aussi bien offensives que défensives font que ces joueurs parcourent souvent la plus grande distance lors d'un match. Ils sont aussi souvent ceux qui touchent le plus de ballons lors d'un match grâce à leurs nombreuses interventions, qu'elles soient dans leur moitié de terrain ou dans la moitié adverse. Néanmoins, le juste équilibre entre rôle défensif et rôle offensif n'est jamais parfait ; il existe des latéraux qui sont très offensifs et d'autres qui sont très défensifs. Physionomie Certains arrières latéraux peuvent être petits ou grands sans que cela soit préjudiciable à leur jeu. En effet, un latéral plutôt petit sera avantagé s'il doit monter pour participer à l'attaque quand un plus grand aura une meilleure présence physique et sera plus à même de gêner un attaquant de l'équipe adverse. Position Le piston est un rôle très spécifique, proche de celui d'arrière latéral, et souvent occupé par des défenseurs latéraux. Sa position est entre celle d'un milieu latéral et d'un arrière latéral dans une formation avec 3 défenseurs centraux tel qu'un 3-5-2. Cette formation, revenue en vogue dans les années 2010, voit notamment plusieurs joueurs classés à ce poste, comme Raphaël Guerreiro (football, 1993) avec le Borussia Dortmund ou Achraf Hakimi positionné à droite avec qui il était associé avec Raphaël Guerreiro (football, 1993) positionné à gauche lors de la saison 2019-2020 avec le Borussia Dortmund Rôles Le piston est souvent assimilé à un arrière latéral plus offensif que la moyenne, avec une grosse pointe de vitesse, une bonne qualité de centre et une constante envie d'aller vers l'avant sans pour autant délaisser ses tâches défensives dans le couloir. Physionomie Proche de l'arrière latéral, il possède des qualités semblables, même si la densité physique y est moins privilégiée par rapport à la finesse et la rapidité. Position Poste d'arrière central auparavant très populaire, il est tombé en désuétude depuis la fin des années 1990, la défense centrale à 2 étant devenue beaucoup plus utilisée entre-temps. Ce poste ressemble aussi à celui d'arrière central à l'époque où il n'y avait que trois défenseurs dans les lignes arrières, même si ce terme est désormais synonyme de défenseur central.Le libéro est le joueur de champ qui joue le plus bas sur le terrain, et est déchargé de tout marquage individuel. Parce qu'il a une position plus reculée et une meilleure lecture du jeu adverse, il est considéré comme le « patron » de l'animation défensive, qui peut transmettre les consignes tactiques : son placement était en couverture du stoppeur dans les systèmes homme à homme des arrières sur les avants. Rôles Il peut ""remonter"", c'est-à-dire s'avancer vers la cible adverse, en vue d'opposer une pression sur les avants de l'autre équipe pour les pousser au risque d'être en position de hors-jeu. Il est aussi le dernier recours parmi les joueurs de champ, venant suppléer un de ses partenaires débordés, ou anticipant une trajectoire pour intercepter le ballon. C'est un poste qui requiert une grande intelligence de jeu afin de savoir quelle est la bonne action à réaliser à un moment donné. Dans certains dispositifs tactiques, notamment en Allemagne, dans les années 1970 et 80, le libéro par sa liberté sur le terrain, avait souvent un rôle offensif et participait au jeu d'attaque, en montant aux avant-postes. Franz Beckenbauer fut même à ce poste et à son époque, le véritable meneur de jeu de l'équipe d'Allemagne. Physionomie  Franz Beckenbauer (1972 et 1976) Matthias Sammer (1996) Fabio Cannavaro (2006) Billy Wright (1957) Giacinto Facchetti (1965) Bobby Moore (1970) Franz Beckenbauer (1974 et 1975) Franco Baresi (1989) Roberto Carlos (2002) Virgil van Dijk  (2019) Karl-Heinz Schnellinger (1962) Franz Beckenbauer (1966) Ruud Krol (1979) Andreas Brehme (1990) Paolo Maldini (1994 et 2003)Dispositifs tactiques en football Portail du football"
sport;La flamme olympique (en grec : ????????? ????? / Olympiakí Flóga), appelée aussi torche olympique ou flambeau olympique bien que le Comité international olympique fasse une distinction entre ces termes, est un symbole olympique. Elle fait partie du cérémonial des Jeux olympiques : allumage puis relais de la flamme olympique, le dernier relayeur faisant le tour du stade avant de rejoindre une vasque (ou « chaudron olympique ») qu'il embrase grâce à sa torche.La chorégraphie et les costumes de la cérémonie actuelle existent depuis les Jeux olympiques d'été de 1936. Ils s'inspirent de l'Antiquité : dans la Grèce antique, le feu sacré brûlait en permanence dans les sanctuaires, son allumage étant réalisé par un miroir parabolique, le skaphia, qui concentrait les rayons du soleil. Au sanctuaire d'Olympie qui accueillait les Jeux olympiques antiques, une flamme brûlait en permanence sur l'autel de l'Héraion, temple d'Héra. De même une flamme était placée au milieu des sites sportifs et du banquet offert dans le Prytanée aux vainqueurs des Jeux.La flamme olympique des Jeux olympiques modernes est allumée au cours d'une cérémonie par des femmes, jouant le rôle de prêtresses d'Héra, vêtues de tuniques similaires à celles portées par les Grecs de l'Antiquité. La cérémonie se déroule, plusieurs mois avant le début des Jeux, sur les ruines du temple d'Héra à Olympie, en Grèce, à l'aide de rayons du soleil concentrés par un miroir parabolique (par précaution, s'il n'y a pas de soleil le jour de cette cérémonie officielle, la flamme est allumée selon le procédé antique du miroir, plusieurs jours avant, un jour de soleil). Les prêtresses, autour de l'autel, invoquent Apollon. La flamme sacrée est alors placée dans une urne en céramique qui est transportée dans l'ancien stade d'Olympie au cours d'une procession qui passe devant un olivier sauvage dont un rameau, symbole de paix et récompense du vainqueur des Jeux, est coupé. La grande prêtresse allume la torche et la remet au premier relayeur. Plusieurs autres relayeurs la transportent jusqu'au Stade panathénaïque qui a accueilli les Jeux olympiques d'été de 1896. Le Comité olympique hellénique qui avait la responsabilité des relais jusqu'à ce stade passe lui-même le relais au Comité d'organisation des Jeux Olympiques (COJO) du pays hôte.Chaque participant (sélectionné en raison de son « accomplissement personnel » ou de sa contribution à la vie locale) porte ensuite le plus souvent à pied la torche ou le flambeau olympiques (ou leurs répliques) sur une courte distance et la remet à un autre porteur. Le relais de la flamme olympique prend fin lors de la cérémonie d'ouverture des jeux au cours de laquelle traditionnellement le dernier porteur, généralement un champion ou un jeune sportif du pays organisateur des jeux, allume de façon spectaculaire et originale avec sa torche une vasque ou un chaudron monumental, lequel brûle pendant toute la durée des jeux. Le choix de ce dernier porteur est en principe gardé secret jusqu'à la dernière minute.La flamme est finalement éteinte lors de la cérémonie de clôture finale.La flamme olympique a brûlé pour la première fois le 28 juillet 1928 lors des Jeux olympiques d'été de 1928, à Amsterdam. Il n'y avait pas encore de relais pour porter la torche.Sur une idée attribuée à Carl Diem et retenue par Adolf Hitler, inspirée des lampadédromies antiques, le premier relais avec la torche a eu lieu lors des Jeux olympiques d'été de 1936 à Berlin, dans le but de glorifier le Troisième Reich. Depuis, le relais et l'allumage de la flamme ont eu lieu à chaque olympiade.Le long passage de la flamme olympique est parfois l'occasion de manifestations politiques ou sociales dirigées contre le pays organisateur. Ainsi, le passage de la flamme en 2008 à Istanbul, Londres, Paris, San Francisco, etc. fut le prétexte de manifestations pour les droits de la personne concernant la controverse tibétaine. Similairement, le passage de la flamme olympique des jeux de 2010 à Vancouver fut le prétexte de manifestations pour les droits de la personne concernant la situation des peuples autochtones du Canada.La flamme des Jeux olympiques d'hiver a été allumée pour la première fois pour les Jeux olympiques d'hiver de 1952 à Oslo. À cette occasion, la flamme a été allumée dans la maison de Sondre Norheim, pionnier norvégien des sports d'hiver.Ainsi, depuis 1952, tous les 4 ans, puis tous les 2 ans, la flamme est allumée à Olympie grâce à l'énergie solaire puis transportée de ville en ville jusqu'à la cérémonie d'ouverture.Parfois les torches sont fabriquées pour chacun des relayeurs qui peuvent ensuite les racheter et les revendre.En raison de la pandémie de Covid-19, la cérémonie d'allumage de la flamme olympique est exceptionnellement tenue à huis-clos pour les Jeux olympiques d'été de 2020 prévus à Tokyo.À quelques rares occasions, la flamme olympique s'est éteinte de façon fortuite ou provoquée. Elle fut à chaque fois rallumée par une des lanternes contenant la « flamme-mère », une flamme « de secours » règlementaire issue d'Olympie :En 1976, à Montréal, un orage violent éteignit la flamme pendant le déroulement des Jeux, quelques jours après l'ouverture. La flamme fut d'abord rallumée par un organisateur présent, à l'aide d'un simple briquet. Elle fut ensuite éteinte volontairement afin d'être correctement rallumée par la flamme de secours règlementaire.En 2004, au Stade panathénaïque, un vent violent éteignit la flamme alors que Yánna Angelopoúlou-Daskaláki, membre du Comité d'organisation, tentait de l'allumer pour le départ nocturne d'un grand relais de 78 000 kilomètres.En 2008, à Paris au cours du parcours des jeux olympiques de Pékin, la torche fut volontairement éteinte à trois ou cinq reprises par les organisateurs chinois des Jeux olympiques, à cause de manifestations de protestation pour les droits de l'Homme en Chine. Par contre, la flamme est restée allumée à l'abri dans sa cage transportée par bus. Le relais de la flamme dut être écourté, plusieurs relayeurs délaissés et son transport vers le stade Charléty s'acheva en autobus, avant son départ pour San Francisco, l'étape suivante.Aux Jeux olympiques d'été de 2012, elle s'est éteinte accidentellement pendant l'un des relais qui la menaient à Londres. Lors de ces mêmes Jeux, elle a été éteinte de façon volontaire le lendemain de la cérémonie d'ouverture afin de transporter la vasque du centre du stade jusqu'à son emplacement définitif, au pied de l'un des virages où elle a été rallumée de façon réglementaire.Depuis l'origine du parcours de la flamme, certains pays organisateurs et participants ont innové en matière de moyen de transport :En 1976, la flamme olympique a été transformée en signal radio. Le signal a été transmis depuis Athènes jusqu'au Canada où il a servi à rallumer une autre flamme au moyen d'un rayon laser.En 1996, la flamme olympique a été embarquée à bord de la navette Columbia lors de la mission STS-78 pour son 1e vol dans l'espace.En 2000, la torche olympique a été transportée sous l'eau par des plongeurs, au voisinage de la Grande barrière de corail.La même année, d'autres étapes de transport originales ont été réalisées à l'aide d'un canoë amérindien, d'un chameau et d'un avion Concorde.En 2004, une course par relais de 78 jours fut organisée. La torche a parcouru 78 000 kilomètres, en passant entre les mains de 11 300 porteurs de torche.En 2008, la flamme olympique a été emmenée jusqu'au sommet de l'Everest, à 8 849 mètres d'altitude, protégée du manque d'oxygène par une lampe de mineur spéciale.En 2013, la flamme a fait un voyage à bord d'un vaisseau Soyouz jusqu'à la Station spatiale internationale (ISS).                                                                                                                                                                                                           La torche de 2012 créée par Edward Barber et Jay Osgerby, designers britanniques, présente « des lignes moyenâgeuses sous un alliance d'aluminium doré criblé de 8 000 cercles découpés au laser, symbolisant à la fois les anneaux olympiques et les 8 000 relayeurs qui se [succédèrent] feu au poing jusqu'à l'ouverture officielle des Jeux le 27 juillet ». La géométrie trilatérale anguleuse rappelle que la ville de Londres a organisé trois fois les Jeux (1908, 1948 et 2012). Elle a été testée dans la soufflerie du constructeur automobile BMW pour s'assurer qu'elle ne s'éteindrait pas, et cela à des conditions extrêmes : « douches d'eau et trombes de neige, amplitudes insolentes de températures (entre -5 et +30 °C), tourbillons de vents à 8 kilomètres à l'heure… La flamme a tenu bon, mais néanmoins montré quelques faiblesses en début de parcours, dans le Devon, en raison d'un brûleur défectueux ». Quoi qu'il en soit, il existe une « flamme mère » conservée à Athènes, et qui est transportée dans des lanternes désignées au cas où il faudrait rallumer la torche.                                                                                                            Serment olympiqueRelais de la flamme olympique 2008 Portail des Jeux olympiques
sport;"Le football américain est un sport collectif opposant deux équipes de onze joueurs qui alternent entre la défense et l'attaque. Le but du jeu est de marquer des points en amenant le ballon dans la zone d'en-but adverse. Pour conserver la possession, l'équipe attaquante doit parcourir au moins 10 yards en 4 tentatives (appelées « down »). Dans le même temps, l'équipe en défense doit empêcher l'attaque d'atteindre cet objectif, dans le but de reprendre la possession du ballon. Si l'équipe attaquante valide 10 yards ou plus lors de sa possession, elle bénéficie de quatre nouvelles tentatives pour continuer à gagner du terrain. Sinon, la possession du ballon change de camp et les rôles, défense/attaque, s'inversent.Les points peuvent être marqués de différentes façons : en franchissant la ligne de but avec le ballon, en lançant le ballon à un autre joueur situé de l'autre côté de la ligne de but, en plaquant le porteur du ballon de l'équipe adverse dans sa propre zone d'en-but (safety) ou en tirant au pied le ballon entre les poteaux du but adverse. Le vainqueur est l'équipe ayant marqué le plus de points à la fin du match.Aux États-Unis et au Canada (y compris au Québec), le football américain (ainsi que son pendant canadien) est simplement appelé football. Par contre, le sport dénommé football au niveau mondial y est appelé soccer.Le football américain descend du football association (soccer) et du rugby. Ces deux disciplines codifiées en Angleterre sont introduites en Amérique du Nord dès 1861 pour le football association (soccer) et en 1864 ou 1865 pour le rugby. Les premiers matchs de rugby joués sur le continent nord américain sont probablement disputés en 1864 à Toronto ou en 1865 à Montréal. Cette branche canadienne donnera naissance au football canadien.Aux États-Unis, les équipes universitaires pratiquent aussi le rugby à XV et le football association (soccer). C'est d'ailleurs l'occasion de confusions. Ainsi, pendant longtemps, on considéra que le premier match de football américain se tint le 6 novembre 1869 : Rutgers s'impose 6-4 face au College of New Jersey (futur Princeton). Selon les recherches de l'historien Stephen Fox, ce match de New York Ball pourrait être un match de football association (soccer). Princeton et la NFL admettent désormais cette version.Les Canadiens proposent aux Américains un match universitaire de rugby opposant les Montréalais de l'Université McGill aux étudiants de Harvard qui ne pratiquaient pas du tout le football. On joue une mi-temps selon les règles de chaque université. En effet, chaque école possède ses règlements propres, comme cela était également le cas en Europe quelques années plus tôt. Harvard adopte en 1875 comme règlement celui édicté par Yale, mais il s'agit toujours d'un mélange entre rugby et football association (soccer), nommé combination en Europe. Il faut attendre le 23 novembre 1876, et la fameuse Massasoit Convention, pour voir la mise en place de règles communes à plusieurs universités américaines. De fait, ils adoptent l'ensemble des règlements de la fédération anglaise de rugby à l'exception d'un détail, le comptage des points. Le football américain clairement différent de ses ancêtres reste encore à créer.Entre 1880 et 1883, Walter Camp, entraîneur de Yale, modifie en profondeur les règles et l'esprit du jeu : réduction des joueurs de 15 à 11, réduction de la surface du terrain et introduction du scrimmage. Ces modifications de Walter Camp sont complétées par d'autres à la même période : une équipe doit rendre le ballon à l'adversaire si elle n'est pas parvenue à progresser de cinq yards en trois tentatives. Le football américain est né.Les premiers joueurs professionnels sont recensés en 1892-1893 mais il faut attendre 1896 pour voir la première équipe composée exclusivement de joueurs professionnels : « Alleghany Athletic Association ». Un an plus tard, « Latrobe Athletic Association » est la première équipe à boucler une saison complète avec une formation composée exclusivement de joueurs professionnels (1897).La première passe complétée date du 27 octobre 1906. Avant cette date, les passes en avant n'étaient pas autorisées.La National Football League (NFL) se met en place en 1920 afin de mettre un terme à la gabegie qui règne alors dans le monde du football américain.Le football américain reste longtemps cantonné aux seuls États-Unis où il devient le sport numéro un, devant le baseball, dès les années 1970. Exception notable, le Japon, influence américaine oblige, où se dispute un championnat universitaire depuis 1947. Le jeu est introduit en Europe avec l'arrivée des troupes américaines dans la première guerre mondiale en 1917, et notamment en France avec la création d'un premier championnat de France entre les différents régiments américains présents, remporté en 1918 par l'équipe de la base navale de Pauillac en Gironde.Néanmoins il ne connaîtra pas de succès populaire hors du sol américain avant la fin des années 1970 et le début des années 1980. Une fédération internationale de football américain est créée en 1998.Bien qu'il ne semble pas y avoir de filiation entre les deux sports, la forme moderne du football américain est très proche du rugby à XIII. En effet, la règle des quatre downs est équivalente aux cinq tenus ou tombés terme qui traduit exactement down du rugby à XIII. La seule différence fondamentale est que le quarterback l'homologue du « demi de tenu » en rugby à XIII est autorisé à effectuer une passe en avant. Enfin, il est important de noter que le quarterback n'est pas le seul joueur autorisé à faire une passe vers l'avant, tout joueur pouvant recevoir une passe, peut également en faire une.La plupart des ligues féminines de football américain jouent avec les mêmes règles que celles suivies par les joueurs masculins, à une exception près : les ligues des femmes utilisent un ballon plus petit, adapté à des mains plus petites. Il existe par ailleurs un dérivé du football américain, le flag football, réputé comme plus facile d'accès aux femmes, le contact physique y étant moins important.Les règles telles qu'elles sont présentées ici correspondent à celles du championnat professionnel américain (NFL). Elles diffèrent parfois sensiblement suivant les compétitions. En Europe, le football américain se pratique ainsi en s'inspirant plutôt des règles en vigueur dans le championnat universitaire américain de la NCAA.Le terrain mesure au total 120 yards (110 m) de longueur sur 53,33 yards (48,8 m) de largeur. Les règles américaines de la National Football League précisent les dimensions en pied : 360 pieds par 160.À chaque extrémité du terrain, on trouve la zone d'en-but appelée end zone et, au fond de celle-ci, les poteaux entre lesquels le ballon doit passer pour le botté de transformation après un touchdown ou le botté de placement, field goal. La zone d'en-but mesure 10 yards (9,144 m) de longueur après la ligne d'en-but et ce, sur toute la largeur du terrain. Il est courant de voir les zones d'en-but peintes aux couleurs de l'équipe jouant à domicile.Le terrain est divisé en portions de 5 yards (4,6 m) représentées par des traits de peintures blanches pour mieux visualiser les distances parcourues. Tous les 10 yards (9,1 m), et jusqu'au centre du terrain, la distance séparant la ligne de l'en-but est marquée en police blanche.Au centre du terrain et à chaque yards, deux traits hachurés appelés hashmarks sont disposés. Ces hashmarks, distants de 5,6 m, délimitent la largeur maximale autorisée pour les mises en jeu afin d'éviter que ces dernières se déroulent trop près des lignes de touche.Au fond de chaque zone d'en-but, un poteau jaune est dressé au milieu de la largeur du terrain. Ce poteau est complété par une barre horizontale située à 3,05 m de hauteur et elle-même complétée par deux poteaux verticaux distants de 5,6 m. Ces barres portant la hauteur totale des poteaux à 9 m, au minimum. Contrairement au rugby où les poteaux forment un H, au football américain, les poteaux ont la forme d'un Y.En France, il y a plusieurs catégories au football américain (citées dans l'ordre ci-dessous) :Les minimes (sans contact, pour les minimes et les plus jeunes, certains clubs proposent une « école de foot » pour leur permettre d'apprendre les règles et de se préparer à rentrer en cadet sans contact.) ;Les cadets (14-16 ans) ;Les juniors (16-19 ans) ;Les seniors (à partir de 20 ans)Et les vétérans (pas d'âge précis, le championnat de Vétérans n'existant pas, de nombreux clubs possèdent une équipe B pour permettre aux vétérans d'avoir du temps de jeu avec l'esprit de compétition en moins).Pour le Canada, voir Catégories juvéniles au football canadien.La multiplication des accidents souvent mortels, à la fin du XIXe et au début du XXe siècle, poussèrent les autorités à rendre les protections obligatoires au niveau amateur. Chaque joueur porte aujourd'hui l'équipement de protection suivant :un casque muni de grilles (facemask),une épaulière (shoulder pad),un protège-dents (mouthpiece),des protections sur les cuisses, les hanches, le coccyx, ainsi que sur les genoux (optionnel chez les professionnels).Le casque est l'élément le plus important et le plus lourd de l'équipement, il est composé d'une coque en plastique garnie de chambres à air et de pièces en mousse pour les bas de gamme et d'autres matières plus sécuritaires pour les haut de gamme. Le casque doit tenir parfaitement le crâne du joueur qui veille à le tenir serré grâce à la mentonnière et aux chambres à air. L'utilisation d'un protège-dents est obligatoire.La grille du casque faite d'acier plastifié permet la protection du visage jusqu'au menton. Elle doit donc être solide tout en laissant un maximum de visibilité. Plus le poste d'un joueur est exposé aux chocs, plus sa grille sera protectrice. En effet, les grilles des casques sont différentes selon les postes occupés. Ouvertes pour les quarterbacks et les receveurs qui ont besoin d'avoir une bonne vision du jeu, elles sont plus serrées pour les joueurs de ligne.Certains joueurs portent des gants spécifiques à leur poste. Par exemple, un joueur de ligne (lineman) aura des gants renforcés aux articulations et sur le dessus de la main tandis qu'un wide receiver (receveur éloigné) aura des gants plus fins avec une meilleure adhérence pour mieux capter le ballon. Il faut aussi de bonnes chaussures de football américain (chaussures à crampons ; crampons moulés ou vissés ou encore incrusté). Depuis l'année 2012, la marque Nike a racheté l'ensemble des droits d'équipement à Reebok.Le ballon est de forme ovale comme au rugby, mais légèrement plus petit (30 cm de long et 56 cm de circonférence) et plus léger (397  à   425 grammes), permettant au quarterback de le lancer très loin et rapidement à une seule main. La pression intérieure du ballon doit être de 0.86-0.93 atm. Il comprend un lacet permettant une meilleure prise en main par le quarterback et lui permettant d'imprimer un mouvement rotatif qui stabilise et allonge sa trajectoire. Il est également appelé pig skin, littéralement « peau de cochon », matière dont il était fait à l'origine, aujourd'hui il est fait en cuir de vache.Enfin, certains joueurs portent du eye black, une graisse noire appliquée sous les yeux pour réduire l'éblouissement du soleil ou des projecteurs. Durée La partie dure 60 minutes effectives, pour environ 11 minutes de temps d'actions de jeu — mais près de trois heures et demi au total à cause des interruptions — et comporte quatre quart-temps de 15 minutes. Entre le premier et le deuxième quart-temps, on procède juste à un changement de côté et on garde la même position relative pour le ballon. En revanche, entre les deuxième et troisième quart-temps (première et seconde mi-temps), c'est le repos d'environ 13 minutes. Au début du troisième quart-temps l'équipe qui a reçu l'engagement au premier quart-temps engage à son tour. En cas d'égalité, une prolongation était jouée sur le mode de la « mort subite » jusqu'à la saison 2009-2010. Lors de la réunion annuelle de la NFL pendant l'intersaison de mars 2010, la règle est changée, notamment à la suite de la finale de conférence NFC des derniers playoffs qui vit les Saints battre les Vikings avec un field goal en prolongation. Désormais l'équipe qui a la possession en premier pendant la prolongation ne peut plus gagner sur un field goal. Une équipe commençant les prolongations avec la possession du ballon ne peut gagner le match sans possibilité de réplique qu'en inscrivant un touchdown. Si elle n'inscrit qu'un field goal, l'autre équipe prend le ballon à son tour et peut tenter d'égaliser par un field goal ou gagner en inscrivant un touchdown. Phases de jeu Un match de football américain se déroule en deux phases de jeu bien distinctes : l'attaque et la défense. Le changement entre les deux phases s'opère à plusieurs occasions détaillées plus bas.Le jeu commence par un botté d'engagement ou kickoff. Ce coup de pied est utilisé en entame de chaque début de mi-temps (premier et troisième quart-temps) ou pour reprendre le jeu après qu'une équipe a marqué des points. Il est, dans la plupart des cas, utilisé pour envoyer le ballon le plus loin possible dans le camp adverse. Un joueur de l'équipe adverse se doit de rattraper le ballon et de tenter d'avancer ballon en main. L'endroit où le joueur remontant le ballon sera plaqué déterminera le lieu où la phase d'attaque débutera pour son équipe. Quand le ballon atterrit dans la zone de l'en-but de l'équipe qui va passer en attaque, le joueur qui reçoit le ballon peut soit décider de tenter de jouer le ballon, soit se mettre à genoux pour arrêter le jeu. La phase offensive commencera alors sur la ligne des 25 yards de l'équipe - on parle alors de touchback.L'équipe possédant le ballon est en phase d'attaque et dispose de quatre tentatives pour parcourir une distance de 10 yards - aussi appelés verges au Canada, un yard fait environ 0,91 m - à partir de la ligne du début de la phase d'attaque dénommée line of scrimmage. L'équipe en attaque va alors progresser en gagnant 10 yards par 10 yards, on dit parfois que le football américain est un jeu de « gagne-terrain ». Si l'équipe en attaque recule elle devra regagner le terrain perdu et ainsi que les 10 yards classiques pour avoir une nouvelle série de quatre tentatives (downs).Si, à l'issue des quatre tentatives, l'équipe n'a pas parcouru la distance des 10 yards (ou plus si elle avait reculé), la possession du ballon est donnée à l'adversaire. L'adversaire récupère alors le ballon à l'endroit où l'attaque s'est arrêtée : c'est un turnover. Pour éviter que l'équipe adverse ne récupère le ballon trop près de la zone d'en-but et se trouve donc dans une situation de marquer trop favorable, il est possible de taper un coup de pied afin de dégager le ballon. Ce coup de pied de dégagement est appelé punt (aussi appelé botté de dégagement au Canada). Il est généralement effectué lors de la quatrième tentative.La phase d'attaque débute avec le ballon tenu par le centre de la ligne offensive qui le passe entre ses jambes au quarterback. Deux moyens de progression se présentent alors aux joueurs de l'équipe offensive :la course : le ballon est passé directement de la main à la main par le quarterback aussi appelé le quart-arrière au Canada à un autre joueur appelé running back (demi au Canada). Celui-ci doit alors courir avec le ballon en évitant les défenseurs adverses. Il existe de nombreuses variantes de ce type d'attaque : le quarterback peut par exemple courir avec le ballon sans faire de passe. Dans certaines formations (Wildcat), le ballon peut être transmis directement vers le running back sans passer via le quarterback.la passe en avant : Il ne peut pas y avoir plus d'une seule passe en avant effectuée lors d'une phase de jeu; c'est généralement le quarterback qui lance cette passe. Le quarterback lance une passe à un de ses receveurs (ou autres joueurs éligibles à recevoir une passe) qui se sera, au préalable, déplacé sur le terrain en suivant une trajectoire bien déterminée. Cette trajectoire, connue à l'avance des deux joueurs, permet, outre la synchronisation entre passeur et receveur, de déstabiliser la défense en utilisant des trajectoires atypiques. Une fois le ballon attrapé par le receveur, celui-ci peut continuer à courir ballon en main. Pour qu'une passe soit valide, le ballon doit être attrapé sans qu'il touche le sol par un joueur ayant ses deux pieds (en NFL, un pied en NCAA) à l'intérieur des limites du terrain. Si ce n'est pas le cas, la passe est dite « incomplète ».la passe latérale : cette tactique, qui rappelle le rugby, est très rare car très risquée. Le nombre de passes est illimité à condition que les passes soient uniquement vers l'arrière ou latérale.En phase de défense plusieurs méthodes peuvent être employées pour stopper la progression du ballon :Le plaquage : le but du plaquage est de mettre au sol le porteur du ballon. Le jeu s'arrête dès que le porteur du ballon est au sol, la prochaine tentative d'attaque débutera à l'endroit où le joueur a été plaqué. Seul un joueur porteur du ballon peut être plaqué. Un plaquage effectué sur le quarterback est appelé sack. Si le plaquage a lieu dans la zone d'en-but, cela rapporte 2 points à l'équipe défensive, (on parle alors de safety). Si le joueur plaqué perd le ballon, le ballon peut être récupéré par l'une des deux équipes. Cet événement particulier, pouvant donner lieu à un renversement (turnover) si la défense récupère le ballon, est appelé fumble. Si un joueur est plaqué à l'intérieur du terrain, le chronomètre continue. S'il sort ou est poussé en touche, le chronomètre s'arrête. Cela a un impact très important sur la stratégie du jeu, en attaque.L'interception : elle a lieu lorsqu'un défenseur intercepte une passe destinée à un receveur, donc en attrapant le ballon destiné à un attaquant. Le défenseur ayant réalisé l'interception peut progresser ballon en main jusqu'à ce qu'il soit plaqué. Son équipe débutera alors sa phase d'attaque à l'endroit où le plaquage a eu lieu.Touchdown (6 points) Il a lieu lorsqu'un joueur est en possession du ballon à l'intérieur de la zone d'en-but de l'équipe adverse. Il suffit qu'une partie du ballon traverse une ligne imaginaire au-dessus de la ligne d'en-but pour que le touchdown soit validé sur un jeu de course ou qu'un joueur reçoive le ballon dans la zone avec l'un des deux pieds ou les deux pieds au sol (selon le championnat).Try for extra point (1 ou 2 points)Un touchdown donne lieu à une tentative de transformation, appelée try, extra-point attempt ou plus communément PAT (abréviation de Point After Touchdown) et peut être effectuée de deux manières :L’extra point (1 point) consiste à botter le ballon entre les deux poteaux, à la manière d'un field goal. Le ballon est placé sur la ligne des 15 yards depuis 2015.La two-point conversion (2 points) consiste à marquer l'équivalent d'un touchdown. Cette tentative est jouée à 2 yards de la ligne d'en-but en NFL ou à 3 yards dans les autres championnats. Le taux de succès de la two-point conversion étant proche des 40 % en NFL et en ligue universitaire, ce type de transformation est bien plus risqué que l'extra point ; de ce fait, elle n'est généralement utilisée que dans des cas particuliers où marquer un seul point supplémentaire ne suffirait pas.Si l’attaque perd le ballon à la suite d'un coup de pied bloqué lors d'une tentative d'extra point ou à la suite d'une interception ou d'un fumble lors d'une tentative de two-point conversion, la défense peut marquer deux points en retournant le ballon dans la end-zone adverse.Field goal (3 points)Un coup de pied pour être validé doit passer entre les deux barres verticales du but. Si le coup de pied est raté, la possession du ballon est donnée à l'équipe adverse à l'endroit où le coup de pied a été frappé.Safety (2 points) Safety (points)Un safety se produit dans la zone d'en-but de l'équipe se trouvant en possession du ballon. Il est accordé si le porteur du ballon est plaqué dans sa propre zone d'en-but ou sort des limites de celle-ci ou encore si une faute d'attaque est commise dans cette zone. Fautes et pénalités Au football américain, les pénalités sanctionnent les fautes commises sur le terrain avant, pendant ou après la phase de jeu. La sanction est une perte de yards avec 5 ou 10 yards ou plus de perdus et/ou une perte de tentative (down). Les pénalités sont signalées par des flags (mouchoirs) jaunes que l'un des sept arbitres (en NFL) jette par terre quand il voit une faute. L'arbitre principal, reconnaissable à sa casquette blanche, communique alors par gestes avec les joueurs et le public pour indiquer la nature de la faute et par conséquent le nombre de yards perdus.Les fautes les plus courantes sont les départs anticipés, les contacts illégaux (dans le dos par exemple) - appelés aussi illegal blocks, la saisie de la grille du casque - face mask - et l'interférence de passe - pass interference.Bien qu'il n'y ait que 11 joueurs de chaque camp sur le terrain pendant chaque phase de jeu, une équipe comporte un grand nombre de joueurs. Le roster (effectif) comporte 53 joueurs (en NFL où c'est une limite maximum). Les substitutions sont illimitées entre chaque jeu. Toutefois, il ne peut y avoir de substitution en défense (hors blessure) que si l'attaque fait elle même au moins un changement. Une équipe de football américain compte donc plusieurs formations spécialisées adaptées aux différentes phases de jeu. Ces formations sont composées de joueurs différents bien que certains puissent faire partie de plusieurs unités : il est peu commun qu'un joueur d'attaque joue aussi en défense et vice versa. Les équipes spéciales, elles sont composées pour l'essentiel des joueurs ayant un rôle mineur provenant de l'attaque et la défense. La défense L'équipe mise en place en défense peut être divisée en trois rideaux principaux :Premier rideau : ce rideau est composé des joueurs de ligne défensive. Les defensive ends (D-end), ou ailiers défensifs, et les defensive tackles (D-T), ou plaqueurs défensifs. Ce rideau a pour double objectif de stopper les courses et d'empêcher le quarterback de passer le ballon.Deuxième rideau : le rôle de ce rideau de défense, évoluant entre les lignes de défense longue et courte, est mixte. Les joueurs de cette ligne appelés linebackers, ou seconders, doivent en effet suppléer la première ligne en stoppant les courses et la troisième ligne en intervenant sur les passes.Troisième rideau : les arrières défensifs, ou defensive backs. Leur rôle est de contrecarrer le jeu de passe de l'adversaire. Les joueurs faisant partie de ce rideau sont des cornerbacks, ou demis de coin, et des safeties, ou maraudeurs, ces derniers faisant office d'ultimes défenseurs. L'attaque L'attaque d'une équipe est composée :d'un quarterback (QB), appelé aussi « quart-arrière » au Canada, qui est celui qui dirige l'offensive et appelle les jeux. C'est le premier joueur à toucher au ballon, après le centre. Il doit avoir une bonne vision du jeu, un excellent leadership ainsi qu'un calme et un sang-froid à toute épreuve. Le quarterback n'est pas le seul joueur autorisé à faire une passe en avant : tous les joueurs peuvent le faire, sauf le centre, tant qu'ils ne franchissent pas la ligne d'engagement.de porteurs de ballon - running backs : rapides et puissants, leur but est de franchir le rideau défensif adverse. Il existe deux postes de running backs : le demi-arrière nommé halfback (HB), très rapide, et le fullback (FB) plus puissant mais moins rapide en règle générale que le halfback, il est également utilisé comme nettoyeur de la défense devant le halfback.des ailiers éloignés - wide receivers (WR) - aussi appelé receveurs. Ils doivent être très rapides, agiles et adroits afin de recevoir les longues passes du quarterback et parcourir un maximum de distance ballon en main ; on parle alors de yards after receiving.des ailiers rapprochés - tight ends (TE), joueurs grands, puissants et rapides, ils sont très polyvalents et peuvent se muer en receveur comme en bloqueur.de cinq hommes de ligne offensive - offensive linemen (OL). Ce sont des travailleurs de l'ombre imposants qui protègent le quarterback des défenseurs qui pourraient le menacer et qui ouvrent des brèches dans la défense pour les porteurs de ballon. L'un d'entre eux, le centre, a pour fonction supplémentaire de transmettre le ballon au quarterback au début de chaque phase de jeu - on parle de snap. La ligne offensive est assujettie à des règles particulières qui restreignent leur placement, leur numérotation, ainsi que la possibilité pour eux de réceptionner une passe avant. Il leur est interdit de recevoir le ballon. Ce sont des joueurs capitaux pour le bon fonctionnement d'une attaque, mais ils sont généralement méconnus du public. Cette ligne est composée du centre, des offensive tackles et des offensive guards. Les unités spéciales Ces unités entrent en jeu à l'occasion de situations bien spécifiques. Il existe des formations différentes pour effectuer les bottés d'engagement (kick offs), de dégagement (punts) ainsi que les placements (bottés de trois points, ou field goals). Certains joueurs ultra spécialisés n'entrent sur le terrain que pour ces phases de jeu. Il s'agit notamment du punter qui effectue les bottés de dégagement, du kicker, spécialisé dans les coups de pied placés (transformations et field goals) et du long snapper, qui remplace le centre dans certaines de ces phases de jeu. Une special team défensive est aussi mise en place à l'occasion de chacune de ces situations. Elles agissent pour empêcher l'équipe adverse de marquer ou pour remonter le ballon le plus efficacement possible dans le cas d'un botté d'engagement ou de dégagement.Le caractère extrêmement haché du jeu, le principe du gagne-terrain et les remplacements illimités ont conduit, au fil des années, les entraîneurs à développer des schémas tactiques (en) d'une grande sophistication. Les développements de l'analyse vidéo et de l'informatique ont accentué ce phénomène. Le football américain est souvent comparé à une forme de jeu d'échecs dont les pièces seraient vivantes, et bien des subtilités échappent au spectateur même averti. Philosophies stratégiques Pour construire ce qu'ils appellent leur « système », les entraîneurs sont amenés à faire des choix sur leurs priorités offensives, défensives et sur les équipes spéciales. Faut-il privilégier la possession du ballon, ou jouer avec la position sur le terrain ? En attaque, utilisera-t-on le jeu de passe avant pour libérer ensuite des espaces pour la course, ou au contraire le jeu sera-t-il basé sur des jeux au sol avec de temps en temps des feintes menant à des passes avant (play action) ? En défense, on peut choisir un système risqué mais pouvant rapporter gros consistant à envoyer des défenseurs supplémentaires mettre la pression sur le quarterback (blitz), ou bien on peut décider de laisser l'adversaire gagner du terrain petit à petit sans en concéder de larges portions, en comptant sur le fait qu'il finira par faire une erreur (« Le roseau plie mais ne rompt pas »). Enfin des choix s'imposent aussi quant aux unités spéciales. Quand faut-il dégager en 4e tentative, quand faut-il tenter le tout pour le tout ? Formations offensives La règle du football américain impose que sept joueurs au minimum soient positionnés juste derrière la ligne de scrimmage. Les autres doivent être en retrait d'au moins une verge (1 yard), positionnés dans l'arrière-champ. Ce n'est pas sans conséquences tactiques car seuls ces derniers ainsi que les joueurs situés aux extrémités de la ligne sont autorisés à réceptionner une passe avant.Suivant le système de jeu offensif, l'entraîneur positionnera donc les joueurs afin d'optimiser le résultat. Tactiques de jeu en attaque On distingue grossièrement trois façons de faire avancer le ballon au sol (jeu de course) : en puissance (en utilisant la force des porteurs de ballon et de la ligne offensive), en rapidité (en jouant si rapidement que le second rideau n'a pas le temps de réagir) ou en finesse (en cachant le ballon, en multipliant les feintes).De la même façon, il existe plusieurs types de passes avant : le plus classique est le drop back qui voit le QB reculer de quelques pas en regardant devant lui (il lit la défense) avant de s'arrêter et de lancer sa passe. Les hommes de ligne forment alors une poche pour le protéger. Le QB peut aussi partir directement vers l'extérieur (roll out), il devient plus menaçant pour la défense puisqu'il peut décider de passer ou de courir lui-même. Cette tactique de jeu est particulièrement efficace quand elle est combinée avec une feinte de course à l'opposé (bootleg). Les inconvénients pour l'attaque sont que la précision du QB est moindre puisqu'il court, il est plus exposé à la défense, et il ne peut plus jouer sur l'intégralité du terrain car une passe « en travers du terrain » serait hasardeuse.Enfin, une autre catégorie de passes est particulièrement efficace contre les défenses agressives, il s'agit de la screen pass. Dans ce type de passe, le QB recule exagérément tandis que la ligne offensive laisse pénétrer les défenseurs et se place devant un receveur ou un porteur de ballon qui se sera démarqué. Juste avant de se faire sacker, le QB passe le ballon au receveur qui dispose d'un véritable écran (screen) de protection pour progresser. Codage des formations Le choix des tactiques est dicté au Quarterback par l'entraîneur. Ensuite lors du huddle c’est-à-dire le rassemblement des joueurs avant la séquence de jeu, celui-ci donne le nom de code correspondant à la tactique à exécuter. Lors du décompte, le quarterback peut encore changer la tactique en criant un nouveau nom de code (Audible). Chaque nom de code correspond à une combinaison travaillée à l'entraînement par les joueurs. Toutes les formations (en) sont enregistrés dans un playbook. Formations défensives Le placement en défense est libre. Les formations défensives les plus courantes en NFL sont la 43 (4 hommes de ligne, 3 linebackers et 4 arrières) et la 34 (3 hommes de ligne, 4 linebackers et 4 arrières). En raison de grand nombre de passes avant tentées dans ce championnat, on remplace fréquemment des joueurs du premier et deuxième rideau par un (nickel) voire deux (dime) arrières défensifs supplémentaires. Tactiques de jeu en défense Parmi les choix tactiques que doit faire l'entraîneur de la défense, il y a celui du mode de couverture de passe : zone ou individuelle. La défense de zone est moins susceptible de concéder de larges portions de terrain d'un seul coup. Elle donne de plus grandes chances aux arrières défensifs d'intercepter le ballon, mais elle nécessite des défenseurs intelligents et disciplinés. La défense individuelle requiert des joueurs plus athlétiques, elle génère moins d'interceptions mais plus de passes incomplètes. Elle offre aussi davantage de possibilités quant à envoyer des défenseurs blitzer.Plusieurs jeux sont possibles en défense comme le flex, le standard, le pass defense, etc. Gestion du chronomètre Le chronomètre s'arrête quand une passe est incomplète, que le porteur du ballon sort du terrain, après un temps mort, une faute, etc. et il continue quand un joueur est plaqué. Ceci ajoute une dimension passionnante au football : une équipe qui mène au score et qui a le ballon en sa possession a intérêt à jouer la montre, et utiliser les 40 secondes disponibles avant chaque mise en jeu pour faire perdre du temps. Dans ces conditions, le match peut donc être terminé 2 minutes avant la fin officielle si l'adversaire n'a plus de temps morts : le quart-arrière (quarterback) se contentera de mettre un genou au sol (même conséquence qu'un plaquage donc pas d'arrêt du chronomètre) et ce pour ses quatre tentatives.De la même façon, une équipe en possession du ballon, menée au score en fin de match, multipliera les passes, et ses porteurs de ballon feront tout pour sortir en touche afin de stopper le chronomètre. Un quarterback pourra même jeter le ballon au sol volontairement, sacrifiant une tentative (spike).Le football américain est un sport assez peu diversifié en ce qui concerne les types de joueurs : ce sport privilégie surtout les joueurs grands, puissants et très rapides, et exclut de fait la majorité de la population. Il faut aussi des joueurs extrêmement imposants pour former les lignes, et des joueurs grands et agiles pour les running back et wide receiver. Le quarterback nécessite une agilité et une vision du jeu avant tout, mais il doit être aussi solide pour éviter les blessures. Ainsi, un joueur de NFL mesure en moyenne 187 cm et pèse 112 kg, contre 175 cm et 82 kg pour un américain moyen.En NF"
sport;"Au football, le gardien de but, aussi familièrement appelé goal, est le joueur chargé de protéger le but de son équipe, de manière que le ballon n'en franchisse pas la ligne. Il a le privilège – dans la surface de réparation – de pouvoir utiliser toutes les parties du corps, contrairement aux joueurs de champ. Il peut aussi évoluer sur tout le terrain, alors avec les mêmes restrictions que les autres joueurs. S'il touche le ballon intentionnellement de la main ou du bras hors de sa surface de réparation, il est, comme le seraient tous les autres joueurs, coupable d'une faute d'anti-jeu et, passible d'une sanction (carton jaune ou rouge).Dernier rempart entre le ballon et le but de son équipe, le gardien occupe donc un poste essentiel. Historiquement, il joue de plus en plus en position de libéro (dernier défenseur, donc vers la fin de sa surface de réparation) quand tout danger est écarté.Il doit se distinguer de ses coéquipiers par un maillot différent.Outre ses qualités physiques (détente, réflexes), son caractère et son savoir-faire ont un rôle important : il doit pouvoir faire preuve d'autorité (crier pour réorganiser sa défense lors d'une offensive adverse, pour placer son mur avant un coup franc, etc.) et s'imposer (sorties aériennes au cours d'un corner). On a ainsi souvent l'image d'un gardien « bouillonnant », avec par exemple l'Allemand Oliver Kahn.Son rôle a évolué ces dernières années, pour accompagner un football de plus en plus rapide. Il doit savoir relancer proprement et précisément, des deux pieds si possible, en jeu court ou long, et être le premier contre-attaquant si possible. Son rôle a surtout changé au niveau de la technique balle au pied et de la précision de ses relances.Outre les chaussures à crampon et les protège-tibias communs à tous les joueurs, le gardien dispose de quelques accessoires spécifiques destinés à lui faciliter la tâche :les gants : indispensables, ils servent à la fois de protection aux mains contre les ballons de forte puissance (ou pour les dégagements aux poings) et de support permettant une meilleure prise de balle qu'à mains nues et par différents temps (souvent grâce à un revêtement en latex) ;les maillots et/ou pantalons/shorts rembourrés : ces vêtements sont rembourrés aux endroits les plus susceptibles de faire mal à la retombée d'un plongeon, à savoir les coudes, les genoux, le haut des cuisses, etc. ;des genouillères/coudières (matelassées ou double peau)  : pour atténuer les traumatismes des articulations. Elles sont notamment utilisées sur les terrains en mauvais état (cailloux ou mauvaises herbes). Les genouillères sont fortement recommandées pour les entraînements et en match chez les jeunes ;le casque : assez rare, il est néanmoins utilisé par le gardien tchèque Petr ?ech depuis le 20 janvier 2007 en raison d'une fracture du crâne causée par un choc lors d'un match le 14 octobre 2006. Avec cet accident (qui touche également Carlo Cudicini qui avait remplacé ?ech lors du match), des gardiens professionnels se sont déclarés non opposés au port systématique ou recommandé du casque (entre autres, l'Allemand Jens Lehmann). Ce casque en textile et mousse est homologué par la FIFA ;une casquette : fort utile quand le soleil est de face voire rasant et quand des projecteurs sont dirigés vers les buts. La casquette protège aussi d'une pluie forte, offrant un confort visuel et évitant d'avoir à s'essuyer le visage avec les gants matelassés peu commodes.Avoir de bons réflexes et de la détente ne garantissent pas l'inviolabilité du but (balle renvoyée en plein milieu d'adversaires déchaînés devant un but vide...). Au cours d'un match, le gardien peut être amené à réaliser certains gestes techniques, qu'ils lui soient ou non exclusivement réservés. Voici un aperçu des principaux gestes presque toujours exécutés par le gardien :pour saisir au mieux la balle, il est conseillé de donner une forme en W ou en triangle aux doigts autour du ballon (les pouces jouent un rôle prépondérant). Cela évite par exemple que le ballon, glissant ou trop puissant, ne passe à travers des mains écartées. Dans la mesure du possible, placer le corps sur la trajectoire du ballon, en plus des mains, est une assurance supplémentaire ;on peut aussi former une « niche » avec les bras et une partie du ventre pour y réceptionner sûrement la balle ;le gardien peut bien sûr recevoir une passe de ses coéquipiers. Cependant, si la passe est effectuée avec le pied, il ne peut pas se saisir du ballon avec les mains ;le gardien réalise aussi la traditionnelle « claquette » qui consiste à claquer le ballon en dehors de la cage à l'aide d'une main.Comme indiqué plus haut, le gardien de but est parfois amené à prendre le risque de sortir, que ce soit pour chercher une balle aérienne lors d'un corner ou pour chercher le ballon dans les pieds d'un attaquant isolé et trop avancé. Le risque de cette action, en plus de la possibilité du contournement du gardien par l'attaquant, est notamment physique. Voici quelques actions qui ont plus ou moins marqué le football.Le 5 septembre 1931, le jeune gardien du Celtic Glasgow, John Thomson, âgé de 22 ans seulement, percuta très violemment, lors d'un Old Firm (le derby opposant le Celtic aux Rangers), le genou de Sam English, attaquant des Rangers. Transporté à l'hôpital, il décéda quelques heures plus tard. Ce choc a définitivement marqué l'histoire du football écossais et des Old Firm.Le 8 juillet 1982 à Séville, au cours de la demi-finale de la Coupe du monde opposant la France à l'Allemagne de l'Ouest, Patrick Battiston se présente seul devant les buts de l'allemand Harald Schumacher. Ce dernier sort à la rencontre du milieu français et, après le tir de Battiston (un lob qui manque de peu le cadre), vient le percuter violemment au visage avec sa hanche. Battiston s'écroule, perd trois dents lors du choc et a une vertèbre cérébrale fracturée. Il reste sonné pendant plusieurs minutes. Le gardien n'est pas averti par l'arbitre néerlandais Charles Corver.René Higuita est devenu célèbre lors du match amical entre l'équipe d'Angleterre et la Colombie (0-0) à Wembley le 6 septembre 1995. Sur un centre-tir de l'anglais Jamie Redknapp et alors que le ballon se dirige vers son but, Higuita plonge vers l'avant et propulse le ballon hors de la surface à l'aide de ses talons. Cet arrêt spectaculaire fut baptisé le coup du scorpion.Le 14 octobre 2006, lors d'un match Chelsea contre Reading, Petr ?ech, gardien tchèque, est blessé à la tempe lors d'une collision avec Stephen Hunt à la première minute de jeu. Sorti sur son côté droit, il se couche et saisit la balle mais sa tempe est violemment heurtée par le genou de son adversaire. Il ne rejouera qu'à partir du 20 janvier 2007, avec un casque destiné à empêcher une autre blessure qui pourrait être fatale. Au cours du même match que celui qui vit l'accident arrivé à Cech, c'est son remplaçant, Carlo Cudicini, qui subit un choc (avec Ibrahima Sonko) lors d'un corner, alors qu'il avait repoussé la balle du poing. Inconscient, il est évacué peu avant la fin du match. Ce sera finalement John Terry qui terminera le match en tant que gardien.Pierre Chayriguès est le premier gardien de but français de renom, il a inventé les dégagements aux poings, les sorties dans les pieds des adversaires et surtout le plongeon. Il est le premier gardien français à quitter sa ligne pour anticiper sur les adversaires.Le renommé gardien allemand Sepp Maier fut la première victime du Tchèque Antonín Panenka lors des tirs au but éliminatoires lors du Championnat d'Europe de football 1976. Ce joueur, dont le nom est resté dans l'histoire du football, choisit de tirer une balle en « feuille morte » dans le centre des buts de Maier, qui était déjà parti sur son côté gauche.Lors de la finale de la Ligue des Champions 2006 opposant Arsenal et le FC Barcelone, l'Allemand Jens Lehmann est le premier gardien de l'histoire de cette compétition à être sanctionné d'un carton rouge (dès la 19e minute). Il était sorti sur le joueur Samuel Eto'o et l'avait déséquilibré un peu avant l'entrée de la surface de réparation.Le gardien italien Dino Zoff est le plus vieux vainqueur de la Coupe du monde de football (à 40 ans en 1982). Il ne prendra sa retraite qu'à l'âge de 41 ans pour enchaîner avec une carrière (moins couronnée toutefois) d'entraîneur.Le gardien soviétique Lev Yachine, considéré comme le meilleur gardien du XXe siècle, est le seul gardien de but à avoir remporté le Ballon d'or (en 1963). Aujourd'hui, le trophée récompensant le meilleur gardien de la Coupe du monde de football porte son nom.L'Espagnol Ricardo Zamora, considéré comme le plus grand gardien de l'entre-deux-guerres. Il est le premier footballeur à devenir un phénomène médiatique, participant à des publicités, à des films. Sa renommée est telle que lorsque Staline apprit que Niceto Alcalá-Zamora avait été nommé à la tête de la Seconde République espagnole, il s'exclama : « Ah ! Le gardien de football ».Le Roumain Helmuth Duckadam a réussi l'exploit d'arrêter quatre tirs au but en finale de la Coupe d'Europe des Clubs Champions 1986. Il a permis au Steaua Bucarest de devenir la première équipe de l'Est à gagner ce trophée, aux dépens du FC Barcelone. Personne n'a jamais fait aussi bien que Duckadham dans une finale majeure.Le gardien brésilien Rogério Ceni, auteur de plus de 1200 matchs sous le maillot de São Paulo, fut connu pour tirer les coups francs et les penalties de son équipe. Étant très performant dans ce domaine, il a ainsi pu inscrire 129 buts au cours de sa carrière faisant de lui le gardien le plus prolifique de l'histoire du football.Le gardien espagnol Iker Casillas , est le premier gardien à avoir remporté le triplé historique Euro 2008-Mondial 2010-Euro 2012. De plus, il possède le deuxième meilleur ratio de clean sheet (match sans encaisser de but) derrière Lev Yachine. Il est également le recordman du nombre de trophées du Meilleur gardien de football de l'année (IFFHS) remportés avec 5 titres soit un de plus que Gianluigi Buffon, autre grand gardien de l'histoire du football. Par ailleurs, il est aussi le gardien type de l'équipe type de la FIFA, l'UEFA et des journaux L'Équipe et The Guardian de 2007 à 2012.Le gardien portugais Vítor Baía est le portier le plus titré de l'histoire du football avec 33 trophées et le second joueur le plus titré tout poste confondu (le premier étant le gallois Ryan Giggs avec 35 titres). Baía remporta avec le FC Porto et le FC Barcelone 11 championnats, 7 coupes nationales, 10 supercoupes, 1 Ligue des champions, 1 Coupe UEFA, 1 Coupe des Coupes, 1 Supercoupe de l'UEFA et 1 Coupe intercontinentale et fait partie des six joueurs ayant remporté la C1, la C2, la C3, la Coupe intercontinentale et la Supercoupe de l'UEFA au moins une fois. En Europe, il devance le néerlandais Edwin van der Sar (25 titres), le danois Peter Schmeichel (24 titres) et l'Allemand Oliver Kahn (23 titres).Le gardien allemand Oliver Kahn est le seul gardien à avoir remporté le prix de meilleur joueur de la coupe du monde. C'était en 2002.Le gardien costaricain Keylor Navas est le premier portier de la CONCACAF à avoir remporté la Ligue des Champions et est également le premier à l'avoir remporté 3 fois de suite.Le gardien égyptien Essam El-Hadari devient le plus vieux gardien à avoir joué un match de la Coupe du monde de football (45 ans en 2018).Le gardien allemand Lutz Pfannenstiel est le seul joueur de football à avoir joué sur tous les continents durant sa carrière riche de 31 clubs.En France, les carrières professionnelles de Dominique Baratelli, d'Albert Rust, de Jean-Luc Ettori et de Mickaël Landreau se sont étalées sur 19 saisons (Jean-Paul Bertrand-Demanes 18).Le classement établi par l'IFFHS recense les vingt meilleurs gardiens du XXe siècle. L'IFFHS détermine également chaque année le « meilleur gardien de football de l'année ». À ce jour, le meilleur gardien de l'année est le Belge Thibaut Courtois. Lev Yachine Gordon Banks Dino Zoff Sepp Maier Ricardo Zamora José Luis Chilavert Peter Schmeichel Peter Shilton František Pláni?ka Amadeo Carrizo Gylmar dos Santos Neves Ladislao Mazurkiewicz Pat Jennings Ubaldo Fillol Christian Piot Rinat Dasaev Antonio Carbajal Silviu Lung Ray Clemence Walter Zenga Oliver Kahn Gordon Banks Jean-Marie Pfaff Gianluigi Buffon Dino Zoff Rinat Dasaev Peter Schmeichel Rü?tü Reçber Fabien Barthez Lev Yachine, Ballon d'or en 1963 Ivo Viktor, 3e en 1976 Oliver Kahn, 3e en 2001 et 2002 Gianluigi Buffon, 2e en 2006 Manuel Neuer, 3e en 2014 Lev Yachine (1960, URSS) José Ángel Iribar (1964, Espagne) Dino Zoff (1968, Italie) Sepp Maier (1972, RFA) Ivo Viktor (1976, Tchécoslovaquie) Harald Schumacher (1980, RFA) Joël Bats (1984, France) Hans van Breukelen (1988, Pays-Bas) Peter Schmeichel (1992, Danemark) Andreas Köpke (1996, Allemagne) Fabien Barthez (2000, France) Antónios Nikopolídis (2004, Grèce) Iker Casillas (2008 et 2012, Espagne) Rui Patrício (2016, Portugal) Gianluigi Donnarumma (2020, Italie) Cayetano Saporiti (1916 et 1917, Uruguay) Marcos (1919, Brésil) Juan Legnazzi (es) (1920, Uruguay) Américo Tesoriere (1921 et 1925, Argentine) Júlio Kuntz Filho (1922, Brésil) Pedro Casella (1923, Uruguay) Andrés Mazali (1924, Uruguay) Fausto Batignani (1926, Uruguay) Octavio Díaz (1927, Argentine) Ángel Bossio (1929, Argentine) Enrique Ballestero (1935, Uruguay) Juan Honores (1939, Pérou) Juan Alberto Estrada (en) (1941, Argentine) Aníbal Paz (1942, Uruguay) Claudio Vacca (en) (1946, Argentine) Julio Cozzi (1947, Argentine) Barbosa (1949, Brésil) Adolfo Riquelme (es) (1953, Paraguay) Julio Musimessi (1955, Argentine) Julio Maceiras (1956, Uruguay) Rogelio Domínguez (1957, Argentine) Osvaldo Negri (1959, Argentine) Roberto Sosa (1959, Uruguay) Arturo López (1963, Bolivie) Ottorino Sartor (1975, Pérou) Roberto Fernández (1979, Paraguay) Rodolfo Rodríguez (1983, Uruguay) Eduardo Pereira (1987, Uruguay) Cláudio Taffarel (1989, Brésil) Sergio Goycochea (1991 et 1993, Argentine) Fernando Álvez (1995, Uruguay) Cláudio Taffarel (1997, Brésil) Dida (1999, Brésil) Óscar Córdoba (2001, Colombie) Júlio César (2004, Brésil) Doni (2007, Brésil) Fernando Muslera (2011, Uruguay) Claudio Bravo (2015 et 2016, Chili) Alisson (2019, Brésil) Emiliano Martinez (2021, Argentine) Ham Heung-chul (1956 et 1960, Corée du Sud) Yitzhak Vissoker (1964, Israël) Aziz Asli (1968, Iran) Nasser Hejazi (1972, Iran) Mansour Rashidi (1976, Iran) Jasem Bahman (1980, Koweït) Abdullah Al-Deayea (en) (1984 et 1988, Arabie Saoudite) Shigetatsu Matsunaga (1992, Japon) Mohamed Al-Deayea (1996, Arabie Saoudite) Yoshikatsu Kawaguchi (2000 et 2004, Japon) Noor Sabri (2007, Irak) Eiji Kawashima (2011, Japon) Mathew Ryan (2015, Australie) Sergio Goycochea : 2 matchs (1992, Argentine) Mogens Krogh : 2 matchs (1995, Danemark) Lars Høgh : 2 matchs (1995, Danemark) Dida : 5 matchs (1997, Brésil) et 4 matchs (2005, Brésil) Jorge Campos : 5 matchs (1999, Mexique) Ulrich Ramé : 3 matchs (2001, France) Mickaël Landreau : 1 match (2001, France) et 1 match (2003, France) Grégory Coupet : 1 match (2001, France) et 2 matchs (2003, France) Fabien Barthez : 2 matchs (2003, France) Marcos : 1 match (2005, Brésil) Julio César : 5 matchs (2009 et 2013, Brésil) Bernd Leno : 1 match (Allemagne, 2017) Marc-André ter Stegen : 4 matchs (Allemagne, 2017) Enrique Ballestero (1930, Uruguay) Gianpiero Combi (1934, Italie) Aldo Olivieri (1938, Italie) Roque Máspoli (1950, Uruguay) Anton Turek (1954, RFA) Gilmar (1958 et 1962, Brésil) Gordon Banks (1966, Angleterre) Félix (1970, Brésil)Sepp Maier (1974, RFA) Ubaldo Fillol (1978, Argentine) Dino Zoff (1982, Italie) Nery Pumpido (1986, Argentine) Bodo Illgner (1990, RFA) Cláudio Taffarel (1994, Brésil) Fabien Barthez (1998, France) Marcos (2002, Brésil) Gianluigi Buffon (2006, Italie) Iker Casillas (2010, Espagne) Manuel Neuer (2014, Allemagne) Hugo Lloris (2018, France) Mary Harvey (1991, États-Unis) Bente Nordby (1995, Norvège) Briana Scurry (1999, États-Unis) Silke Rottenberg (2003, Allemagne) Nadine Angerer (2007, Allemagne) Ayumi Kaihori (2011, Japon) Hope Solo (2015, États-Unis) Alyssa Naeher (2019, États-Unis)Dès la création de la Coupe du monde, un gardien est nommé meilleur gardien du tournoi, en étant incorporé dans l'équipe du tournoi :  Enrique Ballestero en 1930 Ricardo Zamora en 1934 František Pláni?ka en 1938 Roque Máspoli en 1950 Gyula Grosics en 1954 Harry Gregg en 1958 Viliam Schrojf en 1962 Gordon Banks en 1966 Ladislao Mazurkiewicz en 1970 Jan Tomaszewski en 1974 Ubaldo Fillol en 1978 Dino Zoff en 1982 Harald Schumacher en 1986 Sergio Goycochea en 1990 Michel Preud'homme en 1994 Fabien Barthez en 1998 Oliver Kahn en 2002 Gianluigi Buffon en 2006 Iker Casillas en 2010 Manuel Neuer en 2014 Thibaut Courtois en 2018 Sepp Maier (RFA, le premier) Dino Zoff (Italie) Fabien Barthez (France, ainsi que des Confédérations) Iker Casillas (Espagne) Sepp Maier (RFA) Bodo Illgner (Allemagne) Fabien Barthez (France) Iker Casillas (Espagne) Manuel Neuer (Allemagne)Trophée Lev YachineLe trophée du meilleur gardien de but de la Coupe du monde de football a été créé en 1994. Il a été nommé en mémoire de l'ancien gardien soviétique (mort en 1990). Il est décerné par un jury constitué d'anciens joueurs et d'entraîneurs, membres du groupe d'étude technique de la FIFA. Il a été attribué à :  Michel Preud'homme en 1994 Fabien Barthez en 1998 Oliver Kahn en 2002 Gianluigi Buffon en 2006 Iker Casillas en 2010 Manuel Neuer en 2014 Thibaut Courtois en 2018Le football a fait l'objet de très nombreuses études scientifiques ou documentaires. Parmi celles-ci, certaines ont porté sur le gardien de but (plus rarement sur les gardiennes), son rôle dans l'équipe, ses capacités physiques (taille, poids, indice de masse corporelle, pourcentage de graisse corporelle, souplesse, réflexes, vitesse, agilité, puissance musculaire et cardiaque, capacité aérobie et anaérobie... ), capacités proprioceptives et psychologiques, son entraînement, son état mental dans certaines circonstances, face à un penalty par exemple, quand il doit tenter de deviner ce que son ""adversaire""  va faire ou tenter de le distraire,, la manière dont il prend ses décisions ou dont on peut évaluer ses décisions.  On a ainsi montré à partir d'un échantillon d'une cinquantaine de gardiens que chez les jeunes (14-20 ans), le poids et la taille du gardien sont plus élevés en moyenne que ceux des attaquants, et que leur taux de graisse corporelle est également plus élevé que chez tous les autres postes, alors que leur souplesse, agilité, puissance anaérobie, capacité anaérobie et valeurs de récupération sont comparables avec celles des autres joueurs. À partir de 17 ans leur temps de sprint sur 30 m était aussi le plus lent parmi les joueurs.Des études spécifiques ont porté sur ses gants. Récemment, des études ont montré que des maillots de gardien de couleurs fluo avaient tendance à grandir la taille des gardiens aux yeux des attaquants et d'attirer leur regard lors de la frappe, donc d'attirer par extension la balle sur eux.Dispositifs tactiques en football Portail du football"
sport;"Le hockey sur glace, appelé le plus souvent hockey, est un sport d’équipe se jouant sur une patinoire spécialement aménagée. L’objectif de chaque équipe est de marquer des buts en envoyant un disque de caoutchouc vulcanisé, appelé rondelle ou palet, à l’intérieur du but adverse situé à une extrémité de la patinoire. L’équipe se compose de plusieurs lignes de cinq joueurs, qui se relaient sur la glace, ainsi que d'un gardien de but, qui se déplacent en patins à glace et manipulent la rondelle à l’aide d’un bâton de hockey également appelée crosse en France ou canne de hockey en Belgique et en Suisse.Le hockey est originaire du Canada et s’est développé à la fin du XIXe siècle en Amérique du Nord. Sport de vitesse, il est souvent qualifié de « sport collectif le plus rapide du monde », mais c’est aussi un sport de contact, voire violent, avec ses mises en échec parfois dangereuses.Au niveau mondial, le hockey est pratiqué dans de nombreux pays et un des championnats les plus connus au monde est celui de la Ligue nationale de hockey en Amérique du Nord qui existe depuis 1917. Des compétitions internationales récurrentes sont également organisées par la Fédération internationale de hockey sur glace, plus connu sous le sigle anglais IIHF. Ainsi, chaque année des championnats du monde sont organisés que ce soit pour le hockey sur glace féminin ou masculin et ceci pour différentes catégories d'âges. Enfin, tous les quatre ans ont lieu les Jeux olympiques où un tournoi féminin et un autre masculin sont organisés.Depuis l’Antiquité, l’homme a joué à des jeux où un objet était frappé avec un bâton incurvé. Ainsi, sur un kouros datant de 400 ans av. J.-C., deux hommes nus sont en train de jouer à un jeu, courbés en avant avec un objet en forme de crosse à la main et une balle entre eux deux. Plus tard, une peinture de Pieter Bruegel, Chasseurs dans la neige (1565), montre des joueurs qui utilisent des bâtons courbés pour jouer avec un petit objet sur la glace.L’utilisation du mot « hockey » pour désigner de tels jeux est attestée depuis 1785 (1527 pour hockie) mais son étymologie est incertaine. Il dérive peut-être du vieux mot français d’origine germanique hoquet qui désigne un bâton de berger en forme de crochet ou encore du mot Iroquois hoghee.Les Européens qui se sont établis en Amérique du Nord y ont introduit une multitude de jeux similaires au hockey, tels que la crosse française, le shinty écossais, le hurling irlandais et le hockey sur gazon, joué en Angleterre. Ces jeux semblent avoir été adaptés pour être joués sur la glace et avoir importé certains aspects du jeu de la crosse (comme la dureté physique) joué par les Amérindiens de la famille iroquoise. Plusieurs villes du Canada peuvent prétendre à être le berceau du hockey sur glace, Halifax, Windsor ou encore Kingston, mais la fondation du jeu moderne du hockey sur glace a lieu à Montréal au Québec.En effet, le 3 mars 1875 un match de hockey sur glace avec des règles est joué pour la première fois en intérieur, au Victoria Skating Rink sous l’impulsion de James Creighton, membre du club de patinage et juge de patinage artistique. Le match oppose deux équipes composées de neuf joueurs chacune, des gardiens de buts, un arbitre, une rondelle, des règles sur lesquelles les protagonistes se sont mis d'accord, un temps de jeu limité à raison de soixante minutes et enfin un score noté. La plupart des matchs ayant eu lieu avant ce match sont souvent des matchs extérieurs, avec des balles et rarement des règles bien définies. Le choix d'une rondelle en bois en lieu et place d'une balle de crosse se justifie par la volonté de ne pas abîmer les vitres des fenêtres et pour limiter les risques de blessure dans le public. Les crosses du match sont fournies par Creighton qui les a fait venir de la Nouvelle-Écosse où elles ont été fabriquées par les Micmacs. Ce match est annoncé au public dans les pages d'un journal de Montréal, la Gazette de Montréal.La tenue des joueurs est rudimentaire puisqu'ils portent des maillots de rugby, des shorts et des bas de laine mais sans aucune protection. Les patins qu'ils portent viennent également de Nouvelle-Écosse où ils ont été fabriqués par la Starr Manufacturing Company ; il s'agit d'une simple lame de métal qui vient serrer la chaussure du joueur et peut être libérée par un levier. Même si le jeu moderne ne naît pas concrètement ce soir-là, le mouvement est lancé et la structure va petit à petit se mettre en place. Le match prend fin au milieu de la deuxième période à la suite d'une bagarre entre les joueurs de hockey et les membres du club de patinage de Victoria.Huit ans après ce premier match, le hockey est un sport exclusivement réservé aux garçons de bonnes familles et le Canada compte moins de cent joueurs. Cependant le jeu devient de plus en plus populaire au sein de l'élite anglophone et en 1883, l'Association des athlètes amateurs de Montréal (MAAA) a pour tâche d'organiser un tournoi de hockey sur glace pour le défilé du Carnaval d'hiver de Montréal de 1883. Le Carnaval est peu fréquenté par les francophones mais les Américains y viennent en nombre. Grâce à la couverture du tournoi par les journaux, pour la première fois le hockey brille sur la scène internationale. Même si en fin de compte, le Carnaval est une manifestation bourgeoise, il s'agit surtout d'une vitrine extraordinaire pour le hockey et l’engouement se répand à l'est des États-Unis et du Canada.En 1888, le Canada accueille un nouveau Gouverneur général en la personne de Frederick Stanley, un sportif qui aime la chasse, les chevaux de course et la pêche,. La famille Stanley assiste, pour la première fois, à un match de hockey au cours du Carnaval d'hiver de Montréal de 1888 et toute la famille est conquise. Ainsi, Lord Stanley fait construire une patinoire près de sa résidence, Rideau Hall, à Ottawa et sur cette patinoire, une de ses filles, Isobel, participe au premier match féminin de hockey sur glace. Deux des garçons, Arthur et Algernon, forment leur propre équipe, les Rideau Hall Rebels. Comme la vie de la famille du Gouverneur est largement couverte par la presse canadienne mais également par la presse américaine, de nombreuses personnes entendent alors parler du hockey sur glace. En 1892, les fils du Gouverneur parviennent à convaincre leur père d'acheter une coupe en argent pour cinquante dollars pour la meilleure équipe du Canada. L'équipe qui remporte le trophée ne peut pas le conserver et doit le remettre en jeu régulièrement. La coupe est nommée initialement Dominion Challenge Trophy, trophée qui sera nommé plus tard Coupe Stanley. En 1893, à la fin de la saison de l'Association de hockey amateur du Canada, les joueurs du club de hockey de Montréal, équipe dépendante de l'Association des athlètes amateurs de Montréal, finissent à la première place du classement et remportent la coupe offerte par le Gouverneur du Canada. En mai 1893, un article de la Gazette renomme le trophée The Stanley Hockey Championship Cup. Le coût pour une paire de patins est d'un dollar et de 25 cents pour un bâton micmac alors que le hockey se joue dans plusieurs villes du Canada (Ottawa, Calgary, Montréal…) Ainsi, il se crée même un championnat de la ligue des noirs de la Nouvelle-Écosse. À Montréal, l'élite francophone commence à jouer au hockey dans les collèges classiques catholiques. En effet, ces derniers sont également fréquentés par les Irlandais qui y jouent depuis quinze ans et présentent donc le jeu aux francophones.Le hockey se joue aussi à Winnipeg avec la création en 1892 de la Manitoba Hockey Association qui comprend trois équipes : les Rifles de Fort Osborne, Hockey Club de Winnipeg et les Victorias de Winnipeg. La ville a l'avantage d'avoir de la glace plus longtemps que Montréal et ainsi les saisons comportent plus de matchs. Les Victorias, nommés ainsi en l'honneur de la Reine Victoria du Royaume-Uni, passe une petite annonce en 1895 dans le journal local afin de trouver de nouveaux joueurs. Dan Bain, sportif reconnu et talentueux dans de nombreuses disciplines, fait un essai au sein de l'équipe et bien qu'il joue avec sa propre crosse qui est cassée et réparée avec un bout de fil, il est engagé après cinq minutes de jeu. En 1896, les joueurs de Winnipeg défient les Victorias de Montréal, détenteurs de la Coupe Stanley. La rencontre a lieu le 14 février 1896 dans la salle du Victoria Skating Rink de Montréal pleine de supporteurs des deux formations. Les joueurs de Winnipeg ayant mis au point un tir du poignet, le gardien de l'équipe, George « Whitey » Merritt, joue avec des jambières de cricket. Ce nouvel équipement permet à Merritt de repousser tous les lancers de Montréal alors que son équipe inscrit deux buts pour un blanchissage,. L'équipe devient la première équipe extérieure à une ville du Québec à remporter le trophée et fait un retour triomphal à Winnipeg en étant accueillie à la gare du Canadien Pacifique par une foule de partisans. Deux traditions de la Coupe Stanley voient le jour : la presse publie que le bol contient deux gallons de bière et au retour à Winnipeg, les joueurs défilent dans la rue principale à bord d'une voiture.En 1902, après deux titres en 1899 et 1900, les Shamrocks de Montréal, équipe composée principalement de joueurs originaires d'Irlande, cherchent à retrouver un second souffle. Ils recrutent alors deux joueurs francophones des Montagnards de Montréal, Louis Hurtubise et Louis Viaut, qui jouent leur premier match contre les puissants Silver Seven d'Ottawa. Les Shamrocks remportent le match qui devient un événement et un moment charnière : le public découvre en effet que les joueurs francophones savent jouer au hockey et à un bon niveau. En décembre 1904, les Nuggets de Dawson City, petite ville du nord-ouest du Canada lancent un défi aux « Silver Seven » pour la Coupe Stanley. Après avoir traversé tout le Canada, les Nuggets rejoignent Ottawa en janvier et avec seulement un repos de deux jours, ils sont opposés aux champions en titre menés par Frank McGee. Ce dernier est un buteur hors pair alors que depuis qu'il a reçu un palet dans son visage à dix-huit ans, il ne voit plus que d'un seul œil. L'équipe d'Ottawa bat Dawson City 9 buts à 2 puis écrase littéralement leurs adversaires 23-2. Au cours du deuxième match, 14 buts sont inscrits par McGee dont huit en moins de neuf minutes.Trente ans après le premier match de Creighton et ses amis, le jeu est devenu plus rapide, plus excitant et surtout beaucoup plus rude : en 1904, on compte quatre morts au cours des différentes rencontres. La violence est déjà un composant essentiel du hockey sur glace. Malgré tout de nombreuses équipes et ligues sont créées et presque tout le monde gravitant autour du hockey sur glace gagne de l'argent. Les seuls laissés pour compte sont les joueurs qui doivent rester amateurs et ne peuvent pas vivre de leur sport.En 1892 et à des milliers de kilomètres de là, le baron Pierre de Coubertin joue également au hockey sur glace sur les bassins gelés du château de Versailles. La France est alors le premier pays d'Europe où le hockey se joue. La première patinoire de France est construite cette même année 1892 rue de Clichy dans le 9e arrondissement de Paris. Deux ans plus tard, c'est la création du Hockey Club de Paris qui évolue dans cette même patinoire, Pôle nord. Un des premiers matchs internationaux a lieu le 12 décembre 1897 avec une opposition entre le HCP et le Prince's Club de Londres. Deux matchs ont lieu et les joueurs de Londres l'emportent à chaque fois, 23-6 et 11-5. En 1906-1907, le premier championnat de France est mis en place avec quatre équipes et le Sporting Club de Lyon, fondé en 1903, remporte ce premier championnat en battant en finale le Club des patineurs de Paris 8-2 devant 3 000 spectateurs. À l'invitation du français Louis Magnus, des représentants du hockey sur glace de Belgique, de France, de Grande-Bretagne, et de Suisse sont réunis le 15 mai 1908 à Paris et ensemble ils fondent la Ligue Internationale de Hockey sur Glace afin d'unifier les différentes règles du hockey sur glace. Louis Magnus est désigné président et Robert Planque secrétaire général. Au cours de cette année, les quatre pays représentés, ainsi que la Bohême, deviennent membres de la LIHG et le premier congrès se déroule à Paris.Alors que depuis 1904, une Ligue de hockey sur glace de la Suisse romande existe, ce n'est que le 27 septembre 1908, que la Suisse se dote d'une fédération, le Schweizerischer Eishockeyverband, et à la fin de la saison 1908-1909, le Hockey Club Bellerive Vevey remporte le premier championnat Suisse. La Fédération de Belgique de hockey sur glace est créée le 8 décembre 1908 alors que le bandy est très répandu dans le pays. Le 10 novembre 1909, la première patinoire de hockey est construite en Autriche, autre pays où se pratique le bandy.Le premier championnat d'Europe de hockey a lieu en 1910 à Montreux. Après le retrait de la France, seulement quatre nations participent à cette première internationale : l'Allemagne, représentée par le Berliner SC, le Prince's club Londres en tant qu'équipe de Grande-Bretagne, la Suisse et enfin la Belgique. Ces deux dernières formations sont des mélanges de joueurs d'au moins deux équipes. Une particularité de ce tournoi est qu'une même équipe peut jouer deux matchs la même journée. Enfin, une équipe d'étudiants canadiens d'Oxford participe au tournoi. Avec deux victoires en trois rencontres, les joueurs de Londres sont sacrés champions. Le 15 janvier 1912, l'Union autrichienne de hockey est créée et l'Autriche rejoint la Fédération internationale de hockey alors que la Belgique organise son premier championnat remporté par le Brussels Royal IHSC.Aux États-Unis et au début du XXe siècle, l'industrie du cuivre est en plein essor afin de répondre aux demandes croissantes en électricité. Le Michigan possède de nombreuses mines et la ville de Houghton en est un des exemples. Les habitants de la ville ont un petit peu d’argent et beaucoup de temps libre car ils sont pour la majorité isolés de leur famille restée au pays. Le Portage Lakes Hockey Club attire les foules et James R. Dee, son propriétaire, veut en faire une entreprise lucrative. Ainsi, en 1902, l’amphidrome est construit par Dee qui la qualifie de plus grande patinoire intérieure des États-Unis. Pour remplir sa patinoire, Dee décide de créer la première ligue professionnelle au monde et pour trouver des joueurs, il s’adresse à Jack « Doc » Gibson, un dentiste venant de Berlin en Ontario. Gibson est passionné de hockey mais a été banni du hockey en Ontario après avoir accepté des pièces d'or par le maire de Berlin à la suite d'une victoire contre la ville voisine.Une réunion pour former la Ligue internationale de hockey a lieu le 5 novembre 1904 et elle réunit des promoteurs de Pittsburgh, Sault-Sainte-Marie et Houghton. À la fin de l'hiver 1904, le club de Portage Lake, qui compte dans ses rangs Hod Stuart, bat les Bankers de Pittsburgh pour remporter le titre de champion des États-Unis. James Dee en veut plus et lance un défi aux Montréal AAA pour jouer un match pour la Coupe Stanley mais la formation de Montréal décline. Malgré tout, Gibson rentre au Canada et fait passer le mot qu'au Michigan les joueurs sont payés pour jouer au hockey. L'exode des joueurs de hockey débute : Jean-Baptiste Laviolette, vingt-six ans, et Didier Pitre sont parmi les premiers joueurs à tenter l’aventure et à quitter Montréal. Une autre vedette de l'époque rejoint la LIH : Frederick « Cyclone » Taylor. Ce jeune joueur de vingt-et-un ans est depuis peu banni du hockey en Ontario pour avoir refusé la proposition du secrétaire de l'Association de hockey de l'Ontario, William Hewitt. Même si Hewitt veut voir Taylor dans son équipe des Marlboros de Toronto, le jeune joueur doit continuer à travailler pour aider sa famille à vivre. Taylor se voit proposer un contrat par le Portage Lakes Hockey Club de 400 dollars plus les frais de déplacement pour rejoindre le Michigan.Pendant ce temps les magnats du hockey au Canada fulminent de voir leurs meilleurs joueurs partir aux États-Unis. Ils ripostent et brandissent leur menace habituelle : le bannissement des joueurs professionnels mais l’appât de l’argent est plus important. Cependant en 1907, une récession frappe l'économie américaine en même temps que le marché du cuivre et Gibson et les autres Canadiens sont de retour au Canada. Malgré cette courte existence, la LIH a changé le visage du hockey en Amérique du Nord car désormais le hockey professionnel existe et on ne peut plus faire marche arrière. En 1907, les Thistles de Kenora remportent la Coupe Stanley : c'est la dernière fois qu'une petite ville remporte la Coupe et également la dernière équipe amateur à mettre la main sur le trophée.Le 13 novembre 1909, à la suite d’un différend qui oppose les propriétaires des clubs membres de l’Eastern Canada Amateur Hockey Association et le nouveau propriétaire des Wanderers de Montréal, James Strachan, une nouvelle ligue voit le jour. Ce dernier s'associe avec John Ambrose O'Brien, le fils du Sénateur Michael John O'Brien propriétaire des Creamery Kings de Renfrew en Ontario. Ensemble, ils créent l'Association nationale de hockey (ANH). Jimmy Gardner, un directeur des Wanderers, et Ambrose O’Brien ont l’idée d’exploiter commercialement la rivalité entre les anglophones et les francophones de Montréal et d’établir un club de hockey majoritairement, sinon totalement, composé de joueurs d’expression française : le « Club Athlétique Canadien », connu par la suite sous le nom de Canadiens de Montréal.Une guerre au plus offrant se déroule afin de recruter les meilleurs joueurs en activité. Ainsi, Frank et Lester Patrick se joignent à Renfrew pour la somme de 3 000 dollars pour Frank et 2 000 pour son frère. Les Comets signent Art Ross pour 2 700 dollars. Renfrew signe un autre gros nom du hockey de l'époque : Cyclone Taylor rejoint les Creamery Kings, très vite surnommés Millionnaires, pour 5 250 dollars et une saison d'une douzaine de matches. Il est alors le joueur le mieux payé de toute la ligue et même le sportif canadien le mieux payé – sa paye étant supérieure à celle du premier ministre canadien, qui touche alors 2 500 dollars par année. En 1910-1911, l'ANH décide d'abandonner la forme classique de deux périodes et de diviser le temps de jeu en trois périodes de vingt minutes.Le 7 décembre 1911, les frères Patrick utilisent les finances familiales et créent une nouvelle ligue de hockey à l'Ouest du Canada : l'Association de hockey de la Côte du Pacifique (PCHA) dont les règles sont calquées sur celles de l'Association nationale de hockey, la grande ligue de hockey de l'époque. Il existe désormais deux compétitions de grande ampleur au Canada : la PCHA à l'Ouest et l'ANH à l'Est et à l'issue de la saison 1914-1915, les Millionnaires de Vancouver battent les Silver Seven d'Ottawa pour mettre la main sur la Coupe Stanley.Le 28 octobre 1916, le président historique de l'ANH, Emmett Quinn, décide de démissionner ; il est à la tête de l'ANH depuis 1910 et est remplacé par le Major Frank Robinson. Afin d'exclure les Blueshirts de Toronto d'Edward Livingstone de la ligue, les présidents des autres équipes se réunissent à l'Hôtel Windsor le 26 novembre 1917 et décident de dissoudre l'Association et de créer une nouvelle structure sans lui : la Ligue nationale de hockey. De son côté la PCHA voit ses finances se dégrader petit à petit et en 1924, les Maroons de Vancouver déposent le bilan. Il ne reste alors plus que deux équipes qui rejoignent la Western Canada Hockey League. Deux saisons plus tard, les finances de la WCHL ne sont pas meilleures et cette dernière met également fin à ses activités. La majorité des anciens joueurs de la PCHA et de la WCHL rejoignent alors la Ligue nationale de hockey.En janvier 1920, le Comité international olympique annonce que lors des Jeux olympiques d'été, prévus en avril 1920, à Anvers, en Belgique, une compétition de hockey aura lieu entre les équipes masculines des différentes nations. Six joueurs sont alignés de chaque côté alors que les changements de ligne ne peuvent s'effectuer que lors d'un arrêt du jeu. Les sept équipes participant au tournoi sont les suivantes : la Belgique, le Canada, les États-Unis, la France, la Suède, la Suisse et la Tchécoslovaquie. La Suisse compte dans ses rangs Max Sillig, président de la LIHG alors que le Canada est représenté par les Falcons de Winnipeg, équipe détentrice de la Coupe Allan et composée uniquement de joueurs de familles originaires d'Islande. Il est décidé de distribuer les médailles selon le « système Bergvall » : l'une des sept équipes est tirée au sort et exemptée du premier tour – la France. La première phase se joue avec trois matchs au premier tour puis les trois vainqueurs et l'équipe exempte du premier tour se rencontrent. Une finale est organisée à l'issue de laquelle la médaille d'or est donnée à la meilleure équipe. Par la suite, les trois équipes éliminées par l'équipe championne participent à une deuxième phase avec une équipe exemptée de premier tour. L'équipe qui sort victorieuse de ce tour reçoit la médaille d'argent. Enfin, les trois équipes éliminées par les deux équipes médaillées se rencontrent dans une troisième phase afin d'attribuer la médaille de bronze. Les Falcons, emmenés par Frank Fredrickson, remportent le tournoi grâce à trois victoires 15-0 contre la Tchécoslovaquie, 2-0 contre les États-Unis puis 12-1 contre la Suède. La Tchécoslovaquie est exemptée du premier tour de la deuxième phase mais perd en finale contre les États-Unis 16-0. La troisième phase oppose la Tchécoslovaquie, la Suède et la Suisse. Une nouvelle fois exemptée du premier match, la Tchécoslovaquie remporte son premier match du tournoi lors de la dernière rencontre contre la Suède 1-0 ; l'équipe se classe alors troisième,. Cinq ans plus tard, le CIO déclare que finalement, le tournoi de 1920 n'était pas un tournoi officiel et que la médaille d'or remportée par le Canada ne compte pas. Le résultat sera rétabli plus tard et la médaille d'or officiellement remportée par le Canada. En 1983, la Fédération internationale de hockey sur glace reconnaît ce tournoi comme son premier championnat du monde.La plus vieille compétition de club en Europe voit le jour en 1923 : la Coupe Spengler a lieu chaque année à Davos et se déroule entre Noël et le Jour de l'an. Les équipes participantes sont invitées par l'équipe hôte, le Hockey Club Davos. En janvier 1925, le 1. ?sŠK Bratislava l'emporte contre le SK Velké Mezi?í?í sur le score de 9-2 au cours du premier match de l'histoire entre deux équipes tchécoslovaques alors que Bratislava perd le premier match de son histoire un mois plus tôt contre Wiener EV. L'année suivante, la première patinoire artificielle est créée en Suisse dans la ville de Davos.Au début des années 1940, James T. Sutherland, le président de l'Association canadienne de hockey amateur, désire fonder un Temple de la renommée du hockey sur glace nord-américain. En 1943, la LNH et l'ACHA trouvent un accord pour que le temple soit situé dans la ville de Kingston, supposé le berceau du hockey sur glace. Le 10 septembre 1943, Stuart Crawford, maire de la ville de Kingston est désigné premier président du Temple et les premiers joueurs à en faire partie sont : Hobey Baker, Charlie « Chuck » Gardiner, Eddie Gerard, Francis « Frank » McGee, Howie Morenz, Tom Phillips, Harvey Pulford, William « Hod » Stuart et Georges Vézina.Les femmes ne sont autorisées à participer aux Jeux Olympiques qu'en 1998.L'histoire du hockey sur glace féminin débute quasiment en même temps que le premier match de hockey sur glace masculin a lieu en salle. En effet, en 1889, quelques mois après que Lord Stanley a vu son premier match de hockey lors du Carnaval d'hiver de Montréal, la presse locale rapporte qu'une de ses filles, Isobel, a joué au sein d'une équipe composée de jeunes filles de la maison du gouverneur et a battu une autre formation féminine. Au cours des années qui suivent, différentes équipes féminines font parler d'elles : en 1896, une jeune fille reconnue comme étant une des personnes les plus rapides sur patins de toute la Saskatchewan, Annie McIntyre, forme une équipe de hockey. Le 11 mars 1897, le journal Medicine Hat Times fait part d'une rencontre entre deux équipes féminines. Avant la fin du siècle, des équipes se créent un peu partout au Canada : Calgary, Banff, Medicine Hat, Vancouver, Kingston, Toronto, Ottawa, Montréal ou encore Québec. En 1900, la première ligue féminine est organisée au Québec ; elle comporte alors trois formations de Montréal, une de Québec et une dernière de Trois-Rivières.Au fur et à mesure des années qui passent, les rencontres de hockey féminin deviennent de plus en plus populaires et des compétitions d'un côté à l'autre du Canada sont organisées. Dans les années 1930, certaines rencontres attirent près de 3 000 spectateurs. Au cours de cette même période, l'équipe la plus connue du Canada est celle des Rivulettes de Preston qui remporte la majorité des titres entre 1931 et 1940. Au cours des dix saisons, l'équipe remporte toutes les éditions du championnat de l'Ontario, Ladies Ontario Hockey Association Senior Division et met la main sur quatre trophées de la meilleure équipe du Canada, en 1935, 1937, 1938 et 1939. Selon les relevés des résultats réalisés par des historiens du hockey sur glace féminin, l'équipe ne perd que deux rencontres en près de 350 parties disputées,.La fin de la Seconde Guerre mondiale met un coup d'arrêt au hockey féminin : l'interdiction des mises en échec et la séparation en deux sphères — avec des villes qui vont jusqu'à interdire aux hommes d'assister aux matchs féminins — ont raison de l'engouement pour la pratique féminine. De 1936 à 1948, et de 1951 à 1960, il n'y a aucune compétition nationale au Canada. Il faudra attendre les années 1960 avant de revoir du hockey féminin. En 1989, la Fédération internationale réalise un sondage auprès des nations adhérentes pour savoir si une compétition féminine internationale aurait des chances de réussir. À la suite du retour positif du sondage, le premier championnat d'Europe féminin a lieu en 1989. La saison suivante, le premier championnat du monde est organisé avec les cinq meilleures équipes européennes en 1989, le Canada, les États-Unis et le Japon. Invaincues du tournoi, les deux équipes nord-américaines se retrouvent en finale et la médaille d'or revient au Canada qui l'emporte 5-2. En 1998, en même temps que les joueurs de la LNH font leurs débuts au cours des Jeux olympiques de 1998, les joueuses de hockey font également leur entrée dans le monde du sport olympique alors que la médaille d'or est remportée par les États-Unis qui battent le Canada en finale. Il s'agit alors de la première défaite en compétition internationale du CanadaBien qu’il n’existe pas autant de ligues féminines que de ligues masculines, il en existe de tous niveaux. Ainsi en Amérique du Nord, la Ligue canadienne de hockey féminin, fondée en 2007, et la Ligue nationale de hockey féminin, fondée en 2015, sont les deux ligues majeures du Canada et des États-Unis. De plus il existe plusieurs championnats européens féminins, des équipes nationales, olympiques, universitaires et de loisirs. La grande différence entre le hockey sur glace masculin et féminin est l’interdiction des charges chez les femmes.Les règles internationales du hockey sur glace sont édictées par la Fédération internationale de hockey sur glace (IIHF). C’est également le règlement qui s’applique dans les championnats nationaux de la plupart des pays, en dehors de l’Amérique du Nord.En effet, aux États-Unis et au Canada (et bien que ces pays soient membres de l’IIHF), les ligues peuvent édicter leurs propres règles. Ainsi la Ligue nationale de hockey possède un règlement qui diffère quelque peu de celui de l’IIHF, notamment au sujet des combats, totalement interdits dans le règlement international.Le hockey sur glace se joue sur une patinoire qui peut être soit intérieure soit extérieure, la glace pouvant être naturelle ou artificielle. Dans la plupart des rencontres organisées au sein d'une compétition, la glace est artificielle. Il existe différentes tailles de patinoires mais la Fédération internationale de hockey sur glace définit des dimensions réglementaires : entre 56 et 61 m de long et entre 26 et 30 m de large. Pour les compétitions internationales, les dimensions minimales passent à 60 m sur 29 m. L'aire de jeu possède des angles arrondis avec un rayon compris entre 7 et 8,5 m. Des glaces de protection doivent être installées au-dessus des bords de la patinoire, les glaces faisant entre 1,6 et 2 m de haut derrière les buts et au minimum 80 cm de hauteur sur le reste de la patinoire.Plusieurs parties composent la patinoire : une zone défensive entre le fond de la patinoire et la première ligne bleue, une zone neutre entre les deux lignes bleues et enfin une zone offensive entre la deuxième ligne bleue et l'autre extrémité de la glace. Des lignes rouges au nombre de trois coupent également la patinoire dans le sens de la largeur : une ligne au centre de la patinoire et deux lignes constituant les lignes de but et distantes de chaque extrémité de 4 m. Les lignes bleues sont tracées de manière que l'espacement entre ces deux lignes et la ligne centrale soit égal. En Amérique du Nord, le territoire de but a également deux lignes rouges qui sont situées derrière le but qui forme un trapèze. La zone ainsi délimitée est la seule où le gardien de but peut toucher le puck avec sa canne; en dehors de cette dernière, une pénalité mineure lui est infligée.Différents points d'engagement sont présents sur la patinoire. Le point principal se trouve au milieu de la ligne centrale. Dans la zone neutre, il existe quatre autres points d'engagement. Deux sont situés du côté de la zone offensive d'une équipe, un à gauche et l'autre à droite, à 1,5 m de la ligne bleue ; et deux sont situés du côté de la zone offensive de l'autre équipe de manière symétrique. Les quatre derniers points d'engagement sont placés deux à deux dans la zone défensive de chaque équipe, un de chaque côté des buts. Les points servent à réaliser les engagements. À chaque début de période ou après chaque but, la remise en jeu se fait au centre de la patinoire. Pour les autres arrêts de jeu, c'est le point le plus proche de l'endroit où le jeu a été arrêté qui est utilisé (sauf cas de hors-jeu ou de dégagement interdit). Autour du point central et des points situés dans les zones défensives, des cercles de 4,5 m de rayon sont tracés au sol. Le point central est également le point de départ des tirs de fusillade et des lancers de pénalité.Deux autres catégories de tracés au sol réglementaires sont présents sur la glace. Le premier, le territoire des arbitres, est constitué d'un demi-cercle de 3 m de rayon centré sur le centre de la table de marque. Aucun joueur n'est autorisé à pénétrer dans cette zone quand l'arbitre est en communication avec un autre arbitre ; il peut encourir une pénalité de méconduite (dix minutes). La deuxième zone est celle des gardiens de but. Centrée sur le milieu du but, elle a un rayon de 1,8 m et se termine de chaque côté sur la ligne de but. Cette zone est peinte en bleue et aucun attaquant n'a le droit de s'y trouver quand un but est inscrit.Les buts de chaque équipe sont placés au centre de la ligne de but et sont constitués de deux poteaux de 1,22 m de haut séparés de 1,83 m. Ils sont reliés ensemble par une barre transversale. L'ensemble des éléments sont des tubes de 5 cm de diamètre et peints en rouge. Les buts sont complétés par un filet blanc qui doit se situer entre 60 et 1,12 cm de la ligne de but.Au cours d'une rencontre de hockey sur glace, chaque équipe est composée d'un nombre maximum de joueurs. Ce nombre est de vingt joueurs de champ et deux gardiens de but pour la Fédération internationale alors que pour la LNH, seulement 18 joueurs de champ sont acceptés. Chaque équipe doit présenter un capitaine et deux capitaines-adjoints, ou à la rigueur trois adjoints pour la LNH. Ce sont alors les seules personnes autorisées à discuter avec les arbitres de leur décision. Un gardien de but ne peut pas être désigné capitaine de son équipe.Au même moment, une équipe ne peut avoir que six joueurs sur la glace. En dehors de situations de jeu particulières, elle se compose d'un gardien de but, de deux défenseurs et de trois attaquants : un centre et deux ailiers. Les entraîneurs sont libres de changer les joueurs comme ils veulent sur la glace tant qu'il n'y a pas plus d'un gardien de but en même temps. En plus des joueurs, une équipe a le droit d'a"
sport;"Le hockey est un sport dans lequel deux équipes jouent l'une contre l'autre en essayant de manœuvrer une balle ou un palet (rondelle) dans le but de l'adversaire à l'aide d'une crosse. Il existe de nombreux types de hockey comme le bandy, le hockey sur gazon, le hockey sur glace ou le rink hockey.Dans la plupart des pays du monde, le terme hockey désigne en soi le hockey sur gazon, tandis qu'au Canada, aux États-Unis, en Russie et dans la plupart des pays d'Europe de l'Est et du Nord, le terme désigne généralement le hockey sur glace.La première utilisation enregistrée du mot hockey se trouve dans un livre de 1773, Juvenile Sports and Pastimes, to Which Are Prefixed, Memoirs of the Author: Including a New Mode of Infant Education par Richard Johnson, dont le chapitre XI est intitulé Nouvelles améliorations sur le jeu du hockey. La croyance que le hockey est mentionné dans une proclamation de 1363 par le roi Édouard III d'Angleterre est basée sur des traductions modernes de la proclamation, qui est à l'origine en latin et interdit explicitement les jeux Pilam Manualem, Pedivam, & Bacularem: & ad Canibucam & Gallorum Pugnam,. L'historien et biographe anglais John Strype n'utilise pas le mot « hockey » lorsqu'il traduit la proclamation en 1720, traduisant plutôt Canibucam par Cambuck. Cela peut faire référence à une forme précoce de hockey ou à un jeu plus semblable au golf ou au croquet.Le mot hockey lui-même est d'origine inconnue. Une supposition est qu'il s'agit d'un dérivé de hoquet, un mot du moyen français pour une houlette. Les extrémités incurvées ou «crochues» des bâtons utilisés pour le hockey auraient en effet ressemblé à ces bâtons. Une autre supposition découle de l'utilisation connue de bouchons de liège à la place de boules en bois pour jouer au jeu. Les bouchons provenaient de barriques contenant de la bière ""hock"", également appelée ""hocky"".Les jeux joués avec des bâtons courbes et une balle peuvent trouver dans l'histoire de nombreuses cultures. En Égypte ancienne, des sculptures vieilles de 4000 ans présentent des équipes avec des bâtons et un projectile, le hurling précède 1272 av. J.-C. en Irlande, et il y a une représentation d'environ 600 av. J.-C. dans la Grèce antique, où le jeu peut avoir été appelé ker?tízein ou (??????????) parce qu'il était joué avec une corne ou un bâton en forme de corne (kéras, ?????). En Mongolie-Intérieure, le peuple Daur joue au beikou, un jeu similaire au hockey sur gazon moderne, depuis environ 1000 ans.La plupart des preuves de jeux de type hockey au Moyen Âge se trouvent dans la législation concernant les sports et les jeux. Le Statut de Galway promulgué en Irlande en 1527 interdit certains types de jeux de balle, y compris les jeux utilisant des bâtons «à crochets» (écrits « hockie », semblables à «hooky»).Au XIXe siècle, les diverses formes et divisions des jeux historiques commencent à se fondre dans les sports individuels définis aujourd'hui. Des organisations dédiées à la codification des règles et règlements se forment, et des organismes nationaux et internationaux voient le jour pour gérer la concurrence nationale et internationale.Le bandy se joue avec un ballon sur une patinoire de la taille d'un terrain de football, généralement à l'extérieur, et avec de nombreuses règles similaires au football. Il est joué professionnellement en Russie et en Suède. Le sport est reconnu par le CIO ; son organe directeur international est la Fédération internationale de bandy.Le bandy prend ses racines en Angleterre au XIXe siècle, appelé à l'origine ""hockey sur glace"", avant de se répandre à d'autres pays européens vers 1900. Un sport russe similaire peut également être considéré comme un prédécesseur et en Russie, le bandy est parfois appelé «hockey russe». Des championnats du monde de Bandy ont lieu depuis 1957 et les championnats du monde féminin de bandy depuis 2004. Il existe des championnats nationaux de clubs dans de nombreux pays.Le hockey sur gazon se joue sur du gravier, du gazon naturel ou du gazon artificiel avec une petite balle dure d'environ 73 mm de diamètre. Le jeu est populaire parmi les hommes et les femmes dans de nombreuses régions du monde, en particulier en Europe, en Asie, en Australie, en Nouvelle-Zélande, en Afrique du Sud et en Argentine. Dans la plupart des pays, le jeu se joue entre des équipes non mixtes, bien qu'elles puissent l'être.L'organe directeur est la Fédération internationale de hockey sur gazon (FIH), qui compte 126 membres. Le hockey sur gazon masculin est pratiqué à chaque Jeux olympiques d'été depuis 1908, sauf en 1912 et 1924, tandis que le hockey sur gazon féminin est joué aux Jeux olympiques d'été depuis 1980.Les bâtons de hockey sur gazon modernes sont fabriqués à partir d'un composite de bois, de fibre de verre ou de fibre de carbone (parfois les deux) et sont en forme de ""J"", avec un crochet incurvé à l'extrémité de jeu, une surface plane du côté du jeu et une surface incurvée à l'arrière côté. Tous les bâtons sont droitiers - les bâtons gauchers ne sont pas autorisés.Alors que le hockey sur gazon dans sa forme actuelle est apparu au milieu du XVIIIe siècle en Angleterre, principalement dans les écoles, ce n'est que dans la première moitié du XIXe siècle qu'il s'est fermement établi. Le premier club est créé en 1849 à Blackheath dans le sud-est de Londres. Le hockey sur gazon est le sport national du Pakistan. C'était le sport national de l'Inde jusqu'à ce que le ministère de la Jeunesse et des Sports déclare en août 2012 que l'Inde n'a pas de sport national.Le hockey sur glace se joue entre deux équipes de patineurs sur une grande surface plane de glace, en utilisant un palet de 76,2 mm diamètre aussi appelé rondelle. Cette dernière est souvent gelée avant les parties de haut niveau pour réduire la quantité de rebond et de friction sur la glace. Le jeu est joué partout en Amérique du Nord, en Europe et à des degrés divers dans de nombreux autres pays du monde. C'est le sport le plus populaire au Canada, en Finlande, en Lettonie, en République tchèque et en Slovaquie. Le hockey sur glace est le sport national de la Lettonie et le sport national d'hiver du Canada.L'organe directeur du jeu international est la Fédération internationale de hockey sur glace (IIHF), composée de 77 membres. Le hockey sur glace masculin est joué aux Jeux olympiques d'hiver depuis 1924 et l'a été aux Jeux olympiques d'été de 1920. Le hockey sur glace féminin est ajouté aux Jeux olympiques d'hiver de 1998. La Ligue nationale de hockey (LNH) d'Amérique du Nord est la ligue professionnelle de hockey sur glace la plus célèbre, attirant les meilleurs joueurs de hockey sur glace du monde entier. Les règles de la LNH sont légèrement différentes de celles utilisées dans le hockey sur glace olympique dans de nombreuses catégories. Les règles internationales du hockey sur glace sont adoptées à partir des règles canadiennes au début des années 1900.Le sport contemporain s'est développé au Canada à partir d'influences européennes et indigènes. Celles-ci comprennent divers jeux de bâton et de balle similaires au hockey sur gazon, au bandy et à d'autres jeux où deux équipes poussent une balle ou un objet d'avant en arrière avec des bâtons. Celles-ci ont été jouées à l'extérieur sur glace sous le nom de «hockey» en Angleterre tout au long du XIXe siècle, et même plus tôt sous divers autres noms. Au Canada, il y a 24 rapports  de jeux de type hockey au XIXe siècle avant 1875 (cinq d'entre eux utilisant le nom «hockey»). Le premier match de hockey sur glace organisé et enregistré est joué à Montréal, le 3 mars 1875 avec plusieurs étudiants de l'Université McGill.Les crosses sont de longs bâtons en forme de ""L"" faits de bois, de graphite ou de composites avec une lame au bas qui peut reposer à plat sur la surface de jeu lorsque le bâton est tenu droit et peut légalement se courber dans les deux sens, que le joueur soit gaucher ou droitier.Le hockey en fauteuil roulant électrique, aussi appelé power hockey, est une forme de hockey pour les personnes nécessitant l'utilisation d'un fauteuil roulant électrique. Il est pratiqué en Europe et est régi par l'ICEWH (International Wheelchair and Amputies Sport).Le hockey sur luge ou para-hockey sur glace est une forme de hockey sur glace conçue pour les joueurs ayant un handicap physique affectant le bas du corps. Les joueurs s'assoient sur des traîneaux à double lame et utilisent deux bâtons. Chaque bâton a une lame à une extrémité et de petits pics à l'autre. Les joueurs utilisent les bâtons pour passer, manier et tirer le palet ainsi que propulser leurs traîneaux. Les règles sont très similaires aux règles de hockey sur glace de l'IIHF.Le roller in line hockey est une variante du roller hockey très similaire au hockey sur glace, dont il est dérivé. Il est joué par deux équipes, composées de quatre patineurs et d'un gardien de but, sur une patinoire sèche divisée en deux moitiés par une ligne médiane, avec un filet à chaque extrémité de la patinoire. Le match se joue en trois périodes de 15 minutes avec une variante de la règle du hors-jeu du hockey sur glace. L'organe directeur est l'IIHF, comme pour le hockey sur glace, mais certaines ligues et compétitions ne suivent pas les règlements de l'IIHF, en particulier USA Inline et Canada Inline.Le street-hockey est une variante de hockey sur glace et de roller hockey jouée toute l'année sur une surface dure (généralement de l'asphalte). Une balle est généralement utilisée à la place d'une rondelle et aucun équipement de protection n'est généralement porté.Les joueurs de street-hockey portent généralement des patins à roulettes ou des chaussures. Puisque le street-hockey est généralement joué de façon récréative (et non compétitive), les joueurs peuvent utiliser un ou deux filets en fonction de l'équipement et l'espace disponible.Le rink hockey, ou hockey sur patins, est un sport de hockey se jouant sur des patins à roulettes. Ce sport est pratiqué dans plus de soixante pays et a un public mondial. Le roller hockey est un sport de démonstration aux Jeux olympiques d'été de Barcelone de 1992.D'autres jeux dérivés du hockey ou de ses prédécesseurs existent :Air Hockey : se joue à l'intérieur avec une rondelle sur une table à coussin d'air.Le hockey de plage, une variante du hockey de rue, est un sport courant sur les plages du sud de la Californie.Le ballon-balai se joue sur une patinoire de hockey sur glace, mais avec une balle au lieu d'une rondelle et un «balai» (en fait un bâton avec un petit outil en plastique à l'extrémité) à la place du bâton de hockey sur glace. Au lieu des patins, des chaussures spéciales sont utilisées avec des semelles en caoutchouc très souples pour maximiser l'adhérence lors de la course.Le floorball est une forme de hockey jouée dans un gymnase ou dans une salle de sport. Une balle en plastique est utilisée et les bâtons ne mesurent qu'un mètre de long et sont fabriqués à partir de matériaux composites.Gena, ou Ganna, un sport de hockey sur gazon joué en Ethiopie.Le hockey en salle est une forme de hockey sur glace pratiquée dans un gymnase. Il utilise des bâtons avec des extrémités en mousse et une balle en mousse ou une rondelle en plastique.Le hurling et le camogie sont des jeux irlandais qui ont des ressemblances avec le hockey.Le hockey en salle est une variante du hockey sur gazon en salle.Le hockey d'antan est une forme simplifiée de hockey sur glace qui se joue sur de la glace naturellement gelée.La ringuette est une variante de hockey sur glace conçue pour les joueuses; il utilise un bâton droit et un anneau en caoutchouc à la place d'une rondelle. Les règles diffèrent de celles du hockey et ressemblent à un mélange de crosse et de basketball.Shinty est un jeu écossais qui se joue maintenant principalement dans les HighlandsLe hockey sur table se joue à l'intérieur sur une table.Le hockey subaquatique se joue avec une rondelle lestée au fond d'une piscine.Le hockey sous glace est similaire au hockey sous-marin, mais il se joue avec une rondelle flottante sur le dessous d'une piscine gelée.Bowlsby, Craig. 1913: The Year They Invented The Future of Hockey (2013)Ellison, Jenny.  and Jennifer Anderson, eds. Hockey: Challenging Canada’s Game (2018)Carl Gidén, Patrick Houda et Jean-Patrice Martel, On the Origin of Hockey, Createspace, 2014 (ISBN 9780993799808)Gruneau, Richard.  and David Whitson. Hockey Night in Canada: Sport, Identities, and Cultural Politics (1993),Hardy, Stephen and Andrew C. Holman. Hockey: A Global History (U of Illinois Press, 2018).  online review 600 ppHolzman, Morey,  and Joseph Nieforth. Deceptions and Doublecross: How The NHL Conquered Hockey (2002),McKinley, Michael. Putting A Roof on Winter: Hockey’s Rise from Sport Spectacle (2000), on Canada and U.S.Andrew Podnieks et Szymon Szemberg, World of hockey : celebrating a century of the IIHF, Fenn Publishing, 2007 (ISBN 9781551683072) Portail du sport"
sport;"Le volley-ball, ou volleyball, est un sport collectif opposant deux équipes de six joueurs séparées par un filet de hauteur variable selon le niveau, qui s'affrontent avec un ballon sur un terrain rectangulaire de 18 mètres de long sur 9 mètres de large. Le volley-ball se joue soit à l'intérieur d'un gymnase, soit sur un terrain sablé de dimensions différentes; on parlera alors de volley-ball de plage. Avec 260 millions de pratiquants licenciés au niveau mondial en 2018, il s'agit de l'un des sports les plus pratiqués dans le monde,.  Le volley-ball a été inventé le 9 février 1895 aux États-Unis par un professeur d’éducation physique des UCJG (YMCA), à Holyoke dans le Massachusetts, William G. Morgan (1870-1942), afin d'occuper les athlètes pendant l'hiver. C'est en s'inspirant à la fois du basket-ball et également du tennis, mais surtout du badminton (qui fournit le premier modèle de filet), qu'est née la « mintonette », le 2 décembre 1895. Un autre sport de salle, le basket-ball, a été inventé seulement dix miles (seize kilomètres) plus loin dans la ville de Springfield (Massachusetts), seulement quatre années auparavant. La mintonette se devait d'être un sport de salle moins violent que le basket-ball, pour les membres les plus âgés du Young Men's Christian Association, tout en exigeant toujours un minimum d'effort physique.Les premières règles, écrites par William G. Morgan, instauraient un filet de 1,98 m de hauteur, un terrain de 15,2 × 7,6 m, et un nombre de joueurs illimité. Un match était composé de neuf tours avec trois services pour chaque équipe dans chaque tour, avec un nombre de contacts avec la balle illimité pour chaque équipe avant son renvoi à l'adversaire. En cas d'une erreur de service, un deuxième essai était permis. Le fait de frapper la balle dans le filet était considéré comme une faute (avec la perte du point ou d'un temps-mort) sauf si cela se passait à la première tentative de service[réf. nécessaire].Après avoir observé ce sport, Alfred Halstead remarqua la nature de « volée » dans le jeu à son premier match d'exhibition en 1896. Joué à l'International YMCA Training School (aujourd'hui appelé Springfield College), le jeu fut rapidement connu sous le nom de volley-ball (il a été à l'origine orthographié en deux mots: « volley ball »). Les règles du volley-ball furent légèrement modifiées par l'International YMCA Training School et la propagation du jeu dans les UCJG (YMCA) différents,.La nature du premier ballon officiel utilisé au volley-ball est contestée ; plusieurs sources disent que Spalding a créé le premier ballon officiel en 1896. Les règles ont évolué au cours du temps. En 1916, l'attaque est introduite, et trois années plus tard, la règle des trois touches est instaurée. En 1917, le set passe de 21 à 25 points. En 1919, environ 16 000 ballons de volley-ball sont distribués par l'American Expeditionary Forces à leurs troupes et leurs alliés, ce qui suscite la croissance de volley-ball dans de nouveaux pays,.Le premier pays, en dehors des États-Unis, à adopter le volley-ball est le Canada en 1900. Une fédération internationale, la Fédération internationale de volley-ball (FIVB), est fondée à Paris en 1947, et le premier championnat du monde se tient en 1949 pour les hommes et en 1952 pour les femmes. Le sport est aujourd'hui populaire au Brésil, en Europe (les équipes d'Italie, des Pays-Bas et des pays de l'Europe de l'Est sont des équipes de premier plan depuis la fin des années 1980), en Russie, et dans d'autres pays incluant la Chine et le reste de l'Asie, aussi bien qu'aux États-Unis,.Le beach-volley, une variante du jeu joué sur le sable avec seulement deux joueurs par équipe, intègre la FIVB en 1987 et devient un sport olympique aux jeux de 1996,.L'histoire du volley-ball aux Jeux olympiques remonte aux Jeux olympiques d'été de 1924 à Paris, où il est pratiqué dans le cadre d'un événement de démonstration sportif américain. Après la fondation de la FIVB et de quelques confédérations continentales, on commence à envisager son inclusion officielle. En 1957, un tournoi spécial se tient à la 53e session du Comité international olympique à Sofia (Bulgarie), pour soutenir une telle demande. La compétition est un succès et le sport est officiellement inclus dans le programme pour les Jeux olympiques d'été de 1964.Le tournoi de volley-ball Olympique était à l'origine une compétition simple, dont le format est semblable à celui toujours employé dans la Coupe du Monde : toutes les équipes jouent l'une contre l'autre et sont ensuite classées par les victoires, la moyenne de set, et la moyenne de point. Un inconvénient de ce système de round-robin est que les vainqueurs de médaille pourraient être déterminés avant la fin des jeux, entraînant une perte d'audience pour le résultat des matches restants. Pour changer cette situation, la compétition fut composée de deux phases avec l'addition d'une « final round », un tournoi d'élimination se composant des quarts de finale, des demi-finales et des finales en 1972. Le nombre d'équipes impliquées au tournoi Olympique a grandi progressivement depuis 1964. Depuis 1996, les événements masculins et féminins comptent douze nations participantes. Chacune des cinq confédérations de volley-ball continentales a au moins une fédération nationale affiliée impliquée dans les Jeux olympiques.L'URSS remporte deux médailles d'or dans la compétition masculine en 1964 et 1968. Après avoir remporté le bronze en 1964 et l'argent en 1968, le Japon remporte finalement l'or chez les garçons en 1972. Chez les femmes, l'or est revenu aux Japonaises en 1964, qui ont reproduit la même performance en 1976. La même année, l'introduction d'une nouvelle adresse offensive a permis à la Pologne de gagner la compétition masculine sur les Soviétiques dans un match très serré de cinq sets. Depuis que les plus fortes équipes masculines de volley-ball appartenaient aux pays de l'Est, le boycott américain n'avait pas autant d'effet sur ces événements que sur la compétition féminine. L'URSS remporte son troisième titre olympique chez les hommes en battant en finale la Bulgarie trois sets à un (même résultat pour l'équipe féminine qui remporte son troisième titre). Avec le boycott de l'URSS aux Jeux olympiques de 1984 à Los Angeles, les États-Unis ont balayé le Brésil dans les finales pour la médaille d'or des hommes. L'Italie remporte sa première médaille (le bronze chez les garçons) en 1984.Aux jeux de 1988, Karch Kiraly et Steve Timmons conduisent les États-Unis à un deuxième titre olympique de suite après celui de 1984. En 1992, le Brésil contrarie les Pays-Bas et l'Italie dans la compétition masculine pour remporter sa première médaille d'or. Second, les Pays-Bas, médaillés d'argent chez les hommes en 1992, reviennent emmenés par leurs leaders, Ron Zwerver et Olof van der Meulen, aux jeux de 1996 pour un match de cinq sets en battant l'Italie et remportent le tournoi. Médaillée de bronze masculin en 1996, la Serbie-et-Monténégro (jouant en 1996 et 2000 en tant que République fédérale de Yougoslavie) bat la Russie en finale en 2000, remportant sa première médaille d'or. En 2004, le Brésil remporte un deuxième titre olympique masculin en battant l'Italie en finale. En 2008, après vingt années de disette, la sélection américaine remporte son troisième titre olympique aux dépens du Brésil. C'est son premier succès mondial depuis sa domination des années 1980.Le matériel nécessaire se compose des éléments suivants :2 poteaux2 mires (rouge/blanc) posées sur le filet aux 2 extrémités du terrain. On peut aussi dire dans le langage courant les mires mais le terme exact est antenne.1 filet1 ballon1 terrainLe volley-ball se pratique le plus souvent en salle. Le terrain a une forme rectangulaire de 18 mètres de longueur sur 9 mètres de largeur. Les lignes de délimitation sont à l'intérieur du terrain. Une ligne centrale s'étend sous le filet sur toute la largeur du terrain et sépare les deux camps. Une ligne d'attaque est peinte au sol dans chaque moitié de terrain, à trois mètres du filet ; elle est communément appelée « ligne des 3 mètres ».Les dimensions du terrain de volley ne varient jamais.2 camps de 9 × 9 m chacun que ce soit pour une équipe minime ou internationale.Voir la photo du terrain à droite.La taille du terrain est plus petite chez les poussin(ne)s (9-10 ans) ou les benjamin(ne)s (11-12 ans)Pour le public scolaire, la dimension du terrain et le nombre de joueurs sur le terrain diminuent.Chaque équipe occupe une moitié du terrain séparée de l'autre par un filet d'1 mètre de large et 9m de long, dont la bordure supérieure est placée à une hauteur variable en fonction de la catégorie d'âge des joueuses ou joueurs : Deux antennes (barres verticales également appelées « mires ») sont accrochées au filet à hauteur des limites du terrain (le filet est généralement plus large que ce dernier). Le ballon doit passer entre ces antennes lors des échanges (sauf exceptions) entre équipes lors des phases de jeu. Deux bandes blanches, accolées aux antennes, sont situées sur le filet à la verticale des lignes de côté.Le ballon de volley-ball est plus souple et plus léger que celui de football. Il doit avoir une circonférence comprise entre 65 et 67 cm, une masse comprise entre 260 et 280 g et une pression comprise entre 0,300 et 0,325 7 bar. Depuis 1978, pour diminuer le temps de jeu, trois ballons sont utilisés lors des rencontres internationales et nationales. Finies les pertes de temps pour le récupérer au service, puisque quatre ramasseurs de balles sont placés autour du terrain. Ce système a permis une diminution d'environ 20 % du temps de jeu.Les points sont marqués soit en faisant tomber le ballon sur le terrain de l'équipe adverse, soit quand l'adversaire commet une faute. La première équipe à atteindre 25 points (avec deux points d'écart minimum) gagne le set et la première équipe qui gagne trois sets gagne le match. Dans le cas d'un score à deux sets partout, les équipes jouent un 5e set décisif. La première équipe ayant marqué 15 points (avec deux points d'écart minimum) remporte le 5e set et le match.Chaque équipe peut toucher le ballon jusqu'à trois fois (en plus d'un éventuel contre) avant que le ballon ne retraverse le filet, et les contacts consécutifs doivent être faits par des joueurs différents. Le ballon est d'habitude joué avec les mains, bras ou les poings mais les joueurs ont le droit de toucher le ballon avec les pieds.Le but du volley-ball est d'éviter de faire tomber la balle dans son propre camp. Pour éviter cela, quatre problèmes majeurs se posent; il faut récupérer la balle, l'envoyer dans les trois mètres, la garder dans les trois mètres, et l'envoyer chez l'adversaire. De plus les deux équipes sont séparées par un filet, il s'agit donc de communiquer avec son équipe et perturber verbalement l'autre équipe. Enfin, pour agresser l'adversaire il est préférable de frapper la balle, c'est pourquoi la gestion de son corps et la gestion de la pression temporelle sont importantes.Une équipe de volley-ball se compose de six joueurs sur le terrain : trois avants et trois arrières. La position des joueurs est généralement désignée par un numéro de 1 à 6 : 1 étant le joueur arrière droit (défenseur droit ou serveur), 2 l'avant droit (attaquant), 3 l'avant centre (attaquant central), 4 l'avant gauche (attaquant), 5 l'arrière gauche (défenseur gauche), et 6 l'arrière centre (défenseur central), ce qui donne cette configuration :Cette numérotation correspond à l'ordre de service lors du début de set. Le poste 1 est occupé par le premier joueur à servir. Le poste 2 correspond au joueur qui servira en deuxième, et ainsi de suite.Chaque joueur est tenu d'être à sa position lors de la mise en jeu de la balle. En revanche, dès la balle mise en jeu, les joueurs sont libres de se déplacer sur le terrain à leur guise, mais les joueurs arrière ne peuvent attaquer qu'en dehors des trois mètres et ne peuvent contrer ; en fait, dès qu'ils sont dans la zone d'attaque, ils ne peuvent pas renvoyer une balle de l'autre côté si le contact a lieu avec la balle entièrement au-dessus du filet.Les joueurs de l'équipe qui récupèrent le service font une rotation dans le sens des aiguilles d'une montre. Le joueur P2 devient P1, etc.Les phases de jeu sont les suivantes :le service est effectué par le joueur en position 1. Il s'effectue depuis l'arrière du terrain. Le joueur se place derrière la ligne de fond du terrain et frappe la balle à une main afin de la faire retomber à l'intérieur du terrain adverse (la balle peut toucher le filet). Le joueur est autorisé à lancer la balle, sauter, et smasher la balle : on appelle cela un service smashé, de plus en plus répandu à haut niveau ; dans tous les cas, ses pieds doivent rester à l'extérieur du terrain lors de l'appel (ne pas mordre la ligne de fond). La réception du saut peut se faire à l'intérieur du terrain après la frappe de la balle ;L'équipe adverse reçoit la balle, elle a le droit de la toucher trois fois (le contre ne comptant pas comme touche) avant de la renvoyer à son tour de l'autre côté du terrain. Un joueur ne peut pas toucher deux fois consécutivement la balle (mais après un contre, la première touche peut être faite par le contreur).L'échange continue alors jusqu'à ce qu'une des deux équipes commette une faute. Les fautes les plus courantes sont les suivantes :'faute de position' : ne pas respecter le placement correct des joueurs à l'instant où le serveur frappe la balle. Le joueur 1 doit être derrière le 2, le 3 entre le 2 et le 4, le 6 entre le 5 et le 1 et derrière le 3, et le 5 derrière le 4.faute de rotation : lorsque le serveur qui a effectué le service n'est pas le bon. En effet, après récupération du point, les joueurs tournent une fois dans le sens des aiguilles d'une montre.balle in : laisser la balle toucher le sol à l'intérieur des limites de son terrain ;faute des quatre touches : effectuer une quatrième touche avant de retourner la balle dans le camp adverse ;balle out : envoyer la balle de telle sorte qu'elle touche le sol en dehors des limites du terrain, ou un joueur envoie une balle qui ensuite touche l'antenne (communément appelée mire) ;faute au filet : toucher la bande blanche qui marque la partie supérieure du filet entre les mires avec une quelconque partie du corps ou des vêtements (nouvelle règle applicable depuis 2009) (Nouveau changement depuis 2015, tout filet touché est faute). Il faut que la faute soit volontaire, toucher le fil en se retournant lorsque le joueur n'est pas dans une action n'est plus considéré comme une faute. De même, les nouvelles règles ne sifflent plus faute de fil lorsque les cheveux touchent le fil.faute de pénétration : mettre le pied dans le camp adverse ou pénétrer dans l'espace adverse en gênant l'adversaire (toucher le camp adverse avec la main n'est plus en soi une faute, nouvelle règle applicable depuis 2009). On peut faire pénétrer n'importe quel partie de son corps de l'autre côté à condition que le pied n'ait pas entièrement passé et que cela ne dérange pas le jeu adverse.faute des deux touches ou double touche : un même joueur touche la balle deux fois successivement (hors contre et hors première touche), néanmoins si la balle touche le filet après la premiere touche le joueur peut retoucher la balle ;faute de frappe d'attaque : un joueur arrière (position 1, 5 ou 6) attaque une balle plus haute que le filet en étant à l'intérieur de la zone d'attaque délimitée par la ligne des trois mètres (sauf s'il prend son appel derrière la ligne des 3 mètres), ou le Libéro effectue une passe à dix doigts à l'intérieur de la zone des trois mètres qui est attaquée par un joueur au-dessus du filet ;ballon tenu ou transport ou portée: la balle est touchée de manière inadéquate par un joueur (toutes les frappes doivent être franches, il est interdit d'attraper le ballon puis de le relancer) ;faute de service trop long : aucun service ne s'est fait de la part d'une équipe 8 secondes après que l'arbitre en a donné le signal (coup de sifflet ainsi que signe de la main).faute de contre : le joueur contre la balle directement au-dessus du filet sur le service adverse.faute de ligne : lorsqu'un joueur arrière saute à l'intérieur de la ligne des 3 mètres lorsqu'il attaque, ou lorsque le serveur touche à la ligne extérieure du terrain au moment du service.Un point est alors marqué et l'équipe ayant marqué ce point gagne (ou conserve) le service. Si cette équipe n'avait pas le service, les joueurs de cette équipe effectuent alors une rotation sur le terrain dans le sens des aiguilles d'une montre (le 1 prend la place du 6, qui prend la place du 5, etc.). Le service est effectué alors par le joueur passant du poste 2 au poste 1.Les règles du volley-ball ont été largement remaniées entre 1998 et 2000 et autorisent désormais de toucher la balle avec toutes les parties du corps : autrefois, seules les parties au-dessus de la ceinture étaient autorisées. Le principe de comptage des points a été modifié : les sets se jouaient auparavant en 15 points et une équipe ne marquait de point que si elle avait le service. Lors du service, le ballon ne devait pas toucher le filet. Enfin, un joueur particulier a été introduit : le libéro, spécialiste en défense, qui ne peut ni attaquer, ni contrer ni servir.Lors de certaines compétitions comme la ligue mondiale, il est possible de faire appel à l'arbitrage vidéo, appelé challenge (utilisé pour la première fois en 2012 lors de la Ligue mondiale). Les équipes ne peuvent demander le ""challenge vidéo"" qu'à la fin d'un échange et sur la dernière action. Les équipes gardent leur droit de demander un autre ""challenge vidéo"" si leur demande est justifiée, avec une limite de deux ""challenge vidéo"" non validés par set.Pour jouer au volley-ball, le joueur peut avoir des tennis qui tiennent très bien la cheville. Les chaussures sont à semelle plates pour mieux adhérer au terrain. La plupart des joueurs plus avancés se procurent des chevillères car les blessures à la cheville sont fréquentes. Certains portent du « strap » pour soutenir une blessure ou pour se durcir les doigts et renforcer la frappe. Les joueurs ont besoin de genouillères pour bien protéger leurs genoux au sol et d'un maillot d'équipe portant un numéro pouvant aller de 1 à 99. La grande majorité du temps tous bijoux doivent être retirés de sur le joueur pour des questions de sécurité. Si celui-ci ne peut pas les enlever, il devra alors les recouvrir d'un ruban adhésif.Ces deux joueurs attaquent à l'aile (en position 2 ou 4) et aux trois mètres en position 6. À l'arrière, le complet est, avec le libéro, prioritaire pour faire la réception.NB : les joueurs appelés « complets » sont parfois aussi ceux qui sont choisis pour être en opposition au passeur, dans les équipes sans pointu. À ce moment-là, ce joueur doit être capable de réceptionner, d'attaquer, de bloquer, de servir. Lorsqu'il reste en position arrière, il peut aussi attaquer derrière les trois mètres. Cette attaque est couramment appelée ""pipe"". Elle se distingue de l'attaque de l'opposite en étant une balle plus rapide et souvent moins haute que l'attaque classique des trois mètres.Aussi appelé « joueur à la technique » ou opposite, c'est le joueur placé à l'opposé du passeur. Il attaque généralement en poste 2 sauf en phase de réception lorsque lui-même est sur la position 4 afin d'éviter des rotations inutiles. Il attaque alors en 4. Lorsqu'il est arrière, le pointu est déchargé de la réception afin de pouvoir attaquer aux trois mètres (le plus souvent en poste 1). Pointus et complets peuvent être regroupés sous l'appellation « ailiers ».À un niveau moins élevé, le pointu devient un « relanceur », et, son rôle consiste à prendre énormément la réception, et, le cas échéant, à remplacer le passeur, puisqu'il lui reste opposé. Lorsque le passeur est en réception *, c'est au pointu de prendre la passe, c'est en quelque sort un second passeur. Les joueurs au centre sont ceux qui se placent en position 3 et 5. En poste 3, les centraux ont pour principale fonction d'attaquer en fixe (passe courte et rapide du passeur) et en décalée (passe rapide où le central se trouve à 2 m du passeur) ou, dans le cas d'une feinte d'attaque, de « fixer » (attirer) la défense adverse (le contre ou mur) pour l'empêcher d'aller contrer un attaquant ailier. Le central sort souvent sur les postes arrière pour laisser sa place au libéro (car c'est un poste très épuisant). Il sort après avoir servi en position 1 et rentre en 4.  Les centraux servent aussi a bloquer la balle de l'adversaire qui attaque.Selon le système tactique mis en place, il y a un passeur (système 5-1) ou deux passeurs (système 4-2). Dans un système 5-1 (5 attaquants et 1 passeur), le passeur se place après le service généralement :en 2 quand il est sur les positions avant (2, 3 ou 4),en 1 quand il est sur les positions arrière (1, 5 ou 6).C'est le système le plus utilisé à haut-niveau, il y a un seul passeur et les autres postes sont relativement spécialisés. Cela permet une grande incertitude sur les possibilités à l'attaque, et les permutations entre les joueurs permettent de les placer là où ils sont les plus performants. En revanche, ce système nécessite des déplacements importants du passeur. Dans un système 4-2 (4 attaquants et 2 passeurs), les deux passeurs sont sur des positions opposées (quand un passeur est devant, l'autre est derrière), ainsi il y a toujours un passeur devant, c'est lui qui fait la passe. La position de chaque passeur après le service fonctionne sur le même principe qu'avec un seul passeur.Quand les deux passeurs attaquent également (le passeur sur les positions avant attaque sur des passes faites par le passeur sur les positions arrière) on parle d'un système en « faux 4-2 », en « 4-2 amélioré », ou en 6-2 qui permet d'avoir toujours 3 attaquants sur les postes avant. Dans ce système de jeu, c'est le passeur arrière qui fait toujours la passe (sauf s'il défend). Les passeurs peuvent aussi réaliser des deuxièmes mains : lorsque la réception est très bien effectuée et qu'il est près du filet, il peut sauter et placer la balle directement de l'autre côté. À haut niveau, le passeur peut sauter sur chaque balle afin d'attirer lui aussi le bloc adverse, et donc pouvoir alléger le travail des ailiers qui n'attaqueront que face à un seul bloc.Le rôle du passeur est de mener le jeu et de distribuer les balles aux attaquants en fonction du jeu adverse; c'est lui qui est responsable en grande partie de l'efficacité du système offensif de son équipe. Les points fort d'un passeur sont la précision, la rapidité, et la bonne lecture de jeu. Le joueur libéro est différent des autres joueurs. Il a pour fonction de faire des réceptions de service, des défenses et des relances vers le passeur. Il se doit donc d'exceller dans la première touche de balle de l'équipe. L'échange de joueurs entre libéro et central par exemple dans ce cas particulier n'a pas besoin d'être noté par l'arbitre. Il ne peut rentrer que sur les trois positions de la ligne arrière et il lui est interdit de servir et d'attaquer le ballon lorsque celui-ci est entièrement au-dessus du plan haut du filet et même de participer au contre. De plus, s'il transmet le ballon à un attaquant en effectuant une touche haute (ou à 10  doigts) et qu'il se trouve à l'intérieur de la zone des 3 mètres, l'attaquant ne peut attaquer ou passer le ballon chez l'adversaire que si le ballon est redescendu sous le plan haut du filet matérialisé par une bande blanche. Il peut par contre transmettre le ballon en manchette sans restriction : celui-ci peut être attaqué normalement. Il est le point fort du secteur réception-défense. Pour un schéma tactique traditionnel, il rentre sur chacun des centraux, après leur position de service, et tourne sur les trois positions arrière. Sur le terrain, il porte un maillot différent des autres joueurs de son équipe. L'équipe peut nommer deux libéros mais il ne peut en avoir qu'un sur le terrain.C'est la touche de base. Le geste consiste à toucher la balle devant soi, au-dessus du front, avec la pulpe des doigts répartis de part et d'autre du ballon. Contrairement à ce qui est visible, le mouvement des bras sert essentiellement à amortir le contact avec la balle, mieux la maîtriser et assurer la direction de la passe. La puissance et la portée de la passe vient des appuis, de la poussée des jambes au moment du contact.Rappelons qu'un contact prolongé avec le ballon est interdit au volley. C'est le seul sport collectif ayant cette caractéristique.La passe étant plus précise que la manchette, elle est de plus en plus utilisée lors de la réception des services flottants (smashés ou non). On différencie  trois types de passes; premièrement la ""passe avant"" qui a été décrite précédemment, où le but est d'envoyer la balle vers l'avant (ou devant soi). Le deuxième type de passe est la passe arrière; le passeur est situé sous le ballon, on a une extension du tronc vers l'arrière. Les bras s'étendent dans le prolongement du tronc et les poignets sont tirés vers l'arrière. Ce type de passe permet d'envoyer la balle derrière soi. Enfin, il existe la passe en suspension, c'est-à-dire au moment du contact avec la balle le joueur se trouve en extension et en parfait équilibre. C'est la même technique que la passe avant mais elle permet d’accélérer le temps de jeu.C'est le mouvement utilisé lorsque la balle est basse ou rapide (réception de service, défense sur un smash). Le plan de contact s'effectue au niveau de l'intérieur des avant-bras, les bras étant tendus et étant plus bas que les épaules, formant un angle avec le buste, cet angle étant variable selon la distance par rapport au passeur. La poussée se fait au niveau des jambes, tout en gardant l'angle entre le buste et les bras. La manchette sert à amener la balle au passeur qui lui va faire une passe.Il est important de bien placer droit les bras, pour obtenir une manchette efficace (qui ne passera pas en ligne droite de l'autre côté du filet) et de les garder parallèles au sol[pas clair].Souvent spectaculaire, la Corse est le geste défensif utilisé en ultime recours, lorsque le défenseur se trouve en « crise de temps ». Elle consiste à plonger vers l'avant pour glisser sa main, paume plaquée au sol, sous le ballon au moment du rebond, en sorte que ce dernier ne touche pas le sol. L'origine de cette appellation pourrait venir du fait que la position du corps (allongé bras tendu) ressemble à la Corse et au Cap Corse; l'expression pourrait aussi être « une allusion au goût supposé des insulaires pour la sieste ».En Belgique francophone, le terme utilisé pour ce geste est « sprawl ». Ce mot vient de l'anglais sprawl qui signifie « s'étendre », « s'étaler ».Les anglophones utilisent eux le terme de pancake.C'est la touche d'attaque. Il s'agit d'un geste très technique, la balle étant frappée par le joueur alors qu'il est en suspension. Il existe différents types d'attaques : l'attaque puissante, qui cherche à forcer le contre, c'est-à-dire à empêcher toute récupération après celui-ci. Elle peut aussi provoquer le block-out, c'est-à-dire frapper les mains du contreur de manière à être déviée vers l'extérieur du terrain.l'attaque feintée, aussi appelée ""roulette"", qui consiste à simuler un smash puissant et à amortir son geste au dernier moment, afin de lober le bloc et de surprendre la défense ;l'attaque placée, en un point où la défense sera incapable de la reprendre ;Les attaques se font sur les postes avant, c'est-à-dire les postes 4, 2 et 3 (central) et sur les postes arrière, généralement sur les postes 1 et 6 (attaques aux trois mètres). Dans ce cas le joueur arrière prend son appel derrière la ligne des trois mètres sans la toucher pour attaquer le ballon au-dessus du filet.Le libéro ne peut attaquer ni dans la zone avant ni dans la zone arrière. Il est le seul joueur qui ne peut pas attaquer au-dessus du filet.Pour surprendre l'adversaire, le smash peut être remplacé par une feinte. Le ballon n'est pas frappé, mais poussé avec trois doigts. L'élan est exactement le même que celui du smash, et le geste permet de placer la balle précisément, en visant un trou. Le but est de prendre de vitesse la défense, qui s'attend à une attaque franche.Sur les balles difficiles, il est possible d'attaquer pieds au sol. Ce type d'attaque est très prévisible. Une alternative est de faire un ""hammershot"", en frappant la balle avec les deux poings joints au-dessus de la tête. Ce geste, puissant et précis, est de plus en plus utilisé dans les équipes masculines de haut niveau.C'est un mouvement défensif (en salle, il n'est pas compté parmi les trois touches autorisées. A contrario, il est compté comme une touche de balle en beach-volley). Son objectif est d'empêcher le ballon de passer dans son camp tout en faisant tomber le ballon dans le terrain adverse. Dans certaines options tactiques, il peut être « défensif » afin de conserver le ballon dans son camp en facilitant le jeu des défenseurs pour enchaîner sur une phase d'attaque. L'objectif ici est de couvrir une zone et de ralentir une attaque puissante pour faciliter la reconstruction. Le bloc peut aussi être offensif, c'est-à-dire essayer d’empêcher la balle de franchir le filet.  Lorsqu'un joueur fait un contre et que le ballon retombe moins d'un mètre derrière le filet, chez l'adversaire, ce contre est alors appelé une « équerre », c'est la plus belle façon de contrer. Le contre est le travail principal du centre. Le libéro ne peut ni contrer, ni effectuer une tentative de contre (c’est-à-dire sauter lors d'un contre seul ou à plusieurs sans intention réelle de contrer). Un joueur arrière ne peut pas contrer ou participer à un contre effectif. Enfin, un service ne peut être contré.C'est la touche d'engagement. C'est toujours le joueur en poste 1 qui sert. Le serveur doit se placer derrière la ligne de fond de son terrain (sans marcher dessus), où il le souhaite en profondeur, mais rester dans les limites du terrain en largeur. Le serveur dispose de 8 secondes après le coup de sifflet de l'arbitre pour effectuer son service. Il doit frapper la balle à une main. Avant la frappe, le ballon doit être lancé ou lâché (il ne peut être tenu). Si le ballon touche le filet, mais passe dans le camp adverse, le service est validé. Au moment du service, les joueurs doivent respecter leur position pour la rotation en cours sous peine de faute. Il n'y a pas d'erreur quant à la position du serveur par rapport aux autres joueurs lors du service. À l'exception du serveur, tous les joueurs doivent être entièrement à l'intérieur du terrain lors du service : ils ne doivent pas toucher le terrain à l'extérieur des lignes. Une fois la balle frappée, les joueurs peuvent sortir du terrain et changer de poste (les joueurs aux postes 1, 5 et les joueurs aux postes 2, 3 et 4 entre eux).Il existe plusieurs types de services : Le « service cuillère » C'est le service utilisé habituellement par les débutants. Il consiste à prendre le ballon de la main gauche (pour un droitier), de tendre ce bras en avant à hauteur du bassin, lancer légèrement le ballon en hauteur et de le frapper par-dessous avec la main droite pour le faire « voler » en avant. Le service flottant Ce type de service ne nécessite pas l'utilisation des jambes. Le joueur lance sa balle en hauteur et la frappe en utilisant le geste caractéristique de l'attaque, sans toutefois rabattre sa main totalement. Le geste est arrêté au moment précis de l'impact entre ballon et main ferme. Le joueur peut également effectuer un contre poids avec son corps pour augmenter la puissance de la frappe. La balle suivra une trajectoire flottante qui rend incertain l'endroit précis où la balle est censée toucher le sol. Ce flottement met donc le réceptionneur en difficulté. Le service smashé Le service smashé est le type de service pratiqué le plus fréquemment par les professionnels. Ce service nécessite l'utilisation des membres inférieurs. Pour ce faire, le joueur doit se placer un peu après la limite du terrain, lancer très haut son ballon et effectuer une petite course d'élan (même course que celle de l'attaque) pour frapper sa balle lors de la suspension. Ce type de service très puissant nécessite une position de réception parfaite, néanmoins il est assez simple à réceptionner car la balle tournante et très rapide rebondit sur les bras du réceptionneur, qui n'a pas besoin de faire d'efforts pour la ramener en l'air. Le service smashé flottant (ou service « sauté » flottant) Ce service est très utilisé par les professionnelles féminines, mais également de plus en plus par les masculins. Il consiste à prendre une course d'élan (moins "
sport;"Les Jeux olympiques d'hiver de 1952, officiellement connus comme les VIes Jeux olympiques d'hiver, ont lieu à Oslo en Norvège, du 14 au 25 février 1952. Les discussions sur l'organisation des Jeux olympiques d'hiver par Oslo commencent dès 1935 puisque la ville veut accueillir les Jeux de 1948 mais la Seconde Guerre mondiale entraîne le report de ce projet. Après la guerre, pour l'accueil des Jeux de 1952, une compétition oppose Oslo aux villes de Cortina d'Ampezzo en Italie et de Lake Placid aux États-Unis, ce sont les Norvégiens qui s'imposent. Tous les sites sont dans la zone métropolitaine d'Oslo à part les épreuves de ski alpin qui ont lieu à Norefjell, à 113 kilomètres de la capitale. Un nouvel hôtel est construit pour héberger la presse et les délégations avec trois dortoirs pour accueillir les athlètes et les entraîneurs ; cela devient le premier village olympique moderne. La ville d'Oslo assume la charge financière de l'accueil des Jeux en échange des revenus générés.Les Jeux rassemblent 694 athlètes de 30 pays qui participent dans six sports et 22 épreuves. Le Japon et l'Allemagne font leur retour dans la compétition olympique, après avoir été écartés du « Mouvement olympique » à la suite de leur rôle durant la Seconde Guerre mondiale. L'Allemagne est uniquement représentée par des athlètes de l'Allemagne de l'Ouest car l'Allemagne de l'Est refuse de participer en tant qu'équipe unifiée. Le Portugal et la Nouvelle-Zélande font leurs débuts aux Jeux d'hiver et, pour la première fois, les femmes sont autorisées à participer au ski de fond à travers une nouvelle épreuve, le 10 km.Le camionneur norvégien Hjalmar Andersen gagne trois des quatre épreuves de patinage de vitesse pour devenir l'athlète le plus médaillé de ces Jeux. L'Allemagne retrouve son hégémonie passée au bobsleigh en remportant les épreuves du bob à deux et du bob à quatre. L'Américain Dick Button exécute le premier saut à triple rotation dans une compétition internationale pour obtenir son second titre olympique consécutif chez les hommes en patinage artistique. Les Jeux de 1952 comportent un sport de démonstration, le bandy, mais uniquement trois pays, tous nordiques, participent au tournoi. La Norvège domine le tableau général des médailles avec seize récompenses dont sept en or. Les Jeux se terminent avec la présentation d'un drapeau qui est transmis d'une ville hôte des Jeux olympiques d'hiver à la suivante. La bannière, connue plus tard sous le nom du « drapeau d'Oslo », est déployée depuis dans la ville hôte lors de chaque édition des Jeux olympiques d'hiver.Oslo soumet en vain sa candidature pour l'organisation des Jeux olympiques d'hiver de 1936 mais perd contre l'Allemagne qui accueille les Jeux olympiques d'été de 1936. À cette époque, la nation qui organise les Jeux d'été accueille également les Jeux d'hiver. Après les Jeux de 1936, le Comité international olympique décide de dissocier l'organisation des Jeux d'hiver et d'été, mais les Jeux sont suspendus durant la Seconde Guerre mondiale. Londres accueille les premiers Jeux de l'après-guerre, les Jeux olympiques d'été de 1948 et recommande Oslo pour les Jeux d'hiver de 1948. Cependant le conseil municipal refuse. À la place, les Jeux olympiques d'hiver de 1948 ont lieu à Saint-Moritz en Suisse.Les Norvégiens sont indécis quant à l'accueil de Jeux olympiques d'hiver. Culturellement, ils refusent d'associer les sports d'hiver avec l'esprit de compétition, plus particulièrement en ce qui concerne les épreuves de ski, malgré le succès des athlètes norvégiens aux précédents Jeux d'hiver. Les organisateurs croient néanmoins que les Jeux de 1952 peuvent être une opportunité pour promouvoir l'unité nationale et montrer au monde que la Norvège s'est remise de la guerre. Les concurrents d'Oslo pour l'organisation de ces Jeux sont Cortina d'Ampezzo en Italie et Lake Placid aux États-Unis. Le Comité international olympique (CIO) vote l'attribution des Jeux d'hiver de 1952 à Oslo le 1er juin 1947 lors de la quarantième session du CIO à Stockholm en Suède. Ce n'est que partie remise pour les villes écartées : Cortina d'Ampezzo obtient l'organisation des Jeux de 1956 et Lake Placid, qui a déjà accueilli les Jeux olympiques d'hiver de 1932, est choisie pour organiser les Jeux de 1980. La Norvège devient le premier pays scandinave à se voir attribuer des Jeux d'hiver et les Jeux d'hiver de 1952 sont les premiers qui ont lieu dans la capitale d'une nation.Un comité spécial est assigné à l'organisation des Jeux de 1952. Il comprend quatre dirigeants sportifs norvégiens et quatre représentants de la municipalité d'Oslo, dont le maire Brynjulf Bull. Le comité est mis en place en décembre 1947. Pour organiser les Jeux, ce comité s'est réuni à 28 reprises pour traiter 335 dossiers, en plus du comité exécutif composé du président et du vice-président du comité d'organisation qui lui s'est réuni 33 fois pour régler 273 questions. Pour aider à organiser les Jeux, 107 employés (en février 1952) ont travaillé au secrétariat du comité d'organisation. Aussi, des attachés sont désignés pour chaque pays pour établir le contact entre le comité d'organisation et les comités nationaux.La ville d'Oslo finance entièrement les Jeux, mais en contrepartie elle conserve l'intégralité des revenus générés. Pour construire ou améliorer les sites des Jeux olympiques, la municipalité d'Oslo a dû dépenser 11 663 000 couronnes norvégiennes, dont 4 400 000 pour la construction du Jordal Amfi et 1 029 000 pour la rénovation du Bislett Stadion. Les dépenses ne concernant pas les sites s'élèvent à 2 688 000 couronnes. Elles concernent les travaux préparatoires, l'administration ainsi que l'hébergement et le transport des athlètes, ces derniers frais représentant la majeure partie de la somme. Les recettes des Jeux sont, elles, de 4 182 000 couronnes. Elles se partagent entre les produits dérivés commercialisés lors des Jeux pour 572 000 couronnes et par la vente de billets d'entrée pour 3 610 000 couronnes. Les billets d'entrée, vendus entre 2 et 25 couronnes selon l'évènement, se sont écoulés à 541 407 unités. Le surplus entre les dépenses et les recettes s'élèvent à 1 494 000 couronnes norvégiennes. Il est reversé en majeure partie à la ville d'Oslo, ainsi qu'au comité national olympique norvégien.Pour faciliter l'acheminement des visiteurs étrangers vers les sites olympiques, plusieurs mesures sont prises comme l'extension des heures d'ouverture des stations de service la nuit ou encore l'ajout de trains des compagnies ferroviaires norvégiennes et suédoises pour les spectateurs, les athlètes et les entraîneurs durant la période des Jeux. Pour aider les automobilistes, une carte de la ville d'Oslo avec les sites olympiques est distribuée dans le journal Aftenposten et, pour réguler le trafic, des militaires sont appelés en renfort, les policiers étant en nombre insuffisant. Enfin, pour le public, le nombre de bus, de taxis (qui passent de 655 à 785) et de passages du tramway est augmenté, tandis que les athlètes et leurs entraîneurs sont transportés de leur côté en bus, en tram ou en train pour aller à Holmenkollen.Le mois de février 1952 voit un accroissement du nombre de visiteurs étrangers venant en Norvège, puisqu'ils sont alors 31 629 à s'y rendre alors qu'ils n’étaient que 13 781 en février 1950 et 16 624 en février 1951. Le nombre de touristes lors du mois de février 1953 sera également supérieur à ceux de 1950 et 1951 avec 19 430 visiteurs. La majorité de ces touristes viennent de la Suède voisine ainsi que du Danemark. Les moyens de transport utilisés pour arriver en Norvège sont en grande partie la voiture ou le bus avec 13 446 personnes ainsi véhiculées, et le train avec 11 673 voyageurs étrangers.L'emblème des Jeux olympiques est choisi parmi 335 dessins envoyés à un comité chargé de le choisir. C'est celui de Gunnar Furuholmen qui est sélectionné. Il est composé d'une simple forme circulaire circonscrivant la silhouette du nouvel hôtel de ville d'Oslo sur un fond bleu pâle. Les cinq anneaux olympiques viennent s'y superposer, et la désignation des Jeux se trouve sur l'extérieur du cercle. En outre, des affiches de grande et de petite taille, dessinées par Knut Yran, sont produites en plusieurs langues dont le norvégien, le français et l'anglais. Plus de 42 000 de ces affiches sont imprimées dans les deux tailles et distribuées dans les instances sportives mondiales et dans les représentations diplomatiques norvégiennes comme les ambassades. Enfin, un autre poster dessiné par Helge Kittelsen est utilisé pour les décorations de la ville d'Oslo avec un message en norvégien, anglais et français : « Oslo vous salue ».De nombreux produits dérivés en rapport avec ces Jeux olympiques ont été créés puisque 135 licences ont été accordées. Tout d'abord, 33 000 écharpes olympiques composées de cinq couleurs ont été fabriquées et vendues pour 12 couronnes norvégiennes. Par ailleurs, deux sortes de pins différents (un de 17 millimètres et un autre de 18 millimètres) ont été mis en vente par les clubs et fédérations sportives norvégiennes. 100 000 pins ont ainsi été vendus. Pour garder la coutume des précédents Jeux d'hiver, la Norvège imprime des timbres spéciaux de différentes tailles et de couleurs rouge, vert ou bleu. Ces timbres mis en circulation du 1er octobre 1951 au 29 février 1952 et coûtant 15, 30 ou 55 øres, ont été vendus à plus d'1,8 million d'exemplaires. Des publications spéciales sont aussi éditées : le dépliant du programme olympique, en norvégien, en anglais et en français, de 64 pages, vendu à 4,25 couronnes ; les programmes quotidiens, imprimés pour 370 000 exemplaires et vendus à 140 000 unités pour le prix d'une couronne ; ou encore le Olympia-Revy, un magazine de 32 pages composé des images des Jeux et vendu à 68 000 copies lors de la cérémonie de clôture et les jours suivants,. Enfin, une médaille commémorative est tirée à 2 000 exemplaires. Composée d'un alliage de bronze, elle reprend le motif des médailles olympiques avec l'hôtel de ville d’Oslo et les anneaux olympiques, tandis que le revers de cette médaille montre la devise olympique entre les branches d'un cristal de neige.À la suite de l'occupation allemande de la Norvège durant la Seconde Guerre mondiale, un sentiment anti-allemand commence à affecter la préparation des Jeux olympiques de 1952. Des discussions ont lieu pour savoir si l'Allemagne doit y participer. Lorsqu'en 1950 le comité national olympique de l'Allemagne de l'Ouest demande la reconnaissance du Comité international olympique, il s'inquiète d'éventuels boycotts politiques lors des Jeux suivants. Le comité national olympique ouest-allemand ayant été reconnu par le CIO, la RFA est officiellement invitée à participer aux Jeux d'hiver de 1952. L'Allemagne de l'Est est invitée à participer avec l'Allemagne de l'Ouest au sein d'une équipe unifiée, mais elle refuse.Au départ, la Norvège ne voit pas d'un bon œil la venue d'athlètes allemands ou ayant affiché une proximité avec le nazisme par le passé. Par exemple, le Norvégien Finn Hodt n'est pas autorisé à intégrer l'équipe de patinage de vitesse car il a collaboré avec les nazis durant la guerre. Finalement, malgré ces réticences, la Norvège accepte que les athlètes allemands et japonais participent aux Jeux. L'Union soviétique n'envoie pas d'athlètes à Oslo bien qu'elle soit reconnue par le Comité international olympique. Ayant l'intention d'inscrire une équipe au tournoi de hockey sur glace, elle prend contact trop tardivement avec la fédération internationale de hockey sur glace pour le faire,.Trente nations envoient une délégation à Oslo, ce qui constitue un record à l'époque pour des Jeux d'hiver. Au 31 décembre 1951, 32 pays sont inscrits aux Jeux mais deux d'entre eux se retirent avant qu'ils ne débutent. La Nouvelle-Zélande et le Portugal prennent part aux Jeux olympiques d'hiver pour la première fois. L'Australie, l'Allemagne et le Japon reviennent aux Jeux après 16 ans d'absence. Trois nations présentes en 1948 sont absentes en 1952 : la Turquie, le Liechtenstein et la Corée du Sud. Cette dernière ne participe pas aux Jeux en raison de la guerre de Corée qui est en cours à ce moment-là.Le nombre indiqué entre parenthèses est le nombre d'athlètes engagés par pays.La cérémonie d'ouverture a lieu dans le Bislett Stadion le 15 février. Le roi George VI du Royaume-Uni décède le 6 février 1952, huit jours avant le début des Jeux. En conséquence, tous les drapeaux nationaux sont mis en berne et la princesse Ragnhild de Norvège ouvre les Jeux à la place de son grand-père, le roi Haakon VII, alors à Londres pour assister aux funérailles. C'est la première fois que les Jeux olympiques sont déclarés ouverts par une femme. La parade des nations a lieu selon la tradition avec la Grèce en premier, le reste des nations défilant suivant l'ordre alphabétique norvégien et avec la nation hôte en dernier. Les équipes britanniques, australiennes, canadiennes et néo-zélandaises portent toutes des brassards noirs lors de la cérémonie d'ouverture en hommage à leur monarque. Après la parade des nations, la flamme olympique est allumée. La torche olympique avait été allumée le 13 février dans le foyer d'une maison à Morgedal, le lieu de naissance du pionnier du ski Sondre Norheim. Ce premier relais de la flamme organisé lors des Jeux d'hiver dure deux jours et a lieu entièrement sur des skis avec 94 relayeurs qui parcourent environ 225 kilomètres,,. Pour la cérémonie d'ouverture, le dernier relayeur, Eigil Nansen, reçoit la torche olympique et la porte jusqu'à un escalier où il enlève ses skis, monte, et allume la flamme.Les épreuves de bobsleigh et de ski alpin ont lieu la veille de la cérémonie d'ouverture. Les sportifs de ces épreuves ne pouvant pas participer aux festivités à Oslo, ils bénéficient tout de même de cérémonies d'ouverture plus simples qui se déroulent à Frognerseteren, site des épreuves de bobsleigh, et à Norefjell, site des épreuves de ski alpin.La cérémonie d'ouverture officielle a lieu le 15 février même si deux cérémonies plus confidentielles ont eu lieu le 14 février pour se conformer au programme des compétitions. Du 15 au 25 février, jour de la cérémonie de clôture, au moins une finale d'épreuve se tient chaque jour.† Le bandy est un sport de démonstration aux Jeux d'hiver de 1952 et aucune médaille n'est décernée.‡ Le chiffre indique le nombre de finales qui se tiennent ce jour-là pour chaque sport. Bobsleigh Un an avant la compétition olympique, en février 1951, des essais ont lieu pour tester le parcours de bobsleigh de la Korketrekkeren, située dans Oslo même, avec des équipages français, italiens, suédois et norvégiens. Les entraînements pour les épreuves ont lieu entre le 6 et le 13 février 1952, puis du 16 au 19 février. Lors de ces entraînements, une équipe belge est accidentée et plusieurs de ses membres sont blessés aux mains, épaules et coudes, et doivent abandonner la compétition.Les épreuves olympiques débutent les 14 et 15 février par le bob à deux avec deux manches par jour. 18 équipes participent et l'équipe allemande composée de Andreas Ostler et Lorenz Nieberl remporte l'épreuve avec un temps de 5 min 24 s 54. Ils sont suivis des équipes suisse et allemande qui s'adjugent respectivement l'argent et le bronze. La seconde épreuve, le bob à quatre, a lieu les 21 et 22 février avec 15 équipes au départ. La seconde équipe autrichienne abandonne après la troisième course de l'épreuve et l'Allemagne remporte la compétition, suivie par les quatuors suisse et américaine. En remportant les épreuves du bob à deux et à quatre, l'Allemagne effectue un retour triomphal aux Jeux olympiques dans les compétitions du bobsleigh après une interruption de 16 ans. Le Suisse Fritz Feierabend concourt dans les deux épreuves. Ses deux médailles de bronze sont la quatrième et la cinquième d'une carrière olympique qui a duré 16 ans avec trois éditions des Jeux. Les Allemands Ostler et Nieberl sont également doubles médaillés d'or en participant aux deux épreuves de bobsleigh.En l'absence de restrictions sur les bobeurs, les observateurs remarquent que le poids moyen de chaque membre de l'équipe allemande gagnante du bob à quatre était de 117 kilogrammes, ce qui est supérieur au poids du champion olympique poids lourds en boxe aux Jeux olympiques d'été de 1952. En voyant l'avantage apporté par le surpoids des athlètes à leur équipe, la Fédération internationale de bobsleigh et de tobogganing institue une limite de poids pour les Jeux olympiques suivants. Patinage de vitesse Toutes les épreuves de patinage de vitesse ont lieu au Bislett Stadion. Les Américains Ken Henry et Don McDermott se classent premier et second du 500 mètres mais le camionneur norvégien Hjalmar Andersen soulève l'enthousiasme de son public en remportant les épreuves du 1 500 mètres, du 5 000 mètres et du 10 000 mètres,, avec des avances sur ses adversaires qui sont les plus importantes dans l'histoire olympique. Le Néerlandais Wim van der Voort se place à la seconde place du 1 500 mètres et son compatriote Kees Broekman se classe derrière Andersen dans les courses du 5 000 et du 10 000 mètres, devenant ainsi les premiers médaillés néerlandais en patinage de vitesse. Le grand absent de la compétition est l'ancien champion du monde Kornél Pajor. Le patineur de vitesse, né en Hongrie, avait remporté les deux épreuves de longue distance aux Championnats du monde d'Oslo en 1949 avant de faire défection pour la Suède, mais il n'a pas été en mesure d'obtenir la citoyenneté suédoise à temps pour concourir en 1952. Lors de cette épreuve, plusieurs patineurs sont tombés malades à cause de la prise d'amphétamines. C'est la première fois que l'utilisation de ces produits dopants est prouvée dans le sport. Ski alpin Il y a trois épreuves de ski alpin au programme olympique : le slalom, le slalom géant et la descente. Les hommes comme les femmes concourent dans les trois épreuves qui ont lieu à Norefjell et Rødkleiva. Le slalom géant fait ses débuts olympiques lors de ces Jeux. Les skieurs autrichiens dominent la compétition en remportant sept médailles sur les 18 possibles alors que c'est la terrible désillusion pour leurs éternels rivaux suisses qui n'en gagnent aucune.Les épreuves de ski alpin commencent par le slalom géant dames, le 14 février à Norefjell, avec 45 concurrentes. L'Américaine Andrea Mead-Lawrence gagne avec un temps de 2 min 6 s 8. Elle est la seule double médaillée d'or en remportant cette épreuve ainsi que le slalom, et elle est la première skieuse américaine à remporter deux médailles d'or en ski alpin. L'Autrichienne Dagmar Rom et l'Allemande Annemarie Buchner complètent le podium et cinq athlètes sont disqualifiés lors de la course. La seconde épreuve féminine, la descente, devait avoir lieu le 16 février mais pour des raisons techniques, elle a lieu le lendemain, avec 42 concurrentes dont 7 ne terminent pas la course,. L'Autrichienne Trude Beiser-Jochum, l'Allemande Buchner et l'Italienne Giuliana Minuzzo composent le podium de cette épreuve. Enfin, la dernière compétition chez les femmes, le slalom, a lieu à Rødkleiva le 20 février et voit concourir 40 athlètes. L'Américaine Mead-Lawrence remporte l'épreuve en 2 min 10 s 6 tandis que les Allemandes Ossi Reichert et Annemarie Buchner, qui remporte sa troisième médaille, arrivent en deuxième et troisième positions.Chez les hommes, les épreuves débutent aussi par le slalom géant, le 15 février, avec 83 skieurs dont un qui est disqualifié durant la compétition. Le Norvégien Stein Eriksen remporte l'or avec un temps de 2 min 25 s. Il remporte l'argent aussi dans le slalom. Il est entouré sur le podium par les Autrichiens Christian Pravda et Toni Spiess. Leur compatriote Othmar Schneider gagne le slalom, qui a lieu le 19 février avec 86 concurrents, dans un temps de 2 min. Il gagne également la médaille d'argent dans la descente. Eriksen et son compatriote Guttorm Berge complètent le podium. Enfin, la descente a lieu le 17 février avec 81 athlètes au départ et 72 à l'arrivée. L'Italien Zeno Colò remporte la médaille d'or en 2 min 30 s 8 devant les Autrichiens Schneider et Pravda. Durant l'épreuve, qui a rassemblé le plus de public à Norefjell, le descendeur grec Antoin Miliordos chute 18 fois et franchit la ligne d'arrivée à reculons. Ski de fond Toutes les épreuves de ski de fond ont lieu à côté du tremplin du saut à ski à Holmenkollen. Comme cela avait été le cas en 1948, il y a trois épreuves chez les hommes : le 18 kilomètres, le 50 kilomètres et un relais de 10 kilomètres. Une course de dix kilomètres pour les femmes est ajoutée pour la première fois au programme olympique. Toutes les médailles sont remportées par les pays nordiques et les skieurs finlandais en gagnent huit sur douze possibles.80 athlètes participent au 18 kilomètres, le 18 février, et 75 terminent la course. Hallgeir Brenden remporte l'épreuve avec un temps de 1 h 1 min 34 s et aide aussi la Norvège à prendre l'argent sur le relais 4 × 10 kilomètres. Brenden gagne également une autre médaille d'or dans le 15 kilomètres en 1956 et une d'argent dans le relais en 1960. Lors du 18 km, les Finlandais Tapio Mäkelä et Paavo Lonkila prennent l'argent et le bronze. Le 20 février, se déroule le 50 kilomètres avec 36 concurrents au départ de la course et 33 à l'arrivée. La course, considérée comme « excellente » par les spécialistes et se déroulant sans problèmes sous une météo nuageuse, voit la victoire du Finlandais Veikko Hakulinen en 3 h 33 min 33 s,. Il débute ici sa carrière olympique qui aboutira à sept médailles dont trois en or. Il est suivi, lors de la course, par son compatriote Eero Kolehmainen et par le Norvégien Magnar Estenstad. La course féminine de 10 kilomètres a lieu le 23 février avec 20 concurrentes au départ dont 18 franchissent la ligne d'arrivée. L'épreuve, considérée comme exigeante, est remportée par la Finlandaise Lydia Wideman qui devient la première championne olympique en ski de fond, et ses compatriotes Mirja Hietamies et Siiri Rantanen remportent respectivement l'argent et le bronze,. Enfin, l'épreuve de relais a lieu le même jour avec 13 équipes au départ dont une qui abandonne au cours de la course. La Finlande gagne le relais avec un temps de 2 h 20 min 16 s, la Norvège et la Suède arrivant en deuxième et troisième positions. Combiné nordique L'épreuve de combiné nordique a lieu sur les sites du ski de fond et du saut à ski les 17 et 18 février. 25 athlètes participent à l'épreuve dont 22 parviennent à la terminer. La compétition débute par le saut à ski, les athlètes effectuant trois sauts sur le tremplin de Holmenkollen. Les athlètes s'entrainent du 13 au 15 février et concourent le 17. Lors de la compétition, les deux meilleurs sauts sont notés avec les résultats de la course de ski de fond pour déterminer le vainqueur. Un point dans l'épreuve de saut équivaut à 17 secondes dans la course de ski de fond. Cette dernière, de 18 kilomètres, se déroule le lendemain pour les concurrents restants. La course, considérée comme exigeante, se passe sans aucun problème. Lors de ce parcours, les départs s'effectuent toutes les 30 secondes. Les Norvégiens Simon Slåttvik et Sverre Stenersen remportent respectivement l'or et le bronze avec environ 451 et 436 points. Stenersen gagnera l'or dans les Jeux de 1956 dans la même épreuve. Le Finlandais Heikki Hasu s'adjuge la médaille d'argent avec environ 447 points et empêche un podium entièrement norvégien. Saut à ski Avant la compétition, plusieurs sauteurs sont favoris comme le Norvégien Torbjørn Falkanger, champion de Norvège en 1950 et en 1951, le Finlandais Antti Hyvärinen, le Suédois Thure Lindgren, vice-champion du monde en 1950 ou encore l'Allemand Sepp Weiler, un ancien soldat de 31 ans qui a perdu son œil gauche sur le front russe durant la Seconde Guerre mondiale. Les athlètes norvégiens sont très attendus car ils ont remporté la médaille d'or en saut à ski lors de chaque Jeux d'hiver de 1924 à 1952.Les entraînements pour la compétition se déroulent les 19, 21 et 22 février. Le 24 février 1952, une foule de plus de 115 000 personnes accueillent les sauteurs à ski qui concourent à Holmenkollen pour la seule épreuve au programme, le tremplin normal hommes. Le roi, le prince héritier Harald et la princesse Ragnhild sont présents. 44 sauteurs de 13 pays débutent l'épreuve et 43 la terminent. La compétition comprend deux sauts pour chaque athlète. Lors des premiers passages de concurrents, le Norvégien Arnfinn Bergmann, qui est médaillé de bronze aux championnats du monde de 1950, réalise un saut de 67,5 m et reste en tête malgré les bons sauts de Bror Östman et de l'Allemand Toni Brutscher, jusqu'à ce que Torbjørn Falkanger s'élance et réalise un saut de 68 mètres sous les ovations du public. Lors du second tour, Bergmann réalise un saut parfait, égalisant la distance du premier saut de Falkanger mais obtient plus de points pour la réalisation du saut. Grâce à un saut de 62,5 mètres, Brutscher et le Norvégien Halvor Næs se retrouvent à égalité à la seconde place temporaire mais après cela Karl Holmström et Falkanger passent devant eux. Finalement, les athlètes norvégiens ne déçoivent pas la foule puisque Arnfinn Bergmann et Torbjørn Falkanger se classent à la première et seconde place de l'épreuve avec respectivement 226 et 221,5 points, et que le sauteur suédois Karl Holmström obtient la médaille de bronze avec 219,5 points. Patinage artistique Il y a trois épreuves dans la compétition olympique de patinage artistique : l'épreuve masculine, féminine et par couple. Les épreuves prennent place dans le Bislett Stadion, sur une patinoire construite à l'intérieur de la piste de patinage de vitesse. L'entente des juges pour influencer les résultats est une tendance émergente dans les années précédant les Jeux d'Oslo. Entre 1949 et 1952, l'International Skating Union bannit cinq juges qui ont tenté d'arranger des résultats, même si aucun méfait n'a été prouvé concernant les compétitions olympiques. Aussi, des ordinateurs sont utilisés pour la première fois afin de faire le décompte des notes attribuées par chaque juge dans toutes les épreuves.L'Américain Dick Button remporte l'épreuve hommes. L'Autrichien Helmut Seibt prend l'argent et un autre Américain, James Grogan, obtient la médaille de bronze. Button devient le premier patineur à exécuter un triple saut en compétition quand il réalise le triple boucle dans le programme libre hommes,. La patineuse britannique Jeannette Altwegg remporte la médaille d'or dans l'épreuve féminine tandis que l'argent est décerné à l'Américaine Tenley Albright, qui obtiendra l'or aux Jeux d'hiver 1956 à Cortina d'Ampezzo. La Française Jacqueline du Bief complète le podium. Après la compétition, Jeannette Altwegg prend sa retraite sportive tandis que du Bief devient championne du monde quelques jours plus tard à Paris. La paire allemande composée des époux Ria et Paul Falk gagne l'épreuve de couple. Ils battent les Américains Karol et Peter Kennedy qui se classent deuxièmes de l'épreuve, tandis que les Hongrois Marianna et László Nagy, qui sont sœur et frère, obtiennent la médaille de bronze. Hockey sur glace La majorité des matches de hockey sur glace ont lieu au Jordal Amfi, un nouveau stade de hockey construit pour les Jeux. Huit équipes participent au tournoi et le Canada obtient à nouveau la médaille d'or après avoir remporté tous les tournois de hockey olympiques précédents sauf un – en 1936, la Grande-Bretagne obtient l'or avec une équipe composée d'une majorité de joueurs anglais mais formés au Canada. Le Canada est représenté lors de ces Jeux par les Edmonton Mercurys, une équipe de hockey amateur sponsorisée par le propriétaire des concessions automobiles Mercury. Il s'agit de la dernière victoire olympique du Canada dans cette discipline avant longtemps puisqu'en 1956 l'équipe soviétique fait ses débuts aux Jeux et la domination canadienne prend fin.Le format du tournoi consiste en une série de rencontres entre toutes les équipes, série à l'issue de laquelle ces dernières sont classées en fonction des points récoltés. La dernière journée voit s'opposer le Canada et les États-Unis d'un côté, la Suède et la Tchécoslovaquie de l'autre. Les deux équipes d'Amérique du Nord se séparent sur le score de trois buts partout, ce qui donne la médaille d'or pour le Canada et l'argent pour les Américains, lesquels comptent alors un point de plus que les Suédois et les Tchécoslovaques,. Le résultat est critiqué par la presse soviétique qui accuse les équipes canadiennes et américaines de connivence pour parvenir à une égalité et empêcher ainsi une équipe d'un pays communiste de remporter le tournoi,. Les équipes d'Amérique du Nord sont également critiquées pour leur jeu agressif car bien que les mises en échec soient autorisées, elles ne sont pas utilisées par les équipes européennes. Aussi, les spectateurs ont une piètre opinion sur leur style de jeu,. Bandy Le Comité international olympique fait pression sur le comité d'organisation pour accueillir la patrouille militaire ou le curling en tant que sport de démonstration. Le comité préfère choisir le bandy qui n'avait jamais été inclus dans les Jeux olympiques d'hiver. Le bandy se joue par équipes de onze sur une patinoire de la taille d'un terrain de football à l'aide d'une balle au lieu d'un palet. Avec des crosses de 1,2 mètre de long, les joueurs tentent de marquer un but en expédiant la balle dans le filet de l'équipe adverse. En tant que sport de démonstration, les joueurs sont inéligibles pour des médailles. Trois nations participent : la Finlande, la Norvège et la Suède. Chacune des trois équipes gagne un match et en perd un autre ; la Suède remporte la compétition grâce au nombre de buts marqués, la Norvège suit en seconde position et la Finlande finit à la troisième place. Deux matches sont joués au Dæhlenenga Stadion et un au Bislett.Aux Jeux olympiques d'hiver de 1952, la cérémonie de clôture a un déroulement différent des Jeux précédents durant lesquels la cérémonie de clôture se tenait directement après la dernière épreuve. À Oslo, la cérémonie est distincte des compétitions et se tient dans le Bislett Stadion le soir du lundi 25 février. Les porte-drapeaux entrent dans le stade en suivant le même ordre que lors de la cérémonie d'ouverture. Ce soir-là, quatre cérémonies de remise de médailles sont organisées : pour la course féminine et le relais hommes en ski de fond, pour la compétition de saut à ski et pour le tournoi de hockey sur glace.Depuis 1920, le « drapeau d'Anvers » est transmis de la ville organisatrice à la ville hôte suivante durant les cérémonies de clôture des Jeux d'été. La ville d'Oslo offre un drapeau olympique pour établir la même tradition avec les Jeux d'hiver. Brynjulf Bull, le maire d'Oslo, confie le drapeau au président du Comité international olympique, Sigfrid Edström, qui déclare que le drapeau passera désormais de ville hôte à ville hôte lors des Jeux d'hiver. Ce drapeau, connu par la suite comme le « drapeau d'Oslo », est depuis préservé dans une vitrine avec le nom de chaque ville hôte des Jeux d'hiver gravé sur des plaques en cuivre, et il est apporté à chaque évènement olympique pour être présenté. Une réplique est utilisée durant les cérémonies de clôture.Après la cérémonie des drapeaux, la flamme olympique est éteinte, une course de patinage de vitesse s'ensuit, puis les participants à cette course donnent un spectacle, suivis par 40 enfants habillés en costumes nationaux qui effectuent une danse sur la glace. Pour terminer la cérémonie et clore les Jeux, les lumières sont éteintes et un feu d'artifice de 20 minutes illumine le ciel nocturne.Parmi les trente nations qui participent à ces Jeux, treize repartent avec au moins une médaille, comme il est détaillé dans le tableau ci-dessous. Huit de ces pays gagnent au moins une médaille d'or et dix remportent plus d'une médaille. La Norvège, pays organisateur, arrive à la première place de ce tableau avec seize médailles dont sept en or, trois en argent et six en bronze. Les États-Unis et la Finlande prennent les deuxième et troisième places avec respectivement onze et neuf médailles. Les Pays-Bas remportent, à Oslo, leurs premières médailles olympiques dans des Jeux d'hiver.Les médaillés reçoivent une médaille en vermeil, en argent ou en bronze dessinée par le grec Vasos Falireus et par Knut Yran. Elle présente sur l'avers une torche olympique au centre et en bas les anneaux olympiques. En arrière-plan on peut lire aussi le mot « Olympie » en grec. L'inscription en français « Jeux Olympiques » et la devise « Citius Altius Fortius » figurent également sur cette face de la médaille. Le revers de la médaille montre pour sa p"
sport;"Les Jeux olympiques d'été de 1936, Jeux de la XIe olympiade de l'ère moderne, sont célébrés à Berlin, en Allemagne du 1er au 16 août 1936. La capitale allemande est désignée pour la seconde fois comme pays organisateur, mais les Jeux olympiques de 1916 ont été annulés en raison de la Première Guerre mondiale.Dans le contexte du moment, les JO de Berlin prennent vite une signification très politique, même si personne ne peut encore prévoir les changements politiques qui vont survenir en Allemagne quand, en 1931, le CIO confie à Berlin et à la République de Weimar l'organisation des jeux. Après l'instauration du régime nazi en 1933, plusieurs pays demandent le boycott de ces Jeux olympiques et organisent des jeux alternatifs, les Olympiades populaires, à Barcelone, dont le déclenchement de la guerre d'Espagne la veille empêchent l'inauguration. Les Jeux de Berlin se déroulent dans une atmosphère de xénophobie et d'antisémitisme, Adolf Hitler voulant se servir de cet événement pour faire la propagande du nazisme et la promotion de l'idéologie de la supériorité de la race aryenne, notamment à travers le documentaire Les Dieux du stade de Leni Riefenstahl. Ces jeux sont souvent cités comme exemple de « blanchiment par le sport » organisé par un gouvernement autoritaire et belliqueux.Sur les 49 nations et 3 967 athlètes (dont 335 femmes) qui prennent part à 129 épreuves dans 19 sports, l'Allemagne est le pays le plus médaillé.Dans le contexte particulier des « Jeux nazis », les quatre médailles d'or remportées par l'athlète noir américain  Jesse Owens en sprint et saut en longueur représentent un important symbole dans l'histoire des Jeux olympiques modernes. Mais l'athlète le plus médaillé est le gymnaste allemand Konrad Frey (six médailles dont trois d'or). Au tableau des médailles, les athlètes allemands imposeront leur large domination tout au long des Jeux, remportant 89 médailles dont 33 d'or, devant les États-Unis, avec 56 médailles dont 24 d'or.Les Jeux olympiques étaient déjà attribués à l’Allemagne en 1916, mais ont été annulés à cause de la Première Guerre mondiale. Incriminée et tenue responsable pour le déclenchement du conflit mondial, l’Allemagne est suspendue des Jeux de 1920 et de 1924. Cependant, après un long processus de négociation, les autorités allemandes ont réussi à faire réintégrer leur pays pour participer aux Jeux olympiques d'été de 1928 et postulent pour accueillir les Jeux d’été de 1936. Leur argument est que les Jeux ont déjà été attribués à l’Allemagne dans le passé — en 1916 — donc, les infrastructures sont déjà prêtes, et la candidature est présentée comme un moyen de redorer son blason.Malgré les nombreuses confusions, l’événement sportif mondial est attribué au régime de Weimar, donc avant l'arrivée au pouvoir des nazis. En 1933, avec l’accession au pouvoir d’Adolf Hitler, la capacité d’organiser un tel événement est sérieusement remise en question, notamment en raison de l'idéologie raciste et discriminatoire du parti nazi. En fait, le régime nazi a aggravé la situation lorsqu’il suggéra l’exclusion des Juifs des Jeux de Berlin.À la surprise générale, malgré ses propos houleux et haineux envers les Juifs, Hitler approuve la réception des Jeux et promet de tout faire pour la réussite de l’événement. Le dictateur allemand clame publiquement la promotion des relations entre les Nations et le développement du sport chez les jeunes ; cependant, son but ultime est la prospérité acquise des atouts politiques non négligeables de l’organisation des Jeux olympiques[source insuffisante].Le Comité international olympique confie l'organisation des Jeux olympiques d'été de 1936 à la ville de Berlin, au cours de la 29e session du 26 avril 1931, à Barcelone. La capitale allemande l'emporte face à la candidature de Barcelone par 43 voix à 16. Alexandrie (Égypte), Budapest (Hongrie), Buenos Aires (Argentine), Cologne, Francfort et Nuremberg (Allemagne), Dublin (Irlande), Helsinki (Finlande), Lausanne (Suisse), Rio de Janeiro (Brésil) et Rome (Italie) sont candidates.Le contexte socio-politique de l’Allemagne a drastiquement changé en 1933 avec la montée de l’extrême droite et l’accès au pouvoir d’Adolf Hitler. Arrivé au pouvoir dans un contexte de crise économique et politique, Hitler tente d’exclure tous les peuples distincts qui ne cadrent pas dans son idéal de race aryenne. Pour y arriver, Hitler décide de lancer un programme de réarmement qui mène à une politique d’agression. Dès 1934, il établit un régime totalitaire et élimine tous les autres partis politiques, le parti nazi est le seul accepté. La dictature totalitaire d’Hitler module l’Allemagne en État autoritaire et centralisé autour du parti nazi. Il abolit le commerce étranger dans le but de restreindre l’Allemagne à l’autarcie et à l’autosuffisance. Pour atteindre ses objectifs, Hitler crée la police militaire (SS), une police nazie (SA) et une police secrète d'État (Gestapo). La politique allemande devient très vite raciste et antisémite. Selon Hitler, c’est l’idée de race qui domine l’Histoire, et les Jeux olympiques de 1936 représentaient une opportunité incroyable pour montrer la domination et la suprématie de la race aryenne[source insuffisante].À côté de l'aspect sportif, les JO de Berlin eurent une signification politique très importante dans le cadre de la montée des tensions au sein de l'Europe. Le souvenir de ces jeux reste lui aussi en très large partie politique : il reste un cas d'école exemplaire de la confusion du sport et de la politique et de la propagande par le sport.Alors que le choix de la ville de Berlin date de 1931, l'arrivée au pouvoir du parti nazi en 1933 et la montée consécutive des tensions internationales va donner à ces Jeux une dimension fortement politique.« En 1936, les organisations juives, le mouvement ouvrier international et plusieurs associations démocratiques et humanitaires appelèrent à boycotter les Jeux du Reich. » Les États-Unis menacent l'Allemagne de boycott, mais ne mettent pas leur menace à exécution.Les arguments des partisans du boycott sont les suivants :l’Allemagne nazie discrimine les Juifs, principal motif du boycott ou de la relocalisation des Jeux ;la discrimination n’est guère compatible avec l’esprit sportif ;les Jeux demeurent une plateforme pour le régime nazi afin de promouvoir la supériorité de la race aryenne ;une participation aux Jeux sous-entendrait une adhésion aux persécutions et au racisme.Les adhérents de la participation aux Jeux olympiques, dont le Comité International Olympique, défendent les arguments suivants :les aspects sportif et politique doivent être dissociés ;il n’y a pas de discrimination, donc il n’y a pas besoin de boycott ;il n’y a pas de discrimination seulement en Allemagne, il ne faut pas associer le fléau social à l’Allemagne uniquement. Les États-Unis, opposants au régime nazi et favorable au boycott, menaient eux-mêmes des ségrégations raciales contre leur propre population ;les Jeux olympiques sont porteurs de paix, de tolérance, d’égalité et de fraternité.Les pays qui décident le boycott organisent des « contre-Jeux populaires » parallèles à Barcelone. Les Jeux populaires de Barcelone abandonnent rapidement ses préoccupations initiales et deviennent une alternative aux Jeux de Berlin de 1936 et le slogan de la protestation contre l’organisation de l’événement sportif par l’Allemagne fasciste. Initialement, sa raison d’existence était d’enrayer la distinction entre la classe bourgeoise et la classe ouvrière dans le milieu du sport. Toutefois, les Jeux populaires de Barcelone avaient leurs propres caractéristiques définies et en contradiction avec certaines règles des Jeux olympiques :il n’y a pas de place pour la commercialisation et la militarisationon prône la participation des athlètes des nations non souveraines et des athlètes italiens et allemands exiléstous les athlètes ont la chance de concourir : des athlètes de haut niveau, des athlètes intermédiaires et des amateurson sacralise la participation des femmesil n’y a pas que des tournois sportifs, on assiste à des compétitions de peinture, de sculpture, de photographie, de littérature, de design ; on met aussi une emphase sur les activités folkloriques et intellectuelles.En tout, vingt-trois nations sont représentées : la Suède, la Suisse, la Hongrie, la Palestine, le Maroc, la Norvège, la Grande Bretagne, la Belgique, le Canada, les États-Unis, la France, la Grèce, le Portugal, les Pays-Bas, l’Algérie, le Danemark, la Tchécoslovaquie, les Émigrés juifs, l’Alsace, l’Espagne, les pays Basques, la Galice et la Catalogne. Les pays les mieux représentés sont la France (1 500 sportifs), la Suisse (deux cents), les Pays-Bas, la Belgique et la Grande Bretagne (cinquante représentants).Cependant, en plus des difficultés d'organisation, le déclenchement de la Guerre d'Espagne compromet définitivement le projet.Pour le régime du IIIe Reich, ces jeux devaient être l'occasion de prouver sa puissance et la « suprématie de la race aryenne », selon la terminologie nazie.Sur le plan intérieur, les Jeux furent utilisés par le régime nazi pour renforcer l'adhésion populaire envers lui. Ils servirent de support de propagande, dont l'expression la plus connue est le film Les Dieux du stade de Leni Riefenstahl. Ce film en soi est cependant plus un documentaire, sorte d'ancêtre des retransmissions télévisées actuelles (à côté de séquences esthétisantes comme les introductions ou celles dévolues à la gymnastique, à l'escrime et aux plongeons) : Riefenstahl montre en détail les exploits d'Owens, mais aussi, de manière plus étonnante, des défaites allemandes. Tout aussi étonnant est le fait que l'hymne le plus entendu à l'écran est l'hymne des États-Unis et non celui de l'Allemagne. Seule concession réelle à l'idéologie : les athlètes français, britanniques ou du Commonwealth sont peu représentés (malgré la victoire française en cyclisme montrée en détail, la participation française ne fut pas à la hauteur des espérances, en dépit de 7 médailles. Elle revint les mains vides sur les disciplines majeures comme l'athlétisme, la gymnastique et la natation).Au niveau de la politique extérieure, les Jeux olympiques contribuèrent à faire passer momentanément Hitler pour un pacifiste et de rassurer l'Europe quant à ses intentions belliqueuses.Hitler a le soutien de Pierre de Coubertin qui bien qu'ayant démissionné du CIO en 1925, participa activement à l'organisation de ces jeux. Il en fit le discours de clôture en prononçant ces mots : « Que le peuple allemand et son chef soient remerciés pour ce qu’ils viennent d’accomplir... ». Coubertin admirait « intensément » Hitler, et à la question qu'on lui posait de ce soutien, il répondait : « Comment voudriez-vous que je répudie la célébration de la XIe Olympiade ? Puisque aussi bien cette glorification du régime nazi a été le choc émotionnel qui a permis le développement qu’ils ont connu ». Selon Coubertin, Hitler a ainsi beaucoup fait pour le retentissement des Jeux olympiques.Les Jeux olympiques d'été de 1936 furent organisés par le Deutscher Reichsbund für Leibesübungen (DRL), le Bureau de Sports du Reich. Hans von Tschammer und Osten, le Reichssportführer ou chef du DRL, a nommé Theodor Lewald président et Carl Diem secrétaire général du Comité Organisateur des Jeux Olympiques à Berlin. Diem et Lewald introduisent des innovations originales, comme la cérémonie de la flamme olympique. Pour cacher les traces de l'antisémitisme nazi les panneaux antisémites furent provisoirement enlevés et les journaux mirent un bémol à leurs attaques. De cette façon, le régime exploita les Jeux olympiques pour fournir aux spectateurs et aux journalistes étrangers une fausse image d’une Allemagne pacifique et tolérante.Les Jeux olympiques de 1936 à Berlin étaient le parfait moyen de propagande pour Adolf Hitler. En effet, il a utilisé ces JO comme une vitrine pour mettre en avant ses idéologies. L’objectif des Jeux de Berlin était de refléter l’image de l’Allemagne nazie à travers le monde entier. L’organisation de cet événement avait pour but de célébrer la gloire d’Hitler et du nazisme en Allemagne. Dans le cadre des JO, Hitler a utilisé différentes formes de propagande : notamment grâce au cinéma, avec les réalisations cinématographiques de Leni Riefenstahl. Toute la propagande des JO, et plus généralement du nazisme, était organisée par Joseph Goebbels. Il organise toute une mise en scène afin de montrer la supériorité de la race aryenne. Toute la publicité autour des JO a permis d’attirer près de 3 millions de spectateurs : c’est une véritable réussite pour les nazis. Leurs idées et leur autorité sont propagées à travers la manifestation sportive. Les nazis ont essayé de faire oublier leur programme antisémite à travers les Jeux Olympiques afin de diffuser une fausse image de l’Allemagne nazie. Le sport a donné l’opportunité aux nazis d’afficher tous les moyens de propagande, et les Jeux de 1936 ont permis aux nazis de montrer la supériorité de la « race aryenne » et de mettre en avant leurs qualités physiques. Afin de ne pas être identifiées comme un régime nazi, les affiches de propagandes nazies ont été retirées durant la période des Jeux Olympiques. Les nazis ont donc présenté une fausse image d’une Allemagne pacifique. La propagande autour des JO a continué après les 1936 puisqu’en 1938, Leni Riefenstahl a sorti un documentaire pour mettre en avant le parti nazi lors des JO. Village olympique Les athlètes furent logés au village olympique de Dallgow-Döberitz, dont s'occupait l'officier allemand Wolfgang Fürstner. Ils eurent à disposition une salle de cinéma, de théâtre, de music-hall et une bibliothèque. Chaque chambre disposa d'une salle de bain et du chauffage central. Stade olympique Le monumental Stade olympique de Berlin d'une capacité de 100 000 places fut construit par l'architecte Werner March. Un virage entier est réservé aux SA. Le stade a accueilli les cérémonies d'ouverture et de clôture, les épreuves d'athlétisme, d'équitation et les finales de handball à onze et de football. Le baseball y fut en démonstration. Autres sites Stade nautique : natation, plongeon, water poloStade May Field : polo, équitationThéâtre de plein-air Dietrich Eckart : gymnastiqueStade de Hockey : hockey sur gazonCentre de tennis : basket-ball, escrimeSalle Cupola : escrimeCentre nautique de Grünau : aviron, canoë-kayakDeutschlandhalle : boxe, lutte, haltérophilieVélodrome : cyclisme sur pisteStand de tir olympique : tirChamp de manœuvres de Döberitz : pentathlon moderneLes régates de voile furent disputées dans la ville de KielLa cérémonie d'ouverture, orchestrée par Rudolf Laban, se déroula le 1er août 1936 devant les 100 000 spectateurs du Stade olympique de Berlin qui assistèrent dans un premier temps au défilé des brigades des Jeunesses hitlériennes. Alors que la Marche d’hommage du compositeur allemand Richard Wagner était entonné par l’orchestre, le chancelier Adolf Hitler pénétra dans le stade sous le salut nazi des spectateurs et rejoignit dans les tribunes le comte Henri de Baillet-Latour, président du Comité international olympique, ainsi que les membres du comité d’organisation.Un court enregistrement du baron Pierre de Coubertin fut diffusé dans l’enceinte :« L'important aux Jeux olympiques n'est pas d'y gagner, mais d'y prendre part ; car l'essentiel dans la vie n'est pas tant de conquérir que de bien lutter. »Peu après, Adolf Hitler déclara officiellement ouverts les Jeux olympiques de Berlin, sans autre discours. La flamme entra dans le stade après un relais de plus de 3 000 athlètes. Le dernier porteur du flambeau fut l’athlète allemand Fritz Schilgen, qui alluma la vasque olympique. Pour la première fois, la flamme olympique, à l’instigation du professeur Carl Diem, était introduite dans la cérémonie d'ouverture des Jeux. Ce fut aussi le premier grand événement retransmis en direct via la télévision.Les invitations sont lancées par le gouvernement présidé par Adolf Hitler par le biais du comité olympique allemand. L'Espagne, qui entame sa guerre civile, déclare forfait le matin même de la cérémonie d'ouverture. Finalement, 49 nations participent à ces jeux de Berlin. Cinq d'entre elles apparaissent pour la première fois : l'Afghanistan, les Bermudes, la Bolivie, le Costa Rica et le Liechtenstein.L'Allemagne et les États-Unis disposent du plus gros contingent d'athlètes avec respectivement 348 et 310 engagés. La France, la Hongrie et le Royaume-Uni se présentent à Berlin avec près de deux cents sportifs chacun.Dix-neuf sports et 129 épreuves composent le programme des Jeux olympiques de 1936. Trois nouvelles disciplines olympiques voient le jour : une forme de handball à onze, le canoë-kayak et le basket-ball. Des compétitions de vol à voile et de baseball sont disputées en démonstration.Le sprinteur noir-américain Jesse Owens fut le héros de ces jeux de Berlin en s'adjugeant quatre titres olympiques sur quatre épreuves auxquelles il participa. Le 3 août 1936 sur le 100 m, Owens est situé à la deuxième ligne. En quelques foulées, il dispose de tous ses adversaires, et en particulier de son compatriote Ralph Metcalfe pour réaliser le temps de 10 s 3. Le lendemain, Owens, âgé alors de 23 ans, décroche sa deuxième médaille d'or dans l'épreuve du saut en longueur sous les yeux d’Adolf Hitler. Dans son duel serré avec l'Allemand Luz Long, il prend l'avantage lors de son dernier essai qui est mesuré à 8,06 m, soit un nouveau record olympique. Le lendemain, l'Américain remporte sa victoire la plus nette sur le 200 m en battant de quatre dixièmes (4 m environ) Mark Robinson. Enfin, le triomphe de Jesse Owens s'achève le 9 août avec ses partenaires du 4 × 100 m américain. Au départ du premier relais, il creuse l'écart sur ses concurrents italiens et allemands. L'équipe des États-Unis remporte la course en établissant un nouveau record du monde en 39 s 8 qui tiendra vingt ans.Les exploits de cet athlète ont d'autant plus de retentissement qu'ils se situent à Berlin en 1936 dans le cadre d’une manifestation olympique servant de propagande aux thèses sur la supériorité de la race aryenne sur les Juifs ou les Noirs. Après la guerre, une légende a prétendu qu'Adolf Hitler avait quitté la tribune afin de ne pas saluer le vainqueur du 100 m, Jesse Owens, parce que celui-ci était Noir. La raison en est beaucoup plus simple. Le premier jour des jeux, Hitler avait félicité tous les athlètes allemands, ce qui avait eu pour conséquence que le Comité olympique avait demandé, par souci de neutralité olympique, qu'il félicite tous les athlètes ou aucun. Hitler choisit cette dernière option et ne serra plus la main à aucun athlète durant les jeux.À l'encontre de cette légende, Owens précise dans son autobiographie comment Hitler s'est levé et l'a salué :« Après avoir passé le chancelier, il surgit en me saluant de la main, je l'ai salué en retour. Je pense que des auteurs ont montré un mauvais goût en critiquant l'homme de l'heure en Allemagne. »Lors de l'inauguration du nouveau stade olympique de Berlin en 1984, la veuve de Jesse Owens déclara que son mari avait été plus respecté par les autorités nazies que par les dirigeants de sa propre équipe nationale.En athlétisme, les États-Unis remportent près de la moitié des épreuves. L'Américain Glenn Morris s'adjuge le titre alors qu'il participe à son troisième et ultime décathlon. Sa compatriote Helen Stephens décroche deux médailles d'or au total. Les cinq titres allemands reviennent à des lanceurs. Le marathon bénéficie de repères kilométriques, qui permettent aux concurrents de mesurer leur effort. Tous les trois kilomètres, des points de ravitaillement bien fournis ont été prévus, avec des points chronométriques qui leur donnent l'écart avec leurs prédécesseurs. Ces dispositions permettent de limiter le nombre d'abandons.L'équipe de France de cyclisme repart de ces jeux avec sept médailles en six épreuves au programme. Robert Charpentier, remporte la course sur route individuelle, le contre-la-montre par équipes et la poursuite par équipes (4 000 m).En gymnastique, les Allemands Alfred Schwarzmann et Konrad Frey remportent six titres olympiques au total. Dans l'épreuve du deux de couple d'aviron, l'équipe britannique (Leslie Southwood et Jack Beresford) remporte la victoire sur le fil. Âgé de douze ans et demi, le barreur français Noël Vandernotte, obtient deux podiums en deux et quatre barré et devient le plus jeune médaillé de l'histoire des Jeux olympiques.En natation, le Japon domine les compétitions (onze médailles au total). Au plongeon, l'Américaine Marjorie Gestring, âgée de treize ans et 267 jours, devient la plus jeune championne olympique de l'histoire. Les épreuves d'équitation sont toutes remportées par les cavaliers allemands. En sports collectifs, ces Jeux de Berlin voient le sacre des États-Unis en basket-ball, de l'Italie en football, de l'Allemagne en handball à onze et de l'Inde au hockey sur gazon.C'est la dernière année où le sport automobile a été inscrit aux Jeux (en démonstration à Berlin). Sur les 125 voitures inscrites, les nombreuses voitures allemandes partaient favorites, accompagnées d'une seule voiture britannique, une Singer Le Mans 1500 pilotée par la Britannique Betty Haig, petite nièce du maréchal Douglas Haig. Neuf jours plus tard, elle remportait l'épreuve, devenant ainsi la première femme de l'histoire à battre des hommes à une épreuve olympique.Parmi les quarante-neuf nations qui participent à ces Jeux, trente-deux repartent avec au moins une médaille, comme il est détaillé dans le tableau ci-dessous. Vingt et un de ces pays gagnent au moins une médaille d'or et vingt-neuf remportent plus d'une médaille. L'Allemagne, pays organisateur, arrive à la première place de ce tableau avec quatre-vingt-neuf médailles dont trente-trois en or, vingt-six en argent et trente en bronze. Les États-Unis et la Hongrie prennent les deuxième et troisième places avec respectivement cinquante-six et seize médailles.Une controverse nourrie surgit relative au salut olympique de quelques délégations devant la tribune officielle présidée par Adolf Hitler. Le salut olympique s'inspire du salut du Bataillon de Joinville bras tendu puis replié vers le torse ainsi que le justifia Pierre de Coubertin dont les jeux olympiques de 1924 furent les derniers qu'il organisa.Lors des Jeux olympiques de 1936, la Grèce qui est toujours le premier pays à faire son entrée sur le stade, fit le salut olympique, ainsi que le Canada, la France et l'Italie. Majoritairement, les autres nations choisirent de découvrir la tête, de saluer militairement ou de ne pas saluer.Les nazis assimilèrent le salut olympique au salut fasciste, et crurent à l'adhésion des délégations à leur idéologie, ce qui déclencha des applaudissements nourris et des levées de saluts fascistes en réponse.Il est à noter que le salut olympique dit « salut de Joinville » a été modifié 10 ans après les Jeux olympiques de 1936 mais n'a pas totalement disparu des cérémonies puisque, lors de la cérémonie d'ouverture des Jeux de Munich, la délégation de Bolivie le pratiquait encore.Ce fut aussi l'occasion pour Leni Riefenstahl de réaliser un film d'anthologie sur les Jeux : Les Dieux du stade, tout autant considéré comme un grand classique du cinéma de propagande que novateur par sa façon de filmer les compétitions sportives.Ce furent également les premiers Jeux olympiques de l'histoire retransmis à la télévision.La comédie L'as des as (Gérard Oury, 1982) se déroule à Berlin durant les Jeux olympiques. Le héros du film, interprété par Jean-Paul Belmondo, est entraîneur de l'équipe française de boxe et se trouve embarqué à sauver une famille juive, avant de croiser la route d'Hitler.Berlin 1936, réalisé par Edward Cotterill, 2016.1936, les Jeux de Berlin, réalisé par Bernd Wilting, 2016.Les Jeux d'Hitler, Berlin 1936, réalisé par Jérôme Prieur, Arte France-Roche productions, 90 min, 2016.Dans les secrets des JO de Berlin, réalisé par Laure Philippon, 2016.Berlin 1936 - Dans les coulisses des Jeux olympiques, réalisé par Mira Thiel et Florian Huber, 2016.Charlie Chan aux Jeux olympiques réalisé par H. Bruce Humberstone en 1937.L'Épreuve du temps d'Eduard von Borsody en 1940.Berlin 36 de Kaspar Heidelbach en 2009.La Couleur de la victoire réalisé par Stephen Hopkins en 2016.                               Jeux olympiquesJeux olympiques d'étéStade olympique de BerlinCarl Diem (inventeur de relais de la flamme olympique, instauré en 1936)Les Dieux du stade (film)Olympiades populaires (contre-jeux organisés en Espagne)Hôtel AdlonSportswashing Bibliographie Alexandre Najjar, Berlin 36 : roman, Paris, Plon, 2009, 284 p. (ISBN 978-2-259-21082-9, OCLC 466658576).Jean-Michel Blaizeau, Les Jeux défigurés Berlin 1936, Éditions les Indes savantes, 294 pages, 350 photos, 2012.Fabrice Abgrall & François Thomazeau, 1936 : La France à l’épreuve des Jeux olympiques de Berlin, Alvik, 2006.Jean-Marie Brohm, Jeux olympiques à Berlin, Bruxelles, Complexe (« La mémoire du siècle »), 1983.Daphné Bolz, Les arènes totalitaires, CNRS, 2007.Jérôme Prieur, Berlin, Les Jeux de 36, Éditions La Bibliothèque, Paris, 2017  (ISBN 9782909688862)(fr) La page des Jeux olympiques de Berlin sur le site officiel du CIO.Rapport olympique des jeux de 1936(en) United States Holocaust Memorial Museum, Online Exhibition: Nazi Olympics: Berlin 1936United States Holocaust Memorial Museum - Library Bibliography: 1936 OlympicsJeux Olympiques [1] : Le Comité International Olympique est l’organisation responsable de l’organisation des Jeux Olympiques et du choix des villes hôtes. Tous les communiqués, toute l’historique, toute l’actualité sportive ou politique concernant les Jeux Olympiques peuvent être retrouvés là-dessus. Portail des Jeux olympiques   Portail des années 1930   Portail de Berlin   Portail du nazisme"
sport;Les Jeux olympiques d'été de 2000, jeux de la XXVIIe olympiade de l'ère moderne, se sont déroulés à Sydney (Nouvelle-Galles du Sud, Australie) du 15 septembre au 1er octobre 2000. Ces derniers jeux du millénaire sont les deuxièmes à se tenir en Australie quarante-quatre ans après Melbourne en 1956.Le comité d'organisation (SOCOG) est composé de 2 500 personnes et est assisté par 50 000 volontaires. Les compétitions se répartissent sur 36 sites. La plus grande partie des épreuves, ainsi que le village des athlètes sont concentrés dans le parc olympique de Homebush Bay près du centre-ville de Sydney. 199 nations et 10 651 athlètes (dont 4 069 femmes) prennent part à 300 épreuves dans 28 sports, dont le taekwondo et le triathlon qui font leur première apparition officielle au programme olympique.Avec ses cinq médailles obtenues, la sprinteuse américaine Marion Jones fut considérée pendant longtemps comme l'héroïne de ces jeux, avant que le scandale de dopage lié au laboratoire Balco éclate. Après avoir avoué fin 2007 la prise de substances interdites, elle fut contrainte de restituer au CIO ses médailles obtenues à Sydney.Comme tous les Jeux olympiques organisés dans l'hémisphère sud (à l'exception de ceux de Melbourne), l'appellation ne correspond pas à la saison en cours. Ainsi, les Jeux olympiques d'été de Sydney se sont en fait déroulés tout à la fin de l'hiver austral et au début du printemps.L’Australie n’avait pas accueilli cet évènement depuis les Jeux olympiques d'été de 1956 à Melbourne, les villes de Brisbane et Melbourne furent battues aux élections des jeux d’été de 1992 et 1996. En 1989, la ville de Sydney et le gouvernement de la Nouvelle-Galles du Sud obtiennent le soutien du comité olympique australien à la candidature des jeux de l’an 2000. Immédiatement, Sydney obtient un large soutien de la population, des entreprises et des médias locaux.Le 23 septembre 1993, au cours de sa 101e session tenue à Monaco, le Comité international olympique décide de confier l'organisation les derniers Jeux olympiques du millénaire à Sydney. Après une présentation de 30 minutes de chaque candidat, les membres du CIO désignent la ville australienne à l’issue du 4e tour de scrutin. Sydney devance la ville de Pékin de deux voix seulement. Les autres finalistes Manchester, Berlin et Istanbul sont éliminés lors des tours précédents. Deux autres villes retirèrent leur candidature au cours du processus d'appel d'offres : Milan et Brasilia.Le logo des jeux de Sydney représente un athlète aux symboles et aux couleurs associés aux paysages de l'Australie. Les jambes, formés par un boomerang, sont rouges comme la terre de l'intérieur du pays. Les bras et la tête sont de couleur jaune à l'image du soleil. La silhouette porte une torche olympique laissant échapper un éclair bleu évoquant les plages et la baie de Sydney. La torche olympique est inspirée de l'Opéra de Sydney ainsi que des eaux bleues de l'Océan Pacifique. Elle est constituée de trois couches symbolisant la terre, le feu et l'eau. Comme pour les jeux précédents, la médaille olympique représente la déesse de la victoire brandissant une couronne de vainqueur. Sur le revers, figurent l'Opéra de Sydney, la torche et les anneaux olympiques. Les trois mascottes officielles reprennent les symboles de l'olympisme et du pays organisateur. Olly (diminutif d'olympique) est un kookaburra représente la générosité et l'universalité, symboles de l'olympisme. Syd (diminutif de Sydney) l'ornithorynque met en avant l'environnement ainsi que l'énergie du peuple australien. Millie (diminutif de millénaire) est un échidné représentant la technologie de l'an 2000.Le Parc olympiqueLe Sydney Olympic Park ou Parc Olympique de Sydney, situé à Homebush Bay à proximité du centre-ville, constitue le cœur des Jeux olympiques de Sydney. Il comprend le village olympique, ainsi qu'une dizaine d'installations sportives. Le Stade Olympique (Stadium Australia) fut construit spécialement pour les jeux de l’an 2000. D’une capacité de 110 000 places, cette enceinte sportive accueille les épreuves d’athlétisme, la finale de football ainsi que les cérémonies d’ouverture et de clôture. L'Aquatic Center (17 500 places) est le siège des compétitions de natation, de natation synchronisée, de plongeon, et de water polo. Ce bassin olympique fut considéré par Juan Antonio Samaranch, président du Comité international olympique, comme « la plus belle piscine qu’il ait vue de sa vie ». Les autres sites du parc olympique de Sydney sont le Centre de tennis NSW  (16 000 places), le State Hockey Centre (15 000 places), le Baseball Stadium et le Sydney International Archery Park. Par ailleurs, le Dome accueille les compétitions de badminton, de handball et de volleyball. Le Superdome celles de gymnastique et de basketball. Enfin, les épreuves de tennis de table et de taekwondo ont lieu au State Sports Centre.Les autres sites Il s'agit de la salle Convention and Exhibition Centre (lutte, boxe, judo et escrime), du vélodrome Dunc Gray, des Centres internationaux de tir et d'équitation, du Blacktown Olympic Centre (baseball et softball) et du Stade de football de Sydney. Les courses d'aviron et de canoë-Kayak sont disputés au Regatta Centre, les régates de voile dans la baie de Sydney. Les matchs de volley-ball ont lieu au Sydney Entertainment Centre ainsi que sur la plage de Bondi Beach. Des matchs du tournoi olympique de football sont disputés à l'extérieur de Sydney : Canberra (Bruce Stadium), Adelaide (Hindmarsh Stadium), Melbourne (Cricket Ground) et Brisbane (Cricket Ground).Le village olympiqueSitué au sein du parc de Homebush Bay, le village olympique de Sydney est le premier à héberger l'ensemble des compétiteurs dans un seul endroit. 10 000 athlètes et 5 000 officiels sont logés dans 800 maisons et 350 appartements dont l'architecture rappelle les immeubles de la ville de Sydney. Construit sur 94 hectares et mesurant 1,5 km de long, le village comprend un restaurant, un centre commercial, une banque, un hôpital et même une discothèque.Ce lieu fut conçu en respect de l'environnement. Fonctionnant avec l'énergie solaire, il fut construit en matériaux recyclables et produits verts. Sydney fut la première ville à inclure le volet écologique dans son dossier de candidature.La cérémonie d'ouverture des Jeux olympiques d'été 2000 se déroule le 15 septembre 2000 devant les 110 000 spectateurs du stade olympique de Sydney. Le spectacle débute avec la star international australienne Olivia Newton-john et le chanteur populaire australien John Farham avec la chanson Dare to dream. Le spectacle est conçu par Ric Birch et David Atkins, est un hommage à l'histoire de l'Australie. Il débute par l’entrée de près de 120 cavaliers avant d’évoquer les symboles de l’Australie que sont l’océan, le désert et les animaux. La culture aborigène est à de nombreuses reprises mise en avant par l’intermédiaire de chants et de danses traditionnelles. Les 199 nations pénètrent ensuite dans le stade. Les deux Corée défilent ensemble, dans les mêmes tenues, et derrière le même drapeau : le profil bleu ciel de la péninsule sur fond blanc, et porté par deux athlètes, un de chaque pays. Le serment olympique est prêté par la joueuse de Hockey Rechelle Hawkes. Cathy Freeman, Australienne aux origines aborigènes, allume la flamme olympique en tant que dernière relayeuse de la torche. Elle fut choisie comme symbole de la volonté de réconciliation entre les Aborigènes et les descendants des migrants européens. Elle remporta lors de ces jeux la médaille d'or de la course du 400 mètres. Pour conclure cette cérémonie, William Deane, gouverneur général d’Australie, déclare officiellement l’ouverture des jeux de la XXVIIe olympiade.La cérémonie de clôture met en scène de manière festive la culture populaire australienne avec le rappel de films locaux (Crocodile Dundee, Priscilla, folle du désert, ...). Kylie Minogue, la chanteuse australienne a participé à cette cérémonie en chantant les titres Dancing Queen et On A Night Like This. L'événement se conclut par un concert du groupe INXS.Quatre pays ont fait leur entrée dans les Jeux olympiques : l'Érythrée, les États fédérés de Micronésie, les Palaos et le Timor oriental. Ce pays, nouvellement indépendant et victime de combats contre l'armée indonésienne, fut invité au dernier moment par le Comité international olympique. Quatre athlètes timorais participèrent à titre individuel en tant qu'« athlètes internationaux olympiques ».199 nations ont participé aux Jeux olympiques de Sydney, soit deux de plus que les jeux précédents d'Atlanta de 1996. Au total, 10 651 athlètes (4 069 femmes et 6 582 hommes) ont participé aux épreuvesDeux nouveaux sports font leur entrée lors de ces jeux, le taekwondo et le triathlon. Par ailleurs, de nouvelles disciplines apparaissent (le duo et le plongeon synchronisés en natation, le trampoline en gymnastique, le keirin et la course américaine en cyclisme, le fortyniner en voile).Enfin, des disciplines réservées jusqu'alors aux hommes s'ouvrent aux femmes tels le marteau et le saut à la perche (athlétisme), la fosse et le skeet olympique (tir), le water polo, l'haltérophilie et le pentathlon moderne.Au total, ce sont 28 sports et 300 épreuves qui figurent au programme de ces jeux de 2000.AthlétismeRésultats détaillésEn sprint, Maurice Greene devient l'homme le plus rapide du monde sur 100 m en 9 s 87, alors que Konstadínos Kedéris remporte le 200 m, le premier titre de la Grèce depuis 1896. Sur le tour de piste, Michael Johnson conserve son titre d’Atlanta, Cathy Freeman réussit quant à elle le pari de s’imposer sur son sol. L’Australienne d’origine aborigène devient ainsi la première athlète ayant allumé la flamme olympique à remporter une médaille d'or dans les mêmes jeux. Angelo Taylor permet aux États-Unis de remporter leur cinquième titre consécutif sur le 400 m haies. Avec la médaille d’argent de la Jamaïque sur le relais 4 × 100 m, Merlene Ottey devient l’athlète féminine la plus décorée aux Jeux olympiques. Sur le 800 m, le favori Wilson Kipketer subit la loi de l’Allemand Nils Schumann, alors que le Kényan Noah Ngeny surprend le favori marocain Hicham El Guerrouj sur 1 500 m. En fond, l’Éthiopien Haile Gebrselassie est pour la seconde fois victorieux du 10 000 m. Sa compatriote Derartu Tulu remporte également cette épreuve après son titre en 1992 et une grossesse. Le Polonais Robert Korzeniowski devient le premier homme à s'adjuger les deux épreuves de marche lors de la même olympiade. Jan Železný conserve son titre du lancer du javelot et signe une troisième victoire de rang. Au disque, la Biélorusse Ellina Zvereva devient, à près de 40 ans, l’athlète la plus âgée à remporter un titre olympique. Nick Hysong s’impose dans l’épreuve du saut à la perche, une première américaine depuis Mexico en 1968.AvironRésultats détaillésLes nations européennes s’adjugent treize des quatorze titres disputés. À 38 ans, le rameur britannique Steve Redgrave réalise l’exploit de remporter son cinquième titre olympique consécutif, record sans précédent. Il décroche avec l'équipe du Royaume-Uni la médaille d’or dans l'épreuve du quatre sans barreur en devançant l’équipage italien de près de quatre dixièmes. Steve Redgrave conclut ainsi une série débutée aux Jeux olympiques de 1984 à Los Angeles. Sir Steve Redgrave est considéré comme l'un des plus grands sportifs du Royaume-Uni.BadmintonRésultats détaillésL’équipe de Chine poursuit sa domination mondiale dans ce sport en remportant quatre des cinq titres mis en jeu. En double messieurs, la victoire revient à la paire indonésienne.BaseballRésultats détaillésL’équipe masculine des États-Unis bat les champions olympiques en titre cubains en finale (4-0). Cuba avait remporté les deux premiers titres de baseball à Barcelone et à Atlanta.Basket-ballRésultats détaillésLes États-Unis d'Amérique remportent leur troisième titre olympique consécutif, leur douzième en quatorze participations. Elle bat l’équipe de France en finale de 10 points seulement, après avoir frôlé la correctionnelle en demi-finale face à la Lituanie. Côté féminin, les États-Unis conservent leur titre obtenu quatre ans plus tôt en disposant de l’Australie (76-54).BoxeRésultats détaillésLes boxeurs cubains sont une nouvelle fois dominateurs en s’adjugeant six médailles dont quatre titres olympiques. Le Russe Oleg Saitov conserve son titre des welters alors que le Cubain Felix Savon remporte en catégorie poids-lourds sa troisième médaille d’or consécutive. Le Royaume-Uni décroche son premier titre en boxe depuis 1968. Brahim Asloum offre à la France une médaille d’or 64 ans après sa dernière victoire olympique.Canoë-kayakRésultats détaillésL’Allemande Birgit Fischer devient l’athlète la plus titrée du canoë-kayak en remportant sa sixième et septième médaille d’or (K2 et K4). L’Allemagne et la Hongrie s’adjugent la moitié des épreuves.CyclismeRésultats détaillésLa cycliste Leontien van Moorsel s'adjuge quatre médailles dont trois titres lors de ces jeux de Sydney. Sur route, la Néerlandaise remporte les deux épreuves. Elle s'impose au sprint final dans la course en ligne et devance ses rivales Mari Holden et Jeannie Longo-Ciprelli dans le contre-la-montre. Sur piste, Van Moorsel remporte sa troisième médaille d'or de ces jeux dans l'épreuve de la poursuite individuelle où elle bat en finale la Française Marion Clignet de près de cinq secondes. La cycliste de 30 ans parachève ses exploits par une médaille d'argent obtenue dans la course aux points. Félicia Ballanger est l'autre triomphatrice de ces Jeux avec deux médailles d'or, dont le titre du sprint. Chez les hommes, le cycliste américain Marty Nothstein offre à son pays la première médaille olympique sur piste depuis les Jeux de Los Angeles en 1984. Le titre de la course en ligne revient à l’Allemand Jan Ullrich et celui du contre-la-montre au Russe Viatcheslav Ekimov. La France, grâce à notamment à Florian Rousseau et à Miguel Martinez (en VTT), récolte huit médailles au total.ÉquitationRésultats détaillésL’Allemagne et les Pays-Bas remportent quatre des six épreuves d’équitation. L’Australie signe une troisième victoire d’affilée au concours complet par équipe, les Allemands une cinquième au dressage.EscrimeRésultats détaillésL’italienne Valentina Vezzali ajoute deux médailles d’or supplémentaires à son palmarès, le Russe Pavel Kolobkov obtient son sixième podium olympique douze ans après sa première médaille à Séoul. La Hongroise Tímea Nagy conserve sa couronne olympique à l’épée individuel. L’Italie et la Russie remportent six des dix titres mis en jeu.FootballRésultats détaillésL’équipe du Cameroun crée la surprise en battant l’Espagne en finale après les tirs au but. Il s’agit de la première médaille d’or olympique de l’histoire pour ce pays. Le tournoi féminin revient à la Norvège qui bat les États-Unis en finale (avec un score de 3 buts à 2 après prolongations).GymnastiqueRésultats détaillésLa Chine remporte le concours général masculin par équipe après quatre décennies de frustration. Ils mettent fin au règne russe débuté en 1980. Le titre féminin revient à l’équipe de Roumanie. En individuel, Alexei Nemov est le gymnaste le plus en vue de ces Jeux avec six médailles dont deux titres au concours général et à la barre fixe. Sa compatriote Svetlana Khorkina remporte l’or aux barres asymétriques. La Russie s’impose également en gymnastique rythmique et au trampoline, portant son total à neuf médailles d’or sur dix-huit épreuves.HaltérophilieRésultats détaillésLe Turc Naim Suleymanoglu échoue dans sa tentative de conquête d’un quatrième titre olympique consécutif. Les Grecs Pyrros Dimas et Kakhi Kakhiashvili remportent leur troisièmes médailles d’or d’affilée. Les Chinois s’adjugent cinq des quinze titres olympiques.HandballRésultats détaillésLa Russie remporte pour la quatrième fois le tournoi olympique de handball masculin en battant la Suède 28 à 26 en finale. Les Suédois laisse échapper la victoire pour la troisième fois d'affilée. Le russe Andreï Lavrov devient à 38 ans le premier homme à remporter trois médailles d'or olympique en handball. Chez les dames, le Danemark bat la Hongrie en finale (31-26). La Corée du Sud ne figure pas sur le podium pour la première fois en 20 ans.Hockey sur gazonRésultats détaillésL’équipe des Pays-Bas dispose de la Corée du Sud en finale après l’épreuve des tirs au but, et devient la première nation à conserver son titre depuis l’Inde en 1956. L’Australie s’impose face à l’Argentine dans le tournoi féminin et décroche à l’occasion sa troisième médaille d’or dans cette discipline.JudoRésultats détaillésLe Français David Douillet conserve son titre des poids-lourds décroché quatre ans plus tôt à Atlanta, le japonais Tadahiro Nomura en fait de même en super-légers. Ces deux nations dominent par ailleurs la compétition avec 14 médailles en autant d’épreuves.LutteRésultats détaillésDans le choc de ces Jeux olympiques en moins de 130 kg, l’Américain Rulon Gardner bat en finale le Russe Alexandre Kareline après treize ans d’invicibilité. « L’homme le plus fort du monde » n’avait pas perdu un combat international depuis 1987 et fut champion olympique en 1988, 1992 et 1996.NatationRésultats détaillésAprès un début de saison marqué par de multiples records du monde, la nageuse Inge de Bruijn confirme son état de forme en remportant quatre médailles dont trois d'or. Ses trois victoires sont toutes gagnées dans des épreuves individuelles. La Néerlandaise s'impose tout d'abord sur le 50 m nage libre où elle devance la Suédoise Therese Alshammar de 19 centièmes avant de remporter le 100 m nage libre devant la même concurrente pour 50 centièmes. Dans le 100 m papillon, Inge de Bruijn établit un nouveau record du monde de la distance (56 s 61) et décroche sa troisième médaille d'or de ces jeux. Elle obtient en supplément une médaille d'argent dans le relais 4 × 100 m avec l'équipe des Pays-Bas. Sur un plan général, les États-Unis dominent les épreuves avec 14 couronnes sur 32 épreuves. Ian Thorpe remporte quatre médailles, dont le titre du 400 m nage libre et deux relais et Pieter van den Hoogenband s'impose sur le 100 m et sur 200 m où il établit un nouveau record du monde. L'Américaine Jenny Thompson ajoute quatre médailles olympiques supplémentaires à son palmarès (10 médailles dont 8 titres). Michael Klim décroche quatre médailles alors que sa compatriote Brooke Bennett réalise le doublé 400 m et 800 m nage libre. Les épreuves de plongeon sont dominés par les Chinois, celles de natation synchronisée par les Russes. L'équipe de Hongrie remporte le tournoi masculin de water polo et l'Australie le tournoi féminin. Le nageur équatoguinéen Éric Moussambani, surnommé « Éric le nageur » ou « Éric l'anguille » (Eric the Eel) par les médias, il a connu une célébrité internationale éphémère, lorsqu'il réalisa son 100 m nage libre en 1 min 52 s 72, soit plus de deux fois le temps mis par ses concurrents (le record du monde de la discipline était de 47 s 84 secondes lors de ces Jeux olympiques), et 10 secondes de plus que le record du monde du 200 m, qui était de 1 min 42 s 96. Les médias relevèrent la caractère incongru de sa performance, tout en applaudissant son courage, perçu comme un symbole de l'esprit olympique.Pentathlon moderneRésultats détaillésLe Russe Dmitry Svatkovsky remporte le titre masculin et la Britannique Stephanie Cook le titre féminin.SoftballRésultats détaillésÀ l’instar de leurs homologues masculins du baseball, l’équipe américaine de softball remporte le tournoi olympique en s’imposant lors des derniers matchs contre des adversaires les ayant battus lors du premier tour.TaekwondoRésultats détaillésSix nations se partagent les titres de cette nouvelle discipline olympique, la Corée du Sud en remporte trois.TennisRésultats détaillésL’Américaine Venus Williams décroche deux médailles d’or, le simple dames et le double avec sa sœur Serena. Le Russe Ievgueni Kafelnikov remporte le tournoi masculin.Tennis de tableRésultats détaillésComme à l’accoutumée, la Chine domine les épreuves du tennis de table avec quatre titres sur quatre et huit médailles sur douze. Wang Nan décroche deux médailles d’or.TirRésultats détaillésLes tireurs chinois montent huit fois sur le podium, dont trois sur la plus haute marche. Le Français Franck Dumoulin remporte le titre du pistolet à 10 mètres.Tir à l'arcRésultats détaillésL’Australien Simon Fairweather remporte l’épreuve individuelle masculine alors que les archers de Corée du Sud s’emparent des trois autres titres, dont les épreuves féminines.TriathlonRésultats détaillésInscrit pour la première fois au programme des Jeux olympiques, le triathlon consacre le Canadien Simon Whitfield et la Suissesse Brigitte McMahon.VoileRésultats détaillésL’Australie remporte, avec sa victoire en 470, sa première médaille d’or depuis 1972. Les Britanniques s’imposent à trois reprises (Europe, Finn et Laser). 10 000 spectateurs assistent à la dernière course des Jeux sur la baie de Sydney, soit la plus grande affluence enregistrée à l’occasion des régates olympiques.Volley-ballRésultats détaillésL’équipe de Yougoslavie remporte le tournoi masculin face à la Russie alors que l’équipe de Cuba remporte son troisième titre féminin consécutif.En beach volley, la paire américaine Dain Blanton et Eric Fonoimoana classée onzième mondiale bat les favoris brésiliens en finale. Chez les dames, la victoire revient à l’Australie.NB : L'athlète américaine Marion Jones qui avait obtenu cinq médailles lors de ces jeux (trois d'or et deux de bronze) a reconnu s'être dopée le 5 octobre 2007. Par conséquent, ses médailles ont dû être restituées au CIO.Quatre-vingt délégations repartent des jeux de Sydney avec au moins une médaille. Les États-Unis totalisent le plus grand nombre de podiums (94 dont 37 médailles d'or), devançant la Russie (88) et la Chine (59). L'Australie, pays hôte, réalise la plus belle moisson de récompenses olympiques de son histoire avec 58 médailles dont 16 titres.48 heures avant le début des épreuves du 400 m, l'athlète française Marie-José Pérec quitte brusquement les Jeux olympiques en raison du harcèlement dont elle s'estime victime depuis son arrivée dans la ville australienne. L'athlète évoquera notamment l'agression verbale d'un individu ayant forcé la porte de sa chambre. Elle dut faire face également à la pression médiatique australienne dans la mesure où sa principale concurrente, Cathy Freeman, est une des athlètes les plus populaires d'Australie. Lors de son départ de l'aéroport de Singapour, son compagnon Anthuan Maybank aura une altercation avec un cadreur australien.Marie-José Pérec avait comme objectif de remporter une troisième couronne consécutive sur le tour de piste mais cet épisode la poussa à mettre un terme définitif à sa carrière.Dix cas de dopage ont été enregistrés durant ces jeux de Sydney. Ils concernent en majorité des haltérophiles et des lutteurs. Les athlètes suivants furent disqualifiés et déchus de leur médaille éventuelle.En France, France Télévisions, Canal+ et Eurosport diffusèrent l’événement.La participation d'Éric Moussambani, qui a réalisé l'anti-record de 1 min 52 s 72 au 100 mètres nage libre (le record du monde et olympique était de 47 s 84) a été très remarquée. Le nageur équatoguinéen évoluait pour la première fois dans un bassin olympique. Le public australien lui réserva une ovation durant les cérémonies de médailles en scandant son prénom. Les médias relevèrent le caractère incongru de sa performance, tout en applaudissant son courage, perçu comme un symbole de l'esprit olympique,.Jeux olympiques - Jeux olympiques d'étéLes Jeux de Sydney sur le site du Comité International Olympique(en) Rapport officiel des Jeux olympiques de Sydney 2000 [PDF]Rétrospective de la XXVIIe olympiade, sur le site gamesinfo.com.au Portail des Jeux olympiques   Portail des années 2000   Portail de l’Australie   Portail de Sydney
sport;"Les Jeux olympiques (JO), aussi appelés Jeux olympiques modernes, puisqu'ils prolongent la tradition des jeux olympiques de la Grèce antique, sont des événements sportifs internationaux majeurs, regroupant les sports d’été ou d’hiver, auxquels des milliers d’athlètes participent à travers différentes compétitions tous les quatre ans, pour chaque olympiade moderne.  Originellement tenus dans le centre religieux d’Olympie, dans la Grèce antique du VIIIe siècle av. J.-C. au Ve siècle apr. J.-C., les Jeux sont rénovés par le baron français Pierre de Coubertin en 1894 lorsqu’il fonde le Comité international olympique (CIO). Depuis lors, le CIO est devenu l’organisation gouvernant le mouvement olympique dont la structure et les décisions sont définies par la Charte olympique. Les premiers Jeux olympiques modernes se déroulent en 1896 à Athènes et l'instauration des Jeux olympiques d'hiver date de 1924 à Chamonix. Ils ont lieu la même année tous les quatre ans, les années bissextiles, souvent dans le même pays sous réserves qu'il possède un territoire montagneux, puis sont décalés de deux ans à partir de 1994. Annulés en 1916, 1940 et 1944 pour cause de guerres mondiales, les Jeux ont vu leur édition de 2020 reportée d'un an en raison de la pandémie de Covid-19. Pendant le XXe siècle, le CIO adapte les Jeux à sa perception des changements économiques, politiques et techniques du monde. Ainsi, les Jeux olympiques sont, comme le voulait Pierre de Coubertin, d'abord réservés aux purs amateurs, le règlement du CIO interdisant la participation de sportifs professionnels. Bien que malmenée par les supercheries (notamment l'amateurisme marron) autour du statut faussement « amateur » de nombreux sportifs, l'exclusion du professionnalisme reste en vigueur jusqu'en 1981. Si le passage de l’amateurisme pur au professionnalisme est dans les faits progressif, le XIe Congrès olympique en 1981 marque une révolution pour l'olympisme, avec l'admission des sportifs officiellement professionnels,. Une autre évolution importante concerne la féminisation des épreuves, d'aucune femme en compétition en 1896 et un fort déséquilibre par la suite, jusqu'à une quasi-parité de nos jours. Le CIO adapte aussi les Jeux aux changements sociaux qui se produisent au XXe siècle. Certains de ces ajustements incluent l'instauration des Jeux olympiques d’hiver, les Jeux paralympiques ou encore les Jeux olympiques de la jeunesse et la création de nombreuses épreuves mixtes. En outre, l’importante croissance des médias de masse apporte aux Jeux des sources de financement considérables, entraînant parfois des problèmes de corruption,.Actuellement, le mouvement olympique comprend les fédérations sportives internationales, les comités nationaux olympiques et la mise sur pied de comités d'organisation locaux pour chaque édition des Jeux olympiques. La ville hôte est chargée d’organiser les Jeux olympiques de manière qu’ils soient en accord avec la Charte olympique. Le CIO décide aussi des sports présents ou non à chaque édition. La célébration des Jeux inclut de nombreux rituels et des symboles, comme le drapeau olympique et la flamme olympique, le relais de la flamme, ainsi que les cérémonies d’ouverture et de clôture. Les trois meilleurs athlètes ou équipes de chaque compétition reçoivent respectivement une médaille d’or (1re place), d’argent (2e place) et de bronze (3e place). Pour les Jeux d'été, la participation est plafonnée à environ 10 500 athlètes et à 28 sports se déclinant en plus de 300 épreuves. Les Jeux olympiques sont devenus si importants que presque chaque nation est représentée. Une telle ampleur a causé de nombreux défis, comme le boycott, le dopage, la corruption et le terrorisme. Tous les deux ans, les Jeux et leur exposition médiatique permettent à des athlètes d'acquérir une notoriété nationale, voire mondiale dans certains cas. Les Jeux sont aussi une excellente occasion pour la ville hôte et le pays d'accueil d'assurer leur promotion sur la scène internationale.Le sportif le plus médaillé des Jeux olympiques, été comme hiver, est le nageur américain Michael Phelps, qui gagne entre 2004 et 2016, vingt-huit médailles dont vingt-trois en or. Aux Jeux d'hiver, la fondeuse norvégienne Marit Bjørgen détient un record de quinze podiums, dont huit médailles d'or.De nombreuses légendes entourent l'origine des Jeux olympiques antiques. L'une dit qu'Héraclès construisit le stade olympique ainsi que les bâtiments alentour en l'honneur de son père Zeus, après avoir accompli ses douze travaux. Il aurait également défini la longueur du stade olympique en l'arpentant avec la longueur de son pied en avançant de 600 pas.Les premiers Jeux olympiques sont réputés pour avoir pris place en 776 av. J.-C. sur l'initiative d'Iphitos, roi d'Élide. Cette année marque le début du calendrier olympique, selon lequel les années sont regroupées en olympiades, et l'an 1 du calendrier grec adopté en 260 av. J.-C. Toutefois, il est probable que les Jeux aient été encore plus anciens, compte tenu de l'abondance des offrandes de l'époque géométrique retrouvées à Olympie. Dès lors, les Jeux gagnèrent en importance dans toute la Grèce antique, mais il existe près de 300 réunions sportives du même type, les agônes. On passe à plus de 500 sous l'Empire romain. Les Jeux olympiques forment, avec les Jeux pythiques, les Jeux néméens, et les Jeux isthmiques, un cycle des jeux sacrés dont l'un revient chaque année. L'athlète qui gagne des prix à ces quatre Jeux panhelléniques est désigné par le titre de « periodonikès ».Le programme des compétitions comprend des épreuves hippiques (chars à deux ou quatre chevaux) et des épreuves athlétiques dites de gymnastique (course à pied sur plusieurs distances, lancer du disque, saut en longueur, lancer du javelot, pentathlon, lutte, pugilat et pancrace). Disque, longueur et javelot ne donnent pas de titre olympique, mais font partie des cinq épreuves du pentathlon avec la course du stade et la lutte.Corèbe d'Élis ouvre le palmarès olympique officiel en remportant la course pédestre du stade en 776 av. J.-C. Parmi les autres principaux athlètes grecs des Jeux antiques, citons Milon de Crotone (lutte, VIe siècle av. J.-C.), Diagoras de Rhodes (boxe, Ve siècle av. J.-C.), Polydamas de Scoutoussa (pancrace, VIe siècle av. J.-C.), Léonidas de Rhodes (course, IIe siècle av. J.-C.) et Mélancomas de Carie (boxe, au Ier siècle). À partir de la septième olympiade (752 av. J.-C.), le champion olympique reçoit une couronne d’olivier sauvage, une branche de palmier et un ruban de laine rouge appelé la tænia. Le Messénien Daikles est le premier champion olympique honoré ainsi.Réservés d'abord aux seuls citoyens grecs masculins et riches, les Jeux entraînent une trêve olympique. Cette dernière n'arrête pas les conflits, mais autorise les athlètes et spectateurs à traverser librement des zones de guerre sans être inquiétés. La portée d'un titre olympique est considérable. Les champions sont d'authentiques héros populaires et sont couverts de cadeaux et d'honneurs à leur retour dans leur cité. Ils sont de plus pleinement professionnels depuis le Ve siècle av. J.-C. et peuvent décider de défendre les couleurs d'une autre cité. Ces changements d'allégeance provoquent souvent des troubles, parfois importants, dans la cité « trahie ». On peut ainsi citer le cas de Astylos de Crotone (6 titres olympiques), qui passe de Crotone à Syracuse en 484 av. J.-C., provoquant de graves troubles à Crotone.Un serment olympique en quatorze points régit l'organisation des Jeux depuis 338 av. J.-C. Le Xe point concerne les cas de tricheries qui sont nombreux et durement sanctionnés.I. Être sujet hellène libre, ni esclave, ni métèque.II. N'être ni repris de justice, ni d’une moralité douteuse.III. S’inscrire à l’avance au stage d’un mois du gymnase d’Elis.IV. Tout retardataire sera hors concours.V. Interdiction aux femmes mariées d’assister aux Jeux ou de se montrer dans l’Altis sous peine d’être précipitées du rocher du Typaion.VI. Pendant les exercices, les maîtres (entraîneurs) des athlètes devront être parqués et nus.VII. Défense de tuer son adversaire, ou de chercher à le tuer.VIII. Défense de le pousser hors des limites.IX. Défense de l’intimider.X. Toute corruption d’arbitre ou d’adversaire sera punie.XI. Tout concurrent contre lequel ne se présentera pas l’adversaire désigné sera déclaré vainqueur.XII. Défense aux concurrents de manifester contre le public ou contre les juges.XIII. Tout concurrent mécontent d'une décision peut en appeler au Sénat contre les arbitres : ceux-ci seront punis ou leur décision annulée si elle est jugée erronée.XIV. Sera hors concours tout membre du Collège des Juges.À la suite de l'invasion romaine, les Jeux s'ouvrent aux non-Grecs. Le prestige des Jeux est tel que plusieurs empereurs y prennent part. Sur les conseils de l'évêque Ambroise de Milan, l'empereur Théodose Ier interdit les Jeux en 393-394 en raison de leur caractère païen. Cette interdiction ne vise d'ailleurs pas spécifiquement les Jeux olympiques mais de façon générale les Jeux du cirque dont ceux-là sont un événement particulier.Les Jeux olympiques connaissent quelques timides tentatives de rénovation entre la fin du XVIIIe siècle, époque à laquelle on découvre les ruines des sites d'Olympie, et la fin du XIXe siècle. Citons ainsi l'Olympiade de la République qui se tient à Paris en 1796, 1797 et 1798. Esprit-Paul De Laffont-Poulotti réclame même le rétablissement des Jeux olympiques. Il va jusqu'à présenter un projet à la municipalité parisienne, qui rejette l’idée. Le CIO honora la mémoire de ce visionnaire en 1924. Parmi les autres tentatives, citons les Jeux du petit séminaire du Rondeau à Grenoble à partir de 1832, les Jeux scandinaves (en 1834 et 1836), les festivals olympiques britanniques (depuis 1849) comme les Jeux de Much Wenlock, les Jeux athlétiques disputés à Montréal (Canada) en 1843 et qui sont rebaptisés Jeux olympiques pour les éditions 1844 et 1845 et les jeux olympiques de Zappas à Athènes en 1859 et 1870. L'Allemagne tient également un rôle important dans cette rénovation en étant déterminante en matière de fouilles archéologiques menées par Ernst Curtius sur le site d'Olympie et en devenant, très tôt, favorable à la rénovation.Il faut préciser que la rénovation des Jeux olympiques n'est pas seulement inspirée par les Jeux antiques. L'actualité de cette fin de XIXe siècle influence nettement l'esprit de ceux qui vont lancer le nouveau mouvement olympique : la défaite grecque contre les turcs en 1897, celle des Français contre les Allemands en 1870 incitent les gouvernements à réformer l'éducation de leur jeunesse en favorisant le sport et l'éducation physique pour endurcir les corps, fortifier les esprits et préparer cette jeunesse à combattre pour la revanche. C'est cependant la volonté de Pierre de Coubertin de favoriser les interactions culturelles entre les pays et de promouvoir les valeurs éducatives et universelles du pays qui l'oriente vers son projet de rénover les Jeux. De même, l'inspiration puise également ses sources dans des pratiques profondément ancrées dans la culture européenne comme celle des joutes chevaleresques médiévales. Cette tradition nobiliaire explique que les Jeux olympiques attendent de leurs athlètes qu'ils aient l'étoffe d'aristocrates en cultivant le fair-play des gentlemen, les attitudes gestuelles et l'amateurisme éthique (seuls les athlètes issus des classes les plus favorisées pouvant consacrer leur temps à faire du sport, notamment l'escrime, le yachting, le tennis ou l'équitation, épreuves phares des premiers Jeux olympiques) qui se développe en réaction à la professionnalisation du sport par les classes populaires, le « shamateurisme » (de shame, « la honte », et d'amateurisme) des sportifs roturiers étant perçu comme une subversion des codes de l'amateurisme,. La fédération omnisports française d'athlétisme USFSA fête son cinquième anniversaire le 25 novembre 1892 dans le grand amphithéâtre de la Sorbonne à Paris. À cette occasion, Pierre de Coubertin appelle à la rénovation des Jeux olympiques. Deux ans plus tard, du 16 au 23 juin 1894, se tient également à la Sorbonne le « Congrès pour le rétablissement des Jeux olympiques ». Devant l’absence de réactions à son appel deux ans plus tôt, Pierre de Coubertin parvient à convaincre les représentants britanniques et américains, mais aussi d'autres nations, notamment la Jamaïque, la Nouvelle-Zélande ou la Suède. Plus de 2 000 personnes représentant douze nations assistent finalement au congrès, qui vote à l’unanimité la rénovation des Jeux olympiques. L'autre décision importante prise à l’occasion de ce Congrès est la condamnation des règlements sportifs de certaines fédérations (britanniques notamment) excluant les ouvriers et les artisans au nom d’un élitisme social qui allait à l’encontre des idéaux égalitaires français.À l'origine, les Jeux sont exclusivement estivaux. Le patinage artistique et le hockey sur glace font ainsi des apparitions au programme olympique avant même la création de Jeux d'hiver, en 1924.Après le succès initial des épreuves à Athènes en 1896, les olympiades de Paris en 1900 (qui virent pour la première fois des femmes participer aux épreuves, Charlotte Cooper étant la première championne olympique) et de Saint Louis en 1904 sont noyées dans les programmes des expositions universelles. Le premier athlète noir à participer, à remporter une médaille et à être champion olympique est l'Haïtien d'origine Constantin Henriquez, en 1900.Les Jeux olympiques intercalaires de 1906 d'Athènes, non reconnus ultérieurement par le CIO, marquèrent un regain d'intérêt du public et des athlètes, avec une participation très internationale alors que 80 % des sportifs ayant pris part aux Jeux de Saint-Louis étaient américains. Les nations européennes avaient en effet renoncé à faire le long et coûteux déplacement outre-Atlantique.De 241 athlètes de quatorze nations en 1896, les Jeux passent à 10 568 sportifs représentant 204 délégations lors des Jeux olympiques de Londres en 2012. C'est désormais l'un des événements les plus médiatisés. Les Jeux de Sydney en 2000 réunissent ainsi plus de 16 000 journalistes et diffuseurs. La dimension de l'épreuve est telle que cela pose des problèmes aux villes hôtes, que le sponsoring ne couvre que partiellement. Les villes hôtes profitent en effet des Jeux pour s'équiper notamment en transports en commun et autres équipements sportifs. À titre d'exemple, le budget estimé des Jeux de Londres en 2012 est de neuf milliards de livres sterling.Sous la tutelle du CIO ont également lieu des jeux régionaux. Les plus anciens sont les Jeux d'Amérique centrale et des Caraïbes, tenus pour la première fois à Mexico en 1926.Localisation des Jeux olympiques modernesLe programme des compétitions se met progressivement en place. Lors de la première édition des Jeux (776 av. J.-C.), une seule épreuve est disputée : c'est la course pédestre du stade (environ 192 m). En 724 av. J.-C., la course pédestre du double stade (diaulos) est introduite dans le programme, puis quatre ans plus tard, la première épreuve de fond fait son apparition : le dolichos, soit vingt-quatre stades (environ 4 600 m). Le pentathlon est introduit au programme olympique en 708 av. J.-C. en même temps que la lutte. Le pugilat arrive en 688 av. J.-C. et le pancrace en 648 av. J.-C. La course d’hoplites (course pédestre en tenue militaire) fait son entrée au programme en 520 av. J.-C.Du côté des courses hippiques, les courses de quadriges (quatre chevaux) figurent au programme olympique depuis 680 av. J.C.. Les courses montées se disputent depuis 648 av. J.-C.Des épreuves de course et de lutte réservées aux juniors sont ajoutées au programme olympique en 632 av. J.-C. Un concours de pentathlon (628 av. J.-C.) et un autre de pugilat (616 av. J.-C.) viennent ensuite compléter le programme olympique des juniors.En plus de ce programme sportif, des concours culturels étaient organisés. Platon est ainsi sacré deux fois « olympionique ». Femmes aux Jeux En ce qui concerne les épreuves, les femmes ne pouvaient pas participer. On retrouve tout de même des noms de femmes dans les palmarès des vainqueurs de courses de chars. Cela tient au fait qu'on n'inscrivait pas le nom du conducteur, mais celui du propriétaire de l'attelage.La nudité des athlètes lors des épreuves est parfois expliquée comme une conséquence de la victoire d'une femme lors d'une olympiade, alors que les participants concouraient encore vêtus. Cette pratique serait donc une solution pour exclure à coup sûr les femmes des épreuves. Mais aucune explication sérieuse sur ce sujet n'a encore été donnée, les Grecs se contentant eux-mêmes d'anecdotes peu convaincantes.Contrairement aux Jeux antiques, le programme olympique moderne est beaucoup moins stable. Chaque édition des Jeux apporte ainsi son lot de nouveautés, nouvelles disciplines et nouvelles catégories. Conservateur et colonialiste convaincu, Pierre de Coubertin n'imagine pas des Jeux olympiques valorisant le corps de l'athlète noir ou celui de la femme, mais ses convictions sont initialement peu appliquées car le CIO a le contrôle de la doctrine mais pas de l'organisation des premiers Jeux qui est déléguée à des entrepreneurs de spectacle.Le CIO est devenu progressivement le seul décisionnaire sur l'admission d'une discipline au programme olympique. En 1919, Alice Milliat demande au Comité international olympique d'inclure des épreuves féminines lors des prochains Jeux olympiques, mais sa demande est refusée. Les femmes sont finalement admises aux épreuves athlétiques des Jeux olympiques d'été de 1928 à Amsterdam. Le CIO doit désormais composer avec les Fédérations internationales gérant les disciplines. Le programme des compétitions sportives ne propose pas l'ensemble des disciplines sportives, ni même la totalité des différentes épreuves possibles. Les Jeux d'été comptent 302 podiums, et c'est un plafond que le CIO ne souhaite pas dépasser. Ainsi, nombre de sports sont écartés du programme, comme c'est le cas du baseball et du softball après les Jeux de 2008, tandis que d'autres disciplines souhaitant profiter de la vitrine olympique sont priées d'attendre. Les Jeux mondiaux rassemblent certains de ces sports non-olympiques mais dont les fédérations internationales sont reconnues par le CIO. Jusqu'en 1996, ces sports pouvaient profiter du statut de sport de démonstration.Le nombre des participants aux Jeux olympiques d'hiver est plus modeste avec environ 2 500 athlètes à Turin en 2006. Et du côté du programme, on cherche plutôt à l'étoffer. Certaines disciplines de salle ont été approchées pour passer des JO d'été à ceux d'hiver mais les fédérations internationales concernées ont refusé.Afin de contenir l'expansion, le nombre d'athlètes participants aux Jeux est désormais plafonné à 10 500 en été et les participants doivent désormais réaliser des minima dans les disciplines chiffrées ou profiter de quotas olympiques gagnés lors des grandes compétitions précédant les Jeux. Pour permettre à toutes les nations de participer, les minima sont à géométrie variable selon les nations et un Comité olympique n'ayant aucun athlète qualifié aux Jeux profite d'invitations, généralement en athlétisme, natation, judo ou haltérophilie pour les Jeux d'été. Les femmes aux Jeux olympiques Les femmes ne sont acceptées que dans quelques disciplines lors de leurs premiers Jeux Olympiques telles que le golf, l’équitation, le tennis, la voile et le croquet. Elles représentent seulement 2.2% de tous les athlètes olympiques qui y sont présents, des statistiques qui restent stables durant de nombreux Jeux.Depuis les Jeux de 2004 à Athènes, les femmes représentent plus de 40% des athlètes. Ne pouvant pas concourir dans toutes les disciplines, les femmes restent inférieures jusqu’en 2012 où l’épreuve de la boxe féminine fait son entrée dans le programme. Cette édition des Jeux Olympiques est la première où les femmes concourent dans tous les sports.Les côtes d’écoute des épreuves féminines étaient et sont encore à ce jour, en 2022, inférieures à celle des hommes et c’est donc pour promouvoir l’innovation et la plus grande diversité des sexes que le Comité International Olympique ajoute, depuis les Jeux de Tokyo 2020, des épreuves mixtes dans plusieurs disciplines. On en compte 18 lors de ces Jeux, en athlétisme, badminton, judo, natation, sports équestres, tennis, tennis de table, tir, tir à l’arc, triathlon et voile. Comme l'affirme Kit McConnell, le directeur des sports du CIO : « Il n'y a rien de plus égal qu'un homme et une femme qui concourent en tant qu'équipe sur la même aire de compétition en vue de la même performance sportive ». Disciplines, sports et nombre d'épreuves par sport aux Jeux olympiques d'été NotaLe fond grisé mentionne les sports non admis ;Le point indique les sports qui ne sont que sports de démonstration. Disciplines, sports et nombre d'épreuves par sport aux Jeux d'hiver NB : Le fond bleu indique les épreuves disputées à l'occasion des Jeux olympiques d'été.En 1948, Sir Ludwig Guttman, fermement décidé à promouvoir la réhabilitation des soldats de la Seconde Guerre mondiale, organisa une compétition sportive entre différents hôpitaux au même moment que les Jeux olympiques d'été de 1948 à Londres. Cette compétition, connue alors sous le nom de Jeux de Stoke Mandeville, devint annuelle. Durant les douze années suivantes, Guttman et d’autres continuèrent d’utiliser le sport comme thérapie de guérison. Aux Jeux olympiques d'été de 1960 à Rome, Guttman réunit 400 athlètes pour concourir dans les « Jeux olympiques parallèles » et devinrent les premiers Jeux paralympiques. Depuis, les Paralympiques ont lieu chaque année olympique et se déroulent dans la même ville que les Jeux olympiques depuis les Jeux de Séoul en 1988.À partir de 2010, les Jeux olympiques accueillent les Jeux olympiques de la jeunesse (JOJ), où les athlètes ont entre 14 et 18 ans. Les JOJ sont créés par Jacques Rogge, président du CIO, en 2001. La décision est approuvée pendant le 119e Congrès du CIO,. Les Jeux olympiques de la jeunesse d'été de 2010 se tiennent à Singapour et ceux d’hiver en 2012, à Innsbruck en Autriche. Ces Jeux durent moins longtemps que les Jeux olympiques traditionnels. Ceux d’été durent douze jours et ceux d’hiver, neuf jours. 3 500 athlètes et 875 officiels vont participer aux JO d’été de 2010, et 970 athlètes et 580 officiels aux JO d’hiver,. Les sports au programme coïncident avec ceux des Jeux olympiques traditionnels, cependant le nombre de disciplines et d’épreuves est diminué.Le CIO est fondé lors du Congrès olympique de 1894 à Paris. Il a pour mission d'organiser les Jeux. Composé de 115 membres qui se réunissent au moins une fois par an, et élisent un président pour une durée de huit ans. Le mouvement olympique regroupe un grand nombre d’organisations et de fédérations sportives nationales et internationales, de partenaires médiatiques reconnus, d’athlètes, d’officiels, et juges et toutes les personnes et institutions qui sont d’accord pour respecter les règles de la Charte olympique. Organisation de coordination du mouvement olympique, le CIO est responsable du choix de la ville hôte, la négociation des partenaires et des droits de diffusion, de superviser le programme du déroulement des Jeux olympiques, actualiser et approuver le programme sportif.Le CIO reconnaît 206 comités nationaux, selon des critères différents de ceux définissant un État au sens du droit international. De nombreuses dépendances prennent ainsi part aux Jeux sous leur propre drapeau, tel que les Bermudes, Porto Rico ou Hong Kong, alors qu'elles sont légalement parties intégrante d'un autre État. Depuis 1980, Taïwan participe sous le nom de Chine de Taipei, la République populaire de Chine refusant sa propre participation si Taïwan était présent sous le nom de République de Chine. Les Îles Marshall ont quant à elles été reconnues par le CIO le 9 février 2003.Le mouvement olympique regroupe trois grands éléments :Les fédérations sportives internationales régissent un sport au niveau international. Par exemple, la Fédération internationale de football association (FIFA) est la fédération internationale du football et la Fédération internationale de volley-ball est la fédération internationale qui régit le volleyball. On compte actuellement 35 fédérations internationales dans le mouvement olympique représentant chaque sport olympique.Le Comité national olympique (CNO) représente et régule le mouvement olympique dans chaque pays. Par exemple, le comité olympique français est le Comité national olympique (CNOSF) de la France. On compte aujourd’hui 206 CNO reconnus par le CIO.Les Comités d’Organisation des Jeux Olympiques sont des comités temporaires responsables de l’organisation de Jeux olympiques spécifiques, un comité pour chaque ville organisatrice. Chaque comité est donc dissout après chaque Jeux, une fois que le compte-rendu définitif est donné au CIO.Le français et l’anglais sont les langues officielles du mouvement olympique. La langue du pays organisateur des Jeux olympiques est aussi utilisée. Toutes les annonces (comme celle du nom du pays lors du défilé des nations pendant la cérémonie d’ouverture) sont déclarées dans ces trois langues, dans cet ordre.Le CIO a souvent été critiqué car c’est une organisation intraitable, avec plusieurs de ses membres élus à vie. Les directions de Avery Brundage et Juan Antonio Samaranch furent en particulier controversées. Brundage fut président du CIO pendant plus de 20 ans. Pendant sa présidence, il protégea les Jeux olympiques de toutes implications politiques préjudiciables. Il fut accusé de racisme pour sa gestion du problème de l’apartheid avec la délégation Sud-Africaine et d’antisémitisme. Samaranch fut accusé de népotisme et de corruption. Les liens qu’entretenait Samaranch avec le régime de Franco furent aussi une source de vives critiques. En 1998, on révéla que plusieurs membres du CIO avaient reçu des pots de vin de la part du comité d’organisation de Salt Lake City pour s’assurer que leurs votes iraient en leur faveur. Le CIO entama une enquête qui aboutit à la démission de quatre membres et à l’exclusion de six autres. Le scandale eut aussi pour conséquence la mise en place de réformes pour la sélection des villes organisatrices afin d’éviter ce genre de cas à l’avenir.Un documentaire de la BBC intitulé Panorama: Buying the Games diffusé en août 2004, retrace l’enquête qui eut lieu sur les pots de vin lors de la sélection de la ville organisatrice pour les Jeux olympiques d'été de 2012. Le documentaire montra qu’il était possible d’acheter les membres du CIO afin qu’ils votent pour une ville en particulier. Après la défaite de Paris pour les Jeux de 2012, Bertrand Delanoë accusa en particulier Tony Blair, Premier ministre britannique, et le comité londonien (dont Sebastian Coe était à la tête) d’enfreindre les règles des votes. Il cita comme témoin Jacques Chirac. La sélection de Turin pour les Jeux olympiques d'hiver de 2006 fut aussi controversée. Marc Hodler, éminent membre du CIO, et en faveur de la ville concurrente de Sion en Suisse, affirma que certains membres du CIO avaient été achetés par le Comité d’organisation de Turin. Ces accusations menèrent à une enquête et desservirent la candidature de Sion en faveur de Turin.Le calendrier olympique, le déroulement des cérémonies et leur symbolique est le résultat d'une évolution. Ainsi, il n'y a pas de cérémonie d'ouverture en 1900 à Paris. Le drapeau olympique dessiné par Coubertin en 1913 apparaît aux Jeux de 1920 tout comme le serment olympique. La flamme olympique, symbolisant le lien entre Jeux antiques et Jeux modernes, est en usage depuis 1928. Depuis 1936 elle effectue un parcours sous forme de relais avant la tenue des Jeux. Cette dernière innovation fut créée par Goebbels. Un hymne olympique existe depuis 1896. Cette pièce de musique grecque est officiellement hymne olympique depuis 1960. Le défilé des athlètes est la plus longue des séquences des cérémonies d'ouverture et de clôture. Le défilé est toujours ouvert par la délégation grecque et le pays qui accueille les Jeux ferme la marche.Entre les cérémonies d'ouverture et de clôtures, deux semaines de compétitions se tiennent sur différents sites, parfois assez éloignés. Les athlètes sont logés dans un village olympique exclusivement réservé aux athlètes et aux entraîneurs. Les journalistes sont regroupés au sein d'un centre médias et ont un accès limité au village olympique des athlètes. L'organisation fait appel à des milliers de volontaires bénévoles afin d'assister les athlètes, les officiels, les journalistes et les spectateurs. L'une des traditions typiques des Jeux est l'échange de Pin's entre délégations et médias. Les volontaires terminent souvent les Jeux couverts de ces épinglettes.La mascotte olympique apparaît officiellement pendant les Jeux d'hiver de 1968 à Grenoble. Depuis, chaque édition crée sa propre mascotte afin de symboliser les valeurs de l'olympisme.La devise latine des Jeux olympiques est, depuis 1894, année du premier congrès olympique : citius, altius, fortius… (plus vite, plus haut, plus fort…). C'est Pierre de Coubertin qui proposa cette devise, empruntée à son ami dominicain, l'abbé Henri Didon, ancien vainqueur en 1855 des jeux olympiques du petit séminaire du Rondeau de Grenoble. Le mardi 20 juillet 2021, le CIO s'est réuni à Tokyo à trois jours de la Cérémonie d'ouverture des Jeux olympiques d'été de 2020, lors de sa 138e session. Il a alors décidé de marquer une rupture dans l'histoire des Jeux modernes en modifiant la devise qui est désormais : Plus vite, Plus haut, Plus fort - Ensemble (Citius, Altius, Fortius - Communiter).Les langues en usage pendant les Jeux sont, dans cet ordre, le français, l'anglais et la langue locale. À l'usage, le français recule pourtant clairement devant l'anglais au niveau de la signalisation sur les sites olympiques tandis que l'anglais est privilégié dans les discours des cérémonies d'ouverture et de clôture. C'est pourtant bien en français que débute la cérémonie de remise des médailles, comme le prévoit le protocole olympique.L'extinction de la flamme olympique marque la fin de la parenthèse olympique.Conçu en 1913 par Pierre de Coubertin, il fut présenté officiellement au congrès olympique de Paris en juin 1914. Mais c’est seulement en 1920 aux Jeux d’Anvers qu’on le voit flotter pour la première fois.Le baron Pierre de Coubertin expliquait lui-même :« Le drapeau olympique, on le sait, est tout blanc avec, au centre, cinq anneaux enlacés : bleu, jaune, noir, vert, rouge ; l’anneau bleu en haut et à gauche à côté de la hampe. Ainsi dessiné, il est symbolique ; il représente les cinq parties du monde unies par l’Olympisme et ses cinq couleurs d’autre part reproduisent celles de tous les drapeaux nationaux qui flottent à travers l’univers de nos jours. »— Coubertin (1931), Textes choisisLes équipes ou athlètes qui se classent en première, deuxième ou troisième place dans chaque épreuve reçoivent des médailles. Les vainqueurs de l'épreuve reçoivent des médailles d'or, qui étaient en or massif jusqu'en 1912, puis en argent doré et maintenant en argent plaqué or. Chaque médaille d'or doit toutefois contenir au moins six grammes d'or pur. Les finalistes recevront des médailles d'argent et pour la troisième place les athlètes sont récompensés par une médaille de bronze. Dans les épreuves contestées par un tournoi à élimination directe (comme la boxe), la troisième place ne pourrait être déterminée et les deux perdants des demi-finales reçoivent des médailles de bronze. Aux Jeux olympiques de 1896, seulement les deux premiers ont reçu une médaille, l'argent pour le premier et le cuivre pour le deuxième. Le format actuel de trois médailles a été introduit aux Jeux olympiques de 1904. Depuis 1948, les athlètes classés quatrièmes, cinquièmes et sixièmes ont reçu des certificats dont le nom est aujourd'hui diplôme olympique. En 1984, le diplôme est élargi aux septième et huitième places. Lors des Jeux de 1896 à Athènes, les médaillés ont reçu des diplômes ainsi qu’un rameau d’olivier pour les premiers et une branche de lauriers pour les deuxièmes. Lors des Jeux suivants ayant lieu à Athènes, en 2004, les athlètes médaillés recevaient également une couronne d'olivier en souvenir de ces premiers Jeux. Le CIO ne tient pas de statistiques pour les médailles remportées, mais les comités nationaux olympiques et les médias tiennent des statistiques concernant les médailles et les records pour mesurer les succès des différentes nations participantes.Au départ, le CIO trouvait ses fonds grâce à des société"
sport;Les Jeux paralympiques sont un événement sportif international regroupant les sports d’été ou d’hiver, auquel des milliers d’athlètes atteints de handicap participent à travers différentes compétitions tous les quatre ans à la suite des Jeux olympiques, pour chaque olympiade. Y participent des athlètes atteints par un handicap physique, visuel ou mental. Ils sont organisés par le Comité international paralympique (et non pas par le Comité international olympique).Les personnes atteintes de surdité peuvent prendre part aux Deaflympics. Les personnes atteintes d'un handicap mental pouvaient aussi participer aux Jeux olympiques spéciaux jusqu'à leur réintégration en 2012 aux Jeux paralympiques d'été de 2012 (après en avoir été exclus depuis 2000).Mr Ludwig Guttmann, médecin neurologue de l'hôpital de Stoke Mandeville dans le comté de Buckinghamshire près de Londres, eut l'idée d'organiser dès 1948 sur le terrain de l’hôpital les premiers « Jeux mondiaux des chaises-roulantes et des amputés » (« World Wheelchair and Amputee Gates »). Connus plus tard sous le nom de « Jeux de Stoke Mandeville », ils étaient destinés à réhabiliter par la pratique physique des victimes et anciens combattants de la Seconde Guerre mondiale devenus paraplégiques. Deux équipes d’anciens combattants ont alors participé à une unique épreuve, le tir à l’arc.Les 9e jeux de Stoke-Mandeville eurent lieu à Rome en 1960 une semaine après les Jeux olympiques d'été de 1960, et l'on considère qu'il s'agit des premiers Jeux paralympiques. Les premiers Jeux paralympiques d'hiver eurent lieu à Örnsköldsvik en Suède en 1976.Parallèlement, ont lieu à Saint-Étienne, à l'initiative d'Yves Nayme, plusieurs éditions de jeux internationaux pour les personnes handicapées physiques (jeux européens de 1966,  jeux mondiaux de 1970 et 1975 et championnats du monde de 1990),.Les personnes atteintes de paralysie cérébrale participent aux Jeux paralympiques depuis les Jeux d'Arnhem en 1980.Depuis les Jeux paralympiques d’été de Séoul en 1988, les Jeux olympiques et les Jeux paralympiques sont organisés dans la même ville et sont organisés après les jeux olympiques.Les premiers Jeux paralympiques africains auraient dû avoir lieu à Rabat au Maroc en janvier 2020. Ils ont cependant été reportés pour des raisons logistiques et matérielles.Les Jeux paralympiques d'été de 2020 à Tokyo ont été reportés à 2021, tout comme les Jeux olympiques d'été de Tokyo en 2020, en raison de la pandémie de Covid-19, ce qui est une première dans l'histoire des Jeux paralympiques.À l'origine, le nom « paralympique » était une combinaison de « paraplégique» et de « olympique ». Avec la participation d'athlètes avec différents handicaps, le terme « paralympique » est aujourd'hui défini comme la réunion de « para », préfixe d'origine grecque signifiant « à côté de » ou « parallèle » et de la terminaison « lympique » des Jeux olympiques. Les Jeux paralympiques sont ainsi considérés comme solidaires des Jeux olympiques.L'objectif du Mouvement paralympique est de donner l’occasion aux athlètes ayant un handicap physique ou mental de se dépasser et de réaliser des performances sportives comparables à celles des athlètes olympiques.Les Jeux paralympiques regroupent des athlètes en situation de handicap physique ou visuel appartenant aux catégories suivantes : tétraplégie et paraplégie, séquelles neurologiques assimilables, amputation et assimilé, infirmes moteurs cérébraux, grands handicaps (myopathie), non-voyants et malvoyants.Pour que la compétition soit équitable, les athlètes sont regroupés par catégories selon leur handicap. L'objectif est de faire concourir ensemble des athlètes ayant des aptitudes fonctionnelles comparables. Dans chaque handisport, on définit des catégories. Ainsi en athlétisme, il y a des épreuves de course pour les personnes atteintes de cécité (acuité visuelle inférieure à 3/60), de déficience visuelle (inférieure à 3/10 et supérieure à 1/20), pour les personnes amputés qui courent avec une prothèse et des courses en fauteuil roulant.Les sourds et malentendants n'ont toujours pas le droit de participer aux Jeux paralympiques. Ceci peut paraître logique, dans la mesure où les personnes faiblement sourdes ont des capacités physiques peu altérées. Par contre, il est difficile de comprendre pourquoi les sourds profonds qui peuvent avoir une altération de l'équilibre ne participent pas avec les autres handicapés. En réalité, la non-intégration des sourds et malentendants semble découler du fait qu'ils ont leur propre concours, les Deaflympics, qui sont historiquement la plus ancienne compétition handisport internationale. Il peut aussi y avoir des cas de tricherie comme cela s'est produit avec les handicapés mentaux.Entre 2004 et 2012, les personnes en situation de handicap mental ont été exclues des Jeux paralympiques auxquels ils prenaient part depuis 1996, pour des problèmes de classification de handicap et de fausse déficience intellectuelle. Les personnes en situation de handicap mental pouvaient cependant participer aux Jeux olympiques spéciaux qui n'avaient pas lieu la même année que les Jeux olympiques ordinaires et les Jeux paralympiques.Cependant, depuis 2012, les personnes en situation de handicap mental sont réintégrées dans les compétitions officielles et participent aux Jeux paralympiques de Londres en athlétisme, natation, et tennis de table.Après des années de travail « main dans la main » la Fédération française du sport adapté (FFSA), les différentes fédérations nationales et la Fédération internationale des sports pour personnes en situation de handicap mental ont pu « établir de nouveaux critères d'éligibilité incomparablement plus solides que par le passé ».La décision de réintégrer les personnes en situation de handicap mental a été prise lors de l'assemblée générale du Comité international paralympique à Kuala Lumpur. Gérard Masson, président de la Fédération française handisport, a soutenu cette décision même si parfois « l'intégration n'est pas aussi évidente dans le monde du handicap ». Pour lui, la réticence de certains pays à réintégrer les personnes en situation de handicap mental dans les jeux paralympiques « tenait plus aux critères de classification qu'à un rejet de la part des autres athlètes paralympiques ».Dans un communiqué, la ministre de la santé et des sports Roselyne Bachelot et la secrétaire d'État chargée des sports ont alors salué « le travail remarquable engagé depuis plusieurs années par la Fédération internationale de sport adapté, avec le soutien du Comité paralympique et sportif français et de la Fédération française de sport adapté, pour réintégrer les sportifs en situation de handicap mental dans le mouvement paralympique ».                   Les Jeux paralympiques reprennent la plupart des symboles olympiques : les cérémonies d'ouverture et de clôture, la flamme olympique, les mascottes.AthlétismeAvironBoccia (sport ressemblant aux boules, pratiqué avec des balles en cuir par des handicapés moteurs)Basket-ball en fauteuil roulantCyclismeÉquitationEscrimeFootball à 5 (ou cécifoot) (pratiqué par des athlètes malvoyants ou non-voyants)Football à 7 (pratiqué par des athlètes handicapés moteur)Goalball (sport de ballon pratiqué par des athlètes malvoyants ou non-voyants avec un ballon sonore)HaltérophilieJudo (pratiqué par des athlètes malvoyants ou non-voyants)NatationRugby en fauteuil roulantTennis en fauteuil roulantTennis de tableTir à l'arcTir sportifTriathlonSki alpinSki nordique (ski de fond et biathlon)Para-hockey sur glace (anciennement hockey sur luge)CurlingSnowboardLa classification Comité international paralympique (CIP) pour les sports d'hiver indique la classification des athlètes en fonction de leur handicap pour les disciplines de sport d'hiver et en particulier pour les Jeux paralympiques d'hiver.Les athlètes sont classés par catégorie de handicaps assimilables et selon les matériels techniques utilisés. Les classes B concernent les handicapés visuels (Blinds). Les classes LW (Locomotion Winter) les handicapés moteurs, debout : LW1 à LW9 ou assis : LW10 à LW12.Les skieurs assis peuvent effectuer des tests fonctionnels. L'objectif est de définir l'appartenance à l'une des trois classes LW10, LW11 ou LW12. Pour la classe LW12 également classer les athlètes dans les sous-classes LW12/1 et LW12/2.Catégorie:Tableau des médailles des Jeux paralympiques d'étéCatégorie:Tableau des médailles des Jeux paralympiques d'hiverListe d'athlètes olympiques ou paralympiques devenus parlementairesInternational Paralympic Committee« Toute l'actualité handisport et sport adapté »(Archive.org • Wikiwix • Archive.is • Google • Que faire ?)L'actualité paralympique francophone  Portail des Jeux olympiques   Portail du handisport
sport;"Au football, le libéro est un joueur défensif dont le rôle est de couvrir ses coéquipiers défenseurs. Il se trouve par conséquent derrière les autres joueurs de la défense mais devant le gardien de but, et n'est pas tenu au marquage individuel.Libero veut dire « libre » en italien. Le mot a été employé au sens actuel pour la première fois par le journaliste italien Gianni Brera ; il s'est ensuite répandu dans maintes langues. À l'origine, il caractérisait le joueur qui évoluait derrière ses défenseurs et qui verrouillait les brèches de sa défense, dans le championnat italien. Ce rôle nécessitait que son positionnement fût libre, derrière la défense, d'où l'appellation. Au début, il était dévolu aux joueurs plutôt rugueux et physiques, il était capable d'intervenir comme dernier défenseur. Le système du libéro prenait tout son sens dans le catenaccio italien mis en place par Helenio Herrera à l'Inter Milan au milieu des années 1960, qu'Alfredo Foni avait déjà testé quelques années auparavant. Ce système s'inspirait de l'organisation de jeu mise en place par l'Autrichien Karl Rappan dans l'équipe de Suisse dans les années 1930 et 1940. En plaçant un défenseur juste devant le gardien, Rappan avait inventé le poste de libéro.Cependant, le rôle du libéro va considérablement évoluer à la fin des années 1960, grâce à l'Allemand Franz Beckenbauer. Sur les conseils de son entraîneur Branko Zebec, Beckenbauer réinvente le poste et fait du libéro un atout offensif. En effet, par sa liberté sur le terrain, le libéro, exempt de tout marquage, peut dès lors se joindre à l'attaque et participer au jeu offensif. Dans cette organisation tactique, le libéro est alors souvent un milieu offensif reconverti, possédant une bonne vision du jeu et de grandes qualités techniques, et capable de se transformer en véritable meneur de jeu. Le système de jeu avec un libéro va être particulièrement prisé dans les années 1970 et 1980, en particulier en Allemagne, où le rôle de libéro était souvent réservé aux meilleurs joueurs de l'équipe.Néanmoins, le recours à un libéro va tomber en désuétude au cours des années 1990. La systématisation de la défense en ligne imposée par Arrigo Sacchi (où les quatre défenseurs jouent tous sur une même ligne afin de mettre l'adversaire hors-jeu) et le choix de privilégier le marquage dit en zone plutôt que le marquage individuel vont sonner le glas du libéro. De plus, l'obligation pour le gardien de systématiquement jouer les ballons donnés par ses coéquipiers au pied ou de la tête (et non plus de la main comme auparavant) va précipiter la disparition des libéros. C'est désormais le gardien qui assure les relances comme dernier défenseur.Le terme « libéro » est cependant encore usité dans le football, souvent à tort puisqu'il est difficile de considérer ce rôle dans une défense en ligne. Dans ce contexte de complémentarité des défenseurs centraux, on désigne comme « libéro » celui des deux qui est le plus technique et privilégie le placement et l'anticipation, par opposition au stoppeur, plutôt voué à faire valoir ses qualités physiques. En réalité, les meilleures équipes ont des défenseurs centraux jouant sur les deux registres, alors que les moins bonnes se contentent en général de joueur au profil de stoppeur.On peut aussi remarquer que cette disparition du libéro est quelquefois compensée, lors de balle en profondeur derrière la défense, par la sortie du gardien au-delà de sa surface de réparation — ceci étant facilité par la capacité des gardiens modernes à utiliser aussi bien leurs pieds que leurs mains.Tactique (football) Portail du football"
sport;Une mise en échec ou charge est une technique défensive au hockey sur glace qui consiste à bousculer l'adversaire pour le gêner ou lui faire perdre la rondelle (palet). Pour ce faire, les joueurs utilisent leur épaule, leur hanche ou leur bras.En France, l'utilisation de la mise en échec est autorisée depuis 1951. Elle l'est uniquement à partir de la catégorie Minimes U15, et interdite pour le hockey sur glace féminin.Au Canada, les mises en échec sont permises en Alberta au niveau peewee (11-12 ans) et au Québec au niveau bantam (13-14 ans).Un « bloc » consiste à faire perdre la rondelle par le biais d'une mise en échec lorsque celui-ci se trouve le long de la bande (balustrade). Lors de cette tentative, c'est la responsabilité d'un coéquipier de reprendre possession de la rondelle et non du joueur qui bloque l'adversaire. Cette technique est plutôt complexe car un joueur ne peut pas bloquer son adversaire plus que deux secondes sans risquer une pénalité pour obstruction.Un allègement consiste à soutirer la rondelle d'un adversaire le long de la bande en même temps de le sortir du jeu en utilisant une mise en échec. Cette tentative est plus pratique que le bloc car elle implique seulement un joueur comparativement à deux.Les joueurs doivent savoir appliquer correctement une mise en échec, car certaines charges sont sanctionnées (voir article sur les pénalités):cinglage (frapper ou tenter de frapper un joueur avec le bâton de hockey)faire trébucher (que ce soit avec la crosse, le pied, le bras)charge incorrecte (mauvaise position, prise d'élan, saut...)ou plus grave : charge avec le bâton, dans la région de la tête, dans le dos, en dessous du genou, crosse haute (au-dessus des épaules), charge contre la bande (si l'adversaire n'a pas le palet), coup de coude,Un joueur ne doit pas tenter de blesser un adversaire.La mise en échec n'est pas sans risques, elle fait plus que tripler le risque de blessures et de commotions cérébrales dans le hockey pee-wee, selon une étude réalisée par l'Université de Calgary.Les joueurs doivent se protéger pour réduire les risques de blessures. Les plus courantes sont les fractures, les commotions cérébrales et les coupures. Certaines peuvent occasionner des lésions permanentes comme celles infligées au joueur canadien de 17 ans Francis Gariépy, devenu paraplégique lors d’un match au niveau secondaire à la suite d’une mise en échec en 2009.Certains groupes s’opposent au Canada à cette pratique violente du hockey.Dans l'arrêt de principe Morin c. Blais de la Cour suprême du Canada, la Cour énonce que la violation d'une norme élémentaire de prudence fait présumer le lien de causalité entre la faute et le préjudice. Dans la décision Zaccardo c. Chartis Insurance Company of Canada, la Cour supérieure du Québec a appliqué cette règle dans le contexte d'une violation des règles d'une ligue de hockey par un joueur qui a commis une mise en échec par-derrière interdite par Hockey Canada et qui a rendu un autre joueur tétraplégique. Ce qu'il faut retenir de l'arrêt Zaccardo, c'est que la violation d'une règle élémentaire de prudence contenue dans la soft law (par ex. règle sportive, règle d'hygiène, règle de sécurité) va faire présumer le lien causal entre la faute et le dommage. Portail du hockey sur glace
sport;"La musique populaire désigne les genres de musique tirant leur origine et trouvant leur public dans les milieux populaires. Elle se développe dans un milieu urbain et industrialisé et est souvent associée à l'histoire de la révolution industrielle et technologique ayant amené la technique phonographique, ainsi qu'à l'histoire de la mondialisation.Le terme est souvent utilisé comme un comparatif par certains défenseurs de la musique savante, qui perçoivent la musique populaire comme un produit commercial et pointent ses faiblesses esthétiques, qu'ils jugent en comparaison de la musique classique européenne. Si la musique populaire est souvent associée à la musique commerciale ou de masse, elle s'en distingue néanmoins par des critères qualitatifs et par sa capacité à former des communautés de mélomanes en se nourrissant de formes musicales inscrites dans diverses traditions historiques.Il ne faut pas confondre la musique populaire avec la musique pop, qui est un genre spécifique de musique populaire.Le terme de musique populaire est l'objet de débats. Le sociologue Simon Frith estime que le terme de culture populaire « n'a de sens qu'en tant que comparatif » et que ses plus fréquents objets de comparaison sont la haute culture, la culture folklorique et la culture de masse. La musique populaire est en effet souvent comparée à la musique savante, la musique traditionnelle et la musique commerciale.Depuis les années 1980, le milieu universitaire retient la « définition anglo-saxonne » du terme, comme le rappelle le musicologue français Olivier Julien : « sont populaires non pas les musiques qui ne sont pas savantes, mais les musiques qui ne sont ni savantes, ni folkloriques ». Citant Philip Tagg, pionnier des études sur la musique populaire, le même auteur précise que « les musiques populaires […] partagent avec les musiques folkloriques l'absence de cadre institutionnel, mais ont en commun avec la musique savante d'être jouées et composées par des musiciens professionnels ». La musique populaire se distingue aussi de la musique savante (transmise par la partition) et de la musique traditionnelle (transmise par la tradition orale) par son rapport avec la technique phonographique, qui lui fait traverser l'histoire. Pour Simon Frith, elle se distingue aussi de la musique de masse, car si la musique populaire est « consommée d'une manière particulière clairement différenciée de celle des élites culturelles », l'adjectif « populaire » ne se confond pas avec celui de « masse » : « de nombreuses musiques populaires […] ont de plus faibles ventes […] que des enregistrements de musique classique à succès ».La musique populaire européenne a hérité de certains des usages de la musique modale, du système tonal et des instruments de la musique classique, mais la musique populaire de manière générale peut aussi se référer à d'autres genres musicaux et à d'autres traditions musicales. Par exemple la musique populaire japonaise se nourrit à la fois du jazz et de sa musique traditionnelle, et certains morceaux des Beatles empruntent aussi bien à la musique classique européenne qu'à la pop américaine ou à la musique traditionnelle indienne. La variété des genres et l'éclatement des frontières musicales qui caractérise la musique populaire la lient intimement à l'histoire de la mondialisation et de la révolution industrielle.Pour ses détracteurs, la musique populaire est assimilée à la culture de masse ou à la musique « commerciale ». C'est le cas de Theodor W. Adorno, qui a rendu célèbre le concept d'industrie culturelle et voyait dans les genres de musique populaire comme le jazz des simples modes ou produits commerciaux.Il a des racines très anciennes dans le chant traditionnel dit folklorique ou de folklore vivant, en France pour partie chanté en Breton, basque, provençal, corse, flamand, alsacien, etc. puis en français surtout à partir du XIXe siècle. Les thèmes des saisons, des amours, des âges de la vie, du mariage, des guerres et de la mort sont récurrents. Il accompagnait la vie de tous les jours, les travaux des champs et la garde des troupeaux par les enfants, les danses, les fêtes, etc.C'est une personne (homme ou femme), souvent anonyme, qui chante sur la voie publique, parfois associée au camelot. Ce chanteur vit de l'argent que ses auditeurs lui donnent. Des styles et modes particuliers existent selon les époques et les pays (ex : les prosopopées dites lamenti italiens composés et imprimés durant la Renaissance, de 1453 aux années 1630-1650 ; lamenti storici, parodiques, satiriques et musicaux). Parfois sans instruments, parfois muni d'un porte-voix, il cherche à attirer et captiver un maximum d'auditoire en un temps très court et s'appuie pour cela sur une musique mélodieuse, un air déjà connu et/ou un texte accrocheur, parfois politique et satirique, devant alors parfois se jouer de la police.Très populaires avant l'invention des médias modernes (radio, télévision, enregistrement sonore), ils ont largement contribué à la diffusion d'idées ou d'informations au même titre que les journaux. En effet, en dehors de quelques grands standards de la musique populaire, leur répertoire s'inspirait souvent de faits majeurs ou de faits divers remarquables, assurant une publicité à ces événements. Au XIXe siècle avec la révolution industrielle, l'apparition d'une classe ouvrière urbaine et son exode rural, elle contribue à porter et diffuser la chanson ouvrière et « sociale ».Habitués à se mettre en public dans des conditions difficiles, les chanteurs de rue avaient souvent une personnalité originale et extravertie. Au nombre de ceux-ci le célèbre Aubert (né vers 1769, attesté en vie en 1848), doyen des chanteurs des rues de Paris fut nommé par ses confrères « Syndic des chanteurs des rues » de Paris. En 1848, il parle au nom de la délégation de 800 chanteurs, musiciens et mendiants des rues de Paris venus rendre hommage à l'Élysée au chansonnier Béranger membre de la commission des secours.Le chanteur des rues a toujours fait partie des « Cris (et bruits) de la rue », mais le développement de la voiture et l'augmentation du volume sonore lié à la vie moderne, la difficulté d'occuper la voie publique, l'accusation de mendicité et surtout la banalisation des enregistrements sonores ont réduit la présence des chanteurs de rue. Il en reste malgré tout, y compris officiellement,.Le marchand de musique ou de chanson est une profession aujourd'hui disparue en Europe mais qui était encore active dans l'entre deux-guerres, avant la large diffusion de la radio puis de la télévision. C'est un métier connexe à la chanson populaire depuis plusieurs siècles (chanson autrefois spécifiquement éditée et diffusée sur feuilles volantes). Les marchands de musique étaient itinérants et vendaient des partitions de chant en entonnant eux-mêmes la musique. Ils parcouraient les villes et se déplaçaient de foire en foire. Ils proposaient leurs chansons sous forme de feuille volante, souvent grossièrement imprimée, à des personnes qui ne savaient globalement pas lire la musique, mais qui étaient intéressées par la mélodie ou par le texte de la chanson. Ces feuilles volantes étaient parfois également illustrées par des gravures, œuvres d'illustrateurs connus, intéressantes du point de vue artistique et iconographique. Certains marchands de musique ne déchiffraient pas les partitions, mais avaient une bonne mémoire des airs. Leurs feuilles volantes restent une mine d'information sur les idées, les coutumes et les centres d'intérêt des Européens au XIXe et au début du XXe siècle.Dès le début du XIXe siècle, les orphéons fédèrent les masses. Il s'agit d'abord de chorales d'enfants puis d'ouvriers. Quelques noms : Wilhem, pédagogue et fondateur du premier orphéon en 1833. Delaporte qui contribuera dans la seconde partie du XIXe siècle à donner une ampleur nationale au mouvement. À partir des années 1850, le terme « Orphéon » désigne les chorales, les fanfares et les orchestres d'harmonie qui ont connu un essor dû au développement de l'industrie des instruments de musique. Les héritiers actuels du mouvement sont le mouvement À Cœur Joie (chorales) l'UFF (fanfares) la CFBF (batterie-fanfare), la CMF (orchestres d'harmonie, orchestres à plectre).Au XIXe siècle, des centaines de goguettes rassemblent à Paris, dans sa banlieue et aux alentours des dizaines de milliers d'ouvriers ou journaliers, hommes ou femmes. Il en existe encore au siècle suivant. La goguette de la Muse rouge disparaît seulement en 1939.La musique populaire s'appuie sur quelques standards musicaux et commerciaux. Elle est aussi à l'origine d'un certain vocabulaire.Il s'agit essentiellement de chansons (des paroles soutenues par une musique instrumentale ou un petit chœur). Une chanson dure la plupart du temps entre 3 et 5 minutes (durée initiale de la face d'un disque 78 tours ou d'un vinyle 45 tours). Les textes utilisent le vocabulaire courant, voire familier. La musique est essentiellement tonale, écrite dans le mode majeur ou en mode mineur. Sa structure repose souvent sur une alternance entre un refrain et quelques couplets (en général, moins de cinq).L'ensemble, musique et paroles, est facile à mémoriser par écoute répétée. Elle s'efforce ainsi d'être facilement compréhensible et donc diffusable internationalement. À cet effet, on note une nette prédominance de l'anglais dans les paroles, au moins en ce qui concerne celle qui s'exporte massivement. La musique s'efforce de pouvoir être diffusée le plus largement possible : utilisation d'instruments courants (guitares, claviers, cuivres, cordes, percussions), arrangements musicaux standards, quasi-monopole de la langue anglaise pour les paroles de la version dite « internationale » sans pour autant éliminer toute forme de production nationale.Avant l'invention des médias audios modernes (radio, télévision, disques), la diffusion était assurée par des chanteurs de rue qui vendaient les partitions sur les marchés en entonnant eux-mêmes les chansons. La généralisation de la radio a favorisé l'émergence d'une diffusion sur les ondes par des chanteurs qui initialement interprétaient en direct puis se sont enregistrés. Actuellement, la diffusion est massive et se fait par ondes radio, par CD (on parle alors d'EPK), et par diffusion de clips vidéo au cours d'émissions de télévision, mais surtout sur YouTube, ou par des applications de musique numérique, comme Spotify, Deezer ou Apple Music.Un tube, ou « hit », est une chanson qui a particulièrement « bien marché », c'est-à-dire qu'elle a atteint des sommets de vente.Un disque d'or ou de platine récompense l'auteur d'une musique qui s'est bien vendue.Un hit-parade (en anglais : chart) est une compétition permanente de musique populaire organisée par des chaînes de radio ou de télévision. L'objectif est d'être no 1 (être « au top »), ce qui est théoriquement déterminé par le nombre de disques vendus ou par le vote des auditeurs. Plus longtemps une chanson est en tête du hit-parade, plus elle s'assure une large diffusion, favorisant les retombées commerciales.Il est notable que l'aspect commercial et promotionnel soit une caractéristique dominante de la musique populaire depuis la deuxième moitié du XXe siècle : première en termes de part de marché dans le monde de la musique, la musique populaire est l'objet d'enjeux commerciaux énormes pour les producteurs de musique, ce qui justifie l'emploi de méthodes commerciales poussées, identiques à celles utilisées pour les produits de consommation courante : méthodes dites des « grands lessiviers » : Procter & Gamble, Henkel, etc. C'est ainsi qu'une musique fait l'objet d'une « politique de lancement » pour toucher une « cible privilégiée », qu'on « fait la promotion » d'un nouveau chanteur en espérant que ses ventes « décollent », ou qu'on résilie le contrat d'un chanteur qui ne « se vend plus assez » ou dont le genre « arrive en fin de vie », quitte à le rappeler s'il « rebondit ». Les droits d'exploitation des musiques les plus populaires représentent une source importante de revenus que l'on ne cède pas facilement.La principale production de musique populaire est donc le résultat d'une politique visant à générer des profits. Ces enjeux commerciaux sont surtout le fait des grandes majors du disque (Universal, EMI, Sony, BMG). Les maisons de disques indépendantes (comme Tôt ou Tard, Naïve Records) à la diffusion plus limitée semblent être moins à la recherche de profits. Certains musiciens ne trouvant pas de maisons de disques « s'autoproduisent », mais ils bénéficient alors d'une distribution « classique » (vente de CD en magasins) et d'une visibilité réduite, bien que le développement d'Internet ait changé la donne au cours des dernières années ; on voit notamment des sites permettant de participer à la production d'artistes inconnus du grand public et des outils de diffusion comme MySpace ou autres,.Si l'enregistrement de musique en studio fait toujours appel à des professionnels, la musique populaire est la musique la plus jouée par des amateurs. De nombreux « groupes de garage » se créent dans le but de reprendre leurs musiques préférées à partir des enregistrements de leurs vedettes. Les plus talentueux et les plus constants pourront même arriver à jouer en public (soirées privées, clubs d'étudiants, bals…). Ce type de réinterprétation à partir des disques a remplacé le modèle de la musique traditionnelle fondé en grande partie sur la transmission par le jeu. De nombreux groupes de rock, de pop ou de jazz ont commencé par faire de la musique sous cette forme. Parmi les plus célèbres on peut citer les Beatles et les Rolling Stones.Le karaoké est également une forme de réinterprétation devenue courante : à partir d'un enregistrement de l'arrangement musical « réputé exact », un soliste au micro chante la mélodie. Très utilisé dans les soirées conviviales et exclusivement fondé sur des chansons à succès, le karaoké laisse une part d'interprétation au soliste. Actuellement, les musiciens amateurs peuvent profiter de la vulgarisation des outils d'enregistrement et de reproduction (stations de mixage, samplers, logiciels de mixage, graveurs de CD, etc.) pour autoproduire leur musique et n'hésitent plus à la diffuser, par Internet notamment.(en) Frans A. J. Birrer, « Definitions and research orientation: do we need a definition of popular music? » (Définitions et axe de recherche : avons-nous besoin d'une définition de la musique populaire ?), 1985, in D. Horn, ed., Popular Music Perspectives, 2 (Gothenburge, Exeter, Ottawa et Reggio Emilia), p. 99-106.Hugh Dauncey & Philippe Le Guern, Stéréo, Sociologie comparée des musiques populaires France-Angleterre, Bordeaux, IRMA / Éditions Mélanie Seteun, 2008.Marcello Sorce Keller, Contextes socioéconomiques et pratiques musicales dans les cultures traditionnelles, in Jean-Jacques Nattiez (general ed.), Musiques, Une encyclopédie du XXIe siècle, Volume 3 : Éd. Actes Sud / Cité de la musique, p. 559–592.(en) Marcello Sorce Keller, The Problem of Classification in Folksong Research: A Short History, Folklore, XCV(1984), no 1, 100- 104.Vassal, Jacques. Folksong [soi-disant]: une histoire de la musique populaire [en majeure partie] aux États-Unis. Nouv. éd. Paris : Éditions Albin-Michel, 1972, cop. 1971. 354 p.Volume! La revue des musiques populaires. La seule revue universitaire française exclusivement consacrée à l'analyse pluridisciplinaire des musiques populaires.Philippe Darriulat, La Muse du peuple : chansons politiques et sociales en France 1815-1871, Presses universitaires de Rennes, 2010R. Thérien, I d'Amours (1992), Dictionnaire de la musique populaire au Québec, 1955-1992, Lavoisier.B. Bartók (1948), Pourquoi et comment recueille-t-on la musique populaire ?, Impr. A. KundigC. D. Pessin (2004), Chanson sociale et chanson réaliste - Cités, avec cairn.infoMarie-Dominique Amaouche-Antoine, « Le cahier de chansons du conscrit » ; Revue d'histoire moderne et contemporaine (1954-) T. 34e, No 4 (oct. - déc., 1987), p. 679–685, Éd. : Société d'Histoire Moderne et Contemporaine ([URL : https://www.jstor.org/stable/20529337 1re page])Top 50Billboard magazineCatégorie:Classement musical Portail de la musique"
sport;"Le palet, la rondelle, le disque, le ou la puck est un disque rond et épais notamment utilisé en hockey sur glace et en roller in line hockey. Le joueur lance le palet vers le but de l’adversaire à l'aide d'une  crosse ou bâton de hockey.Le palet est fabriqué en caoutchouc vulcanisé de 2,54 cm d'épaisseur (un pouce) et 7,62 cm de diamètre (trois pouces). Il pèse entre 156 et 170 grammes (5,5 à 6 onces).Le palet de hockey sur glace fut créé en 1877 par William F. Robertson, en coupant une balle en deux, pour éviter les rebonds incessants vers les spectateurs.La vitesse maximale enregistrée d'un tir a été de 183,67 km/h par Aleksandr Riazantsev lors du 4e Match des étoiles de la Ligue continentale de hockey, en 2012.La chaîne de télévision Fox, dans le but de rendre les matchs de la Ligue nationale de hockey plus faciles à suivre à la télévision, inventa le FoxTrax, rondelle munie de diodes électroluminescentes.Au roller in line hockey, le palet est fait en plastique. Il mesure environ 2,5 cm d'épaisseur et de 7,62 cm de diamètre, et est légèrement rebondi en son centre ou muni de pastilles de roulement. Les rondelles de bonne qualité sont fabriquées de manière à rester toujours couchées sur le sol et à ne pas rouler sur leur tranche.Généralement noir, il peut aussi être orange, jaune, rouge, rose ou vert.Il existe de nombreuses formes de jeux de palets en Angleterre, en Espagne, en, France, au Portugal et en Vallée d'Aoste (Palet valdôtain).On peut diviser le jeu en deux grandes pratiques : le jeu de palet proprement dit qui consiste à lancer des palets le plus près possible d'un autre palet plus petit, préalablement lancé sur une surface délimitée (planche de bois, plaque de plomb) ou directement sur le sol (terre, route).Le jeu de galoche consiste lui à faire tomber un cylindre sur lequel on place la mise ou ce qui tient lieu de ""bouchon"" le plus près possible de ses palets.En Belgique francophone, en France et en Vallée d'Aoste, on utilise le terme « palet ».En Suisse romande, c'est le terme puck (au masculin) qui est le plus courant, prononcé autrefois « pok », mais le terme « palet » est aussi utilisé, ainsi que le terme « rondelle ».Au Canada français, on emploie les termes « rondelle », « disque » et puck (féminin ou masculin ; dans le langage familier).Le mot anglais puck est utilisé dans de nombreuses langues, dont l'allemand, le néerlandais et l'italien. Son origine est incertaine et ne semble liée ni à la mythologie, ni à Shakespeare ; l'Oxford English Dictionary suggère que le mot vient du verbe to puck (toucher ou frapper en français).Brittanie Cecil, une spectatrice de 13 ans d'un match de hockey, est morte à Columbus le 16 mars 2002 en recevant un tir du joueur professionnel norvégien Espen Knutsen. Désormais, toutes les patinoires internationales ont, au-dessus du plexiglas, un filet pour empêcher le palet de sortir.En France, un garçon de 8 ans est mort après avoir été heurté par un palet lors d'un match entre Dunkerque et Reims le 1er novembre 2014, dans une patinoire non équipée de vitres de plexiglas autour de la glace.(en) Cet article est partiellement ou en totalité issu de l’article de Wikipédia en anglais intitulé « Hockey puck » (voir la liste des auteurs).Cet article est partiellement ou en totalité issu de l'article intitulé « Jeux de palets » (voir la liste des auteurs). Portail du hockey sur glace"
sport;"Un sport de raquette est un sport où les participants utilisent des raquettes, qui servent à renvoyer vers l'adversaire un objet, le plus souvent une balle.En règle générale, ces sports utilisent le principe du duel et les adversaires se renvoient un objet à l'intérieur des limites d'un terrain. Il existe des variations sur l'objet renvoyé, comme le badminton où un volant remplace la balle. D'autres sports se rapprochent des jeux de raquettes, comme la balle au tambourin où le tambourin remplace la raquette ; certains sports anciens se jouent à main nue, tel initialement le jeu de paume dont la plupart des sports de raquette actuels sont plus ou moins dérivés.Les sports de raquette ont comme ancêtre commun le jeu de paume, inventé en France au XIIIe siècle, qui se jouait à l'origine à mains nues, puis avec des gants. Vers la fin du XVe siècle les gants sont renforcés avec une sorte de cordage, puis apparaissent des battoirs en bois. La première mention d'une raquette date du début du XVIe siècle. Les premières raquettes avaient un long manche et un cordage en boyau de mouton, pesaient environ 400 grammes et mesuraient 65 centimètres. Au XVIe siècle le jeu de paume devient le « jeu des rois », et François Ier, Henri II, Charles IX et Henri IV le pratiquent souvent. Les Anglais jouaient au jeu de paume avec des raquettes, et vers 1850 ils inventent le jeu de rackets qui se joue à plusieurs contre un mur et avec des raquettes en 15 points, mais ce jeu passe de mode; il réapparaîtra en 1924 sous le nom de squash. Le badminton apparaît en 1873 et l'histoire du tennis commence en 1874 avec le major Walter Clopton Wingfield qui en codifie les règles. C'est vers 1890 qu’apparaît le tennis de table. Les autres sports de raquette sont codifiés par la suite à partir du début du XXe siècle.Les sports de raquette font partie des activités qui contribuent à améliorer l'endurance cardiovasculaire. Ils ont comme caractéristiques communes de demander une acuité visuelle dynamique, une bonne perception de la profondeur, une perception périphérique du mouvement et une bonne coordination générale ainsi que le sens de l'anticipation.BadmintonTennisTennis de table (ou ping-pong)Beach ballBeach tennisJeu de paumeJeu de raquettesPadelRacketlonRacquetballSpeed-ballSpeed Badminton ou Speedminton et son équivalent nocturne le BlackmintonSquashTennis légerTouchtennisBalle au tambourinCrosse au champ (Lacrosse au Canada et aux États-Unis, La crosse au champ est au programme des Jeux olympiques d'été de 1904 à Saint-Louis et 1908 à Londres )Crosse en enclos (Lacrosse au Canada et aux États-Unis)Pelote basqueJokariFrescobolUltimate pingUn filet et des sports, Approches sociologique, historique, prospective, comportementaliste, Stéphane Méry, Logiques sociales, janvier 2008,  (ISBN 978-2-296-04632-0) Portail du sport   Portail des sports de raquette"
sport;"Le roller in line hockey, souvent désigné par le terme plus générique roller hockey, qui englobe également la discipline du rink hockey, ou par son nom anglais inline hockey, parfois abrégé RILH, est une des disciplines de hockey en patins à roulettes. Il se pratique avec des patins aux roues alignées (rollers en ligne), généralement en intérieur. Dans certains pays comme la France, le roller in line hockey est parfois appelé « street hockey » lorsqu'il n'est pas pratiqué en salle, bien qu'en toute rigueur ce terme désigne une variante spécifique, jouée avec une balle.Chaque match oppose deux équipes, composées d'un gardien de but et de 4 joueurs de champ présents sur le terrains (deux défenseurs et deux attaquants). L'objectif est de marquer plus de buts que l'autre équipe, en envoyant à l'aide d'une crosse (ou bâton de hockey) un disque en plastique, appelé rondelle ou palet, dans le but des adversaires, situé à l'extrémité du terrain à l'opposée de son propre but.Le roller in line hockey s'est développé à partir des années 1990 avec la popularisation des rollers en ligne. Largement inspiré du hockey sur glace, il s'en démarque notamment par l'interdiction des mises en échec (également interdites au hockey sur glace féminin), ce qui rend sa pratique nettement moins dangereuse, et un joueur en moins par équipe. Les contacts entre joueurs sont toutefois autorisés, contrairement au rink hockey.Au niveau international, deux fédérations gèrent le roller hockey, la Fédération internationale de roller sports (FIRS) et la Fédération internationale de hockey sur glace (IIHF pour International Ice Hockey Federation, son nom anglais). Chacune organise ses propres compétitions indépendamment et avec des équipes qui diffèrent (notamment les championnats du monde masculin, le championnat du monde féminin n'étant quant à lui organisé que par la FIRS). Le RILH fait partie des sports officiels des jeux mondiaux depuis l'édition 2005.Les premières images connues de hockey avec des rollers en ligne, au lieu des « quads » habituellement utilisés au rink hockey, datent de 1938. Elles sont filmées à Vienne (Autriche) et diffusées dans le film d'actualité Giornale Luce (it) B1401 du 3 novembre 1938. La vidéo montre un match, joué en extérieur sur une surface rectangulaire, visiblement en asphalte. Les joueurs portent des patins avec cinq roulettes plates en métal alignées et un frein à l'avant, et utilisent des crosses de hockey sur glace et une balle. Chaque équipe est composée de quatre joueurs et d'un gardien de but.Bien que plus rapides que les « quads », les rollers en ligne, considérés comme moins maniables, restent peu répandus jusqu'en 1993, quand les Hosers de San Diego deviennent la première équipe à remporter le championnat national américain en étant équipés uniquement de ce type de patins. La fédération américaine de roller sports (en), sous l'égide de la fédération internationale de roller sports (FIRS), organise à Chicago le premier championnat du monde en 1995, puis le premier championnat du monde junior l'année suivante, toujours à Chicago. Le premier championnat du monde féminin a lui eu lieu en 2002 à Rochester, dans l'État de New York. La fédération internationale de hockey sur glace (IIHF) organise un championnat du monde masculin, distinct de celui de la FIRS, à partir de 1996.Depuis 2005, le roller in line hockey fait partie des disciplines des Jeux mondiaux et depuis 2017 des World Roller Games.La crosse est constituée d'un manche mesurant au maximum 1,57 m, présentant un coude à sa base, entre le manche à proprement parler et la palette, large, plate et incurvée vers l'intérieur.Pour les joueurs, la longueur maximum d’une palette est de 32 cm tout en conservant une largeur comprise entre 5 et 9 cm.Pour les gardiens, la longueur maximum est de 39 cm et sa largeur à 13 cm.Le bâton de hockey peut être fait de différents matériaux : traditionnellement en bois, on en trouve en fibre de carbone, voire en aluminium, ou en plastique type PVC pour ceux bon marché.Le palet est un cylindre en plastique dur, d'environ 2,5 cm d'épaisseur et de 7,62 cm de diamètre, légèrement rebondi en son centre ou muni de pastilles de roulement. Les rondelles de bonne qualité sont fabriquées de manière à rester toujours couchées sur le sol et à ne pas rouler sur leur tranche.Les rollers utilisés sont dépourvus de frein. La platine en métal est courte et accueille quatre roues. Contrairement aux rollers utilisés pour le fitness, de structure droite et à semelle parallèle au sol, les roues sont généralement de tailles différentes (les deux roues arrière ont un plus grand diamètre que les deux roues avant, avec deux ou trois diamètres de roues par patins), ou alors le patin est légèrement incliné (talon plus haut que la pointe) afin de faciliter les changements brusques de direction et certaines prises d'appui. La chaussure montante est dure afin de protéger des coups de crosse et des tirs, et présente un évasement laissant à la cheville une mobilité accrue. Les rollers de gardien peuvent avoir cinq roues, plus petites et plus dures, pour une meilleure stabilité debout et une mise au sol rapide pour les arrêts, particulièrement pour les gardiens de style papillon.Le roller hockey est un sport avec rythme rapide et collectif, les contacts physiques entre les joueurs sont assez courants au cours d'un match ou d'un entraînement. C'est contacts peuvent entraîner des blessures et pour parer à cela, les joueurs utilisent différents équipements de protection. L'équipement utilisé par les joueurs est différents selon le sexe du joueur et différent de l'équipement du gardien de but.Les joueurs sont équipés d'un casque qui doit être une protection faciale intégrale. Il porte également une gaine, des jambières, des coudières, des gants et des rollers. Il est obligatoire d'avoir une tenue qui recouvre l'équipement excepté les gants, le casques et les rollers. Les tenues des joueurs de champ sont constituées de maillots identiques à manches longues et de pantalons longs.Détail de l'équipement de joueur : des jambières (obligatoires) qui protègent du haut du pied jusqu'au genou inclus. Elles protègent des coups de crosse, des jets de palets et des chutes.une coquille (obligatoire) qui protège essentiellement du palet et des crosses ; pour les femmes, une protection pelvienne est requise.des gants (obligatoire) qui possèdent une large ouverture pour permettre de plier le poignet. Le pouce doit contenir une tige rigide pour protéger des chutes. Identiques aux gants de hockey sur glace, ils sont articulés suivant la structure des phalanges, et contiennent des blocs mobiles de mousse dense protégeant des coups.un casque (obligatoire) avec une grille ou une visière intégrale. Seuls les seniors (nés avant le 1er janvier 1989) sont autorisés à ne pas porter de protection faciale intégrale (source : règles du RILH février 2008).des coudières (obligatoire) qui protègent des chutes et des coups de crosse sur les coudes et les avant-bras.une culotte (facultatif) qui protège le coccyx, les cuisses et les hanches.un protège-dents (facultatif mais recommandé si le casque n'a ni visière intégrale ni grille).un plastron, obligatoire pour les femmes au-dessus de 15 ans ; les épaulières rigides sont désormais interdites car jugées dangereuses lors des chocs accidentels épaule contre tête.L'équipement du gardien est plus spécifique : un gants appelé bouclier, un casque comprenant un protège cou obligatoire depuis la blessure de Clint Malarchuck, une crosse plus large, des jambières aussi appelé ""bottes"", un plastron épais, une gaine et une mitaine.L'équipement du gardien comprend :des bottes,une culotte protégeant les cuisses, les fesses, le coccyx et les hanches,un plastron protégeant le ventre, le torse et les bras,un casque, un protège cou,une mitaine : gant en cuir possédant une « poche » pour attraper le palet en l'air ou le bloquer au sol,un bouclier : gant en cuir protégé par une plaque rectangulaire rigide au-dessus de l'avant-bras,une crosse, plus courte que celles des joueurs et dont la moitié inférieure est plus large.Les tenues sont composées d'un pantalon long et d'un maillot à manches longues. Le numéro du joueur doit y être inscrit et être compris entre 00 et 99. Toutes les protections doivent être placées sous la tenue, à l'exception du casque, des gants et des bottes du gardien.La largeur du terrain est comprise entre 20 et 30 m et sa longueur entre 40 et 60 m avec un ratio de un sur deux. La FIRS préconise l'usage de cages en acier, dont les dimensions internes sont de 105 par 170 cm. Historiquement, le RILH se jouait avec des cages de hockey sur glace, mais lorsque la FIRS en a pris en charge la gestion, elle a choisi d'aligner la taille des buts sur celle du rink hockey, sa discipline historique. L'IIHF ne reconnait pas ce changement, et continue d'utiliser des cages de 122 par 183 cm, comme pour le hockey sur glace. Même parmi les compétitions estampillées FIRS, nombreuses sont celles qui persistent à utiliser des cages de hockey sur glace, particulièrement en Amérique du Nord, où le rink hockey est une pratique très marginale.Le terrain est entouré par des balustrades hautes de 1,06 m et comporte des marquages. Le marquage au sol réglementaire est similaire au hockey sur glace. Ce marquage doit comporter les éléments suivants :Une ligne au centre du terrain partageant le terrain en deux camps distinctsDeux lignes de but situés à 3,80 m de chaque extrémité de la pisteDeux zones de but, une pour chaque camp, rectangulaireCinq points d'engagement. Un point situé au centre du terrain entouré d'un cercle de 3 m de rayon. Deux fois deux points situés à 6,10 m de chaque ligne de but.D'une zone pour les arbitres, représentée par un demi-cercle de 3 m de rayon, située au bord du terrain, traversée en son centre par la ligne central.Le sol peut être fait dans un revêtement spécialement étudié pour le roller in line hockey, qui optimise l'adhérence des roues en polyuréthane. Il doit être rigide et le plus lisse possible pour obtenir une bonne glisse du palet.En France, les terrains sont en moyenne de l'ordre de 40 × 20 m et sont souvent partagés au sein de salles omnisports. De fait on trouve peu de surfaces « parfaites », mais des terrains recouverts de résine ou encore du béton peint très lisse. Ces surfaces sont tout de même très adhérentes et la rondelle peut glisser rapidement.À chaque match, deux arbitres doivent être présents sur le terrain. Ils doivent assurer la protection des joueurs en faisant respecter les règles.En France, contrairement au hockey sur glace, il n'y a pas de hors-jeu. En revanche le hors-jeu existe dans certains pays comme la Belgique. Les charges corporelles sont interdites, contrairement au hockey sur glace. Un joueur ou la crosse d'un joueur adverse ne peut pas être présent dans la zone du gardien.Il s'agit d'un sport collectif de glisse, très rapide, se jouant avec un palet, opposant deux équipes de 5 joueurs présents en même temps sur le terrain (1 gardien de but et 4 joueurs de champs). Une équipe peut comprendre au total 16 joueurs (14 joueurs et 2 gardiens) pouvant se remplacer à tout moment. Le match comprend, en Senior (divisions nationales) 2 périodes de 25 minutes effectives, avec une mi-temps de 10 minutes.Le gardien de but possède un statut particulier. En effet, s'il commet une faute il ne peut être directement sanctionné par une exclusion. Un joueur de l'équipe sera exclu à sa place. Suivant la gravité de la faute, l'exclusion sera plus ou moins longue. Le joueur exclu devra alors se mettre à côté du terrain et pourra rentrer, une fois le temps écoulé, lors du prochain arrêt de jeu.Les exclusions se classent en différentes catégories :mineure (2 min de prison) : trois pénalités mineures d'un même joueur lors d'un même match entraînent une pénalité additionnelle de méconduite de 10 minutes.majeure (5 min de prison) : deux pénalités majeures d'un même joueur lors d'un même match entraînent une pénalité majeure de 5 minutes d'un autre joueur de l'équipe et une pénalité de méconduite pour le joueur concerné pour le match.de méconduite (10 min de prison)de méconduite pour le match : le joueur fautif est exclu du match et du suivant, mais est remplacé par un autre joueur.Deux pénalités de méconduite d'un même joueur lors d'un même tournoi entraînent une pénalité de match.pénalité de match : le joueur est exclu du match et du reste du tournoi. Un autre joueur de l'équipe fera 5 min de prison.Au niveau international, deux instances gèrent ce sport : l'IIHF, fédération internationale de hockey sur glace et la FIRS, fédération internationale de roller sports.Chacune de ces fédérations possède son propre règlement, organise ses propres championnats du monde et est reconnue par certains pays mais pas d'autres. En règle générale, les pays où le hockey sur glace est particulièrement développé (comme la Suède ou la Russie) reconnaissent l'IIHF comme instance internationale (et ne participent donc pas aux championnats organisés par la FIRS).En France, la Fédération française de roller sports est, elle, affiliée à la FIRS. Dans d'autres pays, par exemple en Belgique, les deux fédérations sont présentes, les clubs reconnaissants l'une ou l'autre fédération.(en) Cet article est partiellement ou en totalité issu de l’article de Wikipédia en anglais intitulé « Roller in-line hockey » (voir la liste des auteurs).Roller ou patin à roulettesClub de roller in line hockey | Compétition de roller in line hockey | Joueur de roller in line hockeyRoller in line hockey en FranceChampionnat de France de roller in line hockeyComité international de roller in line hockey, affilié à la Fédération internationale de roller sports (FIRS)Roller Hockey & Fun (RHAF), site d'information français, plus mis à jour depuis 2014World Inline Hockey Association (WILHA), association de promotion du roller-hockeyLigueElite.fr, site d'actualités sur la Ligue Elite de roller-hockey en France, affilié à la Fédération française roller sportsRollerhockeyfeminin.fr, site d'actualités sur le roller-hockey en France, indépendantLes Archives du roller-hockey, site d'archives des résultats et classement, indépendant Portail du sport   Portail du roller et du skateboard"
sport;"Le sport, d'usage récent (XIXe siècle) dans la langue française, est un ensemble d'exercices physiques se pratiquant sous forme de jeux individuels ou collectifs pouvant donner lieu à des compétitions.Le sport, qui ne peut être dénommé ainsi avant le XIXe siècle (gymnastique, exercice physique, voir Pociello, C. Sociologie du sport, 1983), est un phénomène presque universel dans le temps et dans l'espace humain. La Grèce antique, la Rome antique, Byzance, l'Occident médiéval puis moderne, mais aussi l'Amérique précolombienne ou l'Asie, sont tous marqués par l'importance de pratiques physiques. Certaines périodes sont surtout marquées par des interdits.Le terme de « sport » a pour racine le mot de vieux français desport qui signifie « divertissement, plaisir physique ou de l'esprit ». Antérieurement il peut être rattaché au latin portus, comme dans transport, export, import, déporter, déport, etc., qui désignait simplement un port, un lieu de passage marin. En traversant la Manche, desport se mue en « sport » et évacue de son champ la notion générale de loisirs pour se concentrer sur les seules activités physiques et mentales. La langue allemande admet aussi le terme « sport » et sa définition anglaise en 1831 ; la France en fait usage pour la première fois dès 1828, mais à ce moment-là il est essentiellement associé aux courses de chevaux et aux paris sur ces courses de chevaux (Cf. le journal Le Sport). La frontière entre jeux et sports n'est pourtant pas très claire. La Fédération française des échecs fondée en 1921 reçoit ainsi un agrément sportif du Ministère de la Jeunesse et des Sports en 2000, mais uniquement parce qu'elle était une fédération « associée » au CNOSF. Certaines pratiques traditionnelles posent également problème : sport ou jeu ? La question reste encore ouverte.Le sport moderne se définit par quatre éléments indispensables :la mise en œuvre d'une ou plusieurs qualités physiques : activités d'endurance, de résistance, de force, de coordination, d'adresse, de souplesse, etc ;une activité institutionnalisée, ses règles tendent à être identiques pour l'ensemble de la planète ;une pratique majoritairement orientée vers la compétition ;une pratique fédérée (sous la tutelle d'une fédération sportive).Ces piliers qui mettent surtout en avant l'organisation des différentes disciplines sportives n'excluent nullement les pratiques comme le sport-loisir, le sport-aventure, le sport-santé, le sport scolaire ou l'éducation physique et sportive. Si la compétition est prédominante, il existe toutefois d'autres formes de pratique mettant plutôt en avant le plaisir, la santé, l'éducation ou l'épanouissement.Le Conseil de l'Europe propose ainsi la définition suivante dans sa « Charte européenne du sport » (article 2.1) (2001) : « On entend par « sport » toutes formes d'activités physiques qui, à travers une participation organisée ou non, ont pour objectif l'expression ou l'amélioration de la condition physique et psychique, le développement des relations sociales ou l'obtention de résultats en compétition de tous niveaux ».La question de l'histoire du sport bute sur un débat qui oppose deux thèses.Pour un courant de pensée, le sport est un phénomène universel, qui a toujours existé et partout sous des formes très diverses. Ce serait un « invariant culturel » (selon les termes de Frédéric Baillette, enseignant et directeur de la revue Quasimodo). Cette thèse est notamment soutenue en 1991 par le médecin français Jean-Paul Escande (Les avatars du sport moderne, in Ardoino, Brohm, Anthropologie du sport, Perspectives critiques, 1991). Cette thèse est implicitement soutenue par ceux qui parlent de « sport antique », de « sport médiéval », etc. Le médiéviste américain Charles Homer Haskins est le premier historien à utiliser le terme de « sport » dans le cadre d'une étude portant sur le Moyen Âge dans son livre The Latin Litterature of Sport (1927). Au début du XXIe siècle, Wolfgang Decker (Institut d'Histoire du Sport de l'École Supérieure du Sport de Cologne) et Jean-Paul Thuillier (directeur du Département des Sciences de l'Antiquité à l'École normale supérieure) estiment que : « contrairement à ce que l'on estime souvent, le sport n'est pas né à Olympie, pas plus qu'il ne s'est éteint dans l'Attique ou le Péloponnèse. L'Égypte nous offre de nombreuses scènes sportives, entre autres de lutte, dès le IIIe millénaire avant notre ère, et les Romains, héritiers des Étrusques sur bien des points et en particulier dans ce domaine, ont peut-être créé le sport moderne, avec ses spectacles de masse, ses clubs puissants et ses enjeux financiers colossaux ».Pour un autre courant de pensée, le sport est un phénomène apparu à un moment précis de l'histoire et dans un contexte particulier : au sein de l'élite sociale de l'Angleterre industrielle du XIXe siècle. Cette thèse est notamment développée en 1921 par l'écrivain allemand Heinz Risse (Soziologie des Sports, Berlin, 1921 et Sociologie du sport, Presses universitaires de Rennes, 1991) qui estime qu'« il est erroné de regarder le passé avec nos modes de pensée actuels et d'imaginer que les pratiques qui ressemblent à celles que nous connaissons peuvent se rapporter à cette appellation ""sport"" ». Cette thèse est notamment soutenue par l'historien français Roger Chartier et par les sociologues Norbert Elias, et Pierre Bourdieu,. En 2000, l'historien du sport Philippe Lyotard (université de Montpellier) juge qu'« il y a une coupure très nette entre le sport moderne et le sport antique : c’est la notion de record (et donc de performance). Le record et la performance expriment une vision du monde qui est profondément différente entre les Grecs et les modernes. La culture du corps est différente. Pour les Grecs, cette culture est rituelle, culturelle, d’inspiration religieuse, pour les modernes, le corps est une machine de rendement ».À travers l'exemple des joutes au XVe siècle en France et en Espagne, Sébastien Nadot avance dans sa thèse intitulée Joutes emprises et pas d'armes en Castille, Bourgogne et France, 1428-1470 (soutenue à l'EHESS en 2009) que l'on peut effectivement parler de sport au Moyen Âge et que la plupart des historiens confondent la notion de naissance avec celle de démocratisation du sport quand ils évoquent son apparition seulement à partir du XVIIIe siècle. Mais une autre façon de résoudre la question est de forger la notion de « sport moderne » pour distinguer ce phénomène d'autres pratiques historiquement attestées. Dans une étude, une équipe de l'UFR-STAPS de l'université de Bourgogne estime ainsi en 2004 que « Le sport moderne [...] renvoie à l’idéologie de Coubertin, caractérisée par la compétition, la performance, l’entraînement dans des structures institutionnelles (fédérales et scolaires) afin de lutter contre l’oisiveté et les risques de dégénérescence psychologique et physiologique de l’homme ». Cette notion de « sport moderne » est exposée par l'historien américain Allen Guttmann dans From Ritual To Record, The Nature of Modern Sports (1978). Auteur notamment de Sports: The First Five Millennia, Guttmann ne renonce pas à l'emploi du mot « sport » de l'Antiquité à nos jours.Selon l'interprétation large de la notion, le sport est un phénomène universel dans le temps et dans l'espace humain, et, pour reprendre une maxime byzantine, « les peuples sans sport sont des peuples tristes ». Nombre de phénomènes qui paraissent récents, accompagnent en fait l'histoire du sport depuis l'origine : du professionnalisme au dopage, des supporters aux problèmes d'arbitrage.La Grèce, Rome, Byzance, l'Occident médiéval puis moderne, mais aussi l'Amérique précolombienne ou l'Asie, donnent tous une importance au sport. Certaines périodes sont surtout marquées par des interdits concernant le sport, comme c'est le cas en Grande-Bretagne du Moyen Âge à l'époque Moderne. Interrogée sur la question, la Justice anglaise tranche ainsi en 1748 que le cricket n’est pas un jeu illégal. Ce sport, comme tous les autres, figurait en effet sur des édits royaux d'interdiction régulièrement publiés par les monarques britanniques du XIe  au  XVe siècle. En 1477, la pratique d'un « jeu interdit » est ainsi passible de trois ans de prison. Malgré l'interdit, la pratique perdure, nécessitant un rappel quasi permanent à la règle.Le sport est l'une des pierres d'angle de l'éducation humaniste du XVIe siècle. Les Anciens mettaient déjà sur le même plan éducation physique et intellectuelle. Pythagore était un brillant philosophe qui fut également champion de lutte puis entraîneur du grand champion Milon de Crotone. La Renaissance redécouvre les vertus éducatives du sport et, de Montaigne à Rabelais en passant par Girolamo Mercuriale, tous les auteurs à la base du mouvement humaniste intègrent le sport dans l'éducation (relire par exemple Gargantua). Chaque époque a eu son « sport-roi ». L'Antiquité fut ainsi l'âge d'or de la course de chars. Pendant plus d'un millénaire, les auriges, cochers des chars de course, étaient des « stars » adulées par les foules dans tout l'Empire romain. Le tournoi, qui consiste à livrer une véritable bataille de chevaliers, mais « sans haine », fut l'activité à la mode en Occident entre le XIe et le XIIIe siècle (il ne faut pas confondre le tournoi et la joute équestre, version très allégée du tournoi). La violence du tournoi cause sa perte, d'autant que le jeu de paume s'impose dès le XIIIe siècle et jusqu'au XVIIe siècle comme le sport roi en Occident. Ce jeu de raquettes embrase Paris, la France puis le reste du monde occidental. Le XVIIIe siècle voit le déclin du jeu de paume et l'arrivée, ou plutôt le retour, des courses hippiques qui s'imposent comme le sport roi des XVIIIe et XIXe siècles. La succession des courses hippiques fut âprement disputée car le nombre des sports structurés augmente spectaculairement dès la fin du XIXe siècle. Le football devient ensuite et reste encore aujourd'hui (2018) l'incontestable sport « numéro un » sur la planète.Au-delà de ce tableau général coexistent des nuances régionales parfois très marquées. Ainsi, le football tient une place secondaire dans les pays de l'ancien empire britannique. En revanche, il cultive les autres sports que soutenait jadis la bonne société anglaise, du tennis au hockey sur gazon en passant par le rugby et le cricket. Le cricket a ainsi le statut national dans des pays comme l'Inde ou le Pakistan. De même, l'Amérique du Nord a donné naissance à plusieurs sports, le hockey sur glace et le basket-ball au Canada, le baseball et le football américain aux États-Unis, parvenant ainsi à échapper à la vague du football (appelé soccer en Amérique du Nord). En France, le sport roi de la fin du XIXe siècle est le cyclisme qui garde la palme jusqu'au triomphe du football, entre les deux guerres mondiales. Le rugby n'est pas parvenu à mettre fin à la domination de ces deux sports, freiné par une implantation trop régionale.La puissance du mouvement sportif est aujourd'hui considérable, il est une des composantes de la mondialisation. Une fédération internationale comme la FIFA a la capacité de modifier les règlements et d'exiger sa mise en application à la planète entière. Certains ont donc pu estimer que le sport proposerait ainsi un premier modèle de mondialisation réelle.À l'inverse de cette structure centralisée, notons l'existence d'un mouvement sportif plus indépendant, notamment aux États-Unis. La NBA a des règles particulières distinctes de celles de la Fédération internationale de basket-ball, sauf pour les Jeux olympiques pour lesquels c'est la FIBA qui est chargée des épreuves. Le baseball américain illustre encore plus fortement cette décentralisation : les deux ligues qui s'affrontent pour le trophée des World Series - Ligue américaine et Ligue nationale - ne suivent pas les mêmes règles du jeu.La liste suivante regroupe les sports les plus connus, classés par catégories usuelles. D'autres sports pourraient compléter cette liste. Certains sports peuvent appartenir à plusieurs catégories. La présence des catégories « sports mécaniques » et, plus récemment, « sports cérébraux » dans cette liste, longtemps contestée, se justifie par les qualités communes aux sports physiques qu'ils demandent, pratiqués à haut niveau de compétition, comme en particulier la concentration ou l'endurance.La plupart de ces sports ont leur équivalent pour les personnes handicapées : les handisports.Les Jeux olympiques sont une compétition internationale qui regroupe une sélection de disciplines sportives. Ainsi, il est possible de classer les sports entre ceux qui sont inscrits aux Jeux olympiques, dits « Sports olympiques » et ceux qui le sont pas.Un sport ne peut être olympique que s'il fait partie d'une fédération internationale reconnue par le Comité international olympique (c'est-à-dire qui répond à de multiples critères de sélection très stricts). Celles-ci sont alors divisées en trois groupes :les fédérations internationales parmi lesquelles au moins un des sports dont elles ont la gouvernance se trouve inscrit au programme des jeux olympiques d'été (ASOIF, Association of Summer Olympic International Federations) ;les fédérations internationales parmi lesquelles au moins un des sports dont elles ont la gouvernance se trouve inscrit au programme des jeux olympiques d'hiver (AIOWF, Association of International Olympic Winter Sports Federations) ;les fédérations internationales reconnues par le Comité international olympique n'ayant actuellement aucun des sports dont elles ont la gouvernance aux programme des jeux (ARISF, Association of IOC Recognised International Sports Federations).Les sports olympiques actuels furent tous inclus au programme des jeux à un moment spécifique de l’histoire, au cours d'une décision commune prise entre le CIO et les fédérations internationales. Une fois qu'un sport a été désigné comme sport olympique, il ne peut plus être retiré des programmes des jeux (mais le nombre d'épreuves qui composent ce sport peut être revu à chaque édition des jeux), sauf si la fédération internationale qui dirige ce sport est radiée de la liste ASOIF ou AIOWF auquel cas tous les sports qui dépendent d'elle sont radiés du programme (comme ce fut le cas après les jeux de Pékin en 2008 pour la fédération de baseball-softball, la WBSC).D'autres sports peuvent devenir olympiques à l'avenir sous trois conditions :soit le CIO décide d'augmenter le nombre d'épreuves pour l'une des éditions, et un nouveau sport est proposé par une fédération afin d'occuper un certain nombre de ses épreuves (rentrant ainsi en concurrence avec les sports olympiques dont les fédérations internationales souhaitent inscrire plus d'épreuves aux jeux) ;soit le CIO décide de radier une fédération des listes ASOIF ou AIOWF et propose à d'autres fédérations de proposer de nouveaux sports à la place ;à partir des jeux de Tokyo en 2020, le CIO propose pour toutes les futures éditions des jeux d'été que des sports additionnels soient rajoutés au programme juste pour le temps de l'olympiade à venir, le tout au choix de la ville organisatrice. Un sport peut ainsi devenir olympique le temps d'une édition des jeux (où plusieurs si la fédération gouvernant ce sport parvient à la placer dans différents futurs jeux).Si la candidature d'un nouveau sport d'une fédération ARISF est acceptée, la fédération internationale en question prend aussitôt le statut de ASOIF ou AIOWF, même si cette dernière n'a pas pu imposer aux jeux tous les sports dont elle a la gouvernance (dans le cas des fédérations contrôlant plusieurs sports différents). Une fédération déjà ASOIF ou AIOWF qui disposerait de sports non olympiques sous sa gouvernance peut également poser leur candidature pour de futurs jeux.Le tableau ci-dessous présente par ordre alphabétique les sports olympiques et les sports non olympiques par fédérations reconnues par le CIO.Nombre de fédérations ne sollicitent pas la reconnaissance du CIO (sport automobile, notamment) tandis que d'autres sont en phase de reconnaissance par le CIO.La pratique équilibrée d'un sport aide à se maintenir en bonne santé physique et mentale. En revanche, le surmenage sportif et l'absence totale d'exercice physique sont nocifs pour la santé.La pratique du sport régulier maintient notre organisme en bonne santé, réduit le stress et augmente la capacité de réflexion.La pratique d'un sport se décompose en trois types d'activités : l'entraînement sportif, la compétition et la récupération.L'entraînement a pour objectif de former et d'entraîner le pratiquant pour que ses performances augmentent. Pour être bénéfique, l'entraînement doit être réparti sur une succession de séances régulières, progressives et complémentaires les unes aux autres.La compétition a pour objectif de mesurer les sportifs entre eux et de récompenser les meilleurs. Pour de nombreux sportifs, la compétition est le moment le plus fort et le plus agréable de la pratique du sport.Enfin, la pratique d'un sport comprend des phases de récupération et de détente. L'objectif de ces séances est de laisser au corps de l'athlète le temps et le repos nécessaire pour qu'il se remette en état de produire les meilleurs efforts.Chaque discipline fait appel à des compétences sportives particulières.L'équilibre, la force, la motricité, la vitesse, l'endurance, la concentration, le réflexe, la dextérité sont les compétences les plus connues. Certaines disciplines font plutôt appel à une seule compétence alors que d'autres font appel à un éventail de plusieurs compétences. Hormis les compétences sportives, il existe des facteurs physiques déterminants de la performance sportive, ces facteurs sont la force, la vitesse, l'endurance, la souplesse et la coordination des unités motrices (intra et intermusculaire+proprioception).Le succès dans une discipline dépend de la capacité du sportif à exécuter un geste précis. Certaines disciplines consistent à exécuter le geste le plus précis possible en disposant de tout le temps nécessaire à la préparation du geste. Le tir à l'arc est un exemple de ce type de disciplines. D'autres disciplines laissent peu de temps de préparation et le sportif doit ici exécuter son geste de manière spontanée. Le karaté est exemple de ce type de disciplines.La pratique d'un sport fait travailler le système cardio-respiratoire et différents muscles. Elle permet de brûler des calories et donc de prévenir de l'obésité (prévention de l'obésité). Elle incite à avoir une alimentation correcte (alimentation du sportif). Elle facilite l'évacuation de la tension nerveuse accumulée dans la journée (ex : stress chez l'humain). Elle permet la découverte du corps[C'est-à-dire ?] et de ses limites. Elle facilite l'acquisition du sens de l'équilibre, soit dans des situations prévues (exercices de gymnastique), soit dans des situations imprévues (jeux de ballon, sports de combat). Il permet aussi au pratiquant de construire une méthodologie du travail, réutilisable pour d'autres disciplines.Il est recommandé de pratiquer un sport d’intensité moyenne ou, plus simplement, d’exercer une activité physique pendant un temps allant de 50 min à 1 h 30 min si l'on veut avoir un effet sur le maintien ou l'abaissement de son poids, au moins trois fois/semaine. Une étude de l'ANSES en 2020 révèle que « 95% de la population française adulte est exposée à un risque de détérioration de la santé par manque d’activité physique ou un temps trop long passé assis ». Toujours selon cette enquête, 5% des adultes en France ont une activité physique suffisante pour protéger leur santé : les femmes sont plus exposées que les hommes à un manque d’activité physique. Plus d’un tiers des adultes français cumule un haut niveau de sédentarité et une activité physique insuffisante : en conséquence, ils sont plus exposés au risque d’hypertension ou d’obésité et ont un taux de mortalité et de morbidité plus élevés causés par des maladies cardiovasculaires et certains cancers.La marche est l'activité physique la plus pratiquée par un très grand nombre d'adultes et de personnes âgées.Une grande étude taïwanaise publiée en 2011 dans le journal médical The Lancet, a montré qu'une activité physique modérée de quinze minutes par jour ou quatre-vingt-dix minutes par semaine pouvait diminuer la mortalité globale de 14 % contribuant ainsi à une augmentation de l'espérance de vie de trois ans.Le sport donne lieu à la manifestation d'émotions intenses, souvent surmédiatisées. Elles effacent un corps contraint, policé par les heures d'entraînement. Ces émotions se déclenchent lors des confrontations ou des résultats de l'action. Il est exigé du sportif qu'il évacue les émotions dites négatives, tandis qu'il doit vivre à l'excès, en communion avec le public, les émotions dites positives. Cependant cette vision binaire rend difficile la compréhension des émotions humaines. L'univers sportif dilate les enjeux et l'émotion devient si forte qu'elle sort du corps, par effraction en quelque sorte. Au sortir de l'exercice solitaire et fort, elle restaure le lien social, par elle le héros ou l'héroïne retrouve une place dans la société. La pratique intensive du sport induit une difficulté permanente à exprimer ses émotions, une alexithymie, et ces moments sont les seuls prévus pour se libérer et, les mots manquants, le corps surjoue les expressions. Mais bien vite, le star-système impose sa loi, et exige de revenir à des manifestations codifiées, alors qu'il serait plus bénéfique pour tout le monde d'aider la personne à déchiffrer ses émotions.La pratique du sport présente des risques. Le sportif peut se blesser en faisant un faux mouvement, en chutant (entorse, élongation musculaire, claquage, fracture osseuse, traumatisme crânien) ou en recevant un coup. Il peut être victime d'un accident cardiovasculaire (du type infarctus du myocarde).Certains sports présentent des risques réels d'accidents corporels graves, tels que le traumatisme crânien ou la noyade, et leur pratique n'est autorisée qu'avec un équipement adapté, tels que : gilet de sauvetage pour le canoë, casque pour la descente en VTT, harnachement complet pour le gardien de hockey sur glace. Certains sports dits « extrêmes » présentent même de tels risques d'accidents mortels que leur pratique en est interdite.L'activité sportive intensive est source de blessures graves qui peuvent contraindre le sportif à s'arrêter et qui peuvent laisser des séquelles. La pratique d'un sport doit être adaptée à l'âge du pratiquant et à son état de fatigue. Une personne peut être marquée à vie par une activité sportive trop intense dans son enfance. Un sportif peut être obligé d'arrêter la pratique de son sport à la suite de séances d'entraînement ou de compétitions trop dures et trop fréquentes. La gymnastique artistique est l'exemple d'une discipline où de jeunes sportifs sont soumis à des exercices dangereux pour leur santé.La meilleure prévention contre les accidents consiste à pratiquer un sport dans les règles de l'art qui lui sont applicables : apprentissage des gestes techniques, apprentissage des règles de bonne pratique et de sécurité, entraînement régulier, échauffement préalable aux exercices violents, port des protections recommandées, alimentation adaptée avant, pendant et après l'effort, récupération entre les séances d'entraînement et entre les compétitions, respect des interdictions liées aux conditions météorologiques, pratique en groupe, etc. Des pratiques sportives de compensation sont largement recommandées dans le concept d'ergomotricité initié sur les lieux de travail pour lutter contre les accidents du travail. La visite médicale annuelle en début de saison permet d'obtenir l'avis d'un spécialiste sur la capacité d'un individu à pratiquer un sport.Le refus de poursuivre un effort qui semble trop difficile à supporter est un geste de sauvegarde de sa santé. Tels sont les principaux moyens de prévention des accidents. Le dopage est un des risques pour la santé du sportif. Ce problème n'est toutefois pas spécifique au sportif.Le dopage consiste à utiliser des produits qui augmentent la performance par différents moyens tels que l'augmentation de la masse musculaire ou la résistance à la douleur. L'EPO est un exemple de produits dopants.Le dopage est une pratique de certains sportifs professionnels de haut niveau mais également de certains sportifs amateurs. Le dopage est efficace : il permet en général à ceux qui se dopent d'obtenir des performances supérieures à ce qu'elles seraient sans dopage. Le dopage est illicite : le sportif convaincu de dopage est sanctionné. Le dopage est dangereux pour la santé du sportif : certains décès de sportifs pourraient être la conséquence d'un dopage.La lutte et la prévention antidopage existent. Elles concernent tout le monde et, au tout premier plan, les sportifs, leur entourage professionnel et les organisateurs de compétitions. Les contrôles antidopage permettent de déterminer si le sportif a ou n'a pas été dopé pour obtenir son résultat dans la compétition. La déchéance d'un titre et l'exclusion à vie de toute compétition sont des exemples de sanctions.Les sports où les cas de dopages sont les plus connus du grand public sont le cyclisme, l'athlétisme, la natation et l'haltérophilie.Un numéro de la revue International Journal Of Sport Science and Physical Education fait le point sur le problème du dopage dans le sport. On y voit notamment le fait que les médecins du sport sont largement impliqués dans ce problème notamment dans les pays anglo-saxons. On voit également que la loi existante n'est pas adaptée au problème puisqu'en général les seuls punis sont les athlètes ou coureurs, alors que la plupart du temps c'est un système complexe et que tout l'entourage est impliqué voire dans certains cas (Tour de France cycliste), il s'agit pratiquement d'une tradition liée à l'activité qui donne lieu à un véritable rituel initiatique (lié aux pratiques dopantes) pour les participants. Des articles sont également parus sur le sujet dans la Éthique publique (Canada) et dans la Revista brasileira de ciencas do esporto (Brésil). Le dopage y est analysé notamment par Eric Perera comme associé à la pollution du corps, aux notions de pur et d'impur. Les travaux de l'anthropologue Mary Douglas (Purity and Danger. An analysis of the concept of pollution and taboo, 1965) servent de références pour mieux comprendre ce problème. Température Selon que la température est trop ou pas assez élevée, l'organisme peut être soumis respectivement à l'hyperthermie ou à l'hypothermie.En cas de température élevée, on portera des vêtements légers en textile adapté, mais continuant à protéger du soleil, surtout en altitude ou en cas de canicule. Le rendement sportif pourra être maintenu grâce à l'utilisation d'un gilet réfrigérant.En cas de basse température, on utilisera au contraire des vêtements chauds, en particulier pour les extrémités (doigts, orteils...) qui pourraient facilement subir de graves gelures: c'est le cas typique de l'alpinisme d'altitude ou hivernal. Les nageurs en eau froide se trouvent eux aussi soumis à l'hypothermie d'autant plus rapidement que la température de l'eau est basse; préventivement, ils utilisent une combinaison de plongée ou enduisent leur corps de graisse.Quelle que soit la température, le vent ou la vitesse du sportif augmentent considérablement les échanges thermiques par convection: aussi, les sportifs qui sont dans ce cas augmenteront particulièrement les précautions.De même, l'humidité accélère considérablement les échanges thermiques, rendant beaucoup plus difficiles à supporter les froids et chaleurs humides que secs. Pression Une pression trop basse ne permet pas de respirer convenablement, alors que tout sportif a besoin d'échanges respiratoires élevés pour être performant, ou simplement survivre. Ces limites s'observent pour l'alpinisme d'altitude, où la pression atmosphérique en haut de l'Everest n'est qu'environ 1/3 de la pression au niveau de la mer considérée comme pression normale pour vivre par la plupart de l'humanité (le rendement est fortement dégradé pour tous, et beaucoup d'alpinistes se voient obligés d'utiliser des bouteilles d'oxygène sur les sommets de plus de 8 000 m, et un caisson isobare en cas de malaise aigu ou d'accident). Il est bien connu que l'altitude a un impact sur les compétitions sportives en altitude, comme ce fut par exemple le cas aux Jeux olympiques de Mexico.Pour des raisons inverses, le plongeur ne peut descendre à trop grande profondeur sans équipement (scaphandre autonome avec pression régulée).Le sport se pratique durant le parcours scolaire, au travers de multiples APS, au sein d'un club soit hors de tout club. Les clubs sont affiliés à des fédérations. Les clubs organisent les entraînements et mettent leurs moyens à la disposition des compétitions. Les fédérations organisent les compétitions et édictent les règlements.La grande majorité des sportifs est composée de sportifs amateurs, c'est-à-dire d'hommes et de femmes qui pratiquent leur activité sans recevoir aucun salaire en retour. L'amateurisme possède son revers avec un accès limité aux classes populaires. pour certaines activités et l'amateurisme marron, c'est-à-dire la rémunération occulte ou la fourniture d'emplois de complaisance à des sportifs officiellement amateurs.Certains sportifs perçoivent un salaire en retour de leur activité. Ces sportifs sont dits « professionnels ». La plupart d'entre eux sont sous contrat avec un club. Le football en Europe et le basket-ball aux États-Unis sont deux exemples connus de sports pratiqués par des professionnels. Depuis le début des années 1990 et la professionnalisation des Jeux olympiques, longtemps bastion du sport amateur, le phénomène du professionnalisme sportif touche presque l'ensemble des disciplines.La puissance du mouvement sportif est aujourd'hui considérable. Une fédération internationale comme la FIFA a la capacité de modifier les règlements et d'exiger la mise en application à la planète entière à compter d'une date précise. Le sport propose ainsi un modèle de mondialisation[réf. souhaitée].À l'inverse de cette structure centralisée, notons l'existence d'un mouvement sportif plus indépendant, notamment aux États-Unis. La NBA a des règles particulières et il n'est pas question pour elle de se plier à la Fédération internationale de basket-ball. Aux Jeux olympiques, la FIBA est néanmoins chargée du règlement des épreuves, et les joueurs NBA doivent alors y jouer selon les règles communes au reste du monde. Le baseball américain est encore plus caricatural sur ce point, avec deux ligues qui s'affrontent pour le trophée des World Series : l'American et le National n'ont pas les mêmes règles du jeu.Voici une liste des principaux grands évènements sportifs. Cette liste n'est pas exhaustive.Événements internationauxChampionnat du monde de basket-ballChampionnat d'Europe de footballCoupe de l'AmericaCoupe du monde de cricketCoupe du monde de footballCoupe du monde de rugby à XVJeux olympiques d'étéJeux olympiques d'hiverJeux mondiauxLigue des champions de l'UEFARyder CupTournoi des Six NationsÉvénements nationauxLes compétitions sportives sont des formes de spectacles, mais leur scénario n'est pas écrit d'avance. Pendant l'Antiquité, la sculpture ou la poésie furent de bons vecteurs de médiatisation du sport. Avec l'arrivée des médias modernes avec dans l'ordre chronologique la presse écrite, la radio, la télévision puis internet, le sport dispose de puissants supports médiatiques. Ainsi, il existe depuis 1977 des chaînes de télévisions sportives dont l'objet sont la diffusion d'épreuves et d'informations sportives. Certaines sont généralistes et se consacrent à divers sports tandis que d'autres se spécialisent dans une discipline. Parmi les titres de la presse écrite sportive on citera L'Équipe en France, Sports Illustrated aux États-Unis ou La Gazzetta dello Sport en Italie, notamment.Dans certains sports, la médiatisation d'acte antisportif majeur et violent sont souvent interrompus pour ne pas inciter les téléspectateurs à la violence.Un club sportif (CS) est une infrastructure encadrant les sportifs.La recherche suggère que le sport a la capacité de connecter la jeunesse à des modèles de rôle adultes positifs et de fournir des possibilités de développement positif, tout en favorisant l’acquisition et l’application des compétences utiles dans la vie courante. Ces dernières années, le recours au sport pour lutter contre la délinquance, et prévenir l’extrémisme violent et la radicalisation, est devenu plus fréquent, notamment en tant qu’outil pour améliorer l’estime de soi, resserrer les liens sociaux et donner aux participants le sentiment d’être utile.Récemment, le sport a long"
sport;"Un sport individuel est un sport qui oppose des individus, par opposition à un sport collectif où ce sont des équipes qui s’affrontent. Lors des compétitions, seul un individu est récompensé sur sa seule performance.Cela ne signifie pas forcément que le sportif n'appartient pas à une équipe : par exemple le tennis est un sport individuel lorsque les joueurs jouent en simple mais dans le cadre de la Coupe Davis ou Fed Cup pour les féminins, c'est une nation qui gagne la compétition par ajout des résultats en simple. Le double en tennis est par contre une discipline qualifiée de collective. Exclusivement en simple Athlétisme (ne sont pas concernés les disciplines de relais)BoxeEquitationEscaladeGolfHaltérophilieJudo (parfois épreuve par équipe en compétition)LutteNatation (ne sont pas concernés les disciplines de relais)Pentathlon moderneSkeletonSki alpinSki acrobatiqueSnowboardTaekwondoTir sportifTriathlon (ne sont pas concernés les disciplines de relais) Comportant aussi des épreuves doubles Aviron : seul le skiff comporte un seul rameurBadminton : existe en format double et en format par équipeCanoë-kayak : dans les catégories K-1 ou C-1LugePatinage artistique : hors discipline couple et dansPlongeon : hors plongeon synchroniséTennis : en simpleTennis de table : existe en format double et en format par équipeVoile : seul le dériveur en solitaire et la planche à voile Comportant aussi des épreuves par équipe Badminton : existe en format double et en format par équipeBiathlonCyclisme : épreuve sur piste de vitesse individuelle, sur route individuel, VTT, BMXÉquitationEscrimeGymnastiquePatinage de vitesseShort-trackSaut à ski/Combiné nordiqueSki de fondTennis de table : existe en format double et en format par équipeTir à l'arcArt martial / Sport de combatCourse automobile : la Formule 1 est individuel mais pas le Rallye automobile où le copilote est associé à la victoire ; en Championnat du monde d'endurance, c'est par contre une équipe de relais qui est récompensé.Spéléologie Portail du sport"
sport;"La tactique en football décrit comment les joueurs d'une équipe de football se positionnent sur le terrain et opèrent entre eux. Ces dispositifs recouvrent la mise en place initiale d'un plan de jeu (on parle de « formation »), et une fois la partie commencée, le placement des joueurs les uns par rapport aux autres et leurs actions de déplacements, qui peuvent être « orchestrées » à partir du banc de touche par l'entraîneur.Le football étant un sport d'équipe, les questions de tactique et d'intelligence collective sont primordiales. Le résultat d'un match ne dépend pas seulement de l'habileté des joueurs à manier le ballon, mais aussi des choix tactiques des deux équipes qui, suivant le dispositif, peuvent s'avérer décisifs. Le positionnement des joueurs sur le terrain, la capacité à exécuter parfaitement des phases de jeu répétées à l'entraînement, et, d'une manière générale, l'aptitude des joueurs à pratiquer un football homogène et cohérent entrent pour une grande part dans les résultats.La tactique s'adapte aux trois moments principaux du jeu d'une équipe : la possession du ballon, la possession du ballon par l'adversaire, le changement de possession. La tactique implique l'alternative ; la longueur des séquences de jeu amène le joueur à rencontrer une grande variété de situations d'enchaînements offensifs et défensifs.Dès les débuts du football, il s'est avéré que le principe consistant à ce que tous les joueurs se dirigent vers le ballon était une stratégie perdante : un seul joueur pouvant avoir la maîtrise de la balle, la trop grande proximité de ses coéquipiers ne lui sert pas et peut même le gêner. Les partenaires du porteur de balle ont plutôt à se répartir sur le terrain de manière à lui offrir le maximum de possibilités de passes, tout en restant aptes à défendre leur camp en cas de perte de balle.Certains joueurs, de par leurs qualités physiques ou techniques, sont plus aptes à aller marquer des buts, ou au contraire montrent une grande efficacité de récupération de balle. Il a donc été naturel d'affecter les premiers aux tâches dites offensives, près du but adverse (ils sont appelés « avants », et plus communément aujourd'hui « attaquants »), et les seconds aux tâches dites défensives, à proximité de leur propre but, de manière à en empêcher l'accès aux attaquants adverses (les « arrières » ou « défenseurs »). La règle du hors-jeu, introduite en 1866 en même temps que l'autorisation des passes en avant,, permet d'éviter la formation de deux groupes de joueurs, chacun devant un but, en imposant aux uns et aux autres de se déplacer sur le terrain en fonction des actions de jeu. Un joueur ne pouvant pas multiplier les courses pendant 90 minutes, la liaison entre les lignes d'attaque et de défense est assurée par les joueurs se trouvant dans l'« entre-jeu », initialement appelés « demis », plus connus depuis les années 1970 comme des « milieux de terrain ».Défense, milieu de terrain et attaque sont des concepts constants au cours de l'évolution des dispositifs tactiques, qui sont généralement basés sur ce modèle en trois lignes.Dans le football moderne, et notamment depuis la révolution du football total apparu dans les années 1970, chaque joueur est généralement appelé à participer au jeu collectif de son équipe, quelle que soit la phase de jeu, qu'elle soit offensive ou défensive, avec ou sans la possession de la balle. Ainsi, un avant doit tenter de perturber le jeu adverse quand il n'a pas la balle, par un pressing qui est d'autant plus efficace qu'il est collectif. Inversement, un arrière peut venir apporter le surnombre au cours des phases offensives ou de montée de balle. Les milieux de terrain peuvent s'intégrer aux lignes avant ou arrière en fonction des circonstances, si bien que le dispositif tactique peut radicalement changer au cours de la partie. La capacité d'adaptation du dispositif tactique, selon les circonstances (but marqué ou encaissé, changement de joueur dans l'équipe adverse, expulsion ou blessure, état de la domination ou de l'initiative…) est aujourd'hui souvent déterminante. Par conséquent, la polyvalence d'un joueur est un atout appréciable qu'un entraîneur pourra utiliser quand il le jugera nécessaire. La polyvalence et l'adaptation priment dans le jeu moderne sur le jeu au poste. Les rôles d'attaquants ou de défenseurs qu'un entraîneur peut attribuer à certains de ses joueurs peuvent n'être que temporaires, selon la menace que leurs actions opèrent.« Quand vous disputez un match, c'est statistiquement prouvé qu'un joueur a le ballon trois minutes en moyenne. Donc, le plus important c'est ce que vous faites les 87 minutes pendant lesquelles vous n'avez pas le ballon. C'est ce qui détermine si vous êtes un bon joueur ou pas. »— Johan CruyffOn lit dans la presse des années 1950 : « Au début, le football se jouait à 10 devant, aujourd'hui, il se joue à 10 derrière ». Si le constat est exagéré, il traduit l'évolution tactique connue par le football depuis la fin du XIXe siècle.Le passage du « dribbling game » au « passing game » constitue une première révolution, entre 1860 et 1880. À son origine, le football est très individualiste : les joueurs, tous « attaquants », se ruent vers le but ballon au pied, en enchaînant les dribbles, selon le schéma de jeu du « kick and rush » (balancer le ballon devant et courir après). L'efficacité du geste, l'évolution des règles (l'autorisation des passes vers l'avant et l'apparition du hors-jeu) et l'amélioration continue de la qualité des ballons et des terrains va contribuer à transformer le football en jeu de passes, d'abord en Écosse puis dans toute l'Angleterre, notamment après la victoire en finale de la Coupe d'Angleterre de Blackburn Olympic en 1883. Cependant, avec l'obligation jusqu'en 1925 de compter trois joueurs adverses entre la ligne de but et le joueur à la réception d'une passe, un avant-centre devait toujours avoir de solides qualités de dribble pour espérer conclure une action.Le WM règne en maître dans le football européen jusqu'en 1953 et la défaite des Anglais à domicile face aux Hongrois. Après que de nombreux entraîneurs aient tenté de trouver une parade au WM, Gusztáv Sebes conçoit pour le Budapest Honvéd et la sélection de Hongrie une tactique basée sur les permutations pendant le jeu, une grande nouveauté. En reculant, l'avant-centre propose un point d'appui à ses deux milieux offensifs, qui peuvent monter à sa place et entraîner un surnombre. Les Hongrois sont les premiers à prôner le dépassement du rôle. Ce principe novateur favorise le passage au 4-2-4. Les Brésiliens adoptèrent cette formule du 4-2-4 et la firent évoluer progressivement en 4-3-3 durant les années 1960 ; ce positionnement restera majoritaire jusqu'aux années 1970.En parallèle de cette histoire des tactiques offensives, il existe également une école défensive. Le « Verrou suisse » mis en place dès les années 1930 est le modèle de tous les bétons (français) et autres Catenaccio (italien) qui prennent le relais après la Seconde Guerre mondiale. En France, certaines formations deviennent réputées pour leurs stratégies défensives comme Lyon, Strasbourg et surtout Bordeaux, « la forteresse imprenable »[réf. nécessaire]. L'émergence de milieux de terrain créatifs dans les années 1970 et 1980, à la manière de Cruyff, Platini et Maradona, exige de nouvelles adaptations défensives[réf. nécessaire].La culture tactique diffère dans les différents championnats, l'Angleterre par exemple étant longtemps restée considérée comme un parent pauvre dans ce domaine. Avec la rapidité des transferts de joueurs, les cultures tactiques deviennent cependant moins le fait des championnats et des clubs que des entraîneurs, dont les plus connus et les plus durables au haut niveau développent des préférences pour tel ou tel schéma. On note également un certain nivellement tactique, grâce au développement de la vidéo notamment[réf. nécessaire].L'évolution connue vers un jeu de plus en plus défensif semble avoir atteint ses limites dans les années 2000. Les défenses à cinq joueurs deviennent rares, et compter moins de trois ou quatre joueurs à vocation offensive parmi ses milieux et attaquants est considéré généralement comme contre-productif. L'accent est mis sur la polyvalence et le resserrement des lignes de joueurs, souvent résumés dans l'expression de « bloc-équipe », dans le but de réduire le temps et l'espace disponibles à l'adversaire. L'animation défensive s'uniformise en Europe autour de fondamentaux invariables : participation des dix joueurs de champ, replacement, pressing raisonné des attaquants, défense en zone et en ligne. L'animation offensive est le terrain de plus d'expérimentations, de créativité (de un à trois attaquants, avec ou sans ailiers excentrés, avec ou sans meneur de jeu axial).Deux des principaux schémas tactiques jusqu'aux années 1950           À l'origine, de nombreux dispositifs sont utilisés : 2-2-6, 1-1-8.À partir des années 1880 s'impose le 2-3-5, schéma tactique adapté à l'évolution aux blocs d'équipes aptes à alterner la défense massive et un football offensif. Il est composé de trois lignes : deux arrières, trois demis et cinq avants. C'est l'équipe de Cambridge qui est la première à placer trois milieux de terrain et c'est l'équipe de Preston North End Football Club qui popularise ce schéma tactique avec lequel elle réussit le premier doublé coupe-championnat d'Angleterre en 1889. Ce dispositif « en pyramide » est utilisé par la majorité des clubs pendant plusieurs décennies.Avec l'assouplissement de la règle du hors-jeu en 1925, les données du problème changent, et Herbert Chapman (entraîneur de football anglais) met au point une tactique révolutionnaire, dite du « WM », composée de trois défenseurs, deux demis, deux inters et trois attaquants,. Cette tactique s'impose à travers l'Europe et conforte l'Angleterre dans son rôle de pays « inventeur » du football. Elle offre à Chapman une collection de trophées gagnés avec Arsenal. L'AS Cannes est l'un des premiers clubs français à adopter cette tactique dès 1931[réf. nécessaire].Cette formation est en fait une variante du W-M pour la première fois utilisée par Boris Arkadiev, entraîneur du Dynamo Moscou dans les années 40 et qui sera sa marque de fabrique. Les 2 demis et 2 inters du W-M étaient disposé sur le papier comme formant un losange : un demi à vocation défensive, un demi et un inter qui joue plus ou moins au même niveau et un inter qui était à vocation offensive. Cependant au cours d'un match, les 2 demis, 2 inters ainsi que les 3 avants avaient pour but de constamment dézonner afin d'empêcher l'adversaire d'effectuer un marquage individuel. De plus, les joueurs avaient pour consignes d'effectuer beaucoup de passes courtes. Ce style de jeu est aux origines du football total et du tiki-taka. Les journalistes de l'époque ont appelé cette formation : le désordre organisé.Selon Aksel Vartanyan, historien du football russe, le demi défensif était quelquefois tellement bas qu'il pouvait être considéré comme un quatrième défenseur. Il serait donc le premier entraîneur à avoir utilisé une forme de défense à 4 dans l'histoire du football.Cette formation (quatre défenseurs, deux milieux de terrain et quatre attaquants) est assez peu répandue comme formation de base de par la faiblesse de son milieu de terrain.Elle est le plus souvent une version du 4-4-2 en phase d'attaque, ou formation utilisée notamment en fin de partie (par remplacement de milieux de terrains par des attaquants) par une équipe qui doit absolument marquer.La plupart du temps, elle se résume à un 4-4-2 offensif, avec deux attaquants prenant en charge les couloirs et épaulant les deux avants-centres.Cette formation fut popularisée à la suite de l'exploit de l'équipe nationale de Hongrie qui choisit ce 4-2-4 pour contrer le fameux WM des Anglais. Ce choix tactique leur permit de faire chuter l'Angleterre pour la première fois de son histoire à Wembley, puis par la suite utilisée par le Brésil et qui lui a permis de remporter ses deux premières Coupes du monde. Aujourd'hui elle est devenue complètement désuète, et a presque disparu du football professionnel !À noter, tout de même, qu'il s'agit plus d'une formation intermédiaire entre le 4-3-3 à deux récupérateur et le 4-4-2, qu'un pur 4-2-4 à deux pointes centrales et deux ailiers.Néanmoins le Milan AC, par le biais de son ancien entraîneur Leonardo, l'a remise au goût du jour et plus récemment, Louis Van Gaal, lors de son passage au Bayern de Munich, a réussi à atteindre la finale de la Ligue des champions (2009-2010, perdue contre l'Inter Milan) et a décroché un titre de champion d'Allemagne (2009-2010), s'appuyant sur la technique, la vivacité, et la capacité d'élimination en un contre un de ses ailiers Arjen Robben et Franck Ribéry, tandis que les deux attaquants de pointe se chargent de créer des espaces.L'ancien entraîneur de la Juventus de Turin, Antonio Conte, a utilisé cette tactique lorsqu'il a repris l'équipe en septembre 2011. Depuis, peu d'entraîneurs, comme Roger Schmidt ou encore Hein Vanhaezebrouck, continuent d'employer un 4-2-4, notamment en Ligue des champions[réf. nécessaire].Le 4-4-2 (quatre défenseurs, quatre milieux de terrain, deux attaquants) est l'un des schémas classiques du football actuel. Il existe sous deux formes : le 4-4-2 classique, à plat, ou carré (à droite), et le 4-4-2 losange ou diamant (à gauche). Au niveau de la défense et de l'attaque ces deux formes sont identiques (deux arrières centraux, deux arrières latéraux, et deux avants). C'est au milieu de terrain que la différence est notable.Dans le 4-4-2 classique Le milieu de terrain est composé de deux milieux défensifs : généralement un relayeur et un récupérateur. Il y a également deux milieux offensifs latéraux, un à gauche et un à droite, qui sont chargés de construire le jeu et de combiner avec les autres joueurs offensifs, sur leur aile comme dans l'axe. Les deux milieux offensifs peuvent tout à fait être des ailiers. Seulement, ils auront des consignes défensives (qu'ils respectent ou pas). Manchester United, qui effectua le triplé en 1999, jouait dans cette configuration. C'est l'une des tactiques les plus offensives du football moderne, avec 4 joueurs offensifs, contrairement aux autres configurations.Dans le 4-4-2 losange (ou 4-3-1-2 et 4-1-3-2 selon le profil de base des joueurs utilisés) Qui se joue soit avec un milieu défensif, deux milieux latéraux ou relayeurs, et un milieu offensif (10). Soit avec trois milieux défensifs : un milieu récupérateur dans une position axiale et deux milieux relayeurs occupant les couloirs. Mais ceux-ci ont un profil beaucoup plus défensif que des milieux latéraux. En effet, ils doivent épauler le récupérateur dans sa tâche défensive, sans quoi il se retrouverait bien seul. Devant ces trois milieux défensifs se tient un milieu offensif axial (un meneur de jeu, même si cette appellation indique un rôle, et non un placement sur le terrain). Il est le métronome de son équipe et doit se montrer particulièrement décisif dans ses passes pour les deux attaquants afin de compenser l'absence de véritables milieux de débordement. Cette formation est appelée en anglais diamond (losange). L'équipe du Milan AC de Carlo Ancelloti l'utilisait, tout comme Laurent Blanc aux Girondins de Bordeaux lors du titre de champion de France en 2009.Le 4-3-3 (quatre défenseurs, trois milieux de terrain et trois attaquants) possède une défense qui évolue souvent en ligne comme pour le 4-4-2, mais le milieu de terrain change de fonction. Il est souvent à vocation plus défensive, et doit faire parvenir la balle rapidement à l'attaque. Celle-ci est composée d'un attaquant de pointe et de deux ailiers. Le profil des joueurs la composant est le suivant : une défense classique avec de préférence des latéraux offensifs, trois milieux de terrains (deux relayeurs, et un récupérateur qui fonctionnent comme dans un 4-4-2 losange) avec un important volume de jeu (pour pallier l'absence du quatrième élément), deux ailiers rapides et bons frappeurs et un attaquant de pointe de préférence athlétique et doté d'un bon jeu de tête. C'est cette organisation qui permit à l'Angleterre d'être championne du monde en 1966, et au FC Barcelone de remporter la Ligue des Champions lors de la saison 2008-2009 ainsi que le Championnat d'Espagne de football et la Coupe du Roi cette année-là. Le 4-3-3 a permis à José Mourinho et Chelsea FC de devenir champions d'Angleterre deux fois de suite avec 91 et 95 points (record en Premier League). Ce dispositif, utilisé par le Belge Raymond Goethals dans les années 1990, a permis à Luis Fernandez de remporter la coupe d'Europe des vainqueurs de coupes avec le PSG en 1996, ainsi qu'à l'Olympique Lyonnais de remporter sept titres de champion de France consécutivement.À noter que le 4-1-4-1, comme l'intitulent certains entraîneurs, n'est autre qu'une version défensive du 4-3-3. Dans un 4-1-4-1, il y a toujours un récupérateur et deux relayeurs. Seulement, les deux ailiers sont des milieux offensifs à la charge défensive plus importante.Le 4-2-3-1 (4 défenseurs, 5 milieux, 1 attaquant) est un système qui vise, comme le 4-3-3, à étouffer son adversaire au milieu de terrain par l'utilisation de deux milieux défensifs, généralement un relayeur doté d'une bonne relance et un récupérateur (qui, par son activité, récupère un grand nombre de ballon). Le milieu est celui d'un 4-4-2 carré auquel on ajoute un milieu offensif axial, chargé d'animer le jeu et d'avoir la vista et la technique nécessaires pour créer à lui seul des opportunités de but. L'attaque n'est elle plus animée que par un seul buteur ou finisseur, ce qui permet stratégiquement de libérer un joueur supplémentaire dans la construction du jeu ; il sera néanmoins épaulé par trois milieux offensifs. Cette formation met en valeur les capacités d'un meneur de jeu.Ce fut la tactique principalement utilisée par Raymond Domenech lors de la coupe du monde 2006 (dans le schéma exact expliqué auparavant, Patrick Vieira étant un vrai milieu axial à l'anglaise, avec une grosse activité verticale, aux côtés d'un Claude Makelele plus limité au travail de récupération). C'est aussi l'un des dispositifs les plus utilisés par Roberto Mancini à Manchester City ou José Mourinho au Real Madrid et par la majorité des entraîneurs de Ligue 1. Roger Lemerre a opté pour cette configuration lors de l'Euro 2000 afin de mettre Zinédine Zidane dans les meilleures dispositions. Aimé Jacquet l'a employé à de nombreuses reprises, notamment lors des matches de poule de la Coupe du monde 1998, avant de finir la compétition dans un 4-3-2-1 particulièrement défensif. Jacques Santini, lors de l'Euro 2004, avait finalement opté pour un 4-4-2 losange avec Zidane en numéro 10 et Robert Pirès et Patrick Vieira en relayeurs. Il est également utilisé par Didier Deschamps lors de l'Euro 2016 et de la Coupe du monde 2018 .À noter que le système dit de 4-4-1-1 peut être clairement considéré comme un 4-2-3-1.Cette formation à vocation défensive se base normalement sur trois arrières centraux, dont l'un des joueurs peut prendre le rôle de libéro. Les arrières latéraux supplémentaires viennent soutenir le milieu de terrain. Cette formation est très comparable au 3-5-2 mais elle utilise des latéraux plus défensifs. Au contraire du 3-5-2 elle est en général utilisée par des équipes faibles techniquement qui refusent le jeu.Il existe néanmoins des variantes au sein même de ce système. Si le principe demeure le même (gagner la bataille du milieu et ainsi s'assurer la maîtrise du ballon), il en existe deux principales versions : la version offensive, avec deux milieux défensifs évoluant devant la défense et un trio d'animation en soutien des deux attaquants ; et la version défensive, constituée d'une ligne de quatre récupérateurs devant la défense et d'un seul meneur axial derrière les deux attaquants. Ces formations sont principalement utilisées par des équipes sud-américaines.Cette formation absolument défensive, parfois appelée Catenaccio, est généralement développée durant le cours du match par des équipes ayant déjà marqué suffisamment de buts ou voulant à tout prix éviter la défaite et opérant en contre-attaque. Dans le cas du 5-4-1, on retrouve souvent une défense à quatre avec un libéro, destiné à bloquer les espaces et intercepter d'éventuels ballons en profondeur. Le milieu est lui disposé comme celui d'un 4-4-2 carré. Il s'agit notamment de la tactique utilisée par José Mourinho avec l'Inter Milan lors du match retour au Nou Camp qui lui permet d'arracher la qualification pour la finale de la Ligue des Champions (même si défaite au match retour 1-0, victoire au match aller 3-1).[réf. nécessaire]Quasi identique au 5-4-1 dans le placement des joueurs, mais en plus offensif dans le comportement, cette formation comprend une ligne de trois défenseurs axiaux, une ligne au centre de deux milieux récupérateurs/relayeurs et deux arrières latéraux capables de dédoubler et lancer en profondeur les deux ailiers en attaque qui suppléent l'avant centre. Bien qu'elle puisse offrir une grande polyvalence et permette un surnombre lors des phases de possession, cette tactique mise avant tout sur les contres et les percées par les ailes. Elle peut aussi être utilisée en losange.Les arrières très offensifs envoient les ailiers dans la profondeur ou parfois quand les ailiers sont sur le côté du pied faible (un gaucher jouant ailier droit, comme Lionel Messi par exemple) dédoublent. Cette tactique est utilisée par de plus en plus de clubs, on peut citer comme équipes utilisant ce dispositif depuis plusieurs saisons le Genoa ou son ami historique le SSC Napoli qui maîtrise actuellement ce dispositif avec beaucoup de savoir-faire. Il a aussi été appliqué par Diego Maradona avec l'Argentine lors des éliminatoires de la Coupe du monde 2010, avant d'imposer un 4-2-3-1 plus classique lors de la compétition en Afrique du Sud. On a pu récemment voir le FC Barcelone adopter cette tactique.Compétence (sport)Textes officiels concernant l'éducation physique et sportive en FranceComposition d'une équipe de rugby à XVD. Herr, À propos des domaines d'action, Revue EP Strasbourg, no 2, CRDP Strasbourg, 1995Tactique en footballLexique tactique / 1 : postes et rôles, les Cahiers du football, septembre 2014.Lexique tactique / 2 : principes et philosophies, les Cahiers du football, septembre 2014.Benigni, A. et al. - Football. - Paris : Éditions De Vecchi, 1999. - 403 p. -  (ISBN 2-7328-6739-X) - Cote Dewey : 796.334 F687 1999. - (Les schémas de jeu depuis les origines, pages 271-278).Jean-Francis Gréhaigne, L'organisation du jeu en football, Actio, Paris, 1992  (ISBN 2906411086)Raphaël Cosmidis, Gilles Juan, Christophe Kuchly, Julien Momont, Comment regarder un match de foot ?, Solar, Paris, 2016  (ISBN 2263071687) Portail du football"
